{"id": "2507.18808", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18808", "abs": "https://arxiv.org/abs/2507.18808", "authors": ["Miguel Saavedra-Ruiz", "Samer B. Nashed", "Charlie Gauthier", "Liam Paull"], "title": "Perpetua: Multi-Hypothesis Persistence Modeling for Semi-Static Environments", "comment": "Accepted to the IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS 2025) Code available at\n  https://github.com/montrealrobotics/perpetua-code. Webpage and additional\n  videos at https://montrealrobotics.ca/perpetua/", "summary": "Many robotic systems require extended deployments in complex, dynamic\nenvironments. In such deployments, parts of the environment may change between\nsubsequent robot observations. Most robotic mapping or environment modeling\nalgorithms are incapable of representing dynamic features in a way that enables\npredicting their future state. Instead, they opt to filter certain state\nobservations, either by removing them or some form of weighted averaging. This\npaper introduces Perpetua, a method for modeling the dynamics of semi-static\nfeatures. Perpetua is able to: incorporate prior knowledge about the dynamics\nof the feature if it exists, track multiple hypotheses, and adapt over time to\nenable predicting of future feature states. Specifically, we chain together\nmixtures of \"persistence\" and \"emergence\" filters to model the probability that\nfeatures will disappear or reappear in a formal Bayesian framework. The\napproach is an efficient, scalable, general, and robust method for estimating\nthe states of features in an environment, both in the present as well as at\narbitrary future times. Through experiments on simulated and real-world data,\nwe find that Perpetua yields better accuracy than similar approaches while also\nbeing online adaptable and robust to missing observations.", "AI": {"tldr": "Perpetua\u662f\u4e00\u79cd\u7528\u4e8e\u5efa\u6a21\u534a\u9759\u6001\u7279\u5f81\u52a8\u6001\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u5148\u9a8c\u77e5\u8bc6\u3001\u591a\u5047\u8bbe\u8ddf\u8e2a\u548c\u81ea\u9002\u5e94\u9884\u6d4b\uff0c\u4f18\u4e8e\u7c7b\u4f3c\u65b9\u6cd5\u3002", "motivation": "\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\uff0c\u673a\u5668\u4eba\u9700\u8981\u9884\u6d4b\u534a\u9759\u6001\u7279\u5f81\u7684\u672a\u6765\u72b6\u6001\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u5efa\u6a21\u52a8\u6001\u7279\u5f81\u3002", "method": "\u901a\u8fc7\u6df7\u5408\u201c\u6301\u4e45\u6027\u201d\u548c\u201c\u6d8c\u73b0\u201d\u6ee4\u6ce2\u5668\uff0c\u4ee5\u8d1d\u53f6\u65af\u6846\u67b6\u5efa\u6a21\u7279\u5f81\u6d88\u5931\u6216\u91cd\u73b0\u7684\u6982\u7387\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cPerpetua\u5728\u6a21\u62df\u548c\u771f\u5b9e\u6570\u636e\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u5177\u6709\u5728\u7ebf\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "Perpetua\u4e3a\u52a8\u6001\u73af\u5883\u4e2d\u7684\u7279\u5f81\u72b6\u6001\u9884\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.18819", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.18819", "abs": "https://arxiv.org/abs/2507.18819", "authors": ["Trent Weiss", "Madhur Behl"], "title": "Probabilistic Collision Risk Estimation through Gauss-Legendre Cubature and Non-Homogeneous Poisson Processes", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Overtaking in high-speed autonomous racing demands precise, real-time\nestimation of collision risk; particularly in wheel-to-wheel scenarios where\nsafety margins are minimal. Existing methods for collision risk estimation\neither rely on simplified geometric approximations, like bounding circles, or\nperform Monte Carlo sampling which leads to overly conservative motion planning\nbehavior at racing speeds. We introduce the Gauss-Legendre Rectangle (GLR)\nalgorithm, a principled two-stage integration method that estimates collision\nrisk by combining Gauss-Legendre with a non-homogeneous Poisson process over\ntime. GLR produces accurate risk estimates that account for vehicle geometry\nand trajectory uncertainty. In experiments across 446 overtaking scenarios in a\nhigh-fidelity Formula One racing simulation, GLR outperforms five\nstate-of-the-art baselines achieving an average error reduction of 77% and\nsurpassing the next-best method by 52%, all while running at 1000 Hz. The\nframework is general and applicable to broader motion planning contexts beyond\nautonomous racing.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGauss-Legendre Rectangle (GLR)\u7684\u7b97\u6cd5\uff0c\u7528\u4e8e\u9ad8\u901f\u81ea\u52a8\u9a7e\u9a76\u8d5b\u8f66\u4e2d\u7684\u78b0\u649e\u98ce\u9669\u4f30\u8ba1\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u78b0\u649e\u98ce\u9669\u4f30\u8ba1\u65b9\u6cd5\u5728\u9ad8\u901f\u8d5b\u8f66\u4e2d\u8868\u73b0\u4fdd\u5b88\u6216\u8fc7\u4e8e\u7b80\u5316\uff0c\u65e0\u6cd5\u6ee1\u8db3\u7cbe\u786e\u3001\u5b9e\u65f6\u7684\u9700\u6c42\u3002", "method": "GLR\u7b97\u6cd5\u7ed3\u5408Gauss-Legendre\u79ef\u5206\u548c\u975e\u9f50\u6b21\u6cca\u677e\u8fc7\u7a0b\uff0c\u5206\u4e24\u9636\u6bb5\u4f30\u8ba1\u78b0\u649e\u98ce\u9669\uff0c\u8003\u8651\u8f66\u8f86\u51e0\u4f55\u548c\u8f68\u8ff9\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u5728446\u4e2a\u8d85\u8f66\u573a\u666f\u7684\u5b9e\u9a8c\u4e2d\uff0cGLR\u5e73\u5747\u8bef\u5dee\u51cf\u5c1177%\uff0c\u6027\u80fd\u4f18\u4e8e\u5176\u4ed6\u4e94\u79cd\u5148\u8fdb\u65b9\u6cd5\uff0c\u4e14\u8fd0\u884c\u9891\u7387\u8fbe1000 Hz\u3002", "conclusion": "GLR\u7b97\u6cd5\u5728\u9ad8\u901f\u81ea\u52a8\u9a7e\u9a76\u8d5b\u8f66\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u5177\u6709\u901a\u7528\u6027\uff0c\u53ef\u6269\u5c55\u81f3\u5176\u4ed6\u8fd0\u52a8\u89c4\u5212\u573a\u666f\u3002"}}
{"id": "2507.18820", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.18820", "abs": "https://arxiv.org/abs/2507.18820", "authors": ["Rachel Ringe", "Robin Nolte", "Nima Zargham", "Robert Porzel", "Rainer Malaka"], "title": "MetaMorph -- A Metamodelling Approach For Robot Morphology", "comment": "Copyright 2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "Robot appearance crucially shapes Human-Robot Interaction (HRI) but is\ntypically described via broad categories like anthropomorphic, zoomorphic, or\ntechnical. More precise approaches focus almost exclusively on anthropomorphic\nfeatures, which fail to classify robots across all types, limiting the ability\nto draw meaningful connections between robot design and its effect on\ninteraction. In response, we present MetaMorph, a comprehensive framework for\nclassifying robot morphology. Using a metamodeling approach, MetaMorph was\nsynthesized from 222 robots in the IEEE Robots Guide, offering a structured\nmethod for comparing visual features. This model allows researchers to assess\nthe visual distances between robot models and explore optimal design traits\ntailored to different tasks and contexts.", "AI": {"tldr": "MetaMorph\u6846\u67b6\u901a\u8fc7\u5143\u5efa\u6a21\u65b9\u6cd5\u5206\u7c7b\u673a\u5668\u4eba\u5f62\u6001\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5206\u7c7b\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3aHRI\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u89c6\u89c9\u7279\u5f81\u6bd4\u8f83\u5de5\u5177\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u5916\u89c2\u5206\u7c7b\u65b9\u6cd5\u8fc7\u4e8e\u5bbd\u6cdb\u6216\u5c40\u9650\u4e8e\u62df\u4eba\u5316\u7279\u5f81\uff0c\u65e0\u6cd5\u5168\u9762\u5206\u7c7b\u6240\u6709\u673a\u5668\u4eba\u7c7b\u578b\uff0c\u9650\u5236\u4e86\u8bbe\u8ba1\u4e0e\u4ea4\u4e92\u6548\u679c\u7684\u7814\u7a76\u3002", "method": "\u4eceIEEE Robots Guide\u4e2d\u7684222\u4e2a\u673a\u5668\u4eba\u6570\u636e\u4e2d\u63d0\u53d6\u7279\u5f81\uff0c\u91c7\u7528\u5143\u5efa\u6a21\u65b9\u6cd5\u6784\u5efaMetaMorph\u6846\u67b6\u3002", "result": "MetaMorph\u63d0\u4f9b\u4e86\u673a\u5668\u4eba\u89c6\u89c9\u7279\u5f81\u7684\u5206\u7c7b\u548c\u6bd4\u8f83\u65b9\u6cd5\uff0c\u652f\u6301\u7814\u7a76\u8005\u63a2\u7d22\u8bbe\u8ba1\u4e0e\u4efb\u52a1\u3001\u573a\u666f\u7684\u6700\u4f18\u5339\u914d\u3002", "conclusion": "MetaMorph\u4e3a\u673a\u5668\u4eba\u5916\u89c2\u8bbe\u8ba1\u53ca\u5176\u5bf9\u4ea4\u4e92\u5f71\u54cd\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u66f4\u7cbe\u786e\u7684\u5de5\u5177\u3002"}}
{"id": "2507.18847", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18847", "abs": "https://arxiv.org/abs/2507.18847", "authors": ["Pinhao Song", "Yutong Hu", "Pengteng Li", "Renaud Detry"], "title": "Equivariant Volumetric Grasping", "comment": "19 pages", "summary": "We propose a new volumetric grasp model that is equivariant to rotations\naround the vertical axis, leading to a significant improvement in sample\nefficiency. Our model employs a tri-plane volumetric feature representation --\ni.e., the projection of 3D features onto three canonical planes. We introduce a\nnovel tri-plane feature design in which features on the horizontal plane are\nequivariant to 90{\\deg} rotations, while the sum of features from the other two\nplanes remains invariant to the same transformations. This design is enabled by\na new deformable steerable convolution, which combines the adaptability of\ndeformable convolutions with the rotational equivariance of steerable ones.\nThis allows the receptive field to adapt to local object geometry while\npreserving equivariance properties. We further develop equivariant adaptations\nof two state-of-the-art volumetric grasp planners, GIGA and IGD. Specifically,\nwe derive a new equivariant formulation of IGD's deformable attention mechanism\nand propose an equivariant generative model of grasp orientations based on flow\nmatching. We provide a detailed analytical justification of the proposed\nequivariance properties and validate our approach through extensive simulated\nand real-world experiments. Our results demonstrate that the proposed\nprojection-based design significantly reduces both computational and memory\ncosts. Moreover, the equivariant grasp models built on top of our tri-plane\nfeatures consistently outperform their non-equivariant counterparts, achieving\nhigher performance with only a modest computational overhead. Video and code\ncan be viewed in: https://mousecpn.github.io/evg-page/", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4f53\u79ef\u6293\u53d6\u6a21\u578b\uff0c\u5177\u6709\u5782\u76f4\u8f74\u65cb\u8f6c\u7b49\u53d8\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u6837\u672c\u6548\u7387\u3002\u91c7\u7528\u4e09\u5e73\u9762\u4f53\u79ef\u7279\u5f81\u8868\u793a\uff0c\u7ed3\u5408\u53ef\u53d8\u5f62\u53ef\u64cd\u7eb5\u5377\u79ef\uff0c\u4fdd\u7559\u7b49\u53d8\u6027\u540c\u65f6\u9002\u5e94\u5c40\u90e8\u51e0\u4f55\u3002", "motivation": "\u63d0\u9ad8\u6293\u53d6\u6a21\u578b\u7684\u6837\u672c\u6548\u7387\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u65cb\u8f6c\u7684\u7b49\u53d8\u6027\u3002", "method": "\u4f7f\u7528\u4e09\u5e73\u9762\u7279\u5f81\u8868\u793a\uff0c\u8bbe\u8ba1\u6c34\u5e73\u5e73\u9762\u7279\u5f81\u5bf990\u5ea6\u65cb\u8f6c\u7b49\u53d8\uff0c\u5176\u4ed6\u4e24\u5e73\u9762\u7279\u5f81\u4e0d\u53d8\u3002\u5f15\u5165\u53ef\u53d8\u5f62\u53ef\u64cd\u7eb5\u5377\u79ef\uff0c\u7ed3\u5408\u53ef\u53d8\u5f62\u5377\u79ef\u7684\u9002\u5e94\u6027\u548c\u53ef\u64cd\u7eb5\u5377\u79ef\u7684\u7b49\u53d8\u6027\u3002", "result": "\u6a21\u578b\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u548c\u5185\u5b58\u6210\u672c\uff0c\u7b49\u53d8\u6027\u6293\u53d6\u6a21\u578b\u6027\u80fd\u4f18\u4e8e\u975e\u7b49\u53d8\u6a21\u578b\uff0c\u8ba1\u7b97\u5f00\u9500\u8f83\u5c0f\u3002", "conclusion": "\u4e09\u5e73\u9762\u7279\u5f81\u8bbe\u8ba1\u548c\u53ef\u53d8\u5f62\u53ef\u64cd\u7eb5\u5377\u79ef\u6709\u6548\u63d0\u5347\u6293\u53d6\u6a21\u578b\u7684\u6548\u7387\u548c\u6027\u80fd\u3002"}}
{"id": "2507.18775", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.18775", "abs": "https://arxiv.org/abs/2507.18775", "authors": ["Ilche Georgievski", "Marco Aiello"], "title": "Initial Steps in Integrating Large Reasoning and Action Models for Service Composition", "comment": "16 pages, 3 figures, 19th Symposium and Summer School on\n  Service-Oriented Computing (SummerSOC)", "summary": "Service composition remains a central challenge in building adaptive and\nintelligent software systems, often constrained by limited reasoning\ncapabilities or brittle execution mechanisms. This paper explores the\nintegration of two emerging paradigms enabled by large language models: Large\nReasoning Models (LRMs) and Large Action Models (LAMs). We argue that LRMs\naddress the challenges of semantic reasoning and ecosystem complexity while\nLAMs excel in dynamic action execution and system interoperability. However,\neach paradigm has complementary limitations - LRMs lack grounded action\ncapabilities, and LAMs often struggle with deep reasoning. We propose an\nintegrated LRM-LAM architectural framework as a promising direction for\nadvancing automated service composition. Such a system can reason about service\nrequirements and constraints while dynamically executing workflows, thus\nbridging the gap between intention and execution. This integration has the\npotential to transform service composition into a fully automated,\nuser-friendly process driven by high-level natural language intent.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u7ed3\u5408\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRM\uff09\u548c\u5927\u578b\u52a8\u4f5c\u6a21\u578b\uff08LAM\uff09\u7684\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u670d\u52a1\u7ec4\u5408\u4e2d\u7684\u8bed\u4e49\u63a8\u7406\u548c\u6267\u884c\u95ee\u9898\u3002", "motivation": "\u670d\u52a1\u7ec4\u5408\u5728\u6784\u5efa\u81ea\u9002\u5e94\u667a\u80fd\u7cfb\u7edf\u4e2d\u9762\u4e34\u63a8\u7406\u80fd\u529b\u6709\u9650\u548c\u6267\u884c\u673a\u5236\u8106\u5f31\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u96c6\u6210LRM\u548cLAM\u7684\u67b6\u6784\u6846\u67b6\uff0cLRM\u8d1f\u8d23\u8bed\u4e49\u63a8\u7406\uff0cLAM\u8d1f\u8d23\u52a8\u6001\u6267\u884c\u3002", "result": "\u8be5\u6846\u67b6\u80fd\u591f\u81ea\u52a8\u5316\u670d\u52a1\u7ec4\u5408\uff0c\u5f25\u5408\u610f\u56fe\u4e0e\u6267\u884c\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "conclusion": "LRM-LAM\u96c6\u6210\u6709\u671b\u5c06\u670d\u52a1\u7ec4\u5408\u8f6c\u53d8\u4e3a\u5b8c\u5168\u81ea\u52a8\u5316\u3001\u7528\u6237\u53cb\u597d\u7684\u8fc7\u7a0b\u3002"}}
{"id": "2507.18886", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.18886", "abs": "https://arxiv.org/abs/2507.18886", "authors": ["Zheng Yang", "Kuan Xu", "Shenghai Yuan", "Lihua Xie"], "title": "A Fast and Light-weight Non-Iterative Visual Odometry with RGB-D Cameras", "comment": null, "summary": "In this paper, we introduce a novel approach for efficiently estimating the\n6-Degree-of-Freedom (DoF) robot pose with a decoupled, non-iterative method\nthat capitalizes on overlapping planar elements. Conventional RGB-D visual\nodometry(RGBD-VO) often relies on iterative optimization solvers to estimate\npose and involves a process of feature extraction and matching. This results in\nsignificant computational burden and time delays. To address this, our\ninnovative method for RGBD-VO separates the estimation of rotation and\ntranslation. Initially, we exploit the overlaid planar characteristics within\nthe scene to calculate the rotation matrix. Following this, we utilize a kernel\ncross-correlator (KCC) to ascertain the translation. By sidestepping the\nresource-intensive iterative optimization and feature extraction and alignment\nprocedures, our methodology offers improved computational efficacy, achieving a\nperformance of 71Hz on a lower-end i5 CPU. When the RGBD-VO does not rely on\nfeature points, our technique exhibits enhanced performance in low-texture\ndegenerative environments compared to state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u975e\u8fed\u4ee3\u3001\u89e3\u8026\u76846\u81ea\u7531\u5ea6\u673a\u5668\u4eba\u4f4d\u59ff\u4f30\u8ba1\u65b9\u6cd5\uff0c\u5229\u7528\u91cd\u53e0\u5e73\u9762\u7279\u5f81\uff0c\u907f\u514d\u4e86\u4f20\u7edfRGB-D\u89c6\u89c9\u91cc\u7a0b\u8ba1\u7684\u8fed\u4ee3\u4f18\u5316\u548c\u7279\u5f81\u5339\u914d\uff0c\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u3002", "motivation": "\u4f20\u7edfRGB-D\u89c6\u89c9\u91cc\u7a0b\u8ba1\u4f9d\u8d56\u8fed\u4ee3\u4f18\u5316\u548c\u7279\u5f81\u5339\u914d\uff0c\u8ba1\u7b97\u8d1f\u62c5\u91cd\u4e14\u8017\u65f6\uff0c\u5c24\u5176\u5728\u4f4e\u7eb9\u7406\u9000\u5316\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u901a\u8fc7\u5206\u79bb\u65cb\u8f6c\u548c\u5e73\u79fb\u4f30\u8ba1\uff0c\u5148\u5229\u7528\u573a\u666f\u4e2d\u7684\u5e73\u9762\u7279\u5f81\u8ba1\u7b97\u65cb\u8f6c\u77e9\u9635\uff0c\u518d\u7528\u6838\u4e92\u76f8\u5173\u5668\uff08KCC\uff09\u786e\u5b9a\u5e73\u79fb\uff0c\u907f\u514d\u8fed\u4ee3\u4f18\u5316\u548c\u7279\u5f81\u63d0\u53d6\u3002", "result": "\u5728\u4f4e\u7aefi5 CPU\u4e0a\u5b9e\u73b071Hz\u7684\u6027\u80fd\uff0c\u5728\u4f4e\u7eb9\u7406\u9000\u5316\u73af\u5883\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u548c\u590d\u6742\u73af\u5883\u4e0b\u7684\u673a\u5668\u4eba\u4f4d\u59ff\u4f30\u8ba1\u3002"}}
{"id": "2507.18795", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18795", "abs": "https://arxiv.org/abs/2507.18795", "authors": ["Fatima Al-Ani", "Molly Wang", "Jevon Charles", "Aaron Ong", "Joshua Forday", "Vinayak Modi"], "title": "Simulation-Driven Reinforcement Learning in Queuing Network Routing Optimization", "comment": null, "summary": "This study focuses on the development of a simulation-driven reinforcement\nlearning (RL) framework for optimizing routing decisions in complex queueing\nnetwork systems, with a particular emphasis on manufacturing and communication\napplications. Recognizing the limitations of traditional queueing methods,\nwhich often struggle with dynamic, uncertain environments, we propose a robust\nRL approach leveraging Deep Deterministic Policy Gradient (DDPG) combined with\nDyna-style planning (Dyna-DDPG). The framework includes a flexible and\nconfigurable simulation environment capable of modeling diverse queueing\nscenarios, disruptions, and unpredictable conditions. Our enhanced Dyna-DDPG\nimplementation incorporates separate predictive models for next-state\ntransitions and rewards, significantly improving stability and sample\nefficiency. Comprehensive experiments and rigorous evaluations demonstrate the\nframework's capability to rapidly learn effective routing policies that\nmaintain robust performance under disruptions and scale effectively to larger\nnetwork sizes. Additionally, we highlight strong software engineering practices\nemployed to ensure reproducibility and maintainability of the framework,\nenabling practical deployment in real-world scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u62df\u9a71\u52a8\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff08Dyna-DDPG\uff09\uff0c\u7528\u4e8e\u4f18\u5316\u590d\u6742\u6392\u961f\u7f51\u7edc\u4e2d\u7684\u8def\u7531\u51b3\u7b56\uff0c\u9002\u7528\u4e8e\u5236\u9020\u548c\u901a\u4fe1\u9886\u57df\u3002", "motivation": "\u4f20\u7edf\u6392\u961f\u65b9\u6cd5\u5728\u52a8\u6001\u548c\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7ed3\u5408Deep Deterministic Policy Gradient\uff08DDPG\uff09\u548cDyna-style\u89c4\u5212\uff08Dyna-DDPG\uff09\uff0c\u5e76\u5f00\u53d1\u4e86\u7075\u6d3b\u7684\u6a21\u62df\u73af\u5883\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u5feb\u901f\u5b66\u4e60\u6709\u6548\u7684\u8def\u7531\u7b56\u7565\uff0c\u5728\u5e72\u6270\u4e0b\u4fdd\u6301\u9c81\u68d2\u6027\uff0c\u5e76\u80fd\u6269\u5c55\u5230\u66f4\u5927\u7f51\u7edc\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u589e\u5f3a\u7684Dyna-DDPG\u548c\u826f\u597d\u7684\u8f6f\u4ef6\u5de5\u7a0b\u5b9e\u8df5\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2507.18947", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.18947", "abs": "https://arxiv.org/abs/2507.18947", "authors": ["Asad Ali Shahid", "Angelo Moroncelli", "Drazen Brscic", "Takayuki Kanda", "Loris Roveda"], "title": "GEAR: Gaze-Enabled Human-Robot Collaborative Assembly", "comment": "Accepted for publication at 2025 IEEE/RSJ International Conference on\n  Intelligent Robots and Systems (IROS 2025)", "summary": "Recent progress in robot autonomy and safety has significantly improved\nhuman-robot interactions, enabling robots to work alongside humans on various\ntasks. However, complex assembly tasks still present significant challenges due\nto inherent task variability and the need for precise operations. This work\nexplores deploying robots in an assistive role for such tasks, where the robot\nassists by fetching parts while the skilled worker provides high-level guidance\nand performs the assembly. We introduce GEAR, a gaze-enabled system designed to\nenhance human-robot collaboration by allowing robots to respond to the user's\ngaze. We evaluate GEAR against a touch-based interface where users interact\nwith the robot through a touchscreen. The experimental study involved 30\nparticipants working on two distinct assembly scenarios of varying complexity.\nResults demonstrated that GEAR enabled participants to accomplish the assembly\nwith reduced physical demand and effort compared to the touchscreen interface,\nespecially for complex tasks, maintaining great performance, and receiving\nobjects effectively. Participants also reported enhanced user experience while\nperforming assembly tasks. Project page: sites.google.com/view/gear-hri", "AI": {"tldr": "GEAR\u662f\u4e00\u79cd\u57fa\u4e8e\u51dd\u89c6\u7684\u7cfb\u7edf\uff0c\u7528\u4e8e\u63d0\u5347\u4eba\u673a\u534f\u4f5c\u6548\u7387\uff0c\u901a\u8fc7\u51cf\u5c11\u7269\u7406\u9700\u6c42\u548c\u52aa\u529b\uff0c\u5c24\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u590d\u6742\u88c5\u914d\u4efb\u52a1\u56e0\u4efb\u52a1\u591a\u53d8\u6027\u548c\u7cbe\u786e\u64cd\u4f5c\u9700\u6c42\u800c\u5177\u6709\u6311\u6218\u6027\uff0c\u7814\u7a76\u63a2\u7d22\u673a\u5668\u4eba\u8f85\u52a9\u89d2\u8272\u4ee5\u63d0\u5347\u534f\u4f5c\u6548\u7387\u3002", "method": "\u5f15\u5165GEAR\u7cfb\u7edf\uff0c\u901a\u8fc7\u7528\u6237\u51dd\u89c6\u5b9e\u73b0\u673a\u5668\u4eba\u54cd\u5e94\uff0c\u5e76\u4e0e\u89e6\u6478\u5c4f\u754c\u9762\u8fdb\u884c\u5bf9\u6bd4\u5b9e\u9a8c\uff0c\u6d89\u53ca30\u540d\u53c2\u4e0e\u8005\u5b8c\u6210\u4e0d\u540c\u590d\u6742\u5ea6\u7684\u88c5\u914d\u4efb\u52a1\u3002", "result": "GEAR\u663e\u8457\u51cf\u5c11\u7269\u7406\u9700\u6c42\u548c\u52aa\u529b\uff0c\u5c24\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u540c\u65f6\u63d0\u5347\u7528\u6237\u4f53\u9a8c\u3002", "conclusion": "GEAR\u7cfb\u7edf\u901a\u8fc7\u51dd\u89c6\u4ea4\u4e92\u6709\u6548\u63d0\u5347\u4eba\u673a\u534f\u4f5c\u6548\u7387\uff0c\u5c24\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2507.18868", "categories": ["cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2507.18868", "abs": "https://arxiv.org/abs/2507.18868", "authors": ["Alex Noviello", "Claas Beger", "Jacob Groner", "Kevin Ellis", "Weinan Sun"], "title": "A Neuroscience-Inspired Dual-Process Model of Compositional Generalization", "comment": null, "summary": "Systematic compositional generalization - constructing and understanding\nnovel combinations of known building blocks - remains a core challenge for AI\nsystems. Human cognition achieves this flexibility via the interplay of the\nhippocampus (HPC) and prefrontal cortex (PFC): the hippocampus rapidly encodes\nepisodes, and the prefrontal cortex consolidates them into reusable schemas for\nreasoning. Drawing on these insights, we present MIRAGE (Meta-Inference with\nRules and Abstractions from Generalized Experience), a framework that achieves\nsystematic generalization on compositional tasks. MIRAGE has two interacting\nmodules mirroring the brain's deliberative HPC-PFC loop and intuitive\nneocortical pattern recognition. (1) The meta-trained Transformer Neural\nDecomposer, paralleling neocortical \"System 1\" computation, is trained on a\ntask-agnostic stream of randomly sampled compositional grammars and applies one\ndecomposition step per pass, with successive passes iteratively refining the\nsequence representation. (2) The Schema Engine, analogous to the HPC-PFC\n\"System 2\" loop, dynamically extracts, ranks, and applies reusable schemas,\nstoring variable bindings in episodic memory and expanding them when needed. By\nexplicitly equipping the Transformer component of MIRAGE with actively managed\nschematic structures, our model performs systematic compositional operations\nthrough explicit schema application and transformation, relying solely on\nfrozen weights when solving entirely novel tasks. This approach demonstrates\nsystematic compositional generalization on the SCAN benchmark, achieving > 99%\naccuracy on all task splits with only 1.19M parameters in the transformer\nmodule. Ablation studies confirm that MIRAGE's systematicity critically depends\non the quality of extracted schemas and the model's iterative refinement\nprocess.", "AI": {"tldr": "MIRAGE\u6846\u67b6\u901a\u8fc7\u6a21\u4eff\u4eba\u8111\u7684HPC-PFC\u4ea4\u4e92\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u7cfb\u7edf\u6027\u7ec4\u5408\u6cdb\u5316\uff0c\u5728SCAN\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3AI\u7cfb\u7edf\u5728\u7ec4\u5408\u6cdb\u5316\u4e2d\u7684\u6838\u5fc3\u6311\u6218\uff0c\u501f\u9274\u4eba\u8111\u6d77\u9a6c\u4f53\u4e0e\u524d\u989d\u53f6\u76ae\u5c42\u7684\u534f\u4f5c\u673a\u5236\u3002", "method": "\u7ed3\u5408\u5143\u8bad\u7ec3\u7684Transformer\u795e\u7ecf\u5206\u89e3\u5668\u548c\u52a8\u6001\u63d0\u53d6\u53ef\u91cd\u7528\u6a21\u5f0f\u7684Schema\u5f15\u64ce\uff0c\u5b9e\u73b0\u8fed\u4ee3\u5f0f\u7ec4\u5408\u64cd\u4f5c\u3002", "result": "\u5728SCAN\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230>99%\u7684\u51c6\u786e\u7387\uff0c\u4ec5\u97001.19M\u53c2\u6570\u3002", "conclusion": "MIRAGE\u7684\u7cfb\u7edf\u6027\u4f9d\u8d56\u4e8e\u6a21\u5f0f\u63d0\u53d6\u8d28\u91cf\u548c\u8fed\u4ee3\u4f18\u5316\u8fc7\u7a0b\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2507.18979", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.18979", "abs": "https://arxiv.org/abs/2507.18979", "authors": ["Deokjin Lee", "Junho Song", "Alireza Karimi", "Sehoon Oh"], "title": "Frequency Response Data-Driven Disturbance Observer Design for Flexible Joint Robots", "comment": null, "summary": "Motion control of flexible joint robots (FJR) is challenged by inherent\nflexibility and configuration-dependent variations in system dynamics. While\ndisturbance observers (DOB) can enhance system robustness, their performance is\noften limited by the elasticity of the joints and the variations in system\nparameters, which leads to a conservative design of the DOB. This paper\npresents a novel frequency response function (FRF)-based optimization method\naimed at improving DOB performance, even in the presence of flexibility and\nsystem variability. The proposed method maximizes control bandwidth and\neffectively suppresses vibrations, thus enhancing overall system performance.\nClosed-loop stability is rigorously proven using the Nyquist stability\ncriterion. Experimental validation on a FJR demonstrates that the proposed\napproach significantly improves robustness and motion performance, even under\nconditions of joint flexibility and system variation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9891\u7387\u54cd\u5e94\u51fd\u6570\uff08FRF\uff09\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u6539\u8fdb\u67d4\u6027\u5173\u8282\u673a\u5668\u4eba\uff08FJR\uff09\u7684\u6270\u52a8\u89c2\u6d4b\u5668\uff08DOB\uff09\u6027\u80fd\uff0c\u63d0\u5347\u63a7\u5236\u5e26\u5bbd\u548c\u632f\u52a8\u6291\u5236\u80fd\u529b\u3002", "motivation": "\u67d4\u6027\u5173\u8282\u673a\u5668\u4eba\u7684\u8fd0\u52a8\u63a7\u5236\u53d7\u5173\u8282\u5f39\u6027\u548c\u7cfb\u7edf\u53c2\u6570\u53d8\u5316\u7684\u9650\u5236\uff0c\u4f20\u7edfDOB\u8bbe\u8ba1\u4fdd\u5b88\uff0c\u6027\u80fd\u53d7\u9650\u3002", "method": "\u91c7\u7528FRF\u4f18\u5316\u65b9\u6cd5\uff0c\u6700\u5927\u5316\u63a7\u5236\u5e26\u5bbd\u5e76\u6291\u5236\u632f\u52a8\uff0c\u540c\u65f6\u901a\u8fc7Nyquist\u7a33\u5b9a\u6027\u51c6\u5219\u8bc1\u660e\u95ed\u73af\u7a33\u5b9a\u6027\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u548c\u8fd0\u52a8\u6027\u80fd\uff0c\u5c24\u5176\u5728\u5173\u8282\u5f39\u6027\u548c\u7cfb\u7edf\u53d8\u5316\u6761\u4ef6\u4e0b\u3002", "conclusion": "\u63d0\u51fa\u7684FRF\u4f18\u5316\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u67d4\u6027\u5173\u8282\u673a\u5668\u4ebaDOB\u8bbe\u8ba1\u7684\u4fdd\u5b88\u6027\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6574\u4f53\u6027\u80fd\u3002"}}
{"id": "2507.18883", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.18883", "abs": "https://arxiv.org/abs/2507.18883", "authors": ["Wuhao Wang", "Zhiyong Chen"], "title": "Success in Humanoid Reinforcement Learning under Partial Observation", "comment": "11 pages, 3 figures, and 4 tables. Not published anywhere else", "summary": "Reinforcement learning has been widely applied to robotic control, but\neffective policy learning under partial observability remains a major\nchallenge, especially in high-dimensional tasks like humanoid locomotion. To\ndate, no prior work has demonstrated stable training of humanoid policies with\nincomplete state information in the benchmark Gymnasium Humanoid-v4\nenvironment. The objective in this environment is to walk forward as fast as\npossible without falling, with rewards provided for staying upright and moving\nforward, and penalties incurred for excessive actions and external contact\nforces. This research presents the first successful instance of learning under\npartial observability in this environment. The learned policy achieves\nperformance comparable to state-of-the-art results with full state access,\ndespite using only one-third to two-thirds of the original states. Moreover,\nthe policy exhibits adaptability to robot properties, such as variations in\nbody part masses. The key to this success is a novel history encoder that\nprocesses a fixed-length sequence of past observations in parallel. Integrated\ninto a standard model-free algorithm, the encoder enables performance on par\nwith fully observed baselines. We hypothesize that it reconstructs essential\ncontextual information from recent observations, thereby enabling robust\ndecision-making.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u7684Gymnasium Humanoid-v4\u73af\u5883\u4e2d\u6210\u529f\u8bad\u7ec3\u51fa\u7a33\u5b9a\u7684\u4eba\u5f62\u673a\u5668\u4eba\u7b56\u7565\uff0c\u6027\u80fd\u63a5\u8fd1\u5168\u72b6\u6001\u8bbf\u95ee\u7684\u5148\u8fdb\u7ed3\u679c\u3002", "motivation": "\u89e3\u51b3\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u6761\u4ef6\u4e0b\u9ad8\u7ef4\u4efb\u52a1\uff08\u5982\u4eba\u5f62\u673a\u5668\u4eba\u8fd0\u52a8\uff09\u4e2d\u7b56\u7565\u5b66\u4e60\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u5386\u53f2\u7f16\u7801\u5668\uff0c\u5904\u7406\u56fa\u5b9a\u957f\u5ea6\u7684\u8fc7\u53bb\u89c2\u5bdf\u5e8f\u5217\uff0c\u5e76\u5c06\u5176\u96c6\u6210\u5230\u6807\u51c6\u65e0\u6a21\u578b\u7b97\u6cd5\u4e2d\u3002", "result": "\u5b66\u4e60\u5230\u7684\u7b56\u7565\u4ec5\u4f7f\u7528\u539f\u59cb\u72b6\u6001\u7684\u4e09\u5206\u4e4b\u4e00\u81f3\u4e09\u5206\u4e4b\u4e8c\uff0c\u6027\u80fd\u4e0e\u5168\u72b6\u6001\u57fa\u7ebf\u76f8\u5f53\uff0c\u5e76\u80fd\u9002\u5e94\u673a\u5668\u4eba\u5c5e\u6027\u53d8\u5316\u3002", "conclusion": "\u5386\u53f2\u7f16\u7801\u5668\u901a\u8fc7\u91cd\u5efa\u5173\u952e\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u5b9e\u73b0\u4e86\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u51b3\u7b56\u3002"}}
{"id": "2507.19079", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.19079", "abs": "https://arxiv.org/abs/2507.19079", "authors": ["Feng Zhu", "Zihang Zhang", "Kangcheng Teng", "Abduhelil Yakup", "Xiaohong Zhang"], "title": "SmartPNT-MSF: A Multi-Sensor Fusion Dataset for Positioning and Navigation Research", "comment": null, "summary": "High-precision navigation and positioning systems are critical for\napplications in autonomous vehicles and mobile mapping, where robust and\ncontinuous localization is essential. To test and enhance the performance of\nalgorithms, some research institutions and companies have successively\nconstructed and publicly released datasets. However, existing datasets still\nsuffer from limitations in sensor diversity and environmental coverage. To\naddress these shortcomings and advance development in related fields, the\nSmartPNT Multisource Integrated Navigation, Positioning, and Attitude Dataset\nhas been developed. This dataset integrates data from multiple sensors,\nincluding Global Navigation Satellite Systems (GNSS), Inertial Measurement\nUnits (IMU), optical cameras, and LiDAR, to provide a rich and versatile\nresource for research in multi-sensor fusion and high-precision navigation. The\ndataset construction process is thoroughly documented, encompassing sensor\nconfigurations, coordinate system definitions, and calibration procedures for\nboth cameras and LiDAR. A standardized framework for data collection and\nprocessing ensures consistency and scalability, enabling large-scale analysis.\nValidation using state-of-the-art Simultaneous Localization and Mapping (SLAM)\nalgorithms, such as VINS-Mono and LIO-SAM, demonstrates the dataset's\napplicability for advanced navigation research. Covering a wide range of\nreal-world scenarios, including urban areas, campuses, tunnels, and suburban\nenvironments, the dataset offers a valuable tool for advancing navigation\ntechnologies and addressing challenges in complex environments. By providing a\npublicly accessible, high-quality dataset, this work aims to bridge gaps in\nsensor diversity, data accessibility, and environmental representation,\nfostering further innovation in the field.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86SmartPNT\u591a\u6e90\u96c6\u6210\u5bfc\u822a\u3001\u5b9a\u4f4d\u548c\u59ff\u6001\u6570\u636e\u96c6\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u6570\u636e\u96c6\u5728\u4f20\u611f\u5668\u591a\u6837\u6027\u548c\u73af\u5883\u8986\u76d6\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u96c6\u5728\u4f20\u611f\u5668\u591a\u6837\u6027\u548c\u73af\u5883\u8986\u76d6\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u963b\u788d\u4e86\u9ad8\u7cbe\u5ea6\u5bfc\u822a\u548c\u591a\u4f20\u611f\u5668\u878d\u5408\u7814\u7a76\u7684\u8fdb\u5c55\u3002", "method": "\u6570\u636e\u96c6\u6574\u5408\u4e86GNSS\u3001IMU\u3001\u5149\u5b66\u76f8\u673a\u548cLiDAR\u7b49\u591a\u4f20\u611f\u5668\u6570\u636e\uff0c\u5e76\u8be6\u7ec6\u8bb0\u5f55\u4e86\u4f20\u611f\u5668\u914d\u7f6e\u3001\u5750\u6807\u7cfb\u5b9a\u4e49\u548c\u6821\u51c6\u8fc7\u7a0b\u3002", "result": "\u901a\u8fc7VINS-Mono\u548cLIO-SAM\u7b49SLAM\u7b97\u6cd5\u7684\u9a8c\u8bc1\uff0c\u6570\u636e\u96c6\u5728\u590d\u6742\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u9002\u7528\u6027\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u586b\u8865\u4e86\u4f20\u611f\u5668\u591a\u6837\u6027\u3001\u6570\u636e\u53ef\u8bbf\u95ee\u6027\u548c\u73af\u5883\u4ee3\u8868\u6027\u7684\u7a7a\u767d\uff0c\u63a8\u52a8\u4e86\u5bfc\u822a\u6280\u672f\u7684\u521b\u65b0\u3002"}}
{"id": "2507.18977", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.18977", "abs": "https://arxiv.org/abs/2507.18977", "authors": ["Mehrnoosh Mirtaheri", "Ryan A. Rossi", "Sungchul Kim", "Kanak Mahadik", "Tong Yu", "Xiang Chen", "Mohammad Rostami"], "title": "Towards Improving Long-Tail Entity Predictions in Temporal Knowledge Graphs through Global Similarity and Weighted Sampling", "comment": null, "summary": "Temporal Knowledge Graph (TKG) completion models traditionally assume access\nto the entire graph during training. This overlooks challenges stemming from\nthe evolving nature of TKGs, such as: (i) the model's requirement to generalize\nand assimilate new knowledge, and (ii) the task of managing new or unseen\nentities that often have sparse connections. In this paper, we present an\nincremental training framework specifically designed for TKGs, aiming to\naddress entities that are either not observed during training or have sparse\nconnections. Our approach combines a model-agnostic enhancement layer with a\nweighted sampling strategy, that can be augmented to and improve any existing\nTKG completion method. The enhancement layer leverages a broader, global\ndefinition of entity similarity, which moves beyond mere local neighborhood\nproximity of GNN-based methods. The weighted sampling strategy employed in\ntraining accentuates edges linked to infrequently occurring entities. We\nevaluate our method on two benchmark datasets, and demonstrate that our\nframework outperforms existing methods in total link prediction, inductive link\nprediction, and in addressing long-tail entities. Notably, our method achieves\na 10\\% improvement and a 15\\% boost in MRR for these datasets. The results\nunderscore the potential of our approach in mitigating catastrophic forgetting\nand enhancing the robustness of TKG completion methods, especially in an\nincremental training context", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u65f6\u5e8f\u77e5\u8bc6\u56fe\u8c31\uff08TKG\uff09\u7684\u589e\u91cf\u8bad\u7ec3\u6846\u67b6\uff0c\u89e3\u51b3\u65b0\u5b9e\u4f53\u548c\u7a00\u758f\u8fde\u63a5\u95ee\u9898\uff0c\u7ed3\u5408\u6a21\u578b\u65e0\u5173\u7684\u589e\u5f3a\u5c42\u548c\u52a0\u6743\u91c7\u6837\u7b56\u7565\u3002", "motivation": "\u4f20\u7edfTKG\u5b8c\u6210\u6a21\u578b\u5047\u8bbe\u8bad\u7ec3\u65f6\u53ef\u8bbf\u95ee\u6574\u4e2a\u56fe\u8c31\uff0c\u5ffd\u7565\u4e86\u56fe\u8c31\u52a8\u6001\u6f14\u5316\u5e26\u6765\u7684\u6311\u6218\uff0c\u5982\u65b0\u77e5\u8bc6\u7684\u6cdb\u5316\u548c\u7a00\u758f\u8fde\u63a5\u5b9e\u4f53\u7684\u5904\u7406\u3002", "method": "\u7ed3\u5408\u6a21\u578b\u65e0\u5173\u7684\u589e\u5f3a\u5c42\uff08\u57fa\u4e8e\u5168\u5c40\u5b9e\u4f53\u76f8\u4f3c\u6027\uff09\u548c\u52a0\u6743\u91c7\u6837\u7b56\u7565\uff08\u7a81\u51fa\u7a00\u758f\u5b9e\u4f53\u8fde\u63a5\uff09\uff0c\u53ef\u589e\u5f3a\u73b0\u6709TKG\u5b8c\u6210\u65b9\u6cd5\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0c\u6846\u67b6\u5728\u6574\u4f53\u94fe\u63a5\u9884\u6d4b\u3001\u5f52\u7eb3\u94fe\u63a5\u9884\u6d4b\u53ca\u957f\u5c3e\u5b9e\u4f53\u5904\u7406\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0cMRR\u63d0\u534710%-15%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u7f13\u89e3\u707e\u96be\u6027\u9057\u5fd8\uff0c\u63d0\u5347TKG\u5b8c\u6210\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\uff0c\u7279\u522b\u9002\u7528\u4e8e\u589e\u91cf\u8bad\u7ec3\u573a\u666f\u3002"}}
{"id": "2507.19082", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.19082", "abs": "https://arxiv.org/abs/2507.19082", "authors": ["Rachel Ringe", "Leandra Thiele", "Mihai Pomarlan", "Nima Zargham", "Robin Nolte", "Lars Hurrelbrink", "Rainer Malaka"], "title": "Bot App\u00e9tit! Exploring how Robot Morphology Shapes Perceived Affordances via a Mise en Place Scenario in a VR Kitchen", "comment": "Copyright 2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "This study explores which factors of the visual design of a robot may\ninfluence how humans would place it in a collaborative cooking scenario and how\nthese features may influence task delegation. Human participants were placed in\na Virtual Reality (VR) environment and asked to set up a kitchen for cooking\nalongside a robot companion while considering the robot's morphology. We\ncollected multimodal data for the arrangements created by the participants,\ntranscripts of their think-aloud as they were performing the task, and\ntranscripts of their answers to structured post-task questionnaires. Based on\nanalyzing this data, we formulate several hypotheses: humans prefer to\ncollaborate with biomorphic robots; human beliefs about the sensory\ncapabilities of robots are less influenced by the morphology of the robot than\nbeliefs about action capabilities; and humans will implement fewer avoidance\nstrategies when sharing space with gracile robots. We intend to verify these\nhypotheses in follow-up studies.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u673a\u5668\u4eba\u89c6\u89c9\u8bbe\u8ba1\u56e0\u7d20\u5982\u4f55\u5f71\u54cd\u4eba\u7c7b\u5728\u534f\u4f5c\u70f9\u996a\u573a\u666f\u4e2d\u7684\u5e03\u5c40\u53ca\u4efb\u52a1\u5206\u914d\uff0c\u53d1\u73b0\u4eba\u7c7b\u504f\u597d\u4e0e\u751f\u7269\u5f62\u6001\u673a\u5668\u4eba\u5408\u4f5c\uff0c\u673a\u5668\u4eba\u5f62\u6001\u5bf9\u5176\u611f\u77e5\u80fd\u529b\u7684\u5f71\u54cd\u8f83\u5c0f\uff0c\u800c\u7ea4\u7ec6\u673a\u5668\u4eba\u80fd\u51cf\u5c11\u4eba\u7c7b\u7684\u907f\u8ba9\u7b56\u7565\u3002", "motivation": "\u63a2\u7d22\u673a\u5668\u4eba\u89c6\u89c9\u8bbe\u8ba1\u5bf9\u4eba\u7c7b\u534f\u4f5c\u884c\u4e3a\u548c\u4efb\u52a1\u5206\u914d\u7684\u5f71\u54cd\uff0c\u4ee5\u4f18\u5316\u4eba\u673a\u534f\u4f5c\u4f53\u9a8c\u3002", "method": "\u901a\u8fc7\u865a\u62df\u73b0\u5b9e\u73af\u5883\u8ba9\u53c2\u4e0e\u8005\u4e0e\u4e0d\u540c\u5f62\u6001\u673a\u5668\u4eba\u534f\u4f5c\u70f9\u996a\uff0c\u6536\u96c6\u5e03\u5c40\u6570\u636e\u3001\u53e3\u5934\u53cd\u9988\u53ca\u95ee\u5377\u56de\u7b54\u3002", "result": "\u4eba\u7c7b\u66f4\u503e\u5411\u4e0e\u751f\u7269\u5f62\u6001\u673a\u5668\u4eba\u5408\u4f5c\uff1b\u673a\u5668\u4eba\u5f62\u6001\u5bf9\u5176\u611f\u77e5\u80fd\u529b\u7684\u4fe1\u5ff5\u5f71\u54cd\u8f83\u5c0f\uff1b\u7ea4\u7ec6\u673a\u5668\u4eba\u51cf\u5c11\u4eba\u7c7b\u7684\u907f\u8ba9\u884c\u4e3a\u3002", "conclusion": "\u673a\u5668\u4eba\u89c6\u89c9\u8bbe\u8ba1\u663e\u8457\u5f71\u54cd\u4eba\u7c7b\u534f\u4f5c\u884c\u4e3a\uff0c\u672a\u6765\u7814\u7a76\u5c06\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u8fd9\u4e9b\u5047\u8bbe\u3002"}}
{"id": "2507.19089", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19089", "abs": "https://arxiv.org/abs/2507.19089", "authors": ["Shuhao Li", "Weidong Yang", "Yue Cui", "Xiaoxing Liu", "Lingkai Meng", "Lipeng Ma", "Fan Zhang"], "title": "Fine-Grained Traffic Inference from Road to Lane via Spatio-Temporal Graph Node Generation", "comment": null, "summary": "Fine-grained traffic management and prediction are fundamental to key\napplications such as autonomous driving, lane change guidance, and traffic\nsignal control. However, obtaining lane-level traffic data has become a\ncritical bottleneck for data-driven models due to limitations in the types and\nnumber of sensors and issues with the accuracy of tracking algorithms. To\naddress this, we propose the Fine-grained Road Traffic Inference (FRTI) task,\nwhich aims to generate more detailed lane-level traffic information using\nlimited road data, providing a more energy-efficient and cost-effective\nsolution for precise traffic management. This task is abstracted as the first\nscene of the spatio-temporal graph node generation problem. We designed a\ntwo-stage framework--RoadDiff--to solve the FRTI task. solve the FRTI task.\nThis framework leverages the Road-Lane Correlation Autoencoder-Decoder and the\nLane Diffusion Module to fully utilize the limited spatio-temporal dependencies\nand distribution relationships of road data to accurately infer fine-grained\nlane traffic states. Based on existing research, we designed several baseline\nmodels with the potential to solve the FRTI task and conducted extensive\nexperiments on six datasets representing different road conditions to validate\nthe effectiveness of the RoadDiff model in addressing the FRTI task. The\nrelevant datasets and code are available at\nhttps://github.com/ShuhaoLii/RoadDiff.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFRTI\u7684\u4efb\u52a1\uff0c\u65e8\u5728\u901a\u8fc7\u6709\u9650\u7684\u9053\u8def\u6570\u636e\u751f\u6210\u66f4\u8be6\u7ec6\u7684\u8f66\u9053\u7ea7\u4ea4\u901a\u4fe1\u606f\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u4e24\u9636\u6bb5\u6846\u67b6RoadDiff\u6765\u89e3\u51b3\u8be5\u4efb\u52a1\u3002", "motivation": "\u7531\u4e8e\u4f20\u611f\u5668\u7c7b\u578b\u548c\u6570\u91cf\u7684\u9650\u5236\u4ee5\u53ca\u8ddf\u8e2a\u7b97\u6cd5\u7684\u51c6\u786e\u6027\u4e0d\u8db3\uff0c\u83b7\u53d6\u8f66\u9053\u7ea7\u4ea4\u901a\u6570\u636e\u6210\u4e3a\u6570\u636e\u9a71\u52a8\u6a21\u578b\u7684\u5173\u952e\u74f6\u9888\u3002", "method": "\u8bbe\u8ba1\u4e86RoadDiff\u6846\u67b6\uff0c\u5305\u62ecRoad-Lane Correlation Autoencoder-Decoder\u548cLane Diffusion Module\uff0c\u5229\u7528\u6709\u9650\u7684\u7a7a\u95f4-\u65f6\u95f4\u4f9d\u8d56\u6027\u548c\u5206\u5e03\u5173\u7cfb\u63a8\u65ad\u8f66\u9053\u4ea4\u901a\u72b6\u6001\u3002", "result": "\u5728\u516d\u4e2a\u4e0d\u540c\u9053\u8def\u6761\u4ef6\u7684\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86RoadDiff\u6a21\u578b\u7684\u6709\u6548\u6027\u3002", "conclusion": "RoadDiff\u4e3a\u89e3\u51b3FRTI\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u6210\u672c\u6548\u76ca\u9ad8\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.19100", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.19100", "abs": "https://arxiv.org/abs/2507.19100", "authors": ["Taewon Kang", "Ji-Wook Kwon", "Il Bae", "Jin Hyo Kim"], "title": "Monocular Vision-Based Swarm Robot Localization Using Equilateral Triangular Formations", "comment": null, "summary": "Localization of mobile robots is crucial for deploying robots in real-world\napplications such as search and rescue missions. This work aims to develop an\naccurate localization system applicable to swarm robots equipped only with\nlow-cost monocular vision sensors and visual markers. The system is designed to\noperate in fully open spaces, without landmarks or support from positioning\ninfrastructures. To achieve this, we propose a localization method based on\nequilateral triangular formations. By leveraging the geometric properties of\nequilateral triangles, the accurate two-dimensional position of each\nparticipating robot is estimated using one-dimensional lateral distance\ninformation between robots, which can be reliably and accurately obtained with\na low-cost monocular vision sensor. Experimental and simulation results\ndemonstrate that, as travel time increases, the positioning error of the\nproposed method becomes significantly smaller than that of a conventional\ndead-reckoning system, another low-cost localization approach applicable to\nopen environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7b49\u8fb9\u4e09\u89d2\u5f62\u7f16\u961f\u7684\u4f4e\u6210\u672c\u5355\u76ee\u89c6\u89c9\u4f20\u611f\u5668\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5f00\u653e\u7a7a\u95f4\u4e2d\u7684\u7fa4\u4f53\u673a\u5668\u4eba\u5b9a\u4f4d\u3002", "motivation": "\u5728\u5f00\u653e\u7a7a\u95f4\u4e2d\uff0c\u7fa4\u4f53\u673a\u5668\u4eba\u9700\u8981\u4f4e\u6210\u672c\u4e14\u51c6\u786e\u7684\u5b9a\u4f4d\u7cfb\u7edf\uff0c\u800c\u4f20\u7edf\u65b9\u6cd5\u5982\u822a\u4f4d\u63a8\u7b97\u5728\u957f\u65f6\u95f4\u8fd0\u884c\u4e2d\u8bef\u5dee\u8f83\u5927\u3002", "method": "\u5229\u7528\u7b49\u8fb9\u4e09\u89d2\u5f62\u7684\u51e0\u4f55\u7279\u6027\uff0c\u901a\u8fc7\u5355\u76ee\u89c6\u89c9\u4f20\u611f\u5668\u83b7\u53d6\u7684\u4e00\u7ef4\u6a2a\u5411\u8ddd\u79bb\u4fe1\u606f\uff0c\u4f30\u8ba1\u673a\u5668\u4eba\u7684\u4e8c\u7ef4\u4f4d\u7f6e\u3002", "result": "\u5b9e\u9a8c\u548c\u4eff\u771f\u8868\u660e\uff0c\u968f\u7740\u8fd0\u884c\u65f6\u95f4\u589e\u52a0\uff0c\u8be5\u65b9\u6cd5\u7684\u4f4d\u7f6e\u8bef\u5dee\u663e\u8457\u4f4e\u4e8e\u4f20\u7edf\u822a\u4f4d\u63a8\u7b97\u7cfb\u7edf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5f00\u653e\u7a7a\u95f4\u4e2d\u7684\u7fa4\u4f53\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u4e00\u79cd\u4f4e\u6210\u672c\u4e14\u51c6\u786e\u7684\u5b9a\u4f4d\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.19109", "categories": ["cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2507.19109", "abs": "https://arxiv.org/abs/2507.19109", "authors": ["No\u00e9 Lallouet", "Tristan Cazenave", "Cyrille Enderli"], "title": "Pareto-NRPA: A Novel Monte-Carlo Search Algorithm for Multi-Objective Optimization", "comment": "Preprint ; accepted to ECAI 2025", "summary": "We introduce Pareto-NRPA, a new Monte-Carlo algorithm designed for\nmulti-objective optimization problems over discrete search spaces. Extending\nthe Nested Rollout Policy Adaptation (NRPA) algorithm originally formulated for\nsingle-objective problems, Pareto-NRPA generalizes the nested search and policy\nupdate mechanism to multi-objective optimization. The algorithm uses a set of\npolicies to concurrently explore different regions of the solution space and\nmaintains non-dominated fronts at each level of search. Policy adaptation is\nperformed with respect to the diversity and isolation of sequences within the\nPareto front. We benchmark Pareto-NRPA on two classes of problems: a novel\nbi-objective variant of the Traveling Salesman Problem with Time Windows\nproblem (MO-TSPTW), and a neural architecture search task on well-known\nbenchmarks. Results demonstrate that Pareto-NRPA achieves competitive\nperformance against state-of-the-art multi-objective algorithms, both in terms\nof convergence and diversity of solutions. Particularly, Pareto-NRPA strongly\noutperforms state-of-the-art evolutionary multi-objective algorithms on\nconstrained search spaces. To our knowledge, this work constitutes the first\nadaptation of NRPA to the multi-objective setting.", "AI": {"tldr": "Pareto-NRPA\u662f\u4e00\u79cd\u65b0\u7684\u8499\u7279\u5361\u6d1b\u7b97\u6cd5\uff0c\u7528\u4e8e\u79bb\u6563\u641c\u7d22\u7a7a\u95f4\u4e2d\u7684\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\uff0c\u6269\u5c55\u4e86\u5355\u76ee\u6807\u4f18\u5316\u7684NRPA\u7b97\u6cd5\u3002", "motivation": "\u89e3\u51b3\u591a\u76ee\u6807\u4f18\u5316\u95ee\u9898\uff0c\u7279\u522b\u662f\u79bb\u6563\u641c\u7d22\u7a7a\u95f4\u4e2d\u7684\u95ee\u9898\uff0c\u6269\u5c55NRPA\u7b97\u6cd5\u7684\u5e94\u7528\u8303\u56f4\u3002", "method": "\u4f7f\u7528\u4e00\u7ec4\u7b56\u7565\u5e76\u884c\u63a2\u7d22\u89e3\u7a7a\u95f4\u7684\u4e0d\u540c\u533a\u57df\uff0c\u5e76\u5728\u641c\u7d22\u7684\u6bcf\u4e2a\u5c42\u7ea7\u7ef4\u62a4\u975e\u652f\u914d\u524d\u6cbf\uff0c\u7b56\u7565\u66f4\u65b0\u57fa\u4e8ePareto\u524d\u6cbf\u7684\u591a\u6837\u6027\u548c\u9694\u79bb\u6027\u3002", "result": "\u5728MO-TSPTW\u548c\u795e\u7ecf\u67b6\u6784\u641c\u7d22\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u73b0\u6709\u591a\u76ee\u6807\u8fdb\u5316\u7b97\u6cd5\u3002", "conclusion": "Pareto-NRPA\u662fNRPA\u7b97\u6cd5\u5728\u591a\u76ee\u6807\u4f18\u5316\u4e2d\u7684\u9996\u6b21\u6210\u529f\u6269\u5c55\uff0c\u5177\u6709\u7ade\u4e89\u529b\u548c\u6f5c\u529b\u3002"}}
{"id": "2507.19146", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.19146", "abs": "https://arxiv.org/abs/2507.19146", "authors": ["Ahmed Abouelazm", "Johannes Ratz", "Philip Sch\u00f6rner", "J. Marius Z\u00f6llner"], "title": "Diverse and Adaptive Behavior Curriculum for Autonomous Driving: A Student-Teacher Framework with Multi-Agent RL", "comment": "Paper accepted in IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS 2025)", "summary": "Autonomous driving faces challenges in navigating complex real-world traffic,\nrequiring safe handling of both common and critical scenarios. Reinforcement\nlearning (RL), a prominent method in end-to-end driving, enables agents to\nlearn through trial and error in simulation. However, RL training often relies\non rule-based traffic scenarios, limiting generalization. Additionally, current\nscenario generation methods focus heavily on critical scenarios, neglecting a\nbalance with routine driving behaviors. Curriculum learning, which\nprogressively trains agents on increasingly complex tasks, is a promising\napproach to improving the robustness and coverage of RL driving policies.\nHowever, existing research mainly emphasizes manually designed curricula,\nfocusing on scenery and actor placement rather than traffic behavior dynamics.\nThis work introduces a novel student-teacher framework for automatic curriculum\nlearning. The teacher, a graph-based multi-agent RL component, adaptively\ngenerates traffic behaviors across diverse difficulty levels. An adaptive\nmechanism adjusts task difficulty based on student performance, ensuring\nexposure to behaviors ranging from common to critical. The student, though\nexchangeable, is realized as a deep RL agent with partial observability,\nreflecting real-world perception constraints. Results demonstrate the teacher's\nability to generate diverse traffic behaviors. The student, trained with\nautomatic curricula, outperformed agents trained on rule-based traffic,\nachieving higher rewards and exhibiting balanced, assertive driving.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b66\u751f-\u6559\u5e08\u6846\u67b6\u7684\u81ea\u52a8\u8bfe\u7a0b\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u7684\u9c81\u68d2\u6027\u548c\u8986\u76d6\u8303\u56f4\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u5728\u590d\u6742\u4ea4\u901a\u73af\u5883\u4e2d\u9762\u4e34\u6311\u6218\uff0c\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u89c4\u5219\u5316\u573a\u666f\u751f\u6210\uff0c\u7f3a\u4e4f\u5bf9\u5e38\u89c1\u548c\u5173\u952e\u573a\u666f\u7684\u5e73\u8861\u5904\u7406\u3002", "method": "\u91c7\u7528\u56fe\u57fa\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4f5c\u4e3a\u6559\u5e08\u6a21\u578b\uff0c\u52a8\u6001\u751f\u6210\u4e0d\u540c\u96be\u5ea6\u4ea4\u901a\u884c\u4e3a\uff1b\u5b66\u751f\u6a21\u578b\u4e3a\u90e8\u5206\u53ef\u89c2\u6d4b\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u3002", "result": "\u6559\u5e08\u6a21\u578b\u80fd\u751f\u6210\u591a\u6837\u5316\u4ea4\u901a\u884c\u4e3a\uff0c\u5b66\u751f\u6a21\u578b\u5728\u81ea\u52a8\u8bfe\u7a0b\u8bad\u7ec3\u4e0b\u8868\u73b0\u4f18\u4e8e\u89c4\u5219\u5316\u8bad\u7ec3\uff0c\u9a7e\u9a76\u884c\u4e3a\u66f4\u5e73\u8861\u548c\u81ea\u4fe1\u3002", "conclusion": "\u81ea\u52a8\u8bfe\u7a0b\u5b66\u4e60\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u7b56\u7565\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\u3002"}}
{"id": "2507.19132", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.19132", "abs": "https://arxiv.org/abs/2507.19132", "authors": ["Xuetian Chen", "Yinghao Chen", "Xinfeng Yuan", "Zhuo Peng", "Lu Chen", "Yuekeng Li", "Zhoujia Zhang", "Yingqian Huang", "Leyan Huang", "Jiaqing Liang", "Tianbao Xie", "Zhiyong Wu", "Qiushi Sun", "Biqing Qi", "Bowen Zhou"], "title": "OS-MAP: How Far Can Computer-Using Agents Go in Breadth and Depth?", "comment": "Work in progress", "summary": "Computer-using agents have shown strong potential to boost human productivity\nand enable new application forms across platforms. While recent advances have\nled to usable applications, existing benchmarks fail to account for the\ninternal task heterogeneity and the corresponding agent capabilities, as well\nas their alignment with actual user demands-hindering both targeted capability\ndevelopment and the reliable transition of research progress into practical\ndeployment. To bridge the gap, we present OS-MAP, a benchmark for daily\ncomputer-using automation that organizes its 416 realistic tasks across 15\napplications along two key dimensions: a five-level taxonomy of automation and\na generalization scope derived from a real-world user demand hierarchy. To\nenable fine-grained analysis of required capabilities and alignment with\nreal-world scenarios, OS-MAP evaluates agents along two dimensions: automation\nlevel across a five-level taxonomy, and generalization scope across a demand\nhierarchy. This design captures varying levels of required agent autonomy and\ngeneralization, forming a performance-generalization evaluation matrix for\nstructured and comprehensive assessment. Experiments show that even\nState-of-the-Art agents with VLM backbones struggle with higher-level tasks\ninvolving perception, reasoning, and coordination-highlighting the need for a\ndeeper understanding of current strengths and limitations to drive the future\nprogress in computer-using agents research and deployment. All code,\nenvironments, baselines, and data are publicly available at\nhttps://github.com/OS-Copilot/OS-Map.", "AI": {"tldr": "OS-MAP\u662f\u4e00\u4e2a\u7528\u4e8e\u65e5\u5e38\u8ba1\u7b97\u673a\u81ea\u52a8\u5316\u4efb\u52a1\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u4e94\u7ea7\u81ea\u52a8\u5316\u5206\u7c7b\u548c\u9700\u6c42\u5c42\u6b21\u7ed3\u6784\u8bc4\u4f30\u4ee3\u7406\u80fd\u529b\uff0c\u63ed\u793a\u5f53\u524d\u4ee3\u7406\u5728\u9ad8\u9636\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u672a\u80fd\u5145\u5206\u53cd\u6620\u4efb\u52a1\u5f02\u8d28\u6027\u548c\u7528\u6237\u9700\u6c42\uff0c\u963b\u788d\u4e86\u4ee3\u7406\u80fd\u529b\u7684\u9488\u5bf9\u6027\u5f00\u53d1\u548c\u5b9e\u9645\u90e8\u7f72\u3002", "method": "OS-MAP\u8bbe\u8ba1\u4e86416\u4e2a\u73b0\u5b9e\u4efb\u52a1\uff0c\u57fa\u4e8e\u4e94\u7ea7\u81ea\u52a8\u5316\u5206\u7c7b\u548c\u9700\u6c42\u5c42\u6b21\u7ed3\u6784\uff0c\u5f62\u6210\u6027\u80fd-\u6cdb\u5316\u8bc4\u4f30\u77e9\u9635\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5373\u4f7f\u6700\u5148\u8fdb\u7684\u4ee3\u7406\u5728\u6d89\u53ca\u611f\u77e5\u3001\u63a8\u7406\u548c\u534f\u8c03\u7684\u9ad8\u9636\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "OS-MAP\u4e3a\u8ba1\u7b97\u673a\u4ee3\u7406\u7684\u7814\u7a76\u548c\u90e8\u7f72\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u8bc4\u4f30\u6846\u67b6\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6280\u672f\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2507.19151", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.MA", "I.2.9"], "pdf": "https://arxiv.org/pdf/2507.19151", "abs": "https://arxiv.org/abs/2507.19151", "authors": ["Michael Amir", "Guang Yang", "Zhan Gao", "Keisuke Okumura", "Heedo Woo", "Amanda Prorok"], "title": "ReCoDe: Reinforcement Learning-based Dynamic Constraint Design for Multi-Agent Coordination", "comment": null, "summary": "Constraint-based optimization is a cornerstone of robotics, enabling the\ndesign of controllers that reliably encode task and safety requirements such as\ncollision avoidance or formation adherence. However, handcrafted constraints\ncan fail in multi-agent settings that demand complex coordination. We introduce\nReCoDe--Reinforcement-based Constraint Design--a decentralized, hybrid\nframework that merges the reliability of optimization-based controllers with\nthe adaptability of multi-agent reinforcement learning. Rather than discarding\nexpert controllers, ReCoDe improves them by learning additional, dynamic\nconstraints that capture subtler behaviors, for example, by constraining agent\nmovements to prevent congestion in cluttered scenarios. Through local\ncommunication, agents collectively constrain their allowed actions to\ncoordinate more effectively under changing conditions. In this work, we focus\non applications of ReCoDe to multi-agent navigation tasks requiring intricate,\ncontext-based movements and consensus, where we show that it outperforms purely\nhandcrafted controllers, other hybrid approaches, and standard MARL baselines.\nWe give empirical (real robot) and theoretical evidence that retaining a\nuser-defined controller, even when it is imperfect, is more efficient than\nlearning from scratch, especially because ReCoDe can dynamically change the\ndegree to which it relies on this controller.", "AI": {"tldr": "ReCoDe\u6846\u67b6\u7ed3\u5408\u4f18\u5316\u63a7\u5236\u5668\u4e0e\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff0c\u901a\u8fc7\u52a8\u6001\u7ea6\u675f\u6539\u8fdb\u4e13\u5bb6\u63a7\u5236\u5668\uff0c\u63d0\u5347\u591a\u667a\u80fd\u4f53\u5bfc\u822a\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u624b\u5de5\u7ea6\u675f\u5728\u591a\u667a\u80fd\u4f53\u590d\u6742\u534f\u8c03\u573a\u666f\u4e2d\u53ef\u80fd\u5931\u6548\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u7ed3\u5408\u53ef\u9760\u6027\u4e0e\u9002\u5e94\u6027\u7684\u65b9\u6cd5\u3002", "method": "ReCoDe\u901a\u8fc7\u5c40\u90e8\u901a\u4fe1\u5b66\u4e60\u52a8\u6001\u7ea6\u675f\uff0c\u6539\u8fdb\u4e13\u5bb6\u63a7\u5236\u5668\uff0c\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u534f\u8c03\u3002", "result": "\u5728\u590d\u6742\u5bfc\u822a\u4efb\u52a1\u4e2d\uff0cReCoDe\u4f18\u4e8e\u7eaf\u624b\u5de5\u63a7\u5236\u5668\u3001\u5176\u4ed6\u6df7\u5408\u65b9\u6cd5\u53ca\u6807\u51c6MARL\u57fa\u7ebf\u3002", "conclusion": "\u4fdd\u7559\u7528\u6237\u5b9a\u4e49\u63a7\u5236\u5668\u5e76\u52a8\u6001\u8c03\u6574\u5176\u4f9d\u8d56\u7a0b\u5ea6\uff0c\u6bd4\u4ece\u5934\u5b66\u4e60\u66f4\u9ad8\u6548\u3002"}}
{"id": "2507.19172", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.19172", "abs": "https://arxiv.org/abs/2507.19172", "authors": ["Jiyao Wang", "Xiao Yang", "Qingyong Hu", "Jiankai Tang", "Can Liu", "Dengbo He", "Yuntao Wang", "Yingcong Chen", "Kaishun Wu"], "title": "PhysDrive: A Multimodal Remote Physiological Measurement Dataset for In-vehicle Driver Monitoring", "comment": "It is the initial version, not the final version", "summary": "Robust and unobtrusive in-vehicle physiological monitoring is crucial for\nensuring driving safety and user experience. While remote physiological\nmeasurement (RPM) offers a promising non-invasive solution, its translation to\nreal-world driving scenarios is critically constrained by the scarcity of\ncomprehensive datasets. Existing resources are often limited in scale, modality\ndiversity, the breadth of biometric annotations, and the range of captured\nconditions, thereby omitting inherent real-world challenges in driving. Here,\nwe present PhysDrive, the first large-scale multimodal dataset for contactless\nin-vehicle physiological sensing with dedicated consideration on various\nmodality settings and driving factors. PhysDrive collects data from 48 drivers,\nincluding synchronized RGB, near-infrared camera, and raw mmWave radar data,\naccompanied with six synchronized ground truths (ECG, BVP, Respiration, HR, RR,\nand SpO2). It covers a wide spectrum of naturalistic driving conditions,\nincluding driver motions, dynamic natural light, vehicle types, and road\nconditions. We extensively evaluate both signal-processing and deep-learning\nmethods on PhysDrive, establishing a comprehensive benchmark across all\nmodalities, and release full open-source code with compatibility for mainstream\npublic toolboxes. We envision PhysDrive will serve as a foundational resource\nand accelerate research on multimodal driver monitoring and smart-cockpit\nsystems.", "AI": {"tldr": "PhysDrive\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u65e0\u63a5\u89e6\u8f66\u5185\u751f\u7406\u76d1\u6d4b\uff0c\u6db5\u76d6\u591a\u79cd\u9a7e\u9a76\u6761\u4ef6\u548c\u751f\u7406\u4fe1\u53f7\uff0c\u65e8\u5728\u63a8\u52a8\u9a7e\u9a76\u5458\u76d1\u6d4b\u548c\u667a\u80fd\u5ea7\u8231\u7cfb\u7edf\u7684\u7814\u7a76\u3002", "motivation": "\u73b0\u6709\u8fdc\u7a0b\u751f\u7406\u76d1\u6d4b\uff08RPM\uff09\u6570\u636e\u96c6\u89c4\u6a21\u5c0f\u3001\u591a\u6837\u6027\u4e0d\u8db3\uff0c\u65e0\u6cd5\u53cd\u6620\u771f\u5b9e\u9a7e\u9a76\u573a\u666f\u7684\u590d\u6742\u6027\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u5168\u9762\u7684\u6570\u636e\u96c6\u6765\u652f\u6301\u7814\u7a76\u3002", "method": "PhysDrive\u6536\u96c6\u4e8648\u540d\u9a7e\u9a76\u5458\u7684\u591a\u6a21\u6001\u6570\u636e\uff0c\u5305\u62ecRGB\u3001\u8fd1\u7ea2\u5916\u6444\u50cf\u5934\u3001\u6beb\u7c73\u6ce2\u96f7\u8fbe\u6570\u636e\uff0c\u4ee5\u53ca\u516d\u79cd\u540c\u6b65\u751f\u7406\u4fe1\u53f7\uff08ECG\u3001BVP\u3001\u547c\u5438\u3001\u5fc3\u7387\u3001\u547c\u5438\u7387\u548c\u8840\u6c27\uff09\u3002", "result": "\u6570\u636e\u96c6\u8986\u76d6\u4e86\u591a\u79cd\u81ea\u7136\u9a7e\u9a76\u6761\u4ef6\uff0c\u5e76\u901a\u8fc7\u4fe1\u53f7\u5904\u7406\u548c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u8fdb\u884c\u4e86\u5168\u9762\u8bc4\u4f30\uff0c\u5efa\u7acb\u4e86\u591a\u6a21\u6001\u57fa\u51c6\u3002", "conclusion": "PhysDrive\u5c06\u6210\u4e3a\u591a\u6a21\u6001\u9a7e\u9a76\u5458\u76d1\u6d4b\u548c\u667a\u80fd\u5ea7\u8231\u7cfb\u7edf\u7814\u7a76\u7684\u57fa\u7840\u8d44\u6e90\uff0c\u5e76\u63a8\u52a8\u76f8\u5173\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2507.19196", "categories": ["cs.RO", "cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.19196", "abs": "https://arxiv.org/abs/2507.19196", "authors": ["Ruben Janssens", "Tony Belpaeme"], "title": "Towards Multimodal Social Conversations with Robots: Using Vision-Language Models", "comment": "Submitted to the workshop \"Human - Foundation Models Interaction: A\n  Focus On Multimodal Information\" (FoMo-HRI) at IEEE RO-MAN 2025", "summary": "Large language models have given social robots the ability to autonomously\nengage in open-domain conversations. However, they are still missing a\nfundamental social skill: making use of the multiple modalities that carry\nsocial interactions. While previous work has focused on task-oriented\ninteractions that require referencing the environment or specific phenomena in\nsocial interactions such as dialogue breakdowns, we outline the overall needs\nof a multimodal system for social conversations with robots. We then argue that\nvision-language models are able to process this wide range of visual\ninformation in a sufficiently general manner for autonomous social robots. We\ndescribe how to adapt them to this setting, which technical challenges remain,\nand briefly discuss evaluation practices.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u5347\u793e\u4ea4\u673a\u5668\u4eba\u7684\u591a\u6a21\u6001\u4ea4\u4e92\u80fd\u529b\u3002", "motivation": "\u793e\u4ea4\u673a\u5668\u4eba\u76ee\u524d\u7f3a\u4e4f\u5229\u7528\u591a\u6a21\u6001\u4fe1\u606f\u7684\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5728\u5f00\u653e\u57df\u5bf9\u8bdd\u4e2d\u3002", "method": "\u63d0\u51fa\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5904\u7406\u5e7f\u6cdb\u7684\u89c6\u89c9\u4fe1\u606f\uff0c\u5e76\u63cf\u8ff0\u5176\u9002\u5e94\u6027\u548c\u6280\u672f\u6311\u6218\u3002", "result": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u4e3a\u81ea\u4e3b\u793e\u4ea4\u673a\u5668\u4eba\u63d0\u4f9b\u8db3\u591f\u901a\u7528\u7684\u591a\u6a21\u6001\u5904\u7406\u80fd\u529b\u3002", "conclusion": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u662f\u63d0\u5347\u793e\u4ea4\u673a\u5668\u4eba\u591a\u6a21\u6001\u4ea4\u4e92\u80fd\u529b\u7684\u6709\u6548\u9014\u5f84\uff0c\u4f46\u4ecd\u9700\u89e3\u51b3\u6280\u672f\u6311\u6218\u3002"}}
{"id": "2507.19182", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.19182", "abs": "https://arxiv.org/abs/2507.19182", "authors": ["Kuncheng Zou", "Jiahao Mai", "Yonggang Zhang", "Yuyi Wang", "Ond\u0159ej Ku\u017eelka", "Yuanhong Wang", "Yi Chang"], "title": "Faster Lifting for Ordered Domains with Predecessor Relations", "comment": null, "summary": "We investigate lifted inference on ordered domains with predecessor\nrelations, where the elements of the domain respect a total (cyclic) order, and\nevery element has a distinct (clockwise) predecessor. Previous work has\nexplored this problem through weighted first-order model counting (WFOMC),\nwhich computes the weighted sum of models for a given first-order logic\nsentence over a finite domain. In WFOMC, the order constraint is typically\nencoded by the linear order axiom introducing a binary predicate in the\nsentence to impose a linear ordering on the domain elements. The immediate and\nsecond predecessor relations are then encoded by the linear order predicate.\nAlthough WFOMC with the linear order axiom is theoretically tractable, existing\nalgorithms struggle with practical applications, particularly when the\npredecessor relations are involved. In this paper, we treat predecessor\nrelations as a native part of the axiom and devise a novel algorithm that\ninherently supports these relations. The proposed algorithm not only provides\nan exponential speedup for the immediate and second predecessor relations,\nwhich are known to be tractable, but also handles the general k-th predecessor\nrelations. The extensive experiments on lifted inference tasks and\ncombinatorics math problems demonstrate the efficiency of our algorithm,\nachieving speedups of a full order of magnitude.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7b97\u6cd5\uff0c\u7528\u4e8e\u5904\u7406\u6709\u5e8f\u57df\u4e0a\u7684\u524d\u9a71\u5173\u7cfb\uff0c\u663e\u8457\u63d0\u5347\u4e86\u52a0\u6743\u4e00\u9636\u6a21\u578b\u8ba1\u6570\uff08WFOMC\uff09\u7684\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u524d\u9a71\u5173\u7cfb\u65f6\u6548\u7387\u4f4e\u4e0b\uff0c\u5c24\u5176\u662f\u5728\u6d89\u53ca\u7ebf\u6027\u987a\u5e8f\u516c\u7406\u65f6\u3002", "method": "\u5c06\u524d\u9a71\u5173\u7cfb\u4f5c\u4e3a\u516c\u7406\u7684\u539f\u751f\u90e8\u5206\uff0c\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u7b97\u6cd5\uff0c\u652f\u6301\u76f4\u63a5\u5904\u7406\u524d\u9a71\u5173\u7cfb\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u7b97\u6cd5\u5728\u63d0\u5347\u524d\u9a71\u5173\u7cfb\u5904\u7406\u6548\u7387\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u901f\u5ea6\u63d0\u5347\u4e86\u4e00\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "\u65b0\u7b97\u6cd5\u4e3a\u6709\u5e8f\u57df\u4e0a\u7684\u524d\u9a71\u5173\u7cfb\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u7406\u8bba\u548c\u5b9e\u8df5\u610f\u4e49\u3002"}}
{"id": "2507.19242", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.19242", "abs": "https://arxiv.org/abs/2507.19242", "authors": ["Kang Xiangli", "Yage He", "Xianwu Gong", "Zehan Liu", "Yuru Bai"], "title": "Foundation Model-Driven Grasping of Unknown Objects via Center of Gravity Estimation", "comment": null, "summary": "This study presents a grasping method for objects with uneven mass\ndistribution by leveraging diffusion models to localize the center of gravity\n(CoG) on unknown objects. In robotic grasping, CoG deviation often leads to\npostural instability, where existing keypoint-based or affordance-driven\nmethods exhibit limitations. We constructed a dataset of 790 images featuring\nunevenly distributed objects with keypoint annotations for CoG localization. A\nvision-driven framework based on foundation models was developed to achieve\nCoG-aware grasping. Experimental evaluations across real-world scenarios\ndemonstrate that our method achieves a 49\\% higher success rate compared to\nconventional keypoint-based approaches and an 11\\% improvement over\nstate-of-the-art affordance-driven methods. The system exhibits strong\ngeneralization with a 76\\% CoG localization accuracy on unseen objects,\nproviding a novel solution for precise and stable grasping tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5b9a\u4f4d\u672a\u77e5\u7269\u4f53\u7684\u91cd\u5fc3\uff08CoG\uff09\uff0c\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u6293\u53d6\u4e2d\u56e0\u91cd\u5fc3\u504f\u79fb\u5bfc\u81f4\u7684\u59ff\u6001\u4e0d\u7a33\u5b9a\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5173\u952e\u70b9\u6216\u529f\u80fd\u9a71\u52a8\u7684\u65b9\u6cd5\u5728\u6293\u53d6\u8d28\u91cf\u5206\u5e03\u4e0d\u5747\u7684\u7269\u4f53\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5bfc\u81f4\u6293\u53d6\u5931\u8d25\u7387\u9ad8\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b790\u5f20\u56fe\u50cf\u7684\u6570\u636e\u96c6\uff0c\u6807\u6ce8\u4e86\u91cd\u5fc3\u5173\u952e\u70b9\uff0c\u5e76\u5f00\u53d1\u4e86\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\u7684\u89c6\u89c9\u9a71\u52a8\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6bd4\u4f20\u7edf\u5173\u952e\u70b9\u65b9\u6cd5\u6210\u529f\u7387\u63d0\u9ad849%\uff0c\u6bd4\u6700\u65b0\u529f\u80fd\u9a71\u52a8\u65b9\u6cd5\u63d0\u9ad811%\uff0c\u4e14\u5728\u672a\u89c1\u7269\u4f53\u4e0a\u91cd\u5fc3\u5b9a\u4f4d\u51c6\u786e\u7387\u8fbe76%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u7cbe\u786e\u7a33\u5b9a\u7684\u6293\u53d6\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.19261", "categories": ["cs.AI", "cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2507.19261", "abs": "https://arxiv.org/abs/2507.19261", "authors": ["Osama Almurshed", "Ashish Kaushal", "Asmail Muftah", "Nitin Auluck", "Omer Rana"], "title": "Knowledge Grafting: A Mechanism for Optimizing AI Model Deployment in Resource-Constrained Environments", "comment": "18 pages, 4 figures, ArXiv preprint - Novel \"knowledge grafting\"\n  technique achieving 88.54% AI model size reduction while improving accuracy\n  for resource-constrained deployment", "summary": "The increasing adoption of Artificial Intelligence (AI) has led to larger,\nmore complex models with numerous parameters that require substantial computing\npower -- resources often unavailable in many real-world application scenarios.\nOur paper addresses this challenge by introducing knowledge grafting, a novel\nmechanism that optimizes AI models for resource-constrained environments by\ntransferring selected features (the scion) from a large donor model to a\nsmaller rootstock model. The approach achieves an 88.54% reduction in model\nsize (from 64.39 MB to 7.38 MB), while improving generalization capability of\nthe model. Our new rootstock model achieves 89.97% validation accuracy (vs.\ndonor's 87.47%), maintains lower validation loss (0.2976 vs. 0.5068), and\nperforms exceptionally well on unseen test data with 90.45% accuracy. It\naddresses the typical size vs performance trade-off, and enables deployment of\nAI frameworks on resource-constrained devices with enhanced performance. We\nhave tested our approach on an agricultural weed detection scenario, however,\nit can be extended across various edge computing scenarios, potentially\naccelerating AI adoption in areas with limited hardware/software support -- by\nmirroring in a similar manner the horticultural grafting enables productive\ncultivation in challenging agri-based environments.", "AI": {"tldr": "\u63d0\u51fa\u77e5\u8bc6\u5ac1\u63a5\u65b9\u6cd5\uff0c\u4f18\u5316\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684AI\u6a21\u578b\uff0c\u663e\u8457\u51cf\u5c0f\u6a21\u578b\u5927\u5c0f\u5e76\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3AI\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u90e8\u7f72\u7684\u6311\u6218\uff0c\u907f\u514d\u6027\u80fd\u4e0e\u6a21\u578b\u5927\u5c0f\u7684\u6743\u8861\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u77e5\u8bc6\u5ac1\u63a5\uff0c\u5c06\u5927\u6a21\u578b\uff08\u4f9b\u4f53\uff09\u7684\u9009\u5b9a\u7279\u5f81\uff08\u63a5\u7a57\uff09\u8f6c\u79fb\u5230\u5c0f\u6a21\u578b\uff08\u7827\u6728\uff09\u4e2d\u3002", "result": "\u6a21\u578b\u5927\u5c0f\u51cf\u5c1188.54%\uff0c\u9a8c\u8bc1\u51c6\u786e\u7387\u63d0\u5347\u81f389.97%\uff0c\u6d4b\u8bd5\u51c6\u786e\u7387\u8fbe90.45%\u3002", "conclusion": "\u77e5\u8bc6\u5ac1\u63a5\u65b9\u6cd5\u6709\u6548\uff0c\u9002\u7528\u4e8e\u8fb9\u7f18\u8ba1\u7b97\u573a\u666f\uff0c\u63a8\u52a8AI\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2507.19335", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.19335", "abs": "https://arxiv.org/abs/2507.19335", "authors": ["Ilaria Consoli", "Claudio Mattutino", "Cristina Gena", "Berardina de Carolis", "Giuseppe Palestra"], "title": "How Age Influences the Interpretation of Emotional Body Language in Humanoid Robots -- long paper version", "comment": null, "summary": "This paper presents an empirical study investigating how individuals across\ndifferent age groups, children, young and older adults, interpret emotional\nbody language expressed by the humanoid robot NAO. The aim is to offer insights\ninto how users perceive and respond to emotional cues from robotic agents,\nthrough an empirical evaluation of the robot's effectiveness in conveying\nemotions to different groups of users. By analyzing data collected from elderly\nparticipants and comparing these findings with previously gathered data from\nyoung adults and children, the study highlights similarities and differences\nbetween the groups, with younger and older users more similar but different\nfrom young adults.", "AI": {"tldr": "\u7814\u7a76\u4e0d\u540c\u5e74\u9f84\u6bb5\uff08\u513f\u7ae5\u3001\u5e74\u8f7b\u4eba\u548c\u8001\u5e74\u4eba\uff09\u5982\u4f55\u7406\u89e3\u4eba\u5f62\u673a\u5668\u4ebaNAO\u8868\u8fbe\u7684\u60c5\u611f\u80a2\u4f53\u8bed\u8a00\uff0c\u53d1\u73b0\u8001\u5e74\u4eba\u4e0e\u513f\u7ae5\u7684\u53cd\u5e94\u66f4\u76f8\u4f3c\uff0c\u4f46\u4e0e\u5e74\u8f7b\u4eba\u4e0d\u540c\u3002", "motivation": "\u63a2\u8ba8\u7528\u6237\u5bf9\u673a\u5668\u4eba\u60c5\u611f\u7ebf\u7d22\u7684\u611f\u77e5\u548c\u53cd\u5e94\uff0c\u8bc4\u4f30\u673a\u5668\u4eba\u5bf9\u4e0d\u540c\u5e74\u9f84\u6bb5\u7528\u6237\u7684\u60c5\u611f\u4f20\u8fbe\u6548\u679c\u3002", "method": "\u901a\u8fc7\u6536\u96c6\u8001\u5e74\u53c2\u4e0e\u8005\u7684\u6570\u636e\uff0c\u5e76\u4e0e\u4e4b\u524d\u5e74\u8f7b\u4eba\u548c\u513f\u7ae5\u7684\u6570\u636e\u8fdb\u884c\u6bd4\u8f83\u5206\u6790\u3002", "result": "\u8001\u5e74\u4eba\u548c\u513f\u7ae5\u5bf9\u673a\u5668\u4eba\u60c5\u611f\u7684\u7406\u89e3\u66f4\u76f8\u4f3c\uff0c\u4f46\u4e0e\u5e74\u8f7b\u4eba\u5b58\u5728\u5dee\u5f02\u3002", "conclusion": "\u5e74\u9f84\u5dee\u5f02\u5f71\u54cd\u7528\u6237\u5bf9\u673a\u5668\u4eba\u60c5\u611f\u8868\u8fbe\u7684\u7406\u89e3\uff0c\u8001\u5e74\u4eba\u548c\u513f\u7ae5\u7684\u53cd\u5e94\u6a21\u5f0f\u66f4\u63a5\u8fd1\u3002"}}
{"id": "2507.19263", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19263", "abs": "https://arxiv.org/abs/2507.19263", "authors": ["Achille Morenville", "\u00c9ric Piette"], "title": "Modeling Uncertainty: Constraint-Based Belief States in Imperfect-Information Games", "comment": null, "summary": "In imperfect-information games, agents must make decisions based on partial\nknowledge of the game state. The Belief Stochastic Game model addresses this\nchallenge by delegating state estimation to the game model itself. This allows\nagents to operate on externally provided belief states, thereby reducing the\nneed for game-specific inference logic. This paper investigates two approaches\nto represent beliefs in games with hidden piece identities: a constraint-based\nmodel using Constraint Satisfaction Problems and a probabilistic extension\nusing Belief Propagation to estimate marginal probabilities. We evaluated the\nimpact of both representations using general-purpose agents across two\ndifferent games. Our findings indicate that constraint-based beliefs yield\nresults comparable to those of probabilistic inference, with minimal\ndifferences in agent performance. This suggests that constraint-based belief\nstates alone may suffice for effective decision-making in many settings.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5728\u4e0d\u5b8c\u5168\u4fe1\u606f\u6e38\u620f\u4e2d\uff0c\u901a\u8fc7\u7ea6\u675f\u6ee1\u8db3\u95ee\u9898\u548c\u4fe1\u5ff5\u4f20\u64ad\u4e24\u79cd\u65b9\u6cd5\u8868\u793a\u9690\u85cf\u68cb\u5b50\u8eab\u4efd\u7684\u4fe1\u5ff5\uff0c\u53d1\u73b0\u7ea6\u675f\u65b9\u6cd5\u6548\u679c\u63a5\u8fd1\u6982\u7387\u63a8\u65ad\u3002", "motivation": "\u89e3\u51b3\u4e0d\u5b8c\u5168\u4fe1\u606f\u6e38\u620f\u4e2d\u4ee3\u7406\u9700\u57fa\u4e8e\u90e8\u5206\u77e5\u8bc6\u51b3\u7b56\u7684\u95ee\u9898\uff0c\u51cf\u5c11\u6e38\u620f\u7279\u5b9a\u63a8\u7406\u903b\u8f91\u7684\u9700\u6c42\u3002", "method": "\u91c7\u7528\u7ea6\u675f\u6ee1\u8db3\u95ee\u9898\u548c\u4fe1\u5ff5\u4f20\u64ad\u4e24\u79cd\u65b9\u6cd5\u8868\u793a\u4fe1\u5ff5\uff0c\u5e76\u5728\u4e24\u79cd\u6e38\u620f\u4e2d\u8bc4\u4f30\u5176\u6548\u679c\u3002", "result": "\u7ea6\u675f\u65b9\u6cd5\u4e0e\u6982\u7387\u63a8\u65ad\u6548\u679c\u76f8\u8fd1\uff0c\u4ee3\u7406\u6027\u80fd\u5dee\u5f02\u5fae\u5c0f\u3002", "conclusion": "\u7ea6\u675f\u4fe1\u5ff5\u72b6\u6001\u53ef\u80fd\u8db3\u4ee5\u652f\u6301\u6709\u6548\u51b3\u7b56\u3002"}}
{"id": "2507.19364", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.19364", "abs": "https://arxiv.org/abs/2507.19364", "authors": ["Patrick Taillandier", "Jean Daniel Zucker", "Arnaud Grignard", "Benoit Gaudou", "Nghi Quang Huynh", "Alexis Drogoul"], "title": "Integrating LLM in Agent-Based Social Simulation: Opportunities and Challenges", "comment": null, "summary": "This position paper examines the use of Large Language Models (LLMs) in\nsocial simulation, analyzing both their potential and their limitations from a\ncomputational social science perspective. The first part reviews recent\nfindings on the ability of LLMs to replicate key aspects of human cognition,\nincluding Theory of Mind reasoning and social inference, while also\nhighlighting significant limitations such as cognitive biases, lack of true\nunderstanding, and inconsistencies in behavior. The second part surveys\nemerging applications of LLMs in multi-agent simulation frameworks, focusing on\nsystem architectures, scale, and validation strategies. Notable projects such\nas Generative Agents (Smallville) and AgentSociety are discussed in terms of\ntheir design choices, empirical grounding, and methodological innovations.\nParticular attention is given to the challenges of behavioral fidelity,\ncalibration, and reproducibility in large-scale LLM-driven simulations. The\nfinal section distinguishes between contexts where LLMs, like other black-box\nsystems, offer direct value-such as interactive simulations and serious\ngames-and those where their use is more problematic, notably in explanatory or\npredictive modeling. The paper concludes by advocating for hybrid approaches\nthat integrate LLMs into traditional agent-based modeling platforms (GAMA,\nNetlogo, etc), enabling modelers to combine the expressive flexibility of\nlanguage-based reasoning with the transparency and analytical rigor of\nclassical rule-based systems.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u793e\u4f1a\u6a21\u62df\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u4e0e\u9650\u5236\uff0c\u63d0\u51fa\u7ed3\u5408\u4f20\u7edf\u5efa\u6a21\u65b9\u6cd5\u7684\u6df7\u5408\u65b9\u6848\u3002", "motivation": "\u4ece\u8ba1\u7b97\u793e\u4f1a\u79d1\u5b66\u89c6\u89d2\u5206\u6790LLMs\u5728\u6a21\u62df\u4eba\u7c7b\u8ba4\u77e5\u548c\u793e\u4f1a\u884c\u4e3a\u4e2d\u7684\u4ef7\u503c\u4e0e\u95ee\u9898\u3002", "method": "\u5206\u4e3a\u4e09\u90e8\u5206\uff1a1\uff09\u8bc4\u4f30LLMs\u7684\u8ba4\u77e5\u80fd\u529b\u4e0e\u5c40\u9650\uff1b2\uff09\u8c03\u7814\u591a\u667a\u80fd\u4f53\u6a21\u62df\u6846\u67b6\u4e2d\u7684\u5e94\u7528\uff1b3\uff09\u533a\u5206\u9002\u7528\u573a\u666f\u5e76\u63d0\u5021\u6df7\u5408\u65b9\u6cd5\u3002", "result": "LLMs\u5728\u4ea4\u4e92\u6a21\u62df\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u89e3\u91ca\u6027\u6216\u9884\u6d4b\u6027\u5efa\u6a21\u4e2d\u5b58\u5728\u6311\u6218\u3002", "conclusion": "\u5efa\u8bae\u5c06LLMs\u4e0e\u4f20\u7edf\u57fa\u4e8e\u89c4\u5219\u7684\u5efa\u6a21\u5e73\u53f0\u7ed3\u5408\uff0c\u4ee5\u517c\u987e\u7075\u6d3b\u6027\u4e0e\u900f\u660e\u5ea6\u3002"}}
{"id": "2507.19372", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.19372", "abs": "https://arxiv.org/abs/2507.19372", "authors": ["Flavio Petruzzellis", "Alberto Testolin", "Alessandro Sperduti"], "title": "Learning neuro-symbolic convergent term rewriting systems", "comment": "48 pages, 31 figures. Submitted for review by Artificial Intelligence\n  Journal", "summary": "Building neural systems that can learn to execute symbolic algorithms is a\nchallenging open problem in artificial intelligence, especially when aiming for\nstrong generalization and out-of-distribution performance. In this work, we\nintroduce a general framework for learning convergent term rewriting systems\nusing a neuro-symbolic architecture inspired by the rewriting algorithm itself.\nWe present two modular implementations of such architecture: the Neural\nRewriting System (NRS) and the Fast Neural Rewriting System (FastNRS). As a\nresult of algorithmic-inspired design and key architectural elements, both\nmodels can generalize to out-of-distribution instances, with FastNRS offering\nsignificant improvements in terms of memory efficiency, training speed, and\ninference time. We evaluate both architectures on four tasks involving the\nsimplification of mathematical formulas and further demonstrate their\nversatility in a multi-domain learning scenario, where a single model is\ntrained to solve multiple types of problems simultaneously. The proposed system\nsignificantly outperforms two strong neural baselines: the Neural Data Router,\na recent transformer variant specifically designed to solve algorithmic\nproblems, and GPT-4o, one of the most powerful general-purpose large-language\nmodels. Moreover, our system matches or outperforms the latest o1-preview model\nfrom OpenAI that excels in reasoning benchmarks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u7b26\u53f7\u67b6\u6784\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5b66\u4e60\u6536\u655b\u7684\u9879\u91cd\u5199\u7cfb\u7edf\uff0c\u5e76\u5b9e\u73b0\u4e86\u4e24\u79cd\u6a21\u5757\u5316\u6a21\u578b\uff08NRS\u548cFastNRS\uff09\uff0c\u5728\u6cdb\u5316\u548c\u8de8\u5206\u5e03\u6027\u80fd\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u7b26\u53f7\u7b97\u6cd5\u6267\u884c\u7684\u6cdb\u5316\u548c\u8de8\u5206\u5e03\u6027\u80fd\u95ee\u9898\uff0c\u662f\u4eba\u5de5\u667a\u80fd\u4e2d\u7684\u4e00\u4e2a\u6311\u6218\u6027\u8bfe\u9898\u3002", "method": "\u8bbe\u8ba1\u4e86\u53d7\u91cd\u5199\u7b97\u6cd5\u542f\u53d1\u7684\u795e\u7ecf\u7b26\u53f7\u67b6\u6784\uff0c\u5b9e\u73b0\u4e86NRS\u548cFastNRS\u4e24\u79cd\u6a21\u578b\uff0c\u91cd\u70b9\u4f18\u5316\u4e86\u5185\u5b58\u6548\u7387\u3001\u8bad\u7ec3\u901f\u5ea6\u548c\u63a8\u7406\u65f6\u95f4\u3002", "result": "\u5728\u6570\u5b66\u516c\u5f0f\u7b80\u5316\u7b49\u4efb\u52a1\u4e2d\uff0c\u6a21\u578b\u663e\u8457\u4f18\u4e8eNeural Data Router\u548cGPT-4o\u7b49\u57fa\u7ebf\uff0c\u751a\u81f3\u5339\u914d\u6216\u8d85\u8d8a\u4e86OpenAI\u7684\u6700\u65b0\u63a8\u7406\u6a21\u578b\u3002", "conclusion": "\u63d0\u51fa\u7684\u6846\u67b6\u5728\u7b26\u53f7\u7b97\u6cd5\u5b66\u4e60\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u9ad8\u6548\u6027\u548c\u591a\u4efb\u52a1\u5b66\u4e60\u80fd\u529b\u3002"}}
{"id": "2507.19458", "categories": ["cs.AI", "cs.LG", "cs.SY", "eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.19458", "abs": "https://arxiv.org/abs/2507.19458", "authors": ["Amir Fard", "Arnold X. -X. Yuan"], "title": "Hierarchical Deep Reinforcement Learning Framework for Multi-Year Asset Management Under Budget Constraints", "comment": null, "summary": "Budget planning and maintenance optimization are crucial for infrastructure\nasset management, ensuring cost-effectiveness and sustainability. However, the\ncomplexity arising from combinatorial action spaces, diverse asset\ndeterioration, stringent budget constraints, and environmental uncertainty\nsignificantly limits existing methods' scalability. This paper proposes a\nHierarchical Deep Reinforcement Learning methodology specifically tailored to\nmulti-year infrastructure planning. Our approach decomposes the problem into\ntwo hierarchical levels: a high-level Budget Planner allocating annual budgets\nwithin explicit feasibility bounds, and a low-level Maintenance Planner\nprioritizing assets within the allocated budget. By structurally separating\nmacro-budget decisions from asset-level prioritization and integrating linear\nprogramming projection within a hierarchical Soft Actor-Critic framework, the\nmethod efficiently addresses exponential growth in the action space and ensures\nrigorous budget compliance. A case study evaluating sewer networks of varying\nsizes (10, 15, and 20 sewersheds) illustrates the effectiveness of the proposed\napproach. Compared to conventional Deep Q-Learning and enhanced genetic\nalgorithms, our methodology converges more rapidly, scales effectively, and\nconsistently delivers near-optimal solutions even as network size grows.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u591a\u5e74\u5ea6\u57fa\u7840\u8bbe\u65bd\u9884\u7b97\u89c4\u5212\u4e0e\u7ef4\u62a4\u4f18\u5316\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u6027\u548c\u53ef\u6269\u5c55\u6027\u4e0a\u7684\u9650\u5236\u3002", "motivation": "\u57fa\u7840\u8bbe\u65bd\u8d44\u4ea7\u7ba1\u7406\u4e2d\u7684\u9884\u7b97\u89c4\u5212\u4e0e\u7ef4\u62a4\u4f18\u5316\u9762\u4e34\u7ec4\u5408\u52a8\u4f5c\u7a7a\u95f4\u3001\u8d44\u4ea7\u9000\u5316\u591a\u6837\u6027\u3001\u4e25\u683c\u9884\u7b97\u7ea6\u675f\u548c\u73af\u5883\u4e0d\u786e\u5b9a\u6027\u7b49\u590d\u6742\u6027\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6269\u5c55\u3002", "method": "\u91c7\u7528\u5206\u5c42\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5206\u4e3a\u9ad8\u5c42\u9884\u7b97\u89c4\u5212\u5668\u548c\u4f4e\u5c42\u7ef4\u62a4\u89c4\u5212\u5668\uff0c\u7ed3\u5408\u7ebf\u6027\u89c4\u5212\u6295\u5f71\u548c\u5206\u5c42Soft Actor-Critic\u6846\u67b6\uff0c\u786e\u4fdd\u9884\u7b97\u5408\u89c4\u6027\u548c\u9ad8\u6548\u6027\u3002", "result": "\u572810\u300115\u548c20\u4e2a\u6c61\u6c34\u7ba1\u7f51\u6848\u4f8b\u4e2d\uff0c\u8be5\u65b9\u6cd5\u6bd4\u4f20\u7edf\u6df1\u5ea6Q\u5b66\u4e60\u548c\u9057\u4f20\u7b97\u6cd5\u6536\u655b\u66f4\u5feb\u3001\u6269\u5c55\u6027\u66f4\u597d\uff0c\u5e76\u80fd\u63d0\u4f9b\u63a5\u8fd1\u6700\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u57fa\u7840\u8bbe\u65bd\u89c4\u5212\u4e2d\u7684\u590d\u6742\u6027\u548c\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u4e3a\u591a\u5e74\u5ea6\u9884\u7b97\u4e0e\u7ef4\u62a4\u4f18\u5316\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
