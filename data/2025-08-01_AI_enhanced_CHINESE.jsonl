{"id": "2507.22951", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.22951", "abs": "https://arxiv.org/abs/2507.22951", "authors": ["Alessandro Lonardi", "Samy Badreddine", "Tarek R. Besold", "Pablo Sanchez Martin"], "title": "Unifying Post-hoc Explanations of Knowledge Graph Completions", "comment": null, "summary": "Post-hoc explainability for Knowledge Graph Completion (KGC) lacks\nformalization and consistent evaluations, hindering reproducibility and\ncross-study comparisons. This paper argues for a unified approach to post-hoc\nexplainability in KGC. First, we propose a general framework to characterize\npost-hoc explanations via multi-objective optimization, balancing their\neffectiveness and conciseness. This unifies existing post-hoc explainability\nalgorithms in KGC and the explanations they produce. Next, we suggest and\nempirically support improved evaluation protocols using popular metrics like\nMean Reciprocal Rank and Hits@$k$. Finally, we stress the importance of\ninterpretability as the ability of explanations to address queries meaningful\nto end-users. By unifying methods and refining evaluation standards, this work\naims to make research in KGC explainability more reproducible and impactful.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\uff08KGC\uff09\u540e\u89e3\u91ca\u6027\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u76ee\u6807\u4f18\u5316\u6846\u67b6\u5e73\u8861\u89e3\u91ca\u7684\u6709\u6548\u6027\u548c\u7b80\u6d01\u6027\uff0c\u5e76\u6539\u8fdb\u4e86\u8bc4\u4f30\u534f\u8bae\u3002", "motivation": "\u5f53\u524dKGC\u540e\u89e3\u91ca\u6027\u7f3a\u4e4f\u5f62\u5f0f\u5316\u548c\u4e00\u81f4\u7684\u8bc4\u4f30\uff0c\u963b\u788d\u4e86\u7814\u7a76\u7684\u53ef\u91cd\u590d\u6027\u548c\u8de8\u7814\u7a76\u6bd4\u8f83\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u901a\u7528\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u76ee\u6807\u4f18\u5316\u7edf\u4e00\u73b0\u6709\u540e\u89e3\u91ca\u6027\u7b97\u6cd5\uff0c\u5e76\u6539\u8fdb\u8bc4\u4f30\u534f\u8bae\uff08\u5982MRR\u548cHits@k\uff09\u3002", "result": "\u6846\u67b6\u7edf\u4e00\u4e86\u73b0\u6709\u65b9\u6cd5\uff0c\u8bc4\u4f30\u534f\u8bae\u5f97\u5230\u5b9e\u8bc1\u652f\u6301\uff0c\u5f3a\u8c03\u4e86\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u67e5\u8be2\u89e3\u91ca\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u7edf\u4e00\u65b9\u6cd5\u548c\u6539\u8fdb\u8bc4\u4f30\u6807\u51c6\uff0c\u65e8\u5728\u63d0\u5347KGC\u89e3\u91ca\u6027\u7814\u7a76\u7684\u53ef\u91cd\u590d\u6027\u548c\u5f71\u54cd\u529b\u3002"}}
{"id": "2507.23018", "categories": ["cs.AI", "cs.CE", "cs.DC", "cs.LG", "I.2.6"], "pdf": "https://arxiv.org/pdf/2507.23018", "abs": "https://arxiv.org/abs/2507.23018", "authors": ["Wesley Brewer", "Patrick Widener", "Valentine Anantharaj", "Feiyi Wang", "Tom Beck", "Arjun Shankar", "Sarp Oral"], "title": "Data Readiness for Scientific AI at Scale", "comment": "10 pages, 1 figure, 2 tables", "summary": "This paper examines how Data Readiness for AI (DRAI) principles apply to\nleadership-scale scientific datasets used to train foundation models. We\nanalyze archetypal workflows across four representative domains - climate,\nnuclear fusion, bio/health, and materials - to identify common preprocessing\npatterns and domain-specific constraints. We introduce a two-dimensional\nreadiness framework composed of Data Readiness Levels (raw to AI-ready) and\nData Processing Stages (ingest to shard), both tailored to high performance\ncomputing (HPC) environments. This framework outlines key challenges in\ntransforming scientific data for scalable AI training, emphasizing\ntransformer-based generative models. Together, these dimensions form a\nconceptual maturity matrix that characterizes scientific data readiness and\nguides infrastructure development toward standardized, cross-domain support for\nscalable and reproducible AI for science.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u6570\u636e\u51c6\u5907\uff08DRAI\uff09\u539f\u5219\u5728\u9886\u5bfc\u7ea7\u79d1\u5b66\u6570\u636e\u96c6\u4e0a\u7684\u5e94\u7528\uff0c\u63d0\u51fa\u4e00\u4e2a\u9488\u5bf9\u9ad8\u6027\u80fd\u8ba1\u7b97\u73af\u5883\u7684\u4e8c\u7ef4\u51c6\u5907\u6846\u67b6\uff0c\u7528\u4e8e\u6307\u5bfcAI\u8bad\u7ec3\u6570\u636e\u7684\u6807\u51c6\u5316\u5904\u7406\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u89e3\u51b3\u79d1\u5b66\u6570\u636e\u5728AI\u8bad\u7ec3\u4e2d\u7684\u9884\u5904\u7406\u6311\u6218\uff0c\u5c24\u5176\u662f\u9488\u5bf9\u8de8\u9886\u57df\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u5206\u6790\u56db\u4e2a\u4ee3\u8868\u6027\u9886\u57df\u7684\u6570\u636e\u5de5\u4f5c\u6d41\uff0c\u5e76\u63d0\u51fa\u4e00\u4e2a\u7ed3\u5408\u6570\u636e\u51c6\u5907\u7ea7\u522b\u548c\u5904\u7406\u9636\u6bb5\u7684\u4e24\u7ef4\u6846\u67b6\u3002", "result": "\u7ed3\u679c\u662f\u4e00\u4e2a\u6210\u719f\u5ea6\u77e9\u9635\uff0c\u7528\u4e8e\u8bc4\u4f30\u79d1\u5b66\u6570\u636e\u7684AI\u51c6\u5907\u72b6\u6001\uff0c\u5e76\u6307\u5bfc\u57fa\u7840\u8bbe\u65bd\u5f00\u53d1\u3002", "conclusion": "\u7ed3\u8bba\u5f3a\u8c03\u4e86\u8be5\u6846\u67b6\u5bf9\u8de8\u9886\u57df\u3001\u53ef\u6269\u5c55\u548c\u53ef\u91cd\u590dAI\u79d1\u5b66\u5e94\u7528\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2507.23067", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.23067", "abs": "https://arxiv.org/abs/2507.23067", "authors": ["Zhenyu Pan", "Yutong Zhang", "Jianshu Zhang", "Haoran Lu", "Haozheng Luo", "Yuwei Han", "Philip S. Yu", "Manling Li", "Han Liu"], "title": "FairReason: Balancing Reasoning and Social Bias in MLLMs", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) already achieve state-of-the-art\nresults across a wide range of tasks and modalities. To push their reasoning\nability further, recent studies explore advanced prompting schemes and\npost-training fine-tuning. Although these techniques improve logical accuracy,\nthey frequently leave the models' outputs burdened with pronounced social\nbiases. Clarifying how reasoning gains interact with bias mitigation-and\nwhether the two objectives inherently trade off-therefore remains an open and\npressing research problem. Our study begins by benchmarking three\nbias-mitigation strategies-supervised fine-uning (SFT), knowledge distillation\n(KD), and rule-based reinforcement learning (RL)-under identical conditions,\nestablishing their baseline strengths and weaknesses. Building on these\nresults, we vary the proportion of debias-focused and reasoning-centric samples\nwithin each paradigm to chart the reasoning-versus-bias trade-off. Our sweeps\nreveal a consistent sweet spot: a roughly 1:4 mix trained with reinforcement\nlearning cuts stereotype scores by 10% while retaining 88% of the model's\noriginal reasoning accuracy, offering concrete guidance for balancing fairness\nand capability in MLLMs.", "AI": {"tldr": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u591a\u79cd\u4efb\u52a1\u548c\u6a21\u6001\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u63a8\u7406\u80fd\u529b\u7684\u63d0\u5347\u5e38\u4f34\u968f\u793e\u4f1a\u504f\u89c1\u7684\u589e\u52a0\u3002\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u4e09\u79cd\u504f\u89c1\u7f13\u89e3\u7b56\u7565\uff0c\u53d1\u73b0\u5f3a\u5316\u5b66\u4e60\u7ed3\u54081:4\u7684\u504f\u89c1\u4e0e\u63a8\u7406\u6837\u672c\u6bd4\u4f8b\u80fd\u5728\u51cf\u5c11\u504f\u89c1\u7684\u540c\u65f6\u4fdd\u7559\u5927\u90e8\u5206\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u5728\u63d0\u5347MLLMs\u63a8\u7406\u80fd\u529b\u7684\u540c\u65f6\u6709\u6548\u7f13\u89e3\u5176\u8f93\u51fa\u7684\u793e\u4f1a\u504f\u89c1\uff0c\u89e3\u51b3\u4e24\u8005\u4e4b\u95f4\u7684\u6f5c\u5728\u6743\u8861\u95ee\u9898\u3002", "method": "\u6bd4\u8f83\u4e09\u79cd\u504f\u89c1\u7f13\u89e3\u7b56\u7565\uff08\u76d1\u7763\u5fae\u8c03\u3001\u77e5\u8bc6\u84b8\u998f\u3001\u57fa\u4e8e\u89c4\u5219\u7684\u5f3a\u5316\u5b66\u4e60\uff09\uff0c\u5e76\u901a\u8fc7\u8c03\u6574\u504f\u89c1\u4e0e\u63a8\u7406\u6837\u672c\u6bd4\u4f8b\u5206\u6790\u6743\u8861\u5173\u7cfb\u3002", "result": "\u5f3a\u5316\u5b66\u4e60\u7ed3\u54081:4\u7684\u6837\u672c\u6bd4\u4f8b\u80fd\u51cf\u5c1110%\u7684\u504f\u89c1\uff0c\u540c\u65f6\u4fdd\u755988%\u7684\u539f\u59cb\u63a8\u7406\u51c6\u786e\u6027\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5e73\u8861MLLMs\u7684\u516c\u5e73\u6027\u4e0e\u80fd\u529b\u63d0\u4f9b\u4e86\u5177\u4f53\u6307\u5bfc\uff0c\u5f3a\u5316\u5b66\u4e60\u662f\u6709\u6548\u7684\u6298\u4e2d\u65b9\u6848\u3002"}}
{"id": "2507.23015", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23015", "abs": "https://arxiv.org/abs/2507.23015", "authors": ["Abhinav Jain", "Cindy Grimm", "Stefan Lee"], "title": "Learning to Prune Branches in Modern Tree-Fruit Orchards", "comment": null, "summary": "Dormant tree pruning is labor-intensive but essential to maintaining modern\nhighly-productive fruit orchards. In this work we present a closed-loop\nvisuomotor controller for robotic pruning. The controller guides the cutter\nthrough a cluttered tree environment to reach a specified cut point and ensures\nthe cutters are perpendicular to the branch. We train the controller using a\nnovel orchard simulation that captures the geometric distribution of branches\nin a target apple orchard configuration. Unlike traditional methods requiring\nfull 3D reconstruction, our controller uses just optical flow images from a\nwrist-mounted camera. We deploy our learned policy in simulation and the\nreal-world for an example V-Trellis envy tree with zero-shot transfer,\nachieving a 30% success rate -- approximately half the performance of an oracle\nplanner.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u7684\u95ed\u73af\u63a7\u5236\u673a\u5668\u4eba\u4fee\u526a\u7cfb\u7edf\uff0c\u7528\u4e8e\u9ad8\u6548\u4fee\u526a\u679c\u6811\uff0c\u65e0\u9700\u5b8c\u65743D\u91cd\u5efa\uff0c\u4ec5\u9700\u5149\u5b66\u6d41\u56fe\u50cf\u3002", "motivation": "\u4f20\u7edf\u679c\u6811\u4fee\u526a\u52b3\u52a8\u5bc6\u96c6\u4e14\u6548\u7387\u4f4e\uff0c\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u679c\u56ed\u6a21\u62df\u8bad\u7ec3\u63a7\u5236\u5668\uff0c\u4ec5\u4f9d\u8d56\u8155\u90e8\u6444\u50cf\u5934\u7684\u5149\u5b66\u6d41\u56fe\u50cf\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u8fc1\u79fb\u3002", "result": "\u5728\u4eff\u771f\u548c\u5b9e\u9645\u73af\u5883\u4e2d\u5b9e\u73b030%\u7684\u6210\u529f\u7387\uff0c\u7ea6\u4e3a\u7406\u60f3\u89c4\u5212\u5668\u6027\u80fd\u7684\u4e00\u534a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u673a\u5668\u4eba\u4fee\u526a\u7684\u6f5c\u529b\uff0c\u4f46\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4\u3002"}}
{"id": "2507.23091", "categories": ["cs.AI", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.23091", "abs": "https://arxiv.org/abs/2507.23091", "authors": ["David Noever", "Forrest McKee"], "title": "Moravec's Paradox: Towards an Auditory Turing Test", "comment": null, "summary": "This research work demonstrates that current AI systems fail catastrophically\non auditory tasks that humans perform effortlessly. Drawing inspiration from\nMoravec's paradox (i.e., tasks simple for humans often prove difficult for\nmachines, and vice versa), we introduce an auditory Turing test comprising 917\nchallenges across seven categories: overlapping speech, speech in noise,\ntemporal distortion, spatial audio, coffee-shop noise, phone distortion, and\nperceptual illusions. Our evaluation of state-of-the-art audio models including\nGPT-4's audio capabilities and OpenAI's Whisper reveals a striking failure rate\nexceeding 93%, with even the best-performing model achieving only 6.9% accuracy\non tasks that humans solved at 7.5 times higher success (52%). These results\nexpose focusing failures in how AI systems process complex auditory scenes,\nparticularly in selective attention, noise robustness, and contextual\nadaptation. Our benchmark not only quantifies the human-machine auditory gap\nbut also provides insights into why these failures occur, suggesting that\ncurrent architectures lack fundamental mechanisms for human-like auditory scene\nanalysis. The traditional design of audio CAPTCHAs highlights common filters\nthat humans evolved but machines fail to select in multimodal language models.\nThis work establishes a diagnostic framework for measuring progress toward\nhuman-level machine listening and highlights the need for novel approaches\nintegrating selective attention, physics-based audio understanding, and\ncontext-aware perception into multimodal AI systems.", "AI": {"tldr": "\u5f53\u524dAI\u7cfb\u7edf\u5728\u4eba\u7c7b\u8f7b\u677e\u5b8c\u6210\u7684\u542c\u89c9\u4efb\u52a1\u4e0a\u8868\u73b0\u6781\u5dee\uff0c\u5931\u8d25\u7387\u8d85\u8fc793%\u3002\u7814\u7a76\u901a\u8fc7\u542c\u89c9\u56fe\u7075\u6d4b\u8bd5\u63ed\u793a\u4e86AI\u5728\u9009\u62e9\u6027\u6ce8\u610f\u529b\u3001\u566a\u58f0\u9c81\u68d2\u6027\u548c\u4e0a\u4e0b\u6587\u9002\u5e94\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u53d7Moravec\u6096\u8bba\u542f\u53d1\uff0c\u7814\u7a76\u65e8\u5728\u91cf\u5316AI\u4e0e\u4eba\u7c7b\u5728\u542c\u89c9\u4efb\u52a1\u4e0a\u7684\u5dee\u8ddd\uff0c\u5e76\u63a2\u7d22\u5931\u8d25\u539f\u56e0\u3002", "method": "\u8bbe\u8ba1\u4e86\u5305\u542b917\u4e2a\u6311\u6218\u7684\u542c\u89c9\u56fe\u7075\u6d4b\u8bd5\uff0c\u8986\u76d6\u4e03\u7c7b\u4efb\u52a1\uff0c\u8bc4\u4f30\u4e86GPT-4\u548cWhisper\u7b49\u5148\u8fdb\u6a21\u578b\u3002", "result": "AI\u6a21\u578b\u5e73\u5747\u51c6\u786e\u7387\u4ec56.9%\uff0c\u8fdc\u4f4e\u4e8e\u4eba\u7c7b\u768452%\uff0c\u66b4\u9732\u4e86\u5176\u5728\u590d\u6742\u542c\u89c9\u573a\u666f\u5904\u7406\u4e2d\u7684\u7f3a\u9677\u3002", "conclusion": "\u7814\u7a76\u63d0\u51fa\u4e86\u8bca\u65ad\u6846\u67b6\uff0c\u5f3a\u8c03\u9700\u6574\u5408\u9009\u62e9\u6027\u6ce8\u610f\u529b\u3001\u7269\u7406\u97f3\u9891\u7406\u89e3\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2507.23045", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.23045", "abs": "https://arxiv.org/abs/2507.23045", "authors": ["Emmett Wise", "Pushyami Kaveti", "Qilong Chen", "Wenhao Wang", "Hanumant Singh", "Jonathan Kelly", "David M. Rosen", "Matthew Giamou"], "title": "A Certifably Correct Algorithm for Generalized Robot-World and Hand-Eye Calibration", "comment": "25 pages, 10 figures, submitted to the International Journal of\n  Robotics Research", "summary": "Automatic extrinsic sensor calibration is a fundamental problem for\nmulti-sensor platforms. Reliable and general-purpose solutions should be\ncomputationally efficient, require few assumptions about the structure of the\nsensing environment, and demand little effort from human operators. Since the\nengineering effort required to obtain accurate calibration parameters increases\nwith the number of sensors deployed, robotics researchers have pursued methods\nrequiring few assumptions about the sensing environment and minimal effort from\nhuman operators. In this work, we introduce a fast and certifiably globally\noptimal algorithm for solving a generalized formulation of the\n$\\textit{robot-world and hand-eye calibration}$ (RWHEC) problem. The\nformulation of RWHEC presented is \"generalized\" in that it supports the\nsimultaneous estimation of multiple sensor and target poses, and permits the\nuse of monocular cameras that, alone, are unable to measure the scale of their\nenvironments. In addition to demonstrating our method's superior performance\nover existing solutions, we derive novel identifiability criteria and establish\n$\\textit{a priori}$ guarantees of global optimality for problem instances with\nbounded measurement errors. We also introduce a complementary Lie-algebraic\nlocal solver for RWHEC and compare its performance with our global method and\nprior art. Finally, we provide a free and open-source implementation of our\nalgorithms and experiments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5feb\u901f\u4e14\u5168\u5c40\u6700\u4f18\u7684\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u5e7f\u4e49\u7684\u673a\u5668\u4eba-\u4e16\u754c\u548c\u624b\u773c\u6807\u5b9a\uff08RWHEC\uff09\u95ee\u9898\uff0c\u652f\u6301\u591a\u4f20\u611f\u5668\u548c\u76ee\u6807\u59ff\u6001\u7684\u540c\u65f6\u4f30\u8ba1\uff0c\u5e76\u9002\u7528\u4e8e\u5355\u76ee\u76f8\u673a\u3002", "motivation": "\u591a\u4f20\u611f\u5668\u5e73\u53f0\u7684\u81ea\u52a8\u5916\u53c2\u6807\u5b9a\u662f\u4e00\u4e2a\u57fa\u7840\u95ee\u9898\uff0c\u9700\u8981\u9ad8\u6548\u3001\u901a\u7528\u4e14\u51cf\u5c11\u4eba\u5de5\u5e72\u9884\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u5e7f\u4e49RWHEC\u95ee\u9898\u7684\u65b0\u7b97\u6cd5\uff0c\u652f\u6301\u591a\u4f20\u611f\u5668\u548c\u76ee\u6807\u59ff\u6001\u7684\u540c\u65f6\u4f30\u8ba1\uff0c\u9002\u7528\u4e8e\u5355\u76ee\u76f8\u673a\uff0c\u5e76\u63d0\u4f9b\u4e86\u5168\u5c40\u6700\u4f18\u6027\u4fdd\u8bc1\u3002", "result": "\u7b97\u6cd5\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u8bc6\u522b\u6027\u6807\u51c6\u548c\u5168\u5c40\u6700\u4f18\u6027\u4fdd\u8bc1\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u5728\u591a\u4f20\u611f\u5668\u6807\u5b9a\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u63d0\u4f9b\u4e86\u5f00\u6e90\u5b9e\u73b0\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2507.23163", "categories": ["cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2507.23163", "abs": "https://arxiv.org/abs/2507.23163", "authors": ["Deniz Gorur", "Antonio Rago", "Francesca Toni"], "title": "Argumentatively Coherent Judgmental Forecasting", "comment": "17 pages, 18 figures, ECAI 2025", "summary": "Judgmental forecasting employs human opinions to make predictions about\nfuture events, rather than exclusively historical data as in quantitative\nforecasting. When these opinions form an argumentative structure around\nforecasts, it is useful to study the properties of the forecasts from an\nargumentative perspective. In this paper, we advocate and formally define a\nproperty of argumentative coherence, which, in essence, requires that a\nforecaster's reasoning is coherent with their forecast. We then conduct three\nevaluations with our notion of coherence. First, we assess the impact of\nenforcing coherence on human forecasters as well as on Large Language Model\n(LLM)-based forecasters, given that they have recently shown to be competitive\nwith human forecasters. In both cases, we show that filtering out incoherent\npredictions improves forecasting accuracy consistently, supporting the\npractical value of coherence in both human and LLM-based forecasting. Then, via\ncrowd-sourced user experiments, we show that, despite its apparent\nintuitiveness and usefulness, users do not generally align with this coherence\nproperty. This points to the need to integrate, within argumentation-based\njudgmental forecasting, mechanisms to filter out incoherent opinions before\nobtaining group forecasting predictions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u5e76\u5f62\u5f0f\u5316\u5b9a\u4e49\u4e86\u2018\u8bba\u8bc1\u4e00\u81f4\u6027\u2019\u5c5e\u6027\uff0c\u8981\u6c42\u9884\u6d4b\u8005\u7684\u63a8\u7406\u4e0e\u9884\u6d4b\u4e00\u81f4\u3002\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8fc7\u6ee4\u4e0d\u4e00\u81f4\u9884\u6d4b\u80fd\u63d0\u5347\u4eba\u7c7b\u548cLLM\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u4f46\u7528\u6237\u5b9e\u9a8c\u663e\u793a\u7528\u6237\u901a\u5e38\u4e0d\u9075\u5faa\u8fd9\u4e00\u5c5e\u6027\u3002", "motivation": "\u7814\u7a76\u8bba\u8bc1\u7ed3\u6784\u5bf9\u9884\u6d4b\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u2018\u8bba\u8bc1\u4e00\u81f4\u6027\u2019\u4ee5\u63d0\u5347\u9884\u6d4b\u51c6\u786e\u6027\u3002", "method": "\u5f62\u5f0f\u5316\u5b9a\u4e49\u2018\u8bba\u8bc1\u4e00\u81f4\u6027\u2019\uff0c\u5e76\u8bbe\u8ba1\u5b9e\u9a8c\u8bc4\u4f30\u5176\u5bf9\u4eba\u7c7b\u548cLLM\u9884\u6d4b\u7684\u5f71\u54cd\uff0c\u540c\u65f6\u901a\u8fc7\u7528\u6237\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u63a5\u53d7\u5ea6\u3002", "result": "\u8fc7\u6ee4\u4e0d\u4e00\u81f4\u9884\u6d4b\u663e\u8457\u63d0\u5347\u4eba\u7c7b\u548cLLM\u7684\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u4f46\u7528\u6237\u5b9e\u9a8c\u663e\u793a\u7528\u6237\u901a\u5e38\u4e0d\u9075\u5faa\u8fd9\u4e00\u5c5e\u6027\u3002", "conclusion": "\u8bba\u8bc1\u4e00\u81f4\u6027\u5bf9\u63d0\u5347\u9884\u6d4b\u51c6\u786e\u6027\u6709\u5b9e\u9645\u4ef7\u503c\uff0c\u4f46\u9700\u5728\u7fa4\u4f53\u9884\u6d4b\u4e2d\u5f15\u5165\u673a\u5236\u8fc7\u6ee4\u4e0d\u4e00\u81f4\u610f\u89c1\u3002"}}
{"id": "2507.23053", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.23053", "abs": "https://arxiv.org/abs/2507.23053", "authors": ["Yuanhao Chen", "Liu Zhao", "Ji Ma", "Peng Lu"], "title": "In-between Motion Generation Based Multi-Style Quadruped Robot Locomotion", "comment": null, "summary": "Quadruped robots face persistent challenges in achieving versatile locomotion\ndue to limitations in reference motion data diversity. To address these\nchallenges, this approach introduces an in-between motion generation based\nmulti-style quadruped robot locomotion framework, integrating synergistic\nadvances in motion generation and imitation learning. Our approach establishes\na unified pipeline addressing two fundamental aspects: First, we propose a CVAE\nbased motion generator, synthesizing multi-style dynamically feasible\nlocomotion sequences between arbitrary start and end states. By embedding\nphysical constraints and leveraging joint poses based phase manifold\ncontinuity, this component produces physically plausible motions spanning\nmultiple gait modalities while ensuring kinematic compatibility with robotic\nmorphologies. Second, we adopt the adversarial motion priors algorithm. We\nvalidate the effectiveness of generated motion data in enhancing controller\nstability and improving velocity tracking performance. The proposed framework\ndemonstrates significant improvements in velocity tracking and deployment\nstability. We successfully deploy the framework on a real-world quadruped\nrobot, and the experimental validation confirms the framework's capability to\ngenerate and execute complex motion profiles, including gallop, tripod,\ntrotting and pacing.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u98ce\u683c\u56db\u8db3\u673a\u5668\u4eba\u8fd0\u52a8\u751f\u6210\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u8fd0\u52a8\u751f\u6210\u4e0e\u6a21\u4eff\u5b66\u4e60\uff0c\u63d0\u5347\u8fd0\u52a8\u591a\u6837\u6027\u4e0e\u63a7\u5236\u5668\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u56db\u8db3\u673a\u5668\u4eba\u56e0\u53c2\u8003\u8fd0\u52a8\u6570\u636e\u591a\u6837\u6027\u4e0d\u8db3\u800c\u9762\u4e34\u7684\u8fd0\u52a8\u80fd\u529b\u53d7\u9650\u95ee\u9898\u3002", "method": "\u91c7\u7528CVAE\u751f\u6210\u591a\u98ce\u683c\u8fd0\u52a8\u5e8f\u5217\uff0c\u7ed3\u5408\u7269\u7406\u7ea6\u675f\u4e0e\u76f8\u4f4d\u6d41\u5f62\u8fde\u7eed\u6027\uff1b\u4f7f\u7528\u5bf9\u6297\u8fd0\u52a8\u5148\u9a8c\u7b97\u6cd5\u9a8c\u8bc1\u6570\u636e\u6709\u6548\u6027\u3002", "result": "\u663e\u8457\u63d0\u5347\u901f\u5ea6\u8ddf\u8e2a\u6027\u80fd\u4e0e\u63a7\u5236\u5668\u7a33\u5b9a\u6027\uff0c\u6210\u529f\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u591a\u79cd\u6b65\u6001\u3002", "conclusion": "\u6846\u67b6\u80fd\u751f\u6210\u5e76\u6267\u884c\u590d\u6742\u8fd0\u52a8\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2507.23191", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.23191", "abs": "https://arxiv.org/abs/2507.23191", "authors": ["Meghyn Bienvenu", "Diego Figueira", "Pierre Lafourcade"], "title": "Tractable Responsibility Measures for Ontology-Mediated Query Answering", "comment": "Long version of a paper to appear at KR 2025, which contains further\n  proof details in the appendix", "summary": "Recent work on quantitative approaches to explaining query answers employs\nresponsibility measures to assign scores to facts in order to quantify their\nrespective contributions to obtaining a given answer. In this paper, we study\nthe complexity of computing such responsibility scores in the setting of\nontology-mediated query answering, focusing on a very recently introduced\nfamily of Shapley-value-based responsibility measures defined in terms of\nweighted sums of minimal supports (WSMS). By exploiting results from the\ndatabase setting, we can show that such measures enjoy polynomial data\ncomplexity for classes of ontology-mediated queries that are\nfirst-order-rewritable, whereas the problem becomes \"shP\"-hard when the\nontology language can encode reachability queries (via axioms like $\\exists R.\nA \\sqsubseteq A$). To better understand the tractability frontier, we next\nexplore the combined complexity of WSMS computation. We prove that\nintractability applies already to atomic queries if the ontology language\nsupports conjunction, as well as to unions of `well-behaved' conjunctive\nqueries, even in the absence of an ontology. By contrast, our study yields\npositive results for common DL-Lite dialects: by means of careful analysis, we\nidentify classes of structurally restricted conjunctive queries (which\nintuitively disallow undesirable interactions between query atoms) that admit\ntractable WSMS computation.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u57fa\u4e8eShapley\u503c\u7684\u8d23\u4efb\u8bc4\u5206\uff08WSMS\uff09\u5728ontology-mediated\u67e5\u8be2\u56de\u7b54\u4e2d\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u53d1\u73b0\u5176\u5728\u67d0\u4e9b\u67e5\u8be2\u7c7b\u4e2d\u5177\u6709\u591a\u9879\u5f0f\u6570\u636e\u590d\u6742\u5ea6\uff0c\u800c\u5728\u5176\u4ed6\u60c5\u51b5\u4e0b\u5219\u8868\u73b0\u51fa\u8ba1\u7b97\u56f0\u96be\u3002", "motivation": "\u91cf\u5316\u67e5\u8be2\u7b54\u6848\u4e2d\u5404\u4e8b\u5b9e\u7684\u8d21\u732e\u662f\u89e3\u91ca\u6027\u6570\u636e\u5e93\u7814\u7a76\u7684\u91cd\u8981\u95ee\u9898\uff0c\u672c\u6587\u65e8\u5728\u63a2\u7d22WSMS\u8d23\u4efb\u8bc4\u5206\u5728\u4e0d\u540c\u67e5\u8be2\u548c\u672c\u4f53\u8bed\u8a00\u4e2d\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "method": "\u5229\u7528\u6570\u636e\u5e93\u9886\u57df\u7684\u7ed3\u679c\uff0c\u5206\u6790WSMS\u8d23\u4efb\u8bc4\u5206\u5728first-order-rewritable\u67e5\u8be2\u7c7b\u4e2d\u7684\u591a\u9879\u5f0f\u6570\u636e\u590d\u6742\u5ea6\uff0c\u5e76\u7814\u7a76\u5176\u5728\u5176\u4ed6\u67e5\u8be2\u7c7b\u4e2d\u7684\u8ba1\u7b97\u56f0\u96be\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0cWSMS\u5728first-order-rewritable\u67e5\u8be2\u7c7b\u4e2d\u5177\u6709\u591a\u9879\u5f0f\u6570\u636e\u590d\u6742\u5ea6\uff0c\u4f46\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff08\u5982\u652f\u6301\u5408\u53d6\u7684\u672c\u4f53\u8bed\u8a00\uff09\u8868\u73b0\u51fa\u8ba1\u7b97\u56f0\u96be\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u6784\u9650\u5236\u7684\u67e5\u8be2\u7c7b\uff0c\u53ef\u4ee5\u5728DL-Lite\u65b9\u8a00\u4e2d\u5b9e\u73b0\u9ad8\u6548\u7684WSMS\u8ba1\u7b97\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u3002"}}
{"id": "2507.23088", "categories": ["cs.RO", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.23088", "abs": "https://arxiv.org/abs/2507.23088", "authors": ["Lalithkumar Seenivasan", "Jiru Xu", "Roger D. Soberanis Mukul", "Hao Ding", "Grayson Byrd", "Yu-Chun Ku", "Jose L. Porras", "Masaru Ishii", "Mathias Unberath"], "title": "Beyond Rigid AI: Towards Natural Human-Machine Symbiosis for Interoperative Surgical Assistance", "comment": null, "summary": "Emerging surgical data science and robotics solutions, especially those\ndesigned to provide assistance in situ, require natural human-machine\ninterfaces to fully unlock their potential in providing adaptive and intuitive\naid. Contemporary AI-driven solutions remain inherently rigid, offering limited\nflexibility and restricting natural human-machine interaction in dynamic\nsurgical environments. These solutions rely heavily on extensive task-specific\npre-training, fixed object categories, and explicit manual-prompting. This work\nintroduces a novel Perception Agent that leverages speech-integrated\nprompt-engineered large language models (LLMs), segment anything model (SAM),\nand any-point tracking foundation models to enable a more natural human-machine\ninteraction in real-time intraoperative surgical assistance. Incorporating a\nmemory repository and two novel mechanisms for segmenting unseen elements,\nPerception Agent offers the flexibility to segment both known and unseen\nelements in the surgical scene through intuitive interaction. Incorporating the\nability to memorize novel elements for use in future surgeries, this work takes\na marked step towards human-machine symbiosis in surgical procedures. Through\nquantitative analysis on a public dataset, we show that the performance of our\nagent is on par with considerably more labor-intensive manual-prompting\nstrategies. Qualitatively, we show the flexibility of our agent in segmenting\nnovel elements (instruments, phantom grafts, and gauze) in a custom-curated\ndataset. By offering natural human-machine interaction and overcoming rigidity,\nour Perception Agent potentially brings AI-based real-time assistance in\ndynamic surgical environments closer to reality.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u611f\u77e5\u4ee3\u7406\uff0c\u7ed3\u5408\u8bed\u97f3\u63d0\u793a\u3001LLM\u3001SAM\u548c\u8ddf\u8e2a\u6a21\u578b\uff0c\u63d0\u5347\u624b\u672f\u4e2d\u4eba\u673a\u4ea4\u4e92\u7684\u81ea\u7136\u6027\u548c\u7075\u6d3b\u6027\u3002", "motivation": "\u73b0\u6709AI\u89e3\u51b3\u65b9\u6848\u5728\u52a8\u6001\u624b\u672f\u73af\u5883\u4e2d\u7f3a\u4e4f\u7075\u6d3b\u6027\uff0c\u9650\u5236\u4e86\u81ea\u7136\u4ea4\u4e92\uff0c\u9700\u4f9d\u8d56\u5927\u91cf\u9884\u8bad\u7ec3\u548c\u56fa\u5b9a\u7c7b\u522b\u3002", "method": "\u5229\u7528\u8bed\u97f3\u96c6\u6210\u63d0\u793a\u5de5\u7a0bLLM\u3001SAM\u548c\u8ddf\u8e2a\u6a21\u578b\uff0c\u7ed3\u5408\u8bb0\u5fc6\u5e93\u548c\u4e24\u79cd\u65b0\u673a\u5236\uff0c\u5b9e\u73b0\u5b9e\u65f6\u624b\u672f\u8f85\u52a9\u3002", "result": "\u5b9a\u91cf\u5206\u6790\u663e\u793a\u6027\u80fd\u4e0e\u624b\u52a8\u63d0\u793a\u7b56\u7565\u76f8\u5f53\uff0c\u5b9a\u6027\u5206\u6790\u5c55\u793a\u4e86\u5206\u5272\u65b0\u5143\u7d20\u7684\u7075\u6d3b\u6027\u3002", "conclusion": "\u611f\u77e5\u4ee3\u7406\u901a\u8fc7\u81ea\u7136\u4ea4\u4e92\u548c\u514b\u670d\u521a\u6027\uff0c\u4f7fAI\u5728\u52a8\u6001\u624b\u672f\u73af\u5883\u4e2d\u7684\u5b9e\u65f6\u8f85\u52a9\u66f4\u63a5\u8fd1\u73b0\u5b9e\u3002"}}
{"id": "2507.23197", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.23197", "abs": "https://arxiv.org/abs/2507.23197", "authors": ["Yuke Liao", "Blaise Genest", "Kuldeep Meel", "Shaan Aryaman"], "title": "Solution-aware vs global ReLU selection: partial MILP strikes back for DNN verification", "comment": null, "summary": "To handle complex instances, we revisit a divide-and-conquer approach to\nbreak down the complexity: instead of few complex BaB calls, we rely on many\nsmall {\\em partial} MILP calls. The crucial step is to select very few but very\nimportant ReLUs to treat using (costly) binary variables. The previous attempts\nwere suboptimal in that respect. To select these important ReLU variables, we\npropose a novel {\\em solution-aware} ReLU scoring ({\\sf SAS}), as well as adapt\nthe BaB-SR and BaB-FSB branching functions as {\\em global} ReLU scoring ({\\sf\nGS}) functions. We compare them theoretically as well as experimentally, and\n{\\sf SAS} is more efficient at selecting a set of variables to open using\nbinary variables. Compared with previous attempts, SAS reduces the number of\nbinary variables by around 6 times, while maintaining the same level of\naccuracy. Implemented in {\\em Hybrid MILP}, calling first $\\alpha,\\beta$-CROWN\nwith a short time-out to solve easier instances, and then partial MILP,\nproduces a very accurate yet efficient verifier, reducing by up to $40\\%$ the\nnumber of undecided instances to low levels ($8-15\\%$), while keeping a\nreasonable runtime ($46s-417s$ on average per instance), even for fairly large\nCNNs with 2 million parameters.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u611f\u77e5ReLU\u8bc4\u5206\u65b9\u6cd5\uff08SAS\uff09\uff0c\u901a\u8fc7\u9009\u62e9\u5c11\u91cf\u5173\u952eReLU\u53d8\u91cf\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u4e8c\u5143\u53d8\u91cf\u7684\u4f7f\u7528\uff0c\u63d0\u5347\u4e86\u9a8c\u8bc1\u6548\u7387\u3002", "motivation": "\u5904\u7406\u590d\u6742\u5b9e\u4f8b\u65f6\uff0c\u4f20\u7edf\u65b9\u6cd5\u6548\u7387\u4f4e\u4e0b\uff0c\u9700\u8981\u6539\u8fdbReLU\u53d8\u91cf\u7684\u9009\u62e9\u7b56\u7565\u4ee5\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u3002", "method": "\u7ed3\u5408\u4e86\u89e3\u51b3\u65b9\u6848\u611f\u77e5ReLU\u8bc4\u5206\uff08SAS\uff09\u548c\u5168\u5c40ReLU\u8bc4\u5206\uff08GS\uff09\uff0c\u5e76\u91c7\u7528\u6df7\u5408MILP\u65b9\u6cd5\uff0c\u5148\u4f7f\u7528\u03b1,\u03b2-CROWN\u5feb\u901f\u89e3\u51b3\u7b80\u5355\u5b9e\u4f8b\uff0c\u518d\u5904\u7406\u90e8\u5206MILP\u3002", "result": "SAS\u5c06\u4e8c\u5143\u53d8\u91cf\u6570\u91cf\u51cf\u5c11\u7ea66\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\uff1b\u6df7\u5408MILP\u65b9\u6cd5\u5c06\u672a\u89e3\u51b3\u5b9e\u4f8b\u6570\u91cf\u964d\u4f4e40%\uff0c\u8fd0\u884c\u65f6\u95f4\u5408\u7406\u3002", "conclusion": "SAS\u548c\u6df7\u5408MILP\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u9a8c\u8bc1\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u795e\u7ecf\u7f51\u7edc\u3002"}}
{"id": "2507.23172", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.23172", "abs": "https://arxiv.org/abs/2507.23172", "authors": ["Vira Joshi", "Zifan Xu", "Bo Liu", "Peter Stone", "Amy Zhang"], "title": "Benchmarking Massively Parallelized Multi-Task Reinforcement Learning for Robotics Tasks", "comment": "RLC 2025", "summary": "Multi-task Reinforcement Learning (MTRL) has emerged as a critical training\nparadigm for applying reinforcement learning (RL) to a set of complex\nreal-world robotic tasks, which demands a generalizable and robust policy. At\nthe same time, \\emph{massively parallelized training} has gained popularity,\nnot only for significantly accelerating data collection through GPU-accelerated\nsimulation but also for enabling diverse data collection across multiple tasks\nby simulating heterogeneous scenes in parallel. However, existing MTRL research\nhas largely been limited to off-policy methods like SAC in the\nlow-parallelization regime. MTRL could capitalize on the higher asymptotic\nperformance of on-policy algorithms, whose batches require data from the\ncurrent policy, and as a result, take advantage of massive parallelization\noffered by GPU-accelerated simulation. To bridge this gap, we introduce a\nmassively parallelized $\\textbf{M}$ulti-$\\textbf{T}$ask $\\textbf{Bench}$mark\nfor robotics (MTBench), an open-sourced benchmark featuring a broad\ndistribution of 50 manipulation tasks and 20 locomotion tasks, implemented\nusing the GPU-accelerated simulator IsaacGym. MTBench also includes four base\nRL algorithms combined with seven state-of-the-art MTRL algorithms and\narchitectures, providing a unified framework for evaluating their performance.\nOur extensive experiments highlight the superior speed of evaluating MTRL\napproaches using MTBench, while also uncovering unique challenges that arise\nfrom combining massive parallelism with MTRL. Code is available at\n$\\href{https://github.com/Viraj-Joshi/MTBench}{\nhttps://github.com/Viraj-Joshi/MTBench}$", "AI": {"tldr": "MTBench\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u5e76\u884c\u5316\u7684\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u57fa\u51c6\uff0c\u5305\u542b50\u4e2a\u64cd\u4f5c\u4efb\u52a1\u548c20\u4e2a\u8fd0\u52a8\u4efb\u52a1\uff0c\u7528\u4e8e\u8bc4\u4f30MTRL\u7b97\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709MTRL\u7814\u7a76\u4e3b\u8981\u5c40\u9650\u4e8e\u4f4e\u5e76\u884c\u5316\u73af\u5883\u4e0b\u7684\u79bb\u7b56\u7565\u65b9\u6cd5\uff0c\u800c\u5927\u89c4\u6a21\u5e76\u884c\u5316\u53ef\u4ee5\u52a0\u901f\u6570\u636e\u6536\u96c6\u5e76\u63d0\u5347\u6027\u80fd\u3002", "method": "\u4f7f\u7528GPU\u52a0\u901f\u6a21\u62df\u5668IsaacGym\u5b9e\u73b0MTBench\uff0c\u7ed3\u5408\u56db\u79cd\u57fa\u7840RL\u7b97\u6cd5\u548c\u4e03\u79cd\u5148\u8fdbMTRL\u7b97\u6cd5\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u8868\u660eMTBench\u80fd\u9ad8\u6548\u8bc4\u4f30MTRL\u65b9\u6cd5\uff0c\u540c\u65f6\u63ed\u793a\u4e86\u5927\u89c4\u6a21\u5e76\u884c\u5316\u5e26\u6765\u7684\u65b0\u6311\u6218\u3002", "conclusion": "MTBench\u4e3aMTRL\u7814\u7a76\u63d0\u4f9b\u4e86\u7edf\u4e00\u6846\u67b6\uff0c\u5e76\u5c55\u793a\u4e86\u5927\u89c4\u6a21\u5e76\u884c\u5316\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.23276", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.23276", "abs": "https://arxiv.org/abs/2507.23276", "authors": ["Qiujie Xie", "Yixuan Weng", "Minjun Zhu", "Fuchen Shen", "Shulin Huang", "Zhen Lin", "Jiahui Zhou", "Zilan Mao", "Zijie Yang", "Linyi Yang", "Jian Wu", "Yue Zhang"], "title": "How Far Are AI Scientists from Changing the World?", "comment": null, "summary": "The emergence of large language models (LLMs) is propelling automated\nscientific discovery to the next level, with LLM-based Artificial Intelligence\n(AI) Scientist systems now taking the lead in scientific research. Several\ninfluential works have already appeared in the field of AI Scientist systems,\nwith AI-generated research papers having been accepted at the ICLR 2025\nworkshop, suggesting that a human-level AI Scientist capable of uncovering\nphenomena previously unknown to humans, may soon become a reality. In this\nsurvey, we focus on the central question: How far are AI scientists from\nchanging the world and reshaping the scientific research paradigm? To answer\nthis question, we provide a prospect-driven review that comprehensively\nanalyzes the current achievements of AI Scientist systems, identifying key\nbottlenecks and the critical components required for the emergence of a\nscientific agent capable of producing ground-breaking discoveries that solve\ngrand challenges. We hope this survey will contribute to a clearer\nunderstanding of limitations of current AI Scientist systems, showing where we\nare, what is missing, and what the ultimate goals for scientific AI should be.", "AI": {"tldr": "\u7efc\u8ff0\u63a2\u8ba8\u4e86AI\u79d1\u5b66\u5bb6\u7cfb\u7edf\u5728\u63a8\u52a8\u79d1\u5b66\u53d1\u73b0\u4e2d\u7684\u6f5c\u529b\uff0c\u5206\u6790\u4e86\u5f53\u524d\u6210\u5c31\u3001\u74f6\u9888\u53ca\u672a\u6765\u76ee\u6807\u3002", "motivation": "\u8bc4\u4f30AI\u79d1\u5b66\u5bb6\u7cfb\u7edf\u662f\u5426\u80fd\u6539\u53d8\u4e16\u754c\u5e76\u91cd\u5851\u79d1\u7814\u8303\u5f0f\u3002", "method": "\u901a\u8fc7\u524d\u77bb\u6027\u7efc\u8ff0\uff0c\u5168\u9762\u5206\u6790AI\u79d1\u5b66\u5bb6\u7cfb\u7edf\u7684\u73b0\u72b6\u548c\u5173\u952e\u7ec4\u4ef6\u3002", "result": "\u8bc6\u522b\u4e86\u5f53\u524d\u7cfb\u7edf\u7684\u5c40\u9650\u6027\u548c\u5b9e\u73b0\u7a81\u7834\u6027\u53d1\u73b0\u6240\u9700\u7684\u5173\u952e\u8981\u7d20\u3002", "conclusion": "\u5e0c\u671b\u8be5\u7efc\u8ff0\u80fd\u5e2e\u52a9\u660e\u786eAI\u79d1\u5b66\u5bb6\u7cfb\u7edf\u7684\u73b0\u72b6\u3001\u4e0d\u8db3\u53ca\u7ec8\u6781\u76ee\u6807\u3002"}}
{"id": "2507.23203", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.23203", "abs": "https://arxiv.org/abs/2507.23203", "authors": ["Chenghao Wang", "Eric Sihite", "Kaushik Venkatesh Krishnamurthy", "Shreyansh Pitroda", "Adarsh Salagame", "Alireza Ramezani", "Morteza Gharib"], "title": "Quadratic Programming-Based Posture Manipulation and Thrust-vectoring for Agile Dynamic Walking on Narrow Pathways", "comment": null, "summary": "There has been significant advancement in legged robot's agility where they\ncan show impressive acrobatic maneuvers, such as parkour. These maneuvers rely\nheavily on posture manipulation. To expand the stability and locomotion\nplasticity, we use the multi-modal ability in our legged-aerial platform, the\nHusky Beta, to perform thruster-assisted walking. This robot has thrusters on\neach of its sagittal knee joints which can be used to stabilize its frontal\ndynamic as it walks. In this work, we perform a simulation study of quadruped\nnarrow-path walking with Husky $\\beta$, where the robot will utilize its\nthrusters to stably walk on a narrow path. The controller is designed based on\na centroidal dynamics model with thruster and foot ground contact forces as\ninputs. These inputs are regulated using a QP solver to be used in a model\npredictive control framework. In addition to narrow-path walking, we also\nperform a lateral push-recovery simulation to study how the thrusters can be\nused to stabilize the frontal dynamics.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u56db\u8db3\u673a\u5668\u4ebaHusky Beta\u5728\u7a84\u8def\u5f84\u884c\u8d70\u4e2d\u7684\u7a33\u5b9a\u6027\uff0c\u901a\u8fc7\u63a8\u8fdb\u5668\u8f85\u52a9\u63a7\u5236\u5b9e\u73b0\u52a8\u6001\u5e73\u8861\u3002", "motivation": "\u63d0\u5347\u56db\u8db3\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u7a33\u5b9a\u6027\u548c\u8fd0\u52a8\u53ef\u5851\u6027\uff0c\u5c24\u5176\u662f\u7a84\u8def\u5f84\u884c\u8d70\u548c\u52a8\u6001\u5e73\u8861\u3002", "method": "\u57fa\u4e8e\u8d28\u5fc3\u52a8\u529b\u5b66\u6a21\u578b\u8bbe\u8ba1\u63a7\u5236\u5668\uff0c\u7ed3\u5408\u63a8\u8fdb\u5668\u548c\u8db3\u90e8\u63a5\u89e6\u529b\u8f93\u5165\uff0c\u4f7f\u7528QP\u6c42\u89e3\u5668\u548c\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u6846\u67b6\u3002", "result": "\u6a21\u62df\u7814\u7a76\u8868\u660e\uff0c\u63a8\u8fdb\u5668\u8f85\u52a9\u80fd\u6709\u6548\u63d0\u5347\u673a\u5668\u4eba\u5728\u7a84\u8def\u5f84\u884c\u8d70\u548c\u4fa7\u5411\u63a8\u529b\u6062\u590d\u4e2d\u7684\u7a33\u5b9a\u6027\u3002", "conclusion": "\u63a8\u8fdb\u5668\u8f85\u52a9\u63a7\u5236\u662f\u63d0\u5347\u56db\u8db3\u673a\u5668\u4eba\u52a8\u6001\u5e73\u8861\u548c\u590d\u6742\u73af\u5883\u9002\u5e94\u6027\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2507.23330", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.23330", "abs": "https://arxiv.org/abs/2507.23330", "authors": ["Tosin Adewumi", "Lama Alkhaled", "Florent Imbert", "Hui Han", "Nudrat Habib", "Karl L\u00f6wenmark"], "title": "AI Must not be Fully Autonomous", "comment": "11 pages, 1 figure", "summary": "Autonomous Artificial Intelligence (AI) has many benefits. It also has many\nrisks. In this work, we identify the 3 levels of autonomous AI. We are of the\nposition that AI must not be fully autonomous because of the many risks,\nespecially as artificial superintelligence (ASI) is speculated to be just\ndecades away. Fully autonomous AI, which can develop its own objectives, is at\nlevel 3 and without responsible human oversight. However, responsible human\noversight is crucial for mitigating the risks. To ague for our position, we\ndiscuss theories of autonomy, AI and agents. Then, we offer 12 distinct\narguments and 6 counterarguments with rebuttals to the counterarguments. We\nalso present 15 pieces of recent evidence of AI misaligned values and other\nrisks in the appendix.", "AI": {"tldr": "\u8bba\u6587\u4e3b\u5f20AI\u4e0d\u5e94\u5b8c\u5168\u81ea\u4e3b\uff0c\u63d0\u51fa3\u7ea7\u81ea\u4e3bAI\u5206\u7c7b\uff0c\u5f3a\u8c03\u4eba\u7c7b\u76d1\u7763\u7684\u5fc5\u8981\u6027\uff0c\u5e76\u63d0\u4f9b\u7406\u8bba\u548c\u8bc1\u636e\u652f\u6301\u3002", "motivation": "\u63a2\u8ba8AI\u5b8c\u5168\u81ea\u4e3b\u7684\u98ce\u9669\uff0c\u5c24\u5176\u662f\u8d85\u7ea7\u667a\u80fd\uff08ASI\uff09\u53ef\u80fd\u5e26\u6765\u7684\u5a01\u80c1\uff0c\u4e3b\u5f20\u4eba\u7c7b\u76d1\u7763\u7684\u91cd\u8981\u6027\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\uff08\u81ea\u4e3b\u6027\u3001AI\u548c\u667a\u80fd\u4f53\u7406\u8bba\uff09\u300112\u4e2a\u8bba\u70b9\u30016\u4e2a\u53cd\u9a73\u53ca\u5176\u56de\u5e94\uff0c\u4ee5\u53ca15\u4e2aAI\u4ef7\u503c\u9519\u4f4d\u6848\u4f8b\u652f\u6301\u89c2\u70b9\u3002", "result": "\u63d0\u51fa3\u7ea7\u81ea\u4e3bAI\u5206\u7c7b\uff0c\u5f3a\u8c03\u4eba\u7c7b\u76d1\u7763\u5bf9\u98ce\u9669\u7f13\u89e3\u7684\u5173\u952e\u4f5c\u7528\u3002", "conclusion": "AI\u4e0d\u5e94\u5b8c\u5168\u81ea\u4e3b\uff0c\u4eba\u7c7b\u76d1\u7763\u662f\u786e\u4fddAI\u5b89\u5168\u53d1\u5c55\u7684\u5fc5\u8981\u6761\u4ef6\u3002"}}
{"id": "2507.23270", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.23270", "abs": "https://arxiv.org/abs/2507.23270", "authors": ["Loris Schneider", "Marc Ungen", "Elias Huber", "Jan-Felix Klein"], "title": "Simulation-based planning of Motion Sequences for Automated Procedure Optimization in Multi-Robot Assembly Cells", "comment": null, "summary": "Reconfigurable multi-robot cells offer a promising approach to meet\nfluctuating assembly demands. However, the recurrent planning of their\nconfigurations introduces new challenges, particularly in generating optimized,\ncoordinated multi-robot motion sequences that minimize the assembly duration.\nThis work presents a simulation-based method for generating such optimized\nsequences. The approach separates assembly steps into task-related core\noperations and connecting traverse operations. While core operations are\nconstrained and predetermined, traverse operations offer substantial\noptimization potential. Scheduling the core operations is formulated as an\noptimization problem, requiring feasible traverse operations to be integrated\nusing a decomposition-based motion planning strategy. Several solution\ntechniques are explored, including a sampling heuristic, tree-based search and\ngradient-free optimization. For motion planning, a decomposition method is\nproposed that identifies specific areas in the schedule, which can be solved\nindependently with modified centralized path planning algorithms. The proposed\nmethod generates efficient and collision-free multi-robot assembly procedures\nthat outperform a baseline relying on decentralized, robot-individual motion\nplanning. Its effectiveness is demonstrated through simulation experiments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4eff\u771f\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316\u53ef\u91cd\u6784\u591a\u673a\u5668\u4eba\u5355\u5143\u7684\u8fd0\u52a8\u5e8f\u5217\uff0c\u4ee5\u51cf\u5c11\u88c5\u914d\u65f6\u95f4\u3002", "motivation": "\u53ef\u91cd\u6784\u591a\u673a\u5668\u4eba\u5355\u5143\u80fd\u5e94\u5bf9\u6ce2\u52a8\u7684\u88c5\u914d\u9700\u6c42\uff0c\u4f46\u5176\u914d\u7f6e\u7684\u53cd\u590d\u89c4\u5212\u5e26\u6765\u4e86\u65b0\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u5982\u4f55\u751f\u6210\u4f18\u5316\u7684\u3001\u534f\u8c03\u7684\u591a\u673a\u5668\u4eba\u8fd0\u52a8\u5e8f\u5217\u3002", "method": "\u5c06\u88c5\u914d\u6b65\u9aa4\u5206\u4e3a\u4efb\u52a1\u76f8\u5173\u7684\u6838\u5fc3\u64cd\u4f5c\u548c\u8fde\u63a5\u904d\u5386\u64cd\u4f5c\uff0c\u6838\u5fc3\u64cd\u4f5c\u56fa\u5b9a\uff0c\u904d\u5386\u64cd\u4f5c\u53ef\u4f18\u5316\u3002\u91c7\u7528\u5206\u89e3\u5f0f\u8fd0\u52a8\u89c4\u5212\u7b56\u7565\uff0c\u7ed3\u5408\u91c7\u6837\u542f\u53d1\u5f0f\u3001\u6811\u641c\u7d22\u548c\u65e0\u68af\u5ea6\u4f18\u5316\u7b49\u6280\u672f\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u751f\u6210\u4e86\u9ad8\u6548\u4e14\u65e0\u78b0\u649e\u7684\u591a\u673a\u5668\u4eba\u88c5\u914d\u6d41\u7a0b\uff0c\u6027\u80fd\u4f18\u4e8e\u57fa\u4e8e\u5206\u6563\u5f0f\u673a\u5668\u4eba\u4e2a\u4f53\u8fd0\u52a8\u89c4\u5212\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u4eff\u771f\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u4e3a\u591a\u673a\u5668\u4eba\u88c5\u914d\u63d0\u4f9b\u4e86\u4f18\u5316\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.23336", "categories": ["cs.AI", "cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.23336", "abs": "https://arxiv.org/abs/2507.23336", "authors": ["Ram Mohan Rao Kadiyala", "Siddhant Gupta", "Jebish Purbey", "Giulio Martini", "Suman Debnath", "Hamza Farooq"], "title": "DSBC : Data Science task Benchmarking with Context engineering", "comment": "32 pages", "summary": "Recent advances in large language models (LLMs) have significantly impacted\ndata science workflows, giving rise to specialized data science agents designed\nto automate analytical tasks. Despite rapid adoption, systematic benchmarks\nevaluating the efficacy and limitations of these agents remain scarce. In this\npaper, we introduce a comprehensive benchmark specifically crafted to reflect\nreal-world user interactions with data science agents by observing usage of our\ncommercial applications. We evaluate three LLMs: Claude-4.0-Sonnet,\nGemini-2.5-Flash, and OpenAI-o4-Mini across three approaches: zero-shot with\ncontext engineering, multi-step with context engineering, and with SmolAgent.\nOur benchmark assesses performance across a diverse set of eight data science\ntask categories, additionally exploring the sensitivity of models to common\nprompting issues, such as data leakage and slightly ambiguous instructions. We\nfurther investigate the influence of temperature parameters on overall and\ntask-specific outcomes for each model and approach. Our findings reveal\ndistinct performance disparities among the evaluated models and methodologies,\nhighlighting critical factors that affect practical deployment. The benchmark\ndataset and evaluation framework introduced herein aim to provide a foundation\nfor future research of more robust and effective data science agents.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u6570\u636e\u79d1\u5b66\u4ee3\u7406\u7684\u5168\u9762\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u4e86\u4e09\u79cdLLM\u6a21\u578b\u5728\u4e0d\u540c\u65b9\u6cd5\u4e0b\u7684\u8868\u73b0\uff0c\u63ed\u793a\u4e86\u6027\u80fd\u5dee\u5f02\u548c\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u5173\u952e\u56e0\u7d20\u3002", "motivation": "\u5c3d\u7ba1\u6570\u636e\u79d1\u5b66\u4ee3\u7406\u5728\u81ea\u52a8\u5316\u5206\u6790\u4efb\u52a1\u4e2d\u8fc5\u901f\u666e\u53ca\uff0c\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u6027\u57fa\u51c6\u8bc4\u4f30\u5176\u6548\u80fd\u548c\u5c40\u9650\u6027\u3002", "method": "\u901a\u8fc7\u5546\u4e1a\u5e94\u7528\u89c2\u5bdf\u7528\u6237\u4ea4\u4e92\uff0c\u8bbe\u8ba1\u4e86\u53cd\u6620\u771f\u5b9e\u573a\u666f\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u4e86\u4e09\u79cdLLM\u6a21\u578b\uff08Claude-4.0-Sonnet\u3001Gemini-2.5-Flash\u3001OpenAI-o4-Mini\uff09\u5728\u4e09\u79cd\u65b9\u6cd5\uff08\u96f6\u6837\u672c\u4e0a\u4e0b\u6587\u5de5\u7a0b\u3001\u591a\u6b65\u4e0a\u4e0b\u6587\u5de5\u7a0b\u3001SmolAgent\uff09\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4e0d\u540c\u6a21\u578b\u548c\u65b9\u6cd5\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u6027\u80fd\u5dee\u5f02\uff0c\u5e76\u63a2\u8ba8\u4e86\u6e29\u5ea6\u53c2\u6570\u5bf9\u7ed3\u679c\u7684\u5f71\u54cd\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u51c6\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6846\u67b6\u4e3a\u672a\u6765\u7814\u7a76\u66f4\u9c81\u68d2\u548c\u9ad8\u6548\u7684\u6570\u636e\u79d1\u5b66\u4ee3\u7406\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.23273", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23273", "abs": "https://arxiv.org/abs/2507.23273", "authors": ["Jaeseok Park", "Chanoh Park", "Minsu Kim", "Soohwan Kim"], "title": "GSFusion:Globally Optimized LiDAR-Inertial-Visual Mapping for Gaussian Splatting", "comment": null, "summary": "While 3D Gaussian Splatting (3DGS) has revolutionized photorealistic mapping,\nconventional approaches based on camera sensor, even RGB-D, suffer from\nfundamental limitations such as high computational load, failure in\nenvironments with poor texture or illumination, and short operational ranges.\nLiDAR emerges as a robust alternative, but its integration with 3DGS introduces\nnew challenges, such as the need for exceptional global alignment for\nphotorealistic quality and prolonged optimization times caused by sparse data.\nTo address these challenges, we propose GSFusion, an online\nLiDAR-Inertial-Visual mapping system that ensures high-precision map\nconsistency through a surfel-to-surfel constraint in the global pose-graph\noptimization. To handle sparse data, our system employs a pixel-aware Gaussian\ninitialization strategy for efficient representation and a bounded sigmoid\nconstraint to prevent uncontrolled Gaussian growth. Experiments on public and\nour datasets demonstrate our system outperforms existing 3DGS SLAM systems in\nterms of rendering quality and map-building efficiency.", "AI": {"tldr": "GSFusion\u662f\u4e00\u79cd\u7ed3\u5408LiDAR\u3001\u60ef\u6027\u548c\u89c6\u89c9\u7684\u5728\u7ebf\u6620\u5c04\u7cfb\u7edf\uff0c\u89e3\u51b3\u4e863D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u5728\u7a00\u758f\u6570\u636e\u548c\u5168\u5c40\u5bf9\u9f50\u65b9\u9762\u7684\u6311\u6218\uff0c\u63d0\u5347\u4e86\u6e32\u67d3\u8d28\u91cf\u548c\u5efa\u56fe\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u76f8\u673a\u4f20\u611f\u5668\uff08\u5305\u62ecRGB-D\uff09\u76843DGS\u65b9\u6cd5\u5b58\u5728\u8ba1\u7b97\u8d1f\u8f7d\u9ad8\u3001\u5728\u4f4e\u7eb9\u7406\u6216\u5149\u7167\u5dee\u73af\u5883\u4e2d\u5931\u6548\u3001\u64cd\u4f5c\u8303\u56f4\u77ed\u7b49\u95ee\u9898\uff0c\u800cLiDAR\u867d\u4e3a\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u5176\u4e0e3DGS\u7ed3\u5408\u65f6\u9762\u4e34\u5168\u5c40\u5bf9\u9f50\u548c\u9ad8\u4f18\u5316\u65f6\u95f4\u7684\u6311\u6218\u3002", "method": "GSFusion\u901a\u8fc7\u5168\u5c40\u4f4d\u59ff\u56fe\u4f18\u5316\u4e2d\u7684surfel-to-surfel\u7ea6\u675f\u786e\u4fdd\u9ad8\u7cbe\u5ea6\u5730\u56fe\u4e00\u81f4\u6027\uff0c\u91c7\u7528\u50cf\u7d20\u611f\u77e5\u7684\u9ad8\u65af\u521d\u59cb\u5316\u7b56\u7565\u5904\u7406\u7a00\u758f\u6570\u636e\uff0c\u5e76\u4f7f\u7528\u6709\u754csigmoid\u7ea6\u675f\u9632\u6b62\u9ad8\u65af\u589e\u957f\u5931\u63a7\u3002", "result": "\u5728\u516c\u5f00\u548c\u81ea\u6709\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGSFusion\u5728\u6e32\u67d3\u8d28\u91cf\u548c\u5efa\u56fe\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u67093DGS SLAM\u7cfb\u7edf\u3002", "conclusion": "GSFusion\u901a\u8fc7\u878d\u5408LiDAR\u3001\u60ef\u6027\u548c\u89c6\u89c9\u6570\u636e\uff0c\u6709\u6548\u89e3\u51b3\u4e863DGS\u5728\u7a00\u758f\u6570\u636e\u548c\u5168\u5c40\u5bf9\u9f50\u4e2d\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u5b9e\u65f6\u6620\u5c04\u3002"}}
{"id": "2507.23377", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.23377", "abs": "https://arxiv.org/abs/2507.23377", "authors": ["Zhuo Li", "Xianghuai Deng", "Chiwei Feng", "Hanmeng Li", "Shenjie Wang", "Haichao Zhang", "Teng Jia", "Conlin Chen", "Louis Linchun Wu", "Jia Wang"], "title": "LLM4Rail: An LLM-Augmented Railway Service Consulting Platform", "comment": null, "summary": "Large language models (LLMs) have significantly reshaped different walks of\nbusiness. To meet the increasing demands for individualized railway service, we\ndevelop LLM4Rail - a novel LLM-augmented railway service consulting platform.\nEmpowered by LLM, LLM4Rail can provide custom modules for ticketing, railway\nfood & drink recommendations, weather information, and chitchat. In LLM4Rail,\nwe propose the iterative \"Question-Thought-Action-Observation (QTAO)\" prompting\nframework. It meticulously integrates verbal reasoning with task-oriented\nactions, that is, reasoning to guide action selection, to effectively retrieve\nexternal observations relevant to railway operation and service to generate\naccurate responses. To provide personalized onboard dining services, we first\nconstruct the Chinese Railway Food and Drink (CRFD-25) - a publicly accessible\ntakeout dataset tailored for railway services. CRFD-25 covers a wide range of\nsignature dishes categorized by cities, cuisines, age groups, and spiciness\nlevels. We further introduce an LLM-based zero-shot conversational recommender\nfor railway catering. To address the unconstrained nature of open\nrecommendations, the feature similarity-based post-processing step is\nintroduced to ensure all the recommended items are aligned with CRFD-25\ndataset.", "AI": {"tldr": "LLM4Rail\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u94c1\u8def\u670d\u52a1\u5e73\u53f0\uff0c\u901a\u8fc7QTAO\u63d0\u793a\u6846\u67b6\u548cCRFD-25\u6570\u636e\u96c6\uff0c\u63d0\u4f9b\u4e2a\u6027\u5316\u94c1\u8def\u670d\u52a1\u3002", "motivation": "\u6ee1\u8db3\u65e5\u76ca\u589e\u957f\u7684\u4e2a\u6027\u5316\u94c1\u8def\u670d\u52a1\u9700\u6c42\u3002", "method": "\u63d0\u51faQTAO\u63d0\u793a\u6846\u67b6\uff0c\u7ed3\u5408\u8bed\u8a00\u63a8\u7406\u4e0e\u4efb\u52a1\u5bfc\u5411\u884c\u52a8\uff0c\u5e76\u6784\u5efaCRFD-25\u6570\u636e\u96c6\u652f\u6301\u4e2a\u6027\u5316\u9910\u996e\u63a8\u8350\u3002", "result": "\u5f00\u53d1\u4e86LLM4Rail\u5e73\u53f0\uff0c\u80fd\u591f\u63d0\u4f9b\u7cbe\u51c6\u7684\u94c1\u8def\u670d\u52a1\u54a8\u8be2\u548c\u9910\u996e\u63a8\u8350\u3002", "conclusion": "LLM4Rail\u901a\u8fc7LLM\u548cQTAO\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u94c1\u8def\u670d\u52a1\u7684\u4e2a\u6027\u5316\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2507.23305", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.23305", "abs": "https://arxiv.org/abs/2507.23305", "authors": ["Yixuan Dang", "Qinyang Xu", "Yu Zhang", "Xiangtong Yao", "Liding Zhang", "Zhenshan Bing", "Florian Roehrbein", "Alois Knoll"], "title": "Whisker-based Active Tactile Perception for Contour Reconstruction", "comment": null, "summary": "Perception using whisker-inspired tactile sensors currently faces a major\nchallenge: the lack of active control in robots based on direct contact\ninformation from the whisker. To accurately reconstruct object contours, it is\ncrucial for the whisker sensor to continuously follow and maintain an\nappropriate relative touch pose on the surface. This is especially important\nfor localization based on tip contact, which has a low tolerance for sharp\nsurfaces and must avoid slipping into tangential contact. In this paper, we\nfirst construct a magnetically transduced whisker sensor featuring a compact\nand robust suspension system composed of three flexible spiral arms. We develop\na method that leverages a characterized whisker deflection profile to directly\nextract the tip contact position using gradient descent, with a Bayesian filter\napplied to reduce fluctuations. We then propose an active motion control policy\nto maintain the optimal relative pose of the whisker sensor against the object\nsurface. A B-Spline curve is employed to predict the local surface curvature\nand determine the sensor orientation. Results demonstrate that our algorithm\ncan effectively track objects and reconstruct contours with sub-millimeter\naccuracy. Finally, we validate the method in simulations and real-world\nexperiments where a robot arm drives the whisker sensor to follow the surfaces\nof three different objects.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u78c1\u611f\u5e94\u7684\u89e6\u987b\u4f20\u611f\u5668\u53ca\u5176\u4e3b\u52a8\u63a7\u5236\u7b56\u7565\uff0c\u7528\u4e8e\u7cbe\u786e\u8ddf\u8e2a\u7269\u4f53\u8f6e\u5ed3\u5e76\u91cd\u5efa\u8868\u9762\u5f62\u72b6\u3002", "motivation": "\u89e6\u987b\u4f20\u611f\u5668\u5728\u673a\u5668\u4eba\u611f\u77e5\u4e2d\u7f3a\u4e4f\u57fa\u4e8e\u76f4\u63a5\u63a5\u89e6\u4fe1\u606f\u7684\u4e3b\u52a8\u63a7\u5236\uff0c\u5bfc\u81f4\u96be\u4ee5\u7cbe\u786e\u91cd\u5efa\u7269\u4f53\u8f6e\u5ed3\u3002", "method": "\u8bbe\u8ba1\u4e86\u78c1\u611f\u5e94\u89e6\u987b\u4f20\u611f\u5668\uff0c\u5229\u7528\u68af\u5ea6\u4e0b\u964d\u548c\u8d1d\u53f6\u65af\u6ee4\u6ce2\u63d0\u53d6\u63a5\u89e6\u70b9\u4f4d\u7f6e\uff0c\u5e76\u901a\u8fc7B\u6837\u6761\u66f2\u7ebf\u9884\u6d4b\u8868\u9762\u66f2\u7387\u4ee5\u4f18\u5316\u4f20\u611f\u5668\u59ff\u6001\u3002", "result": "\u7b97\u6cd5\u80fd\u6709\u6548\u8ddf\u8e2a\u7269\u4f53\u5e76\u5b9e\u73b0\u4e9a\u6beb\u7c73\u7ea7\u7cbe\u5ea6\u7684\u8f6e\u5ed3\u91cd\u5efa\u3002", "conclusion": "\u901a\u8fc7\u4eff\u771f\u548c\u5b9e\u7269\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u9002\u7528\u4e8e\u673a\u5668\u4eba\u89e6\u89c9\u611f\u77e5\u4efb\u52a1\u3002"}}
{"id": "2507.23429", "categories": ["cs.AI", "cs.DB", "cs.ET", "cs.HC", "cs.MA", "68T50, 68P20", "I.2.7; H.2.5; H.2.8; H.5.m"], "pdf": "https://arxiv.org/pdf/2507.23429", "abs": "https://arxiv.org/abs/2507.23429", "authors": ["Jorge Ruiz G\u00f3mez", "Lidia Andr\u00e9s Susinos", "Jorge Alamo Oliv\u00e9", "Sonia Rey Osorno", "Manuel Luis Gonzalez Hern\u00e1ndez"], "title": "Chatting with your ERP: A Recipe", "comment": "11 pages, includes 3 tables summarizing schema and model performance.\n  Submitted on July 31, 2025. Targets integration of LLM agents with ERP\n  systems using open-weight models and Ollama deployment", "summary": "This paper presents the design, implementation, and evaluation behind a Large\nLanguage Model (LLM) agent that chats with an industrial production-grade ERP\nsystem. The agent is capable of interpreting natural language queries and\ntranslating them into executable SQL statements, leveraging open-weight LLMs. A\nnovel dual-agent architecture combining reasoning and critique stages was\nproposed to improve query generation reliability.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u4ee3\u7406\uff0c\u7528\u4e8e\u4e0e\u5de5\u4e1a\u7ea7ERP\u7cfb\u7edf\u4ea4\u4e92\uff0c\u901a\u8fc7\u53cc\u4ee3\u7406\u67b6\u6784\u63d0\u5347SQL\u67e5\u8be2\u751f\u6210\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u89e3\u51b3\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u8f6c\u6362\u4e3a\u53ef\u6267\u884cSQL\u8bed\u53e5\u7684\u53ef\u9760\u6027\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u5de5\u4e1a\u7ea7ERP\u7cfb\u7edf\u3002", "method": "\u91c7\u7528\u53cc\u4ee3\u7406\u67b6\u6784\uff0c\u7ed3\u5408\u63a8\u7406\u548c\u6279\u5224\u9636\u6bb5\uff0c\u5229\u7528\u5f00\u6e90\u6743\u91cdLLM\u751f\u6210SQL\u67e5\u8be2\u3002", "result": "\u4ee3\u7406\u80fd\u591f\u53ef\u9760\u5730\u5c06\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u8f6c\u6362\u4e3aSQL\u8bed\u53e5\uff0c\u9002\u7528\u4e8e\u5de5\u4e1a\u73af\u5883\u3002", "conclusion": "\u53cc\u4ee3\u7406\u67b6\u6784\u663e\u8457\u63d0\u5347\u4e86\u67e5\u8be2\u751f\u6210\u7684\u53ef\u9760\u6027\uff0c\u4e3aLLM\u5728\u5de5\u4e1a\u5e94\u7528\u4e2d\u7684\u843d\u5730\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2507.23324", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.23324", "abs": "https://arxiv.org/abs/2507.23324", "authors": ["Lucas Elbert Suryana", "Saeed Rahmani", "Simeon Craig Calvert", "Arkady Zgonnikov", "Bart van Arem"], "title": "Assessing the Alignment of Automated Vehicle Decisions with Human Reasons", "comment": "This version incorporates revisions based on peer-review feedback\n  from a prior submission. The work has not yet been accepted and is being\n  prepared for resubmission", "summary": "A key challenge in deploying automated vehicles (AVs) is ensuring they make\nappropriate decisions in ethically challenging everyday driving situations.\nWhile much attention has been paid to rare, high-stakes dilemmas such as\ntrolley problems, similar tensions also arise in routine scenarios, such as\nnavigating empty intersections, where multiple human considerations, including\nlegality and comfort, often conflict. Current AV planning systems typically\nrely on rigid rules, which struggle to balance these competing considerations\nand can lead to behaviour that misaligns with human expectations. This paper\nproposes a novel reasons-based trajectory evaluation framework that\noperationalises the tracking condition of Meaningful Human Control (MHC). The\nframework models the reasons of human agents, such as regulatory compliance, as\nquantifiable functions and evaluates how well candidate AV trajectories align\nwith these reasons. By assigning adjustable weights to agent priorities and\nintegrating a balance function to discourage the exclusion of any agent, the\nframework supports interpretable decision evaluation. Through a\nreal-world-inspired overtaking scenario, we show how this approach reveals\ntensions, for instance between regulatory compliance, efficiency, and comfort.\nThe framework functions as a modular evaluation layer over existing planning\nalgorithms. It offers a transparent tool for assessing ethical alignment in\neveryday scenarios and provides a practical step toward implementing MHC in\nreal-world AV deployment.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u539f\u56e0\u7684\u8f68\u8ff9\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u4f26\u7406\u6311\u6218\u573a\u666f\u4e2d\u7684\u51b3\u7b56\u95ee\u9898\uff0c\u901a\u8fc7\u91cf\u5316\u4eba\u7c7b\u4ee3\u7406\u7684\u539f\u56e0\uff08\u5982\u6cd5\u89c4\u9075\u4ece\u6027\uff09\u6765\u8bc4\u4f30\u5019\u9009\u8f68\u8ff9\u7684\u5339\u914d\u5ea6\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u5e38\u89c4\u9a7e\u9a76\u573a\u666f\u4e2d\u5e38\u9762\u4e34\u4f26\u7406\u51b2\u7a81\uff08\u5982\u6cd5\u89c4\u4e0e\u8212\u9002\u6027\u7684\u6743\u8861\uff09\uff0c\u73b0\u6709\u89c4\u5212\u7cfb\u7edf\u4f9d\u8d56\u521a\u6027\u89c4\u5219\uff0c\u96be\u4ee5\u5e73\u8861\u8fd9\u4e9b\u51b2\u7a81\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u539f\u56e0\u7684\u8f68\u8ff9\u8bc4\u4f30\u6846\u67b6\uff0c\u91cf\u5316\u4eba\u7c7b\u4ee3\u7406\u7684\u539f\u56e0\uff0c\u5e76\u901a\u8fc7\u53ef\u8c03\u6743\u91cd\u548c\u5e73\u8861\u51fd\u6570\u8bc4\u4f30\u8f68\u8ff9\u7684\u4f26\u7406\u5bf9\u9f50\u6027\u3002", "result": "\u901a\u8fc7\u771f\u5b9e\u4e16\u754c\u542f\u53d1\u7684\u8d85\u8f66\u573a\u666f\uff0c\u5c55\u793a\u4e86\u6846\u67b6\u5982\u4f55\u63ed\u793a\u6cd5\u89c4\u3001\u6548\u7387\u548c\u8212\u9002\u6027\u4e4b\u95f4\u7684\u51b2\u7a81\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8bc4\u4f30\u65e5\u5e38\u573a\u666f\u4e2d\u7684\u4f26\u7406\u5bf9\u9f50\u63d0\u4f9b\u4e86\u900f\u660e\u5de5\u5177\uff0c\u662f\u5b9e\u73b0\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u6709\u610f\u4e49\u4eba\u7c7b\u63a7\u5236\uff08MHC\uff09\u7684\u5b9e\u7528\u6b65\u9aa4\u3002"}}
{"id": "2507.23440", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.23440", "abs": "https://arxiv.org/abs/2507.23440", "authors": ["Mingzhe Li", "Xin Lu", "Yanyan Zhao"], "title": "Self-Foveate: Enhancing Diversity and Difficulty of Synthesized Instructions from Unsupervised Text via Multi-Level Foveation", "comment": "Accepted by Findings of ACL 2025", "summary": "Large language models (LLMs) with instruction following capabilities have\ndemonstrated impressive problem-solving abilities. While synthesizing\ninstructional data from unsupervised text has become a common approach for\ntraining such models, conventional methods rely heavily on human effort for\ndata annotation. Although existing automated synthesis paradigms have\nalleviated this constraint, they still exhibit significant limitations in\nensuring adequate diversity and difficulty of synthesized instructions. To\naddress these challenges, we propose Self-Foveate, an innovative LLM-driven\nmethod for instruction synthesis. This approach introduces a\n\"Micro-Scatter-Macro\" multi-level foveation methodology that effectively guides\nthe LLM to deeply excavate fine-grained information embedded in unsupervised\ntext, thereby enhancing both the diversity and difficulty of synthesized\ninstructions. Comprehensive experiments across multiple unsupervised corpora\nand diverse model architectures validate the effectiveness and superiority of\nour proposed method. We publicly release our data and codes:\nhttps://github.com/Mubuky/Self-Foveate", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSelf-Foveate\u7684\u521b\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u7ea7\u805a\u7126\u6280\u672f\u63d0\u5347\u6307\u4ee4\u5408\u6210\u7684\u591a\u6837\u6027\u548c\u96be\u5ea6\uff0c\u51cf\u5c11\u5bf9\u4eba\u529b\u7684\u4f9d\u8d56\u3002", "motivation": "\u4f20\u7edf\u6307\u4ee4\u5408\u6210\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u6807\u6ce8\u4e14\u591a\u6837\u6027\u4e0d\u8db3\uff0c\u73b0\u6709\u81ea\u52a8\u5316\u65b9\u6cd5\u5728\u591a\u6837\u6027\u548c\u96be\u5ea6\u4e0a\u4ecd\u6709\u5c40\u9650\u3002", "method": "\u91c7\u7528\u201cMicro-Scatter-Macro\u201d\u591a\u7ea7\u805a\u7126\u6280\u672f\uff0c\u6307\u5bfcLLM\u4ece\u65e0\u76d1\u7763\u6587\u672c\u4e2d\u6316\u6398\u7ec6\u7c92\u5ea6\u4fe1\u606f\u3002", "result": "\u5728\u591a\u4e2a\u65e0\u76d1\u7763\u8bed\u6599\u5e93\u548c\u6a21\u578b\u67b6\u6784\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u4f18\u8d8a\u6027\u3002", "conclusion": "Self-Foveate\u663e\u8457\u63d0\u5347\u4e86\u6307\u4ee4\u5408\u6210\u7684\u591a\u6837\u6027\u548c\u96be\u5ea6\uff0c\u4e3aLLM\u8bad\u7ec3\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.23339", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.23339", "abs": "https://arxiv.org/abs/2507.23339", "authors": ["Yihan Zhou", "Yiwen Lu", "Bo Yang", "Jiayun Li", "Yilin Mo"], "title": "Learning to Drift with Individual Wheel Drive: Maneuvering Autonomous Vehicle at the Handling Limits", "comment": null, "summary": "Drifting, characterized by controlled vehicle motion at high sideslip angles,\nis crucial for safely handling emergency scenarios at the friction limits.\nWhile recent reinforcement learning approaches show promise for drifting\ncontrol, they struggle with the significant simulation-to-reality gap, as\npolicies that perform well in simulation often fail when transferred to\nphysical systems. In this paper, we present a reinforcement learning framework\nwith GPU-accelerated parallel simulation and systematic domain randomization\nthat effectively bridges the gap. The proposed approach is validated on both\nsimulation and a custom-designed and open-sourced 1/10 scale Individual Wheel\nDrive (IWD) RC car platform featuring independent wheel speed control.\nExperiments across various scenarios from steady-state circular drifting to\ndirection transitions and variable-curvature path following demonstrate that\nour approach achieves precise trajectory tracking while maintaining controlled\nsideslip angles throughout complex maneuvers in both simulated and real-world\nenvironments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u901a\u8fc7GPU\u52a0\u901f\u5e76\u884c\u4eff\u771f\u548c\u7cfb\u7edf\u9886\u57df\u968f\u673a\u5316\uff0c\u89e3\u51b3\u4e86\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u5dee\u8ddd\uff0c\u5b9e\u73b0\u4e86\u7cbe\u786e\u7684\u6f02\u79fb\u63a7\u5236\u3002", "motivation": "\u6f02\u79fb\u63a7\u5236\u5728\u7d27\u6025\u60c5\u51b5\u4e0b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u4eff\u771f\u5230\u73b0\u5b9e\u8f6c\u79fb\u65f6\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u91c7\u7528GPU\u52a0\u901f\u5e76\u884c\u4eff\u771f\u548c\u7cfb\u7edf\u9886\u57df\u968f\u673a\u5316\uff0c\u7ed3\u5408\u81ea\u5b9a\u4e49\u76841/10\u6bd4\u4f8b\u72ec\u7acb\u8f6e\u9a71\u52a8RC\u8f66\u5e73\u53f0\u3002", "result": "\u5728\u591a\u79cd\u573a\u666f\u4e0b\u5b9e\u73b0\u4e86\u7cbe\u786e\u7684\u8f68\u8ff9\u8ddf\u8e2a\u548c\u53ef\u63a7\u7684\u4fa7\u6ed1\u89d2\uff0c\u4eff\u771f\u548c\u73b0\u5b9e\u73af\u5883\u5747\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u5dee\u8ddd\uff0c\u4e3a\u6f02\u79fb\u63a7\u5236\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.23488", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.23488", "abs": "https://arxiv.org/abs/2507.23488", "authors": ["Kacper Kadziolka", "Saber Salehkaleybar"], "title": "Causal Reasoning in Pieces: Modular In-Context Learning for Causal Discovery", "comment": null, "summary": "Causal inference remains a fundamental challenge for large language models.\nRecent advances in internal reasoning with large language models have sparked\ninterest in whether state-of-the-art reasoning models can robustly perform\ncausal discovery-a task where conventional models often suffer from severe\noverfitting and near-random performance under data perturbations. We study\ncausal discovery on the Corr2Cause benchmark using the emergent OpenAI's\no-series and DeepSeek-R model families and find that these reasoning-first\narchitectures achieve significantly greater native gains than prior approaches.\nTo capitalize on these strengths, we introduce a modular in-context pipeline\ninspired by the Tree-of-Thoughts and Chain-of-Thoughts methodologies, yielding\nnearly three-fold improvements over conventional baselines. We further probe\nthe pipeline's impact by analyzing reasoning chain length, complexity, and\nconducting qualitative and quantitative comparisons between conventional and\nreasoning models. Our findings suggest that while advanced reasoning models\nrepresent a substantial leap forward, carefully structured in-context\nframeworks are essential to maximize their capabilities and offer a\ngeneralizable blueprint for causal discovery across diverse domains.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u57fa\u4e8e\u63a8\u7406\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u56e0\u679c\u53d1\u73b0\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u4e0a\u4e0b\u6587\u7ba1\u9053\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u63a2\u7d22\u5927\u8bed\u8a00\u6a21\u578b\u5728\u56e0\u679c\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u89e3\u51b3\u4f20\u7edf\u6a21\u578b\u5728\u6570\u636e\u6270\u52a8\u4e0b\u7684\u8fc7\u62df\u5408\u548c\u6027\u80fd\u968f\u673a\u6027\u95ee\u9898\u3002", "method": "\u4f7f\u7528OpenAI\u7684o\u7cfb\u5217\u548cDeepSeek-R\u6a21\u578b\uff0c\u7ed3\u5408Tree-of-Thoughts\u548cChain-of-Thoughts\u65b9\u6cd5\uff0c\u8bbe\u8ba1\u6a21\u5757\u5316\u4e0a\u4e0b\u6587\u7ba1\u9053\u3002", "result": "\u63a8\u7406\u6a21\u578b\u5728Corr2Cause\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u6a21\u5757\u5316\u7ba1\u9053\u5e26\u6765\u8fd1\u4e09\u500d\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u9ad8\u7ea7\u63a8\u7406\u6a21\u578b\u5728\u56e0\u679c\u53d1\u73b0\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u4f46\u9700\u7ed3\u5408\u7ed3\u6784\u5316\u4e0a\u4e0b\u6587\u6846\u67b6\u4ee5\u6700\u5927\u5316\u5176\u6f5c\u529b\uff0c\u4e3a\u8de8\u9886\u57df\u5e94\u7528\u63d0\u4f9b\u901a\u7528\u84dd\u56fe\u3002"}}
{"id": "2507.23350", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.23350", "abs": "https://arxiv.org/abs/2507.23350", "authors": ["Mahmoud Ghorab", "Matthias Lorenzen"], "title": "Multi-Waypoint Path Planning and Motion Control for Non-holonomic Mobile Robots in Agricultural Applications", "comment": "6 pages", "summary": "There is a growing demand for autonomous mobile robots capable of navigating\nunstructured agricultural environments. Tasks such as weed control in meadows\nrequire efficient path planning through an unordered set of coordinates while\nminimizing travel distance and adhering to curvature constraints to prevent\nsoil damage and protect vegetation. This paper presents an integrated\nnavigation framework combining a global path planner based on the Dubins\nTraveling Salesman Problem (DTSP) with a Nonlinear Model Predictive Control\n(NMPC) strategy for local path planning and control. The DTSP generates a\nminimum-length, curvature-constrained path that efficiently visits all targets,\nwhile the NMPC leverages this path to compute control signals to accurately\nreach each waypoint. The system's performance was validated through comparative\nsimulation analysis on real-world field datasets, demonstrating that the\ncoupled DTSP-based planner produced smoother and shorter paths, with a\nreduction of about 16% in the provided scenario, compared to decoupled methods.\nBased thereon, the NMPC controller effectively steered the robot to the desired\nwaypoints, while locally optimizing the trajectory and ensuring adherence to\nconstraints. These findings demonstrate the potential of the proposed framework\nfor efficient autonomous navigation in agricultural environments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408DTSP\u5168\u5c40\u8def\u5f84\u89c4\u5212\u548cNMPC\u5c40\u90e8\u63a7\u5236\u7684\u5bfc\u822a\u6846\u67b6\uff0c\u7528\u4e8e\u519c\u4e1a\u73af\u5883\u4e2d\u81ea\u4e3b\u79fb\u52a8\u673a\u5668\u4eba\u7684\u9ad8\u6548\u5bfc\u822a\u3002", "motivation": "\u519c\u4e1a\u73af\u5883\u4e2d\u9700\u8981\u81ea\u4e3b\u79fb\u52a8\u673a\u5668\u4eba\u9ad8\u6548\u5bfc\u822a\uff0c\u540c\u65f6\u6ee1\u8db3\u8def\u5f84\u6700\u77ed\u548c\u66f2\u7387\u7ea6\u675f\u4ee5\u907f\u514d\u571f\u58e4\u548c\u690d\u88ab\u7834\u574f\u3002", "method": "\u7ed3\u5408Dubins\u65c5\u884c\u5546\u95ee\u9898\uff08DTSP\uff09\u7684\u5168\u5c40\u8def\u5f84\u89c4\u5212\u548c\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08NMPC\uff09\u7684\u5c40\u90e8\u8def\u5f84\u89c4\u5212\u4e0e\u63a7\u5236\u3002", "result": "\u4eff\u771f\u5206\u6790\u663e\u793a\uff0c\u8be5\u6846\u67b6\u751f\u6210\u7684\u8def\u5f84\u66f4\u5e73\u6ed1\u3001\u66f4\u77ed\uff0c\u6bd4\u89e3\u8026\u65b9\u6cd5\u51cf\u5c11\u4e86\u7ea616%\u7684\u8def\u5f84\u957f\u5ea6\uff0cNMPC\u63a7\u5236\u5668\u80fd\u51c6\u786e\u5f15\u5bfc\u673a\u5668\u4eba\u5230\u8fbe\u76ee\u6807\u70b9\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u519c\u4e1a\u73af\u5883\u4e2d\u5177\u6709\u9ad8\u6548\u81ea\u4e3b\u5bfc\u822a\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.23497", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23497", "abs": "https://arxiv.org/abs/2507.23497", "authors": ["David A Kelly", "Hana Chockler"], "title": "Causal Identification of Sufficient, Contrastive and Complete Feature Sets in Image Classification", "comment": "13 pages, 13 figures, appendix included", "summary": "Existing algorithms for explaining the outputs of image classifiers are based\non a variety of approaches and produce explanations that lack formal rigor. On\nthe other hand, logic-based explanations are formally and rigorously defined\nbut their computability relies on strict assumptions about the model that do\nnot hold on image classifiers.\n  In this paper, we show that causal explanations, in addition to being\nformally and rigorously defined, enjoy the same formal properties as\nlogic-based ones, while still lending themselves to black-box algorithms and\nbeing a natural fit for image classifiers. We prove formal properties of causal\nexplanations and introduce contrastive causal explanations for image\nclassifiers. Moreover, we augment the definition of explanation with confidence\nawareness and introduce complete causal explanations: explanations that are\nclassified with exactly the same confidence as the original image.\n  We implement our definitions, and our experimental results demonstrate that\ndifferent models have different patterns of sufficiency, contrastiveness, and\ncompleteness. Our algorithms are efficiently computable, taking on average 6s\nper image on a ResNet50 model to compute all types of explanations, and are\ntotally black-box, needing no knowledge of the model, no access to model\ninternals, no access to gradient, nor requiring any properties, such as\nmonotonicity, of the model.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u56e0\u679c\u89e3\u91ca\u65b9\u6cd5\uff0c\u5f25\u8865\u903b\u8f91\u89e3\u91ca\u5728\u56fe\u50cf\u5206\u7c7b\u5668\u4e2d\u7684\u4e0d\u8db3\uff0c\u5177\u6709\u5f62\u5f0f\u5316\u4e25\u8c28\u6027\u4e14\u9002\u7528\u4e8e\u9ed1\u76d2\u7b97\u6cd5\u3002", "motivation": "\u73b0\u6709\u56fe\u50cf\u5206\u7c7b\u5668\u89e3\u91ca\u65b9\u6cd5\u7f3a\u4e4f\u5f62\u5f0f\u5316\u4e25\u8c28\u6027\uff0c\u800c\u903b\u8f91\u89e3\u91ca\u867d\u4e25\u8c28\u4f46\u5047\u8bbe\u6761\u4ef6\u4e25\u683c\uff0c\u4e0d\u9002\u7528\u4e8e\u56fe\u50cf\u5206\u7c7b\u5668\u3002", "method": "\u63d0\u51fa\u56e0\u679c\u89e3\u91ca\u65b9\u6cd5\uff0c\u8bc1\u660e\u5176\u5f62\u5f0f\u5316\u6027\u8d28\uff0c\u5f15\u5165\u5bf9\u6bd4\u6027\u56e0\u679c\u89e3\u91ca\u548c\u7f6e\u4fe1\u5ea6\u611f\u77e5\u7684\u5b8c\u6574\u56e0\u679c\u89e3\u91ca\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u4e0d\u540c\u6a21\u578b\u5728\u5145\u5206\u6027\u3001\u5bf9\u6bd4\u6027\u548c\u5b8c\u6574\u6027\u4e0a\u8868\u73b0\u4e0d\u540c\uff0c\u7b97\u6cd5\u9ad8\u6548\u4e14\u5b8c\u5168\u9ed1\u76d2\u3002", "conclusion": "\u56e0\u679c\u89e3\u91ca\u517c\u5177\u5f62\u5f0f\u5316\u4e25\u8c28\u6027\u548c\u5b9e\u7528\u6027\uff0c\u9002\u7528\u4e8e\u56fe\u50cf\u5206\u7c7b\u5668\u3002"}}
{"id": "2507.23445", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.23445", "abs": "https://arxiv.org/abs/2507.23445", "authors": ["Yuta Kawachi"], "title": "Quantifying and Visualizing Sim-to-Real Gaps: Physics-Guided Regularization for Reproducibility", "comment": null, "summary": "Simulation-to-real transfer using domain randomization for robot control\noften relies on low-gear-ratio, backdrivable actuators, but these approaches\nbreak down when the sim-to-real gap widens. Inspired by the traditional PID\ncontroller, we reinterpret its gains as surrogates for complex, unmodeled plant\ndynamics. We then introduce a physics-guided gain regularization scheme that\nmeasures a robot's effective proportional gains via simple real-world\nexperiments. Then, we penalize any deviation of a neural controller's local\ninput-output sensitivities from these values during training. To avoid the\noverly conservative bias of naive domain randomization, we also condition the\ncontroller on the current plant parameters. On an off-the-shelf two-wheeled\nbalancing robot with a 110:1 gearbox, our gain-regularized,\nparameter-conditioned RNN achieves angular settling times in hardware that\nclosely match simulation. At the same time, a purely domain-randomized policy\nexhibits persistent oscillations and a substantial sim-to-real gap. These\nresults demonstrate a lightweight, reproducible framework for closing\nsim-to-real gaps on affordable robotic hardware.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u5f15\u5bfc\u589e\u76ca\u6b63\u5219\u5316\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6d4b\u91cf\u673a\u5668\u4eba\u7684\u6709\u6548\u6bd4\u4f8b\u589e\u76ca\u5e76\u60e9\u7f5a\u795e\u7ecf\u63a7\u5236\u5668\u5728\u8bad\u7ec3\u4e2d\u7684\u504f\u5dee\uff0c\u7ed3\u5408\u53c2\u6570\u6761\u4ef6\u5316\uff0c\u6210\u529f\u7f29\u5c0f\u4e86\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u5dee\u8ddd\u3002", "motivation": "\u4f20\u7edf\u57df\u968f\u673a\u5316\u65b9\u6cd5\u5728\u4eff\u771f\u4e0e\u73b0\u5b9e\u5dee\u8ddd\u8f83\u5927\u65f6\u5931\u6548\uff0c\u5c24\u5176\u662f\u5bf9\u9ad8\u9f7f\u8f6e\u6bd4\u673a\u5668\u4eba\u3002\u8bba\u6587\u53d7PID\u63a7\u5236\u5668\u542f\u53d1\uff0c\u8bd5\u56fe\u901a\u8fc7\u589e\u76ca\u6b63\u5219\u5316\u548c\u53c2\u6570\u6761\u4ef6\u5316\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5b9e\u9645\u5b9e\u9a8c\u6d4b\u91cf\u673a\u5668\u4eba\u7684\u6709\u6548\u6bd4\u4f8b\u589e\u76ca\uff0c\u5e76\u5728\u8bad\u7ec3\u4e2d\u60e9\u7f5a\u795e\u7ecf\u63a7\u5236\u5668\u7684\u5c40\u90e8\u8f93\u5165\u8f93\u51fa\u654f\u611f\u5ea6\u504f\u5dee\u3002\u540c\u65f6\uff0c\u63a7\u5236\u5668\u6839\u636e\u5f53\u524d\u8bbe\u5907\u53c2\u6570\u8fdb\u884c\u6761\u4ef6\u5316\u3002", "result": "\u5728\u5e26\u6709110:1\u9f7f\u8f6e\u7bb1\u7684\u4e24\u8f6e\u5e73\u8861\u673a\u5668\u4eba\u4e0a\uff0c\u589e\u76ca\u6b63\u5219\u5316\u548c\u53c2\u6570\u6761\u4ef6\u5316\u7684RNN\u5728\u786c\u4ef6\u4e2d\u7684\u89d2\u5ea6\u7a33\u5b9a\u65f6\u95f4\u4e0e\u4eff\u771f\u7ed3\u679c\u63a5\u8fd1\uff0c\u800c\u7eaf\u57df\u968f\u673a\u5316\u7b56\u7565\u5219\u8868\u73b0\u51fa\u6301\u7eed\u632f\u8361\u548c\u663e\u8457\u5dee\u8ddd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u7f29\u5c0f\u4eff\u771f\u4e0e\u73b0\u5b9e\u5dee\u8ddd\u63d0\u4f9b\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u53ef\u590d\u73b0\u7684\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u4f4e\u6210\u672c\u673a\u5668\u4eba\u786c\u4ef6\u3002"}}
{"id": "2507.23554", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.23554", "abs": "https://arxiv.org/abs/2507.23554", "authors": ["Ruoyu Wang", "Junda Wu", "Yu Xia", "Tong Yu", "Ryan A. Rossi", "Julian McAuley", "Lina Yao"], "title": "DICE: Dynamic In-Context Example Selection in LLM Agents via Efficient Knowledge Transfer", "comment": null, "summary": "Large language model-based agents, empowered by in-context learning (ICL),\nhave demonstrated strong capabilities in complex reasoning and tool-use tasks.\nHowever, existing works have shown that the effectiveness of ICL is highly\nsensitive to the choice of demonstrations, with suboptimal examples often\nleading to unstable or degraded performance. While prior work has explored\nexample selection, including in some agentic or multi-step settings, existing\napproaches typically rely on heuristics or task-specific designs and lack a\ngeneral, theoretically grounded criterion for what constitutes an effective\ndemonstration across reasoning steps. Therefore, it is non-trivial to develop a\nprincipled, general-purpose method for selecting demonstrations that\nconsistently benefit agent performance. In this paper, we address this\nchallenge with DICE, Dynamic In-Context Example Selection for LLM Agents, a\ntheoretically grounded ICL framework for agentic tasks that selects the most\nrelevant demonstrations at each step of reasoning. Our approach decomposes\ndemonstration knowledge into transferable and non-transferable components\nthrough a causal lens, showing how the latter can introduce spurious\ndependencies that impair generalization. We further propose a stepwise\nselection criterion with a formal guarantee of improved agent performance.\nImportantly, DICE is a general, framework-agnostic solution that can be\nintegrated as a plug-in module into existing agentic frameworks without any\nadditional training cost. Extensive experiments across diverse domains\ndemonstrate our method's effectiveness and generality, highlighting the\nimportance of principled, context-aware demo selection for robust and efficient\nLLM agents.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faDICE\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u4e0a\u4e0b\u6587\u793a\u4f8b\u63d0\u5347LLM\u4ee3\u7406\u7684\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u542f\u53d1\u5f0f\u6216\u4efb\u52a1\u7279\u5b9a\u8bbe\u8ba1\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eICL\u7684LLM\u4ee3\u7406\u5728\u590d\u6742\u63a8\u7406\u548c\u5de5\u5177\u4f7f\u7528\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u6027\u80fd\u5bf9\u793a\u4f8b\u9009\u62e9\u9ad8\u5ea6\u654f\u611f\uff0c\u7f3a\u4e4f\u901a\u7528\u4e14\u7406\u8bba\u652f\u6301\u7684\u9009\u62e9\u6807\u51c6\u3002", "method": "\u63d0\u51faDICE\u6846\u67b6\uff0c\u901a\u8fc7\u56e0\u679c\u89c6\u89d2\u5206\u89e3\u6f14\u793a\u77e5\u8bc6\u4e3a\u53ef\u8f6c\u79fb\u548c\u4e0d\u53ef\u8f6c\u79fb\u90e8\u5206\uff0c\u5e76\u63d0\u51fa\u9010\u6b65\u9009\u62e9\u6807\u51c6\uff0c\u786e\u4fdd\u6027\u80fd\u63d0\u5347\u3002", "result": "\u5b9e\u9a8c\u8868\u660eDICE\u5728\u591a\u6837\u5316\u9886\u57df\u4e2d\u6709\u6548\u4e14\u901a\u7528\uff0c\u80fd\u663e\u8457\u63d0\u5347\u4ee3\u7406\u6027\u80fd\u3002", "conclusion": "DICE\u662f\u4e00\u79cd\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6210\u672c\u7684\u901a\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u5f3a\u8c03\u4e86\u4e0a\u4e0b\u6587\u611f\u77e5\u793a\u4f8b\u9009\u62e9\u5bf9LLM\u4ee3\u7406\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2507.23523", "categories": ["cs.RO", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23523", "abs": "https://arxiv.org/abs/2507.23523", "authors": ["Hongzhe Bi", "Lingxuan Wu", "Tianwei Lin", "Hengkai Tan", "Zhizhong Su", "Hang Su", "Jun Zhu"], "title": "H-RDT: Human Manipulation Enhanced Bimanual Robotic Manipulation", "comment": null, "summary": "Imitation learning for robotic manipulation faces a fundamental challenge:\nthe scarcity of large-scale, high-quality robot demonstration data. Recent\nrobotic foundation models often pre-train on cross-embodiment robot datasets to\nincrease data scale, while they face significant limitations as the diverse\nmorphologies and action spaces across different robot embodiments make unified\ntraining challenging. In this paper, we present H-RDT (Human to Robotics\nDiffusion Transformer), a novel approach that leverages human manipulation data\nto enhance robot manipulation capabilities. Our key insight is that large-scale\negocentric human manipulation videos with paired 3D hand pose annotations\nprovide rich behavioral priors that capture natural manipulation strategies and\ncan benefit robotic policy learning. We introduce a two-stage training\nparadigm: (1) pre-training on large-scale egocentric human manipulation data,\nand (2) cross-embodiment fine-tuning on robot-specific data with modular action\nencoders and decoders. Built on a diffusion transformer architecture with 2B\nparameters, H-RDT uses flow matching to model complex action distributions.\nExtensive evaluations encompassing both simulation and real-world experiments,\nsingle-task and multitask scenarios, as well as few-shot learning and\nrobustness assessments, demonstrate that H-RDT outperforms training from\nscratch and existing state-of-the-art methods, including Pi0 and RDT, achieving\nsignificant improvements of 13.9% and 40.5% over training from scratch in\nsimulation and real-world experiments, respectively. The results validate our\ncore hypothesis that human manipulation data can serve as a powerful foundation\nfor learning bimanual robotic manipulation policies.", "AI": {"tldr": "H-RDT\u5229\u7528\u4eba\u7c7b\u64cd\u4f5c\u6570\u636e\u589e\u5f3a\u673a\u5668\u4eba\u64cd\u4f5c\u80fd\u529b\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\uff08\u4eba\u7c7b\u6570\u636e\u9884\u8bad\u7ec3\u548c\u673a\u5668\u4eba\u6570\u636e\u5fae\u8c03\uff09\uff0c\u5728\u4eff\u771f\u548c\u771f\u5b9e\u5b9e\u9a8c\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60\u4e2d\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u6f14\u793a\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u5229\u7528\u4eba\u7c7b\u64cd\u4f5c\u6570\u636e\u7684\u4e30\u5bcc\u884c\u4e3a\u5148\u9a8c\u63d0\u5347\u673a\u5668\u4eba\u7b56\u7565\u5b66\u4e60\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a1\uff09\u5927\u89c4\u6a21\u4eba\u7c7b\u64cd\u4f5c\u6570\u636e\u9884\u8bad\u7ec3\uff1b2\uff09\u673a\u5668\u4eba\u6570\u636e\u5fae\u8c03\uff0c\u7ed3\u5408\u6269\u6563\u53d8\u6362\u5668\u67b6\u6784\u548c\u6d41\u5339\u914d\u6280\u672f\u3002", "result": "H-RDT\u5728\u4eff\u771f\u548c\u771f\u5b9e\u5b9e\u9a8c\u4e2d\u5206\u522b\u6bd4\u4ece\u5934\u8bad\u7ec3\u63d0\u534713.9%\u548c40.5%\uff0c\u4f18\u4e8ePi0\u548cRDT\u7b49\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u4eba\u7c7b\u64cd\u4f5c\u6570\u636e\u53ef\u4f5c\u4e3a\u5b66\u4e60\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u7684\u5f3a\u5927\u57fa\u7840\uff0cH-RDT\u9a8c\u8bc1\u4e86\u8fd9\u4e00\u5047\u8bbe\u3002"}}
{"id": "2507.23565", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.23565", "abs": "https://arxiv.org/abs/2507.23565", "authors": ["Botao Zhu", "Xianbin Wang", "Dusit Niyato"], "title": "Semantic Chain-of-Trust: Autonomous Trust Orchestration for Collaborator Selection via Hypergraph-Aided Agentic AI", "comment": null, "summary": "In collaborative systems, the effective completion of tasks hinges on\ntask-specific trust evaluations of potential devices for distributed\ncollaboration. However, the complexity of tasks, the spatiotemporal dynamism of\ndistributed device resources, and the inevitable assessment overhead\ndramatically increase the complexity and resource consumption of the trust\nevaluation process. As a result, ill-timed or overly frequent trust evaluations\ncan reduce utilization rate of constrained resources, negatively affecting\ncollaborative task execution. To address this challenge, this paper proposes an\nautonomous trust orchestration method based on a new concept of semantic\nchain-of-trust. Our technique employs agentic AI and hypergraph to establish\nand maintain trust relationships among devices. By leveraging its strengths in\nautonomous perception, task decomposition, and semantic reasoning, we propose\nagentic AI to perceive device states and autonomously perform trust evaluations\nof collaborators based on historical performance data only during device idle\nperiods, thereby enabling efficient utilization of distributed resources. In\naddition, agentic AI performs task-specific trust evaluations on collaborator\nresources by analyzing the alignment between resource capabilities and task\nrequirements. Moreover, by maintaining a trust hypergraph embedded with trust\nsemantics for each device, agentic AI enables hierarchical management of\ncollaborators and identifies collaborators requiring trust evaluation based on\ntrust semantics, thereby achieving a balance between overhead and trust\naccuracy. Furthermore, local trust hypergraphs from multiple devices can be\nchained together to support multi-hop collaboration, enabling efficient\ncoordination in large-scale systems. Experimental results demonstrate that the\nproposed method achieves resource-efficient trust evaluation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u4e49\u4fe1\u4efb\u94fe\u7684\u81ea\u4e3b\u4fe1\u4efb\u7f16\u6392\u65b9\u6cd5\uff0c\u5229\u7528\u667a\u80fd\u4ee3\u7406\u548c\u8d85\u56fe\u6280\u672f\u4f18\u5316\u5206\u5e03\u5f0f\u534f\u4f5c\u4e2d\u7684\u4fe1\u4efb\u8bc4\u4f30\u3002", "motivation": "\u5206\u5e03\u5f0f\u534f\u4f5c\u4e2d\uff0c\u590d\u6742\u7684\u4efb\u52a1\u3001\u52a8\u6001\u7684\u8bbe\u5907\u8d44\u6e90\u4ee5\u53ca\u9891\u7e41\u7684\u4fe1\u4efb\u8bc4\u4f30\u589e\u52a0\u4e86\u8d44\u6e90\u6d88\u8017\uff0c\u5f71\u54cd\u534f\u4f5c\u6548\u7387\u3002", "method": "\u91c7\u7528\u667a\u80fd\u4ee3\u7406\u548c\u8d85\u56fe\u6280\u672f\uff0c\u901a\u8fc7\u5386\u53f2\u6570\u636e\u5728\u8bbe\u5907\u7a7a\u95f2\u671f\u8fdb\u884c\u4fe1\u4efb\u8bc4\u4f30\uff0c\u5e76\u5229\u7528\u4fe1\u4efb\u8bed\u4e49\u8d85\u56fe\u5b9e\u73b0\u5206\u5c42\u7ba1\u7406\u548c\u591a\u8df3\u534f\u4f5c\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u8d44\u6e90\u9ad8\u6548\u7684\u4fe1\u4efb\u8bc4\u4f30\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u51cf\u5c11\u8d44\u6e90\u6d88\u8017\u7684\u540c\u65f6\uff0c\u63d0\u9ad8\u4e86\u4fe1\u4efb\u8bc4\u4f30\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2507.23540", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.23540", "abs": "https://arxiv.org/abs/2507.23540", "authors": ["Yi Zhang", "Erik Leo Ha\u00df", "Kuo-Yi Chao", "Nenad Petrovic", "Yinglei Song", "Chengdong Wu", "Alois Knoll"], "title": "A Unified Perception-Language-Action Framework for Adaptive Autonomous Driving", "comment": null, "summary": "Autonomous driving systems face significant challenges in achieving\nhuman-like adaptability, robustness, and interpretability in complex,\nopen-world environments. These challenges stem from fragmented architectures,\nlimited generalization to novel scenarios, and insufficient semantic extraction\nfrom perception. To address these limitations, we propose a unified\nPerception-Language-Action (PLA) framework that integrates multi-sensor fusion\n(cameras, LiDAR, radar) with a large language model (LLM)-augmented\nVision-Language-Action (VLA) architecture, specifically a GPT-4.1-powered\nreasoning core. This framework unifies low-level sensory processing with\nhigh-level contextual reasoning, tightly coupling perception with natural\nlanguage-based semantic understanding and decision-making to enable\ncontext-aware, explainable, and safety-bounded autonomous driving. Evaluations\non an urban intersection scenario with a construction zone demonstrate superior\nperformance in trajectory tracking, speed prediction, and adaptive planning.\nThe results highlight the potential of language-augmented cognitive frameworks\nfor advancing the safety, interpretability, and scalability of autonomous\ndriving systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u611f\u77e5-\u8bed\u8a00-\u52a8\u4f5c\uff08PLA\uff09\u6846\u67b6\uff0c\u7ed3\u5408\u591a\u4f20\u611f\u5668\u878d\u5408\u548c\u8bed\u8a00\u6a21\u578b\uff0c\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7684\u9002\u5e94\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5728\u590d\u6742\u5f00\u653e\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\u3001\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u95ee\u9898\u3002", "method": "\u91c7\u7528\u591a\u4f20\u611f\u5668\u878d\u5408\uff08\u76f8\u673a\u3001LiDAR\u3001\u96f7\u8fbe\uff09\u548cGPT-4.1\u589e\u5f3a\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u67b6\u6784\u3002", "result": "\u5728\u57ce\u5e02\u4ea4\u53c9\u8def\u53e3\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u8f68\u8ff9\u8ddf\u8e2a\u3001\u901f\u5ea6\u9884\u6d4b\u548c\u81ea\u9002\u5e94\u89c4\u5212\u80fd\u529b\u3002", "conclusion": "\u8bed\u8a00\u589e\u5f3a\u7684\u8ba4\u77e5\u6846\u67b6\u6709\u671b\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7684\u5b89\u5168\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2507.23633", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.23633", "abs": "https://arxiv.org/abs/2507.23633", "authors": ["Qian Zhao", "Zhuo Sun", "Bin Guo", "Zhiwen Yu"], "title": "MemoCue: Empowering LLM-Based Agents for Human Memory Recall via Strategy-Guided Querying", "comment": null, "summary": "Agent-assisted memory recall is one critical research problem in the field of\nhuman-computer interaction. In conventional methods, the agent can retrieve\ninformation from its equipped memory module to help the person recall\nincomplete or vague memories. The limited size of memory module hinders the\nacquisition of complete memories and impacts the memory recall performance in\npractice. Memory theories suggest that the person's relevant memory can be\nproactively activated through some effective cues. Inspired by this, we propose\na novel strategy-guided agent-assisted memory recall method, allowing the agent\nto transform an original query into a cue-rich one via the judiciously designed\nstrategy to help the person recall memories. To this end, there are two key\nchallenges. (1) How to choose the appropriate recall strategy for diverse\nforgetting scenarios with distinct memory-recall characteristics? (2) How to\nobtain the high-quality responses leveraging recall strategies, given only\nabstract and sparsely annotated strategy patterns? To address the challenges,\nwe propose a Recall Router framework. Specifically, we design a 5W Recall Map\nto classify memory queries into five typical scenarios and define fifteen\nrecall strategy patterns across the corresponding scenarios. We then propose a\nhierarchical recall tree combined with the Monte Carlo Tree Search algorithm to\noptimize the selection of strategy and the generation of strategy responses. We\nconstruct an instruction tuning dataset and fine-tune multiple open-source\nlarge language models (LLMs) to develop MemoCue, an agent that excels in\nproviding memory-inspired responses. Experiments on three representative\ndatasets show that MemoCue surpasses LLM-based methods by 17.74% in recall\ninspiration. Further human evaluation highlights its advantages in\nmemory-recall applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7b56\u7565\u5f15\u5bfc\u7684\u4ee3\u7406\u8f85\u52a9\u8bb0\u5fc6\u56de\u5fc6\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bbe\u8ba1\u7b56\u7565\u5c06\u539f\u59cb\u67e5\u8be2\u8f6c\u5316\u4e3a\u5bcc\u542b\u7ebf\u7d22\u7684\u67e5\u8be2\uff0c\u63d0\u5347\u8bb0\u5fc6\u56de\u5fc6\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u53d7\u9650\u4e8e\u8bb0\u5fc6\u6a21\u5757\u5927\u5c0f\uff0c\u65e0\u6cd5\u5b8c\u6574\u83b7\u53d6\u8bb0\u5fc6\uff0c\u5f71\u54cd\u56de\u5fc6\u6027\u80fd\u3002\u53d7\u8bb0\u5fc6\u7406\u8bba\u542f\u53d1\uff0c\u63d0\u51fa\u901a\u8fc7\u6709\u6548\u7ebf\u7d22\u4e3b\u52a8\u6fc0\u6d3b\u8bb0\u5fc6\u3002", "method": "\u8bbe\u8ba1\u4e865W Recall Map\u5206\u7c7b\u8bb0\u5fc6\u67e5\u8be2\u573a\u666f\uff0c\u5b9a\u4e4915\u79cd\u7b56\u7565\u6a21\u5f0f\uff0c\u7ed3\u5408\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u4f18\u5316\u7b56\u7565\u9009\u62e9\u548c\u54cd\u5e94\u751f\u6210\u3002", "result": "\u5b9e\u9a8c\u8868\u660eMemoCue\u5728\u56de\u5fc6\u7075\u611f\u4e0a\u4f18\u4e8e\u57fa\u4e8eLLM\u7684\u65b9\u6cd517.74%\uff0c\u4eba\u7c7b\u8bc4\u4f30\u4e5f\u9a8c\u8bc1\u4e86\u5176\u4f18\u52bf\u3002", "conclusion": "MemoCue\u901a\u8fc7\u7b56\u7565\u5f15\u5bfc\u548c\u4f18\u5316\u7b97\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u8bb0\u5fc6\u56de\u5fc6\u6548\u679c\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.23544", "categories": ["cs.RO", "cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.23544", "abs": "https://arxiv.org/abs/2507.23544", "authors": ["Ryo Miyoshi", "Yuki Okafuji", "Takuya Iwamoto", "Junya Nakanishi", "Jun Baba"], "title": "User Experience Estimation in Human-Robot Interaction Via Multi-Instance Learning of Multimodal Social Signals", "comment": "This paper has been accepted for presentation at IEEE/RSJ\n  International Conference on Intelligent Robots and Systems 2025 (IROS 2025)", "summary": "In recent years, the demand for social robots has grown, requiring them to\nadapt their behaviors based on users' states. Accurately assessing user\nexperience (UX) in human-robot interaction (HRI) is crucial for achieving this\nadaptability. UX is a multi-faceted measure encompassing aspects such as\nsentiment and engagement, yet existing methods often focus on these\nindividually. This study proposes a UX estimation method for HRI by leveraging\nmultimodal social signals. We construct a UX dataset and develop a\nTransformer-based model that utilizes facial expressions and voice for\nestimation. Unlike conventional models that rely on momentary observations, our\napproach captures both short- and long-term interaction patterns using a\nmulti-instance learning framework. This enables the model to capture temporal\ndynamics in UX, providing a more holistic representation. Experimental results\ndemonstrate that our method outperforms third-party human evaluators in UX\nestimation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u793e\u4ea4\u4fe1\u53f7\uff08\u9762\u90e8\u8868\u60c5\u548c\u8bed\u97f3\uff09\u7684Transformer\u6a21\u578b\uff0c\u7528\u4e8e\u8bc4\u4f30\u4eba\u673a\u4ea4\u4e92\u4e2d\u7684\u7528\u6237\u4f53\u9a8c\uff08UX\uff09\uff0c\u5e76\u901a\u8fc7\u591a\u5b9e\u4f8b\u5b66\u4e60\u6846\u67b6\u6355\u6349\u77ed\u671f\u548c\u957f\u671f\u4ea4\u4e92\u6a21\u5f0f\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u548c\u4eba\u7c7b\u8bc4\u4f30\u8005\u3002", "motivation": "\u793e\u4f1a\u673a\u5668\u4eba\u9700\u6839\u636e\u7528\u6237\u72b6\u6001\u8c03\u6574\u884c\u4e3a\uff0c\u800c\u73b0\u6709UX\u8bc4\u4f30\u65b9\u6cd5\u591a\u5173\u6ce8\u5355\u4e00\u7ef4\u5ea6\uff08\u5982\u60c5\u611f\u6216\u53c2\u4e0e\u5ea6\uff09\uff0c\u7f3a\u4e4f\u5bf9\u591a\u7ef4\u5ea6UX\u7684\u7efc\u5408\u8bc4\u4f30\u3002", "method": "\u6784\u5efaUX\u6570\u636e\u96c6\uff0c\u5f00\u53d1\u57fa\u4e8eTransformer\u7684\u6a21\u578b\uff0c\u5229\u7528\u9762\u90e8\u8868\u60c5\u548c\u8bed\u97f3\u4fe1\u53f7\uff0c\u7ed3\u5408\u591a\u5b9e\u4f8b\u5b66\u4e60\u6846\u67b6\u6355\u6349\u4ea4\u4e92\u7684\u65f6\u7a7a\u52a8\u6001\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728UX\u8bc4\u4f30\u4e0a\u4f18\u4e8e\u7b2c\u4e09\u65b9\u4eba\u7c7b\u8bc4\u4f30\u8005\u548c\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u591a\u6a21\u6001\u4fe1\u53f7\u548c\u65f6\u7a7a\u52a8\u6001\u6355\u6349\u80fd\u66f4\u5168\u9762\u5730\u8bc4\u4f30UX\uff0c\u4e3a\u673a\u5668\u4eba\u884c\u4e3a\u8c03\u6574\u63d0\u4f9b\u66f4\u51c6\u786e\u7684\u4f9d\u636e\u3002"}}
{"id": "2507.23664", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.23664", "abs": "https://arxiv.org/abs/2507.23664", "authors": ["Haipeng Liu", "Yuxuan Liu", "Ting Long"], "title": "Personalized Education with Ranking Alignment Recommendation", "comment": null, "summary": "Personalized question recommendation aims to guide individual students\nthrough questions to enhance their mastery of learning targets. Most previous\nmethods model this task as a Markov Decision Process and use reinforcement\nlearning to solve, but they struggle with efficient exploration, failing to\nidentify the best questions for each student during training. To address this,\nwe propose Ranking Alignment Recommendation (RAR), which incorporates\ncollaborative ideas into the exploration mechanism, enabling more efficient\nexploration within limited training episodes. Experiments show that RAR\neffectively improves recommendation performance, and our framework can be\napplied to any RL-based question recommender. Our code is available in\nhttps://github.com/wuming29/RAR.git.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRAR\u7684\u4e2a\u6027\u5316\u95ee\u9898\u63a8\u8350\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u534f\u4f5c\u601d\u60f3\u878d\u5165\u63a2\u7d22\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u8bad\u7ec3\u4e2d\u63a2\u7d22\u6548\u7387\u4f4e\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5c06\u4e2a\u6027\u5316\u95ee\u9898\u63a8\u8350\u5efa\u6a21\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u5e76\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u89e3\u51b3\uff0c\u4f46\u5728\u8bad\u7ec3\u4e2d\u96be\u4ee5\u9ad8\u6548\u63a2\u7d22\uff0c\u65e0\u6cd5\u4e3a\u6bcf\u4e2a\u5b66\u751f\u627e\u5230\u6700\u4f73\u95ee\u9898\u3002", "method": "\u63d0\u51faRanking Alignment Recommendation (RAR)\uff0c\u5c06\u534f\u4f5c\u601d\u60f3\u878d\u5165\u63a2\u7d22\u673a\u5236\uff0c\u4ee5\u5728\u6709\u9650\u8bad\u7ec3\u8f6e\u6b21\u5185\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u63a2\u7d22\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRAR\u663e\u8457\u63d0\u9ad8\u4e86\u63a8\u8350\u6027\u80fd\uff0c\u4e14\u8be5\u6846\u67b6\u53ef\u5e94\u7528\u4e8e\u4efb\u4f55\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u95ee\u9898\u63a8\u8350\u7cfb\u7edf\u3002", "conclusion": "RAR\u901a\u8fc7\u6539\u8fdb\u63a2\u7d22\u673a\u5236\uff0c\u6709\u6548\u63d0\u5347\u4e86\u63a8\u8350\u6548\u679c\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2507.23589", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.23589", "abs": "https://arxiv.org/abs/2507.23589", "authors": ["Kai Goebel", "Patrik Zips"], "title": "Can LLM-Reasoning Models Replace Classical Planning? A Benchmark Study", "comment": null, "summary": "Recent advancements in Large Language Models have sparked interest in their\npotential for robotic task planning. While these models demonstrate strong\ngenerative capabilities, their effectiveness in producing structured and\nexecutable plans remains uncertain. This paper presents a systematic evaluation\nof a broad spectrum of current state of the art language models, each directly\nprompted using Planning Domain Definition Language domain and problem files,\nand compares their planning performance with the Fast Downward planner across a\nvariety of benchmarks. In addition to measuring success rates, we assess how\nfaithfully the generated plans translate into sequences of actions that can\nactually be executed, identifying both strengths and limitations of using these\nmodels in this setting. Our findings show that while the models perform well on\nsimpler planning tasks, they continue to struggle with more complex scenarios\nthat require precise resource management, consistent state tracking, and strict\nconstraint compliance. These results underscore fundamental challenges in\napplying language models to robotic planning in real world environments. By\noutlining the gaps that emerge during execution, we aim to guide future\nresearch toward combined approaches that integrate language models with\nclassical planners in order to enhance the reliability and scalability of\nplanning in autonomous robotics.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e86\u5f53\u524d\u5148\u8fdb\u8bed\u8a00\u6a21\u578b\u5728\u673a\u5668\u4eba\u4efb\u52a1\u89c4\u5212\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u5728\u7b80\u5355\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u590d\u6742\u573a\u666f\u4e2d\u4ecd\u5b58\u5728\u6311\u6218\u3002", "motivation": "\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u7ed3\u6784\u5316\u3001\u53ef\u6267\u884c\u673a\u5668\u4eba\u4efb\u52a1\u89c4\u5212\u4e2d\u7684\u6f5c\u529b\u4e0e\u5c40\u9650\u6027\u3002", "method": "\u76f4\u63a5\u4f7f\u7528PDDL\u6587\u4ef6\u548c\u95ee\u9898\u6587\u4ef6\u63d0\u793a\u591a\u79cd\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u4e0eFast Downward\u89c4\u5212\u5668\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6bd4\u8f83\u6027\u80fd\u3002", "result": "\u6a21\u578b\u5728\u7b80\u5355\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u590d\u6742\u573a\u666f\uff08\u5982\u8d44\u6e90\u7ba1\u7406\u3001\u72b6\u6001\u8ddf\u8e2a\u548c\u7ea6\u675f\u9075\u5b88\uff09\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "\u672a\u6765\u7814\u7a76\u5e94\u7ed3\u5408\u8bed\u8a00\u6a21\u578b\u4e0e\u4f20\u7edf\u89c4\u5212\u5668\uff0c\u4ee5\u63d0\u9ad8\u673a\u5668\u4eba\u89c4\u5212\u7684\u53ef\u9760\u6027\u548c\u6269\u5c55\u6027\u3002"}}
{"id": "2507.23701", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.23701", "abs": "https://arxiv.org/abs/2507.23701", "authors": ["Long Phan", "Mantas Mazeika", "Andy Zou", "Dan Hendrycks"], "title": "TextQuests: How Good are LLMs at Text-Based Video Games?", "comment": null, "summary": "Evaluating AI agents within complex, interactive environments that mirror\nreal-world challenges is critical for understanding their practical\ncapabilities. While existing agent benchmarks effectively assess skills like\ntool use or performance on structured tasks, they often do not fully capture an\nagent's ability to operate autonomously in exploratory environments that demand\nsustained, self-directed reasoning over a long and growing context. To spur the\ndevelopment of agents capable of more robust intrinsic reasoning over long\nhorizons, we introduce TextQuests, a benchmark based on the Infocom suite of\ninteractive fiction games. These text-based adventures, which can take human\nplayers over 30 hours and require hundreds of precise actions to solve, serve\nas an effective proxy for evaluating AI agents on focused, stateful tasks. The\nbenchmark is specifically designed to assess an LLM agent's capacity for\nself-contained problem-solving by precluding the use of external tools, thereby\nfocusing on intrinsic long-context reasoning capabilities in an exploratory\nenvironment characterized by the need for trial-and-error learning and\nsustained problem-solving within a single interactive session. We release\nTextQuests at https://textquests.ai.", "AI": {"tldr": "TextQuests\u662f\u4e00\u4e2a\u57fa\u4e8e\u4ea4\u4e92\u5f0f\u5c0f\u8bf4\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u65e8\u5728\u8bc4\u4f30AI\u4ee3\u7406\u5728\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u548c\u81ea\u4e3b\u63a2\u7d22\u73af\u5883\u4e2d\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u672a\u80fd\u5145\u5206\u8bc4\u4f30AI\u4ee3\u7406\u5728\u81ea\u4e3b\u63a2\u7d22\u73af\u5883\u4e2d\u7684\u957f\u671f\u63a8\u7406\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u5168\u9762\u7684\u6d4b\u8bd5\u5de5\u5177\u3002", "method": "\u5229\u7528Infocom\u4ea4\u4e92\u5f0f\u5c0f\u8bf4\u6e38\u620f\u4f5c\u4e3a\u6d4b\u8bd5\u73af\u5883\uff0c\u8bbe\u8ba1TextQuests\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7981\u6b62\u5916\u90e8\u5de5\u5177\u4f7f\u7528\uff0c\u4e13\u6ce8\u4e8e\u5185\u5728\u63a8\u7406\u80fd\u529b\u3002", "result": "TextQuests\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u80fd\u591f\u8bc4\u4f30AI\u4ee3\u7406\u5728\u957f\u4e0a\u4e0b\u6587\u548c\u63a2\u7d22\u6027\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "conclusion": "TextQuests\u586b\u8865\u4e86\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u7684\u7a7a\u767d\uff0c\u63a8\u52a8\u4e86AI\u4ee3\u7406\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u63a8\u7406\u80fd\u529b\u53d1\u5c55\u3002"}}
{"id": "2507.23592", "categories": ["cs.RO", "cs.HC", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.23592", "abs": "https://arxiv.org/abs/2507.23592", "authors": ["Haiyun Zhang", "Stefano Dalla Gasperina", "Saad N. Yousaf", "Toshimitsu Tsuboi", "Tetsuya Narita", "Ashish D. Deshpande"], "title": "Human-Exoskeleton Kinematic Calibration to Improve Hand Tracking for Dexterous Teleoperation", "comment": "8 pages, 10 figures, submitted to RA-L", "summary": "Hand exoskeletons are critical tools for dexterous teleoperation and\nimmersive manipulation interfaces, but achieving accurate hand tracking remains\na challenge due to user-specific anatomical variability and donning\ninconsistencies. These issues lead to kinematic misalignments that degrade\ntracking performance and limit applicability in precision tasks. We propose a\nsubject-specific calibration framework for exoskeleton-based hand tracking that\nuses redundant joint sensing and a residual-weighted optimization strategy to\nestimate virtual link parameters. Implemented on the Maestro exoskeleton, our\nmethod improves joint angle and fingertip position estimation across users with\nvarying hand geometries. We introduce a data-driven approach to empirically\ntune cost function weights using motion capture ground truth, enabling more\naccurate and consistent calibration across participants. Quantitative results\nfrom seven subjects show substantial reductions in joint and fingertip tracking\nerrors compared to uncalibrated and evenly weighted models. Qualitative\nvisualizations using a Unity-based virtual hand further confirm improvements in\nmotion fidelity. The proposed framework generalizes across exoskeleton designs\nwith closed-loop kinematics and minimal sensing, and lays the foundation for\nhigh-fidelity teleoperation and learning-from-demonstration applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5197\u4f59\u5173\u8282\u4f20\u611f\u548c\u6b8b\u5dee\u52a0\u6743\u4f18\u5316\u7684\u624b\u90e8\u5916\u9aa8\u9abc\u6807\u5b9a\u6846\u67b6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8ddf\u8e2a\u7cbe\u5ea6\u3002", "motivation": "\u89e3\u51b3\u624b\u90e8\u5916\u9aa8\u9abc\u56e0\u7528\u6237\u89e3\u5256\u5dee\u5f02\u548c\u7a7f\u6234\u4e0d\u4e00\u81f4\u5bfc\u81f4\u7684\u8ddf\u8e2a\u8bef\u5dee\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u5197\u4f59\u5173\u8282\u4f20\u611f\u548c\u6b8b\u5dee\u52a0\u6743\u4f18\u5316\u7b56\u7565\u4f30\u8ba1\u865a\u62df\u94fe\u63a5\u53c2\u6570\uff0c\u5e76\u901a\u8fc7\u8fd0\u52a8\u6355\u6349\u6570\u636e\u8c03\u4f18\u6210\u672c\u51fd\u6570\u6743\u91cd\u3002", "result": "\u5728\u4e03\u540d\u53d7\u8bd5\u8005\u4e2d\uff0c\u5173\u8282\u548c\u6307\u5c16\u8ddf\u8e2a\u8bef\u5dee\u663e\u8457\u964d\u4f4e\uff0c\u8fd0\u52a8\u4fdd\u771f\u5ea6\u63d0\u9ad8\u3002", "conclusion": "\u8be5\u6846\u67b6\u9002\u7528\u4e8e\u95ed\u73af\u8fd0\u52a8\u5b66\u548c\u6700\u5c0f\u4f20\u611f\u7684\u5916\u9aa8\u9abc\u8bbe\u8ba1\uff0c\u4e3a\u9ad8\u4fdd\u771f\u9065\u64cd\u4f5c\u548c\u793a\u8303\u5b66\u4e60\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.23726", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.23726", "abs": "https://arxiv.org/abs/2507.23726", "authors": ["Luoxin Chen", "Jinming Gu", "Liankai Huang", "Wenhao Huang", "Zhicheng Jiang", "Allan Jie", "Xiaoran Jin", "Xing Jin", "Chenggang Li", "Kaijing Ma", "Cheng Ren", "Jiawei Shen", "Wenlei Shi", "Tong Sun", "He Sun", "Jiahui Wang", "Siran Wang", "Zhihong Wang", "Chenrui Wei", "Shufa Wei", "Yonghui Wu", "Yuchen Wu", "Yihang Xia", "Huajian Xin", "Fan Yang", "Huaiyuan Ying", "Hongyi Yuan", "Zheng Yuan", "Tianyang Zhan", "Chi Zhang", "Yue Zhang", "Ge Zhang", "Tianyun Zhao", "Jianqiu Zhao", "Yichi Zhou", "Thomas Hanwen Zhu"], "title": "Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving", "comment": null, "summary": "LLMs have demonstrated strong mathematical reasoning abilities by leveraging\nreinforcement learning with long chain-of-thought, yet they continue to\nstruggle with theorem proving due to the lack of clear supervision signals when\nsolely using natural language. Dedicated domain-specific languages like Lean\nprovide clear supervision via formal verification of proofs, enabling effective\ntraining through reinforcement learning. In this work, we propose\n\\textbf{Seed-Prover}, a lemma-style whole-proof reasoning model. Seed-Prover\ncan iteratively refine its proof based on Lean feedback, proved lemmas, and\nself-summarization. To solve IMO-level contest problems, we design three\ntest-time inference strategies that enable both deep and broad reasoning.\nSeed-Prover proves $78.1\\%$ of formalized past IMO problems, saturates MiniF2F,\nand achieves over 50\\% on PutnamBench, outperforming the previous\nstate-of-the-art by a large margin. To address the lack of geometry support in\nLean, we introduce a geometry reasoning engine \\textbf{Seed-Geometry}, which\noutperforms previous formal geometry engines. We use these two systems to\nparticipate in IMO 2025 and fully prove 5 out of 6 problems. This work\nrepresents a significant advancement in automated mathematical reasoning,\ndemonstrating the effectiveness of formal verification with long\nchain-of-thought reasoning.", "AI": {"tldr": "Seed-Prover\u662f\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u548c\u5f62\u5f0f\u5316\u9a8c\u8bc1\u7684\u5b9a\u7406\u8bc1\u660e\u6a21\u578b\uff0c\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u8bc1\u660e\u8fc7\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u4e86IMO\u7ea7\u6570\u5b66\u95ee\u9898\u7684\u89e3\u51b3\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u5b9a\u7406\u8bc1\u660e\u65b9\u9762\u56e0\u7f3a\u4e4f\u660e\u786e\u7684\u76d1\u7763\u4fe1\u53f7\u800c\u53d7\u9650\uff0c\u5f62\u5f0f\u5316\u9a8c\u8bc1\u8bed\u8a00\u5982Lean\u63d0\u4f9b\u4e86\u6709\u6548\u76d1\u7763\u3002", "method": "\u63d0\u51faSeed-Prover\u6a21\u578b\uff0c\u7ed3\u5408Lean\u7684\u53cd\u9988\u3001\u5df2\u8bc1\u660e\u5f15\u7406\u548c\u81ea\u6211\u603b\u7ed3\u8fed\u4ee3\u4f18\u5316\u8bc1\u660e\uff1b\u8bbe\u8ba1\u4e09\u79cd\u63a8\u7406\u7b56\u7565\u652f\u6301\u6df1\u5ea6\u548c\u5e7f\u5ea6\u63a8\u7406\uff1b\u5f15\u5165Seed-Geometry\u89e3\u51b3\u51e0\u4f55\u95ee\u9898\u3002", "result": "Seed-Prover\u5728IMO\u95ee\u9898\u4e2d\u8fbe\u523078.1%\u7684\u8bc1\u660e\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff1b\u5728IMO 2025\u4e2d\u5b8c\u5168\u8bc1\u660e\u4e865/6\u7684\u95ee\u9898\u3002", "conclusion": "Seed-Prover\u5c55\u793a\u4e86\u5f62\u5f0f\u5316\u9a8c\u8bc1\u4e0e\u957f\u94fe\u63a8\u7406\u7ed3\u5408\u7684\u6709\u6548\u6027\uff0c\u63a8\u52a8\u4e86\u81ea\u52a8\u6570\u5b66\u63a8\u7406\u7684\u8fdb\u6b65\u3002"}}
{"id": "2507.23629", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.23629", "abs": "https://arxiv.org/abs/2507.23629", "authors": ["Yewei Huang", "John McConnell", "Xi Lin", "Brendan Englot"], "title": "DRACo-SLAM2: Distributed Robust Acoustic Communication-efficient SLAM for Imaging Sonar EquippedUnderwater Robot Teams with Object Graph Matching", "comment": null, "summary": "We present DRACo-SLAM2, a distributed SLAM framework for underwater robot\nteams equipped with multibeam imaging sonar. This framework improves upon the\noriginal DRACo-SLAM by introducing a novel representation of sonar maps as\nobject graphs and utilizing object graph matching to achieve time-efficient\ninter-robot loop closure detection without relying on prior geometric\ninformation. To better-accommodate the needs and characteristics of underwater\nscan matching, we propose incremental Group-wise Consistent Measurement Set\nMaximization (GCM), a modification of Pairwise Consistent Measurement Set\nMaximization (PCM), which effectively handles scenarios where nearby\ninter-robot loop closures share similar registration errors. The proposed\napproach is validated through extensive comparative analyses on simulated and\nreal-world datasets.", "AI": {"tldr": "DRACo-SLAM2\u662f\u4e00\u4e2a\u7528\u4e8e\u6c34\u4e0b\u673a\u5668\u4eba\u56e2\u961f\u7684\u591a\u6ce2\u675f\u6210\u50cf\u58f0\u7eb3\u5206\u5e03\u5f0fSLAM\u6846\u67b6\uff0c\u901a\u8fc7\u5bf9\u8c61\u56fe\u8868\u793a\u548c\u5339\u914d\u4f18\u5316\u4e86\u95ed\u73af\u68c0\u6d4b\u6548\u7387\u3002", "motivation": "\u6539\u8fdb\u539f\u6709DRACo-SLAM\u6846\u67b6\uff0c\u9002\u5e94\u6c34\u4e0b\u73af\u5883\u9700\u6c42\uff0c\u89e3\u51b3\u591a\u673a\u5668\u4eba\u95ed\u73af\u68c0\u6d4b\u4e2d\u7684\u51e0\u4f55\u4fe1\u606f\u4f9d\u8d56\u548c\u8bef\u5dee\u5171\u4eab\u95ee\u9898\u3002", "method": "\u5f15\u5165\u5bf9\u8c61\u56fe\u8868\u793a\u58f0\u7eb3\u5730\u56fe\uff0c\u63d0\u51fa\u589e\u91cfGCM\u65b9\u6cd5\u66ff\u4ee3PCM\uff0c\u4f18\u5316\u90bb\u8fd1\u95ed\u73af\u573a\u666f\u7684\u914d\u51c6\u8bef\u5dee\u5904\u7406\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "DRACo-SLAM2\u901a\u8fc7\u5bf9\u8c61\u56fe\u548c\u589e\u91cfGCM\u663e\u8457\u63d0\u5347\u4e86\u6c34\u4e0b\u591a\u673a\u5668\u4ebaSLAM\u7684\u6027\u80fd\u3002"}}
{"id": "2507.23751", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.23751", "abs": "https://arxiv.org/abs/2507.23751", "authors": ["Ping Yu", "Jack Lanchantin", "Tianlu Wang", "Weizhe Yuan", "Olga Golovneva", "Ilia Kulikov", "Sainbayar Sukhbaatar", "Jason Weston", "Jing Xu"], "title": "CoT-Self-Instruct: Building high-quality synthetic prompts for reasoning and non-reasoning tasks", "comment": null, "summary": "We propose CoT-Self-Instruct, a synthetic data generation method that\ninstructs LLMs to first reason and plan via Chain-of-Thought (CoT) based on the\ngiven seed tasks, and then to generate a new synthetic prompt of similar\nquality and complexity for use in LLM training, followed by filtering for\nhigh-quality data with automatic metrics. In verifiable reasoning, our\nsynthetic data significantly outperforms existing training datasets, such as\ns1k and OpenMathReasoning, across MATH500, AMC23, AIME24 and GPQA-Diamond. For\nnon-verifiable instruction-following tasks, our method surpasses the\nperformance of human or standard self-instruct prompts on both AlpacaEval 2.0\nand Arena-Hard.", "AI": {"tldr": "CoT-Self-Instruct\u662f\u4e00\u79cd\u5408\u6210\u6570\u636e\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7Chain-of-Thought\uff08CoT\uff09\u5f15\u5bfcLLMs\u751f\u6210\u9ad8\u8d28\u91cf\u5408\u6210\u63d0\u793a\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406\u548c\u6307\u4ee4\u8ddf\u968f\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u8bad\u7ec3\u6570\u636e\u96c6\u5728\u590d\u6742\u63a8\u7406\u548c\u6307\u4ee4\u8ddf\u968f\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u79cd\u5b50\u4efb\u52a1\uff0c\u5229\u7528CoT\u5f15\u5bfcLLMs\u751f\u6210\u5408\u6210\u63d0\u793a\uff0c\u5e76\u901a\u8fc7\u81ea\u52a8\u6307\u6807\u8fc7\u6ee4\u9ad8\u8d28\u91cf\u6570\u636e\u3002", "result": "\u5728\u53ef\u9a8c\u8bc1\u63a8\u7406\u4efb\u52a1\uff08\u5982MATH500\u7b49\uff09\u548c\u975e\u53ef\u9a8c\u8bc1\u6307\u4ee4\u8ddf\u968f\u4efb\u52a1\uff08\u5982AlpacaEval 2.0\u7b49\uff09\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6570\u636e\u96c6\u3002", "conclusion": "CoT-Self-Instruct\u80fd\u6709\u6548\u751f\u6210\u9ad8\u8d28\u91cf\u5408\u6210\u6570\u636e\uff0c\u63d0\u5347LLMs\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2507.23660", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.23660", "abs": "https://arxiv.org/abs/2507.23660", "authors": ["Haoxuan Jiang", "Peicong Qian", "Yusen Xie", "Xiaocong Li", "Ming Liu", "Jun Ma"], "title": "DuLoc: Life-Long Dual-Layer Localization in Changing and Dynamic Expansive Scenarios", "comment": null, "summary": "LiDAR-based localization serves as a critical component in autonomous\nsystems, yet existing approaches face persistent challenges in balancing\nrepeatability, accuracy, and environmental adaptability. Traditional point\ncloud registration methods relying solely on offline maps often exhibit limited\nrobustness against long-term environmental changes, leading to localization\ndrift and reliability degradation in dynamic real-world scenarios. To address\nthese challenges, this paper proposes DuLoc, a robust and accurate localization\nmethod that tightly couples LiDAR-inertial odometry with offline map-based\nlocalization, incorporating a constant-velocity motion model to mitigate\noutlier noise in real-world scenarios. Specifically, we develop a LiDAR-based\nlocalization framework that seamlessly integrates a prior global map with\ndynamic real-time local maps, enabling robust localization in unbounded and\nchanging environments. Extensive real-world experiments in ultra unbounded port\nthat involve 2,856 hours of operational data across 32 Intelligent Guided\nVehicles (IGVs) are conducted and reported in this study. The results attained\ndemonstrate that our system outperforms other state-of-the-art LiDAR\nlocalization systems in large-scale changing outdoor environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDuLoc\u7684LiDAR\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u7ed3\u5408LiDAR-\u60ef\u6027\u91cc\u7a0b\u8ba1\u4e0e\u79bb\u7ebf\u5730\u56fe\u5b9a\u4f4d\uff0c\u901a\u8fc7\u6052\u5b9a\u901f\u5ea6\u8fd0\u52a8\u6a21\u578b\u51cf\u5c11\u566a\u58f0\uff0c\u5b9e\u73b0\u4e86\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u9ad8\u7cbe\u5ea6\u5b9a\u4f4d\u3002", "motivation": "\u73b0\u6709LiDAR\u5b9a\u4f4d\u65b9\u6cd5\u5728\u91cd\u590d\u6027\u3001\u7cbe\u5ea6\u548c\u73af\u5883\u9002\u5e94\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5728\u957f\u671f\u73af\u5883\u53d8\u5316\u4e0b\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51faDuLoc\u65b9\u6cd5\uff0c\u7d27\u5bc6\u8026\u5408LiDAR-\u60ef\u6027\u91cc\u7a0b\u8ba1\u4e0e\u79bb\u7ebf\u5730\u56fe\u5b9a\u4f4d\uff0c\u5f15\u5165\u6052\u5b9a\u901f\u5ea6\u8fd0\u52a8\u6a21\u578b\uff0c\u5e76\u6574\u5408\u5168\u5c40\u5730\u56fe\u4e0e\u5b9e\u65f6\u5c40\u90e8\u5730\u56fe\u3002", "result": "\u5728\u8d85\u5927\u89c4\u6a21\u6e2f\u53e3\u73af\u5883\u4e2d\u8fdb\u884c\u4e862856\u5c0f\u65f6\u7684\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660eDuLoc\u4f18\u4e8e\u5176\u4ed6\u5148\u8fdbLiDAR\u5b9a\u4f4d\u7cfb\u7edf\u3002", "conclusion": "DuLoc\u5728\u52a8\u6001\u548c\u5927\u89c4\u6a21\u5ba4\u5916\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2507.23773", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.23773", "abs": "https://arxiv.org/abs/2507.23773", "authors": ["Mingkai Deng", "Jinyu Hou", "Yilin Shen", "Hongxia Jin", "Graham Neubig", "Zhiting Hu", "Eric Xing"], "title": "SimuRA: Towards General Goal-Oriented Agent via Simulative Reasoning Architecture with LLM-Based World Model", "comment": null, "summary": "AI agents built on large language models (LLMs) hold enormous promise, but\ncurrent practice focuses on a one-task-one-agent approach, which not only falls\nshort of scalability and generality, but also suffers from the fundamental\nlimitations of autoregressive LLMs. On the other hand, humans are general\nagents who reason by mentally simulating the outcomes of their actions and\nplans. Moving towards a more general and powerful AI agent, we introduce\nSimuRA, a goal-oriented architecture for generalized agentic reasoning. Based\non a principled formulation of optimal agent in any environment, \\modelname\novercomes the limitations of autoregressive reasoning by introducing a world\nmodel for planning via simulation. The generalized world model is implemented\nusing LLM, which can flexibly plan in a wide range of environments using the\nconcept-rich latent space of natural language. Experiments on difficult web\nbrowsing tasks show that \\modelname improves the success of flight search from\n0\\% to 32.2\\%. World-model-based planning, in particular, shows consistent\nadvantage of up to 124\\% over autoregressive planning, demonstrating the\nadvantage of world model simulation as a reasoning paradigm. We are excited\nabout the possibility for training a single, general agent model based on LLMs\nthat can act superintelligently in all environments. To start, we make SimuRA,\na web-browsing agent built on \\modelname with pretrained LLMs, available as a\nresearch demo for public testing.", "AI": {"tldr": "SimuRA\u662f\u4e00\u79cd\u57fa\u4e8e\u4e16\u754c\u6a21\u578b\u7684\u901a\u7528AI\u4ee3\u7406\u67b6\u6784\uff0c\u901a\u8fc7\u6a21\u62df\u89c4\u5212\u514b\u670d\u81ea\u56de\u5f52LLM\u7684\u5c40\u9650\u6027\uff0c\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524dAI\u4ee3\u7406\u591a\u4e3a\u5355\u4e00\u4efb\u52a1\u8bbe\u8ba1\uff0c\u7f3a\u4e4f\u901a\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u4e14\u53d7\u9650\u4e8e\u81ea\u56de\u5f52LLM\u7684\u56fa\u6709\u7f3a\u9677\u3002\u4eba\u7c7b\u901a\u8fc7\u5fc3\u7406\u6a21\u62df\u8fdb\u884c\u63a8\u7406\uff0c\u542f\u53d1\u7814\u7a76\u8005\u63d0\u51fa\u66f4\u901a\u7528\u7684\u4ee3\u7406\u67b6\u6784\u3002", "method": "SimuRA\u57fa\u4e8e\u73af\u5883\u4e2d\u6700\u4f18\u4ee3\u7406\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5229\u7528LLM\u5b9e\u73b0\u901a\u7528\u4e16\u754c\u6a21\u578b\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6f5c\u5728\u7a7a\u95f4\u8fdb\u884c\u7075\u6d3b\u89c4\u5212\u3002", "result": "\u5728\u7f51\u9875\u6d4f\u89c8\u4efb\u52a1\u4e2d\uff0cSimuRA\u5c06\u822a\u73ed\u641c\u7d22\u6210\u529f\u7387\u4ece0%\u63d0\u5347\u81f332.2%\uff0c\u57fa\u4e8e\u4e16\u754c\u6a21\u578b\u7684\u89c4\u5212\u6bd4\u81ea\u56de\u5f52\u89c4\u5212\u4f18\u52bf\u9ad8\u8fbe124%\u3002", "conclusion": "SimuRA\u5c55\u793a\u4e86\u4e16\u754c\u6a21\u578b\u6a21\u62df\u4f5c\u4e3a\u63a8\u7406\u8303\u5f0f\u7684\u4f18\u52bf\uff0c\u4e3a\u8bad\u7ec3\u5355\u4e00\u901a\u7528LLM\u4ee3\u7406\u63d0\u4f9b\u4e86\u53ef\u80fd\u6027\uff0c\u5e76\u5df2\u53d1\u5e03\u4e3a\u516c\u5f00\u7814\u7a76\u6f14\u793a\u3002"}}
{"id": "2507.23677", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.23677", "abs": "https://arxiv.org/abs/2507.23677", "authors": ["Xiaohan Li", "Ziren Gong", "Fabio Tosi", "Matteo Poggi", "Stefano Mattoccia", "Dong Liu", "Jun Wu"], "title": "Stereo 3D Gaussian Splatting SLAM for Outdoor Urban Scenes", "comment": null, "summary": "3D Gaussian Splatting (3DGS) has recently gained popularity in SLAM\napplications due to its fast rendering and high-fidelity representation.\nHowever, existing 3DGS-SLAM systems have predominantly focused on indoor\nenvironments and relied on active depth sensors, leaving a gap for large-scale\noutdoor applications. We present BGS-SLAM, the first binocular 3D Gaussian\nSplatting SLAM system designed for outdoor scenarios. Our approach uses only\nRGB stereo pairs without requiring LiDAR or active sensors. BGS-SLAM leverages\ndepth estimates from pre-trained deep stereo networks to guide 3D Gaussian\noptimization with a multi-loss strategy enhancing both geometric consistency\nand visual quality. Experiments on multiple datasets demonstrate that BGS-SLAM\nachieves superior tracking accuracy and mapping performance compared to other\n3DGS-based solutions in complex outdoor environments.", "AI": {"tldr": "BGS-SLAM\u662f\u9996\u4e2a\u57fa\u4e8e\u53cc\u76eeRGB\u56fe\u50cf\u76843D\u9ad8\u65af\u6cfc\u6e85SLAM\u7cfb\u7edf\uff0c\u4e13\u4e3a\u6237\u5916\u573a\u666f\u8bbe\u8ba1\uff0c\u65e0\u9700LiDAR\u6216\u4e3b\u52a8\u4f20\u611f\u5668\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u7684\u6df1\u5ea6\u7acb\u4f53\u7f51\u7edc\u548c\u591a\u635f\u5931\u7b56\u7565\u4f18\u53163D\u9ad8\u65af\u8868\u793a\u3002", "motivation": "\u73b0\u67093DGS-SLAM\u7cfb\u7edf\u4e3b\u8981\u9488\u5bf9\u5ba4\u5185\u73af\u5883\u5e76\u4f9d\u8d56\u4e3b\u52a8\u6df1\u5ea6\u4f20\u611f\u5668\uff0c\u7f3a\u4e4f\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u6237\u5916\u573a\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528\u53cc\u76eeRGB\u56fe\u50cf\uff0c\u7ed3\u5408\u9884\u8bad\u7ec3\u7684\u6df1\u5ea6\u7acb\u4f53\u7f51\u7edc\u4f30\u8ba1\u6df1\u5ea6\uff0c\u5e76\u901a\u8fc7\u591a\u635f\u5931\u7b56\u7565\u4f18\u53163D\u9ad8\u65af\u8868\u793a\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cBGS-SLAM\u5728\u590d\u6742\u6237\u5916\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u8ddf\u8e2a\u7cbe\u5ea6\u548c\u5efa\u56fe\u6027\u80fd\u3002", "conclusion": "BGS-SLAM\u586b\u8865\u4e86\u6237\u59163DGS-SLAM\u7684\u7a7a\u767d\uff0c\u5c55\u793a\u4e86\u65e0\u9700\u4e3b\u52a8\u4f20\u611f\u5668\u7684\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2507.23682", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.23682", "abs": "https://arxiv.org/abs/2507.23682", "authors": ["Xiaoyu Chen", "Hangxing Wei", "Pushi Zhang", "Chuheng Zhang", "Kaixin Wang", "Yanjiang Guo", "Rushuai Yang", "Yucen Wang", "Xinquan Xiao", "Li Zhao", "Jianyu Chen", "Jiang Bian"], "title": "villa-X: Enhancing Latent Action Modeling in Vision-Language-Action Models", "comment": "Project page: https://aka.ms/villa-x", "summary": "Visual-Language-Action (VLA) models have emerged as a popular paradigm for\nlearning robot manipulation policies that can follow language instructions and\ngeneralize to novel scenarios. Recent work has begun to explore the\nincorporation of latent actions, an abstract representation of visual change\nbetween two frames, into VLA pre-training. In this paper, we introduce villa-X,\na novel Visual-Language-Latent-Action (ViLLA) framework that advances latent\naction modeling for learning generalizable robot manipulation policies. Our\napproach improves both how latent actions are learned and how they are\nincorporated into VLA pre-training. Together, these contributions enable\nvilla-X to achieve superior performance across simulated environments including\nSIMPLER and LIBERO, as well as on two real-world robot setups including gripper\nand dexterous hand manipulation. We believe the ViLLA paradigm holds\nsignificant promise, and that our villa-X provides a strong foundation for\nfuture research.", "AI": {"tldr": "ViLLA-X\u662f\u4e00\u79cd\u65b0\u578b\u89c6\u89c9-\u8bed\u8a00-\u6f5c\u5728\u52a8\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u6539\u8fdb\u6f5c\u5728\u52a8\u4f5c\u7684\u5b66\u4e60\u548c\u6574\u5408\u65b9\u5f0f\uff0c\u63d0\u5347\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u66f4\u597d\u5730\u5c06\u6f5c\u5728\u52a8\u4f5c\uff08\u89c6\u89c9\u53d8\u5316\u7684\u62bd\u8c61\u8868\u793a\uff09\u6574\u5408\u5230\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u4e2d\uff0c\u4ee5\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u7684\u6cdb\u5316\u6027\u3002", "method": "\u63d0\u51faViLLA-X\u6846\u67b6\uff0c\u6539\u8fdb\u6f5c\u5728\u52a8\u4f5c\u7684\u5b66\u4e60\u548c\u9884\u8bad\u7ec3\u6574\u5408\u65b9\u5f0f\u3002", "result": "\u5728\u4eff\u771f\u73af\u5883\uff08SIMPLER\u548cLIBERO\uff09\u548c\u771f\u5b9e\u673a\u5668\u4eba\uff08\u5939\u722a\u548c\u7075\u5de7\u624b\u64cd\u4f5c\uff09\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "ViLLA-X\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\uff0c\u5c55\u793a\u4e86\u6f5c\u5728\u52a8\u4f5c\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.23698", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.23698", "abs": "https://arxiv.org/abs/2507.23698", "authors": ["Shaofei Cai", "Zhancun Mu", "Haiwen Xia", "Bowei Zhang", "Anji Liu", "Yitao Liang"], "title": "Scalable Multi-Task Reinforcement Learning for Generalizable Spatial Intelligence in Visuomotor Agents", "comment": null, "summary": "While Reinforcement Learning (RL) has achieved remarkable success in language\nmodeling, its triumph hasn't yet fully translated to visuomotor agents. A\nprimary challenge in RL models is their tendency to overfit specific tasks or\nenvironments, thereby hindering the acquisition of generalizable behaviors\nacross diverse settings. This paper provides a preliminary answer to this\nchallenge by demonstrating that RL-finetuned visuomotor agents in Minecraft can\nachieve zero-shot generalization to unseen worlds. Specifically, we explore\nRL's potential to enhance generalizable spatial reasoning and interaction\ncapabilities in 3D worlds. To address challenges in multi-task RL\nrepresentation, we analyze and establish cross-view goal specification as a\nunified multi-task goal space for visuomotor policies. Furthermore, to overcome\nthe significant bottleneck of manual task design, we propose automated task\nsynthesis within the highly customizable Minecraft environment for large-scale\nmulti-task RL training, and we construct an efficient distributed RL framework\nto support this. Experimental results show RL significantly boosts interaction\nsuccess rates by $4\\times$ and enables zero-shot generalization of spatial\nreasoning across diverse environments, including real-world settings. Our\nfindings underscore the immense potential of RL training in 3D simulated\nenvironments, especially those amenable to large-scale task generation, for\nsignificantly advancing visuomotor agents' spatial reasoning.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86RL\u5728\u89c6\u89c9\u8fd0\u52a8\u4ee3\u7406\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u901a\u8fc7\u5728Minecraft\u4e2d\u5b9e\u73b0\u96f6\u6837\u672c\u6cdb\u5316\uff0c\u5c55\u793a\u4e86RL\u57283D\u73af\u5883\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "RL\u5728\u8bed\u8a00\u5efa\u6a21\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u89c6\u89c9\u8fd0\u52a8\u4ee3\u7406\u4e2d\u5c1a\u672a\u5b8c\u5168\u5b9e\u73b0\u6cdb\u5316\u80fd\u529b\uff0c\u4e3b\u8981\u6311\u6218\u662f\u6a21\u578b\u5bb9\u6613\u8fc7\u62df\u5408\u7279\u5b9a\u4efb\u52a1\u6216\u73af\u5883\u3002", "method": "\u63d0\u51fa\u8de8\u89c6\u56fe\u76ee\u6807\u89c4\u8303\u4f5c\u4e3a\u7edf\u4e00\u7684\u591a\u4efb\u52a1\u76ee\u6807\u7a7a\u95f4\uff0c\u5e76\u5728Minecraft\u4e2d\u81ea\u52a8\u5316\u4efb\u52a1\u5408\u6210\u4ee5\u652f\u6301\u5927\u89c4\u6a21\u591a\u4efb\u52a1RL\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u8868\u660eRL\u5c06\u4ea4\u4e92\u6210\u529f\u7387\u63d0\u53474\u500d\uff0c\u5e76\u5b9e\u73b0\u96f6\u6837\u672c\u6cdb\u5316\uff0c\u5305\u62ec\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u3002", "conclusion": "RL\u57283D\u6a21\u62df\u73af\u5883\u4e2d\u7684\u8bad\u7ec3\u6f5c\u529b\u5de8\u5927\uff0c\u5c24\u5176\u662f\u652f\u6301\u5927\u89c4\u6a21\u4efb\u52a1\u751f\u6210\u7684\u73af\u5883\uff0c\u53ef\u663e\u8457\u63d0\u5347\u89c6\u89c9\u8fd0\u52a8\u4ee3\u7406\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2507.23719", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.23719", "abs": "https://arxiv.org/abs/2507.23719", "authors": ["Parker McDonnell", "Lingsheng Meng", "Hari Krishna Hariprasad", "Alexander Hedrick", "Eduardo Miscles", "Samuel Gilinsky", "Jean-Michel Mongeau", "Kaushik Jayaram"], "title": "Design of a bioinspired robophysical antenna for insect-scale tactile perception and navigation", "comment": null, "summary": "The American cockroach (Periplaneta americana) uses its soft antennae to\nguide decision making by extracting rich tactile information from tens of\nthousands of distributed mechanosensors. Although tactile sensors enable\nrobust, autonomous perception and navigation in natural systems, replicating\nthese capabilities in insect-scale robots remains challenging due to stringent\nsize, weight, and power constraints that limit existing sensor technologies. To\novercome these limitations, we introduce CITRAS (Cockroach Inspired Tactile\nRobotic Antenna Sensor), a bioinspired, multi-segmented, compliant laminate\nsensor with embedded capacitive angle sensors. CITRAS is compact (73.7x15.6x2.1\nmm), lightweight (491 mg), and low-power (32 mW), enabling seamless integration\nwith miniature robotic platforms. The segmented compliant structure passively\nbends in response to environmental stimuli, achieving accurate hinge angle\nmeasurements with maximum errors of just 0.79 degree (quasistatic bending) and\n3.58 degree (dynamic bending). Experimental evaluations demonstrate CITRAS'\nmultifunctional tactile perception capabilities: predicting base-to-tip\ndistances with 7.75 % error, estimating environmental gap widths with 6.73 %\nerror, and distinguishing surface textures through differential sensor\nresponse. The future integration of this bioinspired tactile antenna in\ninsect-scale robots addresses critical sensing gaps, promising enhanced\nautonomous exploration, obstacle avoidance, and environmental mapping in\ncomplex, confined environments.", "AI": {"tldr": "\u7814\u7a76\u4eba\u5458\u53d7\u7f8e\u6d32\u87d1\u8782\u89e6\u89d2\u7684\u542f\u53d1\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u540d\u4e3aCITRAS\u7684\u8f7b\u91cf\u5316\u3001\u4f4e\u529f\u8017\u89e6\u89c9\u4f20\u611f\u5668\uff0c\u7528\u4e8e\u5fae\u578b\u673a\u5668\u4eba\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u73af\u5883\u611f\u77e5\u548c\u5bfc\u822a\u529f\u80fd\u3002", "motivation": "\u5fae\u578b\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u611f\u77e5\u548c\u5bfc\u822a\u53d7\u9650\u4e8e\u73b0\u6709\u4f20\u611f\u5668\u7684\u5c3a\u5bf8\u3001\u91cd\u91cf\u548c\u529f\u8017\uff0c\u800c\u751f\u7269\u89e6\u89d2\u7684\u9ad8\u6548\u89e6\u89c9\u611f\u77e5\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u63d0\u4f9b\u4e86\u7075\u611f\u3002", "method": "\u901a\u8fc7\u4eff\u751f\u8bbe\u8ba1\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u591a\u6bb5\u5f0f\u3001\u67d4\u6027\u7684\u5c42\u538b\u4f20\u611f\u5668\uff08CITRAS\uff09\uff0c\u5185\u7f6e\u7535\u5bb9\u5f0f\u89d2\u5ea6\u4f20\u611f\u5668\uff0c\u80fd\u591f\u5728\u88ab\u52a8\u5f2f\u66f2\u65f6\u7cbe\u786e\u6d4b\u91cf\u73af\u5883\u523a\u6fc0\u3002", "result": "CITRAS\u5728\u9759\u6001\u548c\u52a8\u6001\u5f2f\u66f2\u4e0b\u7684\u89d2\u5ea6\u6d4b\u91cf\u8bef\u5dee\u5206\u522b\u4e3a0.79\u5ea6\u548c3.58\u5ea6\uff0c\u5e76\u80fd\u9884\u6d4b\u8ddd\u79bb\u3001\u4f30\u8ba1\u95f4\u9699\u5bbd\u5ea6\u548c\u533a\u5206\u8868\u9762\u7eb9\u7406\uff0c\u8bef\u5dee\u7387\u4f4e\u3002", "conclusion": "CITRAS\u4e3a\u5fae\u578b\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u591a\u529f\u80fd\u89e6\u89c9\u611f\u77e5\u80fd\u529b\uff0c\u6709\u671b\u63d0\u5347\u5176\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u63a2\u7d22\u548c\u907f\u969c\u80fd\u529b\u3002"}}
{"id": "2507.23735", "categories": ["cs.RO", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.23735", "abs": "https://arxiv.org/abs/2507.23735", "authors": ["Markus Buchholz", "Ignacio Carlucho", "Michele Grimaldi", "Yvan R. Petillot"], "title": "Distributed AI Agents for Cognitive Underwater Robot Autonomy", "comment": null, "summary": "Achieving robust cognitive autonomy in robots navigating complex,\nunpredictable environments remains a fundamental challenge in robotics. This\npaper presents Underwater Robot Self-Organizing Autonomy (UROSA), a\ngroundbreaking architecture leveraging distributed Large Language Model AI\nagents integrated within the Robot Operating System 2 (ROS 2) framework to\nenable advanced cognitive capabilities in Autonomous Underwater Vehicles. UROSA\ndecentralises cognition into specialised AI agents responsible for multimodal\nperception, adaptive reasoning, dynamic mission planning, and real-time\ndecision-making. Central innovations include flexible agents dynamically\nadapting their roles, retrieval-augmented generation utilising vector databases\nfor efficient knowledge management, reinforcement learning-driven behavioural\noptimisation, and autonomous on-the-fly ROS 2 node generation for runtime\nfunctional extensibility. Extensive empirical validation demonstrates UROSA's\npromising adaptability and reliability through realistic underwater missions in\nsimulation and real-world deployments, showing significant advantages over\ntraditional rule-based architectures in handling unforeseen scenarios,\nenvironmental uncertainties, and novel mission objectives. This work not only\nadvances underwater autonomy but also establishes a scalable, safe, and\nversatile cognitive robotics framework capable of generalising to a diverse\narray of real-world applications.", "AI": {"tldr": "UROSA\u662f\u4e00\u79cd\u57fa\u4e8eROS 2\u6846\u67b6\u7684\u5206\u5e03\u5f0fAI\u67b6\u6784\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u611f\u77e5\u3001\u81ea\u9002\u5e94\u63a8\u7406\u548c\u52a8\u6001\u4efb\u52a1\u89c4\u5212\uff0c\u63d0\u5347\u6c34\u4e0b\u673a\u5668\u4eba\u7684\u8ba4\u77e5\u81ea\u4e3b\u6027\u3002", "motivation": "\u89e3\u51b3\u590d\u6742\u4e0d\u53ef\u9884\u6d4b\u73af\u5883\u4e2d\u673a\u5668\u4eba\u8ba4\u77e5\u81ea\u4e3b\u6027\u7684\u6311\u6218\u3002", "method": "\u5229\u7528\u5206\u5e03\u5f0f\u5927\u578b\u8bed\u8a00\u6a21\u578bAI\u4ee3\u7406\uff0c\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u3001\u5f3a\u5316\u5b66\u4e60\u548c\u52a8\u6001ROS 2\u8282\u70b9\u751f\u6210\u3002", "result": "\u5728\u4eff\u771f\u548c\u5b9e\u9645\u90e8\u7f72\u4e2d\u8868\u73b0\u51fa\u4f18\u4e8e\u4f20\u7edf\u89c4\u5219\u67b6\u6784\u7684\u9002\u5e94\u6027\u548c\u53ef\u9760\u6027\u3002", "conclusion": "UROSA\u4e0d\u4ec5\u63a8\u52a8\u6c34\u4e0b\u81ea\u4e3b\u6027\u53d1\u5c55\uff0c\u8fd8\u4e3a\u901a\u7528\u8ba4\u77e5\u673a\u5668\u4eba\u6846\u67b6\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u5b89\u5168\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
