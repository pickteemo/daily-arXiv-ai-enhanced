{"id": "2508.10144", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.10144", "abs": "https://arxiv.org/abs/2508.10144", "authors": ["Xu Ma", "Jiajie Zhang", "Fujing Xie", "S\u00f6ren Schwertfeger"], "title": "WiFi-based Global Localization in Large-Scale Environments Leveraging Structural Priors from osmAG", "comment": null, "summary": "Global localization is essential for autonomous robotics, especially in\nindoor environments where the GPS signal is denied. We propose a novel\nWiFi-based localization framework that leverages ubiquitous wireless\ninfrastructure and the OpenStreetMap Area Graph (osmAG) for large-scale indoor\nenvironments. Our approach integrates signal propagation modeling with osmAG's\ngeometric and topological priors. In the offline phase, an iterative\noptimization algorithm localizes WiFi Access Points (APs) by modeling wall\nattenuation, achieving a mean localization error of 3.79 m (35.3\\% improvement\nover trilateration). In the online phase, real-time robot localization uses the\naugmented osmAG map, yielding a mean error of 3.12 m in fingerprinted areas\n(8.77\\% improvement over KNN fingerprinting) and 3.83 m in non-fingerprinted\nareas (81.05\\% improvement). Comparison with a fingerprint-based method shows\nthat our approach is much more space efficient and achieves superior\nlocalization accuracy, especially for positions where no fingerprint data are\navailable. Validated across a complex 11,025 &m^2& multi-floor environment,\nthis framework offers a scalable, cost-effective solution for indoor robotic\nlocalization, solving the kidnapped robot problem. The code and dataset are\navailable at https://github.com/XuMa369/osmag-wifi-localization.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eWiFi\u548cOpenStreetMap Area Graph\uff08osmAG\uff09\u7684\u5ba4\u5185\u5b9a\u4f4d\u6846\u67b6\uff0c\u7ed3\u5408\u4fe1\u53f7\u4f20\u64ad\u5efa\u6a21\u4e0e\u51e0\u4f55\u62d3\u6251\u5148\u9a8c\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u7a7a\u95f4\u6548\u7387\u3002", "motivation": "\u89e3\u51b3GPS\u4fe1\u53f7\u7f3a\u5931\u7684\u5ba4\u5185\u73af\u5883\u4e2d\u673a\u5668\u4eba\u5168\u5c40\u5b9a\u4f4d\u95ee\u9898\uff0c\u5229\u7528\u73b0\u6709WiFi\u57fa\u7840\u8bbe\u65bd\u548cosmAG\u63d0\u4f9b\u4f4e\u6210\u672c\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u79bb\u7ebf\u9636\u6bb5\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u7b97\u6cd5\u5b9a\u4f4dWiFi\u63a5\u5165\u70b9\uff08AP\uff09\uff0c\u5728\u7ebf\u9636\u6bb5\u5229\u7528\u589e\u5f3aosmAG\u5730\u56fe\u8fdb\u884c\u5b9e\u65f6\u5b9a\u4f4d\uff0c\u7ed3\u5408\u4fe1\u53f7\u4f20\u64ad\u6a21\u578b\u548c\u6307\u7eb9\u6570\u636e\u3002", "result": "\u79bb\u7ebf\u9636\u6bb5AP\u5b9a\u4f4d\u8bef\u5dee3.79\u7c73\uff08\u6bd4\u4e09\u8fb9\u6d4b\u91cf\u63d0\u534735.3%\uff09\uff0c\u5728\u7ebf\u9636\u6bb5\u6307\u7eb9\u533a\u57df\u8bef\u5dee3.12\u7c73\uff08\u6bd4KNN\u6307\u7eb9\u63d0\u53478.77%\uff09\uff0c\u975e\u6307\u7eb9\u533a\u57df\u8bef\u5dee3.83\u7c73\uff08\u63d0\u534781.05%\uff09\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u590d\u6742\u591a\u697c\u5c42\u73af\u5883\u4e2d\u9a8c\u8bc1\u6709\u6548\uff0c\u89e3\u51b3\u4e86\u7ed1\u67b6\u673a\u5668\u4eba\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u9ad8\u7cbe\u5ea6\u3001\u7a7a\u95f4\u9ad8\u6548\u7684\u5ba4\u5185\u5b9a\u4f4d\u65b9\u6848\u3002"}}
{"id": "2508.10203", "categories": ["cs.RO", "cs.SY", "eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2508.10203", "abs": "https://arxiv.org/abs/2508.10203", "authors": ["Matthew D. Osburn", "Cameron K. Peterson", "John L. Salmon"], "title": "Systematic Constraint Formulation and Collision-Free Trajectory Planning Using Space-Time Graphs of Convex Sets", "comment": "21 pages with references, 20 figures", "summary": "In this paper, we create optimal, collision-free, time-dependent trajectories\nthrough cluttered dynamic environments. The many spatial and temporal\nconstraints make finding an initial guess for a numerical solver difficult.\nGraphs of Convex Sets (GCS) and the recently developed Space-Time Graphs of\nConvex Sets formulation (ST-GCS) enable us to generate optimal minimum distance\ncollision-free trajectories without providing an initial guess to the solver.\nWe also explore the derivation of general GCS-compatible constraints and\ndocument an intuitive strategy for adapting general constraints to the\nframework. We show that ST-GCS produces equivalent trajectories to the standard\nGCS formulation when the environment is static. We then show ST-GCS operating\nin dynamic environments to find minimum distance collision-free trajectories.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u52a8\u6001\u6742\u4e71\u73af\u5883\u751f\u6210\u6700\u4f18\u65e0\u78b0\u649e\u65f6\u95f4\u4f9d\u8d56\u8f68\u8ff9\u7684\u65b9\u6cd5\uff0c\u5229\u7528ST-GCS\u6846\u67b6\u65e0\u9700\u521d\u59cb\u731c\u6d4b\u5373\u53ef\u6c42\u89e3\u3002", "motivation": "\u89e3\u51b3\u5728\u52a8\u6001\u73af\u5883\u4e2d\u56e0\u7a7a\u95f4\u548c\u65f6\u95f4\u7ea6\u675f\u5bfc\u81f4\u96be\u4ee5\u627e\u5230\u6570\u503c\u6c42\u89e3\u5668\u521d\u59cb\u731c\u6d4b\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528ST-GCS\u6846\u67b6\u751f\u6210\u6700\u4f18\u65e0\u78b0\u649e\u8f68\u8ff9\uff0c\u5e76\u63a2\u7d22\u901a\u7528GCS\u517c\u5bb9\u7ea6\u675f\u7684\u63a8\u5bfc\u4e0e\u9002\u914d\u7b56\u7565\u3002", "result": "ST-GCS\u5728\u9759\u6001\u73af\u5883\u4e2d\u4e0e\u6807\u51c6GCS\u7b49\u6548\uff0c\u5728\u52a8\u6001\u73af\u5883\u4e2d\u80fd\u751f\u6210\u6700\u5c0f\u8ddd\u79bb\u65e0\u78b0\u649e\u8f68\u8ff9\u3002", "conclusion": "ST-GCS\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u52a8\u6001\u73af\u5883\u4e2d\u7684\u8f68\u8ff9\u89c4\u5212\u95ee\u9898\uff0c\u65e0\u9700\u521d\u59cb\u731c\u6d4b\u5373\u53ef\u751f\u6210\u6700\u4f18\u89e3\u3002"}}
{"id": "2508.10269", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.10269", "abs": "https://arxiv.org/abs/2508.10269", "authors": ["Kejun Li", "Jeeseop Kim", "Maxime Brunet", "Marine P\u00e9triaux", "Yisong Yue", "Aaron D. Ames"], "title": "Hybrid Data-Driven Predictive Control for Robust and Reactive Exoskeleton Locomotion Synthesis", "comment": "8 pages; 8 figures", "summary": "Robust bipedal locomotion in exoskeletons requires the ability to dynamically\nreact to changes in the environment in real time. This paper introduces the\nhybrid data-driven predictive control (HDDPC) framework, an extension of the\ndata-enabled predictive control, that addresses these challenges by\nsimultaneously planning foot contact schedules and continuous domain\ntrajectories. The proposed framework utilizes a Hankel matrix-based\nrepresentation to model system dynamics, incorporating step-to-step (S2S)\ntransitions to enhance adaptability in dynamic environments. By integrating\ncontact scheduling with trajectory planning, the framework offers an efficient,\nunified solution for locomotion motion synthesis that enables robust and\nreactive walking through online replanning. We validate the approach on the\nAtalante exoskeleton, demonstrating improved robustness and adaptability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u6570\u636e\u9a71\u52a8\u7684\u9884\u6d4b\u63a7\u5236\u6846\u67b6\uff08HDDPC\uff09\uff0c\u7528\u4e8e\u5916\u9aa8\u9abc\u673a\u5668\u4eba\u7684\u52a8\u6001\u6b65\u6001\u89c4\u5212\uff0c\u7ed3\u5408\u63a5\u89e6\u8c03\u5ea6\u548c\u8f68\u8ff9\u89c4\u5212\uff0c\u63d0\u5347\u73af\u5883\u9002\u5e94\u6027\u3002", "motivation": "\u89e3\u51b3\u5916\u9aa8\u9abc\u673a\u5668\u4eba\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5b9e\u65f6\u53cd\u5e94\u7684\u9700\u6c42\uff0c\u63d0\u5347\u6b65\u6001\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u3002", "method": "\u91c7\u7528Hankel\u77e9\u9635\u5efa\u6a21\u7cfb\u7edf\u52a8\u529b\u5b66\uff0c\u7ed3\u5408\u6b65\u95f4\uff08S2S\uff09\u8f6c\u6362\uff0c\u5b9e\u73b0\u63a5\u89e6\u8c03\u5ea6\u4e0e\u8f68\u8ff9\u89c4\u5212\u7684\u7edf\u4e00\u3002", "result": "\u5728Atalante\u5916\u9aa8\u9abc\u4e0a\u9a8c\u8bc1\uff0c\u5c55\u793a\u4e86\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u3002", "conclusion": "HDDPC\u6846\u67b6\u4e3a\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5916\u9aa8\u9abc\u6b65\u6001\u89c4\u5212\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u7edf\u4e00\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.10333", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10333", "abs": "https://arxiv.org/abs/2508.10333", "authors": ["Wenxuan Song", "Ziyang Zhou", "Han Zhao", "Jiayi Chen", "Pengxiang Ding", "Haodong Yan", "Yuxin Huang", "Feilong Tang", "Donglin Wang", "Haoang Li"], "title": "ReconVLA: Reconstructive Vision-Language-Action Model as Effective Robot Perceiver", "comment": null, "summary": "Recent advances in Vision-Language-Action (VLA) models have enabled robotic\nagents to integrate multimodal understanding with action execution. However,\nour empirical analysis reveals that current VLAs struggle to allocate visual\nattention to target regions. Instead, visual attention is always dispersed. To\nguide the visual attention grounding on the correct target, we propose\nReconVLA, a reconstructive VLA model with an implicit grounding paradigm.\nConditioned on the model's visual outputs, a diffusion transformer aims to\nreconstruct the gaze region of the image, which corresponds to the target\nmanipulated objects. This process prompts the VLA model to learn fine-grained\nrepresentations and accurately allocate visual attention, thus effectively\nleveraging task-specific visual information and conducting precise\nmanipulation. Moreover, we curate a large-scale pretraining dataset comprising\nover 100k trajectories and 2 million data samples from open-source robotic\ndatasets, further boosting the model's generalization in visual reconstruction.\nExtensive experiments in simulation and the real world demonstrate the\nsuperiority of our implicit grounding method, showcasing its capabilities of\nprecise manipulation and generalization. Our project page is\nhttps://zionchow.github.io/ReconVLA/.", "AI": {"tldr": "ReconVLA\u63d0\u51fa\u4e86\u4e00\u79cd\u9690\u5f0f\u63a5\u5730\u8303\u5f0f\uff0c\u901a\u8fc7\u91cd\u6784\u76ee\u6807\u533a\u57df\u7684\u89c6\u89c9\u6ce8\u610f\u529b\uff0c\u63d0\u5347VLA\u6a21\u578b\u5728\u4efb\u52a1\u4e2d\u7684\u7cbe\u786e\u64cd\u4f5c\u80fd\u529b\u3002", "motivation": "\u5f53\u524dVLA\u6a21\u578b\u5728\u89c6\u89c9\u6ce8\u610f\u529b\u5206\u914d\u4e0a\u8868\u73b0\u5206\u6563\uff0c\u65e0\u6cd5\u51c6\u786e\u805a\u7126\u76ee\u6807\u533a\u57df\uff0c\u5f71\u54cd\u4e86\u4efb\u52a1\u6267\u884c\u6548\u679c\u3002", "method": "\u63d0\u51faReconVLA\uff0c\u5229\u7528\u6269\u6563\u53d8\u6362\u5668\u91cd\u6784\u56fe\u50cf\u4e2d\u7684\u6ce8\u89c6\u533a\u57df\uff0c\u5f15\u5bfc\u6a21\u578b\u5b66\u4e60\u7ec6\u7c92\u5ea6\u8868\u793a\u5e76\u51c6\u786e\u5206\u914d\u89c6\u89c9\u6ce8\u610f\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cReconVLA\u5728\u4eff\u771f\u548c\u73b0\u5b9e\u573a\u666f\u4e2d\u5747\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u7cbe\u786e\u64cd\u4f5c\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "ReconVLA\u901a\u8fc7\u9690\u5f0f\u63a5\u5730\u8303\u5f0f\u6709\u6548\u63d0\u5347\u4e86VLA\u6a21\u578b\u7684\u89c6\u89c9\u6ce8\u610f\u529b\u5206\u914d\u548c\u4efb\u52a1\u6267\u884c\u7cbe\u5ea6\u3002"}}
{"id": "2508.10047", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10047", "abs": "https://arxiv.org/abs/2508.10047", "authors": ["Ziyang Xiao", "Jingrong Xie", "Lilin Xu", "Shisi Guan", "Jingyan Zhu", "Xiongwei Han", "Xiaojin Fu", "WingYin Yu", "Han Wu", "Wei Shi", "Qingcan Kang", "Jiahui Duan", "Tao Zhong", "Mingxuan Yuan", "Jia Zeng", "Yuan Wang", "Gang Chen", "Dongxiang Zhang"], "title": "A Survey of Optimization Modeling Meets LLMs: Progress and Future Directions", "comment": null, "summary": "By virtue of its great utility in solving real-world problems, optimization\nmodeling has been widely employed for optimal decision-making across various\nsectors, but it requires substantial expertise from operations research\nprofessionals. With the advent of large language models (LLMs), new\nopportunities have emerged to automate the procedure of mathematical modeling.\nThis survey presents a comprehensive and timely review of recent advancements\nthat cover the entire technical stack, including data synthesis and fine-tuning\nfor the base model, inference frameworks, benchmark datasets, and performance\nevaluation. In addition, we conducted an in-depth analysis on the quality of\nbenchmark datasets, which was found to have a surprisingly high error rate. We\ncleaned the datasets and constructed a new leaderboard with fair performance\nevaluation in terms of base LLM model and datasets. We also build an online\nportal that integrates resources of cleaned datasets, code and paper repository\nto benefit the community. Finally, we identify limitations in current\nmethodologies and outline future research opportunities.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u81ea\u52a8\u5316\u6570\u5b66\u5efa\u6a21\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u5305\u62ec\u6570\u636e\u5408\u6210\u3001\u6a21\u578b\u5fae\u8c03\u3001\u63a8\u7406\u6846\u67b6\u3001\u57fa\u51c6\u6570\u636e\u96c6\u548c\u6027\u80fd\u8bc4\u4f30\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684\u516c\u5e73\u8bc4\u4f30\u6392\u884c\u699c\u548c\u5728\u7ebf\u8d44\u6e90\u95e8\u6237\u3002", "motivation": "\u4f18\u5316\u5efa\u6a21\u5728\u89e3\u51b3\u5b9e\u9645\u95ee\u9898\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u4f46\u9700\u8981\u5927\u91cf\u4e13\u4e1a\u77e5\u8bc6\u3002LLMs\u7684\u51fa\u73b0\u4e3a\u81ea\u52a8\u5316\u6570\u5b66\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u673a\u4f1a\u3002", "method": "\u7efc\u8ff0\u4e86\u6280\u672f\u6808\u7684\u5404\u4e2a\u65b9\u9762\uff0c\u5305\u62ec\u6570\u636e\u5408\u6210\u3001\u6a21\u578b\u5fae\u8c03\u3001\u63a8\u7406\u6846\u67b6\u7b49\uff0c\u5e76\u6e05\u7406\u4e86\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u6784\u5efa\u4e86\u65b0\u7684\u6392\u884c\u699c\u548c\u5728\u7ebf\u95e8\u6237\u3002", "result": "\u53d1\u73b0\u57fa\u51c6\u6570\u636e\u96c6\u9519\u8bef\u7387\u8f83\u9ad8\uff0c\u6e05\u7406\u540e\u6784\u5efa\u4e86\u516c\u5e73\u8bc4\u4f30\u7684\u6392\u884c\u699c\u548c\u8d44\u6e90\u95e8\u6237\u3002", "conclusion": "\u603b\u7ed3\u4e86\u5f53\u524d\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2508.10363", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.10363", "abs": "https://arxiv.org/abs/2508.10363", "authors": ["Donipolo Ghimire", "Aamodh Suresh", "Carlos Nieto-Granda", "Solmaz S. Kia"], "title": "BEASST: Behavioral Entropic Gradient based Adaptive Source Seeking for Mobile Robots", "comment": null, "summary": "This paper presents BEASST (Behavioral Entropic Gradient-based Adaptive\nSource Seeking for Mobile Robots), a novel framework for robotic source seeking\nin complex, unknown environments. Our approach enables mobile robots to\nefficiently balance exploration and exploitation by modeling normalized signal\nstrength as a surrogate probability of source location. Building on Behavioral\nEntropy(BE) with Prelec's probability weighting function, we define an\nobjective function that adapts robot behavior from risk-averse to risk-seeking\nbased on signal reliability and mission urgency. The framework provides\ntheoretical convergence guarantees under unimodal signal assumptions and\npractical stability under bounded disturbances. Experimental validation across\nDARPA SubT and multi-room scenarios demonstrates that BEASST consistently\noutperforms state-of-the-art methods, achieving 15% reduction in path length\nand 20% faster source localization through intelligent uncertainty-driven\nnavigation that dynamically transitions between aggressive pursuit and cautious\nexploration.", "AI": {"tldr": "BEASST\u662f\u4e00\u79cd\u65b0\u578b\u673a\u5668\u4eba\u6e90\u641c\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u884c\u4e3a\u71b5\u548c\u6982\u7387\u52a0\u6743\u51fd\u6570\u52a8\u6001\u8c03\u6574\u673a\u5668\u4eba\u884c\u4e3a\uff0c\u5728\u590d\u6742\u672a\u77e5\u73af\u5883\u4e2d\u9ad8\u6548\u5e73\u8861\u63a2\u7d22\u4e0e\u5229\u7528\u3002", "motivation": "\u89e3\u51b3\u79fb\u52a8\u673a\u5668\u4eba\u5728\u672a\u77e5\u73af\u5883\u4e2d\u6e90\u641c\u7d22\u65f6\u63a2\u7d22\u4e0e\u5229\u7528\u7684\u5e73\u8861\u95ee\u9898\uff0c\u63d0\u9ad8\u641c\u7d22\u6548\u7387\u548c\u9002\u5e94\u6027\u3002", "method": "\u5229\u7528\u884c\u4e3a\u71b5\u548cPrelec\u6982\u7387\u52a0\u6743\u51fd\u6570\u5b9a\u4e49\u76ee\u6807\u51fd\u6570\uff0c\u52a8\u6001\u8c03\u6574\u673a\u5668\u4eba\u884c\u4e3a\uff08\u4ece\u98ce\u9669\u89c4\u907f\u5230\u98ce\u9669\u5bfb\u6c42\uff09\uff0c\u5e76\u7ed3\u5408\u7406\u8bba\u6536\u655b\u4fdd\u8bc1\u548c\u5b9e\u9645\u7a33\u5b9a\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cBEASST\u5728\u8def\u5f84\u957f\u5ea6\u51cf\u5c1115%\u548c\u6e90\u5b9a\u4f4d\u901f\u5ea6\u63d0\u9ad820%\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "BEASST\u901a\u8fc7\u667a\u80fd\u4e0d\u786e\u5b9a\u6027\u9a71\u52a8\u5bfc\u822a\uff0c\u5728\u590d\u6742\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u673a\u5668\u4eba\u6e90\u641c\u7d22\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.10108", "categories": ["cs.AI", "cs.CL", "I.2.7; I.2.6; E.0"], "pdf": "https://arxiv.org/pdf/2508.10108", "abs": "https://arxiv.org/abs/2508.10108", "authors": ["Sattvik Sahai", "Prasoon Goyal", "Michael Johnston", "Anna Gottardi", "Yao Lu", "Lucy Hu", "Luke Dai", "Shaohua Liu", "Samyuth Sagi", "Hangjie Shi", "Desheng Zhang", "Lavina Vaz", "Leslie Ball", "Maureen Murray", "Rahul Gupta", "Shankar Ananthakrishna"], "title": "Amazon Nova AI Challenge -- Trusted AI: Advancing secure, AI-assisted software development", "comment": "18 pages, 1st Proceedings of Amazon Nova AI Challenge (Trusted AI\n  2025)", "summary": "AI systems for software development are rapidly gaining prominence, yet\nsignificant challenges remain in ensuring their safety. To address this, Amazon\nlaunched the Trusted AI track of the Amazon Nova AI Challenge, a global\ncompetition among 10 university teams to drive advances in secure AI. In the\nchallenge, five teams focus on developing automated red teaming bots, while the\nother five create safe AI assistants. This challenge provides teams with a\nunique platform to evaluate automated red-teaming and safety alignment methods\nthrough head-to-head adversarial tournaments where red teams have multi-turn\nconversations with the competing AI coding assistants to test their safety\nalignment. Along with this, the challenge provides teams with a feed of high\nquality annotated data to fuel iterative improvement. Throughout the challenge,\nteams developed state-of-the-art techniques, introducing novel approaches in\nreasoning-based safety alignment, robust model guardrails, multi-turn\njail-breaking, and efficient probing of large language models (LLMs). To\nsupport these efforts, the Amazon Nova AI Challenge team made substantial\nscientific and engineering investments, including building a custom baseline\ncoding specialist model for the challenge from scratch, developing a tournament\norchestration service, and creating an evaluation harness. This paper outlines\nthe advancements made by university teams and the Amazon Nova AI Challenge team\nin addressing the safety challenges of AI for software development,\nhighlighting this collaborative effort to raise the bar for AI safety.", "AI": {"tldr": "\u4e9a\u9a6c\u900aNova AI\u6311\u6218\u8d5b\u901a\u8fc7\u5bf9\u6297\u6027\u7ade\u8d5b\u63a8\u52a8AI\u5b89\u5168\u6280\u672f\u53d1\u5c55\uff0c\u5927\u5b66\u56e2\u961f\u5f00\u53d1\u4e86\u81ea\u52a8\u7ea2\u961f\u548c\u5b89\u5168AI\u52a9\u624b\uff0c\u5e76\u5f15\u5165\u521b\u65b0\u65b9\u6cd5\u63d0\u5347AI\u5b89\u5168\u6027\u3002", "motivation": "\u89e3\u51b3AI\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u5b89\u5168\u6027\u95ee\u9898\uff0c\u63a8\u52a8\u5b89\u5168AI\u6280\u672f\u7684\u53d1\u5c55\u3002", "method": "\u901a\u8fc7\u5bf9\u6297\u6027\u7ade\u8d5b\uff08\u7ea2\u961f\u4e0eAI\u52a9\u624b\u5bf9\u8bdd\uff09\u548c\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u56e2\u961f\u5f00\u53d1\u4e86\u63a8\u7406\u5b89\u5168\u5bf9\u9f50\u3001\u6a21\u578b\u62a4\u680f\u7b49\u6280\u672f\u3002", "result": "\u56e2\u961f\u63d0\u51fa\u4e86\u521b\u65b0\u65b9\u6cd5\uff0c\u5982\u591a\u8f6e\u8d8a\u72f1\u548c\u9ad8\u6548\u63a2\u6d4bLLM\uff0c\u4e9a\u9a6c\u900a\u63d0\u4f9b\u4e86\u57fa\u7840\u8bbe\u65bd\u652f\u6301\u3002", "conclusion": "\u8be5\u6311\u6218\u8d5b\u63d0\u5347\u4e86AI\u5b89\u5168\u6027\uff0c\u5c55\u793a\u4e86\u534f\u4f5c\u52aa\u529b\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2508.10371", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.10371", "abs": "https://arxiv.org/abs/2508.10371", "authors": ["Wenqi Zheng", "Yutaka Arakawa"], "title": "Few-shot Vision-based Human Activity Recognition with MLLM-based Visual Reinforcement Learning", "comment": null, "summary": "Reinforcement learning in large reasoning models enables learning from\nfeedback on their outputs, making it particularly valuable in scenarios where\nfine-tuning data is limited. However, its application in multi-modal human\nactivity recognition (HAR) domains remains largely underexplored. Our work\nextends reinforcement learning to the human activity recognition domain with\nmultimodal large language models. By incorporating visual reinforcement\nlearning in the training process, the model's generalization ability on\nfew-shot recognition can be greatly improved. Additionally, visual\nreinforcement learning can enhance the model's reasoning ability and enable\nexplainable analysis in the inference stage. We name our few-shot human\nactivity recognition method with visual reinforcement learning FAVOR.\nSpecifically, our approach first utilizes a multimodal large language model\n(MLLM) to generate multiple candidate responses for the human activity image,\neach containing reasoning traces and final answers. These responses are then\nevaluated using reward functions, and the MLLM model is subsequently optimized\nusing the Group Relative Policy Optimization (GRPO) algorithm. In this way, the\nMLLM model can be adapted to human activity recognition with only a few\nsamples. Extensive experiments on four human activity recognition datasets and\nfive different settings demonstrate the superiority of the proposed method.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFAVOR\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u89c6\u89c9\u5f3a\u5316\u5b66\u4e60\u6269\u5c55\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5c11\u6837\u672c\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\uff08HAR\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u5728\u5c11\u6837\u672c\u6570\u636e\u573a\u666f\u4e0b\uff0c\u5f3a\u5316\u5b66\u4e60\u5728\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\u9886\u57df\u7684\u5e94\u7528\u5c1a\u672a\u5145\u5206\u63a2\u7d22\uff0c\u56e0\u6b64\u7814\u7a76\u5982\u4f55\u5229\u7528\u89c6\u89c9\u5f3a\u5316\u5b66\u4e60\u63d0\u5347\u6a21\u578b\u7684\u6027\u80fd\u3002", "method": "\u7ed3\u5408\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u751f\u6210\u5019\u9009\u54cd\u5e94\uff0c\u901a\u8fc7\u5956\u52b1\u51fd\u6570\u8bc4\u4f30\u5e76\u4f7f\u7528GRPO\u7b97\u6cd5\u4f18\u5316\u6a21\u578b\uff0c\u5b9e\u73b0\u5c11\u6837\u672c\u9002\u5e94\u3002", "result": "\u5728\u56db\u4e2aHAR\u6570\u636e\u96c6\u548c\u4e94\u79cd\u4e0d\u540c\u8bbe\u7f6e\u4e0b\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86FAVOR\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "FAVOR\u65b9\u6cd5\u901a\u8fc7\u89c6\u89c9\u5f3a\u5316\u5b66\u4e60\u6709\u6548\u63d0\u5347\u4e86\u5c11\u6837\u672c\u4eba\u7c7b\u6d3b\u52a8\u8bc6\u522b\u7684\u6027\u80fd\uff0c\u5e76\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u63a8\u7406\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2508.10143", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10143", "abs": "https://arxiv.org/abs/2508.10143", "authors": ["Alexandru-Andrei Avram", "Adrian Groza", "Alexandru Lecu"], "title": "MCP-Orchestrated Multi-Agent System for Automated Disinformation Detection", "comment": "8 pages + 1 page references, 5 figures, 4 tables, Registered for the\n  27th International Symposium on Symbolic and Numeric Algorithms for\n  Scientific Computing, 2025, Timisoara", "summary": "The large spread of disinformation across digital platforms creates\nsignificant challenges to information integrity. This paper presents a\nmulti-agent system that uses relation extraction to detect disinformation in\nnews articles, focusing on titles and short text snippets. The proposed Agentic\nAI system combines four agents: (i) a machine learning agent (logistic\nregression), (ii) a Wikipedia knowledge check agent (which relies on named\nentity recognition), (iii) a coherence detection agent (using LLM prompt\nengineering), and (iv) a web-scraped data analyzer that extracts relational\ntriplets for fact checking. The system is orchestrated via the Model Context\nProtocol (MCP), offering shared context and live learning across components.\nResults demonstrate that the multi-agent ensemble achieves 95.3% accuracy with\nan F1 score of 0.964, significantly outperforming individual agents and\ntraditional approaches. The weighted aggregation method, mathematically derived\nfrom individual agent misclassification rates, proves superior to algorithmic\nthreshold optimization. The modular architecture makes the system easily\nscalable, while also maintaining details of the decision processes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u901a\u8fc7\u5173\u7cfb\u63d0\u53d6\u68c0\u6d4b\u65b0\u95fb\u4e2d\u7684\u865a\u5047\u4fe1\u606f\uff0c\u7ed3\u5408\u56db\u79cd\u667a\u80fd\u4f53\uff0c\u5b9e\u73b0\u9ad8\u51c6\u786e\u7387\u548cF1\u5206\u6570\u3002", "motivation": "\u6570\u5b57\u5e73\u53f0\u4e0a\u865a\u5047\u4fe1\u606f\u7684\u5e7f\u6cdb\u4f20\u64ad\u5bf9\u4fe1\u606f\u5b8c\u6574\u6027\u6784\u6210\u91cd\u5927\u6311\u6218\uff0c\u9700\u8981\u9ad8\u6548\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u7cfb\u7edf\u5305\u542b\u56db\u79cd\u667a\u80fd\u4f53\uff1a\u673a\u5668\u5b66\u4e60\u4ee3\u7406\u3001\u7ef4\u57fa\u767e\u79d1\u77e5\u8bc6\u68c0\u67e5\u4ee3\u7406\u3001\u8fde\u8d2f\u6027\u68c0\u6d4b\u4ee3\u7406\u548c\u7f51\u7edc\u6570\u636e\u6293\u53d6\u5206\u6790\u4ee3\u7406\uff0c\u901a\u8fc7\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\uff08MCP\uff09\u534f\u8c03\u3002", "result": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u51c6\u786e\u7387\u8fbe95.3%\uff0cF1\u5206\u65700.964\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u6a21\u5757\u5316\u8bbe\u8ba1\u6613\u4e8e\u6269\u5c55\uff0c\u4e14\u51b3\u7b56\u8fc7\u7a0b\u900f\u660e\uff0c\u4e3a\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.10378", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.10378", "abs": "https://arxiv.org/abs/2508.10378", "authors": ["Yu Chen", "Shu Miao", "Chunyu Wu", "Jingsong Mu", "Bo OuYang", "Xiang Li"], "title": "A Semantic-Aware Framework for Safe and Intent-Integrative Assistance in Upper-Limb Exoskeletons", "comment": null, "summary": "Upper-limb exoskeletons are primarily designed to provide assistive support\nby accurately interpreting and responding to human intentions. In home-care\nscenarios, exoskeletons are expected to adapt their assistive configurations\nbased on the semantic information of the task, adjusting appropriately in\naccordance with the nature of the object being manipulated. However, existing\nsolutions often lack the ability to understand task semantics or\ncollaboratively plan actions with the user, limiting their generalizability. To\naddress this challenge, this paper introduces a semantic-aware framework that\nintegrates large language models into the task planning framework, enabling the\ndelivery of safe and intent-integrative assistance. The proposed approach\nbegins with the exoskeleton operating in transparent mode to capture the\nwearer's intent during object grasping. Once semantic information is extracted\nfrom the task description, the system automatically configures appropriate\nassistive parameters. In addition, a diffusion-based anomaly detector is used\nto continuously monitor the state of human-robot interaction and trigger\nreal-time replanning in response to detected anomalies. During task execution,\nonline trajectory refinement and impedance control are used to ensure safety\nand regulate human-robot interaction. Experimental results demonstrate that the\nproposed method effectively aligns with the wearer's cognition, adapts to\nsemantically varying tasks, and responds reliably to anomalies.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bed\u4e49\u611f\u77e5\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u5927\u8bed\u8a00\u6a21\u578b\u5230\u4efb\u52a1\u89c4\u5212\u4e2d\uff0c\u4f7f\u4e0a\u80a2\u5916\u9aa8\u9abc\u80fd\u6839\u636e\u4efb\u52a1\u8bed\u4e49\u8c03\u6574\u8f85\u52a9\u914d\u7f6e\uff0c\u63d0\u5347\u5b89\u5168\u6027\u548c\u610f\u56fe\u6574\u5408\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u4e0a\u80a2\u5916\u9aa8\u9abc\u7f3a\u4e4f\u5bf9\u4efb\u52a1\u8bed\u4e49\u7684\u7406\u89e3\u548c\u4e0e\u7528\u6237\u7684\u534f\u4f5c\u89c4\u5212\u80fd\u529b\uff0c\u9650\u5236\u4e86\u5176\u901a\u7528\u6027\u3002", "method": "\u6846\u67b6\u5305\u62ec\u900f\u660e\u6a21\u5f0f\u6355\u6349\u7528\u6237\u610f\u56fe\u3001\u8bed\u4e49\u4fe1\u606f\u63d0\u53d6\u3001\u6269\u6563\u5f02\u5e38\u68c0\u6d4b\u5668\u5b9e\u65f6\u76d1\u6d4b\u53ca\u5728\u7ebf\u8f68\u8ff9\u4f18\u5316\u4e0e\u963b\u6297\u63a7\u5236\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u9002\u5e94\u7528\u6237\u8ba4\u77e5\u3001\u8bed\u4e49\u4efb\u52a1\u53d8\u5316\uff0c\u5e76\u53ef\u9760\u54cd\u5e94\u5f02\u5e38\u3002", "conclusion": "\u8be5\u6846\u67b6\u63d0\u5347\u4e86\u5916\u9aa8\u9abc\u7684\u8bed\u4e49\u7406\u89e3\u548c\u534f\u4f5c\u80fd\u529b\uff0c\u4e3a\u5bb6\u5ead\u62a4\u7406\u573a\u666f\u63d0\u4f9b\u4e86\u66f4\u5b89\u5168\u7684\u8f85\u52a9\u652f\u6301\u3002"}}
{"id": "2508.10146", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10146", "abs": "https://arxiv.org/abs/2508.10146", "authors": ["Hana Derouiche", "Zaki Brahmi", "Haithem Mazeni"], "title": "Agentic AI Frameworks: Architectures, Protocols, and Design Challenges", "comment": null, "summary": "The emergence of Large Language Models (LLMs) has ushered in a transformative\nparadigm in artificial intelligence, Agentic AI, where intelligent agents\nexhibit goal-directed autonomy, contextual reasoning, and dynamic multi-agent\ncoordination. This paper provides a systematic review and comparative analysis\nof leading Agentic AI frameworks, including CrewAI, LangGraph, AutoGen,\nSemantic Kernel, Agno, Google ADK, and MetaGPT, evaluating their architectural\nprinciples, communication mechanisms, memory management, safety guardrails, and\nalignment with service-oriented computing paradigms. Furthermore, we identify\nkey limitations, emerging trends, and open challenges in the field. To address\nthe issue of agent communication, we conduct an in-depth analysis of protocols\nsuch as the Contract Net Protocol (CNP), Agent-to-Agent (A2A), Agent Network\nProtocol (ANP), and Agora. Our findings not only establish a foundational\ntaxonomy for Agentic AI systems but also propose future research directions to\nenhance scalability, robustness, and interoperability. This work serves as a\ncomprehensive reference for researchers and practitioners working to advance\nthe next generation of autonomous AI systems.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u56de\u987e\u5e76\u6bd4\u8f83\u4e86\u4e3b\u6d41Agentic AI\u6846\u67b6\uff0c\u5206\u6790\u4e86\u5176\u67b6\u6784\u3001\u901a\u4fe1\u673a\u5236\u3001\u5185\u5b58\u7ba1\u7406\u7b49\uff0c\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u63a2\u7d22Agentic AI\u6846\u67b6\u7684\u53d1\u5c55\u73b0\u72b6\u4e0e\u6311\u6218\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u81ea\u4e3bAI\u7cfb\u7edf\u63d0\u4f9b\u53c2\u8003\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u56de\u987e\u548c\u6bd4\u8f83\u5206\u6790\uff0c\u8bc4\u4f30\u591a\u4e2a\u6846\u67b6\u7684\u67b6\u6784\u539f\u5219\u3001\u901a\u4fe1\u534f\u8bae\u7b49\u3002", "result": "\u5efa\u7acb\u4e86Agentic AI\u7cfb\u7edf\u7684\u57fa\u7840\u5206\u7c7b\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u672c\u6587\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u4ece\u4e1a\u8005\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u53c2\u8003\uff0c\u63a8\u52a8\u81ea\u4e3bAI\u7cfb\u7edf\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.10398", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.10398", "abs": "https://arxiv.org/abs/2508.10398", "authors": ["Wei Gao", "Jie Zhang", "Mingle Zhao", "Zhiyuan Zhang", "Shu Kong", "Maani Ghaffari", "Dezhen Song", "Cheng-Zhong Xu", "Hui Kong"], "title": "Super LiDAR Reflectance for Robotic Perception", "comment": null, "summary": "Conventionally, human intuition often defines vision as a modality of passive\noptical sensing, while active optical sensing is typically regarded as\nmeasuring rather than the default modality of vision. However, the situation\nnow changes: sensor technologies and data-driven paradigms empower active\noptical sensing to redefine the boundaries of vision, ushering in a new era of\nactive vision. Light Detection and Ranging (LiDAR) sensors capture reflectance\nfrom object surfaces, which remains invariant under varying illumination\nconditions, showcasing significant potential in robotic perception tasks such\nas detection, recognition, segmentation, and Simultaneous Localization and\nMapping (SLAM). These applications often rely on dense sensing capabilities,\ntypically achieved by high-resolution, expensive LiDAR sensors. A key challenge\nwith low-cost LiDARs lies in the sparsity of scan data, which limits their\nbroader application. To address this limitation, this work introduces an\ninnovative framework for generating dense LiDAR reflectance images from sparse\ndata, leveraging the unique attributes of non-repeating scanning LiDAR\n(NRS-LiDAR). We tackle critical challenges, including reflectance calibration\nand the transition from static to dynamic scene domains, facilitating the\nreconstruction of dense reflectance images in real-world settings. The key\ncontributions of this work include a comprehensive dataset for LiDAR\nreflectance image densification, a densification network tailored for\nNRS-LiDAR, and diverse applications such as loop closure and traffic lane\ndetection using the generated dense reflectance images.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u7a00\u758fLiDAR\u6570\u636e\u751f\u6210\u5bc6\u96c6\u53cd\u5c04\u56fe\u50cf\u7684\u65b0\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u4f4e\u6210\u672cLiDAR\u56e0\u6570\u636e\u7a00\u758f\u6027\u53d7\u9650\u7684\u95ee\u9898\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u5728\u673a\u5668\u4eba\u611f\u77e5\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u4f20\u7edf\u4e0a\uff0c\u4e3b\u52a8\u5149\u5b66\u4f20\u611f\u672a\u88ab\u89c6\u4e3a\u4e3b\u6d41\u89c6\u89c9\u6a21\u6001\uff0c\u4f46\u968f\u7740\u6280\u672f\u8fdb\u6b65\uff0cLiDAR\u7b49\u4f20\u611f\u5668\u4e3a\u4e3b\u52a8\u89c6\u89c9\u5f00\u8f9f\u4e86\u65b0\u9886\u57df\u3002\u7136\u800c\uff0c\u4f4e\u6210\u672cLiDAR\u7684\u6570\u636e\u7a00\u758f\u6027\u9650\u5236\u4e86\u5176\u5e7f\u6cdb\u5e94\u7528\u3002", "method": "\u5229\u7528\u975e\u91cd\u590d\u626b\u63cfLiDAR\uff08NRS-LiDAR\uff09\u7684\u7279\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u6846\u67b6\uff0c\u5305\u62ec\u53cd\u5c04\u7387\u6821\u51c6\u548c\u4ece\u9759\u6001\u5230\u52a8\u6001\u573a\u666f\u7684\u8f6c\u6362\uff0c\u4ee5\u751f\u6210\u5bc6\u96c6\u53cd\u5c04\u56fe\u50cf\u3002", "result": "\u6784\u5efa\u4e86\u7528\u4e8eLiDAR\u53cd\u5c04\u56fe\u50cf\u5bc6\u96c6\u5316\u7684\u6570\u636e\u96c6\uff0c\u5f00\u53d1\u4e86\u9488\u5bf9NRS-LiDAR\u7684\u5bc6\u96c6\u5316\u7f51\u7edc\uff0c\u5e76\u5728\u73af\u8def\u95ed\u5408\u548c\u4ea4\u901a\u8f66\u9053\u68c0\u6d4b\u7b49\u5e94\u7528\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6548\u679c\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u4f4e\u6210\u672cLiDAR\u6570\u636e\u7a00\u758f\u6027\u95ee\u9898\uff0c\u6269\u5c55\u4e86\u5176\u5728\u673a\u5668\u4eba\u611f\u77e5\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u8303\u56f4\uff0c\u4e3a\u4e3b\u52a8\u89c6\u89c9\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2508.10152", "categories": ["cs.AI", "I.2.7; H.3.3"], "pdf": "https://arxiv.org/pdf/2508.10152", "abs": "https://arxiv.org/abs/2508.10152", "authors": ["Doaa Allabadi", "Kyle Bradbury", "Jordan M. Malof"], "title": "Improving and Evaluating Open Deep Research Agents", "comment": "8 pages, 2 figures, 2 tables", "summary": "We focus here on Deep Research Agents (DRAs), which are systems that can take\na natural language prompt from a user, and then autonomously search for, and\nutilize, internet-based content to address the prompt. Recent DRAs have\ndemonstrated impressive capabilities on public benchmarks however, recent\nresearch largely involves proprietary closed-source systems. At the time of\nthis work, we only found one open-source DRA, termed Open Deep Research (ODR).\nIn this work we adapt the challenging recent BrowseComp benchmark to compare\nODR to existing proprietary systems. We propose BrowseComp-Small (BC-Small),\ncomprising a subset of BrowseComp, as a more computationally-tractable DRA\nbenchmark for academic labs. We benchmark ODR and two other proprietary systems\non BC-Small: one system from Anthropic and one system from Google. We find that\nall three systems achieve 0% accuracy on the test set of 60 questions. We\nintroduce three strategic improvements to ODR, resulting in the ODR+ model,\nwhich achieves a state-of-the-art 10% success rate on BC-Small among both\nclosed-source and open-source systems. We report ablation studies indicating\nthat all three of our improvements contributed to the success of ODR+.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5f00\u6e90\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\uff08ODR\uff09\u4e0e\u4e13\u6709\u7cfb\u7edf\u5728BrowseComp-Small\u57fa\u51c6\u4e0a\u7684\u8868\u73b0\uff0c\u901a\u8fc7\u6539\u8fdbODR\u63d0\u51fa\u4e86ODR+\u6a21\u578b\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u7cfb\u7edf\u3002", "motivation": "\u5f53\u524d\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\uff08DRA\uff09\u591a\u4e3a\u95ed\u6e90\u7cfb\u7edf\uff0c\u7f3a\u4e4f\u5f00\u6e90\u9009\u62e9\uff0c\u56e0\u6b64\u7814\u7a76\u5f00\u6e90DRA\u7684\u6027\u80fd\u548c\u6539\u8fdb\u6f5c\u529b\u3002", "method": "\u901a\u8fc7\u8c03\u6574BrowseComp\u57fa\u51c6\u4e3aBC-Small\uff0c\u6bd4\u8f83ODR\u4e0e\u4e13\u6709\u7cfb\u7edf\uff0c\u5e76\u5f15\u5165\u4e09\u9879\u6539\u8fdb\u63d0\u51faODR+\u3002", "result": "\u6240\u6709\u7cfb\u7edf\u572860\u4e2a\u6d4b\u8bd5\u95ee\u9898\u4e0a\u51c6\u786e\u7387\u4e3a0%\uff0c\u6539\u8fdb\u540e\u7684ODR+\u8fbe\u523010%\u7684\u6210\u529f\u7387\u3002", "conclusion": "\u5f00\u6e90DRA\u901a\u8fc7\u6539\u8fdb\u53ef\u4ee5\u63a5\u8fd1\u4e13\u6709\u7cfb\u7edf\u6027\u80fd\uff0c\u4e3a\u5b66\u672f\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u884c\u5de5\u5177\u3002"}}
{"id": "2508.10399", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.10399", "abs": "https://arxiv.org/abs/2508.10399", "authors": ["Wenlong Liang", "Rui Zhou", "Yang Ma", "Bing Zhang", "Songlin Li", "Yijia Liao", "Ping Kuang"], "title": "Large Model Empowered Embodied AI: A Survey on Decision-Making and Embodied Learning", "comment": null, "summary": "Embodied AI aims to develop intelligent systems with physical forms capable\nof perceiving, decision-making, acting, and learning in real-world\nenvironments, providing a promising way to Artificial General Intelligence\n(AGI). Despite decades of explorations, it remains challenging for embodied\nagents to achieve human-level intelligence for general-purpose tasks in open\ndynamic environments. Recent breakthroughs in large models have revolutionized\nembodied AI by enhancing perception, interaction, planning and learning. In\nthis article, we provide a comprehensive survey on large model empowered\nembodied AI, focusing on autonomous decision-making and embodied learning. We\ninvestigate both hierarchical and end-to-end decision-making paradigms,\ndetailing how large models enhance high-level planning, low-level execution,\nand feedback for hierarchical decision-making, and how large models enhance\nVision-Language-Action (VLA) models for end-to-end decision making. For\nembodied learning, we introduce mainstream learning methodologies, elaborating\non how large models enhance imitation learning and reinforcement learning\nin-depth. For the first time, we integrate world models into the survey of\nembodied AI, presenting their design methods and critical roles in enhancing\ndecision-making and learning. Though solid advances have been achieved,\nchallenges still exist, which are discussed at the end of this survey,\npotentially as the further research directions.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u5927\u6a21\u578b\u8d4b\u80fd\u7684\u5177\u8eabAI\uff0c\u91cd\u70b9\u63a2\u8ba8\u81ea\u4e3b\u51b3\u7b56\u4e0e\u5177\u8eab\u5b66\u4e60\uff0c\u5206\u6790\u5206\u5c42\u4e0e\u7aef\u5230\u7aef\u51b3\u7b56\u8303\u5f0f\uff0c\u5e76\u9996\u6b21\u5c06\u4e16\u754c\u6a21\u578b\u7eb3\u5165\u5177\u8eabAI\u7814\u7a76\u3002", "motivation": "\u5177\u8eabAI\u65e8\u5728\u5f00\u53d1\u5177\u5907\u611f\u77e5\u3001\u51b3\u7b56\u3001\u884c\u52a8\u548c\u5b66\u4e60\u80fd\u529b\u7684\u667a\u80fd\u7cfb\u7edf\uff0c\u4f46\u5b9e\u73b0\u5f00\u653e\u52a8\u6001\u73af\u5883\u4e2d\u4eba\u7c7b\u6c34\u5e73\u7684\u901a\u7528\u667a\u80fd\u4ecd\u5177\u6311\u6218\u3002\u5927\u6a21\u578b\u7684\u7a81\u7834\u4e3a\u5177\u8eabAI\u5e26\u6765\u9769\u547d\u6027\u8fdb\u5c55\u3002", "method": "\u7814\u7a76\u5206\u5c42\u4e0e\u7aef\u5230\u7aef\u51b3\u7b56\u8303\u5f0f\uff0c\u5206\u6790\u5927\u6a21\u578b\u5982\u4f55\u63d0\u5347\u9ad8\u5c42\u89c4\u5212\u3001\u4f4e\u5c42\u6267\u884c\u53ca\u53cd\u9988\uff1b\u63a2\u8ba8\u5927\u6a21\u578b\u5982\u4f55\u589e\u5f3a\u6a21\u4eff\u5b66\u4e60\u4e0e\u5f3a\u5316\u5b66\u4e60\uff1b\u9996\u6b21\u6574\u5408\u4e16\u754c\u6a21\u578b\u3002", "result": "\u5927\u6a21\u578b\u663e\u8457\u63d0\u5347\u4e86\u5177\u8eabAI\u7684\u51b3\u7b56\u4e0e\u5b66\u4e60\u80fd\u529b\uff0c\u4f46\u4ecd\u6709\u6311\u6218\u3002", "conclusion": "\u5c3d\u7ba1\u53d6\u5f97\u8fdb\u5c55\uff0c\u5177\u8eabAI\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u672a\u6765\u7814\u7a76\u65b9\u5411\u5305\u62ec\u8fdb\u4e00\u6b65\u4f18\u5316\u5927\u6a21\u578b\u7684\u5e94\u7528\u3002"}}
{"id": "2508.10164", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10164", "abs": "https://arxiv.org/abs/2508.10164", "authors": ["Bin Hong", "Jiayu Liu", "Zhenya Huang", "Kai Zhang", "Mengdi Zhang"], "title": "Pruning Long Chain-of-Thought of Large Reasoning Models via Small-Scale Preference Optimization", "comment": "19 pages, 5 figures", "summary": "Recent advances in Large Reasoning Models (LRMs) have demonstrated strong\nperformance on complex tasks through long Chain-of-Thought (CoT) reasoning.\nHowever, their lengthy outputs increase computational costs and may lead to\noverthinking, raising challenges in balancing reasoning effectiveness and\nefficiency. Current methods for efficient reasoning often compromise reasoning\nquality or require extensive resources. This paper investigates efficient\nmethods to reduce the generation length of LRMs. We analyze generation path\ndistributions and filter generated trajectories through difficulty estimation.\nSubsequently, we analyze the convergence behaviors of the objectives of various\npreference optimization methods under a Bradley-Terry loss based framework.\nBased on the analysis, we propose Length Controlled Preference Optimization\n(LCPO) that directly balances the implicit reward related to NLL loss. LCPO can\neffectively learn length preference with limited data and training. Extensive\nexperiments demonstrate that our approach significantly reduces the average\noutput length by over 50\\% across multiple benchmarks while maintaining the\nreasoning performance. Our work highlights the potential for computationally\nefficient approaches in guiding LRMs toward efficient reasoning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLCPO\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u63a7\u5236\u751f\u6210\u8def\u5f84\u957f\u5ea6\uff0c\u663e\u8457\u51cf\u5c11\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u8f93\u51fa\u957f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u7684\u957f\u94fe\u63a8\u7406\uff08CoT\uff09\u867d\u7136\u6027\u80fd\u5f3a\uff0c\u4f46\u8f93\u51fa\u8fc7\u957f\u5bfc\u81f4\u8ba1\u7b97\u6210\u672c\u9ad8\u548c\u8fc7\u601d\u8003\u95ee\u9898\uff0c\u9700\u8981\u5e73\u8861\u63a8\u7406\u6548\u7387\u4e0e\u8d28\u91cf\u3002", "method": "\u5206\u6790\u751f\u6210\u8def\u5f84\u5206\u5e03\u5e76\u901a\u8fc7\u96be\u5ea6\u4f30\u8ba1\u7b5b\u9009\u8f68\u8ff9\uff0c\u7814\u7a76\u504f\u597d\u4f18\u5316\u65b9\u6cd5\u7684\u6536\u655b\u884c\u4e3a\uff0c\u63d0\u51faLCPO\u65b9\u6cd5\u76f4\u63a5\u5e73\u8861\u9690\u5f0f\u5956\u52b1\u4e0eNLL\u635f\u5931\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLCPO\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u8f93\u51fa\u957f\u5ea6\u51cf\u5c1150%\u4ee5\u4e0a\uff0c\u540c\u65f6\u4fdd\u6301\u63a8\u7406\u6027\u80fd\u3002", "conclusion": "LCPO\u5c55\u793a\u4e86\u5728\u6709\u9650\u6570\u636e\u548c\u8bad\u7ec3\u4e0b\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\u7684\u6f5c\u529b\uff0c\u4e3aLRMs\u7684\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2508.10416", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.10416", "abs": "https://arxiv.org/abs/2508.10416", "authors": ["Zhuoyuan Yu", "Yuxing Long", "Zihan Yang", "Chengyan Zeng", "Hongwei Fan", "Jiyao Zhang", "Hao Dong"], "title": "CorrectNav: Self-Correction Flywheel Empowers Vision-Language-Action Navigation Model", "comment": null, "summary": "Existing vision-and-language navigation models often deviate from the correct\ntrajectory when executing instructions. However, these models lack effective\nerror correction capability, hindering their recovery from errors. To address\nthis challenge, we propose Self-correction Flywheel, a novel post-training\nparadigm. Instead of considering the model's error trajectories on the training\nset as a drawback, our paradigm emphasizes their significance as a valuable\ndata source. We have developed a method to identify deviations in these error\ntrajectories and devised innovative techniques to automatically generate\nself-correction data for perception and action. These self-correction data\nserve as fuel to power the model's continued training. The brilliance of our\nparadigm is revealed when we re-evaluate the model on the training set,\nuncovering new error trajectories. At this time, the self-correction flywheel\nbegins to spin. Through multiple flywheel iterations, we progressively enhance\nour monocular RGB-based VLA navigation model CorrectNav. Experiments on R2R-CE\nand RxR-CE benchmarks show CorrectNav achieves new state-of-the-art success\nrates of 65.1% and 69.3%, surpassing prior best VLA navigation models by 8.2%\nand 16.4%. Real robot tests in various indoor and outdoor environments\ndemonstrate \\method's superior capability of error correction, dynamic obstacle\navoidance, and long instruction following.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSelf-correction Flywheel\u7684\u540e\u8bad\u7ec3\u8303\u5f0f\uff0c\u901a\u8fc7\u5229\u7528\u6a21\u578b\u7684\u9519\u8bef\u8f68\u8ff9\u751f\u6210\u81ea\u6821\u6b63\u6570\u636e\uff0c\u9010\u6b65\u63d0\u5347\u89c6\u89c9-\u8bed\u8a00\u5bfc\u822a\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9-\u8bed\u8a00\u5bfc\u822a\u6a21\u578b\u5728\u6267\u884c\u6307\u4ee4\u65f6\u5bb9\u6613\u504f\u79bb\u6b63\u786e\u8f68\u8ff9\uff0c\u4e14\u7f3a\u4e4f\u6709\u6548\u7684\u9519\u8bef\u6821\u6b63\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u8bc6\u522b\u9519\u8bef\u8f68\u8ff9\u4e2d\u7684\u504f\u5dee\uff0c\u81ea\u52a8\u751f\u6210\u611f\u77e5\u548c\u52a8\u4f5c\u7684\u81ea\u6821\u6b63\u6570\u636e\uff0c\u5e76\u5229\u7528\u8fd9\u4e9b\u6570\u636e\u6301\u7eed\u8bad\u7ec3\u6a21\u578b\u3002", "result": "\u5728R2R-CE\u548cRxR-CE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCorrectNav\u6a21\u578b\u5206\u522b\u8fbe\u523065.1%\u548c69.3%\u7684\u6210\u529f\u7387\uff0c\u4f18\u4e8e\u4e4b\u524d\u7684\u6700\u4f73\u6a21\u578b\u3002", "conclusion": "Self-correction Flywheel\u8303\u5f0f\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u9519\u8bef\u6821\u6b63\u80fd\u529b\u548c\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u590d\u6742\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u4efb\u52a1\u3002"}}
{"id": "2508.10177", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10177", "abs": "https://arxiv.org/abs/2508.10177", "authors": ["Stepan Kulibaba", "Artem Dzhalilov", "Roman Pakhomov", "Oleg Svidchenko", "Alexander Gasnikov", "Aleksei Shpilman"], "title": "KompeteAI: Accelerated Autonomous Multi-Agent System for End-to-End Pipeline Generation for Machine Learning Problems", "comment": null, "summary": "Recent Large Language Model (LLM)-based AutoML systems demonstrate impressive\ncapabilities but face significant limitations such as constrained exploration\nstrategies and a severe execution bottleneck. Exploration is hindered by\none-shot methods lacking diversity and Monte Carlo Tree Search (MCTS)\napproaches that fail to recombine strong partial solutions. The execution\nbottleneck arises from lengthy code validation cycles that stifle iterative\nrefinement. To overcome these challenges, we introduce KompeteAI, a novel\nAutoML framework with dynamic solution space exploration. Unlike previous MCTS\nmethods that treat ideas in isolation, KompeteAI introduces a merging stage\nthat composes top candidates. We further expand the hypothesis space by\nintegrating Retrieval-Augmented Generation (RAG), sourcing ideas from Kaggle\nnotebooks and arXiv papers to incorporate real-world strategies. KompeteAI also\naddresses the execution bottleneck via a predictive scoring model and an\naccelerated debugging method, assessing solution potential using early stage\nmetrics to avoid costly full-code execution. This approach accelerates pipeline\nevaluation 6.9 times. KompeteAI outperforms leading methods (e.g., RD-agent,\nAIDE, and Ml-Master) by an average of 3\\% on the primary AutoML benchmark,\nMLE-Bench. Additionally, we propose Kompete-bench to address limitations in\nMLE-Bench, where KompeteAI also achieves state-of-the-art results", "AI": {"tldr": "KompeteAI\u662f\u4e00\u79cd\u65b0\u578bAutoML\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u89e3\u51b3\u65b9\u6848\u7a7a\u95f4\u63a2\u7d22\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6280\u672f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709LLM-based AutoML\u7cfb\u7edf\u7684\u63a2\u7d22\u548c\u6267\u884c\u74f6\u9888\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709LLM-based AutoML\u7cfb\u7edf\u5b58\u5728\u63a2\u7d22\u7b56\u7565\u53d7\u9650\u548c\u6267\u884c\u74f6\u9888\u95ee\u9898\uff0c\u5982\u7f3a\u4e4f\u591a\u6837\u6027\u548c\u65e0\u6cd5\u91cd\u7ec4\u90e8\u5206\u89e3\u51b3\u65b9\u6848\uff0c\u4ee5\u53ca\u4ee3\u7801\u9a8c\u8bc1\u5468\u671f\u957f\u963b\u788d\u8fed\u4ee3\u4f18\u5316\u3002", "method": "KompeteAI\u5f15\u5165\u52a8\u6001\u89e3\u51b3\u65b9\u6848\u7a7a\u95f4\u63a2\u7d22\uff0c\u5305\u62ec\u5408\u5e76\u9636\u6bb5\u7ec4\u5408\u5019\u9009\u65b9\u6848\uff0c\u5e76\u96c6\u6210RAG\u6280\u672f\u4eceKaggle\u548carXiv\u4e2d\u83b7\u53d6\u7b56\u7565\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u9884\u6d4b\u8bc4\u5206\u6a21\u578b\u548c\u52a0\u901f\u8c03\u8bd5\u65b9\u6cd5\u51cf\u5c11\u6267\u884c\u65f6\u95f4\u3002", "result": "KompeteAI\u5728MLE-Bench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd53%\uff0c\u5e76\u5c06\u7ba1\u9053\u8bc4\u4f30\u901f\u5ea6\u63d0\u53476.9\u500d\u3002", "conclusion": "KompeteAI\u901a\u8fc7\u521b\u65b0\u65b9\u6cd5\u89e3\u51b3\u4e86AutoML\u7cfb\u7edf\u7684\u5173\u952e\u95ee\u9898\uff0c\u5e76\u5728\u6027\u80fd\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2508.10423", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.10423", "abs": "https://arxiv.org/abs/2508.10423", "authors": ["Qi Liu", "Xiaopeng Zhang", "Mingshan Tan", "Shuaikang Ma", "Jinliang Ding", "Yanjie Li"], "title": "MASH: Cooperative-Heterogeneous Multi-Agent Reinforcement Learning for Single Humanoid Robot Locomotion", "comment": null, "summary": "This paper proposes a novel method to enhance locomotion for a single\nhumanoid robot through cooperative-heterogeneous multi-agent deep reinforcement\nlearning (MARL). While most existing methods typically employ single-agent\nreinforcement learning algorithms for a single humanoid robot or MARL\nalgorithms for multi-robot system tasks, we propose a distinct paradigm:\napplying cooperative-heterogeneous MARL to optimize locomotion for a single\nhumanoid robot. The proposed method, multi-agent reinforcement learning for\nsingle humanoid locomotion (MASH), treats each limb (legs and arms) as an\nindependent agent that explores the robot's action space while sharing a global\ncritic for cooperative learning. Experiments demonstrate that MASH accelerates\ntraining convergence and improves whole-body cooperation ability, outperforming\nconventional single-agent reinforcement learning methods. This work advances\nthe integration of MARL into single-humanoid-robot control, offering new\ninsights into efficient locomotion strategies.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u534f\u4f5c\u5f02\u6784\u591a\u667a\u80fd\u4f53\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u7684\u65b0\u65b9\u6cd5MASH\uff0c\u7528\u4e8e\u4f18\u5316\u5355\u4e2a\u4eba\u5f62\u673a\u5668\u4eba\u7684\u8fd0\u52a8\u80fd\u529b\uff0c\u901a\u8fc7\u5c06\u6bcf\u4e2a\u80a2\u4f53\u89c6\u4e3a\u72ec\u7acb\u667a\u80fd\u4f53\u5e76\u5171\u4eab\u5168\u5c40\u6279\u8bc4\u5668\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u5168\u8eab\u534f\u4f5c\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u91c7\u7528\u5355\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6216\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7528\u4e8e\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u4efb\u52a1\uff0c\u800c\u672c\u6587\u63d0\u51fa\u5c06\u534f\u4f5c\u5f02\u6784MARL\u5e94\u7528\u4e8e\u5355\u4e2a\u4eba\u5f62\u673a\u5668\u4eba\uff0c\u4ee5\u63a2\u7d22\u66f4\u9ad8\u6548\u7684\u8fd0\u52a8\u7b56\u7565\u3002", "method": "\u63d0\u51faMASH\u65b9\u6cd5\uff0c\u5c06\u4eba\u5f62\u673a\u5668\u4eba\u7684\u6bcf\u4e2a\u80a2\u4f53\uff08\u817f\u548c\u624b\u81c2\uff09\u89c6\u4e3a\u72ec\u7acb\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u5171\u4eab\u5168\u5c40\u6279\u8bc4\u5668\u8fdb\u884c\u534f\u4f5c\u5b66\u4e60\uff0c\u4f18\u5316\u673a\u5668\u4eba\u7684\u52a8\u4f5c\u7a7a\u95f4\u63a2\u7d22\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMASH\u52a0\u901f\u4e86\u8bad\u7ec3\u6536\u655b\u901f\u5ea6\uff0c\u5e76\u63d0\u5347\u4e86\u5168\u8eab\u534f\u4f5c\u80fd\u529b\uff0c\u4f18\u4e8e\u4f20\u7edf\u7684\u5355\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "MASH\u4e3a\u5355\u4e2a\u4eba\u5f62\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u7684MARL\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5c55\u793a\u4e86\u9ad8\u6548\u8fd0\u52a8\u7b56\u7565\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.10241", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10241", "abs": "https://arxiv.org/abs/2508.10241", "authors": ["Mark Zilberman"], "title": "Extending the Entropic Potential of Events for Uncertainty Quantification and Decision-Making in Artificial Intelligence", "comment": "10 pages", "summary": "This work demonstrates how the concept of the entropic potential of events --\na parameter quantifying the influence of discrete events on the expected future\nentropy of a system -- can enhance uncertainty quantification, decision-making,\nand interpretability in artificial intelligence (AI). Building on its original\nformulation in physics, the framework is adapted for AI by introducing an\nevent-centric measure that captures how actions, observations, or other\ndiscrete occurrences impact uncertainty at future time horizons. Both the\noriginal and AI-adjusted definitions of entropic potential are formalized, with\nthe latter emphasizing conditional expectations to account for counterfactual\nscenarios. Applications are explored in policy evaluation, intrinsic reward\ndesign, explainable AI, and anomaly detection, highlighting the metric's\npotential to unify and strengthen uncertainty modeling in intelligent systems.\nConceptual examples illustrate its use in reinforcement learning, Bayesian\ninference, and anomaly detection, while practical considerations for\ncomputation in complex AI models are discussed. The entropic potential\nframework offers a theoretically grounded, interpretable, and versatile\napproach to managing uncertainty in AI, bridging principles from\nthermodynamics, information theory, and machine learning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e8b\u4ef6\u71b5\u52bf\u7684\u6982\u5ff5\uff0c\u7528\u4e8e\u589e\u5f3aAI\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3001\u51b3\u7b56\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u63a2\u8ba8\u4e86\u5176\u5728\u591a\u4e2aAI\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "\u901a\u8fc7\u5f15\u5165\u4e8b\u4ef6\u71b5\u52bf\uff0c\u65e8\u5728\u7edf\u4e00\u548c\u5f3a\u5316\u667a\u80fd\u7cfb\u7edf\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u5efa\u6a21\uff0c\u7ed3\u5408\u7269\u7406\u5b66\u3001\u4fe1\u606f\u8bba\u548c\u673a\u5668\u5b66\u4e60\u7684\u539f\u7406\u3002", "method": "\u5c06\u7269\u7406\u5b66\u4e2d\u7684\u71b5\u52bf\u6982\u5ff5\u8c03\u6574\u5230AI\u9886\u57df\uff0c\u63d0\u51fa\u4e8b\u4ef6\u4e2d\u5fc3\u5ea6\u91cf\uff0c\u5f3a\u8c03\u6761\u4ef6\u671f\u671b\u4ee5\u5904\u7406\u53cd\u4e8b\u5b9e\u573a\u666f\u3002", "result": "\u6846\u67b6\u5728\u7b56\u7565\u8bc4\u4f30\u3001\u5185\u5728\u5956\u52b1\u8bbe\u8ba1\u3001\u53ef\u89e3\u91caAI\u548c\u5f02\u5e38\u68c0\u6d4b\u4e2d\u5c55\u793a\u4e86\u6f5c\u529b\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u3001\u8d1d\u53f6\u65af\u63a8\u7406\u7b49\u5b9e\u4f8b\u9a8c\u8bc1\u3002", "conclusion": "\u71b5\u52bf\u6846\u67b6\u4e3aAI\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u7ba1\u7406\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3001\u53ef\u89e3\u91ca\u4e14\u591a\u529f\u80fd\u7684\u65b9\u6cd5\u3002"}}
{"id": "2508.10497", "categories": ["cs.RO", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.10497", "abs": "https://arxiv.org/abs/2508.10497", "authors": ["Abdullah Farrukh", "Achim Wagner", "Martin Ruskowski"], "title": "Enabling Generic Robot Skill Implementation Using Object Oriented Programming", "comment": "34th International Conference on Robotics in Alpe-Adria-Danube Region\n  (RAAD 2025)", "summary": "Developing robotic algorithms and integrating a robotic subsystem into a\nlarger system can be a difficult task. Particularly in small and medium-sized\nenterprises (SMEs) where robotics expertise is lacking, implementing,\nmaintaining and developing robotic systems can be a challenge. As a result,\nmany companies rely on external expertise through system integrators, which, in\nsome cases, can lead to vendor lock-in and external dependency. In the academic\nresearch on intelligent manufacturing systems, robots play a critical role in\nthe design of robust autonomous systems. Similar challenges are faced by\nresearchers who want to use robotic systems as a component in a larger smart\nsystem, without having to deal with the complexity and vastness of the robot\ninterfaces in detail. In this paper, we propose a software framework that\nreduces the effort required to deploy a working robotic system. The focus is\nsolely on providing a concept for simplifying the different interfaces of a\nmodern robot system and using an abstraction layer for different manufacturers\nand models. The Python programming language is used to implement a prototype of\nthe concept. The target system is a bin-picking cell containing a Yaskawa\nMotoman GP4.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u7b80\u5316\u673a\u5668\u4eba\u7cfb\u7edf\u63a5\u53e3\u7684\u8f6f\u4ef6\u6846\u67b6\uff0c\u65e8\u5728\u964d\u4f4e\u90e8\u7f72\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u590d\u6742\u6027\uff0c\u7279\u522b\u9488\u5bf9\u4e2d\u5c0f\u4f01\u4e1a\u548c\u7814\u7a76\u4eba\u5458\u3002", "motivation": "\u4e2d\u5c0f\u4f01\u4e1a\u548c\u7814\u7a76\u4eba\u5458\u5728\u7f3a\u4e4f\u673a\u5668\u4eba\u4e13\u4e1a\u77e5\u8bc6\u7684\u60c5\u51b5\u4e0b\uff0c\u96be\u4ee5\u5b9e\u73b0\u548c\u7ef4\u62a4\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u4e14\u4f9d\u8d56\u5916\u90e8\u96c6\u6210\u5546\u53ef\u80fd\u5bfc\u81f4\u4f9b\u5e94\u5546\u9501\u5b9a\u3002", "method": "\u4f7f\u7528Python\u5b9e\u73b0\u4e00\u4e2a\u539f\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u62bd\u8c61\u5c42\u7edf\u4e00\u4e0d\u540c\u5236\u9020\u5546\u548c\u578b\u53f7\u7684\u673a\u5668\u4eba\u63a5\u53e3\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u4e13\u6ce8\u4e8e\u7b80\u5316\u73b0\u4ee3\u673a\u5668\u4eba\u7cfb\u7edf\u63a5\u53e3\u7684\u6982\u5ff5\u6846\u67b6\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u4e2d\u5c0f\u4f01\u4e1a\u548c\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u4e00\u79cd\u51cf\u5c11\u673a\u5668\u4eba\u7cfb\u7edf\u90e8\u7f72\u590d\u6742\u6027\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.10265", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2508.10265", "abs": "https://arxiv.org/abs/2508.10265", "authors": ["Jingde Cheng"], "title": "Why Cannot Large Language Models Ever Make True Correct Reasoning?", "comment": "8 pages. arXiv admin note: substantial text overlap with\n  arXiv:2412.12408", "summary": "Recently, with the application progress of AIGC tools based on large language\nmodels (LLMs), led by ChatGPT, many AI experts and more non-professionals are\ntrumpeting the \"understanding ability\" and \"reasoning ability\" of the LLMs. The\npresent author considers that the so-called \"understanding ability\" and\n\"reasoning ability\" of LLMs are just illusions of those people who with vague\nconcepts. In fact, the LLMs can never have the true understanding ability and\ntrue reasoning ability. This paper intents to explain that, because the\nessential limitations of their working principle, the LLMs can never have the\nability of true correct reasoning.", "AI": {"tldr": "\u8bba\u6587\u8ba4\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u201c\u7406\u89e3\u80fd\u529b\u201d\u548c\u201c\u63a8\u7406\u80fd\u529b\u201d\u662f\u8bef\u89e3\uff0c\u672c\u8d28\u9650\u5236\u4f7f\u5176\u65e0\u6cd5\u771f\u6b63\u5177\u5907\u8fd9\u4e9b\u80fd\u529b\u3002", "motivation": "\u9488\u5bf9\u5f53\u524d\u5bf9LLMs\u80fd\u529b\u7684\u8fc7\u5ea6\u4e50\u89c2\uff0c\u4f5c\u8005\u8bd5\u56fe\u6f84\u6e05\u5176\u672c\u8d28\u5c40\u9650\u6027\u3002", "method": "\u901a\u8fc7\u5206\u6790LLMs\u7684\u5de5\u4f5c\u539f\u7406\uff0c\u6307\u51fa\u5176\u65e0\u6cd5\u5b9e\u73b0\u771f\u6b63\u63a8\u7406\u7684\u6839\u672c\u539f\u56e0\u3002", "result": "LLMs\u56e0\u672c\u8d28\u9650\u5236\u65e0\u6cd5\u5177\u5907\u771f\u6b63\u7684\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "LLMs\u7684\u80fd\u529b\u88ab\u8bef\u89e3\uff0c\u5176\u672c\u8d28\u9650\u5236\u51b3\u5b9a\u4e86\u5b83\u4eec\u65e0\u6cd5\u5b9e\u73b0\u771f\u6b63\u7684\u63a8\u7406\u3002"}}
{"id": "2508.10511", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.10511", "abs": "https://arxiv.org/abs/2508.10511", "authors": ["Andrea Rosasco", "Federico Ceola", "Giulia Pasquale", "Lorenzo Natale"], "title": "KDPE: A Kernel Density Estimation Strategy for Diffusion Policy Trajectory Selection", "comment": "9th Conference on Robot Learning (CoRL 2025), Seoul, Korea", "summary": "Learning robot policies that capture multimodality in the training data has\nbeen a long-standing open challenge for behavior cloning. Recent approaches\ntackle the problem by modeling the conditional action distribution with\ngenerative models. One of these approaches is Diffusion Policy, which relies on\na diffusion model to denoise random points into robot action trajectories.\nWhile achieving state-of-the-art performance, it has two main drawbacks that\nmay lead the robot out of the data distribution during policy execution. First,\nthe stochasticity of the denoising process can highly impact on the quality of\ngenerated trajectory of actions. Second, being a supervised learning approach,\nit can learn data outliers from the dataset used for training. Recent work\nfocuses on mitigating these limitations by combining Diffusion Policy either\nwith large-scale training or with classical behavior cloning algorithms.\nInstead, we propose KDPE, a Kernel Density Estimation-based strategy that\nfilters out potentially harmful trajectories output of Diffusion Policy while\nkeeping a low test-time computational overhead. For Kernel Density Estimation,\nwe propose a manifold-aware kernel to model a probability density function for\nactions composed of end-effector Cartesian position, orientation, and gripper\nstate. KDPE overall achieves better performance than Diffusion Policy on\nsimulated single-arm tasks and real robot experiments.\n  Additional material and code are available on our project page\nhttps://hsp-iit.github.io/KDPE/.", "AI": {"tldr": "KDPE\u901a\u8fc7\u57fa\u4e8e\u6838\u5bc6\u5ea6\u4f30\u8ba1\u7684\u7b56\u7565\u6539\u8fdbDiffusion Policy\uff0c\u8fc7\u6ee4\u6709\u5bb3\u8f68\u8ff9\u5e76\u4fdd\u6301\u4f4e\u8ba1\u7b97\u5f00\u9500\uff0c\u5728\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u89e3\u51b3Diffusion Policy\u5728\u673a\u5668\u4eba\u884c\u4e3a\u514b\u9686\u4e2d\u7684\u968f\u673a\u6027\u548c\u6570\u636e\u5f02\u5e38\u95ee\u9898\u3002", "method": "\u63d0\u51faKDPE\uff0c\u5229\u7528\u6d41\u5f62\u611f\u77e5\u6838\u5bc6\u5ea6\u4f30\u8ba1\u8fc7\u6ee4Diffusion Policy\u8f93\u51fa\u7684\u6709\u5bb3\u8f68\u8ff9\u3002", "result": "KDPE\u5728\u4eff\u771f\u5355\u81c2\u4efb\u52a1\u548c\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8eDiffusion Policy\u3002", "conclusion": "KDPE\u6709\u6548\u89e3\u51b3\u4e86Diffusion Policy\u7684\u5c40\u9650\u6027\uff0c\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2508.10293", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10293", "abs": "https://arxiv.org/abs/2508.10293", "authors": ["Chuhuai Yue", "Chengqi Dong", "Yinan Gao", "Hang He", "Jiajun Chai", "Guojun Yin", "Wei Lin"], "title": "Promoting Efficient Reasoning with Verifiable Stepwise Reward", "comment": null, "summary": "Large reasoning models (LRMs) have recently achieved significant progress in\ncomplex reasoning tasks, aided by reinforcement learning with verifiable\nrewards. However, LRMs often suffer from overthinking, expending excessive\ncomputation on simple problems and reducing efficiency. Existing efficient\nreasoning methods typically require accurate task assessment to preset token\nbudgets or select reasoning modes, which limits their flexibility and\nreliability. In this work, we revisit the essence of overthinking and identify\nthat encouraging effective steps while penalizing ineffective ones is key to\nits solution. To this end, we propose a novel rule-based verifiable stepwise\nreward mechanism (VSRM), which assigns rewards based on the performance of\nintermediate states in the reasoning trajectory. This approach is intuitive and\nnaturally fits the step-by-step nature of reasoning tasks. We conduct extensive\nexperiments on standard mathematical reasoning benchmarks, including AIME24 and\nAIME25, by integrating VSRM with PPO and Reinforce++. Results show that our\nmethod achieves substantial output length reduction while maintaining original\nreasoning performance, striking an optimal balance between efficiency and\naccuracy. Further analysis of overthinking frequency and pass@k score before\nand after training demonstrates that our approach in deed effectively\nsuppresses ineffective steps and encourages effective reasoning, fundamentally\nalleviating the overthinking problem. All code will be released upon\nacceptance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c4\u5219\u7684\u53ef\u9a8c\u8bc1\u9010\u6b65\u5956\u52b1\u673a\u5236\uff08VSRM\uff09\uff0c\u901a\u8fc7\u5956\u52b1\u6709\u6548\u63a8\u7406\u6b65\u9aa4\u5e76\u60e9\u7f5a\u65e0\u6548\u6b65\u9aa4\uff0c\u89e3\u51b3\u4e86\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u7684\u8fc7\u5ea6\u601d\u8003\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6548\u7387\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5b58\u5728\u8fc7\u5ea6\u601d\u8003\u95ee\u9898\uff0c\u5bfc\u81f4\u8ba1\u7b97\u8d44\u6e90\u6d6a\u8d39\u548c\u6548\u7387\u4e0b\u964d\u3002\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u9884\u8bbe\u9884\u7b97\u6216\u9009\u62e9\u63a8\u7406\u6a21\u5f0f\uff0c\u7f3a\u4e4f\u7075\u6d3b\u6027\u3002", "method": "\u63d0\u51faVSRM\u673a\u5236\uff0c\u6839\u636e\u63a8\u7406\u8f68\u8ff9\u4e2d\u4e2d\u95f4\u72b6\u6001\u7684\u8868\u73b0\u5206\u914d\u5956\u52b1\uff0c\u7ed3\u5408PPO\u548cReinforce++\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5728AIME24\u548cAIME25\u7b49\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cVSRM\u663e\u8457\u51cf\u5c11\u4e86\u8f93\u51fa\u957f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u63a8\u7406\u6027\u80fd\uff0c\u6709\u6548\u6291\u5236\u4e86\u65e0\u6548\u6b65\u9aa4\u3002", "conclusion": "VSRM\u901a\u8fc7\u5956\u52b1\u6709\u6548\u6b65\u9aa4\u548c\u60e9\u7f5a\u65e0\u6548\u6b65\u9aa4\uff0c\u4ece\u6839\u672c\u4e0a\u7f13\u89e3\u4e86\u8fc7\u5ea6\u601d\u8003\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u6548\u7387\u4e0e\u51c6\u786e\u6027\u7684\u5e73\u8861\u3002"}}
{"id": "2508.10538", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.10538", "abs": "https://arxiv.org/abs/2508.10538", "authors": ["Xin Liu", "Bida Ma", "Chenkun Qi", "Yan Ding", "Zhaxizhuoma", "Guorong Zhang", "Pengan Chen", "Kehui Liu", "Zhongjie Jia", "Chuyue Guan", "Yule Mo", "Jiaqi Liu", "Feng Gao", "Jiangwei Zhong", "Bin Zhao", "Xuelong Li"], "title": "MLM: Learning Multi-task Loco-Manipulation Whole-Body Control for Quadruped Robot with Arm", "comment": null, "summary": "Whole-body loco-manipulation for quadruped robots with arm remains a\nchallenging problem, particularly in achieving multi-task control. To address\nthis, we propose MLM, a reinforcement learning framework driven by both\nreal-world and simulation data. It enables a six-DoF robotic arm--equipped\nquadruped robot to perform whole-body loco-manipulation for multiple tasks\nautonomously or under human teleoperation. To address the problem of balancing\nmultiple tasks during the learning of loco-manipulation, we introduce a\ntrajectory library with an adaptive, curriculum-based sampling mechanism. This\napproach allows the policy to efficiently leverage real-world collected\ntrajectories for learning multi-task loco-manipulation. To address deployment\nscenarios with only historical observations and to enhance the performance of\npolicy execution across tasks with different spatial ranges, we propose a\nTrajectory-Velocity Prediction policy network. It predicts unobservable future\ntrajectories and velocities. By leveraging extensive simulation data and\ncurriculum-based rewards, our controller achieves whole-body behaviors in\nsimulation and zero-shot transfer to real-world deployment. Ablation studies in\nsimulation verify the necessity and effectiveness of our approach, while\nreal-world experiments on the Go2 robot with an Airbot robotic arm demonstrate\nthe policy's good performance in multi-task execution.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMLM\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u771f\u5b9e\u4e16\u754c\u548c\u4eff\u771f\u6570\u636e\uff0c\u4f7f\u914d\u5907\u516d\u81ea\u7531\u5ea6\u673a\u68b0\u81c2\u7684\u56db\u8db3\u673a\u5668\u4eba\u80fd\u591f\u81ea\u4e3b\u6216\u901a\u8fc7\u8fdc\u7a0b\u64cd\u4f5c\u5b8c\u6210\u5168\u8eab\u5b9a\u4f4d\u64cd\u4f5c\u4efb\u52a1\u3002", "motivation": "\u89e3\u51b3\u56db\u8db3\u673a\u5668\u4eba\u914d\u5907\u673a\u68b0\u81c2\u65f6\u5168\u8eab\u5b9a\u4f4d\u64cd\u4f5c\u7684\u591a\u4efb\u52a1\u63a7\u5236\u96be\u9898\u3002", "method": "\u5f15\u5165\u8f68\u8ff9\u5e93\u548c\u81ea\u9002\u5e94\u8bfe\u7a0b\u91c7\u6837\u673a\u5236\uff0c\u63d0\u51fa\u8f68\u8ff9-\u901f\u5ea6\u9884\u6d4b\u7b56\u7565\u7f51\u7edc\uff0c\u5229\u7528\u4eff\u771f\u6570\u636e\u548c\u8bfe\u7a0b\u5956\u52b1\u5b9e\u73b0\u96f6\u6837\u672c\u8fc1\u79fb\u3002", "result": "\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u5728\u591a\u4efb\u52a1\u6267\u884c\u4e2d\u7684\u826f\u597d\u6027\u80fd\u3002", "conclusion": "MLM\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u591a\u4efb\u52a1\u5168\u8eab\u5b9a\u4f4d\u64cd\u4f5c\u95ee\u9898\uff0c\u5e76\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u5c55\u793a\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u3002"}}
{"id": "2508.10337", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10337", "abs": "https://arxiv.org/abs/2508.10337", "authors": ["Chenliang Zhang", "Lin Wang", "Yuanyuan Lu", "Yusheng Qi", "Kexin Wang", "Peixu Hou", "Wenshi Chen"], "title": "A Curriculum Learning Approach to Reinforcement Learning: Leveraging RAG for Multimodal Question Answering", "comment": null, "summary": "This paper describes the solutions of the Dianping-Trust-Safety team for the\nMETA CRAG-MM challenge. The challenge requires building a comprehensive\nretrieval-augmented generation system capable for multi-modal multi-turn\nquestion answering. The competition consists of three tasks: (1) answering\nquestions using structured data retrieved from an image-based mock knowledge\ngraph, (2) synthesizing information from both knowledge graphs and web search\nresults, and (3) handling multi-turn conversations that require context\nunderstanding and information aggregation from multiple sources. For Task 1,\nour solution is based on the vision large language model, enhanced by\nsupervised fine-tuning with knowledge distilled from GPT-4.1. We further\napplied curriculum learning strategies to guide reinforcement learning,\nresulting in improved answer accuracy and reduced hallucination. For Task 2 and\nTask 3, we additionally leveraged web search APIs to incorporate external\nknowledge, enabling the system to better handle complex queries and multi-turn\nconversations. Our approach achieved 1st place in Task 1 with a significant\nlead of 52.38\\%, and 3rd place in Task 3, demonstrating the effectiveness of\nthe integration of curriculum learning with reinforcement learning in our\ntraining pipeline.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86Dianping-Trust-Safety\u56e2\u961f\u5728META CRAG-MM\u6311\u6218\u4e2d\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5305\u62ec\u591a\u6a21\u6001\u591a\u8f6e\u95ee\u7b54\u7cfb\u7edf\u7684\u6784\u5efa\uff0c\u4efb\u52a11\u57fa\u4e8e\u89c6\u89c9\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4efb\u52a12\u548c3\u7ed3\u5408\u4e86\u5916\u90e8\u77e5\u8bc6\uff0c\u53d6\u5f97\u4e86\u663e\u8457\u6210\u7ee9\u3002", "motivation": "\u89e3\u51b3\u591a\u6a21\u6001\u591a\u8f6e\u95ee\u7b54\u7684\u590d\u6742\u9700\u6c42\uff0c\u63d0\u5347\u7cfb\u7edf\u5728\u7ed3\u6784\u5316\u6570\u636e\u68c0\u7d22\u3001\u4fe1\u606f\u5408\u6210\u548c\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u7684\u8868\u73b0\u3002", "method": "\u4efb\u52a11\u4f7f\u7528\u89c6\u89c9\u5927\u8bed\u8a00\u6a21\u578b\u5e76\u4f18\u5316\u8bad\u7ec3\u7b56\u7565\uff1b\u4efb\u52a12\u548c3\u7ed3\u5408\u5916\u90e8\u77e5\u8bc6\u641c\u7d22API\u3002", "result": "\u4efb\u52a11\u4ee552.38%\u7684\u4f18\u52bf\u6392\u540d\u7b2c\u4e00\uff0c\u4efb\u52a13\u6392\u540d\u7b2c\u4e09\u3002", "conclusion": "\u7ed3\u5408\u8bfe\u7a0b\u5b66\u4e60\u4e0e\u5f3a\u5316\u5b66\u4e60\u7684\u8bad\u7ec3\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u95ee\u7b54\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2508.10603", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.10603", "abs": "https://arxiv.org/abs/2508.10603", "authors": ["Agnes Axelsson", "Merle Reimann", "Ronald Cumbal", "Hannah Pelikan", "Divesh Lala"], "title": "Why Report Failed Interactions With Robots?! Towards Vignette-based Interaction Quality", "comment": "Accepted at the workshop on Real-World HRI in Public and Private\n  Spaces: Successes, Failures, and Lessons Learned (PubRob-Fails), held at the\n  IEEE RO-MAN Conference, 2025. 6 pages", "summary": "Although the quality of human-robot interactions has improved with the advent\nof LLMs, there are still various factors that cause systems to be sub-optimal\nwhen compared to human-human interactions. The nature and criticality of\nfailures are often dependent on the context of the interaction and so cannot be\ngeneralized across the wide range of scenarios and experiments which have been\nimplemented in HRI research. In this work we propose the use of a technique\noverlooked in the field of HRI, ethnographic vignettes, to clearly highlight\nthese failures, particularly those that are rarely documented. We describe the\nmethodology behind the process of writing vignettes and create our own based on\nour personal experiences with failures in HRI systems. We emphasize the\nstrength of vignettes as the ability to communicate failures from a\nmulti-disciplinary perspective, promote transparency about the capabilities of\nrobots, and document unexpected behaviours which would otherwise be omitted\nfrom research reports. We encourage the use of vignettes to augment existing\ninteraction evaluation methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4f7f\u7528\u6c11\u65cf\u5fd7\u5c0f\u6545\u4e8b\uff08ethnographic vignettes\uff09\u6765\u7a81\u51fa\u4eba\u673a\u4ea4\u4e92\uff08HRI\uff09\u4e2d\u7684\u5931\u8d25\u6848\u4f8b\uff0c\u5f25\u8865\u73b0\u6709\u7814\u7a76\u7684\u4e0d\u8db3\u3002", "motivation": "\u5c3d\u7ba1LLMs\u63d0\u5347\u4e86\u4eba\u673a\u4ea4\u4e92\u8d28\u91cf\uff0c\u4f46\u4e0e\u4eba\u4eba\u4ea4\u4e92\u76f8\u6bd4\u4ecd\u5b58\u5728\u4e0d\u8db3\uff0c\u4e14\u5931\u8d25\u6848\u4f8b\u56e0\u60c5\u5883\u800c\u5f02\uff0c\u96be\u4ee5\u6cdb\u5316\u3002", "method": "\u63d0\u51fa\u901a\u8fc7\u64b0\u5199\u6c11\u65cf\u5fd7\u5c0f\u6545\u4e8b\u7684\u65b9\u6cd5\uff0c\u8bb0\u5f55\u591a\u5b66\u79d1\u89c6\u89d2\u4e0b\u7684\u5931\u8d25\u6848\u4f8b\uff0c\u589e\u5f3a\u900f\u660e\u5ea6\u548c\u610f\u5916\u884c\u4e3a\u8bb0\u5f55\u3002", "result": "\u5c0f\u6545\u4e8b\u80fd\u6709\u6548\u63ed\u793a\u7f55\u89c1\u5931\u8d25\u6848\u4f8b\uff0c\u4fc3\u8fdb\u5bf9\u673a\u5668\u4eba\u80fd\u529b\u7684\u900f\u660e\u8ba4\u77e5\u3002", "conclusion": "\u5efa\u8bae\u5c06\u5c0f\u6545\u4e8b\u4f5c\u4e3a\u73b0\u6709\u4ea4\u4e92\u8bc4\u4f30\u65b9\u6cd5\u7684\u8865\u5145\u5de5\u5177\u3002"}}
{"id": "2508.10340", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10340", "abs": "https://arxiv.org/abs/2508.10340", "authors": ["Chak Lam Shek", "Guangyao Shi", "Pratap Tokekar"], "title": "Multi-Agent Trust Region Policy Optimisation: A Joint Constraint Approach", "comment": null, "summary": "Multi-agent reinforcement learning (MARL) requires coordinated and stable\npolicy updates among interacting agents. Heterogeneous-Agent Trust Region\nPolicy Optimization (HATRPO) enforces per-agent trust region constraints using\nKullback-Leibler (KL) divergence to stabilize training. However, assigning each\nagent the same KL threshold can lead to slow and locally optimal updates,\nespecially in heterogeneous settings. To address this limitation, we propose\ntwo approaches for allocating the KL divergence threshold across agents:\nHATRPO-W, a Karush-Kuhn-Tucker-based (KKT-based) method that optimizes\nthreshold assignment under global KL constraints, and HATRPO-G, a greedy\nalgorithm that prioritizes agents based on improvement-to-divergence ratio. By\nconnecting sequential policy optimization with constrained threshold\nscheduling, our approach enables more flexible and effective learning in\nheterogeneous-agent settings. Experimental results demonstrate that our methods\nsignificantly boost the performance of HATRPO, achieving faster convergence and\nhigher final rewards across diverse MARL benchmarks. Specifically, HATRPO-W and\nHATRPO-G achieve comparable improvements in final performance, each exceeding\n22.5%. Notably, HATRPO-W also demonstrates more stable learning dynamics, as\nreflected by its lower variance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u65b9\u6cd5\uff08HATRPO-W\u548cHATRPO-G\uff09\u6765\u4f18\u5316\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684KL\u6563\u5ea6\u9608\u503c\u5206\u914d\uff0c\u663e\u8457\u63d0\u5347\u4e86HATRPO\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edfHATRPO\u65b9\u6cd5\u4e2d\uff0c\u4e3a\u6240\u6709\u667a\u80fd\u4f53\u5206\u914d\u76f8\u540c\u7684KL\u6563\u5ea6\u9608\u503c\u53ef\u80fd\u5bfc\u81f4\u8bad\u7ec3\u7f13\u6162\u548c\u5c40\u90e8\u6700\u4f18\uff0c\u5c24\u5176\u5728\u5f02\u6784\u73af\u5883\u4e2d\u3002", "method": "\u63d0\u51faHATRPO-W\uff08\u57fa\u4e8eKKT\u6761\u4ef6\u4f18\u5316\u5168\u5c40KL\u7ea6\u675f\uff09\u548cHATRPO-G\uff08\u57fa\u4e8e\u6539\u8fdb-\u6563\u5ea6\u6bd4\u7684\u8d2a\u5fc3\u7b97\u6cd5\uff09\u6765\u52a8\u6001\u5206\u914dKL\u9608\u503c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4e24\u79cd\u65b9\u6cd5\u5747\u663e\u8457\u63d0\u5347\u6027\u80fd\uff08\u8d85\u8fc722.5%\uff09\uff0cHATRPO-W\u8fd8\u8868\u73b0\u51fa\u66f4\u7a33\u5b9a\u7684\u5b66\u4e60\u52a8\u6001\u3002", "conclusion": "\u52a8\u6001\u5206\u914dKL\u9608\u503c\u80fd\u6709\u6548\u63d0\u5347\u5f02\u6784\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684\u6027\u80fd\u548c\u7a33\u5b9a\u6027\u3002"}}
{"id": "2508.10634", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.10634", "abs": "https://arxiv.org/abs/2508.10634", "authors": ["Mehdi Heydari Shahna", "Jouni Mattila"], "title": "Synthesis of Deep Neural Networks with Safe Robust Adaptive Control for Reliable Operation of Wheeled Mobile Robots", "comment": null, "summary": "Deep neural networks (DNNs) can enable precise control while maintaining low\ncomputational costs by circumventing the need for dynamic modeling. However,\nthe deployment of such black-box approaches remains challenging for heavy-duty\nwheeled mobile robots (WMRs), which are subject to strict international\nstandards and prone to faults and disturbances. We designed a hierarchical\ncontrol policy for heavy-duty WMRs, monitored by two safety layers with\ndiffering levels of authority. To this end, a DNN policy was trained and\ndeployed as the primary control strategy, providing high-precision performance\nunder nominal operating conditions. When external disturbances arise and reach\na level of intensity such that the system performance falls below a predefined\nthreshold, a low-level safety layer intervenes by deactivating the primary\ncontrol policy and activating a model-free robust adaptive control (RAC)\npolicy. This transition enables the system to continue operating while ensuring\nstability by effectively managing the inherent trade-off between system\nrobustness and responsiveness. Regardless of the control policy in use, a\nhigh-level safety layer continuously monitors system performance during\noperation. It initiates a shutdown only when disturbances become sufficiently\nsevere such that compensation is no longer viable and continued operation would\njeopardize the system or its environment. The proposed synthesis of DNN and RAC\npolicy guarantees uniform exponential stability of the entire WMR system while\nadhering to safety standards to some extent. The effectiveness of the proposed\napproach was further validated through real-time experiments using a 6,000 kg\nWMR.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNN\uff09\u548c\u9c81\u68d2\u81ea\u9002\u5e94\u63a7\u5236\uff08RAC\uff09\u7684\u5206\u5c42\u63a7\u5236\u7b56\u7565\uff0c\u7528\u4e8e\u91cd\u578b\u8f6e\u5f0f\u79fb\u52a8\u673a\u5668\u4eba\uff08WMR\uff09\uff0c\u4ee5\u786e\u4fdd\u9ad8\u7cbe\u5ea6\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u91cd\u578bWMR\u5728\u4e25\u683c\u56fd\u9645\u6807\u51c6\u4e0b\u8fd0\u884c\uff0c\u6613\u53d7\u5e72\u6270\u548c\u6545\u969c\u5f71\u54cd\uff0c\u4f20\u7edf\u9ed1\u76d2DNN\u65b9\u6cd5\u96be\u4ee5\u6ee1\u8db3\u5176\u5b89\u5168\u9700\u6c42\u3002", "method": "\u8bbe\u8ba1\u4e86\u5206\u5c42\u63a7\u5236\u7b56\u7565\uff1aDNN\u4f5c\u4e3a\u4e3b\u63a7\u7b56\u7565\uff0cRAC\u4f5c\u4e3a\u4f4e\u5c42\u5b89\u5168\u5c42\uff0c\u9ad8\u5c42\u5b89\u5168\u5c42\u76d1\u63a7\u7cfb\u7edf\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u57286000 kg WMR\u4e0a\u7684\u6709\u6548\u6027\uff0c\u786e\u4fdd\u4e86\u7cfb\u7edf\u7684\u7a33\u5b9a\u6027\u548c\u5b89\u5168\u6027\u3002", "conclusion": "DNN\u4e0eRAC\u7684\u7ed3\u5408\u4e3a\u91cd\u578bWMR\u63d0\u4f9b\u4e86\u9ad8\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u7684\u63a7\u5236\u65b9\u6848\u3002"}}
{"id": "2508.10358", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10358", "abs": "https://arxiv.org/abs/2508.10358", "authors": ["Mengtao Zhou", "Sifan Wu", "Huan Zhang", "Qi Sima", "Bang Liu"], "title": "What to Ask Next? Probing the Imaginative Reasoning of LLMs with TurtleSoup Puzzles", "comment": null, "summary": "We investigate the capacity of Large Language Models (LLMs) for imaginative\nreasoning--the proactive construction, testing, and revision of hypotheses in\ninformation-sparse environments. Existing benchmarks, often static or focused\non social deduction, fail to capture the dynamic, exploratory nature of this\nreasoning process. To address this gap, we introduce a comprehensive research\nframework based on the classic \"Turtle Soup\" game, integrating a benchmark, an\nagent, and an evaluation protocol. We present TurtleSoup-Bench, the first\nlarge-scale, bilingual, interactive benchmark for imaginative reasoning,\ncomprising 800 turtle soup puzzles sourced from both the Internet and expert\nauthors. We also propose Mosaic-Agent, a novel agent designed to assess LLMs'\nperformance in this setting. To evaluate reasoning quality, we develop a\nmulti-dimensional protocol measuring logical consistency, detail completion,\nand conclusion alignment. Experiments with leading LLMs reveal clear capability\nlimits, common failure patterns, and a significant performance gap compared to\nhumans. Our work offers new insights into LLMs' imaginative reasoning and\nestablishes a foundation for future research on exploratory agent behavior.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4fe1\u606f\u7a00\u758f\u73af\u5883\u4e2d\u7684\u60f3\u8c61\u63a8\u7406\u80fd\u529b\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u201cTurtle Soup\u201d\u6e38\u620f\u7684\u6846\u67b6\uff0c\u5305\u62ec\u65b0\u57fa\u51c6\u3001\u4ee3\u7406\u548c\u8bc4\u4f30\u534f\u8bae\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u591a\u4e3a\u9759\u6001\u6216\u5173\u6ce8\u793e\u4ea4\u63a8\u7406\uff0c\u65e0\u6cd5\u6355\u6349\u52a8\u6001\u63a2\u7d22\u6027\u63a8\u7406\u8fc7\u7a0b\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u65b9\u6cd5\u8bc4\u4f30LLMs\u7684\u60f3\u8c61\u63a8\u7406\u80fd\u529b\u3002", "method": "\u5f15\u5165TurtleSoup-Bench\uff08800\u4e2a\u53cc\u8bed\u8c1c\u9898\uff09\u548cMosaic-Agent\uff0c\u5f00\u53d1\u591a\u7ef4\u8bc4\u4f30\u534f\u8bae\uff08\u903b\u8f91\u4e00\u81f4\u6027\u3001\u7ec6\u8282\u5b8c\u6574\u6027\u548c\u7ed3\u8bba\u5bf9\u9f50\uff09\u3002", "result": "\u5b9e\u9a8c\u663e\u793aLLMs\u5728\u60f3\u8c61\u63a8\u7406\u4e2d\u5b58\u5728\u660e\u663e\u80fd\u529b\u9650\u5236\u548c\u5e38\u89c1\u5931\u8d25\u6a21\u5f0f\uff0c\u4e0e\u4eba\u7c7b\u8868\u73b0\u6709\u663e\u8457\u5dee\u8ddd\u3002", "conclusion": "\u7814\u7a76\u4e3aLLMs\u7684\u60f3\u8c61\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\uff0c\u4e3a\u672a\u6765\u63a2\u7d22\u6027\u4ee3\u7406\u884c\u4e3a\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.10686", "categories": ["cs.RO", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.10686", "abs": "https://arxiv.org/abs/2508.10686", "authors": ["Carla Wehner", "Finn Schubert", "Heiko Hellkamp", "Julius Hahnewald", "Kilian Scheafer", "Muhammad Bilal Khan", "Oliver Gutfleisch"], "title": "An Open-Source User-Friendly Interface for Simulating Magnetic Soft Robots using Simulation Open Framework Architecture (SOFA)", "comment": null, "summary": "Soft robots, particularly magnetic soft robots, require specialized\nsimulation tools to accurately model their deformation under external magnetic\nfields. However, existing platforms often lack dedicated support for magnetic\nmaterials, making them difficult to use for researchers at different expertise\nlevels. This work introduces an open-source, user-friendly simulation interface\nusing the Simulation Open Framework Architecture (SOFA), specifically designed\nto model magnetic soft robots. The tool enables users to define material\nproperties, apply magnetic fields, and observe resulting deformations in real\ntime. By integrating intuitive controls and stress analysis capabilities, it\naims to bridge the gap between theoretical modeling and practical design. Four\nbenchmark models - a beam, three- and four-finger grippers, and a butterfly -\ndemonstrate its functionality. The software's ease of use makes it accessible\nto both beginners and advanced researchers. Future improvements will refine\naccuracy through experimental validation and comparison with industry-standard\nfinite element solvers, ensuring realistic and predictive simulations of\nmagnetic soft robots.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u57fa\u4e8eSOFA\u7684\u5f00\u6e90\u3001\u7528\u6237\u53cb\u597d\u7684\u78c1\u6027\u8f6f\u4f53\u673a\u5668\u4eba\u4eff\u771f\u5de5\u5177\uff0c\u65e8\u5728\u586b\u8865\u73b0\u6709\u5e73\u53f0\u5728\u78c1\u6027\u6750\u6599\u5efa\u6a21\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u4eff\u771f\u5de5\u5177\u7f3a\u4e4f\u5bf9\u78c1\u6027\u6750\u6599\u7684\u4e13\u95e8\u652f\u6301\uff0c\u5bfc\u81f4\u4e0d\u540c\u4e13\u4e1a\u6c34\u5e73\u7684\u7814\u7a76\u4eba\u5458\u96be\u4ee5\u4f7f\u7528\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u4e00\u4e2a\u66f4\u6613\u7528\u7684\u4eff\u771f\u5e73\u53f0\u3002", "method": "\u5229\u7528SOFA\u6846\u67b6\u5f00\u53d1\u4e86\u4e00\u4e2a\u4eff\u771f\u754c\u9762\uff0c\u652f\u6301\u5b9a\u4e49\u6750\u6599\u5c5e\u6027\u3001\u65bd\u52a0\u78c1\u573a\u5e76\u5b9e\u65f6\u89c2\u5bdf\u53d8\u5f62\uff0c\u540c\u65f6\u96c6\u6210\u4e86\u76f4\u89c2\u63a7\u5236\u548c\u5e94\u529b\u5206\u6790\u529f\u80fd\u3002", "result": "\u901a\u8fc7\u56db\u4e2a\u57fa\u51c6\u6a21\u578b\uff08\u6881\u3001\u4e09\u6307\u548c\u56db\u6307\u5939\u6301\u5668\u3001\u8774\u8776\uff09\u9a8c\u8bc1\u4e86\u5de5\u5177\u7684\u529f\u80fd\u6027\uff0c\u8bc1\u660e\u5176\u9002\u7528\u4e8e\u521d\u5b66\u8005\u548c\u9ad8\u7ea7\u7814\u7a76\u4eba\u5458\u3002", "conclusion": "\u672a\u6765\u5c06\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u548c\u4e0e\u884c\u4e1a\u6807\u51c6\u6709\u9650\u5143\u6c42\u89e3\u5668\u7684\u6bd4\u8f83\uff0c\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4eff\u771f\u7cbe\u5ea6\uff0c\u786e\u4fdd\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2508.10391", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10391", "abs": "https://arxiv.org/abs/2508.10391", "authors": ["Yaoze Zhang", "Rong Wu", "Pinlong Cai", "Xiaoman Wang", "Guohang Yan", "Song Mao", "Ding Wang", "Botian Shi"], "title": "LeanRAG: Knowledge-Graph-Based Generation with Semantic Aggregation and Hierarchical Retrieval", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) plays a crucial role in grounding Large\nLanguage Models by leveraging external knowledge, whereas the effectiveness is\noften compromised by the retrieval of contextually flawed or incomplete\ninformation. To address this, knowledge graph-based RAG methods have evolved\ntowards hierarchical structures, organizing knowledge into multi-level\nsummaries. However, these approaches still suffer from two critical,\nunaddressed challenges: high-level conceptual summaries exist as disconnected\n``semantic islands'', lacking the explicit relations needed for cross-community\nreasoning; and the retrieval process itself remains structurally unaware, often\ndegenerating into an inefficient flat search that fails to exploit the graph's\nrich topology. To overcome these limitations, we introduce LeanRAG, a framework\nthat features a deeply collaborative design combining knowledge aggregation and\nretrieval strategies. LeanRAG first employs a novel semantic aggregation\nalgorithm that forms entity clusters and constructs new explicit relations\namong aggregation-level summaries, creating a fully navigable semantic network.\nThen, a bottom-up, structure-guided retrieval strategy anchors queries to the\nmost relevant fine-grained entities and then systematically traverses the\ngraph's semantic pathways to gather concise yet contextually comprehensive\nevidence sets. The LeanRAG can mitigate the substantial overhead associated\nwith path retrieval on graphs and minimizes redundant information retrieval.\nExtensive experiments on four challenging QA benchmarks with different domains\ndemonstrate that LeanRAG significantly outperforming existing methods in\nresponse quality while reducing 46\\% retrieval redundancy. Code is available\nat: https://github.com/RaZzzyz/LeanRAG", "AI": {"tldr": "LeanRAG\u901a\u8fc7\u77e5\u8bc6\u805a\u5408\u548c\u68c0\u7d22\u7b56\u7565\u7684\u6df1\u5ea6\u534f\u4f5c\u8bbe\u8ba1\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u77e5\u8bc6\u56fe\u8c31RAG\u65b9\u6cd5\u4e2d\u7684\u8bed\u4e49\u5b64\u5c9b\u548c\u68c0\u7d22\u6548\u7387\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u95ee\u7b54\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u77e5\u8bc6\u56fe\u8c31RAG\u65b9\u6cd5\u5b58\u5728\u8bed\u4e49\u5b64\u5c9b\u548c\u68c0\u7d22\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u6709\u6548\u6027\u3002", "method": "LeanRAG\u91c7\u7528\u8bed\u4e49\u805a\u5408\u7b97\u6cd5\u6784\u5efa\u5b9e\u4f53\u96c6\u7fa4\u548c\u663e\u5f0f\u5173\u7cfb\uff0c\u5e76\u7ed3\u5408\u81ea\u5e95\u5411\u4e0a\u7684\u7ed3\u6784\u5f15\u5bfc\u68c0\u7d22\u7b56\u7565\u3002", "result": "\u5728\u56db\u4e2aQA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLeanRAG\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u51cf\u5c1146%\u7684\u68c0\u7d22\u5197\u4f59\u3002", "conclusion": "LeanRAG\u901a\u8fc7\u4f18\u5316\u77e5\u8bc6\u7ec4\u7ec7\u548c\u68c0\u7d22\u7b56\u7565\uff0c\u6709\u6548\u63d0\u5347\u4e86RAG\u7684\u6027\u80fd\u548c\u6548\u7387\u3002"}}
{"id": "2508.10689", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.10689", "abs": "https://arxiv.org/abs/2508.10689", "authors": ["Matteo Luperto", "Valerii Stakanov", "Giacomo Boracchi", "Nicola Basilico", "Francesco Amigoni"], "title": "Biasing Frontier-Based Exploration with Saliency Areas", "comment": "Accepted at the European Confrence on Mobile Robots (ECMR) 2025", "summary": "Autonomous exploration is a widely studied problem where a robot\nincrementally builds a map of a previously unknown environment. The robot\nselects the next locations to reach using an exploration strategy. To do so,\nthe robot has to balance between competing objectives, like exploring the\nentirety of the environment, while being as fast as possible. Most exploration\nstrategies try to maximise the explored area to speed up exploration; however,\nthey do not consider that parts of the environment are more important than\nothers, as they lead to the discovery of large unknown areas. We propose a\nmethod that identifies \\emph{saliency areas} as those areas that are of high\ninterest for exploration, by using saliency maps obtained from a neural network\nthat, given the current map, implements a termination criterion to estimate\nwhether the environment can be considered fully-explored or not. We use\nsaliency areas to bias some widely used exploration strategies, showing, with\nan extensive experimental campaign, that this knowledge can significantly\ninfluence the behavior of the robot during exploration.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u663e\u8457\u6027\u533a\u57df\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316\u673a\u5668\u4eba\u5728\u672a\u77e5\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u63a2\u7d22\u7b56\u7565\u3002", "motivation": "\u73b0\u6709\u63a2\u7d22\u7b56\u7565\u901a\u5e38\u4ec5\u5173\u6ce8\u6700\u5927\u5316\u63a2\u7d22\u9762\u79ef\uff0c\u800c\u5ffd\u7565\u4e86\u73af\u5883\u4e2d\u67d0\u4e9b\u533a\u57df\u5bf9\u63a2\u7d22\u6548\u7387\u7684\u91cd\u8981\u6027\u3002", "method": "\u5229\u7528\u795e\u7ecf\u7f51\u7edc\u751f\u6210\u7684\u663e\u8457\u6027\u5730\u56fe\u8bc6\u522b\u5173\u952e\u533a\u57df\uff08\u663e\u8457\u6027\u533a\u57df\uff09\uff0c\u5e76\u5c06\u5176\u878d\u5165\u73b0\u6709\u63a2\u7d22\u7b56\u7565\u4e2d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u663e\u8457\u5f71\u54cd\u673a\u5668\u4eba\u7684\u63a2\u7d22\u884c\u4e3a\uff0c\u63d0\u9ad8\u63a2\u7d22\u6548\u7387\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u663e\u8457\u6027\u533a\u57df\uff0c\u53ef\u4ee5\u66f4\u9ad8\u6548\u5730\u6307\u5bfc\u673a\u5668\u4eba\u5728\u672a\u77e5\u73af\u5883\u4e2d\u7684\u63a2\u7d22\u3002"}}
{"id": "2508.10425", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10425", "abs": "https://arxiv.org/abs/2508.10425", "authors": ["Yan Ting Chok", "Soyon Park", "Seungheun Baek", "Hajung Kim", "Junhyun Lee", "Jaewoo Kang"], "title": "HiRef: Leveraging Hierarchical Ontology and Network Refinement for Robust Medication Recommendation", "comment": null, "summary": "Medication recommendation is a crucial task for assisting physicians in\nmaking timely decisions from longitudinal patient medical records. However,\nreal-world EHR data present significant challenges due to the presence of\nrarely observed medical entities and incomplete records that may not fully\ncapture the clinical ground truth. While data-driven models trained on\nlongitudinal Electronic Health Records often achieve strong empirical\nperformance, they struggle to generalize under missing or novel conditions,\nlargely due to their reliance on observed co-occurrence patterns. To address\nthese issues, we propose Hierarchical Ontology and Network Refinement for\nRobust Medication Recommendation (HiRef), a unified framework that combines two\ncomplementary structures: (i) the hierarchical semantics encoded in curated\nmedical ontologies, and (ii) refined co-occurrence patterns derived from\nreal-world EHRs. We embed ontology entities in hyperbolic space, which\nnaturally captures tree-like relationships and enables knowledge transfer\nthrough shared ancestors, thereby improving generalizability to unseen codes.\nTo further improve robustness, we introduce a prior-guided sparse\nregularization scheme that refines the EHR co-occurrence graph by suppressing\nspurious edges while preserving clinically meaningful associations. Our model\nachieves strong performance on EHR benchmarks (MIMIC-III and MIMIC-IV) and\nmaintains high accuracy under simulated unseen-code settings. Extensive\nexperiments with comprehensive ablation studies demonstrate HiRef's resilience\nto unseen medical codes, supported by in-depth analyses of the learned\nsparsified graph structure and medical code embeddings.", "AI": {"tldr": "HiRef\u6846\u67b6\u7ed3\u5408\u533b\u5b66\u672c\u4f53\u5c42\u6b21\u7ed3\u6784\u548cEHR\u5171\u73b0\u6a21\u5f0f\uff0c\u63d0\u5347\u836f\u7269\u63a8\u8350\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3EHR\u6570\u636e\u4e2d\u7f55\u89c1\u5b9e\u4f53\u548c\u7f3a\u5931\u8bb0\u5f55\u5bfc\u81f4\u7684\u6cdb\u5316\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u53cc\u66f2\u7a7a\u95f4\u5d4c\u5165\u533b\u5b66\u672c\u4f53\uff0c\u7ed3\u5408\u5148\u9a8c\u5f15\u5bfc\u7684\u7a00\u758f\u6b63\u5219\u5316\u4f18\u5316EHR\u5171\u73b0\u56fe\u3002", "result": "\u5728MIMIC-III\u548cMIMIC-IV\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5bf9\u672a\u89c1\u4ee3\u7801\u4fdd\u6301\u9ad8\u51c6\u786e\u6027\u3002", "conclusion": "HiRef\u901a\u8fc7\u7ed3\u5408\u672c\u4f53\u548cEHR\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u4e86\u836f\u7269\u63a8\u8350\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.10780", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.10780", "abs": "https://arxiv.org/abs/2508.10780", "authors": ["Alessandro Adami", "Aris Synodinos", "Matteo Iovino", "Ruggero Carli", "Pietro Falco"], "title": "Learning Task Execution Hierarchies for Redundant Robots", "comment": null, "summary": "Modern robotic systems, such as mobile manipulators, humanoids, and aerial\nrobots with arms, often possess high redundancy, enabling them to perform\nmultiple tasks simultaneously. Managing this redundancy is key to achieving\nreliable and flexible behavior. A widely used approach is the Stack of Tasks\n(SoT), which organizes control objectives by priority within a unified\nframework. However, traditional SoTs are manually designed by experts, limiting\ntheir adaptability and accessibility. This paper introduces a novel framework\nthat automatically learns both the hierarchy and parameters of a SoT from\nuser-defined objectives. By combining Reinforcement Learning and Genetic\nProgramming, the system discovers task priorities and control strategies\nwithout manual intervention. A cost function based on intuitive metrics such as\nprecision, safety, and execution time guides the learning process. We validate\nour method through simulations and experiments on the mobile-YuMi platform, a\ndual-arm mobile manipulator with high redundancy. Results show that the learned\nSoTs enable the robot to dynamically adapt to changing environments and inputs,\nbalancing competing objectives while maintaining robust task execution. This\napproach provides a general and user-friendly solution for redundancy\nmanagement in complex robots, advancing human-centered robot programming and\nreducing the need for expert design.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5b66\u4e60\u4efb\u52a1\u5806\u53e0\uff08SoT\uff09\u5c42\u6b21\u548c\u53c2\u6570\u7684\u65b0\u6846\u67b6\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u548c\u9057\u4f20\u7f16\u7a0b\uff0c\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u5373\u53ef\u53d1\u73b0\u4efb\u52a1\u4f18\u5148\u7ea7\u548c\u63a7\u5236\u7b56\u7565\u3002", "motivation": "\u4f20\u7edfSoT\u7531\u4e13\u5bb6\u624b\u52a8\u8bbe\u8ba1\uff0c\u9650\u5236\u4e86\u5176\u9002\u5e94\u6027\u548c\u53ef\u8bbf\u95ee\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u81ea\u52a8\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u548c\u9057\u4f20\u7f16\u7a0b\uff0c\u901a\u8fc7\u57fa\u4e8e\u7cbe\u5ea6\u3001\u5b89\u5168\u6027\u548c\u6267\u884c\u65f6\u95f4\u7684\u6210\u672c\u51fd\u6570\u6307\u5bfc\u5b66\u4e60\u8fc7\u7a0b\u3002", "result": "\u5728\u79fb\u52a8-YuMi\u5e73\u53f0\u4e0a\u9a8c\u8bc1\uff0c\u7ed3\u679c\u8868\u660e\u5b66\u4e60\u7684SoT\u4f7f\u673a\u5668\u4eba\u80fd\u52a8\u6001\u9002\u5e94\u73af\u5883\u548c\u8f93\u5165\uff0c\u5e73\u8861\u7ade\u4e89\u76ee\u6807\u5e76\u4fdd\u6301\u7a33\u5065\u4efb\u52a1\u6267\u884c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u590d\u6742\u673a\u5668\u4eba\u5197\u4f59\u7ba1\u7406\u63d0\u4f9b\u4e86\u901a\u7528\u4e14\u7528\u6237\u53cb\u597d\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u51cf\u5c11\u4e86\u4e13\u5bb6\u8bbe\u8ba1\u7684\u4f9d\u8d56\u3002"}}
{"id": "2508.10429", "categories": ["cs.AI", "cs.CR", "cs.CV", "I.2.10; I.2.6"], "pdf": "https://arxiv.org/pdf/2508.10429", "abs": "https://arxiv.org/abs/2508.10429", "authors": ["Yi Dong", "Yusuke Muraoka", "Scott Shi", "Yi Zhang"], "title": "MM-Food-100K: A 100,000-Sample Multimodal Food Intelligence Dataset with Verifiable Provenance", "comment": "10 pages, 5 figures, 6 tables. The dataset is available at\n  https://huggingface.co/datasets/Codatta/MM-Food-100K", "summary": "We present MM-Food-100K, a public 100,000-sample multimodal food intelligence\ndataset with verifiable provenance. It is a curated approximately 10% open\nsubset of an original 1.2 million, quality-accepted corpus of food images\nannotated for a wide range of information (such as dish name, region of\ncreation). The corpus was collected over six weeks from over 87,000\ncontributors using the Codatta contribution model, which combines community\nsourcing with configurable AI-assisted quality checks; each submission is\nlinked to a wallet address in a secure off-chain ledger for traceability, with\na full on-chain protocol on the roadmap. We describe the schema, pipeline, and\nQA, and validate utility by fine-tuning large vision-language models (ChatGPT\n5, ChatGPT OSS, Qwen-Max) on image-based nutrition prediction. Fine-tuning\nyields consistent gains over out-of-box baselines across standard metrics; we\nreport results primarily on the MM-Food-100K subset. We release MM-Food-100K\nfor publicly free access and retain approximately 90% for potential commercial\naccess with revenue sharing to contributors.", "AI": {"tldr": "MM-Food-100K\u662f\u4e00\u4e2a\u516c\u5f00\u768410\u4e07\u6837\u672c\u591a\u6a21\u6001\u98df\u54c1\u6570\u636e\u96c6\uff0c\u5177\u6709\u53ef\u9a8c\u8bc1\u6765\u6e90\uff0c\u7528\u4e8e\u98df\u54c1\u667a\u80fd\u7814\u7a76\u3002", "motivation": "\u63d0\u4f9b\u9ad8\u8d28\u91cf\u3001\u53ef\u8ffd\u6eaf\u7684\u98df\u54c1\u6570\u636e\u96c6\uff0c\u652f\u6301\u591a\u6a21\u6001\u98df\u54c1\u667a\u80fd\u7814\u7a76\u3002", "method": "\u901a\u8fc7Codatta\u8d21\u732e\u6a21\u578b\u6536\u96c6\u6570\u636e\uff0c\u7ed3\u5408\u793e\u533a\u4f17\u5305\u548cAI\u8f85\u52a9\u8d28\u91cf\u68c0\u67e5\uff0c\u5e76\u91c7\u7528\u94fe\u4e0b\u8d26\u672c\u786e\u4fdd\u53ef\u8ffd\u6eaf\u6027\u3002", "result": "\u5728\u56fe\u50cf\u8425\u517b\u9884\u6d4b\u4efb\u52a1\u4e2d\uff0c\u5fae\u8c03\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08\u5982ChatGPT 5\uff09\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u3002", "conclusion": "MM-Food-100K\u7684\u53d1\u5e03\u4e3a\u98df\u54c1\u667a\u80fd\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u8d44\u6e90\uff0c\u672a\u6765\u53ef\u80fd\u6269\u5c55\u5546\u4e1a\u7528\u9014\u3002"}}
{"id": "2508.10798", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10798", "abs": "https://arxiv.org/abs/2508.10798", "authors": ["Troi Williams"], "title": "The SET Perceptual Factors Framework: Towards Assured Perception for Autonomous Systems", "comment": "4 pages, 4 figures, accepted to the Workshop on Public Trust in\n  Autonomous Systems at the 2025 IEEE International Conference on Robotics &\n  Automation", "summary": "Future autonomous systems promise significant societal benefits, yet their\ndeployment raises concerns about safety and trustworthiness. A key concern is\nassuring the reliability of robot perception, as perception seeds safe\ndecision-making. Failures in perception are often due to complex yet common\nenvironmental factors and can lead to accidents that erode public trust. To\naddress this concern, we introduce the SET (Self, Environment, and Target)\nPerceptual Factors Framework. We designed the framework to systematically\nanalyze how factors such as weather, occlusion, or sensor limitations\nnegatively impact perception. To achieve this, the framework employs SET State\nTrees to categorize where such factors originate and SET Factor Trees to model\nhow these sources and factors impact perceptual tasks like object detection or\npose estimation. Next, we develop Perceptual Factor Models using both trees to\nquantify the uncertainty for a given task. Our framework aims to promote\nrigorous safety assurances and cultivate greater public understanding and trust\nin autonomous systems by offering a transparent and standardized method for\nidentifying, modeling, and communicating perceptual risks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faSET\u611f\u77e5\u56e0\u7d20\u6846\u67b6\uff0c\u7528\u4e8e\u7cfb\u7edf\u5206\u6790\u73af\u5883\u56e0\u7d20\u5bf9\u673a\u5668\u4eba\u611f\u77e5\u7684\u5f71\u54cd\uff0c\u4ee5\u63d0\u5347\u81ea\u4e3b\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u53ef\u4fe1\u5ea6\u3002", "motivation": "\u81ea\u4e3b\u7cfb\u7edf\u7684\u90e8\u7f72\u5f15\u53d1\u4e86\u5bf9\u5b89\u5168\u548c\u53ef\u4fe1\u5ea6\u7684\u62c5\u5fe7\uff0c\u5c24\u5176\u662f\u611f\u77e5\u6545\u969c\u53ef\u80fd\u5bfc\u81f4\u4e8b\u6545\uff0c\u524a\u5f31\u516c\u4f17\u4fe1\u4efb\u3002", "method": "\u5f15\u5165SET\u6846\u67b6\uff0c\u901a\u8fc7SET\u72b6\u6001\u6811\u548c\u56e0\u7d20\u6811\u5206\u7c7b\u548c\u5efa\u6a21\u611f\u77e5\u4efb\u52a1\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u5f00\u53d1\u611f\u77e5\u56e0\u7d20\u6a21\u578b\u91cf\u5316\u98ce\u9669\u3002", "result": "SET\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u900f\u660e\u3001\u6807\u51c6\u5316\u7684\u65b9\u6cd5\u6765\u8bc6\u522b\u3001\u5efa\u6a21\u548c\u4f20\u8fbe\u611f\u77e5\u98ce\u9669\u3002", "conclusion": "\u8be5\u6846\u67b6\u65e8\u5728\u901a\u8fc7\u7cfb\u7edf\u6027\u5206\u6790\u611f\u77e5\u98ce\u9669\uff0c\u589e\u5f3a\u81ea\u4e3b\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u516c\u4f17\u4fe1\u4efb\u3002"}}
{"id": "2508.10433", "categories": ["cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10433", "abs": "https://arxiv.org/abs/2508.10433", "authors": ["Runqi Qiao", "Qiuna Tan", "Peiqing Yang", "Yanzi Wang", "Xiaowan Wang", "Enhui Wan", "Sitong Zhou", "Guanting Dong", "Yuchen Zeng", "Yida Xu", "Jie Wang", "Chong Sun", "Chen Li", "Honggang Zhang"], "title": "We-Math 2.0: A Versatile MathBook System for Incentivizing Visual Mathematical Reasoning", "comment": "Working in progress", "summary": "Multimodal Large Language Models (MLLMs) have demonstrated impressive\ncapabilities across various tasks, but still struggle with complex mathematical\nreasoning. Existing research primarily focuses on dataset construction and\nmethod optimization, often overlooking two critical aspects: comprehensive\nknowledge-driven design and model-centric data space modeling. In this paper,\nwe introduce We-Math 2.0, a unified system that integrates a structured\nmathematical knowledge system, model-centric data space modeling, and a\nreinforcement learning (RL)-based training paradigm to comprehensively enhance\nthe mathematical reasoning abilities of MLLMs. The key contributions of We-Math\n2.0 are fourfold: (1) MathBook Knowledge System: We construct a five-level\nhierarchical system encompassing 491 knowledge points and 1,819 fundamental\nprinciples. (2) MathBook-Standard & Pro: We develop MathBook-Standard, a\ndataset that ensures broad conceptual coverage and flexibility through dual\nexpansion. Additionally, we define a three-dimensional difficulty space and\ngenerate 7 progressive variants per problem to build MathBook-Pro, a\nchallenging dataset for robust training. (3) MathBook-RL: We propose a\ntwo-stage RL framework comprising: (i) Cold-Start Fine-tuning, which aligns the\nmodel with knowledge-oriented chain-of-thought reasoning; and (ii) Progressive\nAlignment RL, leveraging average-reward learning and dynamic data scheduling to\nachieve progressive alignment across difficulty levels. (4) MathBookEval: We\nintroduce a comprehensive benchmark covering all 491 knowledge points with\ndiverse reasoning step distributions. Experimental results show that\nMathBook-RL performs competitively with existing baselines on four widely-used\nbenchmarks and achieves strong results on MathBookEval, suggesting promising\ngeneralization in mathematical reasoning.", "AI": {"tldr": "We-Math 2.0\u901a\u8fc7\u7ed3\u6784\u5316\u6570\u5b66\u77e5\u8bc6\u7cfb\u7edf\u3001\u6a21\u578b\u4e2d\u5fc3\u6570\u636e\u7a7a\u95f4\u5efa\u6a21\u548c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u8303\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u6570\u636e\u96c6\u6784\u5efa\u548c\u65b9\u6cd5\u4f18\u5316\uff0c\u4f46\u5ffd\u89c6\u4e86\u77e5\u8bc6\u9a71\u52a8\u8bbe\u8ba1\u548c\u6570\u636e\u7a7a\u95f4\u5efa\u6a21\u7684\u91cd\u8981\u6027\u3002", "method": "\u63d0\u51faMathBook\u77e5\u8bc6\u7cfb\u7edf\u3001MathBook-Standard & Pro\u6570\u636e\u96c6\u3001MathBook-RL\u4e24\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u548cMathBookEval\u57fa\u51c6\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u5728MathBookEval\u4e0a\u5c55\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "We-Math 2.0\u4e3a\u63d0\u5347MLLMs\u7684\u6570\u5b66\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u5168\u9762\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.10828", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10828", "abs": "https://arxiv.org/abs/2508.10828", "authors": ["Henry Powell", "Guy Laban", "Emily S. Cross"], "title": "A Multimodal Neural Network for Recognizing Subjective Self-Disclosure Towards Social Robots", "comment": "Accepted at 2025 IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS)", "summary": "Subjective self-disclosure is an important feature of human social\ninteraction. While much has been done in the social and behavioural literature\nto characterise the features and consequences of subjective self-disclosure,\nlittle work has been done thus far to develop computational systems that are\nable to accurately model it. Even less work has been done that attempts to\nmodel specifically how human interactants self-disclose with robotic partners.\nIt is becoming more pressing as we require social robots to work in conjunction\nwith and establish relationships with humans in various social settings. In\nthis paper, our aim is to develop a custom multimodal attention network based\non models from the emotion recognition literature, training this model on a\nlarge self-collected self-disclosure video corpus, and constructing a new loss\nfunction, the scale preserving cross entropy loss, that improves upon both\nclassification and regression versions of this problem. Our results show that\nthe best performing model, trained with our novel loss function, achieves an F1\nscore of 0.83, an improvement of 0.48 from the best baseline model. This result\nmakes significant headway in the aim of allowing social robots to pick up on an\ninteraction partner's self-disclosures, an ability that will be essential in\nsocial robots with social cognition.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u60c5\u611f\u8bc6\u522b\u6587\u732e\u7684\u591a\u6a21\u6001\u6ce8\u610f\u529b\u7f51\u7edc\uff0c\u7528\u4e8e\u5efa\u6a21\u4eba\u7c7b\u4e0e\u673a\u5668\u4eba\u4e92\u52a8\u4e2d\u7684\u4e3b\u89c2\u81ea\u6211\u62ab\u9732\u884c\u4e3a\uff0c\u5e76\u901a\u8fc7\u65b0\u8bbe\u8ba1\u7684\u635f\u5931\u51fd\u6570\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u4e3b\u89c2\u81ea\u6211\u62ab\u9732\u662f\u4eba\u7c7b\u793e\u4ea4\u4e92\u52a8\u7684\u91cd\u8981\u7279\u5f81\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u8ba1\u7b97\u6a21\u578b\u6765\u51c6\u786e\u5efa\u6a21\uff0c\u5c24\u5176\u662f\u5728\u4eba\u7c7b\u4e0e\u673a\u5668\u4eba\u4e92\u52a8\u4e2d\u7684\u81ea\u6211\u62ab\u9732\u884c\u4e3a\u3002\u968f\u7740\u793e\u4ea4\u673a\u5668\u4eba\u5728\u591a\u79cd\u793e\u4ea4\u573a\u666f\u4e2d\u7684\u5e94\u7528\u9700\u6c42\u589e\u52a0\uff0c\u8fd9\u4e00\u7814\u7a76\u53d8\u5f97\u5c24\u4e3a\u91cd\u8981\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u60c5\u611f\u8bc6\u522b\u6587\u732e\u7684\u591a\u6a21\u6001\u6ce8\u610f\u529b\u7f51\u7edc\uff0c\u4f7f\u7528\u81ea\u6536\u96c6\u7684\u81ea\u6211\u62ab\u9732\u89c6\u9891\u8bed\u6599\u5e93\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u7684\u635f\u5931\u51fd\u6570\u2014\u2014\u5c3a\u5ea6\u4fdd\u6301\u4ea4\u53c9\u71b5\u635f\u5931\u3002", "result": "\u6700\u4f73\u6a21\u578b\u5728\u65b0\u635f\u5931\u51fd\u6570\u4e0b\u53d6\u5f97\u4e86F1\u5206\u65700.83\uff0c\u6bd4\u57fa\u7ebf\u6a21\u578b\u63d0\u5347\u4e860.48\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u793e\u4ea4\u673a\u5668\u4eba\u8bc6\u522b\u4e92\u52a8\u4f19\u4f34\u7684\u81ea\u6211\u62ab\u9732\u884c\u4e3a\u63d0\u4f9b\u4e86\u91cd\u8981\u8fdb\u5c55\uff0c\u8fd9\u5bf9\u5177\u5907\u793e\u4ea4\u8ba4\u77e5\u80fd\u529b\u7684\u673a\u5668\u4eba\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2508.10467", "categories": ["cs.AI", "cs.DL"], "pdf": "https://arxiv.org/pdf/2508.10467", "abs": "https://arxiv.org/abs/2508.10467", "authors": ["Xueli Pan", "Victor de Boer", "Jacco van Ossenbruggen"], "title": "FIRESPARQL: A LLM-based Framework for SPARQL Query Generation over Scholarly Knowledge Graphs", "comment": "Accepted at 17th International Joint Conference on Knowledge\n  Discovery, Knowledge Engineering and Knowledge Management (IC3K)", "summary": "Question answering over Scholarly Knowledge Graphs (SKGs) remains a\nchallenging task due to the complexity of scholarly content and the intricate\nstructure of these graphs. Large Language Model (LLM) approaches could be used\nto translate natural language questions (NLQs) into SPARQL queries; however,\nthese LLM-based approaches struggle with SPARQL query generation due to limited\nexposure to SKG-specific content and the underlying schema. We identified two\nmain types of errors in the LLM-generated SPARQL queries: (i) structural\ninconsistencies, such as missing or redundant triples in the queries, and (ii)\nsemantic inaccuracies, where incorrect entities or properties are shown in the\nqueries despite a correct query structure. To address these issues, we propose\nFIRESPARQL, a modular framework that supports fine-tuned LLMs as a core\ncomponent, with optional context provided via retrieval-augmented generation\n(RAG) and a SPARQL query correction layer. We evaluate the framework on the\nSciQA Benchmark using various configurations (zero-shot, zero-shot with RAG,\none-shot, fine-tuning, and fine-tuning with RAG) and compare the performance\nwith baseline and state-of-the-art approaches. We measure query accuracy using\nBLEU and ROUGE metrics, and query result accuracy using relaxed exact\nmatch(RelaxedEM), with respect to the gold standards containing the NLQs,\nSPARQL queries, and the results of the queries. Experimental results\ndemonstrate that fine-tuning achieves the highest overall performance, reaching\n0.90 ROUGE-L for query accuracy and 0.85 RelaxedEM for result accuracy on the\ntest set.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faFIRESPARQL\u6846\u67b6\uff0c\u901a\u8fc7\u5fae\u8c03LLM\u3001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548cSPARQL\u67e5\u8be2\u4fee\u6b63\u5c42\uff0c\u89e3\u51b3\u5b66\u672f\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u4e2dLLM\u751f\u6210SPARQL\u67e5\u8be2\u7684\u7ed3\u6784\u548c\u8bed\u4e49\u9519\u8bef\u3002", "motivation": "\u5b66\u672f\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u4e2d\uff0cLLM\u751f\u6210\u7684SPARQL\u67e5\u8be2\u5b58\u5728\u7ed3\u6784\u4e0d\u4e00\u81f4\u548c\u8bed\u4e49\u4e0d\u51c6\u786e\u7684\u95ee\u9898\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "\u63d0\u51faFIRESPARQL\u6846\u67b6\uff0c\u7ed3\u5408\u5fae\u8c03LLM\u3001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u548cSPARQL\u67e5\u8be2\u4fee\u6b63\u5c42\uff0c\u8bc4\u4f30\u4e0d\u540c\u914d\u7f6e\uff08\u96f6\u6837\u672c\u3001\u5355\u6837\u672c\u3001\u5fae\u8c03\u7b49\uff09\u3002", "result": "\u5fae\u8c03\u914d\u7f6e\u8868\u73b0\u6700\u4f73\uff0c\u6d4b\u8bd5\u96c6\u4e0aROUGE-L\u4e3a0.90\uff0cRelaxedEM\u4e3a0.85\u3002", "conclusion": "FIRESPARQL\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u5b66\u672f\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u7684\u51c6\u786e\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2508.10867", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.10867", "abs": "https://arxiv.org/abs/2508.10867", "authors": ["Yizhi Zhou", "Ziwei Kang", "Jiawei Xia", "Xuan Wang"], "title": "CVIRO: A Consistent and Tightly-Coupled Visual-Inertial-Ranging Odometry on Lie Groups", "comment": null, "summary": "Ultra Wideband (UWB) is widely used to mitigate drift in visual-inertial\nodometry (VIO) systems. Consistency is crucial for ensuring the estimation\naccuracy of a UWBaided VIO system. An inconsistent estimator can degrade\nlocalization performance, where the inconsistency primarily arises from two\nmain factors: (1) the estimator fails to preserve the correct system\nobservability, and (2) UWB anchor positions are assumed to be known, leading to\nimproper neglect of calibration uncertainty. In this paper, we propose a\nconsistent and tightly-coupled visual-inertial-ranging odometry (CVIRO) system\nbased on the Lie group. Our method incorporates the UWB anchor state into the\nsystem state, explicitly accounting for UWB calibration uncertainty and\nenabling the joint and consistent estimation of both robot and anchor states.\nFurthermore, observability consistency is ensured by leveraging the invariant\nerror properties of the Lie group. We analytically prove that the CVIRO\nalgorithm naturally maintains the system's correct unobservable subspace,\nthereby preserving estimation consistency. Extensive simulations and\nexperiments demonstrate that CVIRO achieves superior localization accuracy and\nconsistency compared to existing methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u674e\u7fa4\u7684\u89c6\u89c9-\u60ef\u6027-\u6d4b\u8ddd\u91cc\u7a0b\u8ba1\u7cfb\u7edf\uff08CVIRO\uff09\uff0c\u901a\u8fc7\u8054\u5408\u4f30\u8ba1\u673a\u5668\u4eba\u72b6\u6001\u548cUWB\u951a\u70b9\u72b6\u6001\uff0c\u786e\u4fdd\u4e00\u81f4\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u89c6\u89c9-\u60ef\u6027\u91cc\u7a0b\u8ba1\uff08VIO\uff09\u7cfb\u7edf\u4e2d\u56e0\u89c2\u6d4b\u6027\u4e0d\u4e00\u81f4\u548cUWB\u951a\u70b9\u4f4d\u7f6e\u5047\u8bbe\u5bfc\u81f4\u7684\u5b9a\u4f4d\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "method": "\u5c06UWB\u951a\u70b9\u72b6\u6001\u7eb3\u5165\u7cfb\u7edf\u72b6\u6001\uff0c\u5229\u7528\u674e\u7fa4\u7684\u4e0d\u53d8\u8bef\u5dee\u7279\u6027\uff0c\u786e\u4fdd\u89c2\u6d4b\u6027\u4e00\u81f4\u6027\u3002", "result": "CVIRO\u5728\u4eff\u771f\u548c\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u4e00\u81f4\u6027\u3002", "conclusion": "CVIRO\u901a\u8fc7\u8054\u5408\u4f30\u8ba1\u548c\u89c2\u6d4b\u6027\u4e00\u81f4\u6027\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b9a\u4f4d\u7cfb\u7edf\u7684\u6027\u80fd\u3002"}}
{"id": "2508.10486", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10486", "abs": "https://arxiv.org/abs/2508.10486", "authors": ["Ivan Khai Ze Lim", "Ningyi Liao", "Yiming Yang", "Gerald Wei Yong Yip", "Siqiang Luo"], "title": "SEQ-GPT: LLM-assisted Spatial Query via Example", "comment": null, "summary": "Contemporary spatial services such as online maps predominantly rely on user\nqueries for location searches. However, the user experience is limited when\nperforming complex tasks, such as searching for a group of locations\nsimultaneously. In this study, we examine the extended scenario known as\nSpatial Exemplar Query (SEQ), where multiple relevant locations are jointly\nsearched based on user-specified examples. We introduce SEQ-GPT, a spatial\nquery system powered by Large Language Models (LLMs) towards more versatile SEQ\nsearch using natural language. The language capabilities of LLMs enable unique\ninteractive operations in the SEQ process, including asking users to clarify\nquery details and dynamically adjusting the search based on user feedback. We\nalso propose a tailored LLM adaptation pipeline that aligns natural language\nwith structured spatial data and queries through dialogue synthesis and\nmulti-model cooperation. SEQ-GPT offers an end-to-end demonstration for\nbroadening spatial search with realistic data and application scenarios.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u7a7a\u95f4\u67e5\u8be2\u7cfb\u7edfSEQ-GPT\uff0c\u7528\u4e8e\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u5b9e\u73b0\u66f4\u7075\u6d3b\u7684\u7a7a\u95f4\u8303\u4f8b\u67e5\u8be2\uff08SEQ\uff09\u3002", "motivation": "\u5f53\u524d\u7a7a\u95f4\u670d\u52a1\uff08\u5982\u5728\u7ebf\u5730\u56fe\uff09\u4e3b\u8981\u4f9d\u8d56\u7528\u6237\u67e5\u8be2\u8fdb\u884c\u4f4d\u7f6e\u641c\u7d22\uff0c\u4f46\u5728\u6267\u884c\u590d\u6742\u4efb\u52a1\uff08\u5982\u540c\u65f6\u641c\u7d22\u591a\u4e2a\u76f8\u5173\u4f4d\u7f6e\uff09\u65f6\u7528\u6237\u4f53\u9a8c\u53d7\u9650\u3002", "method": "\u5f15\u5165SEQ-GPT\u7cfb\u7edf\uff0c\u5229\u7528LLMs\u7684\u8bed\u8a00\u80fd\u529b\u5b9e\u73b0\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\uff0c\u5305\u62ec\u6f84\u6e05\u67e5\u8be2\u7ec6\u8282\u548c\u52a8\u6001\u8c03\u6574\u641c\u7d22\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9a\u5236\u5316\u7684LLM\u9002\u5e94\u6d41\u7a0b\uff0c\u901a\u8fc7\u5bf9\u8bdd\u5408\u6210\u548c\u591a\u6a21\u578b\u534f\u4f5c\u5c06\u81ea\u7136\u8bed\u8a00\u4e0e\u7ed3\u6784\u5316\u7a7a\u95f4\u6570\u636e\u5bf9\u9f50\u3002", "result": "SEQ-GPT\u4e3a\u6269\u5c55\u7a7a\u95f4\u641c\u7d22\u63d0\u4f9b\u4e86\u7aef\u5230\u7aef\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u771f\u5b9e\u6570\u636e\u548c\u5b9e\u9645\u5e94\u7528\u573a\u666f\u3002", "conclusion": "SEQ-GPT\u901a\u8fc7LLMs\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u7a7a\u95f4\u67e5\u8be2\u7684\u4ea4\u4e92\u6027\u548c\u7075\u6d3b\u6027\u3002"}}
{"id": "2508.10872", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10872", "abs": "https://arxiv.org/abs/2508.10872", "authors": ["Anantha Narayanan", "Battu Bhanu Teja", "Pruthwik Mishra"], "title": "TLE-Based A2C Agent for Terrestrial Coverage Orbital Path Planning", "comment": "8 pages, 6 figures, 5 tables", "summary": "The increasing congestion of Low Earth Orbit (LEO) poses persistent\nchallenges to the efficient deployment and safe operation of Earth observation\nsatellites. Mission planners must now account not only for mission-specific\nrequirements but also for the increasing collision risk with active satellites\nand space debris. This work presents a reinforcement learning framework using\nthe Advantage Actor-Critic (A2C) algorithm to optimize satellite orbital\nparameters for precise terrestrial coverage within predefined surface radii. By\nformulating the problem as a Markov Decision Process (MDP) within a custom\nOpenAI Gymnasium environment, our method simulates orbital dynamics using\nclassical Keplerian elements. The agent progressively learns to adjust five of\nthe orbital parameters - semi-major axis, eccentricity, inclination, right\nascension of ascending node, and the argument of perigee-to achieve targeted\nterrestrial coverage. Comparative evaluation against Proximal Policy\nOptimization (PPO) demonstrates A2C's superior performance, achieving 5.8x\nhigher cumulative rewards (10.0 vs 9.263025) while converging in 31.5x fewer\ntimesteps (2,000 vs 63,000). The A2C agent consistently meets mission\nobjectives across diverse target coordinates while maintaining computational\nefficiency suitable for real-time mission planning applications. Key\ncontributions include: (1) a TLE-based orbital simulation environment\nincorporating physics constraints, (2) validation of actor-critic methods'\nsuperiority over trust region approaches in continuous orbital control, and (3)\ndemonstration of rapid convergence enabling adaptive satellite deployment. This\napproach establishes reinforcement learning as a computationally efficient\nalternative for scalable and intelligent LEO mission planning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\uff08A2C\u7b97\u6cd5\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u4f18\u5316\u4f4e\u5730\u7403\u8f68\u9053\u536b\u661f\u7684\u8f68\u9053\u53c2\u6570\uff0c\u4ee5\u5b9e\u73b0\u7cbe\u786e\u7684\u5730\u9762\u8986\u76d6\uff0c\u5e76\u51cf\u5c11\u78b0\u649e\u98ce\u9669\u3002", "motivation": "\u4f4e\u5730\u7403\u8f68\u9053\u65e5\u76ca\u62e5\u6324\uff0c\u5bf9\u5730\u7403\u89c2\u6d4b\u536b\u661f\u7684\u9ad8\u6548\u90e8\u7f72\u548c\u5b89\u5168\u8fd0\u884c\u63d0\u51fa\u4e86\u6311\u6218\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u5728\u6ee1\u8db3\u4efb\u52a1\u9700\u6c42\u7684\u540c\u65f6\u964d\u4f4e\u78b0\u649e\u98ce\u9669\u3002", "method": "\u4f7f\u7528A2C\u7b97\u6cd5\uff0c\u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDP\uff09\uff0c\u5728\u81ea\u5b9a\u4e49\u7684OpenAI Gymnasium\u73af\u5883\u4e2d\u6a21\u62df\u8f68\u9053\u52a8\u529b\u5b66\uff0c\u8c03\u6574\u4e94\u4e2a\u8f68\u9053\u53c2\u6570\u4ee5\u5b9e\u73b0\u76ee\u6807\u8986\u76d6\u3002", "result": "A2C\u5728\u7d2f\u79ef\u5956\u52b1\uff0810.0 vs 9.263025\uff09\u548c\u6536\u655b\u901f\u5ea6\uff082,000 vs 63,000\u65f6\u95f4\u6b65\uff09\u4e0a\u4f18\u4e8ePPO\uff0c\u4e14\u80fd\u9ad8\u6548\u6ee1\u8db3\u4efb\u52a1\u9700\u6c42\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u4e3a\u53ef\u6269\u5c55\u4e14\u667a\u80fd\u7684\u4f4e\u5730\u7403\u8f68\u9053\u4efb\u52a1\u89c4\u5212\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u8ba1\u7b97\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2508.10492", "categories": ["cs.AI", "cs.CE", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.10492", "abs": "https://arxiv.org/abs/2508.10492", "authors": ["Shicheng Xu", "Xin Huang", "Zihao Wei", "Liang Pang", "Huawei Shen", "Xueqi Cheng"], "title": "Reverse Physician-AI Relationship: Full-process Clinical Diagnosis Driven by a Large Language Model", "comment": "39 pages", "summary": "Full-process clinical diagnosis in the real world encompasses the entire\ndiagnostic workflow that begins with only an ambiguous chief complaint. While\nartificial intelligence (AI), particularly large language models (LLMs), is\ntransforming clinical diagnosis, its role remains largely as an assistant to\nphysicians. This AI-assisted working pattern makes AI can only answer specific\nmedical questions at certain parts within the diagnostic process, but lack the\nability to drive the entire diagnostic process starting from an ambiguous\ncomplaint, which still relies heavily on human physicians. This gap limits AI's\nability to fully reduce physicians' workload and enhance diagnostic efficiency.\nTo address this, we propose a paradigm shift that reverses the relationship\nbetween physicians and AI: repositioning AI as the primary director, with\nphysicians serving as its assistants. So we present DxDirector-7B, an LLM\nendowed with advanced deep thinking capabilities, enabling it to drive the\nfull-process diagnosis with minimal physician involvement. Furthermore,\nDxDirector-7B establishes a robust accountability framework for misdiagnoses,\ndelineating responsibility between AI and human physicians. In evaluations\nacross rare, complex, and real-world cases under full-process diagnosis\nsetting, DxDirector-7B not only achieves significant superior diagnostic\naccuracy but also substantially reduces physician workload than\nstate-of-the-art medical LLMs as well as general-purpose LLMs. Fine-grained\nanalyses across multiple clinical departments and tasks validate its efficacy,\nwith expert evaluations indicating its potential to serve as a viable\nsubstitute for medical specialists. These findings mark a new era where AI,\ntraditionally a physicians' assistant, now drives the entire diagnostic process\nto drastically reduce physicians' workload, indicating an efficient and\naccurate diagnostic solution.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faDxDirector-7B\uff0c\u4e00\u79cd\u80fd\u591f\u4e3b\u5bfc\u5168\u6d41\u7a0b\u4e34\u5e8a\u8bca\u65ad\u7684AI\u6a21\u578b\uff0c\u663e\u8457\u51cf\u5c11\u533b\u751f\u5de5\u4f5c\u91cf\u5e76\u63d0\u9ad8\u8bca\u65ad\u51c6\u786e\u6027\u3002", "motivation": "\u5f53\u524dAI\u5728\u4e34\u5e8a\u8bca\u65ad\u4e2d\u4ec5\u4f5c\u4e3a\u8f85\u52a9\u5de5\u5177\uff0c\u65e0\u6cd5\u4e3b\u5bfc\u5168\u6d41\u7a0b\u8bca\u65ad\uff0c\u9650\u5236\u4e86\u5176\u51cf\u8f7b\u533b\u751f\u8d1f\u62c5\u548c\u63d0\u5347\u6548\u7387\u7684\u6f5c\u529b\u3002", "method": "\u63d0\u51faDxDirector-7B\uff0c\u4e00\u79cd\u5177\u5907\u6df1\u5ea6\u601d\u8003\u80fd\u529b\u7684LLM\uff0c\u80fd\u591f\u4e3b\u5bfc\u8bca\u65ad\u6d41\u7a0b\u5e76\u5efa\u7acb\u8d23\u4efb\u6846\u67b6\u3002", "result": "\u5728\u7f55\u89c1\u3001\u590d\u6742\u548c\u771f\u5b9e\u6848\u4f8b\u4e2d\uff0cDxDirector-7B\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u533b\u5b66LLM\u548c\u901a\u7528LLM\uff0c\u51cf\u5c11\u533b\u751f\u5de5\u4f5c\u91cf\u3002", "conclusion": "DxDirector-7B\u6807\u5fd7\u7740AI\u4ece\u8f85\u52a9\u5de5\u5177\u8f6c\u53d8\u4e3a\u8bca\u65ad\u4e3b\u5bfc\u8005\uff0c\u4e3a\u9ad8\u6548\u51c6\u786e\u7684\u8bca\u65ad\u63d0\u4f9b\u4e86\u65b0\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.10747", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.10747", "abs": "https://arxiv.org/abs/2508.10747", "authors": ["Sangwoo Jeon", "Juchul Shin", "Gyeong-Tae Kim", "YeonJe Cho", "Seongwoo Kim"], "title": "Scaling Up without Fading Out: Goal-Aware Sparse GNN for RL-based Generalized Planning", "comment": "16 pages, 10 figures", "summary": "Generalized planning using deep reinforcement learning (RL) combined with\ngraph neural networks (GNNs) has shown promising results in various symbolic\nplanning domains described by PDDL. However, existing approaches typically\nrepresent planning states as fully connected graphs, leading to a combinatorial\nexplosion in edge information and substantial sparsity as problem scales grow,\nespecially evident in large grid-based environments. This dense representation\nresults in diluted node-level information, exponentially increases memory\nrequirements, and ultimately makes learning infeasible for larger-scale\nproblems. To address these challenges, we propose a sparse, goal-aware GNN\nrepresentation that selectively encodes relevant local relationships and\nexplicitly integrates spatial features related to the goal. We validate our\napproach by designing novel drone mission scenarios based on PDDL within a grid\nworld, effectively simulating realistic mission execution environments. Our\nexperimental results demonstrate that our method scales effectively to larger\ngrid sizes previously infeasible with dense graph representations and\nsubstantially improves policy generalization and success rates. Our findings\nprovide a practical foundation for addressing realistic, large-scale\ngeneralized planning tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7a00\u758f\u3001\u76ee\u6807\u611f\u77e5\u7684GNN\u8868\u793a\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u89c4\u5212\u95ee\u9898\u4e2d\u5bc6\u96c6\u56fe\u8868\u793a\u7684\u4fe1\u606f\u7a00\u91ca\u548c\u5185\u5b58\u7206\u70b8\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f7f\u7528\u5168\u8fde\u63a5\u56fe\u8868\u793a\u89c4\u5212\u72b6\u6001\uff0c\u5bfc\u81f4\u4fe1\u606f\u7a00\u758f\u548c\u5185\u5b58\u9700\u6c42\u6307\u6570\u589e\u957f\uff0c\u96be\u4ee5\u5e94\u5bf9\u5927\u89c4\u6a21\u95ee\u9898\u3002", "method": "\u91c7\u7528\u7a00\u758f\u3001\u76ee\u6807\u611f\u77e5\u7684GNN\u8868\u793a\uff0c\u9009\u62e9\u6027\u7f16\u7801\u5c40\u90e8\u5173\u7cfb\u5e76\u663e\u5f0f\u6574\u5408\u76ee\u6807\u76f8\u5173\u7a7a\u95f4\u7279\u5f81\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u7f51\u683c\u73af\u5883\u4e2d\u6709\u6548\u6269\u5c55\uff0c\u663e\u8457\u63d0\u5347\u7b56\u7565\u6cdb\u5316\u6027\u548c\u6210\u529f\u7387\u3002", "conclusion": "\u4e3a\u5927\u89c4\u6a21\u5e7f\u4e49\u89c4\u5212\u4efb\u52a1\u63d0\u4f9b\u4e86\u5b9e\u7528\u57fa\u7840\u3002"}}
{"id": "2508.10501", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.10501", "abs": "https://arxiv.org/abs/2508.10501", "authors": ["Yushi Feng", "Junye Du", "Yingying Hong", "Qifan Wang", "Lequan Yu"], "title": "PASS: Probabilistic Agentic Supernet Sampling for Interpretable and Adaptive Chest X-Ray Reasoning", "comment": null, "summary": "Existing tool-augmented agentic systems are limited in the real world by (i)\nblack-box reasoning steps that undermine trust of decision-making and pose\nsafety risks, (ii) poor multimodal integration, which is inherently critical\nfor healthcare tasks, and (iii) rigid and computationally inefficient agentic\npipelines. We introduce PASS (Probabilistic Agentic Supernet Sampling), the\nfirst multimodal framework to address these challenges in the context of Chest\nX-Ray (CXR) reasoning. PASS adaptively samples agentic workflows over a\nmulti-tool graph, yielding decision paths annotated with interpretable\nprobabilities. Given the complex CXR reasoning task with multimodal medical\ndata, PASS leverages its learned task-conditioned distribution over the agentic\nsupernet. Thus, it adaptively selects the most suitable tool at each supernet\nlayer, offering probability-annotated trajectories for post-hoc audits and\ndirectly enhancing medical AI safety. PASS also continuously compresses salient\nfindings into an evolving personalized memory, while dynamically deciding\nwhether to deepen its reasoning path or invoke an early exit for efficiency. To\noptimize a Pareto frontier balancing performance and cost, we design a novel\nthree-stage training procedure, including expert knowledge warm-up, contrastive\npath-ranking, and cost-aware reinforcement learning. To facilitate rigorous\nevaluation, we introduce CAB-E, a comprehensive benchmark for multi-step,\nsafety-critical, free-form CXR reasoning. Experiments across various benchmarks\nvalidate that PASS significantly outperforms strong baselines in multiple\nmetrics (e.g., accuracy, AUC, LLM-J.) while balancing computational costs,\npushing a new paradigm shift towards interpretable, adaptive, and multimodal\nmedical agentic systems.", "AI": {"tldr": "PASS\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u6846\u67b6\uff0c\u901a\u8fc7\u6982\u7387\u6807\u6ce8\u7684\u51b3\u7b56\u8def\u5f84\u548c\u81ea\u9002\u5e94\u5de5\u5177\u9009\u62e9\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5de5\u5177\u589e\u5f3a\u4ee3\u7406\u7cfb\u7edf\u5728\u533b\u7597AI\u4e2d\u7684\u4fe1\u4efb\u3001\u591a\u6a21\u6001\u96c6\u6210\u548c\u6548\u7387\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7cfb\u7edf\u5b58\u5728\u9ed1\u76d2\u63a8\u7406\u3001\u591a\u6a21\u6001\u96c6\u6210\u4e0d\u8db3\u548c\u6548\u7387\u4f4e\u4e0b\u7b49\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5728\u533b\u7597\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002PASS\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u5347\u533b\u7597AI\u7684\u5b89\u5168\u6027\u548c\u9002\u5e94\u6027\u3002", "method": "PASS\u901a\u8fc7\u591a\u5de5\u5177\u56fe\u81ea\u9002\u5e94\u91c7\u6837\u4ee3\u7406\u5de5\u4f5c\u6d41\uff0c\u5229\u7528\u4efb\u52a1\u6761\u4ef6\u5206\u5e03\u9009\u62e9\u5de5\u5177\uff0c\u5e76\u901a\u8fc7\u4e09\u9636\u6bb5\u8bad\u7ec3\u4f18\u5316\u6027\u80fd\u4e0e\u6210\u672c\u5e73\u8861\u3002", "result": "PASS\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u540c\u65f6\u5e73\u8861\u4e86\u8ba1\u7b97\u6210\u672c\uff0c\u63a8\u52a8\u4e86\u53ef\u89e3\u91ca\u3001\u81ea\u9002\u5e94\u548c\u591a\u6a21\u6001\u533b\u7597\u4ee3\u7406\u7cfb\u7edf\u7684\u53d1\u5c55\u3002", "conclusion": "PASS\u4e3a\u533b\u7597AI\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u89e3\u91ca\u3001\u9ad8\u6548\u4e14\u591a\u6a21\u6001\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u672a\u6765\u533b\u7597\u4ee3\u7406\u7cfb\u7edf\u8bbe\u5b9a\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2508.10530", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.10530", "abs": "https://arxiv.org/abs/2508.10530", "authors": ["Zetian Sun", "Dongfang Li", "Baotian Hu"], "title": "Diversity First, Quality Later: A Two-Stage Assumption for Language Model Alignment", "comment": null, "summary": "The alignment of language models (LMs) with human preferences is critical for\nbuilding reliable AI systems. The problem is typically framed as optimizing an\nLM policy to maximize the expected reward that reflects human preferences.\nRecently, Direct Preference Optimization (DPO) was proposed as a LM alignment\nmethod that directly optimize the policy from static preference data, and\nfurther improved by incorporating on-policy sampling (i.e., preference\ncandidates generated during the training loop) for better LM alignment.\nHowever, we show on-policy data is not always optimal, with systematic\neffectiveness difference emerging between static and on-policy preference\ncandidates. For example, on-policy data can result in a 3$\\times$ effectiveness\ncompared with static data for Llama-3, and a 0.4$\\times$ effectiveness for\nZephyr. To explain the phenomenon, we propose the alignment stage assumption,\nwhich divides the alignment process into two distinct stages: the preference\ninjection stage, which benefits from diverse data, and the preference\nfine-tuning stage, which favors high-quality data. Through theoretical and\nempirical analysis, we characterize these stages and propose an effective\nalgorithm to identify the boundaries between them. We perform experiments on 5\nmodels (Llama, Zephyr, Phi-2, Qwen, Pythia) and 2 alignment methods (DPO,\nSLiC-HF) to show the generalizability of alignment stage assumption and\nboundary measurement.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u8bed\u8a00\u6a21\u578b\uff08LM\uff09\u5bf9\u9f50\u4eba\u7c7b\u504f\u597d\u7684\u65b9\u6cd5\uff0c\u53d1\u73b0\u52a8\u6001\u7b56\u7565\u6570\u636e\uff08on-policy\uff09\u5e76\u975e\u603b\u662f\u6700\u4f18\uff0c\u5e76\u63d0\u51fa\u5bf9\u9f50\u9636\u6bb5\u5047\u8bbe\u6765\u89e3\u91ca\u8fd9\u4e00\u73b0\u8c61\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u4eba\u7c7b\u504f\u597d\u5bf9\u6784\u5efa\u53ef\u9760AI\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\uff08\u5982DPO\uff09\u7684\u52a8\u6001\u7b56\u7565\u6570\u636e\u6548\u679c\u4e0d\u7a33\u5b9a\uff0c\u9700\u6df1\u5165\u7814\u7a76\u3002", "method": "\u63d0\u51fa\u5bf9\u9f50\u9636\u6bb5\u5047\u8bbe\uff0c\u5c06\u5bf9\u9f50\u5206\u4e3a\u504f\u597d\u6ce8\u5165\u548c\u504f\u597d\u5fae\u8c03\u4e24\u9636\u6bb5\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u548c\u5b9e\u9a8c\u5206\u6790\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u52a8\u6001\u7b56\u7565\u6570\u636e\u5728\u4e0d\u540c\u6a21\u578b\uff08\u5982Llama-3\u548cZephyr\uff09\u4e2d\u6548\u679c\u5dee\u5f02\u663e\u8457\uff083\u00d7 vs. 0.4\u00d7\uff09\uff0c\u9a8c\u8bc1\u4e86\u9636\u6bb5\u5047\u8bbe\u7684\u666e\u9002\u6027\u3002", "conclusion": "\u5bf9\u9f50\u9636\u6bb5\u5047\u8bbe\u4e3aLM\u5bf9\u9f50\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u52a8\u6001\u7b56\u7565\u6570\u636e\u7684\u6548\u679c\u9700\u6839\u636e\u9636\u6bb5\u9009\u62e9\uff0c\u672a\u6765\u53ef\u4f18\u5316\u7b97\u6cd5\u4ee5\u63d0\u5347\u5bf9\u9f50\u6548\u7387\u3002"}}
{"id": "2508.10539", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.10539", "abs": "https://arxiv.org/abs/2508.10539", "authors": ["Zetian Sun", "Dongfang Li", "Baotian Hu", "Min Zhang"], "title": "Improving Value-based Process Verifier via Low-Cost Variance Reduction", "comment": null, "summary": "Large language models (LLMs) have achieved remarkable success in a wide range\nof tasks. However, their reasoning capabilities, particularly in complex\ndomains like mathematics, remain a significant challenge. Value-based process\nverifiers, which estimate the probability of a partial reasoning chain leading\nto a correct solution, are a promising approach for improving reasoning.\nNevertheless, their effectiveness is often hindered by estimation error in\ntheir training annotations, a consequence of the limited number of Monte Carlo\n(MC) samples feasible due to the high cost of LLM inference. In this paper, we\nidentify that the estimation error primarily arises from high variance rather\nthan bias, and the MC estimator is a Minimum Variance Unbiased Estimator\n(MVUE). To address the problem, we propose the \\textsc{Com}pound \\textsc{M}onte\n\\textsc{C}arlo \\textsc{S}ampling (ComMCS) method, which constructs an unbiased\nestimator by linearly combining the MC estimators from the current and\nsubsequent steps. Theoretically, we show that our method leads to a predictable\nreduction in variance, while maintaining an unbiased estimation without\nadditional LLM inference cost. We also perform empirical experiments on the\nMATH-500 and GSM8K benchmarks to demonstrate the effectiveness of our method.\nNotably, ComMCS outperforms regression-based optimization method by 2.8 points,\nthe non-variance-reduced baseline by 2.2 points on MATH-500 on Best-of-32\nsampling experiment.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faComMCS\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u5f53\u524d\u548c\u540e\u7eed\u6b65\u9aa4\u7684\u8499\u7279\u5361\u6d1b\u4f30\u8ba1\u5668\uff0c\u51cf\u5c11\u65b9\u5dee\u5e76\u4fdd\u6301\u65e0\u504f\u4f30\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u9886\u57df\uff08\u5982\u6570\u5b66\uff09\u7684\u63a8\u7406\u80fd\u529b\u4ecd\u6709\u9650\uff0c\u73b0\u6709\u57fa\u4e8e\u503c\u7684\u9a8c\u8bc1\u65b9\u6cd5\u56e0\u8499\u7279\u5361\u6d1b\u91c7\u6837\u65b9\u5dee\u9ad8\u800c\u6548\u679c\u53d7\u9650\u3002", "method": "\u63d0\u51faComMCS\u65b9\u6cd5\uff0c\u7ebf\u6027\u7ec4\u5408\u5f53\u524d\u548c\u540e\u7eed\u6b65\u9aa4\u7684\u8499\u7279\u5361\u6d1b\u4f30\u8ba1\u5668\uff0c\u964d\u4f4e\u65b9\u5dee\u4e14\u65e0\u9700\u989d\u5916\u63a8\u7406\u6210\u672c\u3002", "result": "\u5728MATH-500\u548cGSM8K\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cComMCS\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5982Best-of-32\u5b9e\u9a8c\u4e2d\u63d0\u53472.8\u5206\u3002", "conclusion": "ComMCS\u901a\u8fc7\u51cf\u5c11\u65b9\u5dee\u6709\u6548\u63d0\u5347\u63a8\u7406\u9a8c\u8bc1\u6027\u80fd\uff0c\u4e3a\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u5927\u8bed\u8a00\u6a21\u578b\u4f18\u5316\u63d0\u4f9b\u65b0\u601d\u8def\u3002"}}
{"id": "2508.10599", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10599", "abs": "https://arxiv.org/abs/2508.10599", "authors": ["Xinyan Jiang", "Lin Zhang", "Jiayi Zhang", "Qingsong Yang", "Guimin Hu", "Di Wang", "Lijie Hu"], "title": "MSRS: Adaptive Multi-Subspace Representation Steering for Attribute Alignment in Large Language Models", "comment": null, "summary": "Activation steering offers a promising approach to controlling the behavior\nof Large Language Models by directly manipulating their internal activations.\nHowever, most existing methods struggle to jointly steer multiple attributes,\noften resulting in interference and undesirable trade-offs. To address this\nchallenge, we propose Multi-Subspace Representation Steering (MSRS), a novel\nframework for effective multi-attribute steering via subspace representation\nfine-tuning. MSRS reduces inter-attribute interference by allocating orthogonal\nsubspaces to each attribute, isolating their influence within the model's\nrepresentation space. MSRS also incorporates a hybrid subspace composition\nstrategy: it combines attribute-specific subspaces for unique steering\ndirections with a shared subspace for common steering directions. A dynamic\nweighting function learns to efficiently integrate these components for precise\ncontrol. During inference, MSRS introduces a token-level steering mechanism\nthat dynamically identifies and intervenes on the most semantically relevant\ntokens, enabling fine-grained behavioral modulation. Experimental results show\nthat MSRS significantly reduces attribute conflicts, surpasses existing methods\nacross a range of attributes, and generalizes effectively to diverse downstream\ntasks.", "AI": {"tldr": "MSRS\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u5c5e\u6027\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u6b63\u4ea4\u5b50\u7a7a\u95f4\u5206\u914d\u548c\u6df7\u5408\u5b50\u7a7a\u95f4\u7ec4\u5408\u7b56\u7565\u51cf\u5c11\u5c5e\u6027\u5e72\u6270\uff0c\u5b9e\u73b0\u7cbe\u7ec6\u884c\u4e3a\u8c03\u63a7\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u591a\u5c5e\u6027\u8054\u5408\u63a7\u5236\u65f6\u5b58\u5728\u5e72\u6270\u548c\u6743\u8861\u95ee\u9898\uff0cMSRS\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "MSRS\u901a\u8fc7\u6b63\u4ea4\u5b50\u7a7a\u95f4\u5206\u914d\u9694\u79bb\u5c5e\u6027\u5f71\u54cd\uff0c\u7ed3\u5408\u5171\u4eab\u5b50\u7a7a\u95f4\u548c\u52a8\u6001\u6743\u91cd\u51fd\u6570\u5b9e\u73b0\u7cbe\u786e\u63a7\u5236\uff0c\u5e76\u5f15\u5165\u4ee4\u724c\u7ea7\u8c03\u63a7\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u8868\u660eMSRS\u663e\u8457\u51cf\u5c11\u5c5e\u6027\u51b2\u7a81\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u80fd\u6cdb\u5316\u5230\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u3002", "conclusion": "MSRS\u4e3a\u591a\u5c5e\u6027\u63a7\u5236\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.10669", "categories": ["cs.AI", "cs.IR", "H.3.3; I.2.7; H.2.8"], "pdf": "https://arxiv.org/pdf/2508.10669", "abs": "https://arxiv.org/abs/2508.10669", "authors": ["Zhenye Yang", "Jinpeng Chen", "Huan Li", "Xiongnan Jin", "Xuanyang Li", "Junwei Zhang", "Hongbo Gao", "Kaimin Wei", "Senzhang Wang"], "title": "STEP: Stepwise Curriculum Learning for Context-Knowledge Fusion in Conversational Recommendation", "comment": "10 pages; 4 figures; 6 tables; code available at\n  https://github.com/Alex-bupt/STEP", "summary": "Conversational recommender systems (CRSs) aim to proactively capture user\npreferences through natural language dialogue and recommend high-quality items.\nTo achieve this, CRS gathers user preferences via a dialog module and builds\nuser profiles through a recommendation module to generate appropriate\nrecommendations. However, existing CRS faces challenges in capturing the deep\nsemantics of user preferences and dialogue context. In particular, the\nefficient integration of external knowledge graph (KG) information into\ndialogue generation and recommendation remains a pressing issue. Traditional\napproaches typically combine KG information directly with dialogue content,\nwhich often struggles with complex semantic relationships, resulting in\nrecommendations that may not align with user expectations.\n  To address these challenges, we introduce STEP, a conversational recommender\ncentered on pre-trained language models that combines curriculum-guided\ncontext-knowledge fusion with lightweight task-specific prompt tuning. At its\nheart, an F-Former progressively aligns the dialogue context with\nknowledge-graph entities through a three-stage curriculum, thus resolving\nfine-grained semantic mismatches. The fused representation is then injected\ninto the frozen language model via two minimal yet adaptive prefix prompts: a\nconversation prefix that steers response generation toward user intent and a\nrecommendation prefix that biases item ranking toward knowledge-consistent\ncandidates. This dual-prompt scheme allows the model to share cross-task\nsemantics while respecting the distinct objectives of dialogue and\nrecommendation. Experimental results show that STEP outperforms mainstream\nmethods in the precision of recommendation and dialogue quality in two public\ndatasets.", "AI": {"tldr": "STEP\u662f\u4e00\u79cd\u57fa\u4e8e\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u5bf9\u8bdd\u63a8\u8350\u7cfb\u7edf\uff0c\u901a\u8fc7\u8bfe\u7a0b\u5f15\u5bfc\u7684\u4e0a\u4e0b\u6587-\u77e5\u8bc6\u878d\u5408\u548c\u8f7b\u91cf\u7ea7\u4efb\u52a1\u7279\u5b9a\u63d0\u793a\u8c03\u4f18\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u8350\u548c\u5bf9\u8bdd\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u5bf9\u8bdd\u63a8\u8350\u7cfb\u7edf\u5728\u6355\u6349\u7528\u6237\u504f\u597d\u6df1\u5ea6\u8bed\u4e49\u548c\u5bf9\u8bdd\u4e0a\u4e0b\u6587\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u5c24\u5176\u662f\u5982\u4f55\u9ad8\u6548\u6574\u5408\u5916\u90e8\u77e5\u8bc6\u56fe\u8c31\u4fe1\u606f\u3002", "method": "STEP\u91c7\u7528\u4e09\u9636\u6bb5\u8bfe\u7a0b\u9010\u6b65\u5bf9\u9f50\u5bf9\u8bdd\u4e0a\u4e0b\u6587\u4e0e\u77e5\u8bc6\u56fe\u8c31\u5b9e\u4f53\uff0c\u5e76\u901a\u8fc7\u53cc\u63d0\u793a\u65b9\u6848\uff08\u5bf9\u8bdd\u524d\u7f00\u548c\u63a8\u8350\u524d\u7f00\uff09\u6ce8\u5165\u51bb\u7ed3\u7684\u8bed\u8a00\u6a21\u578b\u3002", "result": "STEP\u5728\u4e24\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u7684\u63a8\u8350\u7cbe\u5ea6\u548c\u5bf9\u8bdd\u8d28\u91cf\u4f18\u4e8e\u4e3b\u6d41\u65b9\u6cd5\u3002", "conclusion": "STEP\u901a\u8fc7\u4e0a\u4e0b\u6587-\u77e5\u8bc6\u878d\u5408\u548c\u53cc\u63d0\u793a\u65b9\u6848\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8bed\u4e49\u5bf9\u9f50\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u63a8\u8350\u548c\u5bf9\u8bdd\u6027\u80fd\u3002"}}
{"id": "2508.10703", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10703", "abs": "https://arxiv.org/abs/2508.10703", "authors": ["Yiping Song", "Jiaoyan Chen", "Renate A. Schmidt"], "title": "GenOM: Ontology Matching with Description Generation and Large Language Model", "comment": null, "summary": "Ontology matching (OM) plays an essential role in enabling semantic\ninteroperability and integration across heterogeneous knowledge sources,\nparticularly in the biomedical domain which contains numerous complex concepts\nrelated to diseases and pharmaceuticals. This paper introduces GenOM, a large\nlanguage model (LLM)-based ontology alignment framework, which enriches the\nsemantic representations of ontology concepts via generating textual\ndefinitions, retrieves alignment candidates with an embedding model, and\nincorporates exact matching-based tools to improve precision. Extensive\nexperiments conducted on the OAEI Bio-ML track demonstrate that GenOM can often\nachieve competitive performance, surpassing many baselines including\ntraditional OM systems and recent LLM-based methods. Further ablation studies\nconfirm the effectiveness of semantic enrichment and few-shot prompting,\nhighlighting the framework's robustness and adaptability.", "AI": {"tldr": "GenOM\u662f\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u6587\u672c\u5b9a\u4e49\u3001\u5d4c\u5165\u6a21\u578b\u68c0\u7d22\u548c\u5bf9\u9f50\u5de5\u5177\u63d0\u5347\u672c\u4f53\u5339\u914d\u6027\u80fd\uff0c\u5728\u751f\u7269\u533b\u5b66\u9886\u57df\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u751f\u7269\u533b\u5b66\u9886\u57df\u5f02\u6784\u77e5\u8bc6\u6e90\u95f4\u7684\u8bed\u4e49\u4e92\u64cd\u4f5c\u6027\u95ee\u9898\uff0c\u63d0\u5347\u672c\u4f53\u5339\u914d\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "method": "\u7ed3\u5408\u751f\u6210\u6587\u672c\u5b9a\u4e49\u3001\u5d4c\u5165\u6a21\u578b\u68c0\u7d22\u548c\u7cbe\u786e\u5339\u914d\u5de5\u5177\uff0c\u8fdb\u884c\u672c\u4f53\u5bf9\u9f50\u3002", "result": "\u5728OAEI Bio-ML\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u4f20\u7edf\u65b9\u6cd5\u548c\u8fd1\u671fLLM\u65b9\u6cd5\u3002", "conclusion": "GenOM\u6846\u67b6\u901a\u8fc7\u8bed\u4e49\u589e\u5f3a\u548c\u5c11\u6837\u672c\u63d0\u793a\uff0c\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.10745", "categories": ["cs.AI", "cs.CV", "cs.LG", "cs.MA", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.10745", "abs": "https://arxiv.org/abs/2508.10745", "authors": ["Sayan Nag", "K J Joseph", "Koustava Goswami", "Vlad I Morariu", "Balaji Vasan Srinivasan"], "title": "Agentic Design Review System", "comment": null, "summary": "Evaluating graphic designs involves assessing it from multiple facets like\nalignment, composition, aesthetics and color choices. Evaluating designs in a\nholistic way involves aggregating feedback from individual expert reviewers.\nTowards this, we propose an Agentic Design Review System (AgenticDRS), where\nmultiple agents collaboratively analyze a design, orchestrated by a meta-agent.\nA novel in-context exemplar selection approach based on graph matching and a\nunique prompt expansion method plays central role towards making each agent\ndesign aware. Towards evaluating this framework, we propose DRS-BENCH\nbenchmark. Thorough experimental evaluation against state-of-the-art baselines\nadapted to the problem setup, backed-up with critical ablation experiments\nbrings out the efficacy of Agentic-DRS in evaluating graphic designs and\ngenerating actionable feedback. We hope that this work will attract attention\nto this pragmatic, yet under-explored research direction.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u4ee3\u7406\u534f\u4f5c\u7684\u8bbe\u8ba1\u8bc4\u5ba1\u7cfb\u7edf\uff08AgenticDRS\uff09\uff0c\u901a\u8fc7\u56fe\u5339\u914d\u548c\u63d0\u793a\u6269\u5c55\u65b9\u6cd5\u63d0\u5347\u4ee3\u7406\u7684\u8bbe\u8ba1\u610f\u8bc6\uff0c\u5e76\u5728DRS-BENCH\u57fa\u51c6\u4e0a\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u56fe\u5f62\u8bbe\u8ba1\u8bc4\u4f30\u9700\u8981\u591a\u7ef4\u5ea6\u7efc\u5408\u53cd\u9988\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u7cfb\u7edf\u6027\u3002", "method": "\u91c7\u7528\u591a\u4ee3\u7406\u534f\u4f5c\u6846\u67b6\uff0c\u7ed3\u5408\u56fe\u5339\u914d\u548c\u63d0\u793a\u6269\u5c55\u6280\u672f\uff0c\u7531\u5143\u4ee3\u7406\u534f\u8c03\u8bc4\u5ba1\u3002", "result": "\u5b9e\u9a8c\u8868\u660eAgenticDRS\u5728\u8bc4\u4f30\u8bbe\u8ba1\u548c\u751f\u6210\u53cd\u9988\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u8bbe\u8ba1\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u547c\u5401\u66f4\u591a\u5173\u6ce8\u8fd9\u4e00\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2508.10769", "categories": ["cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.10769", "abs": "https://arxiv.org/abs/2508.10769", "authors": ["Zhiqi Shen", "Shaojing Fan", "Danni Xu", "Terence Sim", "Mohan Kankanhalli"], "title": "Modeling Human Responses to Multimodal AI Content", "comment": null, "summary": "As AI-generated content becomes widespread, so does the risk of\nmisinformation. While prior research has primarily focused on identifying\nwhether content is authentic, much less is known about how such content\ninfluences human perception and behavior. In domains like trading or the stock\nmarket, predicting how people react (e.g., whether a news post will go viral),\ncan be more critical than verifying its factual accuracy. To address this, we\ntake a human-centered approach and introduce the MhAIM Dataset, which contains\n154,552 online posts (111,153 of them AI-generated), enabling large-scale\nanalysis of how people respond to AI-generated content. Our human study reveals\nthat people are better at identifying AI content when posts include both text\nand visuals, particularly when inconsistencies exist between the two. We\npropose three new metrics: trustworthiness, impact, and openness, to quantify\nhow users judge and engage with online content. We present T-Lens, an LLM-based\nagent system designed to answer user queries by incorporating predicted human\nresponses to multimodal information. At its core is HR-MCP (Human Response\nModel Context Protocol), built on the standardized Model Context Protocol\n(MCP), enabling seamless integration with any LLM. This integration allows\nT-Lens to better align with human reactions, enhancing both interpretability\nand interaction capabilities. Our work provides empirical insights and\npractical tools to equip LLMs with human-awareness capabilities. By\nhighlighting the complex interplay among AI, human cognition, and information\nreception, our findings suggest actionable strategies for mitigating the risks\nof AI-driven misinformation.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86AI\u751f\u6210\u5185\u5bb9\u5bf9\u4eba\u7c7b\u611f\u77e5\u548c\u884c\u4e3a\u7684\u5f71\u54cd\uff0c\u63d0\u51faMhAIM\u6570\u636e\u96c6\u548cT-Lens\u7cfb\u7edf\uff0c\u91cf\u5316\u7528\u6237\u5bf9\u5185\u5bb9\u7684\u4fe1\u4efb\u5ea6\u3001\u5f71\u54cd\u529b\u548c\u5f00\u653e\u6027\uff0c\u5e76\u8bbe\u8ba1HR-MCP\u534f\u8bae\u4ee5\u589e\u5f3aLLM\u7684\u4eba\u7c7b\u611f\u77e5\u80fd\u529b\u3002", "motivation": "\u968f\u7740AI\u751f\u6210\u5185\u5bb9\u7684\u666e\u53ca\uff0c\u5176\u6f5c\u5728\u98ce\u9669\uff08\u5982\u865a\u5047\u4fe1\u606f\uff09\u65e5\u76ca\u51f8\u663e\u3002\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u5185\u5bb9\u771f\u5b9e\u6027\uff0c\u800c\u5bf9\u5176\u5982\u4f55\u5f71\u54cd\u4eba\u7c7b\u884c\u4e3a\u7684\u7814\u7a76\u8f83\u5c11\u3002\u5c24\u5176\u5728\u91d1\u878d\u7b49\u9886\u57df\uff0c\u9884\u6d4b\u4eba\u7c7b\u53cd\u5e94\u6bd4\u9a8c\u8bc1\u5185\u5bb9\u771f\u5b9e\u6027\u66f4\u4e3a\u5173\u952e\u3002", "method": "\u5f15\u5165MhAIM\u6570\u636e\u96c6\uff08\u542b154,552\u6761\u5728\u7ebf\u5e16\u5b50\uff0c\u5176\u4e2d111,153\u6761\u4e3aAI\u751f\u6210\uff09\uff0c\u901a\u8fc7\u4eba\u7c7b\u7814\u7a76\u5206\u6790\u591a\u6a21\u6001\u5185\u5bb9\u5bf9\u8bc6\u522bAI\u751f\u6210\u5185\u5bb9\u7684\u5f71\u54cd\u3002\u63d0\u51fa\u4fe1\u4efb\u5ea6\u3001\u5f71\u54cd\u529b\u548c\u5f00\u653e\u6027\u4e09\u9879\u65b0\u6307\u6807\uff0c\u5e76\u5f00\u53d1\u57fa\u4e8eLLM\u7684T-Lens\u7cfb\u7edf\uff0c\u96c6\u6210HR-MCP\u534f\u8bae\u4ee5\u9884\u6d4b\u4eba\u7c7b\u53cd\u5e94\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5f53\u5e16\u5b50\u5305\u542b\u56fe\u6587\u4e14\u5185\u5bb9\u4e0d\u4e00\u81f4\u65f6\uff0c\u4eba\u7c7b\u66f4\u5bb9\u6613\u8bc6\u522bAI\u751f\u6210\u5185\u5bb9\u3002T-Lens\u7cfb\u7edf\u901a\u8fc7HR-MCP\u534f\u8bae\u663e\u8457\u63d0\u5347\u4e86LLM\u5bf9\u4eba\u7c7b\u53cd\u5e94\u7684\u9884\u6d4b\u80fd\u529b\u3002", "conclusion": "\u7814\u7a76\u4e3aLLM\u63d0\u4f9b\u4e86\u4eba\u7c7b\u611f\u77e5\u80fd\u529b\u7684\u5b9e\u8bc1\u5de5\u5177\uff0c\u63ed\u793a\u4e86AI\u3001\u4eba\u7c7b\u8ba4\u77e5\u4e0e\u4fe1\u606f\u63a5\u6536\u7684\u590d\u6742\u5173\u7cfb\uff0c\u4e3a\u51cf\u5c11AI\u865a\u5047\u4fe1\u606f\u98ce\u9669\u63d0\u4f9b\u4e86\u53ef\u884c\u7b56\u7565\u3002"}}
{"id": "2508.10777", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10777", "abs": "https://arxiv.org/abs/2508.10777", "authors": ["Ma\u00ebl Jullien", "Marco Valentino", "Andr\u00e9 Freitas"], "title": "The Knowledge-Reasoning Dissociation: Fundamental Limitations of LLMs in Clinical Natural Language Inference", "comment": "19 pages", "summary": "Large language models are often assumed to acquire increasingly structured,\ngeneralizable internal representations simply by scaling data and parameters.\nWe interrogate this assumption by introducing a Clinical Trial Natural Language\nInference benchmark comprising four reasoning families, Causal Attribution,\nCompositional Grounding, Epistemic Verification, and Risk State Abstraction.\nEach item is paired with a targeted Ground Knowledge and Meta-Level Reasoning\nVerification (GKMRV) probe, allowing us to dissociate failures of factual\naccess from failures of inference. We evaluate six contemporary LLMs under both\ndirect and chain of thought prompting.\n  Models achieve near-ceiling GKMRV accuracy (mean accuracy 0.918) yet perform\npoorly on the main reasoning tasks (mean accuracy 0.25). Despite low accuracy,\noutput inferences are highly consistent across samples (mean 0.87), indicating\na systematic application of underlying heuristics and shortcuts.\n  These results reveal fundamental structural and representational limitations:\ncurrent LLMs often possess the relevant clinical knowledge but lack the\nstructured, composable internal representations needed to deploy it reliably\n(e.g., integrating constraints, weighing evidence, or simulating\ncounterfactuals). Decoupling knowledge from reasoning with GKMRV makes this\ndissociation explicit and measurable, providing an effective framework for\nprobing the reliability of LLMs in high-stakes domains.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e34\u5e8a\u77e5\u8bc6\u8bbf\u95ee\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u63ed\u793a\u4e86\u5176\u5185\u90e8\u8868\u5f81\u7684\u7ed3\u6784\u6027\u5c40\u9650\u3002", "motivation": "\u63a2\u8ba8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u662f\u5426\u901a\u8fc7\u6570\u636e\u89c4\u6a21\u548c\u53c2\u6570\u6269\u5c55\u83b7\u5f97\u7ed3\u6784\u5316\u3001\u53ef\u6cdb\u5316\u7684\u5185\u90e8\u8868\u5f81\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u98ce\u9669\u7684\u4e34\u5e8a\u63a8\u7406\u9886\u57df\u3002", "method": "\u5f15\u5165\u4e34\u5e8a\u8bd5\u9a8c\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u57fa\u51c6\uff08CT-NLI\uff09\uff0c\u5305\u542b\u56db\u79cd\u63a8\u7406\u7c7b\u578b\uff0c\u5e76\u8bbe\u8ba1GKMRV\u63a2\u9488\u5206\u79bb\u77e5\u8bc6\u8bbf\u95ee\u4e0e\u63a8\u7406\u5931\u8d25\u3002\u8bc4\u4f30\u516d\u79cd\u5f53\u4ee3LLM\u5728\u76f4\u63a5\u548c\u601d\u7ef4\u94fe\u63d0\u793a\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u6a21\u578b\u5728GKMRV\u63a2\u9488\u4e0a\u8868\u73b0\u4f18\u5f02\uff08\u5e73\u5747\u51c6\u786e\u73870.918\uff09\uff0c\u4f46\u5728\u4e3b\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u5dee\uff08\u5e73\u5747\u51c6\u786e\u73870.25\uff09\uff0c\u8f93\u51fa\u63a8\u7406\u9ad8\u5ea6\u4e00\u81f4\uff08\u4e00\u81f4\u60270.87\uff09\uff0c\u8868\u660e\u5176\u4f9d\u8d56\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "conclusion": "\u5f53\u524dLLM\u7f3a\u4e4f\u7ed3\u6784\u5316\u3001\u53ef\u7ec4\u5408\u7684\u5185\u90e8\u8868\u5f81\uff0c\u9650\u5236\u4e86\u5176\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u53ef\u9760\u6027\uff0cGKMRV\u6846\u67b6\u4e3a\u8bc4\u4f30\u9ad8\u98ce\u9669\u9886\u57dfLLM\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2508.10806", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.10806", "abs": "https://arxiv.org/abs/2508.10806", "authors": ["Maria J. P. Peixoto", "Akriti Pandey", "Ahsan Zaman", "Peter R. Lewis"], "title": "Who Benefits from AI Explanations? Towards Accessible and Interpretable Systems", "comment": "Paper accepted for the IJCAI 2025 Workshop on Explainable Artificial\n  Intelligence (XAI): https://sites.google.com/view/xai2025/proceedings", "summary": "As AI systems are increasingly deployed to support decision-making in\ncritical domains, explainability has become a means to enhance the\nunderstandability of these outputs and enable users to make more informed and\nconscious choices. However, despite growing interest in the usability of\neXplainable AI (XAI), the accessibility of these methods, particularly for\nusers with vision impairments, remains underexplored. This paper investigates\naccessibility gaps in XAI through a two-pronged approach. First, a literature\nreview of 79 studies reveals that evaluations of XAI techniques rarely include\ndisabled users, with most explanations relying on inherently visual formats.\nSecond, we present a four-part methodological proof of concept that\noperationalizes inclusive XAI design: (1) categorization of AI systems, (2)\npersona definition and contextualization, (3) prototype design and\nimplementation, and (4) expert and user assessment of XAI techniques for\naccessibility. Preliminary findings suggest that simplified explanations are\nmore comprehensible for non-visual users than detailed ones, and that\nmultimodal presentation is required for more equitable interpretability.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u53ef\u89e3\u91caAI\uff08XAI\uff09\u5728\u89c6\u89c9\u969c\u788d\u7528\u6237\u4e2d\u7684\u53ef\u8bbf\u95ee\u6027\u4e0d\u8db3\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u56db\u6b65\u65b9\u6cd5\u4ee5\u4fc3\u8fdb\u5305\u5bb9\u6027\u8bbe\u8ba1\u3002", "motivation": "\u968f\u7740AI\u5728\u5173\u952e\u9886\u57df\u51b3\u7b56\u652f\u6301\u4e2d\u7684\u5e94\u7528\u589e\u52a0\uff0c\u53ef\u89e3\u91ca\u6027\u6210\u4e3a\u63d0\u5347\u7528\u6237\u7406\u89e3\u548c\u9009\u62e9\u7684\u5173\u952e\uff0c\u4f46\u89c6\u89c9\u969c\u788d\u7528\u6237\u7684\u53ef\u8bbf\u95ee\u6027\u7814\u7a76\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\uff0879\u9879\u7814\u7a76\uff09\u548c\u56db\u6b65\u65b9\u6cd5\uff08\u5206\u7c7b\u3001\u89d2\u8272\u5b9a\u4e49\u3001\u539f\u578b\u8bbe\u8ba1\u3001\u8bc4\u4f30\uff09\u7814\u7a76XAI\u7684\u53ef\u8bbf\u95ee\u6027\u3002", "result": "\u521d\u6b65\u7ed3\u679c\u663e\u793a\uff0c\u7b80\u5316\u89e3\u91ca\u6bd4\u8be6\u7ec6\u89e3\u91ca\u66f4\u6613\u7406\u89e3\uff0c\u591a\u6a21\u6001\u5448\u73b0\u6709\u52a9\u4e8e\u63d0\u5347\u516c\u5e73\u6027\u3002", "conclusion": "XAI\u8bbe\u8ba1\u9700\u66f4\u591a\u5173\u6ce8\u89c6\u89c9\u969c\u788d\u7528\u6237\uff0c\u7b80\u5316\u89e3\u91ca\u548c\u591a\u6a21\u6001\u5448\u73b0\u662f\u63d0\u5347\u53ef\u8bbf\u95ee\u6027\u7684\u5173\u952e\u3002"}}
