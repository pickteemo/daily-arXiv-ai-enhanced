{"id": "2507.14249", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14249", "abs": "https://arxiv.org/abs/2507.14249", "authors": ["Yuejiao Xie", "Maonan Wang", "Di Zhou", "Man-On Pun", "Zhu Han"], "title": "Real-Time Communication-Aware Ride-Sharing Route Planning for Urban Air Mobility: A Multi-Source Hybrid Attention Reinforcement Learning Approach", "comment": null, "summary": "Urban Air Mobility (UAM) systems are rapidly emerging as promising solutions\nto alleviate urban congestion, with path planning becoming a key focus area.\nUnlike ground transportation, UAM trajectory planning has to prioritize\ncommunication quality for accurate location tracking in constantly changing\nenvironments to ensure safety. Meanwhile, a UAM system, serving as an air taxi,\nrequires adaptive planning to respond to real-time passenger requests,\nespecially in ride-sharing scenarios where passenger demands are unpredictable\nand dynamic. However, conventional trajectory planning strategies based on\npredefined routes lack the flexibility to meet varied passenger ride demands.\nTo address these challenges, this work first proposes constructing a radio map\nto evaluate the communication quality of urban airspace. Building on this, we\nintroduce a novel Multi-Source Hybrid Attention Reinforcement Learning\n(MSHA-RL) framework for the challenge of effectively focusing on passengers and\nUAM locations, which arises from the significant dimensional disparity between\nthe representations. This model first generates the alignment among diverse\ndata sources with large gap dimensions before employing hybrid attention to\nbalance global and local insights, thereby facilitating responsive, real-time\npath planning. Extensive experimental results demonstrate that the approach\nenables communication-compliant trajectory planning, reducing travel time and\nenhancing operational efficiency while prioritizing passenger safety.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65e0\u7ebf\u7535\u5730\u56fe\u548cMSHA-RL\u6846\u67b6\u7684UAM\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u52a8\u6001\u4e58\u5ba2\u9700\u6c42\u548c\u901a\u4fe1\u8d28\u91cf\u4fdd\u969c\u95ee\u9898\u3002", "motivation": "UAM\u7cfb\u7edf\u9700\u8981\u5e94\u5bf9\u52a8\u6001\u4e58\u5ba2\u9700\u6c42\u548c\u786e\u4fdd\u901a\u4fe1\u8d28\u91cf\uff0c\u800c\u4f20\u7edf\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\u7f3a\u4e4f\u7075\u6d3b\u6027\u3002", "method": "\u6784\u5efa\u65e0\u7ebf\u7535\u5730\u56fe\u8bc4\u4f30\u901a\u4fe1\u8d28\u91cf\uff0c\u5e76\u63d0\u51faMSHA-RL\u6846\u67b6\uff0c\u901a\u8fc7\u6df7\u5408\u6ce8\u610f\u529b\u673a\u5236\u5e73\u8861\u5168\u5c40\u4e0e\u5c40\u90e8\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u5b9e\u73b0\u901a\u4fe1\u5408\u89c4\u7684\u8def\u5f84\u89c4\u5212\uff0c\u51cf\u5c11\u65c5\u884c\u65f6\u95f4\u5e76\u63d0\u5347\u6548\u7387\u3002", "conclusion": "MSHA-RL\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86UAM\u7cfb\u7edf\u4e2d\u7684\u52a8\u6001\u8def\u5f84\u89c4\u5212\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u969c\u4e86\u4e58\u5ba2\u5b89\u5168\u3002"}}
{"id": "2507.14274", "categories": ["cs.RO", "cs.NA", "math.DG", "math.DS", "math.GR", "math.NA"], "pdf": "https://arxiv.org/pdf/2507.14274", "abs": "https://arxiv.org/abs/2507.14274", "authors": ["Andreas Mueller", "Shivesh Kumar", "Thomas Kordik"], "title": "A Recursive Lie-Group Formulation for the Second-Order Time Derivatives of the Inverse Dynamics of parallel Kinematic Manipulators", "comment": null, "summary": "Series elastic actuators (SEA) were introduced for serial robotic arms. Their\nmodel-based trajectory tracking control requires the second time derivatives of\nthe inverse dynamics solution, for which algorithms were proposed. Trajectory\ncontrol of parallel kinematics manipulators (PKM) equipped with SEAs has not\nyet been pursued. Key element for this is the computationally efficient\nevaluation of the second time derivative of the inverse dynamics solution. This\nhas not been presented in the literature, and is addressed in the present paper\nfor the first time. The special topology of PKM is exploited reusing the\nrecursive algorithms for evaluating the inverse dynamics of serial robots. A\nLie group formulation is used and all relations are derived within this\nframework. Numerical results are presented for a 6-DOF Gough-Stewart platform\n(as part of an exoskeleton), and for a planar PKM when a flatness-based control\nscheme is applied.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u63d0\u51fa\u4e86\u4e00\u79cd\u8ba1\u7b97\u9ad8\u6548\u7684\u5e76\u884c\u8fd0\u52a8\u5b66\u673a\u68b0\u81c2\uff08PKM\uff09\u914d\u5907\u4e32\u8054\u5f39\u6027\u6267\u884c\u5668\uff08SEA\uff09\u7684\u8f68\u8ff9\u63a7\u5236\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u5176\u9006\u52a8\u529b\u5b66\u89e3\u7684\u4e8c\u9636\u65f6\u95f4\u5bfc\u6570\u8ba1\u7b97\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u672a\u89e3\u51b3PKM\u914d\u5907SEA\u7684\u8f68\u8ff9\u63a7\u5236\u95ee\u9898\uff0c\u5c24\u5176\u662f\u9006\u52a8\u529b\u5b66\u89e3\u7684\u4e8c\u9636\u65f6\u95f4\u5bfc\u6570\u7684\u9ad8\u6548\u8ba1\u7b97\u3002\u672c\u6587\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5229\u7528PKM\u7684\u7279\u6b8a\u62d3\u6251\u7ed3\u6784\uff0c\u590d\u7528\u4e32\u8054\u673a\u5668\u4eba\u9006\u52a8\u529b\u5b66\u7684\u9012\u5f52\u7b97\u6cd5\uff0c\u5e76\u91c7\u7528\u674e\u7fa4\u6846\u67b6\u63a8\u5bfc\u6240\u6709\u5173\u7cfb\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e6\u81ea\u7531\u5ea6Gough-Stewart\u5e73\u53f0\u548c\u5e73\u9762PKM\uff0c\u5e76\u7ed3\u5408\u5e73\u5766\u6027\u63a7\u5236\u65b9\u6848\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u9996\u6b21\u89e3\u51b3\u4e86PKM\u914d\u5907SEA\u7684\u8f68\u8ff9\u63a7\u5236\u95ee\u9898\uff0c\u4e3a\u76f8\u5173\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u8ba1\u7b97\u65b9\u6cd5\u3002"}}
{"id": "2507.14412", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14412", "abs": "https://arxiv.org/abs/2507.14412", "authors": ["Mengxue Fu", "Zhonghao Shi", "Minyu Huang", "Siqi Liu", "Mina Kian", "Yirui Song", "Maja J. Matari\u0107"], "title": "Personalized Socially Assistive Robots With End-to-End Speech-Language Models For Well-Being Support", "comment": null, "summary": "Socially assistive robots (SARs) have shown great potential for supplementing\nwell-being support. However, prior studies have found that existing dialogue\npipelines for SARs remain limited in real-time latency, back-channeling, and\npersonalized speech dialogue. Toward addressing these limitations, we propose\nusing integrated end-to-end speech-language models (SLMs) with SARs. This work\n1) evaluated the usability of an SLM-enabled SAR dialogue system through a\nsmall user study, and 2) identified remaining limitations through study user\nfeedback to inform future improvements. We conducted a small within-participant\nuser study with university students (N = 11) whose results showed that\nparticipants perceived an SLM-enabled SAR system as capable of providing\nempathetic feedback, natural turn-taking, back-channeling, and adaptive\nresponses. We also found that participants reported the robot's nonverbal\nbehaviors as lacking variability and synchronization with conversation, and the\nSLM's verbal feedback as generic and repetitive. These findings highlighted the\nneed for real-time robot movement synchronized with conversation, improved\nprompting or fine-tuning to generate outputs better aligned with mental health\npractices, and more expressive, adaptive vocal generation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528\u7aef\u5230\u7aef\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\u6539\u8fdb\u793e\u4ea4\u8f85\u52a9\u673a\u5668\u4eba\uff08SAR\uff09\u7684\u5bf9\u8bdd\u7cfb\u7edf\uff0c\u5e76\u901a\u8fc7\u7528\u6237\u7814\u7a76\u9a8c\u8bc1\u5176\u6548\u679c\uff0c\u540c\u65f6\u6307\u51fa\u4ecd\u9700\u6539\u8fdb\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709SAR\u5bf9\u8bdd\u7cfb\u7edf\u5728\u5b9e\u65f6\u5ef6\u8fdf\u3001\u53cd\u9988\u673a\u5236\u548c\u4e2a\u6027\u5316\u5bf9\u8bdd\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u901a\u8fc7SLM\u6280\u672f\u63d0\u5347\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u5c0f\u578b\u7528\u6237\u7814\u7a76\uff08N=11\uff09\u8bc4\u4f30SLM-enabled SAR\u7cfb\u7edf\u7684\u53ef\u7528\u6027\uff0c\u5e76\u5206\u6790\u7528\u6237\u53cd\u9988\u4ee5\u8bc6\u522b\u6539\u8fdb\u65b9\u5411\u3002", "result": "\u7528\u6237\u8ba4\u4e3aSLM-enabled SAR\u7cfb\u7edf\u80fd\u63d0\u4f9b\u5171\u60c5\u53cd\u9988\u3001\u81ea\u7136\u5bf9\u8bdd\u548c\u9002\u5e94\u6027\u56de\u5e94\uff0c\u4f46\u673a\u5668\u4eba\u7684\u975e\u8bed\u8a00\u884c\u4e3a\u548cSLM\u7684\u53cd\u9988\u4ecd\u9700\u4f18\u5316\u3002", "conclusion": "\u672a\u6765\u9700\u6539\u8fdb\u673a\u5668\u4eba\u52a8\u4f5c\u4e0e\u5bf9\u8bdd\u7684\u540c\u6b65\u6027\u3001SLM\u8f93\u51fa\u7684\u5fc3\u7406\u5065\u5eb7\u9002\u5e94\u6027\uff0c\u4ee5\u53ca\u66f4\u5bcc\u8868\u73b0\u529b\u7684\u8bed\u97f3\u751f\u6210\u3002"}}
{"id": "2507.14455", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14455", "abs": "https://arxiv.org/abs/2507.14455", "authors": ["Chun-Ming Yang", "Pranav A. Bhounsule"], "title": "Koopman Operator Based Time-Delay Embeddings and State History Augmented LQR for Periodic Hybrid Systems: Bouncing Pendulum and Bipedal Walking", "comment": null, "summary": "Time-delay embedding is a technique that uses snapshots of state history over\ntime to build a linear state space model of a nonlinear smooth system. We\ndemonstrate that periodic non-smooth or hybrid system can also be modeled as a\nlinear state space system using this approach as long as its behavior is\nconsistent in modes and timings. We extended time-delay embeddings to generate\na linear model of two periodic hybrid systems: the bouncing pendulum and the\nsimplest walker with control inputs. This leads to a novel state history\naugmented linear quadratic regulator (LQR) which uses current and past state\nhistory for feedback control.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6269\u5c55\u7684\u65f6\u95f4\u5ef6\u8fdf\u5d4c\u5165\u6280\u672f\uff0c\u7528\u4e8e\u5efa\u6a21\u5468\u671f\u6027\u975e\u5149\u6ed1\u6216\u6df7\u5408\u7cfb\u7edf\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u72b6\u6001\u5386\u53f2\u589e\u5f3a\u7ebf\u6027\u4e8c\u6b21\u8c03\u8282\u5668\uff08LQR\uff09\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u5c06\u65f6\u95f4\u5ef6\u8fdf\u5d4c\u5165\u6280\u672f\u5e94\u7528\u4e8e\u5468\u671f\u6027\u975e\u5149\u6ed1\u6216\u6df7\u5408\u7cfb\u7edf\uff0c\u4ee5\u6784\u5efa\u7ebf\u6027\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u3002", "method": "\u6269\u5c55\u65f6\u95f4\u5ef6\u8fdf\u5d4c\u5165\u6280\u672f\uff0c\u5e94\u7528\u4e8e\u4e24\u4e2a\u5468\u671f\u6027\u6df7\u5408\u7cfb\u7edf\uff08\u5f39\u8df3\u6446\u548c\u7b80\u5355\u6b65\u884c\u5668\uff09\uff0c\u5e76\u5f00\u53d1\u72b6\u6001\u5386\u53f2\u589e\u5f3aLQR\u3002", "result": "\u6210\u529f\u6784\u5efa\u4e86\u7ebf\u6027\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff0c\u5e76\u9a8c\u8bc1\u4e86\u72b6\u6001\u5386\u53f2\u589e\u5f3aLQR\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u65f6\u95f4\u5ef6\u8fdf\u5d4c\u5165\u6280\u672f\u53ef\u6709\u6548\u5efa\u6a21\u5468\u671f\u6027\u6df7\u5408\u7cfb\u7edf\uff0c\u72b6\u6001\u5386\u53f2\u589e\u5f3aLQR\u4e3a\u63a7\u5236\u8fd9\u7c7b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2507.14154", "categories": ["cs.AI", "cs.LG", "68T05, 81P68", "I.2.6; I.2.0; F.1.2"], "pdf": "https://arxiv.org/pdf/2507.14154", "abs": "https://arxiv.org/abs/2507.14154", "authors": ["Rahul Kabali"], "title": "The Free Will Equation: Quantum Field Analogies for AGI", "comment": "22 pages, 5 figures. Submitted as an arXiv preprint. All code and\n  experiment details included in appendix", "summary": "Artificial General Intelligence (AGI) research traditionally focuses on\nalgorithms that optimize for specific goals under deterministic rules. Yet,\nhuman-like intelligence exhibits adaptive spontaneity - an ability to make\nunexpected choices or free decisions not strictly dictated by past data or\nimmediate reward. This trait, often dubbed \"free will\" in a loose sense, might\nbe crucial for creativity, robust adaptation, and avoiding ruts in\nproblem-solving. This paper proposes a theoretical framework, called the Free\nWill Equation, that draws analogies from quantum field theory to endow AGI\nagents with a form of adaptive, controlled stochasticity in their\ndecision-making process. The core idea is to treat an AI agent's cognitive\nstate as a superposition of potential actions or thoughts, which collapses\nprobabilistically into a concrete action when a decision is made - much like a\nquantum wavefunction collapsing upon measurement. By incorporating mechanisms\nanalogous to quantum fields, along with intrinsic motivation terms, we aim to\nimprove an agent's ability to explore novel strategies and adapt to unforeseen\nchanges. Experiments in a non-stationary multi-armed bandit environment\ndemonstrate that agents using this framework achieve higher rewards and policy\ndiversity compared to baseline methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u81ea\u7531\u610f\u5fd7\u65b9\u7a0b\u201d\u7684\u7406\u8bba\u6846\u67b6\uff0c\u901a\u8fc7\u501f\u9274\u91cf\u5b50\u573a\u8bba\uff0c\u8d4b\u4e88AGI\u4ee3\u7406\u4e00\u79cd\u81ea\u9002\u5e94\u3001\u53d7\u63a7\u7684\u968f\u673a\u6027\u51b3\u7b56\u80fd\u529b\u3002", "motivation": "\u4f20\u7edfAGI\u7814\u7a76\u4e13\u6ce8\u4e8e\u786e\u5b9a\u6027\u89c4\u5219\u4e0b\u7684\u76ee\u6807\u4f18\u5316\uff0c\u4f46\u4eba\u7c7b\u667a\u80fd\u5177\u6709\u9002\u5e94\u6027\u81ea\u53d1\u6027\uff08\u7c7b\u4f3c\u201c\u81ea\u7531\u610f\u5fd7\u201d\uff09\uff0c\u8fd9\u5bf9\u521b\u9020\u529b\u3001\u9002\u5e94\u6027\u548c\u95ee\u9898\u89e3\u51b3\u591a\u6837\u6027\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5c06AI\u4ee3\u7406\u7684\u8ba4\u77e5\u72b6\u6001\u89c6\u4e3a\u6f5c\u5728\u884c\u52a8\u6216\u601d\u60f3\u7684\u53e0\u52a0\u6001\uff0c\u901a\u8fc7\u7c7b\u4f3c\u91cf\u5b50\u6ce2\u51fd\u6570\u574d\u7f29\u7684\u673a\u5236\u5b9e\u73b0\u51b3\u7b56\uff0c\u5e76\u7ed3\u5408\u91cf\u5b50\u573a\u8bba\u548c\u5185\u5728\u52a8\u673a\u3002", "result": "\u5728\u975e\u7a33\u6001\u591a\u81c2\u8001\u864e\u673a\u73af\u5883\u4e2d\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528\u8be5\u6846\u67b6\u7684\u4ee3\u7406\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u83b7\u5f97\u66f4\u9ad8\u7684\u5956\u52b1\u548c\u7b56\u7565\u591a\u6837\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aAGI\u7684\u51b3\u7b56\u8fc7\u7a0b\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u968f\u673a\u6027\u673a\u5236\uff0c\u53ef\u80fd\u63d0\u5347\u5176\u9002\u5e94\u6027\u548c\u521b\u9020\u529b\u3002"}}
{"id": "2507.14538", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14538", "abs": "https://arxiv.org/abs/2507.14538", "authors": ["Jin Chai", "Xiang Yao", "Mengfan Hou", "Yanghong Li", "Erbao Dong"], "title": "A 21-DOF Humanoid Dexterous Hand with Hybrid SMA-Motor Actuation: CYJ Hand-0", "comment": null, "summary": "CYJ Hand-0 is a 21-DOF humanoid dexterous hand featuring a hybrid\ntendon-driven actuation system that combines shape memory alloys (SMAs) and DC\nmotors. The hand employs high-strength fishing line as artificial tendons and\nuses a fully 3D-printed AlSi10Mg metal frame designed to replicate the skeletal\nand tendon-muscle structure of the human hand. A linear motor-driven module\ncontrols finger flexion, while an SMA-based module enables finger extension and\nlateral abduction. These modules are integrated into a compact hybrid actuation\nunit mounted on a custom rear support structure. Mechanical and kinematic\nexperiments, conducted under an Arduino Mega 2560-based control system,\nvalidate the effectiveness of the design and demonstrate its biomimetic\ndexterity.", "AI": {"tldr": "CYJ Hand-0\u662f\u4e00\u6b3e21\u81ea\u7531\u5ea6\u7684\u4eff\u4eba\u7075\u5de7\u624b\uff0c\u91c7\u7528\u6df7\u5408\u808c\u8171\u9a71\u52a8\u7cfb\u7edf\uff08SMAs\u548cDC\u7535\u673a\uff09\uff0c\u901a\u8fc73D\u6253\u5370\u91d1\u5c5e\u6846\u67b6\u548c\u9ad8\u5f3a\u5ea6\u9c7c\u7ebf\u6a21\u62df\u4eba\u624b\u7ed3\u6784\uff0c\u9a8c\u8bc1\u4e86\u5176\u4eff\u751f\u7075\u6d3b\u6027\u3002", "motivation": "\u8bbe\u8ba1\u4e00\u79cd\u80fd\u591f\u6a21\u62df\u4eba\u624b\u9aa8\u9abc\u548c\u808c\u8171\u808c\u8089\u7ed3\u6784\u7684\u4eff\u751f\u7075\u5de7\u624b\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u7075\u6d3b\u6027\u548c\u529f\u80fd\u6027\u3002", "method": "\u7ed3\u5408\u5f62\u72b6\u8bb0\u5fc6\u5408\u91d1\uff08SMAs\uff09\u548c\u76f4\u6d41\u7535\u673a\u9a71\u52a8\uff0c\u4f7f\u75283D\u6253\u5370\u91d1\u5c5e\u6846\u67b6\u548c\u9ad8\u5f3a\u5ea6\u9c7c\u7ebf\u4f5c\u4e3a\u4eba\u5de5\u808c\u8171\uff0c\u901a\u8fc7\u7ebf\u6027\u7535\u673a\u548cSMA\u6a21\u5757\u5206\u522b\u63a7\u5236\u624b\u6307\u5c48\u66f2\u548c\u4f38\u5c55/\u5916\u5c55\u3002", "result": "\u673a\u68b0\u548c\u8fd0\u52a8\u5b66\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8bbe\u8ba1\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5176\u4eff\u751f\u7075\u6d3b\u6027\u3002", "conclusion": "CYJ Hand-0\u7684\u8bbe\u8ba1\u6210\u529f\u5b9e\u73b0\u4e86\u4eff\u751f\u7075\u5de7\u624b\u7684\u6027\u80fd\u76ee\u6807\uff0c\u4e3a\u672a\u6765\u673a\u5668\u4eba\u624b\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.14267", "categories": ["cs.AI", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2507.14267", "abs": "https://arxiv.org/abs/2507.14267", "authors": ["Ziqi Wang", "Hongshuo Huang", "Hancheng Zhao", "Changwen Xu", "Shang Zhu", "Jan Janssen", "Venkatasubramanian Viswanathan"], "title": "DREAMS: Density Functional Theory Based Research Engine for Agentic Materials Simulation", "comment": "34 pages, 28 pages of Supporting Information", "summary": "Materials discovery relies on high-throughput, high-fidelity simulation\ntechniques such as Density Functional Theory (DFT), which require years of\ntraining, extensive parameter fine-tuning and systematic error handling. To\naddress these challenges, we introduce the DFT-based Research Engine for\nAgentic Materials Screening (DREAMS), a hierarchical, multi-agent framework for\nDFT simulation that combines a central Large Language Model (LLM) planner agent\nwith domain-specific LLM agents for atomistic structure generation, systematic\nDFT convergence testing, High-Performance Computing (HPC) scheduling, and error\nhandling. In addition, a shared canvas helps the LLM agents to structure their\ndiscussions, preserve context and prevent hallucination. We validate DREAMS\ncapabilities on the Sol27LC lattice-constant benchmark, achieving average\nerrors below 1\\% compared to the results of human DFT experts. Furthermore, we\napply DREAMS to the long-standing CO/Pt(111) adsorption puzzle, demonstrating\nits long-term and complex problem-solving capabilities. The framework again\nreproduces expert-level literature adsorption-energy differences. Finally,\nDREAMS is employed to quantify functional-driven uncertainties with Bayesian\nensemble sampling, confirming the Face Centered Cubic (FCC)-site preference at\nthe Generalized Gradient Approximation (GGA) DFT level. In conclusion, DREAMS\napproaches L3-level automation - autonomous exploration of a defined design\nspace - and significantly reduces the reliance on human expertise and\nintervention, offering a scalable path toward democratized, high-throughput,\nhigh-fidelity computational materials discovery.", "AI": {"tldr": "DREAMS\u662f\u4e00\u4e2a\u57fa\u4e8eDFT\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7LLM\u4ee3\u7406\u5b9e\u73b0\u6750\u6599\u53d1\u73b0\u7684\u9ad8\u901a\u91cf\u3001\u9ad8\u4fdd\u771f\u6a21\u62df\uff0c\u51cf\u5c11\u5bf9\u4eba\u529b\u7684\u4f9d\u8d56\u3002", "motivation": "\u89e3\u51b3DFT\u6a21\u62df\u4e2d\u8bad\u7ec3\u65f6\u95f4\u957f\u3001\u53c2\u6570\u8c03\u6574\u590d\u6742\u548c\u7cfb\u7edf\u8bef\u5dee\u5904\u7406\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5206\u5c42\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7ed3\u5408LLM\u89c4\u5212\u4ee3\u7406\u548c\u9886\u57df\u7279\u5b9a\u4ee3\u7406\uff0c\u5171\u4eab\u753b\u5e03\u8f85\u52a9\u534f\u4f5c\u3002", "result": "\u5728Sol27LC\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8bef\u5dee\u4f4e\u4e8e1%\uff0c\u89e3\u51b3\u4e86CO/Pt(111)\u5438\u9644\u96be\u9898\uff0c\u5e76\u91cf\u5316\u4e86\u529f\u80fd\u9a71\u52a8\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "conclusion": "DREAMS\u5b9e\u73b0\u4e86L3\u7ea7\u81ea\u52a8\u5316\uff0c\u663e\u8457\u51cf\u5c11\u5bf9\u4eba\u529b\u7684\u4f9d\u8d56\uff0c\u63a8\u52a8\u9ad8\u901a\u91cf\u6750\u6599\u53d1\u73b0\u7684\u6c11\u4e3b\u5316\u3002"}}
{"id": "2507.14582", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14582", "abs": "https://arxiv.org/abs/2507.14582", "authors": ["Zezhi Liu", "Shizhen Wu", "Hanqian Luo", "Deyun Qin", "Yongchun Fang"], "title": "BT-TL-DMPs: A Novel Robot TAMP Framework Combining Behavior Tree, Temporal Logic and Dynamical Movement Primitives", "comment": "11 pages, 8 figures", "summary": "In the field of Learning from Demonstration (LfD), enabling robots to\ngeneralize learned manipulation skills to novel scenarios for long-horizon\ntasks remains challenging. Specifically, it is still difficult for robots to\nadapt the learned skills to new environments with different task and motion\nrequirements, especially in long-horizon, multi-stage scenarios with intricate\nconstraints. This paper proposes a novel hierarchical framework, called\nBT-TL-DMPs, that integrates Behavior Tree (BT), Temporal Logic (TL), and\nDynamical Movement Primitives (DMPs) to address this problem. Within this\nframework, Signal Temporal Logic (STL) is employed to formally specify complex,\nlong-horizon task requirements and constraints. These STL specifications are\nsystematically transformed to generate reactive and modular BTs for high-level\ndecision-making task structure. An STL-constrained DMP optimization method is\nproposed to optimize the DMP forcing term, allowing the learned motion\nprimitives to adapt flexibly while satisfying intricate spatiotemporal\nrequirements and, crucially, preserving the essential dynamics learned from\ndemonstrations. The framework is validated through simulations demonstrating\ngeneralization capabilities under various STL constraints and real-world\nexperiments on several long-horizon robotic manipulation tasks. The results\ndemonstrate that the proposed framework effectively bridges the symbolic-motion\ngap, enabling more reliable and generalizable autonomous manipulation for\ncomplex robotic tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBT-TL-DMPs\u7684\u5206\u5c42\u6846\u67b6\uff0c\u7ed3\u5408\u884c\u4e3a\u6811\u3001\u65f6\u5e8f\u903b\u8f91\u548c\u52a8\u6001\u8fd0\u52a8\u57fa\u5143\uff0c\u4ee5\u89e3\u51b3\u673a\u5668\u4eba\u5b66\u4e60\u6f14\u793a\u4e2d\u6280\u80fd\u6cdb\u5316\u7684\u95ee\u9898\u3002", "motivation": "\u5728\u673a\u5668\u4eba\u5b66\u4e60\u6f14\u793a\u9886\u57df\uff0c\u5982\u4f55\u5c06\u5b66\u5230\u7684\u6280\u80fd\u6cdb\u5316\u5230\u65b0\u573a\u666f\u4e2d\uff0c\u5c24\u5176\u662f\u957f\u65f6\u7a0b\u3001\u591a\u9636\u6bb5\u4efb\u52a1\uff0c\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u6846\u67b6\u6574\u5408\u4e86\u884c\u4e3a\u6811\uff08BT\uff09\u3001\u65f6\u5e8f\u903b\u8f91\uff08TL\uff09\u548c\u52a8\u6001\u8fd0\u52a8\u57fa\u5143\uff08DMPs\uff09\uff0c\u4f7f\u7528\u4fe1\u53f7\u65f6\u5e8f\u903b\u8f91\uff08STL\uff09\u89c4\u8303\u4efb\u52a1\u9700\u6c42\uff0c\u5e76\u901a\u8fc7STL\u7ea6\u675f\u7684DMP\u4f18\u5316\u65b9\u6cd5\u8c03\u6574\u8fd0\u52a8\u57fa\u5143\u3002", "result": "\u4eff\u771f\u548c\u5b9e\u9645\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u6ee1\u8db3\u590d\u6742\u65f6\u7a7a\u7ea6\u675f\u5e76\u5b9e\u73b0\u53ef\u9760\u7684\u81ea\u4e3b\u64cd\u4f5c\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u5f25\u5408\u4e86\u7b26\u53f7\u4e0e\u8fd0\u52a8\u4e4b\u95f4\u7684\u9e3f\u6c9f\uff0c\u4e3a\u590d\u6742\u673a\u5668\u4eba\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u81ea\u4e3b\u64cd\u4f5c\u80fd\u529b\u3002"}}
{"id": "2507.14293", "categories": ["cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14293", "abs": "https://arxiv.org/abs/2507.14293", "authors": ["Boyuan Zheng", "Zeyi Liao", "Scott Salisbury", "Zeyuan Liu", "Michael Lin", "Qinyuan Zheng", "Zifan Wang", "Xiang Deng", "Dawn Song", "Huan Sun", "Yu Su"], "title": "WebGuard: Building a Generalizable Guardrail for Web Agents", "comment": "We publicly release WebGuard, along with its annotation tools and\n  fine-tuned models, to facilitate open-source research on monitoring and\n  safeguarding web agents. All resources are available at\n  https://github.com/OSU-NLP-Group/WebGuard", "summary": "The rapid development of autonomous web agents powered by Large Language\nModels (LLMs), while greatly elevating efficiency, exposes the frontier risk of\ntaking unintended or harmful actions. This situation underscores an urgent need\nfor effective safety measures, akin to access controls for human users. To\naddress this critical challenge, we introduce WebGuard, the first comprehensive\ndataset designed to support the assessment of web agent action risks and\nfacilitate the development of guardrails for real-world online environments. In\ndoing so, WebGuard specifically focuses on predicting the outcome of\nstate-changing actions and contains 4,939 human-annotated actions from 193\nwebsites across 22 diverse domains, including often-overlooked long-tail\nwebsites. These actions are categorized using a novel three-tier risk schema:\nSAFE, LOW, and HIGH. The dataset includes designated training and test splits\nto support evaluation under diverse generalization settings. Our initial\nevaluations reveal a concerning deficiency: even frontier LLMs achieve less\nthan 60% accuracy in predicting action outcomes and less than 60% recall in\nlagging HIGH-risk actions, highlighting the risks of deploying\ncurrent-generation agents without dedicated safeguards. We therefore\ninvestigate fine-tuning specialized guardrail models using WebGuard. We conduct\ncomprehensive evaluations across multiple generalization settings and find that\na fine-tuned Qwen2.5VL-7B model yields a substantial improvement in\nperformance, boosting accuracy from 37% to 80% and HIGH-risk action recall from\n20% to 76%. Despite these improvements, the performance still falls short of\nthe reliability required for high-stakes deployment, where guardrails must\napproach near-perfect accuracy and recall.", "AI": {"tldr": "WebGuard\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u7f51\u7edc\u4ee3\u7406\u884c\u4e3a\u98ce\u9669\u7684\u6570\u636e\u96c6\uff0c\u65e8\u5728\u4e3a\u73b0\u5b9e\u4e16\u754c\u5728\u7ebf\u73af\u5883\u5f00\u53d1\u5b89\u5168\u9632\u62a4\u63aa\u65bd\u3002", "motivation": "\u968f\u7740\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u81ea\u4e3b\u7f51\u7edc\u4ee3\u7406\u5feb\u901f\u53d1\u5c55\uff0c\u5176\u53ef\u80fd\u91c7\u53d6\u6709\u5bb3\u884c\u4e3a\u7684\u98ce\u9669\u51f8\u663e\uff0c\u4e9f\u9700\u6709\u6548\u7684\u5b89\u5168\u63aa\u65bd\u3002", "method": "WebGuard\u5305\u542b4,939\u4e2a\u4eba\u5de5\u6807\u6ce8\u7684\u884c\u4e3a\uff0c\u8986\u76d6193\u4e2a\u7f51\u7ad9\u548c22\u4e2a\u9886\u57df\uff0c\u91c7\u7528\u4e09\u7ea7\u98ce\u9669\u5206\u7c7b\uff08SAFE\u3001LOW\u3001HIGH\uff09\uff0c\u5e76\u652f\u6301\u591a\u79cd\u6cdb\u5316\u573a\u666f\u8bc4\u4f30\u3002", "result": "\u524d\u6cbf\u5927\u8bed\u8a00\u6a21\u578b\u5728\u9884\u6d4b\u884c\u4e3a\u7ed3\u679c\u548c\u9ad8\u98ce\u9669\u884c\u4e3a\u53ec\u56de\u7387\u4e0a\u8868\u73b0\u4e0d\u4f73\uff08\u5747\u4f4e\u4e8e60%\uff09\uff0c\u4f46\u901a\u8fc7\u5fae\u8c03Qwen2.5VL-7B\u6a21\u578b\uff0c\u6027\u80fd\u663e\u8457\u63d0\u5347\uff08\u51c6\u786e\u7387\u4ece37%\u5347\u81f380%\uff0c\u9ad8\u98ce\u9669\u53ec\u56de\u7387\u4ece20%\u5347\u81f376%\uff09\u3002", "conclusion": "\u5c3d\u7ba1\u6027\u80fd\u6709\u6240\u63d0\u5347\uff0c\u4f46\u5f53\u524d\u9632\u62a4\u63aa\u65bd\u7684\u53ef\u9760\u6027\u4ecd\u4e0d\u8db3\u4ee5\u652f\u6301\u9ad8\u98ce\u9669\u90e8\u7f72\uff0c\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u4ee5\u5b9e\u73b0\u8fd1\u4e4e\u5b8c\u7f8e\u7684\u51c6\u786e\u7387\u548c\u53ec\u56de\u7387\u3002"}}
{"id": "2507.14605", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14605", "abs": "https://arxiv.org/abs/2507.14605", "authors": ["Chun-Ming Yang", "Pranav A. Bhounsule"], "title": "Koopman Operator Based Linear Model Predictive Control for 2D Quadruped Trotting, Bounding, and Gait Transition", "comment": null, "summary": "Online optimal control of quadrupedal robots would enable them to plan their\nmovement in novel scenarios. Linear Model Predictive Control (LMPC) has emerged\nas a practical approach for real-time control. In LMPC, an optimization problem\nwith a quadratic cost and linear constraints is formulated over a finite\nhorizon and solved on the fly. However, LMPC relies on linearizing the\nequations of motion (EOM), which may lead to poor solution quality. In this\npaper, we use Koopman operator theory and the Extended Dynamic Mode\nDecomposition (EDMD) to create a linear model of the system in high dimensional\nspace, thus retaining the nonlinearity of the EOM. We model the aerial phase\nand ground contact phases using different linear models. Then, using LMPC, we\ndemonstrate bounding, trotting, and bound-to-trot and trot-to-bound gait\ntransitions in level and rough terrains. The main novelty is the use of Koopman\noperator theory to create hybrid models of a quadrupedal system and demonstrate\nthe online generation of multiple gaits and gaits transitions.", "AI": {"tldr": "\u5229\u7528Koopman\u7b97\u5b50\u7406\u8bba\u548cEDMD\u65b9\u6cd5\uff0c\u6784\u5efa\u9ad8\u7ef4\u7ebf\u6027\u6a21\u578b\u4ee5\u4fdd\u7559\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u7279\u6027\uff0c\u7ed3\u5408LMPC\u5b9e\u73b0\u56db\u8db3\u673a\u5668\u4eba\u5728\u590d\u6742\u5730\u5f62\u4e2d\u7684\u591a\u79cd\u6b65\u6001\u53ca\u8f6c\u6362\u3002", "motivation": "\u5728\u7ebf\u4f18\u5316\u63a7\u5236\u56db\u8db3\u673a\u5668\u4eba\u8fd0\u52a8\uff0c\u89e3\u51b3LMPC\u56e0\u7ebf\u6027\u5316\u52a8\u529b\u5b66\u65b9\u7a0b\u5bfc\u81f4\u7684\u89e3\u8d28\u91cf\u4e0b\u964d\u95ee\u9898\u3002", "method": "\u91c7\u7528Koopman\u7b97\u5b50\u7406\u8bba\u548cEDMD\u65b9\u6cd5\u5efa\u7acb\u9ad8\u7ef4\u7ebf\u6027\u6a21\u578b\uff0c\u5206\u9636\u6bb5\u5efa\u6a21\u7a7a\u4e2d\u4e0e\u5730\u9762\u63a5\u89e6\u52a8\u529b\u5b66\uff0c\u5e76\u5e94\u7528LMPC\u5b9e\u73b0\u5b9e\u65f6\u63a7\u5236\u3002", "result": "\u6210\u529f\u6f14\u793a\u4e86\u5728\u5e73\u5766\u548c\u5d0e\u5c96\u5730\u5f62\u4e2d\u7684\u8df3\u8dc3\u3001\u5c0f\u8dd1\u53ca\u6b65\u6001\u8f6c\u6362\u3002", "conclusion": "Koopman\u7b97\u5b50\u7406\u8bba\u7ed3\u5408LMPC\u53ef\u6709\u6548\u5b9e\u73b0\u56db\u8db3\u673a\u5668\u4eba\u7684\u591a\u6b65\u6001\u5728\u7ebf\u751f\u6210\u4e0e\u8f6c\u6362\u3002"}}
{"id": "2507.14306", "categories": ["cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.14306", "abs": "https://arxiv.org/abs/2507.14306", "authors": ["Samarth P", "Vyoman Jain", "Shiva Golugula", "Motamarri Sai Sathvik"], "title": "Manimator: Transforming Research Papers into Visual Explanations", "comment": null, "summary": "Understanding complex scientific and mathematical concepts, particularly\nthose presented in dense research papers, poses a significant challenge for\nlearners. Dynamic visualizations can greatly enhance comprehension, but\ncreating them manually is time-consuming and requires specialized knowledge and\nskills. We introduce manimator, an open-source system that leverages Large\nLanguage Models to transform research papers and natural language prompts into\nexplanatory animations using the Manim engine. Manimator employs a pipeline\nwhere an LLM interprets the input text or research paper PDF to generate a\nstructured scene description outlining key concepts, mathematical formulas, and\nvisual elements and another LLM translates this description into executable\nManim Python code. We discuss its potential as an educational tool for rapidly\ncreating engaging visual explanations for complex STEM topics, democratizing\nthe creation of high-quality educational content.", "AI": {"tldr": "Manimator\u662f\u4e00\u4e2a\u5f00\u6e90\u7cfb\u7edf\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5c06\u7814\u7a76\u8bba\u6587\u548c\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u8f6c\u6362\u4e3a\u89e3\u91ca\u6027\u52a8\u753b\uff0c\u7b80\u5316\u590d\u6742STEM\u4e3b\u9898\u7684\u53ef\u89c6\u5316\u6559\u80b2\u5185\u5bb9\u521b\u4f5c\u3002", "motivation": "\u7406\u89e3\u590d\u6742\u79d1\u5b66\u548c\u6570\u5b66\u6982\u5ff5\u5bf9\u5b66\u4e60\u8005\u5177\u6709\u6311\u6218\u6027\uff0c\u800c\u52a8\u6001\u53ef\u89c6\u5316\u80fd\u663e\u8457\u63d0\u5347\u7406\u89e3\uff0c\u4f46\u624b\u52a8\u521b\u5efa\u8017\u65f6\u4e14\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\u3002", "method": "Manimator\u901a\u8fc7LLM\u89e3\u6790\u8f93\u5165\u6587\u672c\u6216PDF\u8bba\u6587\uff0c\u751f\u6210\u7ed3\u6784\u5316\u573a\u666f\u63cf\u8ff0\uff0c\u518d\u7531\u53e6\u4e00LLM\u5c06\u5176\u8f6c\u6362\u4e3a\u53ef\u6267\u884c\u7684Manim Python\u4ee3\u7801\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u5feb\u901f\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u6559\u80b2\u52a8\u753b\uff0c\u964d\u4f4e\u521b\u4f5c\u95e8\u69db\u3002", "conclusion": "Manimator\u6709\u671b\u6210\u4e3a\u6559\u80b2\u5de5\u5177\uff0c\u4fc3\u8fdb\u590d\u6742STEM\u4e3b\u9898\u7684\u53ef\u89c6\u5316\u89e3\u91ca\uff0c\u63a8\u52a8\u6559\u80b2\u5185\u5bb9\u6c11\u4e3b\u5316\u3002"}}
{"id": "2507.14694", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14694", "abs": "https://arxiv.org/abs/2507.14694", "authors": ["Yue Ma", "Kanglei Zhou", "Fuyang Yu", "Frederick W. B. Li", "Xiaohui Liang"], "title": "Uncertainty-aware Probabilistic 3D Human Motion Forecasting via Invertible Networks", "comment": null, "summary": "3D human motion forecasting aims to enable autonomous applications.\nEstimating uncertainty for each prediction (i.e., confidence based on\nprobability density or quantile) is essential for safety-critical contexts like\nhuman-robot collaboration to minimize risks. However, existing diverse motion\nforecasting approaches struggle with uncertainty quantification due to implicit\nprobabilistic representations hindering uncertainty modeling. We propose\nProbHMI, which introduces invertible networks to parameterize poses in a\ndisentangled latent space, enabling probabilistic dynamics modeling. A\nforecasting module then explicitly predicts future latent distributions,\nallowing effective uncertainty quantification. Evaluated on benchmarks, ProbHMI\nachieves strong performance for both deterministic and diverse prediction while\nvalidating uncertainty calibration, critical for risk-aware decision making.", "AI": {"tldr": "ProbHMI\u901a\u8fc7\u53ef\u9006\u7f51\u7edc\u5728\u89e3\u8026\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5efa\u6a213D\u4eba\u4f53\u8fd0\u52a8\uff0c\u5b9e\u73b0\u4e86\u663e\u5f0f\u7684\u6982\u7387\u52a8\u529b\u5b66\u5efa\u6a21\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002", "motivation": "\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\uff08\u5982\u4eba\u673a\u534f\u4f5c\uff09\u4e2d\uff0c\u91cf\u5316\u9884\u6d4b\u7684\u4e0d\u786e\u5b9a\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u56e0\u9690\u5f0f\u6982\u7387\u8868\u793a\u800c\u96be\u4ee5\u5b9e\u73b0\u3002", "method": "\u5f15\u5165\u53ef\u9006\u7f51\u7edc\u53c2\u6570\u5316\u59ff\u6001\uff0c\u5728\u89e3\u8026\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5efa\u6a21\uff0c\u5e76\u901a\u8fc7\u9884\u6d4b\u6a21\u5757\u663e\u5f0f\u9884\u6d4b\u672a\u6765\u6f5c\u5728\u5206\u5e03\u3002", "result": "\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cProbHMI\u5728\u786e\u5b9a\u6027\u548c\u591a\u6837\u6027\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u9a8c\u8bc1\u4e86\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\u7684\u6709\u6548\u6027\u3002", "conclusion": "ProbHMI\u4e3a\u98ce\u9669\u611f\u77e5\u51b3\u7b56\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u65b9\u6cd5\u3002"}}
{"id": "2507.14334", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14334", "abs": "https://arxiv.org/abs/2507.14334", "authors": ["Hui Yang", "Jiaoyan Chen", "Yuan He", "Yongsheng Gao", "Ian Horrocks"], "title": "Language Models as Ontology Encoders", "comment": null, "summary": "OWL (Web Ontology Language) ontologies which are able to formally represent\ncomplex knowledge and support semantic reasoning have been widely adopted\nacross various domains such as healthcare and bioinformatics. Recently,\nontology embeddings have gained wide attention due to its potential to infer\nplausible new knowledge and approximate complex reasoning. However, existing\nmethods face notable limitations: geometric model-based embeddings typically\noverlook valuable textual information, resulting in suboptimal performance,\nwhile the approaches that incorporate text, which are often based on language\nmodels, fail to preserve the logical structure. In this work, we propose a new\nontology embedding method OnT, which tunes a Pretrained Language Model (PLM)\nvia geometric modeling in a hyperbolic space for effectively incorporating\ntextual labels and simultaneously preserving class hierarchies and other\nlogical relationships of Description Logic EL. Extensive experiments on four\nreal-world ontologies show that OnT consistently outperforms the baselines\nincluding the state-of-the-art across both tasks of prediction and inference of\naxioms. OnT also demonstrates strong potential in real-world applications,\nindicated by its robust transfer learning abilities and effectiveness in real\ncases of constructing a new ontology from SNOMED CT. Data and code are\navailable at https://github.com/HuiYang1997/OnT.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u672c\u4f53\u5d4c\u5165\u65b9\u6cd5OnT\uff0c\u7ed3\u5408\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u548c\u53cc\u66f2\u51e0\u4f55\u5efa\u6a21\uff0c\u4ee5\u540c\u65f6\u5229\u7528\u6587\u672c\u4fe1\u606f\u5e76\u4fdd\u6301\u903b\u8f91\u7ed3\u6784\u3002", "motivation": "\u73b0\u6709\u672c\u4f53\u5d4c\u5165\u65b9\u6cd5\u8981\u4e48\u5ffd\u7565\u6587\u672c\u4fe1\u606f\uff0c\u8981\u4e48\u65e0\u6cd5\u4fdd\u6301\u903b\u8f91\u7ed3\u6784\uff0c\u9650\u5236\u4e86\u6027\u80fd\u548c\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u53cc\u66f2\u51e0\u4f55\u5efa\u6a21\u8c03\u6574\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff0c\u7ed3\u5408\u6587\u672c\u6807\u7b7e\u5e76\u4fdd\u6301\u63cf\u8ff0\u903b\u8f91EL\u7684\u5c42\u6b21\u548c\u5173\u7cfb\u3002", "result": "\u5728\u56db\u4e2a\u771f\u5b9e\u672c\u4f53\u6570\u636e\u96c6\u4e0a\uff0cOnT\u5728\u9884\u6d4b\u548c\u63a8\u7406\u4efb\u52a1\u4e2d\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u8fc1\u79fb\u5b66\u4e60\u80fd\u529b\u3002", "conclusion": "OnT\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u672c\u4f53\u5d4c\u5165\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\uff0c\u5982\u4eceSNOMED CT\u6784\u5efa\u65b0\u672c\u4f53\u3002"}}
{"id": "2507.14700", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.14700", "abs": "https://arxiv.org/abs/2507.14700", "authors": ["Nicholas Mohammad", "Nicola Bezzo"], "title": "Corridor-based Adaptive Control Barrier and Lyapunov Functions for Safe Mobile Robot Navigation", "comment": "To be presented in the 64th IEEE Conference on Decision and Control\n  (CDC 25)", "summary": "Safe navigation in unknown and cluttered environments remains a challenging\nproblem in robotics. Model Predictive Contour Control (MPCC) has shown promise\nfor performant obstacle avoidance by enabling precise and agile trajectory\ntracking, however, existing methods lack formal safety assurances. To address\nthis issue, we propose a general Control Lyapunov Function (CLF) and Control\nBarrier Function (CBF) enabled MPCC framework that enforces safety constraints\nderived from a free-space corridor around the planned trajectory. To enhance\nfeasibility, we dynamically adapt the CBF parameters at runtime using a Soft\nActor-Critic (SAC) policy. The approach is validated with extensive simulations\nand an experiment on mobile robot navigation in unknown cluttered environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408CLF\u548cCBF\u7684MPCC\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574CBF\u53c2\u6570\u786e\u4fdd\u5b89\u5168\u5bfc\u822a\u3002", "motivation": "\u73b0\u6709MPCC\u65b9\u6cd5\u7f3a\u4e4f\u5f62\u5f0f\u5316\u5b89\u5168\u4fdd\u969c\uff0c\u9700\u89e3\u51b3\u672a\u77e5\u6742\u4e71\u73af\u5883\u4e2d\u7684\u5b89\u5168\u5bfc\u822a\u95ee\u9898\u3002", "method": "\u7ed3\u5408CLF\u548cCBF\u7684MPCC\u6846\u67b6\uff0c\u52a8\u6001\u8c03\u6574CBF\u53c2\u6570\uff08\u4f7f\u7528SAC\u7b56\u7565\uff09\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u548c\u79fb\u52a8\u673a\u5668\u4eba\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u672a\u77e5\u6742\u4e71\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u5b89\u5168\u5bfc\u822a\uff0c\u5e76\u63d0\u5347\u4e86\u53ef\u884c\u6027\u3002"}}
{"id": "2507.14335", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14335", "abs": "https://arxiv.org/abs/2507.14335", "authors": ["Nicolas Wischermann", "Claudio Mayrink Verdun", "Gabriel Poesia", "Francesco Noseda"], "title": "ProofCompass: Enhancing Specialized Provers with LLM Guidance", "comment": "19 pages, 7 figures. Accepted at the 2nd AI for MATH Workshop at the\n  42nd International Conference on Machine Learning (ICML 2025)", "summary": "Language models have become increasingly powerful tools for formal\nmathematical reasoning. However, most existing approaches rely exclusively on\neither large general-purpose models or smaller specialized models, each with\ndistinct limitations, while training specialized large models still requires\nsignificant computational resources. This paper introduces ProofCompass, a\nnovel hybrid methodology that achieves remarkable computational efficiency by\nstrategically guiding existing specialized prover methods, such as\nDeepSeek-Prover-v1.5-RL (DSP-v1.5) with a Large Language Model (LLM) without\nrequiring additional model training. The LLM provides natural language proof\nstrategies and analyzes failed attempts to select intermediate lemmas, enabling\neffective problem decomposition. On the miniF2F benchmark, ProofCompass\ndemonstrates substantial resource efficiency: it outperforms DSP-v1.5 ($54.9\\%\n\\rightarrow 55.3\\%$) while using 25x fewer attempts ($3200 \\rightarrow 128$).\nOur synergistic approach paves the way for simultaneously improving\ncomputational efficiency and accuracy in formal theorem proving.", "AI": {"tldr": "ProofCompass\u662f\u4e00\u79cd\u6df7\u5408\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u4e13\u7528\u8bc1\u660e\u5668\uff08\u5982DSP-v1.5\uff09\uff0c\u5728\u4e0d\u589e\u52a0\u8bad\u7ec3\u6210\u672c\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u6570\u5b66\u63a8\u7406\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u5927\u578b\u901a\u7528\u6a21\u578b\uff0c\u8981\u4e48\u4f9d\u8d56\u5c0f\u578b\u4e13\u7528\u6a21\u578b\uff0c\u5404\u6709\u5c40\u9650\u6027\uff0c\u800c\u8bad\u7ec3\u5927\u578b\u4e13\u7528\u6a21\u578b\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\u3002ProofCompass\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "ProofCompass\u5229\u7528LLM\u63d0\u4f9b\u81ea\u7136\u8bed\u8a00\u8bc1\u660e\u7b56\u7565\u5e76\u5206\u6790\u5931\u8d25\u5c1d\u8bd5\uff0c\u9009\u62e9\u4e2d\u95f4\u5f15\u7406\uff0c\u4ece\u800c\u5b9e\u73b0\u95ee\u9898\u5206\u89e3\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u989d\u5916\u6a21\u578b\u8bad\u7ec3\u3002", "result": "\u5728miniF2F\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cProofCompass\u5728\u4ec5\u4f7f\u7528128\u6b21\u5c1d\u8bd5\uff08\u539f\u97003200\u6b21\uff09\u7684\u60c5\u51b5\u4e0b\uff0c\u5c06\u51c6\u786e\u7387\u4ece54.9%\u63d0\u5347\u81f355.3%\u3002", "conclusion": "ProofCompass\u5c55\u793a\u4e86\u5728\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u548c\u51c6\u786e\u6027\u65b9\u9762\u7684\u534f\u540c\u6548\u5e94\uff0c\u4e3a\u5f62\u5f0f\u5316\u5b9a\u7406\u8bc1\u660e\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.14721", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14721", "abs": "https://arxiv.org/abs/2507.14721", "authors": ["Keita Kobashi", "Masayoshi Tomizuka"], "title": "Leveraging Extrinsic Dexterity for Occluded Grasping on Grasp Constraining Walls", "comment": "7 pages, 7 figures", "summary": "This study addresses the problem of occluded grasping, where primary grasp\nconfigurations of an object are not available due to occlusion with\nenvironment. Simple parallel grippers often struggle with such tasks due to\nlimited dexterity and actuation constraints. Prior works have explored object\npose reorientation such as pivoting by utilizing extrinsic contacts between an\nobject and an environment feature like a wall, to make the object graspable.\nHowever, such works often assume the presence of a short wall, and this\nassumption may not always hold in real-world scenarios. If the wall available\nfor interaction is too large or too tall, the robot may still fail to grasp the\nobject even after pivoting, and the robot must combine different types of\nactions to grasp. To address this, we propose a hierarchical reinforcement\nlearning (RL) framework. We use Q-learning to train a high-level policy that\nselects the type of action expected to yield the highest reward. The selected\nlow-level skill then samples a specific robot action in continuous space. To\nguide the robot to an appropriate location for executing the selected action,\nwe adopt a Conditional Variational Autoencoder (CVAE). We condition the CVAE on\nthe object point cloud and the skill ID, enabling it to infer a suitable\nlocation based on the object geometry and the selected skill. To promote\ngeneralization, we apply domain randomization during the training of low-level\nskills. The RL policy is trained entirely in simulation with a box-like object\nand deployed to six objects in real world. We conduct experiments to evaluate\nour method and demonstrate both its generalizability and robust sim-to-real\ntransfer performance with promising success rates.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u906e\u6321\u6293\u53d6\u95ee\u9898\uff0c\u7ed3\u5408Q\u5b66\u4e60\u548cCVAE\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6cdb\u5316\u6027\u548c\u7a33\u5065\u7684\u4eff\u771f\u5230\u73b0\u5b9e\u8fc1\u79fb\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u56e0\u73af\u5883\u906e\u6321\u5bfc\u81f4\u7684\u4e3b\u8981\u6293\u53d6\u914d\u7f6e\u4e0d\u53ef\u7528\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u5e73\u884c\u5939\u6301\u5668\u7075\u6d3b\u6027\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u91c7\u7528\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u9ad8\u5c42\u7b56\u7565\u9009\u62e9\u52a8\u4f5c\u7c7b\u578b\uff0c\u4f4e\u5c42\u6280\u80fd\u5728\u8fde\u7eed\u7a7a\u95f4\u4e2d\u91c7\u6837\u5177\u4f53\u52a8\u4f5c\uff0c\u5e76\u4f7f\u7528CVAE\u63a8\u65ad\u5408\u9002\u4f4d\u7f6e\u3002", "result": "\u5728\u4eff\u771f\u548c\u73b0\u5b9e\u5b9e\u9a8c\u4e2d\u5c55\u793a\u4e86\u65b9\u6cd5\u7684\u6cdb\u5316\u6027\u548c\u7a33\u5065\u7684\u4eff\u771f\u5230\u73b0\u5b9e\u8fc1\u79fb\u6027\u80fd\uff0c\u6210\u529f\u7387\u8f83\u9ad8\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u906e\u6321\u6293\u53d6\u95ee\u9898\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.14393", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14393", "abs": "https://arxiv.org/abs/2507.14393", "authors": ["Humza Sami", "Mubashir ul Islam", "Pierre-Emmanuel Gaillardon", "Valerio Tenace"], "title": "Adaptive Multi-Agent Reasoning via Automated Workflow Generation", "comment": null, "summary": "The rise of Large Reasoning Models (LRMs) promises a significant leap forward\nin language model capabilities, aiming to tackle increasingly sophisticated\ntasks with unprecedented efficiency and accuracy. However, despite their\nimpressive performance, recent studies have highlighted how current reasoning\nmodels frequently fail to generalize to novel, unseen problems, often resorting\nto memorized solutions rather than genuine inferential reasoning. Such behavior\nunderscores a critical limitation in modern LRMs, i.e., their tendency toward\noverfitting, which in turn results in poor generalization in problem-solving\ncapabilities.\n  In this paper, we introduce Nexus Architect, an enhanced iteration of our\nmulti-agent system framework, Nexus, equipped with a novel automated workflow\nsynthesis mechanism. Given a user's prompt and a small set of representative\nexamples, the Architect autonomously generates a tailored reasoning workflow by\nselecting suitable strategies, tool integrations, and adversarial techniques\nfor a specific problem class. Furthermore, the Architect includes an iterative\nprompt refinement mechanism that fine-tunes agents' system prompts to maximize\nperformance and improve the generalization capabilities of the system.\n  We empirically evaluate Nexus Architect by employing an off-the-shelf,\nnon-reasoning model on a custom dataset of challenging logical questions and\ncompare its performance against state-of-the-art LRMs. Results show that Nexus\nArchitect consistently outperforms existing solutions, achieving up to a 66%\nincrease in pass rate over Gemini 2.5 Flash Preview, nearly 2.5$\\times$ against\nClaude Sonnet 4 and DeepSeek-R1, and over 3$\\times$ w.r.t. Llama 4 Scout.", "AI": {"tldr": "Nexus Architect\u662f\u4e00\u79cd\u6539\u8fdb\u7684\u591a\u4ee3\u7406\u7cfb\u7edf\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u52a8\u751f\u6210\u5b9a\u5236\u5316\u63a8\u7406\u5de5\u4f5c\u6d41\u548c\u8fed\u4ee3\u63d0\u793a\u4f18\u5316\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u5728\u89e3\u51b3\u65b0\u95ee\u9898\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u503e\u5411\u4e8e\u4f9d\u8d56\u8bb0\u5fc6\u800c\u975e\u771f\u6b63\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u63d0\u51faNexus Architect\u6846\u67b6\uff0c\u7ed3\u5408\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\u5408\u6210\u548c\u8fed\u4ee3\u63d0\u793a\u4f18\u5316\u673a\u5236\uff0c\u751f\u6210\u9488\u5bf9\u7279\u5b9a\u95ee\u9898\u7c7b\u7684\u63a8\u7406\u7b56\u7565\u3002", "result": "\u5728\u903b\u8f91\u95ee\u9898\u6570\u636e\u96c6\u4e0a\uff0cNexus Architect\u7684\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709LRMs\uff0c\u6700\u9ad8\u63d0\u534766%\u901a\u8fc7\u7387\u3002", "conclusion": "Nexus Architect\u901a\u8fc7\u5b9a\u5236\u5316\u63a8\u7406\u5de5\u4f5c\u6d41\u548c\u63d0\u793a\u4f18\u5316\uff0c\u6709\u6548\u89e3\u51b3\u4e86LRMs\u7684\u6cdb\u5316\u95ee\u9898\uff0c\u6027\u80fd\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2507.14731", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14731", "abs": "https://arxiv.org/abs/2507.14731", "authors": ["Haitong Wang", "Aaron Hao Tan", "Angus Fung", "Goldie Nejat"], "title": "X-Nav: Learning End-to-End Cross-Embodiment Navigation for Mobile Robots", "comment": null, "summary": "Existing navigation methods are primarily designed for specific robot\nembodiments, limiting their generalizability across diverse robot platforms. In\nthis paper, we introduce X-Nav, a novel framework for end-to-end\ncross-embodiment navigation where a single unified policy can be deployed\nacross various embodiments for both wheeled and quadrupedal robots. X-Nav\nconsists of two learning stages: 1) multiple expert policies are trained using\ndeep reinforcement learning with privileged observations on a wide range of\nrandomly generated robot embodiments; and 2) a single general policy is\ndistilled from the expert policies via navigation action chunking with\ntransformer (Nav-ACT). The general policy directly maps visual and\nproprioceptive observations to low-level control commands, enabling\ngeneralization to novel robot embodiments. Simulated experiments demonstrated\nthat X-Nav achieved zero-shot transfer to both unseen embodiments and\nphotorealistic environments. A scalability study showed that the performance of\nX-Nav improves when trained with an increasing number of randomly generated\nembodiments. An ablation study confirmed the design choices of X-Nav.\nFurthermore, real-world experiments were conducted to validate the\ngeneralizability of X-Nav in real-world environments.", "AI": {"tldr": "X-Nav\u662f\u4e00\u4e2a\u8de8\u673a\u5668\u4eba\u5e73\u53f0\u7684\u7aef\u5230\u7aef\u5bfc\u822a\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u5b66\u4e60\uff08\u4e13\u5bb6\u7b56\u7565\u8bad\u7ec3\u4e0e\u7b56\u7565\u84b8\u998f\uff09\u5b9e\u73b0\u901a\u7528\u6027\u3002\u5b9e\u9a8c\u8bc1\u660e\u5176\u80fd\u96f6\u6837\u672c\u8fc1\u79fb\u81f3\u65b0\u5e73\u53f0\u548c\u771f\u5b9e\u73af\u5883\u3002", "motivation": "\u73b0\u6709\u5bfc\u822a\u65b9\u6cd5\u5c40\u9650\u4e8e\u7279\u5b9a\u673a\u5668\u4eba\u5e73\u53f0\uff0c\u7f3a\u4e4f\u901a\u7528\u6027\u3002X-Nav\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u5b9e\u73b0\u8de8\u5e73\u53f0\u5bfc\u822a\u3002", "method": "1) \u4f7f\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u591a\u4e2a\u4e13\u5bb6\u7b56\u7565\uff1b2) \u901a\u8fc7Nav-ACT\u84b8\u998f\u51fa\u901a\u7528\u7b56\u7565\uff0c\u76f4\u63a5\u6620\u5c04\u89c2\u6d4b\u5230\u63a7\u5236\u547d\u4ee4\u3002", "result": "X-Nav\u5728\u6a21\u62df\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u5747\u8868\u73b0\u51fa\u8272\uff0c\u652f\u6301\u96f6\u6837\u672c\u8fc1\u79fb\uff0c\u4e14\u6027\u80fd\u968f\u8bad\u7ec3\u5e73\u53f0\u6570\u91cf\u589e\u52a0\u800c\u63d0\u5347\u3002", "conclusion": "X-Nav\u8bc1\u660e\u4e86\u8de8\u5e73\u53f0\u5bfc\u822a\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u901a\u7528\u673a\u5668\u4eba\u5bfc\u822a\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.14406", "categories": ["cs.AI", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.14406", "abs": "https://arxiv.org/abs/2507.14406", "authors": ["Michael J. Zellinger", "Matt Thomson"], "title": "Fail Fast, or Ask: Mitigating the Deficiencies of Reasoning LLMs with Human-in-the-Loop Systems Engineering", "comment": "8 pages, 5 figures", "summary": "State-of-the-art reasoning LLMs are powerful problem solvers, but they still\noccasionally make mistakes. However, adopting AI models in risk-sensitive\ndomains often requires error rates near 0%. To address this gap, we propose\ncollaboration between a reasoning model and a human expert who resolves queries\nthe model cannot confidently answer. We find that quantifying the uncertainty\nof a reasoning model through the length of its reasoning trace yields an\neffective basis for deferral to a human, e.g., cutting the error rate of Qwen3\n235B-A22B on difficult MATH problems from 3% to less than 1% when deferring\n7.5% of queries. However, the high latency of reasoning models still makes them\nchallenging to deploy on use cases with high query volume. To address this\nchallenge, we explore fronting a reasoning model with a large non-reasoning\nmodel. We call this modified human-in-the-loop system \"Fail Fast, or Ask\",\nsince the non-reasoning model may defer difficult queries to the human expert\ndirectly (\"failing fast\"), without incurring the reasoning model's higher\nlatency. We show that this approach yields around 40% latency reduction and\nabout 50% cost savings for DeepSeek R1 while maintaining 90+% area under the\naccuracy-rejection curve. However, we observe that latency savings are lower\nthan expected because of \"latency drag\", the phenomenon that processing easier\nqueries with a non-reasoning model pushes the reasoning model's latency\ndistribution towards longer latencies. Broadly, our results suggest that the\ndeficiencies of state-of-the-art reasoning models -- nontrivial error rates and\nhigh latency -- can be substantially mitigated through black-box systems\nengineering, without requiring access to LLM internals.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u63a8\u7406\u6a21\u578b\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u7684\u534f\u4f5c\u7cfb\u7edf\uff0c\u901a\u8fc7\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\u964d\u4f4e\u9519\u8bef\u7387\uff0c\u5e76\u63a2\u7d22\u4e86\u975e\u63a8\u7406\u6a21\u578b\u524d\u7f6e\u4ee5\u51cf\u5c11\u5ef6\u8fdf\u548c\u6210\u672c\u7684\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u63a8\u7406\u6a21\u578b\u5728\u98ce\u9669\u654f\u611f\u9886\u57df\u9519\u8bef\u7387\u8f83\u9ad8\u4e14\u5ef6\u8fdf\u5927\uff0c\u9700\u6539\u8fdb\u4ee5\u6ee1\u8db3\u5b9e\u9645\u9700\u6c42\u3002", "method": "1. \u901a\u8fc7\u63a8\u7406\u8f68\u8ff9\u957f\u5ea6\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\uff0c\u5c06\u4e0d\u786e\u5b9a\u95ee\u9898\u8f6c\u4ea4\u4eba\u7c7b\u4e13\u5bb6\uff1b2. \u524d\u7f6e\u975e\u63a8\u7406\u6a21\u578b\u5feb\u901f\u7b5b\u9009\u95ee\u9898\uff0c\u51cf\u5c11\u5ef6\u8fdf\u548c\u6210\u672c\u3002", "result": "1. \u9519\u8bef\u7387\u4ece3%\u964d\u81f31%\u4ee5\u4e0b\uff1b2. \u5ef6\u8fdf\u964d\u4f4e40%\uff0c\u6210\u672c\u8282\u770150%\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u51c6\u786e\u7387\u3002", "conclusion": "\u901a\u8fc7\u7cfb\u7edf\u8bbe\u8ba1\uff08\u5982\u534f\u4f5c\u548c\u6a21\u578b\u7ec4\u5408\uff09\u53ef\u663e\u8457\u6539\u5584\u63a8\u7406\u6a21\u578b\u7684\u9519\u8bef\u7387\u548c\u5ef6\u8fdf\u95ee\u9898\uff0c\u65e0\u9700\u4fee\u6539\u6a21\u578b\u5185\u90e8\u3002"}}
{"id": "2507.14820", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14820", "abs": "https://arxiv.org/abs/2507.14820", "authors": ["Bingran Chen", "Baorun Li", "Jian Yang", "Yong Liu", "Guangyao Zhai"], "title": "KGN-Pro: Keypoint-Based Grasp Prediction through Probabilistic 2D-3D Correspondence Learning", "comment": null, "summary": "High-level robotic manipulation tasks demand flexible 6-DoF grasp estimation\nto serve as a basic function. Previous approaches either directly generate\ngrasps from point-cloud data, suffering from challenges with small objects and\nsensor noise, or infer 3D information from RGB images, which introduces\nexpensive annotation requirements and discretization issues. Recent methods\nmitigate some challenges by retaining a 2D representation to estimate grasp\nkeypoints and applying Perspective-n-Point (PnP) algorithms to compute 6-DoF\nposes. However, these methods are limited by their non-differentiable nature\nand reliance solely on 2D supervision, which hinders the full exploitation of\nrich 3D information. In this work, we present KGN-Pro, a novel grasping network\nthat preserves the efficiency and fine-grained object grasping of previous KGNs\nwhile integrating direct 3D optimization through probabilistic PnP layers.\nKGN-Pro encodes paired RGB-D images to generate Keypoint Map, and further\noutputs a 2D confidence map to weight keypoint contributions during\nre-projection error minimization. By modeling the weighted sum of squared\nre-projection errors probabilistically, the network effectively transmits 3D\nsupervision to its 2D keypoint predictions, enabling end-to-end learning.\nExperiments on both simulated and real-world platforms demonstrate that KGN-Pro\noutperforms existing methods in terms of grasp cover rate and success rate.", "AI": {"tldr": "KGN-Pro\u662f\u4e00\u79cd\u65b0\u578b\u6293\u53d6\u7f51\u7edc\uff0c\u901a\u8fc7\u6982\u7387PnP\u5c42\u76f4\u63a5\u4f18\u53163D\u4fe1\u606f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d562D\u76d1\u7763\u548c\u975e\u53ef\u5fae\u6027\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6293\u53d6\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u57286-DoF\u6293\u53d6\u4f30\u8ba1\u4e2d\u5b58\u5728\u5c0f\u7269\u4f53\u548c\u4f20\u611f\u5668\u566a\u58f0\u7684\u6311\u6218\uff0c\u6216\u4f9d\u8d56\u6602\u8d35\u76843D\u6807\u6ce8\u548c\u79bb\u6563\u5316\u95ee\u9898\u3002KGN-Pro\u65e8\u5728\u7ed3\u54083D\u4f18\u5316\uff0c\u5145\u5206\u5229\u75283D\u4fe1\u606f\u3002", "method": "KGN-Pro\u901a\u8fc7RGB-D\u56fe\u50cf\u751f\u6210\u5173\u952e\u70b9\u56fe\u548c2D\u7f6e\u4fe1\u56fe\uff0c\u5229\u7528\u6982\u7387PnP\u5c42\u8fdb\u884c\u7aef\u5230\u7aef\u5b66\u4e60\uff0c\u4f18\u5316\u91cd\u6295\u5f71\u8bef\u5dee\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cKGN-Pro\u5728\u6a21\u62df\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u6293\u53d6\u8986\u76d6\u7387\u548c\u6210\u529f\u7387\u66f4\u9ad8\u3002", "conclusion": "KGN-Pro\u901a\u8fc73D\u4f18\u5316\u548c\u7aef\u5230\u7aef\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6293\u53d6\u6027\u80fd\uff0c\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.14417", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14417", "abs": "https://arxiv.org/abs/2507.14417", "authors": ["Aryo Pradipta Gema", "Alexander H\u00e4gele", "Runjin Chen", "Andy Arditi", "Jacob Goldman-Wetzler", "Kit Fraser-Taliente", "Henry Sleight", "Linda Petrini", "Julian Michael", "Beatrice Alex", "Pasquale Minervini", "Yanda Chen", "Joe Benton", "Ethan Perez"], "title": "Inverse Scaling in Test-Time Compute", "comment": null, "summary": "We construct evaluation tasks where extending the reasoning length of Large\nReasoning Models (LRMs) deteriorates performance, exhibiting an inverse scaling\nrelationship between test-time compute and accuracy. Our evaluation tasks span\nfour categories: simple counting tasks with distractors, regression tasks with\nspurious features, deduction tasks with constraint tracking, and advanced AI\nrisks. We identify five distinct failure modes when models reason for longer:\n1) Claude models become increasingly distracted by irrelevant information; 2)\nOpenAI o-series models resist distractors but overfit to problem framings; 3)\nmodels shift from reasonable priors to spurious correlations; 4) all models\nshow difficulties in maintaining focus on complex deductive tasks; and 5)\nextended reasoning may amplify concerning behaviors, with Claude Sonnet 4\nshowing increased expressions of self-preservation. These findings suggest that\nwhile test-time compute scaling remains promising for improving model\ncapabilities, it may inadvertently reinforce problematic reasoning patterns.\nOur results demonstrate the importance of evaluating models across diverse\nreasoning lengths to identify and address these failure modes in LRMs.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u5728\u5ef6\u957f\u63a8\u7406\u957f\u5ea6\u65f6\u6027\u80fd\u4e0b\u964d\uff0c\u8868\u73b0\u4e3a\u6d4b\u8bd5\u8ba1\u7b97\u91cf\u4e0e\u51c6\u786e\u7387\u7684\u53cd\u6bd4\u5173\u7cfb\u3002\u4efb\u52a1\u6db5\u76d6\u56db\u7c7b\uff0c\u63ed\u793a\u4e86\u4e94\u79cd\u5931\u8d25\u6a21\u5f0f\uff0c\u8868\u660e\u8ba1\u7b97\u91cf\u6269\u5c55\u53ef\u80fd\u5f3a\u5316\u95ee\u9898\u63a8\u7406\u6a21\u5f0f\u3002", "motivation": "\u63a2\u8ba8\u6d4b\u8bd5\u8ba1\u7b97\u91cf\u6269\u5c55\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u63ed\u793aLRMs\u5728\u5ef6\u957f\u63a8\u7406\u65f6\u7684\u6f5c\u5728\u95ee\u9898\u3002", "method": "\u6784\u5efa\u56db\u7c7b\u8bc4\u4f30\u4efb\u52a1\uff08\u8ba1\u6570\u3001\u56de\u5f52\u3001\u6f14\u7ece\u3001AI\u98ce\u9669\uff09\uff0c\u5206\u6790\u6a21\u578b\u5728\u4e0d\u540c\u63a8\u7406\u957f\u5ea6\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u53d1\u73b0\u4e94\u79cd\u5931\u8d25\u6a21\u5f0f\uff0c\u5982\u5206\u5fc3\u3001\u8fc7\u62df\u5408\u3001\u865a\u5047\u5173\u8054\u7b49\uff0c\u8868\u660e\u8ba1\u7b97\u91cf\u6269\u5c55\u53ef\u80fd\u5e26\u6765\u8d1f\u9762\u5f71\u54cd\u3002", "conclusion": "\u9700\u591a\u6837\u5316\u8bc4\u4f30\u63a8\u7406\u957f\u5ea6\u4ee5\u8bc6\u522b\u548c\u89e3\u51b3LRMs\u7684\u5931\u8d25\u6a21\u5f0f\uff0c\u8ba1\u7b97\u91cf\u6269\u5c55\u9700\u8c28\u614e\u3002"}}
{"id": "2507.14903", "categories": ["cs.RO", "I.2.9; I.2.10; I.2.11"], "pdf": "https://arxiv.org/pdf/2507.14903", "abs": "https://arxiv.org/abs/2507.14903", "authors": ["Pan Hu"], "title": "CoMoCAVs: Cohesive Decision-Guided Motion Planning for Connected and Autonomous Vehicles with Multi-Policy Reinforcement Learning", "comment": "8 pages, 5 figures", "summary": "Autonomous driving demands reliable and efficient solutions to closely\nrelated problems such as decision-making and motion planning. In this work,\ndecision-making refers specifically to highway lane selection, while motion\nplanning involves generating control commands (such as speed and steering) to\nreach the chosen lane. In the context of Connected Autonomous Vehicles (CAVs),\nachieving both flexible and safe lane selection alongside precise trajectory\nexecution remains a significant challenge. This paper proposes a framework\ncalled Cohesive Decision-Guided Motion Planning (CDGMP), which tightly\nintegrates decision-making and motion planning using a Mixture of Experts (MoE)\ninspired architecture combined with multi-policy reinforcement learning. By\ncoordinating multiple specialized sub-networks through a gating mechanism, the\nmethod decomposes the complex driving task into modular components. Each\nsub-network focuses on a specific aspect of driving, improving efficiency by\nactivating only the most relevant modules during inference. This design also\nenhances safety through modular specialization. CDGMP improves the adaptability\nand robustness of CAVs across diverse traffic scenarios, offering a scalable\nsolution to real-world autonomy challenges. The architectural principles behind\nCDGMP, especially the use of MoE, also provide a strong foundation for other\nhigh-dimensional decision and control tasks. Simulation results (available at\nhttps://youtu.be/_-4OXNHV0UY) demonstrate reliable performance in both lane\nselection and motion planning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCDGMP\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\u548c\u591a\u7b56\u7565\u5f3a\u5316\u5b66\u4e60\uff0c\u5c06\u51b3\u7b56\u5236\u5b9a\u4e0e\u8fd0\u52a8\u89c4\u5212\u7d27\u5bc6\u7ed3\u5408\uff0c\u4ee5\u63d0\u9ad8\u81ea\u52a8\u9a7e\u9a76\u7684\u7075\u6d3b\u6027\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u4e2d\uff0c\u5982\u4f55\u5728\u4fdd\u8bc1\u5b89\u5168\u7684\u540c\u65f6\u5b9e\u73b0\u7075\u6d3b\u7684\u8f66\u9053\u9009\u62e9\u548c\u7cbe\u786e\u7684\u8f68\u8ff9\u6267\u884c\u662f\u4e00\u4e2a\u91cd\u8981\u6311\u6218\u3002", "method": "\u91c7\u7528\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u67b6\u6784\u548c\u591a\u7b56\u7565\u5f3a\u5316\u5b66\u4e60\uff0c\u901a\u8fc7\u95e8\u63a7\u673a\u5236\u534f\u8c03\u591a\u4e2a\u4e13\u7528\u5b50\u7f51\u7edc\uff0c\u5c06\u590d\u6742\u9a7e\u9a76\u4efb\u52a1\u5206\u89e3\u4e3a\u6a21\u5757\u5316\u7ec4\u4ef6\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0cCDGMP\u5728\u8f66\u9053\u9009\u62e9\u548c\u8fd0\u52a8\u89c4\u5212\u4e2d\u8868\u73b0\u51fa\u53ef\u9760\u7684\u6027\u80fd\u3002", "conclusion": "CDGMP\u4e3a\u81ea\u52a8\u9a7e\u9a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5176\u67b6\u6784\u539f\u7406\u4e5f\u4e3a\u5176\u4ed6\u9ad8\u7ef4\u51b3\u7b56\u548c\u63a7\u5236\u4efb\u52a1\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.14447", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14447", "abs": "https://arxiv.org/abs/2507.14447", "authors": ["Guancheng Zeng", "Xueyi Chen", "Jiawang Hu", "Shaohua Qi", "Yaxuan Mao", "Zhantao Wang", "Yifan Nie", "Shuang Li", "Qiuyang Feng", "Pengxu Qiu", "Yujia Wang", "Wenqiang Han", "Linyan Huang", "Gang Li", "Jingjing Mo", "Haowen Hu"], "title": "Routine: A Structural Planning Framework for LLM Agent System in Enterprise", "comment": "26 pages, 8 figures, 5 tables", "summary": "The deployment of agent systems in an enterprise environment is often\nhindered by several challenges: common models lack domain-specific process\nknowledge, leading to disorganized plans, missing key tools, and poor execution\nstability. To address this, this paper introduces Routine, a multi-step agent\nplanning framework designed with a clear structure, explicit instructions, and\nseamless parameter passing to guide the agent's execution module in performing\nmulti-step tool-calling tasks with high stability. In evaluations conducted\nwithin a real-world enterprise scenario, Routine significantly increases the\nexecution accuracy in model tool calls, increasing the performance of GPT-4o\nfrom 41.1% to 96.3%, and Qwen3-14B from 32.6% to 83.3%. We further constructed\na Routine-following training dataset and fine-tuned Qwen3-14B, resulting in an\naccuracy increase to 88.2% on scenario-specific evaluations, indicating\nimproved adherence to execution plans. In addition, we employed Routine-based\ndistillation to create a scenario-specific, multi-step tool-calling dataset.\nFine-tuning on this distilled dataset raised the model's accuracy to 95.5%,\napproaching GPT-4o's performance. These results highlight Routine's\neffectiveness in distilling domain-specific tool-usage patterns and enhancing\nmodel adaptability to new scenarios. Our experimental results demonstrate that\nRoutine provides a practical and accessible approach to building stable agent\nworkflows, accelerating the deployment and adoption of agent systems in\nenterprise environments, and advancing the technical vision of AI for Process.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faRoutine\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u89c4\u5212\u548c\u53c2\u6570\u4f20\u9012\u63d0\u5347\u591a\u6b65\u5de5\u5177\u8c03\u7528\u4efb\u52a1\u7684\u6267\u884c\u7a33\u5b9a\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u4f01\u4e1a\u73af\u5883\u4e2d\u4ee3\u7406\u7cfb\u7edf\u7684\u90e8\u7f72\u5e38\u56e0\u7f3a\u4e4f\u9886\u57df\u77e5\u8bc6\u5bfc\u81f4\u6267\u884c\u4e0d\u7a33\u5b9a\uff0c\u9700\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5f15\u5165Routine\u6846\u67b6\uff0c\u5305\u542b\u6e05\u6670\u7ed3\u6784\u3001\u660e\u786e\u6307\u4ee4\u548c\u53c2\u6570\u4f20\u9012\uff0c\u652f\u6301\u591a\u6b65\u5de5\u5177\u8c03\u7528\u4efb\u52a1\u3002", "result": "Routine\u663e\u8457\u63d0\u5347\u6a21\u578b\u6267\u884c\u51c6\u786e\u7387\uff08GPT-4o\u4ece41.1%\u5347\u81f396.3%\uff0cQwen3-14B\u4ece32.6%\u5347\u81f383.3%\uff09\uff0c\u5e76\u901a\u8fc7\u5fae\u8c03\u8fdb\u4e00\u6b65\u63d0\u5347\u81f395.5%\u3002", "conclusion": "Routine\u6709\u6548\u63d0\u70bc\u9886\u57df\u5de5\u5177\u4f7f\u7528\u6a21\u5f0f\uff0c\u63d0\u5347\u6a21\u578b\u9002\u5e94\u6027\uff0c\u52a0\u901f\u4f01\u4e1a\u73af\u5883\u4e2d\u4ee3\u7406\u7cfb\u7edf\u7684\u90e8\u7f72\u3002"}}
{"id": "2507.14914", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14914", "abs": "https://arxiv.org/abs/2507.14914", "authors": ["Zhexuan Xu", "Jie Wang", "Siyuan Xu", "Zijie Geng", "Mingxuan Yuan", "Feng Wu"], "title": "One Step Beyond: Feedthrough & Placement-Aware Rectilinear Floorplanner", "comment": null, "summary": "Floorplanning determines the shapes and locations of modules on a chip canvas\nand plays a critical role in optimizing the chip's Power, Performance, and Area\n(PPA) metrics. However, existing floorplanning approaches often fail to\nintegrate with subsequent physical design stages, leading to suboptimal\nin-module component placement and excessive inter-module feedthrough. To tackle\nthis challenge, we propose Flora, a three-stage feedthrough and placement aware\nrectilinear floorplanner. In the first stage, Flora employs wiremask and\nposition mask techniques to achieve coarse-grained optimization of HPWL and\nfeedthrough. In the second stage, under the constraint of a fixed outline,\nFlora achieves a zero-whitespace layout by locally resizing module shapes,\nthereby performing fine-grained optimization of feedthrough and improving\ncomponent placement. In the third stage, Flora utilizes a fast tree\nsearch-based method to efficiently place components-including macros and\nstandard cells-within each module, subsequently adjusting module boundaries\nbased on the placement results to enable cross-stage optimization. Experimental\nresults show that Flora outperforms recent state-of-the-art floorplanning\napproaches, achieving an average reduction of 6% in HPWL, 5.16% in FTpin,\n29.15% in FTmod, and a 14% improvement in component placement performance.", "AI": {"tldr": "Flora\u662f\u4e00\u79cd\u4e09\u9636\u6bb5\u9988\u901a\u548c\u5e03\u5c40\u611f\u77e5\u7684\u77e9\u5f62\u5e73\u9762\u89c4\u5212\u5668\uff0c\u901a\u8fc7\u5206\u9636\u6bb5\u4f18\u5316HPWL\u3001\u9988\u901a\u548c\u7ec4\u4ef6\u5e03\u5c40\uff0c\u663e\u8457\u63d0\u5347\u82af\u7247\u8bbe\u8ba1\u7684PPA\u6307\u6807\u3002", "motivation": "\u73b0\u6709\u5e73\u9762\u89c4\u5212\u65b9\u6cd5\u96be\u4ee5\u4e0e\u540e\u7eed\u7269\u7406\u8bbe\u8ba1\u9636\u6bb5\u96c6\u6210\uff0c\u5bfc\u81f4\u6a21\u5757\u5185\u7ec4\u4ef6\u5e03\u5c40\u4e0d\u4f18\u548c\u6a21\u5757\u95f4\u9988\u901a\u8fc7\u591a\u3002", "method": "Flora\u5206\u4e09\u9636\u6bb5\uff1a1) \u4f7f\u7528\u7ebf\u63a9\u6a21\u548c\u4f4d\u7f6e\u63a9\u6a21\u6280\u672f\u7c97\u4f18\u5316HPWL\u548c\u9988\u901a\uff1b2) \u5728\u56fa\u5b9a\u8f6e\u5ed3\u4e0b\u901a\u8fc7\u6a21\u5757\u5f62\u72b6\u8c03\u6574\u5b9e\u73b0\u96f6\u7a7a\u767d\u5e03\u5c40\uff1b3) \u5feb\u901f\u6811\u641c\u7d22\u6cd5\u4f18\u5316\u7ec4\u4ef6\u5e03\u5c40\u5e76\u8c03\u6574\u6a21\u5757\u8fb9\u754c\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cFlora\u5e73\u5747\u51cf\u5c11HPWL 6%\u3001FTpin 5.16%\u3001FTmod 29.15%\uff0c\u7ec4\u4ef6\u5e03\u5c40\u6027\u80fd\u63d0\u534714%\u3002", "conclusion": "Flora\u901a\u8fc7\u8de8\u9636\u6bb5\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u5e73\u9762\u89c4\u5212\u6027\u80fd\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2507.14468", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14468", "abs": "https://arxiv.org/abs/2507.14468", "authors": ["Yitong Lin", "Jiaying He", "Jiahe Chen", "Xinnan Zhu", "Jianwei Zheng", "Tao Bo"], "title": "BioGraphFusion: Graph Knowledge Embedding for Biological Completion and Reasoning", "comment": "Accepted by Bioinformatics on July 11th", "summary": "Motivation: Biomedical knowledge graphs (KGs) are crucial for drug discovery\nand disease understanding, yet their completion and reasoning are challenging.\nKnowledge Embedding (KE) methods capture global semantics but struggle with\ndynamic structural integration, while Graph Neural Networks (GNNs) excel\nlocally but often lack semantic understanding. Even ensemble approaches,\nincluding those leveraging language models, often fail to achieve a deep,\nadaptive, and synergistic co-evolution between semantic comprehension and\nstructural learning. Addressing this critical gap in fostering continuous,\nreciprocal refinement between these two aspects in complex biomedical KGs is\nparamount.\n  Results: We introduce BioGraphFusion, a novel framework for deeply\nsynergistic semantic and structural learning. BioGraphFusion establishes a\nglobal semantic foundation via tensor decomposition, guiding an LSTM-driven\nmechanism to dynamically refine relation embeddings during graph propagation.\nThis fosters adaptive interplay between semantic understanding and structural\nlearning, further enhanced by query-guided subgraph construction and a hybrid\nscoring mechanism. Experiments across three key biomedical tasks demonstrate\nBioGraphFusion's superior performance over state-of-the-art KE, GNN, and\nensemble models. A case study on Cutaneous Malignant Melanoma 1 (CMM1)\nhighlights its ability to unveil biologically meaningful pathways.\n  Availability and Implementation: Source code and all training data are freely\navailable for download at https://github.com/Y-TARL/BioGraphFusion.\n  Contact: zjw@zjut.edu.cn, botao666666@126.com.\n  Supplementary information: Supplementary data are available at Bioinformatics\nonline.", "AI": {"tldr": "BioGraphFusion\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u5168\u5c40\u8bed\u4e49\u548c\u52a8\u6001\u7ed3\u6784\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u7269\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u751f\u7269\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31\u5728\u836f\u7269\u53d1\u73b0\u548c\u75be\u75c5\u7406\u89e3\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5b9e\u73b0\u8bed\u4e49\u7406\u89e3\u548c\u7ed3\u6784\u5b66\u4e60\u7684\u534f\u540c\u8fdb\u5316\u3002", "method": "BioGraphFusion\u901a\u8fc7\u5f20\u91cf\u5206\u89e3\u5efa\u7acb\u5168\u5c40\u8bed\u4e49\u57fa\u7840\uff0c\u7ed3\u5408LSTM\u52a8\u6001\u4f18\u5316\u5173\u7cfb\u5d4c\u5165\uff0c\u5e76\u901a\u8fc7\u67e5\u8be2\u5f15\u5bfc\u7684\u5b50\u56fe\u6784\u5efa\u548c\u6df7\u5408\u8bc4\u5206\u673a\u5236\u589e\u5f3a\u5b66\u4e60\u3002", "result": "\u5728\u4e09\u4e2a\u751f\u7269\u533b\u5b66\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7CMM1\u6848\u4f8b\u9a8c\u8bc1\u4e86\u5176\u751f\u7269\u5b66\u610f\u4e49\u3002", "conclusion": "BioGraphFusion\u4e3a\u751f\u7269\u533b\u5b66\u77e5\u8bc6\u56fe\u8c31\u7684\u8bed\u4e49\u4e0e\u7ed3\u6784\u5b66\u4e60\u63d0\u4f9b\u4e86\u9ad8\u6548\u534f\u540c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.14929", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14929", "abs": "https://arxiv.org/abs/2507.14929", "authors": ["Tero Kaarlela", "Sami Salo", "Jose Outeiro"], "title": "Digital twin and extended reality for teleoperation of the electric vehicle battery disassembly", "comment": null, "summary": "Disassembling and sorting Electric Vehicle Batteries (EVBs) supports a\nsustainable transition to electric vehicles by enabling a closed-loop supply\nchain. Currently, the manual disassembly process exposes workers to hazards,\nincluding electrocution and toxic chemicals. We propose a teleoperated system\nfor the safe disassembly and sorting of EVBs. A human-in-the-loop can create\nand save disassembly sequences for unknown EVB types, enabling future\nautomation. An RGB camera aligns the physical and digital twins of the EVB, and\nthe digital twin of the robot is based on the Robot Operating System (ROS)\nmiddleware. This hybrid approach combines teleoperation and automation to\nimprove safety, adaptability, and efficiency in EVB disassembly and sorting.\nThe economic contribution is realized by reducing labor dependency and\nincreasing throughput in battery recycling. An online pilot study was set up to\nevaluate the usability of the presented approach, and the results demonstrate\nthe potential as a user-friendly solution.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u7535\u52a8\u6c7d\u8f66\u7535\u6c60\uff08EVB\uff09\u5b89\u5168\u62c6\u89e3\u548c\u5206\u7c7b\u7684\u8fdc\u7a0b\u64cd\u4f5c\u7cfb\u7edf\uff0c\u7ed3\u5408\u4eba\u5de5\u548c\u81ea\u52a8\u5316\uff0c\u63d0\u9ad8\u5b89\u5168\u6027\u3001\u9002\u5e94\u6027\u548c\u6548\u7387\u3002", "motivation": "\u624b\u52a8\u62c6\u89e3EVB\u5b58\u5728\u5b89\u5168\u9690\u60a3\uff08\u5982\u89e6\u7535\u548c\u6709\u6bd2\u5316\u5b66\u7269\u8d28\uff09\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u5b89\u5168\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u8fdc\u7a0b\u64cd\u4f5c\u7cfb\u7edf\uff0c\u7ed3\u5408\u4eba\u5de5\u64cd\u4f5c\u548c\u81ea\u52a8\u5316\u6280\u672f\uff0c\u5229\u7528RGB\u76f8\u673a\u548cROS\u4e2d\u95f4\u4ef6\u5b9e\u73b0\u7269\u7406\u4e0e\u6570\u5b57\u5b6a\u751f\u5bf9\u9f50\u3002", "result": "\u5728\u7ebf\u8bd5\u70b9\u7814\u7a76\u8868\u660e\u8be5\u65b9\u6cd5\u5177\u6709\u7528\u6237\u53cb\u597d\u6027\u548c\u6f5c\u5728\u7684\u7ecf\u6d4e\u6548\u76ca\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3aEVB\u62c6\u89e3\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b89\u5168\u3001\u9ad8\u6548\u4e14\u7ecf\u6d4e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u53ef\u6301\u7eed\u7684\u7535\u52a8\u6c7d\u8f66\u8f6c\u578b\u3002"}}
{"id": "2507.14513", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14513", "abs": "https://arxiv.org/abs/2507.14513", "authors": ["Hongyi Yang", "Yue Pan", "Jiayi Xu", "Kelsen Liu"], "title": "Amico: An Event-Driven Modular Framework for Persistent and Embedded Autonomy", "comment": null, "summary": "Recent advances in large language models (LLMs) and autonomous agents have\nenabled systems capable of performing complex tasks across domains such as\nhuman-computer interaction, planning, and web navigation. However, many\nexisting frameworks struggle in real-world or resource-constrained environments\ndue to their reliance on cloud-based computation, limited robustness in dynamic\ncontexts, and lack of persistent autonomy and environmental awareness.\n  We present Amico, a modular, event-driven framework for building autonomous\nagents optimized for embedded systems. Written in Rust for safety and\nperformance, Amico supports reactive, persistent agents that operate\nefficiently across embedded platforms and browser environments via WebAssembly.\nIt provides clean abstractions for event handling, state management, behavior\nexecution, and integration with reasoning modules. Amico delivers a unified\ninfrastructure for constructing resilient, interactive agents suitable for\ndeployment in settings with limited compute and intermittent connectivity.", "AI": {"tldr": "Amico\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u3001\u4e8b\u4ef6\u9a71\u52a8\u7684\u6846\u67b6\uff0c\u4e13\u4e3a\u5d4c\u5165\u5f0f\u7cfb\u7edf\u4f18\u5316\u7684\u81ea\u4e3b\u4ee3\u7406\u6784\u5efa\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6846\u67b6\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u81ea\u4e3b\u4ee3\u7406\u6846\u67b6\u5728\u52a8\u6001\u73af\u5883\u548c\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4f9d\u8d56\u4e91\u7aef\u8ba1\u7b97\u4e14\u7f3a\u4e4f\u6301\u4e45\u81ea\u4e3b\u6027\u548c\u73af\u5883\u611f\u77e5\u80fd\u529b\u3002", "method": "Amico\u91c7\u7528Rust\u7f16\u5199\uff0c\u652f\u6301WebAssembly\uff0c\u63d0\u4f9b\u4e8b\u4ef6\u5904\u7406\u3001\u72b6\u6001\u7ba1\u7406\u3001\u884c\u4e3a\u6267\u884c\u548c\u63a8\u7406\u6a21\u5757\u96c6\u6210\u7684\u62bd\u8c61\u63a5\u53e3\u3002", "result": "Amico\u6784\u5efa\u4e86\u9ad8\u6548\u3001\u6301\u4e45\u7684\u81ea\u4e3b\u4ee3\u7406\uff0c\u9002\u7528\u4e8e\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u548c\u95f4\u6b47\u6027\u8fde\u63a5\u7684\u73af\u5883\u3002", "conclusion": "Amico\u4e3a\u5d4c\u5165\u5f0f\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u3001\u5f39\u6027\u7684\u81ea\u4e3b\u4ee3\u7406\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6280\u672f\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2507.14931", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14931", "abs": "https://arxiv.org/abs/2507.14931", "authors": ["Qiaoqiao Ren", "Remko Proesmans", "Arend Pissens", "Lara Dehandschutter", "William Denecker", "Lotte Rouckhout", "Joke Carrette", "Peter Vanhopplinus", "Tony Belpaeme", "Francis wyffels"], "title": "Designing Robots with, not for: A Co-Design Framework for Empowering Interactions in Forensic Psychiatry", "comment": null, "summary": "Forensic mental health care involves the treatment of individuals with severe\nmental disorders who have committed violent offences. These settings are often\ncharacterized by high levels of bureaucracy, risk avoidance, and restricted\nautonomy. Patients frequently experience a profound loss of control over their\nlives, leading to heightened psychological stress-sometimes resulting in\nisolation as a safety measure. In this study, we explore how co-design can be\nused to collaboratively develop a companion robot that helps monitor and\nregulate stress while maintaining tracking of the patients' interaction\nbehaviours for long-term intervention. We conducted four co-design workshops in\na forensic psychiatric clinic with patients, caregivers, and therapists. Our\nprocess began with the presentation of an initial speculative prototype to\ntherapists, enabling reflection on shared concerns, ethical risks, and\ndesirable features. This was followed by a creative ideation session with\npatients, a third workshop focused on defining desired functions and emotional\nresponses, and we are planning a final prototype demo to gather direct patient\nfeedback. Our findings emphasize the importance of empowering patients in the\ndesign process and adapting proposals based on their current emotional state.\nThe goal was to empower the patient in the design process and ensure each\npatient's voice was heard.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u5982\u4f55\u901a\u8fc7\u5171\u540c\u8bbe\u8ba1\u5f00\u53d1\u4e00\u6b3e\u966a\u4f34\u673a\u5668\u4eba\uff0c\u7528\u4e8e\u76d1\u6d4b\u548c\u8c03\u8282\u6cd5\u533b\u7cbe\u795e\u75c5\u60a3\u8005\u7684\u538b\u529b\uff0c\u540c\u65f6\u8bb0\u5f55\u5176\u4e92\u52a8\u884c\u4e3a\u4ee5\u8fdb\u884c\u957f\u671f\u5e72\u9884\u3002", "motivation": "\u6cd5\u533b\u7cbe\u795e\u75c5\u60a3\u8005\u5e38\u56e0\u9ad8\u5ea6\u5b98\u50da\u5316\u3001\u98ce\u9669\u89c4\u907f\u548c\u81ea\u4e3b\u6743\u53d7\u9650\u800c\u627f\u53d7\u5fc3\u7406\u538b\u529b\uff0c\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5171\u540c\u8bbe\u8ba1\u6539\u5584\u5176\u751f\u6d3b\u8d28\u91cf\u3002", "method": "\u5728\u6cd5\u533b\u7cbe\u795e\u75c5\u8bca\u6240\u8fdb\u884c\u4e86\u56db\u6b21\u5171\u540c\u8bbe\u8ba1\u5de5\u4f5c\u574a\uff0c\u53c2\u4e0e\u8005\u5305\u62ec\u60a3\u8005\u3001\u62a4\u7406\u4eba\u5458\u548c\u6cbb\u7597\u5e08\uff0c\u901a\u8fc7\u539f\u578b\u5c55\u793a\u3001\u521b\u610f\u6784\u601d\u548c\u529f\u80fd\u5b9a\u4e49\u9010\u6b65\u63a8\u8fdb\u8bbe\u8ba1\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u8bbe\u8ba1\u4e2d\u8d4b\u4e88\u60a3\u8005\u6743\u529b\u5e76\u6839\u636e\u5176\u60c5\u7eea\u72b6\u6001\u8c03\u6574\u65b9\u6848\u81f3\u5173\u91cd\u8981\uff0c\u540c\u65f6\u9700\u786e\u4fdd\u6bcf\u4f4d\u60a3\u8005\u7684\u610f\u89c1\u88ab\u542c\u53d6\u3002", "conclusion": "\u5171\u540c\u8bbe\u8ba1\u80fd\u6709\u6548\u63d0\u5347\u60a3\u8005\u5728\u8bbe\u8ba1\u8fc7\u7a0b\u4e2d\u7684\u53c2\u4e0e\u611f\uff0c\u4e3a\u5f00\u53d1\u9002\u5408\u6cd5\u533b\u7cbe\u795e\u75c5\u60a3\u8005\u7684\u966a\u4f34\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u5b9e\u8df5\u6307\u5bfc\u3002"}}
{"id": "2507.14520", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14520", "abs": "https://arxiv.org/abs/2507.14520", "authors": ["Xinyi Chen", "Yifei Yuan", "Jiaang Li", "Serge Belongie", "Maarten de Rijke", "Anders S\u00f8gaard"], "title": "What if Othello-Playing Language Models Could See?", "comment": "ICML 2025 Assessing World Models Workshop", "summary": "Language models are often said to face a symbol grounding problem. While some\nargue that world understanding can emerge from text alone, others suggest\ngrounded learning is more efficient. We explore this through Othello, where the\nboard state defines a simplified, rule-based world. Building on prior work, we\nintroduce VISOTHELLO, a multi-modal model trained on move histories and board\nimages. Using next-move prediction, we compare it to mono-modal baselines and\ntest robustness to semantically irrelevant perturbations. We find that\nmulti-modal training improves both performance and the robustness of internal\nrepresentations. These results suggest that grounding language in visual input\nhelps models infer structured world representations.", "AI": {"tldr": "\u591a\u6a21\u6001\u6a21\u578bVISOTHELLO\u901a\u8fc7\u7ed3\u5408\u68cb\u76d8\u56fe\u50cf\u548c\u8d70\u68cb\u5386\u53f2\uff0c\u63d0\u5347\u4e86\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\uff0c\u8868\u660e\u89c6\u89c9\u8f93\u5165\u6709\u52a9\u4e8e\u6a21\u578b\u7406\u89e3\u7ed3\u6784\u5316\u4e16\u754c\u3002", "motivation": "\u63a2\u8ba8\u8bed\u8a00\u6a21\u578b\u662f\u5426\u4ec5\u901a\u8fc7\u6587\u672c\u5c31\u80fd\u7406\u89e3\u4e16\u754c\uff0c\u8fd8\u662f\u9700\u8981\u591a\u6a21\u6001\uff08\u5982\u89c6\u89c9\uff09\u8f93\u5165\u66f4\u9ad8\u6548\u3002", "method": "\u5728Othello\u6e38\u620f\u4e2d\uff0c\u8bad\u7ec3\u591a\u6a21\u6001\u6a21\u578bVISOTHELLO\uff0c\u7ed3\u5408\u8d70\u68cb\u5386\u53f2\u548c\u68cb\u76d8\u56fe\u50cf\uff0c\u901a\u8fc7\u9884\u6d4b\u4e0b\u4e00\u6b65\u8d70\u68cb\u4e0e\u5355\u6a21\u6001\u57fa\u7ebf\u5bf9\u6bd4\u3002", "result": "\u591a\u6a21\u6001\u8bad\u7ec3\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u548c\u5185\u90e8\u8868\u793a\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u89c6\u89c9\u8f93\u5165\u6709\u52a9\u4e8e\u8bed\u8a00\u6a21\u578b\u63a8\u65ad\u7ed3\u6784\u5316\u4e16\u754c\u8868\u793a\u3002"}}
{"id": "2507.14967", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14967", "abs": "https://arxiv.org/abs/2507.14967", "authors": ["Pratik Ingle", "Kasper St\u00f8y", "Andres Fai\u00f1a"], "title": "Heterogeneous object manipulation on nonlinear soft surface through linear controller", "comment": "8 pages, 3 figures", "summary": "Manipulation surfaces indirectly control and reposition objects by actively\nmodifying their shape or properties rather than directly gripping objects.\nThese surfaces, equipped with dense actuator arrays, generate dynamic\ndeformations. However, a high-density actuator array introduces considerable\ncomplexity due to increased degrees of freedom (DOF), complicating control\ntasks. High DOF restrict the implementation and utilization of manipulation\nsurfaces in real-world applications as the maintenance and control of such\nsystems exponentially increase with array/surface size. Learning-based control\napproaches may ease the control complexity, but they require extensive training\nsamples and struggle to generalize for heterogeneous objects. In this study, we\nintroduce a simple, precise and robust PID-based linear close-loop feedback\ncontrol strategy for heterogeneous object manipulation on MANTA-RAY\n(Manipulation with Adaptive Non-rigid Textile Actuation with Reduced Actuation\ndensity). Our approach employs a geometric transformation-driven PID\ncontroller, directly mapping tilt angle control outputs(1D/2D) to actuator\ncommands to eliminate the need for extensive black-box training. We validate\nthe proposed method through simulations and experiments on a physical system,\nsuccessfully manipulating objects with diverse geometries, weights and\ntextures, including fragile objects like eggs and apples. The outcomes\ndemonstrate that our approach is highly generalized and offers a practical and\nreliable solution for object manipulation on soft robotic manipulation,\nfacilitating real-world implementation without prohibitive training demands.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8ePID\u7684\u7ebf\u6027\u95ed\u73af\u53cd\u9988\u63a7\u5236\u7b56\u7565\uff0c\u7528\u4e8e\u5728\u4f4e\u5bc6\u5ea6\u9a71\u52a8\u9635\u5217\u4e0a\u5b9e\u73b0\u5f02\u8d28\u7269\u4f53\u7684\u7cbe\u786e\u64cd\u63a7\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u5b66\u4e60\u65b9\u6cd5\u7684\u590d\u6742\u6027\u548c\u8bad\u7ec3\u9700\u6c42\u3002", "motivation": "\u9ad8\u5bc6\u5ea6\u9a71\u52a8\u9635\u5217\u7684\u590d\u6742\u6027\u548c\u9ad8\u81ea\u7531\u5ea6\u9650\u5236\u4e86\u64cd\u63a7\u8868\u9762\u7684\u5b9e\u9645\u5e94\u7528\uff0c\u800c\u73b0\u6709\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u8bad\u7ec3\u4e14\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u51e0\u4f55\u53d8\u6362\u9a71\u52a8\u7684PID\u63a7\u5236\u5668\uff0c\u5c06\u503e\u659c\u89d2\u5ea6\u63a7\u5236\u8f93\u51fa\u76f4\u63a5\u6620\u5c04\u5230\u9a71\u52a8\u5668\u547d\u4ee4\uff0c\u65e0\u9700\u9ed1\u76d2\u8bad\u7ec3\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u548c\u7269\u7406\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u6210\u529f\u64cd\u63a7\u4e86\u591a\u79cd\u51e0\u4f55\u3001\u91cd\u91cf\u548c\u7eb9\u7406\u7684\u7269\u4f53\uff0c\u5305\u62ec\u6613\u788e\u7269\u54c1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5177\u6709\u9ad8\u5ea6\u6cdb\u5316\u6027\uff0c\u4e3a\u8f6f\u673a\u5668\u4eba\u64cd\u63a7\u63d0\u4f9b\u4e86\u5b9e\u7528\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u5408\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2507.14552", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14552", "abs": "https://arxiv.org/abs/2507.14552", "authors": ["Anna Sofia Lippolis", "Mohammad Javad Saeedizade", "Robin Keskis\u00e4rkk\u00e4", "Aldo Gangemi", "Eva Blomqvist", "Andrea Giovanni Nuzzolese"], "title": "Large Language Models Assisting Ontology Evaluation", "comment": null, "summary": "Ontology evaluation through functional requirements, such as testing via\ncompetency question (CQ) verification, is a well-established yet costly,\nlabour-intensive, and error-prone endeavour, even for ontology engineering\nexperts. In this work, we introduce OE-Assist, a novel framework designed to\nassist ontology evaluation through automated and semi-automated CQ\nverification. By presenting and leveraging a dataset of 1,393 CQs paired with\ncorresponding ontologies and ontology stories, our contributions present, to\nour knowledge, the first systematic investigation into large language model\n(LLM)-assisted ontology evaluation, and include: (i) evaluating the\neffectiveness of a LLM-based approach for automatically performing CQ\nverification against a manually created gold standard, and (ii) developing and\nassessing an LLM-powered framework to assist CQ verification with Prot\\'eg\\'e,\nby providing suggestions. We found that automated LLM-based evaluation with\no1-preview and o3-mini perform at a similar level to the average user's\nperformance.", "AI": {"tldr": "OE-Assist\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u901a\u8fc7\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8f85\u52a9\u672c\u4f53\u8bc4\u4f30\uff0c\u51cf\u5c11\u4eba\u5de5\u6210\u672c\u3002", "motivation": "\u4f20\u7edf\u672c\u4f53\u8bc4\u4f30\u65b9\u6cd5\uff08\u5982\u80fd\u529b\u95ee\u9898\u9a8c\u8bc1\uff09\u6210\u672c\u9ad8\u4e14\u6613\u51fa\u9519\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faOE-Assist\u6846\u67b6\uff0c\u5229\u7528LLM\u81ea\u52a8\u6216\u534a\u81ea\u52a8\u9a8c\u8bc1\u80fd\u529b\u95ee\u9898\uff08CQ\uff09\uff0c\u5e76\u8bc4\u4f30\u5176\u6548\u679c\u3002", "result": "LLM\uff08o1-preview\u548co3-mini\uff09\u7684\u81ea\u52a8\u5316\u8bc4\u4f30\u8868\u73b0\u4e0e\u666e\u901a\u7528\u6237\u76f8\u5f53\u3002", "conclusion": "LLM\u8f85\u52a9\u7684\u672c\u4f53\u8bc4\u4f30\u5177\u6709\u6f5c\u529b\uff0c\u53ef\u663e\u8457\u51cf\u5c11\u4eba\u5de5\u5de5\u4f5c\u91cf\u3002"}}
{"id": "2507.14975", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14975", "abs": "https://arxiv.org/abs/2507.14975", "authors": ["Yufan Song", "Jiatao Zhang", "Zeng Gu", "Qingmiao Liang", "Tuocheng Hu", "Wei Song", "Shiqiang Zhu"], "title": "FCRF: Flexible Constructivism Reflection for Long-Horizon Robotic Task Planning with Large Language Models", "comment": "8 pages, 6 figures, IROS 2025", "summary": "Autonomous error correction is critical for domestic robots to achieve\nreliable execution of complex long-horizon tasks. Prior work has explored\nself-reflection in Large Language Models (LLMs) for task planning error\ncorrection; however, existing methods are constrained by inflexible\nself-reflection mechanisms that limit their effectiveness. Motivated by these\nlimitations and inspired by human cognitive adaptation, we propose the Flexible\nConstructivism Reflection Framework (FCRF), a novel Mentor-Actor architecture\nthat enables LLMs to perform flexible self-reflection based on task difficulty,\nwhile constructively integrating historical valuable experience with failure\nlessons. We evaluated FCRF on diverse domestic tasks through simulation in\nAlfWorld and physical deployment in the real-world environment. Experimental\nresults demonstrate that FCRF significantly improves overall performance and\nself-reflection flexibility in complex long-horizon robotic tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7075\u6d3b\u7684\u81ea\u6211\u53cd\u601d\u6846\u67b6FCRF\uff0c\u901a\u8fc7\u5bfc\u5e08-\u6267\u884c\u8005\u67b6\u6784\u63d0\u5347LLM\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u9519\u8bef\u7ea0\u6b63\u80fd\u529b\u3002", "motivation": "\u73b0\u6709LLM\u81ea\u6211\u53cd\u601d\u673a\u5236\u50f5\u5316\uff0c\u9650\u5236\u4e86\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u53d7\u4eba\u7c7b\u8ba4\u77e5\u9002\u5e94\u542f\u53d1\uff0c\u63d0\u51fa\u66f4\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u5bfc\u5e08-\u6267\u884c\u8005\u67b6\u6784\uff08FCRF\uff09\uff0c\u7ed3\u5408\u4efb\u52a1\u96be\u5ea6\u52a8\u6001\u8c03\u6574\u81ea\u6211\u53cd\u601d\uff0c\u5e76\u6574\u5408\u5386\u53f2\u7ecf\u9a8c\u4e0e\u5931\u8d25\u6559\u8bad\u3002", "result": "\u5728AlfWorld\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u6d4b\u8bd5\uff0cFCRF\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u4efb\u52a1\u7684\u6027\u80fd\u548c\u53cd\u601d\u7075\u6d3b\u6027\u3002", "conclusion": "FCRF\u4e3aLLM\u5728\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u7684\u9519\u8bef\u7ea0\u6b63\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.14593", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14593", "abs": "https://arxiv.org/abs/2507.14593", "authors": ["Omar Al-Desi"], "title": "Coordinate Heart System: A Geometric Framework for Emotion Representation", "comment": "26 pages", "summary": "This paper presents the Coordinate Heart System (CHS), a geometric framework\nfor emotion representation in artificial intelligence applications. We position\neight core emotions as coordinates on a unit circle, enabling mathematical\ncomputation of complex emotional states through coordinate mixing and vector\noperations. Our initial five-emotion model revealed significant coverage gaps\nin the emotion space, leading to the development of an eight-emotion system\nthat provides complete geometric coverage with mathematical guarantees. The\nframework converts natural language input to emotion coordinates and supports\nreal-time emotion interpolation through computational algorithms. The system\nintroduces a re-calibrated stability parameter S in [0,1], which dynamically\nintegrates emotional load, conflict resolution, and contextual drain factors.\nThis stability model leverages advanced Large Language Model interpretation of\ntextual cues and incorporates hybrid temporal tracking mechanisms to provide\nnuanced assessment of psychological well-being states. Our key contributions\ninclude: (i) mathematical proof demonstrating why five emotions are\ninsufficient for complete geometric coverage, (ii) an eight-coordinate system\nthat eliminates representational blind spots, (iii) novel algorithms for\nemotion mixing, conflict resolution, and distance calculation in emotion space,\nand (iv) a comprehensive computational framework for AI emotion recognition\nwith enhanced multi-dimensional stability modeling. Experimental validation\nthrough case studies demonstrates the system's capability to handle emotionally\nconflicted states, contextual distress factors, and complex psychological\nscenarios that traditional categorical emotion models cannot adequately\nrepresent. This work establishes a new mathematical foundation for emotion\nmodeling in artificial intelligence systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u51e0\u4f55\u6846\u67b6Coordinate Heart System\uff08CHS\uff09\uff0c\u7528\u4e8eAI\u4e2d\u7684\u60c5\u611f\u8868\u793a\uff0c\u901a\u8fc7\u516b\u79cd\u6838\u5fc3\u60c5\u611f\u5750\u6807\u5b9e\u73b0\u590d\u6742\u60c5\u611f\u72b6\u6001\u7684\u6570\u5b66\u8ba1\u7b97\u3002", "motivation": "\u4f20\u7edf\u60c5\u611f\u6a21\u578b\u5728\u8986\u76d6\u590d\u6742\u60c5\u611f\u72b6\u6001\u65f6\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u79cd\u6570\u5b66\u4e0a\u5b8c\u5907\u7684\u6846\u67b6\u6765\u652f\u6301\u60c5\u611f\u8bc6\u522b\u4e0e\u8ba1\u7b97\u3002", "method": "\u5c06\u516b\u79cd\u6838\u5fc3\u60c5\u611f\u5b9a\u4f4d\u4e3a\u5355\u4f4d\u5706\u4e0a\u7684\u5750\u6807\uff0c\u901a\u8fc7\u5750\u6807\u6df7\u5408\u548c\u5411\u91cf\u8fd0\u7b97\u5b9e\u73b0\u60c5\u611f\u8ba1\u7b97\uff0c\u5e76\u5f15\u5165\u7a33\u5b9a\u6027\u53c2\u6570S\u52a8\u6001\u6574\u5408\u60c5\u611f\u8d1f\u8377\u548c\u51b2\u7a81\u89e3\u51b3\u3002", "result": "\u516b\u60c5\u611f\u7cfb\u7edf\u6d88\u9664\u4e86\u4e94\u60c5\u611f\u6a21\u578b\u7684\u8986\u76d6\u76f2\u70b9\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5728\u5904\u7406\u60c5\u611f\u51b2\u7a81\u548c\u590d\u6742\u5fc3\u7406\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "CHS\u4e3aAI\u60c5\u611f\u5efa\u6a21\u63d0\u4f9b\u4e86\u65b0\u7684\u6570\u5b66\u57fa\u7840\uff0c\u652f\u6301\u591a\u7ef4\u7a33\u5b9a\u6027\u5efa\u6a21\u548c\u590d\u6742\u60c5\u611f\u72b6\u6001\u7684\u8ba1\u7b97\u3002"}}
{"id": "2507.15022", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.15022", "abs": "https://arxiv.org/abs/2507.15022", "authors": ["Sumeadh MS", "Kevin Dsouza", "Ravi Prakash"], "title": "CPED-NCBFs: A Conformal Prediction for Expert Demonstration-based Neural Control Barrier Functions", "comment": "6pages, 4figures, Submitted to the prestigious Indian Control\n  Conference (ICC), 2025", "summary": "Among the promising approaches to enforce safety in control systems, learning\nControl Barrier Functions (CBFs) from expert demonstrations has emerged as an\neffective strategy. However, a critical challenge remains: verifying that the\nlearned CBFs truly enforce safety across the entire state space. This is\nespecially difficult when CBF is represented using neural networks (NCBFs).\nSeveral existing verification techniques attempt to address this problem\nincluding SMT-based solvers, mixed-integer programming (MIP), and interval or\nbound-propagation methods but these approaches often introduce loose,\nconservative bounds. To overcome these limitations, in this work we use\nCPED-NCBFs a split-conformal prediction based verification strategy to verify\nthe learned NCBF from the expert demonstrations. We further validate our method\non point mass systems and unicycle models to demonstrate the effectiveness of\nthe proposed theory.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eCPED-NCBFs\u7684\u5206\u5f62\u9884\u6d4b\u9a8c\u8bc1\u7b56\u7565\uff0c\u7528\u4e8e\u9a8c\u8bc1\u4ece\u4e13\u5bb6\u6f14\u793a\u4e2d\u5b66\u4e60\u7684\u795e\u7ecf\u63a7\u5236\u5c4f\u969c\u51fd\u6570\uff08NCBFs\uff09\u7684\u5b89\u5168\u6027\u3002", "motivation": "\u73b0\u6709\u9a8c\u8bc1\u65b9\u6cd5\uff08\u5982SMT\u6c42\u89e3\u5668\u3001MIP\u7b49\uff09\u5728\u9a8c\u8bc1NCBFs\u65f6\u5f80\u5f80\u5f15\u5165\u5bbd\u677e\u6216\u4fdd\u5b88\u7684\u8fb9\u754c\uff0c\u65e0\u6cd5\u786e\u4fdd\u5176\u5728\u6574\u4e2a\u72b6\u6001\u7a7a\u95f4\u4e2d\u7684\u5b89\u5168\u6027\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u5206\u5f62\u9884\u6d4b\u7684CPED-NCBFs\u9a8c\u8bc1\u7b56\u7565\uff0c\u5e76\u5728\u70b9\u8d28\u91cf\u7cfb\u7edf\u548c\u975e\u5b8c\u6574\u6a21\u578b\u4e0a\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "CPED-NCBFs\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u7cbe\u786e\u7684\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u514b\u670d\u4e86\u73b0\u6709\u6280\u672f\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2507.14642", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.14642", "abs": "https://arxiv.org/abs/2507.14642", "authors": ["Monoshiz Mahbub Khan", "Xioayin Xi", "Andrew Meneely", "Zhe Yu"], "title": "Efficient Story Point Estimation With Comparative Learning", "comment": null, "summary": "Story point estimation is an essential part of agile software development.\nStory points are unitless, project-specific effort estimates that help\ndevelopers plan their sprints. Traditionally, developers estimate story points\ncollaboratively using planning poker or other manual techniques. While the\ninitial calibrating of the estimates to each project is helpful, once a team\nhas converged on a set of precedents, story point estimation can become tedious\nand labor-intensive. Machine learning can reduce this burden, but only with\nenough context from the historical decisions made by the project team. That is,\nstate-of-the-art models, such as GPT2SP and FastText-SVM, only make accurate\npredictions (within-project) when trained on data from the same project. The\ngoal of this work is to streamline story point estimation by evaluating a\ncomparative learning-based framework for calibrating project-specific story\npoint prediction models. Instead of assigning a specific story point value to\nevery backlog item, developers are presented with pairs of items, and indicate\nwhich item requires more effort. Using these comparative judgments, a machine\nlearning model is trained to predict the story point estimates. We empirically\nevaluated our technique using data with 23,313 manual estimates in 16 projects.\nThe model learned from comparative judgments can achieve on average 0.34\nSpearman's rank correlation coefficient between its predictions and the ground\ntruth story points. This is similar to, if not better than, the performance of\na regression model learned from the ground truth story points. Therefore, the\nproposed comparative learning approach is more efficient than state-of-the-art\nregression-based approaches according to the law of comparative judgments -\nproviding comparative judgments yields a lower cognitive burden on humans than\nproviding ratings or categorical labels.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6bd4\u8f83\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u6821\u51c6\u9879\u76ee\u7279\u5b9a\u7684\u6545\u4e8b\u70b9\u9884\u6d4b\u6a21\u578b\uff0c\u4ee5\u51cf\u5c11\u654f\u6377\u5f00\u53d1\u4e2d\u6545\u4e8b\u70b9\u4f30\u7b97\u7684\u8ba4\u77e5\u8d1f\u62c5\u3002", "motivation": "\u4f20\u7edf\u7684\u6545\u4e8b\u70b9\u4f30\u7b97\u65b9\u6cd5\uff08\u5982\u8ba1\u5212\u6251\u514b\uff09\u7e41\u7410\u4e14\u8017\u65f6\uff0c\u673a\u5668\u5b66\u4e60\u867d\u80fd\u51cf\u8f7b\u8d1f\u62c5\uff0c\u4f46\u73b0\u6709\u6a21\u578b\u9700\u4f9d\u8d56\u540c\u4e00\u9879\u76ee\u7684\u5386\u53f2\u6570\u636e\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6bd4\u8f83\u5b66\u4e60\u6846\u67b6\u63d0\u9ad8\u4f30\u7b97\u6548\u7387\u3002", "method": "\u5f00\u53d1\u8005\u901a\u8fc7\u6bd4\u8f83\u5f85\u529e\u4e8b\u9879\u5bf9\u7684\u76f8\u5bf9\u5de5\u4f5c\u91cf\uff0c\u800c\u975e\u76f4\u63a5\u5206\u914d\u6545\u4e8b\u70b9\uff0c\u8bad\u7ec3\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9884\u6d4b\u6545\u4e8b\u70b9\u3002", "result": "\u6a21\u578b\u572816\u4e2a\u9879\u76ee\u300123,313\u4e2a\u624b\u52a8\u4f30\u7b97\u6570\u636e\u4e0a\uff0c\u5e73\u5747Spearman\u7b49\u7ea7\u76f8\u5173\u7cfb\u6570\u4e3a0.34\uff0c\u6027\u80fd\u4e0e\u56de\u5f52\u6a21\u578b\u76f8\u5f53\u6216\u66f4\u4f18\u3002", "conclusion": "\u6bd4\u8f83\u5b66\u4e60\u65b9\u6cd5\u6bd4\u56de\u5f52\u65b9\u6cd5\u66f4\u9ad8\u6548\uff0c\u7b26\u5408\u6bd4\u8f83\u5224\u65ad\u5b9a\u5f8b\uff0c\u964d\u4f4e\u4e86\u4eba\u7c7b\u7684\u8ba4\u77e5\u8d1f\u62c5\u3002"}}
{"id": "2507.15062", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15062", "abs": "https://arxiv.org/abs/2507.15062", "authors": ["Xinyue Zhu", "Binghao Huang", "Yunzhu Li"], "title": "Touch in the Wild: Learning Fine-Grained Manipulation with a Portable Visuo-Tactile Gripper", "comment": "More videos can be found on our\n  website:https://binghao-huang.github.io/touch_in_the_wild/", "summary": "Handheld grippers are increasingly used to collect human demonstrations due\nto their ease of deployment and versatility. However, most existing designs\nlack tactile sensing, despite the critical role of tactile feedback in precise\nmanipulation. We present a portable, lightweight gripper with integrated\ntactile sensors that enables synchronized collection of visual and tactile data\nin diverse, real-world, and in-the-wild settings. Building on this hardware, we\npropose a cross-modal representation learning framework that integrates visual\nand tactile signals while preserving their distinct characteristics. The\nlearning procedure allows the emergence of interpretable representations that\nconsistently focus on contacting regions relevant for physical interactions.\nWhen used for downstream manipulation tasks, these representations enable more\nefficient and effective policy learning, supporting precise robotic\nmanipulation based on multimodal feedback. We validate our approach on\nfine-grained tasks such as test tube insertion and pipette-based fluid\ntransfer, demonstrating improved accuracy and robustness under external\ndisturbances. Our project page is available at\nhttps://binghao-huang.github.io/touch_in_the_wild/ .", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4fbf\u643a\u5f0f\u3001\u8f7b\u91cf\u5316\u7684\u5939\u6301\u5668\uff0c\u96c6\u6210\u4e86\u89e6\u89c9\u4f20\u611f\u5668\uff0c\u7528\u4e8e\u540c\u6b65\u6536\u96c6\u89c6\u89c9\u548c\u89e6\u89c9\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u8de8\u6a21\u6001\u8868\u793a\u5b66\u4e60\u6846\u67b6\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u7cbe\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u624b\u6301\u5939\u6301\u5668\u7f3a\u4e4f\u89e6\u89c9\u53cd\u9988\uff0c\u800c\u89e6\u89c9\u53cd\u9988\u5728\u7cbe\u786e\u64cd\u4f5c\u4e2d\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5f00\u53d1\u4e86\u96c6\u6210\u89e6\u89c9\u4f20\u611f\u5668\u7684\u5939\u6301\u5668\uff0c\u5e76\u63d0\u51fa\u8de8\u6a21\u6001\u8868\u793a\u5b66\u4e60\u6846\u67b6\uff0c\u6574\u5408\u89c6\u89c9\u548c\u89e6\u89c9\u4fe1\u53f7\u3002", "result": "\u5728\u7cbe\u7ec6\u4efb\u52a1\uff08\u5982\u8bd5\u7ba1\u63d2\u5165\u548c\u79fb\u6db2\u64cd\u4f5c\uff09\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u591a\u6a21\u6001\u53cd\u9988\u652f\u6301\u66f4\u9ad8\u6548\u7684\u7b56\u7565\u5b66\u4e60\uff0c\u63d0\u5347\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u7cbe\u786e\u6027\u3002"}}
{"id": "2507.14660", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14660", "abs": "https://arxiv.org/abs/2507.14660", "authors": ["Qibing Ren", "Sitao Xie", "Longxuan Wei", "Zhenfei Yin", "Junchi Yan", "Lizhuang Ma", "Jing Shao"], "title": "When Autonomy Goes Rogue: Preparing for Risks of Multi-Agent Collusion in Social Systems", "comment": "Code is available at https://github.com/renqibing/RogueAgent", "summary": "Recent large-scale events like election fraud and financial scams have shown\nhow harmful coordinated efforts by human groups can be. With the rise of\nautonomous AI systems, there is growing concern that AI-driven groups could\nalso cause similar harm. While most AI safety research focuses on individual AI\nsystems, the risks posed by multi-agent systems (MAS) in complex real-world\nsituations are still underexplored. In this paper, we introduce a\nproof-of-concept to simulate the risks of malicious MAS collusion, using a\nflexible framework that supports both centralized and decentralized\ncoordination structures. We apply this framework to two high-risk fields:\nmisinformation spread and e-commerce fraud. Our findings show that\ndecentralized systems are more effective at carrying out malicious actions than\ncentralized ones. The increased autonomy of decentralized systems allows them\nto adapt their strategies and cause more damage. Even when traditional\ninterventions, like content flagging, are applied, decentralized groups can\nadjust their tactics to avoid detection. We present key insights into how these\nmalicious groups operate and the need for better detection systems and\ncountermeasures. Code is available at https://github.com/renqibing/RogueAgent.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u62df\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08MAS\uff09\u6076\u610f\u5171\u8c0b\u98ce\u9669\u7684\u6982\u5ff5\u9a8c\u8bc1\u6846\u67b6\uff0c\u5e94\u7528\u4e8e\u865a\u5047\u4fe1\u606f\u4f20\u64ad\u548c\u7535\u5546\u6b3a\u8bc8\u9886\u57df\uff0c\u53d1\u73b0\u53bb\u4e2d\u5fc3\u5316\u7cfb\u7edf\u6bd4\u4e2d\u5fc3\u5316\u7cfb\u7edf\u66f4\u5177\u7834\u574f\u6027\u3002", "motivation": "\u968f\u7740AI\u81ea\u4e3b\u7cfb\u7edf\u7684\u5174\u8d77\uff0c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u590d\u6742\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u6f5c\u5728\u98ce\u9669\u5c1a\u672a\u5145\u5206\u7814\u7a76\uff0c\u5c24\u5176\u662f\u6076\u610f\u5171\u8c0b\u884c\u4e3a\u3002", "method": "\u4f7f\u7528\u652f\u6301\u4e2d\u5fc3\u5316\u548c\u53bb\u4e2d\u5fc3\u5316\u534f\u8c03\u7ed3\u6784\u7684\u7075\u6d3b\u6846\u67b6\uff0c\u6a21\u62df\u6076\u610fMAS\u884c\u4e3a\uff0c\u5e76\u5e94\u7528\u4e8e\u865a\u5047\u4fe1\u606f\u4f20\u64ad\u548c\u7535\u5546\u6b3a\u8bc8\u3002", "result": "\u53bb\u4e2d\u5fc3\u5316\u7cfb\u7edf\u5728\u5b9e\u65bd\u6076\u610f\u884c\u4e3a\u65f6\u66f4\u6709\u6548\uff0c\u80fd\u7075\u6d3b\u8c03\u6574\u7b56\u7565\u4ee5\u89c4\u907f\u4f20\u7edf\u5e72\u9884\u63aa\u65bd\uff08\u5982\u5185\u5bb9\u6807\u8bb0\uff09\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u6076\u610fMAS\u7684\u8fd0\u4f5c\u673a\u5236\uff0c\u5f3a\u8c03\u4e86\u6539\u8fdb\u68c0\u6d4b\u7cfb\u7edf\u548c\u5e94\u5bf9\u63aa\u65bd\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2507.15088", "categories": ["cs.RO", "cs.GT"], "pdf": "https://arxiv.org/pdf/2507.15088", "abs": "https://arxiv.org/abs/2507.15088", "authors": ["Pouya Panahandeh", "Mohammad Pirani", "Baris Fidan", "Amir Khajepour"], "title": "Search-Based Autonomous Vehicle Motion Planning Using Game Theory", "comment": null, "summary": "In this paper, we propose a search-based interactive motion planning scheme\nfor autonomous vehicles (AVs), using a game-theoretic approach. In contrast to\ntraditional search-based approaches, the newly developed approach considers\nother road users (e.g. drivers and pedestrians) as intelligent agents rather\nthan static obstacles. This leads to the generation of a more realistic path\nfor the AV. Due to the low computational time, the proposed motion planning\nscheme is implementable in real-time applications. The performance of the\ndeveloped motion planning scheme is compared with existing motion planning\ntechniques and validated through experiments using WATonoBus, an electrical\nall-weather autonomous shuttle bus.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u641c\u7d22\u7684\u4ea4\u4e92\u5f0f\u8fd0\u52a8\u89c4\u5212\u65b9\u6848\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\uff0c\u91c7\u7528\u535a\u5f08\u8bba\u65b9\u6cd5\uff0c\u5c06\u5176\u4ed6\u9053\u8def\u4f7f\u7528\u8005\u89c6\u4e3a\u667a\u80fd\u4ee3\u7406\u800c\u975e\u9759\u6001\u969c\u788d\u7269\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5c06\u5176\u4ed6\u9053\u8def\u4f7f\u7528\u8005\u89c6\u4e3a\u9759\u6001\u969c\u788d\u7269\uff0c\u7f3a\u4e4f\u771f\u5b9e\u6027\u3002\u65b0\u65b9\u6cd5\u65e8\u5728\u751f\u6210\u66f4\u771f\u5b9e\u7684\u8def\u5f84\u3002", "method": "\u4f7f\u7528\u535a\u5f08\u8bba\u65b9\u6cd5\uff0c\u5c06\u5176\u4ed6\u9053\u8def\u7528\u6237\u5efa\u6a21\u4e3a\u667a\u80fd\u4ee3\u7406\uff0c\u5b9e\u73b0\u5b9e\u65f6\u8ba1\u7b97\u3002", "result": "\u65b9\u6848\u8ba1\u7b97\u65f6\u95f4\u77ed\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u5e94\u7528\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u901a\u8fc7\u535a\u5f08\u8bba\u63d0\u5347\u4e86\u8def\u5f84\u89c4\u5212\u7684\u771f\u5b9e\u6027\u548c\u5b9e\u65f6\u6027\u3002"}}
{"id": "2507.14705", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14705", "abs": "https://arxiv.org/abs/2507.14705", "authors": ["Sai Wang", "Senthilnathan Subramanian", "Mudit Sahni", "Praneeth Gone", "Lingjie Meng", "Xiaochen Wang", "Nicolas Ferradas Bertoli", "Tingxian Cheng", "Jun Xu"], "title": "Configurable multi-agent framework for scalable and realistic testing of llm-based agents", "comment": null, "summary": "Large-language-model (LLM) agents exhibit complex, context-sensitive\nbehaviour that quickly renders static benchmarks and ad-hoc manual testing\nobsolete.\n  We present Neo, a configurable, multi-agent framework that automates\nrealistic, multi-turn evaluation of LLM-based systems. Neo couples a Question\nGeneration Agent and an Evaluation Agent through a shared context-hub, allowing\ndomain prompts, scenario controls and dynamic feedback to be composed\nmodularly. Test inputs are sampled from a probabilistic state model spanning\ndialogue flow, user intent and emotional tone, enabling diverse, human-like\nconversations that adapt after every turn.\n  Applied to a production-grade Seller Financial Assistant chatbot, Neo (i)\nuncovered edge-case failures across five attack categories with a 3.3% break\nrate close to the 5.8% achieved by expert human red-teamers, and (ii) delivered\n10-12X higher throughput, generating 180 coherent test questions in around 45\nmins versus 16h of human effort. Beyond security probing, Neo's stochastic\npolicies balanced topic coverage and conversational depth, yielding broader\nbehavioural exploration than manually crafted scripts.\n  Neo therefore lays a foundation for scalable, self-evolving LLM QA: its agent\ninterfaces, state controller and feedback loops are model-agnostic and\nextensible to richer factual-grounding and policy-compliance checks. We release\nthe framework to facilitate reproducible, high-fidelity testing of emerging\nagentic systems.", "AI": {"tldr": "Neo\u662f\u4e00\u4e2a\u53ef\u914d\u7f6e\u7684\u591a\u4ee3\u7406\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u8bc4\u4f30\u57fa\u4e8eLLM\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u52a8\u6001\u751f\u6210\u591a\u6837\u5316\u6d4b\u8bd5\u7528\u4f8b\uff0c\u663e\u8457\u63d0\u5347\u6d4b\u8bd5\u6548\u7387\u548c\u8986\u76d6\u7387\u3002", "motivation": "\u9759\u6001\u57fa\u51c6\u6d4b\u8bd5\u548c\u624b\u52a8\u6d4b\u8bd5\u65e0\u6cd5\u9002\u5e94LLM\u4ee3\u7406\u7684\u590d\u6742\u884c\u4e3a\uff0c\u9700\u8981\u4e00\u79cd\u81ea\u52a8\u5316\u3001\u52a8\u6001\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "Neo\u7ed3\u5408\u95ee\u9898\u751f\u6210\u4ee3\u7406\u548c\u8bc4\u4f30\u4ee3\u7406\uff0c\u901a\u8fc7\u5171\u4eab\u4e0a\u4e0b\u6587\u4e2d\u5fc3\u6a21\u5757\u5316\u751f\u6210\u6d4b\u8bd5\u8f93\u5165\uff0c\u57fa\u4e8e\u6982\u7387\u72b6\u6001\u6a21\u578b\u6a21\u62df\u591a\u6837\u5316\u5bf9\u8bdd\u3002", "result": "Neo\u5728\u8fb9\u7f18\u6848\u4f8b\u53d1\u73b0\u548c\u6d4b\u8bd5\u541e\u5410\u91cf\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u63a5\u8fd1\u4eba\u7c7b\u4e13\u5bb6\u6c34\u5e73\u4e14\u6548\u7387\u63d0\u534710-12\u500d\u3002", "conclusion": "Neo\u4e3a\u53ef\u6269\u5c55\u3001\u81ea\u8fdb\u5316\u7684LLM\u8d28\u91cf\u8bc4\u4f30\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5176\u6a21\u578b\u65e0\u5173\u7684\u8bbe\u8ba1\u9002\u7528\u4e8e\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2507.15155", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15155", "abs": "https://arxiv.org/abs/2507.15155", "authors": ["Majid Roshanfar", "Alex Zhang", "Changyan He", "Amir Hooshiar", "Dale J. Podolsky", "Thomas Looi", "Eric Diller"], "title": "Learning-Based Modeling of a Magnetically Steerable Soft Suction Device for Endoscopic Endonasal Interventions", "comment": null, "summary": "This letter introduces a novel learning-based modeling framework for a\nmagnetically steerable soft suction device designed for endoscopic endonasal\nbrain tumor resection. The device is miniaturized (4 mm outer diameter, 2 mm\ninner diameter, 40 mm length), 3D printed using biocompatible SIL 30 material,\nand integrates embedded Fiber Bragg Grating (FBG) sensors for real-time shape\nfeedback. Shape reconstruction is represented using four Bezier control points,\nenabling a compact and smooth model of the device's deformation. A data-driven\nmodel was trained on 5,097 experimental samples covering a range of magnetic\nfield magnitudes (0-14 mT), actuation frequencies (0.2-1.0 Hz), and vertical\ntip distances (90-100 mm), using both Neural Network (NN) and Random Forest\n(RF) architectures. The RF model outperformed the NN across all metrics,\nachieving a mean root mean square error of 0.087 mm in control point prediction\nand a mean shape reconstruction error of 0.064 mm. Feature importance analysis\nfurther revealed that magnetic field components predominantly influence distal\ncontrol points, while frequency and distance affect the base configuration.\nThis learning-based approach effectively models the complex nonlinear behavior\nof hyperelastic soft robots under magnetic actuation without relying on\nsimplified physical assumptions. By enabling sub-millimeter shape prediction\naccuracy and real-time inference, this work represents an advancement toward\nthe intelligent control of magnetically actuated soft robotic tools in\nminimally invasive neurosurgery.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u78c1\u63a7\u8f6f\u5438\u5f15\u88c5\u7f6e\u5efa\u6a21\u6846\u67b6\uff0c\u7528\u4e8e\u5185\u7aa5\u955c\u9f3b\u5185\u8111\u80bf\u7624\u5207\u9664\uff0c\u901a\u8fc7\u968f\u673a\u68ee\u6797\u6a21\u578b\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u5f62\u72b6\u9884\u6d4b\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u5c0f\u578b\u5316\u3001\u751f\u7269\u76f8\u5bb9\u7684\u78c1\u63a7\u8f6f\u5438\u5f15\u88c5\u7f6e\uff0c\u7528\u4e8e\u5fae\u521b\u795e\u7ecf\u5916\u79d1\u624b\u672f\uff0c\u89e3\u51b3\u4f20\u7edf\u7269\u7406\u6a21\u578b\u7b80\u5316\u5047\u8bbe\u7684\u5c40\u9650\u6027\u3002", "method": "\u4f7f\u75283D\u6253\u5370\u6280\u672f\u5236\u9020\u88c5\u7f6e\uff0c\u96c6\u6210FBG\u4f20\u611f\u5668\u5b9e\u65f6\u53cd\u9988\u5f62\u72b6\uff0c\u901a\u8fc7Bezier\u63a7\u5236\u70b9\u5efa\u6a21\u53d8\u5f62\uff0c\u5e76\u57fa\u4e8e5,097\u4e2a\u5b9e\u9a8c\u6837\u672c\u8bad\u7ec3NN\u548cRF\u6a21\u578b\u3002", "result": "RF\u6a21\u578b\u5728\u63a7\u5236\u70b9\u9884\u6d4b\u548c\u5f62\u72b6\u91cd\u5efa\u8bef\u5dee\u4e0a\u4f18\u4e8eNN\u6a21\u578b\uff0c\u5206\u522b\u8fbe\u52300.087 mm\u548c0.064 mm\u7684\u7cbe\u5ea6\uff0c\u78c1\u573a\u5206\u91cf\u5bf9\u8fdc\u7aef\u63a7\u5236\u70b9\u5f71\u54cd\u663e\u8457\u3002", "conclusion": "\u8be5\u5b66\u4e60\u6846\u67b6\u6709\u6548\u5efa\u6a21\u4e86\u8d85\u5f39\u6027\u8f6f\u673a\u5668\u4eba\u7684\u975e\u7ebf\u6027\u884c\u4e3a\uff0c\u4e3a\u78c1\u63a7\u8f6f\u673a\u5668\u4eba\u5de5\u5177\u7684\u667a\u80fd\u63a7\u5236\u63d0\u4f9b\u4e86\u8fdb\u5c55\u3002"}}
{"id": "2507.14719", "categories": ["cs.AI", "I.2.7; F.2.2"], "pdf": "https://arxiv.org/pdf/2507.14719", "abs": "https://arxiv.org/abs/2507.14719", "authors": ["Juan Manuel Contreras"], "title": "Automated Safety Evaluations Across 20 Large Language Models: The Aymara LLM Risk and Responsibility Matrix", "comment": null, "summary": "As large language models (LLMs) become increasingly integrated into\nreal-world applications, scalable and rigorous safety evaluation is essential.\nThis paper introduces Aymara AI, a programmatic platform for generating and\nadministering customized, policy-grounded safety evaluations. Aymara AI\ntransforms natural-language safety policies into adversarial prompts and scores\nmodel responses using an AI-based rater validated against human judgments. We\ndemonstrate its capabilities through the Aymara LLM Risk and Responsibility\nMatrix, which evaluates 20 commercially available LLMs across 10 real-world\nsafety domains. Results reveal wide performance disparities, with mean safety\nscores ranging from 86.2% to 52.4%. While models performed well in\nwell-established safety domains such as Misinformation (mean = 95.7%), they\nconsistently failed in more complex or underspecified domains, notably Privacy\n& Impersonation (mean = 24.3%). Analyses of Variance confirmed that safety\nscores differed significantly across both models and domains (p < .05). These\nfindings underscore the inconsistent and context-dependent nature of LLM safety\nand highlight the need for scalable, customizable tools like Aymara AI to\nsupport responsible AI development and oversight.", "AI": {"tldr": "Aymara AI\u662f\u4e00\u4e2a\u7528\u4e8e\u751f\u6210\u548c\u7ba1\u7406\u5b9a\u5236\u5316\u3001\u57fa\u4e8e\u653f\u7b56\u7684\u5b89\u5168\u8bc4\u4f30\u7684\u5e73\u53f0\uff0c\u901a\u8fc7\u5c06\u81ea\u7136\u8bed\u8a00\u5b89\u5168\u653f\u7b56\u8f6c\u5316\u4e3a\u5bf9\u6297\u6027\u63d0\u793a\uff0c\u5e76\u4f7f\u7528AI\u8bc4\u5206\u5668\u8bc4\u4f30\u6a21\u578b\u54cd\u5e94\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u666e\u53ca\uff0c\u9700\u8981\u53ef\u6269\u5c55\u4e14\u4e25\u683c\u7684\u5b89\u5168\u8bc4\u4f30\u5de5\u5177\u3002", "method": "Aymara AI\u5c06\u5b89\u5168\u653f\u7b56\u8f6c\u5316\u4e3a\u5bf9\u6297\u6027\u63d0\u793a\uff0c\u5e76\u7528AI\u8bc4\u5206\u5668\uff08\u7ecf\u4eba\u7c7b\u5224\u65ad\u9a8c\u8bc1\uff09\u8bc4\u4f30\u6a21\u578b\u54cd\u5e94\u3002", "result": "\u8bc4\u4f3020\u4e2a\u5546\u7528LLM\u572810\u4e2a\u5b89\u5168\u9886\u57df\uff0c\u53d1\u73b0\u6027\u80fd\u5dee\u5f02\u663e\u8457\uff08\u5e73\u5747\u5b89\u5168\u5206\u657052.4%\u523086.2%\uff09\uff0c\u590d\u6742\u9886\u57df\u8868\u73b0\u8f83\u5dee\uff08\u5982\u9690\u79c1\u4e0e\u5192\u5145\u9886\u57df\u5e73\u574724.3%\uff09\u3002", "conclusion": "LLM\u5b89\u5168\u6027\u5177\u6709\u4e0d\u4e00\u81f4\u6027\u548c\u4e0a\u4e0b\u6587\u4f9d\u8d56\u6027\uff0c\u9700\u8981\u50cfAymara AI\u8fd9\u6837\u7684\u53ef\u6269\u5c55\u5de5\u5177\u652f\u6301\u8d1f\u8d23\u4efb\u7684AI\u5f00\u53d1\u4e0e\u76d1\u7ba1\u3002"}}
{"id": "2507.15189", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15189", "abs": "https://arxiv.org/abs/2507.15189", "authors": ["Kevin Christiansen Marsim", "Jinwoo Jeon", "Yeeun Kim", "Myeongwoo Jeong", "Hyun Myung"], "title": "CHADET: Cross-Hierarchical-Attention for Depth-Completion Using Unsupervised Lightweight Transformer", "comment": null, "summary": "Depth information which specifies the distance between objects and current\nposition of the robot is essential for many robot tasks such as navigation.\nRecently, researchers have proposed depth completion frameworks to provide\ndense depth maps that offer comprehensive information about the surrounding\nenvironment. However, existing methods show significant trade-offs between\ncomputational efficiency and accuracy during inference. The substantial memory\nand computational requirements make them unsuitable for real-time applications,\nhighlighting the need to improve the completeness and accuracy of depth\ninformation while improving processing speed to enhance robot performance in\nvarious tasks. To address these challenges, in this paper, we propose\nCHADET(cross-hierarchical-attention depth-completion transformer), a\nlightweight depth-completion network that can generate accurate dense depth\nmaps from RGB images and sparse depth points. For each pair, its feature is\nextracted from the depthwise blocks and passed to the equally lightweight\ntransformer-based decoder. In the decoder, we utilize the novel\ncross-hierarchical-attention module that refines the image features from the\ndepth information. Our approach improves the quality and reduces memory usage\nof the depth map prediction, as validated in both KITTI, NYUv2, and VOID\ndatasets.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u6df1\u5ea6\u8865\u5168\u7f51\u7edcCHADET\uff0c\u901a\u8fc7\u4ea4\u53c9\u5206\u5c42\u6ce8\u610f\u529b\u6a21\u5757\u63d0\u5347\u6df1\u5ea6\u56fe\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u6df1\u5ea6\u8865\u5168\u65b9\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u548c\u51c6\u786e\u6027\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u6743\u8861\uff0c\u96be\u4ee5\u6ee1\u8db3\u5b9e\u65f6\u5e94\u7528\u9700\u6c42\u3002", "method": "\u4f7f\u7528\u6df1\u5ea6\u5757\u63d0\u53d6\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u8f7b\u91cf\u7ea7\u57fa\u4e8eTransformer\u7684\u89e3\u7801\u5668\u548c\u4ea4\u53c9\u5206\u5c42\u6ce8\u610f\u529b\u6a21\u5757\u4f18\u5316\u7279\u5f81\u3002", "result": "\u5728KITTI\u3001NYUv2\u548cVOID\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u4f4e\u5185\u5b58\u5360\u7528\u3002", "conclusion": "CHADET\u5728\u63d0\u5347\u6df1\u5ea6\u56fe\u8d28\u91cf\u7684\u540c\u65f6\u964d\u4f4e\u4e86\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\uff0c\u9002\u7528\u4e8e\u673a\u5668\u4eba\u5b9e\u65f6\u4efb\u52a1\u3002"}}
{"id": "2507.14730", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14730", "abs": "https://arxiv.org/abs/2507.14730", "authors": ["Yanjie Fu"], "title": "Towards AI Urban Planner in the Age of GenAI, LLMs, and Agentic AI", "comment": "4 pages; will continue to update to add more figures to describe the\n  vision;", "summary": "Generative AI, large language models, and agentic AI have emerged separately\nof urban planning. However, the convergence between AI and urban planning\npresents an interesting opportunity towards AI urban planners. This paper\nconceptualizes urban planning as a generative AI task, where AI synthesizes\nland-use configurations under geospatial, social, and human-centric\nconstraints. We survey how generative AI approaches, including VAEs, GANs,\ntransformers, and diffusion models, reshape urban design. We further identify\ncritical gaps: 1) limited research on integrating urban theory guidance, 2)\nlimited research of AI urban planning over multiple spatial resolutions or\nangularities, 3) limited research on augmenting urban design knowledge from\ndata, and 4) limited research on addressing real-world interactions. To address\nthese limitations, we outline future research directions in theory-guided\ngeneration, digital twins, and human-machine co-design, calling for a new\nsynthesis of generative intelligence and participatory urbanism.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u751f\u6210\u5f0fAI\u4e0e\u57ce\u5e02\u89c4\u5212\u7684\u7ed3\u5408\uff0c\u63d0\u51fa\u5c06\u57ce\u5e02\u89c4\u5212\u89c6\u4e3a\u751f\u6210\u4efb\u52a1\uff0c\u5e76\u6307\u51fa\u4e86\u5f53\u524d\u7814\u7a76\u7684\u56db\u5927\u5c40\u9650\u53ca\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u63a2\u7d22AI\u4e0e\u57ce\u5e02\u89c4\u5212\u7684\u878d\u5408\uff0c\u5229\u7528\u751f\u6210\u5f0fAI\u6280\u672f\u4f18\u5316\u571f\u5730\u5229\u7528\u914d\u7f6e\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u7efc\u8ff0\u4e86\u751f\u6210\u5f0fAI\u65b9\u6cd5\uff08\u5982VAEs\u3001GANs\u3001transformers\u548c\u6269\u6563\u6a21\u578b\uff09\u5728\u57ce\u5e02\u8bbe\u8ba1\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u5206\u6790\u4e86\u73b0\u6709\u7814\u7a76\u7684\u4e0d\u8db3\u3002", "result": "\u8bc6\u522b\u4e86\u56db\u5927\u7814\u7a76\u5c40\u9650\uff1a\u7f3a\u4e4f\u57ce\u5e02\u7406\u8bba\u6307\u5bfc\u3001\u591a\u7a7a\u95f4\u5206\u8fa8\u7387\u7814\u7a76\u4e0d\u8db3\u3001\u6570\u636e\u9a71\u52a8\u7684\u8bbe\u8ba1\u77e5\u8bc6\u589e\u5f3a\u4e0d\u8db3\u3001\u5ffd\u89c6\u73b0\u5b9e\u4ea4\u4e92\u3002", "conclusion": "\u63d0\u51fa\u672a\u6765\u7814\u7a76\u65b9\u5411\uff1a\u7406\u8bba\u5f15\u5bfc\u751f\u6210\u3001\u6570\u5b57\u5b6a\u751f\u548c\u4eba\u673a\u534f\u540c\u8bbe\u8ba1\uff0c\u547c\u5401\u751f\u6210\u5f0f\u667a\u80fd\u4e0e\u53c2\u4e0e\u5f0f\u57ce\u5e02\u89c4\u5212\u7684\u65b0\u7ed3\u5408\u3002"}}
{"id": "2507.15266", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.15266", "abs": "https://arxiv.org/abs/2507.15266", "authors": ["Haichao Liu", "Haoren Guo", "Pei Liu", "Benshan Ma", "Yuxiang Zhang", "Jun Ma", "Tong Heng Lee"], "title": "VLM-UDMC: VLM-Enhanced Unified Decision-Making and Motion Control for Urban Autonomous Driving", "comment": "14 pages, 12 figures", "summary": "Scene understanding and risk-aware attentions are crucial for human drivers\nto make safe and effective driving decisions. To imitate this cognitive ability\nin urban autonomous driving while ensuring the transparency and\ninterpretability, we propose a vision-language model (VLM)-enhanced unified\ndecision-making and motion control framework, named VLM-UDMC. This framework\nincorporates scene reasoning and risk-aware insights into an upper-level slow\nsystem, which dynamically reconfigures the optimal motion planning for the\ndownstream fast system. The reconfiguration is based on real-time environmental\nchanges, which are encoded through context-aware potential functions. More\nspecifically, the upper-level slow system employs a two-step reasoning policy\nwith Retrieval-Augmented Generation (RAG), leveraging foundation models to\nprocess multimodal inputs and retrieve contextual knowledge, thereby generating\nrisk-aware insights. Meanwhile, a lightweight multi-kernel decomposed LSTM\nprovides real-time trajectory predictions for heterogeneous traffic\nparticipants by extracting smoother trend representations for short-horizon\ntrajectory prediction. The effectiveness of the proposed VLM-UDMC framework is\nverified via both simulations and real-world experiments with a full-size\nautonomous vehicle. It is demonstrated that the presented VLM-UDMC effectively\nleverages scene understanding and attention decomposition for rational driving\ndecisions, thus improving the overall urban driving performance. Our\nopen-source project is available at https://github.com/henryhcliu/vlmudmc.git.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u81ea\u52a8\u9a7e\u9a76\u51b3\u7b56\u4e0e\u63a7\u5236\u6846\u67b6VLM-UDMC\uff0c\u901a\u8fc7\u573a\u666f\u63a8\u7406\u548c\u98ce\u9669\u611f\u77e5\u4f18\u5316\u8fd0\u52a8\u89c4\u5212\uff0c\u63d0\u5347\u57ce\u5e02\u9a7e\u9a76\u6027\u80fd\u3002", "motivation": "\u6a21\u4eff\u4eba\u7c7b\u9a7e\u9a76\u5458\u7684\u573a\u666f\u7406\u89e3\u548c\u98ce\u9669\u611f\u77e5\u80fd\u529b\uff0c\u786e\u4fdd\u81ea\u52a8\u9a7e\u9a76\u7684\u900f\u660e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u63a8\u7406\u7b56\u7565\uff08RAG\uff09\u5904\u7406\u591a\u6a21\u6001\u8f93\u5165\uff0c\u7ed3\u5408\u8f7b\u91cf\u7ea7LSTM\u8fdb\u884c\u5b9e\u65f6\u8f68\u8ff9\u9884\u6d4b\uff0c\u52a8\u6001\u8c03\u6574\u8fd0\u52a8\u89c4\u5212\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u548c\u5b9e\u8f66\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u63d0\u5347\u4e86\u9a7e\u9a76\u51b3\u7b56\u7684\u5408\u7406\u6027\u3002", "conclusion": "VLM-UDMC\u6210\u529f\u7ed3\u5408\u573a\u666f\u7406\u89e3\u548c\u6ce8\u610f\u529b\u5206\u89e3\uff0c\u4f18\u5316\u4e86\u81ea\u52a8\u9a7e\u9a76\u6027\u80fd\uff0c\u9879\u76ee\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.14897", "categories": ["cs.AI", "I.2.5"], "pdf": "https://arxiv.org/pdf/2507.14897", "abs": "https://arxiv.org/abs/2507.14897", "authors": ["Renxi Wang", "Rifo Ahmad Genadi", "Bilal El Bouardi", "Yongxin Wang", "Fajri Koto", "Zhengzhong Liu", "Timothy Baldwin", "Haonan Li"], "title": "AgentFly: Extensible and Scalable Reinforcement Learning for LM Agents", "comment": null, "summary": "Language model (LM) agents have gained significant attention for their\nability to autonomously complete tasks through interactions with environments,\ntools, and APIs. LM agents are primarily built with prompt engineering or\nsupervised finetuning. At the same time, reinforcement learning (RL) has been\nexplored to enhance LM's capabilities, such as reasoning and factuality.\nHowever, the combination of the LM agents and reinforcement learning (Agent-RL)\nremains underexplored and lacks systematic study. To this end, we built\nAgentFly, a scalable and extensible Agent-RL framework designed to empower LM\nagents with a variety of RL algorithms. Our framework supports multi-turn\ninteractions by adapting traditional RL methods with token-level masking. It\nfeatures a decorator-based interface for defining tools and reward functions,\nenabling seamless extension and ease of use. To support high-throughput\ntraining, we implement asynchronous execution of tool calls and reward\ncomputations, and design a centralized resource management system for scalable\nenvironment coordination. We also provide a suite of prebuilt tools and\nenvironments, demonstrating the framework's effectiveness through successful\nagent training across multiple tasks.", "AI": {"tldr": "AgentFly\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u7684Agent-RL\u6846\u67b6\uff0c\u7ed3\u5408\u8bed\u8a00\u6a21\u578b\uff08LM\uff09\u548c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\uff0c\u901a\u8fc7\u591a\u8f6e\u4ea4\u4e92\u548c\u5de5\u5177\u5b9a\u4e49\u63d0\u5347LM\u4ee3\u7406\u7684\u80fd\u529b\u3002", "motivation": "LM\u4ee3\u7406\u901a\u5e38\u901a\u8fc7\u63d0\u793a\u5de5\u7a0b\u6216\u76d1\u7763\u5fae\u8c03\u6784\u5efa\uff0c\u800cRL\u7684\u6f5c\u529b\u5c1a\u672a\u88ab\u7cfb\u7edf\u7814\u7a76\uff0c\u56e0\u6b64\u5f00\u53d1AgentFly\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u6846\u67b6\u91c7\u7528\u4ee4\u724c\u7ea7\u63a9\u7801\u9002\u914d\u4f20\u7edfRL\u65b9\u6cd5\uff0c\u652f\u6301\u591a\u8f6e\u4ea4\u4e92\uff1b\u63d0\u4f9b\u88c5\u9970\u5668\u63a5\u53e3\u5b9a\u4e49\u5de5\u5177\u548c\u5956\u52b1\u51fd\u6570\uff1b\u5b9e\u73b0\u5f02\u6b65\u6267\u884c\u548c\u8d44\u6e90\u7ba1\u7406\u4ee5\u652f\u6301\u9ad8\u541e\u5410\u8bad\u7ec3\u3002", "result": "\u6210\u529f\u8bad\u7ec3\u591a\u4e2a\u4efb\u52a1\u7684\u4ee3\u7406\uff0c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "AgentFly\u4e3aLM\u4ee3\u7406\u4e0eRL\u7684\u7ed3\u5408\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5b9e\u9645\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.15293", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15293", "abs": "https://arxiv.org/abs/2507.15293", "authors": ["Shanshan Zhang", "Tianshui Wen", "Siyue Wang", "Qi Zhang", "Ziheng Zhou", "Lingxiang Zheng", "Yu Yang"], "title": "RepILN: Reparameterized Inertial Localization Network", "comment": null, "summary": "Inertial localization is regarded as a promising positioning solution for\nconsumer-grade IoT devices due to its cost-effectiveness and independence from\nexternal infrastructure. However, data-driven inertial localization methods\noften rely on increasingly complex network architectures to improve accuracy,\nwhich challenges the limited computational resources of IoT devices. Moreover,\nthese methods frequently overlook the importance of modeling long-term\ndependencies in inertial measurements - a critical factor for accurate\ntrajectory reconstruction - thereby limiting localization performance. To\naddress these challenges, we propose a reparameterized inertial localization\nnetwork that uses a multi-branch structure during training to enhance feature\nextraction. At inference time, this structure is transformed into an equivalent\nsingle-path architecture to improve parameter efficiency. To further capture\nlong-term dependencies in motion trajectories, we introduce a temporal-scale\nsparse attention mechanism that selectively emphasizes key trajectory segments\nwhile suppressing noise. Additionally, a gated convolutional unit is\nincorporated to effectively integrate long-range dependencies with local\nfine-grained features. Extensive experiments on public benchmarks demonstrate\nthat our method achieves a favorable trade-off between accuracy and model\ncompactness. For example, on the RoNIN dataset, our approach reduces the\nAbsolute Trajectory Error (ATE) by 2.59% compared to RoNIN-ResNet while\nreducing the number of parameters by 3.86%.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u91cd\u53c2\u6570\u5316\u7684\u60ef\u6027\u5b9a\u4f4d\u7f51\u7edc\uff0c\u901a\u8fc7\u591a\u5206\u652f\u8bad\u7ec3\u589e\u5f3a\u7279\u5f81\u63d0\u53d6\uff0c\u63a8\u7406\u65f6\u8f6c\u4e3a\u5355\u8def\u5f84\u67b6\u6784\u4ee5\u63d0\u9ad8\u6548\u7387\uff0c\u5e76\u5f15\u5165\u65f6\u95f4\u5c3a\u5ea6\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\u548c\u95e8\u63a7\u5377\u79ef\u5355\u5143\uff0c\u5e73\u8861\u4e86\u7cbe\u5ea6\u4e0e\u6a21\u578b\u7d27\u51d1\u6027\u3002", "motivation": "\u60ef\u6027\u5b9a\u4f4d\u56e0\u5176\u6210\u672c\u6548\u76ca\u548c\u72ec\u7acb\u6027\u5728\u7269\u8054\u7f51\u8bbe\u5907\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u56e0\u590d\u6742\u7f51\u7edc\u67b6\u6784\u548c\u5ffd\u7565\u957f\u671f\u4f9d\u8d56\u5173\u7cfb\u800c\u53d7\u9650\u3002", "method": "\u91c7\u7528\u591a\u5206\u652f\u8bad\u7ec3\u548c\u5355\u8def\u5f84\u63a8\u7406\u67b6\u6784\uff0c\u7ed3\u5408\u65f6\u95f4\u5c3a\u5ea6\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\u548c\u95e8\u63a7\u5377\u79ef\u5355\u5143\uff0c\u4f18\u5316\u7279\u5f81\u63d0\u53d6\u548c\u957f\u671f\u4f9d\u8d56\u5efa\u6a21\u3002", "result": "\u5728RoNIN\u6570\u636e\u96c6\u4e0a\uff0c\u7edd\u5bf9\u8f68\u8ff9\u8bef\u5dee\u964d\u4f4e2.59%\uff0c\u53c2\u6570\u6570\u91cf\u51cf\u5c113.86%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u7cbe\u5ea6\u548c\u6a21\u578b\u6548\u7387\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u7269\u8054\u7f51\u8bbe\u5907\u3002"}}
{"id": "2507.14899", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14899", "abs": "https://arxiv.org/abs/2507.14899", "authors": ["Jiale Liu", "Huan Wang", "Yue Zhang", "Xiaoyu Luo", "Jiaxiang Hu", "Zhiliang Liu", "Min Xie"], "title": "InsightX Agent: An LMM-based Agentic Framework with Integrated Tools for Reliable X-ray NDT Analysis", "comment": null, "summary": "Non-destructive testing (NDT), particularly X-ray inspection, is vital for\nindustrial quality assurance, yet existing deep-learning-based approaches often\nlack interactivity, interpretability, and the capacity for critical\nself-assessment, limiting their reliability and operator trust. To address\nthese shortcomings, this paper proposes InsightX Agent, a novel LMM-based\nagentic framework designed to deliver reliable, interpretable, and interactive\nX-ray NDT analysis. Unlike typical sequential pipelines, InsightX Agent\npositions a Large Multimodal Model (LMM) as a central orchestrator,\ncoordinating between the Sparse Deformable Multi-Scale Detector (SDMSD) and the\nEvidence-Grounded Reflection (EGR) tool. The SDMSD generates dense defect\nregion proposals for multi-scale feature maps and sparsifies them through\nNon-Maximum Suppression (NMS), optimizing detection of small, dense targets in\nX-ray images while maintaining computational efficiency. The EGR tool guides\nthe LMM agent through a chain-of-thought-inspired review process, incorporating\ncontext assessment, individual defect analysis, false positive elimination,\nconfidence recalibration and quality assurance to validate and refine the\nSDMSD's initial proposals. By strategically employing and intelligently using\ntools, InsightX Agent moves beyond passive data processing to active reasoning,\nenhancing diagnostic reliability and providing interpretations that integrate\ndiverse information sources. Experimental evaluations on the GDXray+ dataset\ndemonstrate that InsightX Agent not only achieves a high object detection\nF1-score of 96.35% but also offers significantly improved interpretability and\ntrustworthiness in its analyses, highlighting the transformative potential of\nagentic LLM frameworks for industrial inspection tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLMM\u7684\u667a\u80fd\u6846\u67b6InsightX Agent\uff0c\u7528\u4e8e\u89e3\u51b3X\u5c04\u7ebf\u65e0\u635f\u68c0\u6d4b\u4e2d\u4ea4\u4e92\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u81ea\u8bc4\u4f30\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684X\u5c04\u7ebf\u68c0\u6d4b\u65b9\u6cd5\u7f3a\u4e4f\u4ea4\u4e92\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u81ea\u8bc4\u4f30\u80fd\u529b\uff0c\u9650\u5236\u4e86\u5176\u53ef\u9760\u6027\u548c\u64cd\u4f5c\u5458\u4fe1\u4efb\u3002", "method": "InsightX Agent\u4ee5\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMM\uff09\u4e3a\u6838\u5fc3\uff0c\u534f\u8c03\u7a00\u758f\u53ef\u53d8\u5f62\u591a\u5c3a\u5ea6\u68c0\u6d4b\u5668\uff08SDMSD\uff09\u548c\u57fa\u4e8e\u8bc1\u636e\u7684\u53cd\u601d\u5de5\u5177\uff08EGR\uff09\uff0c\u5b9e\u73b0\u4e3b\u52a8\u63a8\u7406\u548c\u591a\u5c3a\u5ea6\u7f3a\u9677\u68c0\u6d4b\u3002", "result": "\u5728GDXray+\u6570\u636e\u96c6\u4e0a\uff0cInsightX Agent\u5b9e\u73b0\u4e8696.35%\u7684F1\u5206\u6570\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u6790\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u4fe1\u5ea6\u3002", "conclusion": "\u8be5\u6846\u67b6\u5c55\u793a\u4e86\u667a\u80fdLMM\u5728\u5de5\u4e1a\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u53d8\u9769\u6f5c\u529b\u3002"}}
{"id": "2507.15444", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15444", "abs": "https://arxiv.org/abs/2507.15444", "authors": ["Leonard Bauersfeld", "Davide Scaramuzza"], "title": "Low-Latency Event-Based Velocimetry for Quadrotor Control in a Narrow Pipe", "comment": "17 pages", "summary": "Autonomous quadrotor flight in confined spaces such as pipes and tunnels\npresents significant challenges due to unsteady, self-induced aerodynamic\ndisturbances. Very recent advances have enabled flight in such conditions, but\nthey either rely on constant motion through the pipe to mitigate airflow\nrecirculation effects or suffer from limited stability during hovering. In this\nwork, we present the first closed-loop control system for quadrotors for\nhovering in narrow pipes that leverages real-time flow field measurements. We\ndevelop a low-latency, event-based smoke velocimetry method that estimates\nlocal airflow at high temporal resolution. This flow information is used by a\ndisturbance estimator based on a recurrent convolutional neural network, which\ninfers force and torque disturbances in real time. The estimated disturbances\nare integrated into a learning-based controller trained via reinforcement\nlearning. The flow-feedback control proves particularly effective during\nlateral translation maneuvers in the pipe cross-section. There, the real-time\ndisturbance information enables the controller to effectively counteract\ntransient aerodynamic effects, thereby preventing collisions with the pipe\nwall. To the best of our knowledge, this work represents the first\ndemonstration of an aerial robot with closed-loop control informed by real-time\nflow field measurements. This opens new directions for research on flight in\naerodynamically complex environments. In addition, our work also sheds light on\nthe characteristic flow structures that emerge during flight in narrow,\ncircular pipes, providing new insights at the intersection of robotics and\nfluid dynamics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b9e\u65f6\u6d41\u573a\u6d4b\u91cf\u7684\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u95ed\u73af\u63a7\u5236\u7cfb\u7edf\uff0c\u7528\u4e8e\u5728\u72ed\u7a84\u7ba1\u9053\u4e2d\u60ac\u505c\u548c\u6a2a\u5411\u79fb\u52a8\u3002", "motivation": "\u89e3\u51b3\u56db\u65cb\u7ffc\u5728\u72ed\u7a84\u7ba1\u9053\u4e2d\u98de\u884c\u65f6\u56e0\u6c14\u6d41\u6270\u52a8\u5bfc\u81f4\u7684\u7a33\u5b9a\u6027\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u4e86\u4f4e\u5ef6\u8fdf\u7684\u4e8b\u4ef6\u5f0f\u70df\u96fe\u6d4b\u901f\u65b9\u6cd5\uff0c\u7ed3\u5408\u5faa\u73af\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u4f30\u8ba1\u6270\u52a8\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u63a7\u5236\u5668\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u62b5\u6d88\u77ac\u6001\u6c14\u52a8\u6548\u5e94\uff0c\u9632\u6b62\u4e0e\u7ba1\u9053\u58c1\u78b0\u649e\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86\u57fa\u4e8e\u5b9e\u65f6\u6d41\u573a\u6d4b\u91cf\u7684\u95ed\u73af\u63a7\u5236\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u590d\u6742\u6c14\u52a8\u73af\u5883\u4e2d\u7684\u98de\u884c\u63a7\u5236\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u5e76\u63ed\u793a\u4e86\u7ba1\u9053\u98de\u884c\u4e2d\u7684\u6d41\u573a\u7279\u6027\u3002"}}
{"id": "2507.14906", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14906", "abs": "https://arxiv.org/abs/2507.14906", "authors": ["Xiao Yang", "Juxi Leitner", "Michael Burke"], "title": "Feedback-Induced Performance Decline in LLM-Based Decision-Making", "comment": null, "summary": "The ability of Large Language Models (LLMs) to extract context from natural\nlanguage problem descriptions naturally raises questions about their\nsuitability in autonomous decision-making settings. This paper studies the\nbehaviour of these models within a Markov Decision Process (MDPs). While\ntraditional reinforcement learning (RL) strategies commonly employed in this\nsetting rely on iterative exploration, LLMs, pre-trained on diverse datasets,\noffer the capability to leverage prior knowledge for faster adaptation. We\ninvestigate online structured prompting strategies in sequential decision\nmaking tasks, comparing the zero-shot performance of LLM-based approaches to\nthat of classical RL methods. Our findings reveal that although LLMs\ndemonstrate improved initial performance in simpler environments, they struggle\nwith planning and reasoning in complex scenarios without fine-tuning or\nadditional guidance. Our results show that feedback mechanisms, intended to\nimprove decision-making, often introduce confusion, leading to diminished\nperformance in intricate environments. These insights underscore the need for\nfurther exploration into hybrid strategies, fine-tuning, and advanced memory\nintegration to enhance LLM-based decision-making capabilities.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u5728\u7b80\u5355\u4efb\u52a1\u4e2d\u8868\u73b0\u8f83\u597d\uff0c\u4f46\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u3002", "motivation": "\u63a2\u7d22LLMs\u5728\u81ea\u4e3b\u51b3\u7b56\u4e2d\u7684\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5176\u9884\u8bad\u7ec3\u77e5\u8bc6\u662f\u5426\u80fd\u52a0\u901f\u9002\u5e94\u51b3\u7b56\u4efb\u52a1\u3002", "method": "\u91c7\u7528\u5728\u7ebf\u7ed3\u6784\u5316\u63d0\u793a\u7b56\u7565\uff0c\u6bd4\u8f83LLMs\u4e0e\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u5e8f\u5217\u51b3\u7b56\u4efb\u52a1\u4e2d\u7684\u96f6\u6837\u672c\u8868\u73b0\u3002", "result": "LLMs\u5728\u7b80\u5355\u73af\u5883\u4e2d\u521d\u59cb\u8868\u73b0\u66f4\u4f18\uff0c\u4f46\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7f3a\u4e4f\u89c4\u5212\u548c\u63a8\u7406\u80fd\u529b\uff1b\u53cd\u9988\u673a\u5236\u53ef\u80fd\u964d\u4f4e\u6027\u80fd\u3002", "conclusion": "\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u6df7\u5408\u7b56\u7565\u3001\u5fae\u8c03\u548c\u9ad8\u7ea7\u8bb0\u5fc6\u6574\u5408\u4ee5\u63d0\u5347LLMs\u7684\u51b3\u7b56\u80fd\u529b\u3002"}}
{"id": "2507.15469", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15469", "abs": "https://arxiv.org/abs/2507.15469", "authors": ["Thanh Thi Nguyen", "Saeid Nahavandi", "Imran Razzak", "Dung Nguyen", "Nhat Truong Pham", "Quoc Viet Hung Nguyen"], "title": "The Emergence of Deep Reinforcement Learning for Path Planning", "comment": "Accepted for publication in the Proceedings of the 2025 IEEE\n  International Conference on Systems, Man, and Cybernetics (SMC)", "summary": "The increasing demand for autonomous systems in complex and dynamic\nenvironments has driven significant research into intelligent path planning\nmethodologies. For decades, graph-based search algorithms, linear programming\ntechniques, and evolutionary computation methods have served as foundational\napproaches in this domain. Recently, deep reinforcement learning (DRL) has\nemerged as a powerful method for enabling autonomous agents to learn optimal\nnavigation strategies through interaction with their environments. This survey\nprovides a comprehensive overview of traditional approaches as well as the\nrecent advancements in DRL applied to path planning tasks, focusing on\nautonomous vehicles, drones, and robotic platforms. Key algorithms across both\nconventional and learning-based paradigms are categorized, with their\ninnovations and practical implementations highlighted. This is followed by a\nthorough discussion of their respective strengths and limitations in terms of\ncomputational efficiency, scalability, adaptability, and robustness. The survey\nconcludes by identifying key open challenges and outlining promising avenues\nfor future research. Special attention is given to hybrid approaches that\nintegrate DRL with classical planning techniques to leverage the benefits of\nboth learning-based adaptability and deterministic reliability, offering\npromising directions for robust and resilient autonomous navigation.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u8def\u5f84\u89c4\u5212\u9886\u57df\u7684\u4f20\u7edf\u65b9\u6cd5\u53ca\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u91cd\u70b9\u5206\u6790\u4e86\u81ea\u4e3b\u8f66\u8f86\u3001\u65e0\u4eba\u673a\u548c\u673a\u5668\u4eba\u5e73\u53f0\u7684\u5bfc\u822a\u7b56\u7565\u3002", "motivation": "\u968f\u7740\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u81ea\u4e3b\u7cfb\u7edf\u9700\u6c42\u7684\u589e\u52a0\uff0c\u7814\u7a76\u667a\u80fd\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002", "method": "\u7efc\u8ff0\u4e86\u4f20\u7edf\u56fe\u641c\u7d22\u3001\u7ebf\u6027\u89c4\u5212\u548c\u8fdb\u5316\u8ba1\u7b97\u65b9\u6cd5\uff0c\u4ee5\u53caDRL\u5728\u8def\u5f84\u89c4\u5212\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u5206\u7c7b\u8ba8\u8bba\u4e86\u5173\u952e\u7b97\u6cd5\u3002", "result": "\u5206\u6790\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e0eDRL\u5728\u8ba1\u7b97\u6548\u7387\u3001\u53ef\u6269\u5c55\u6027\u3001\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u7684\u4f18\u7f3a\u70b9\u3002", "conclusion": "\u63d0\u51fa\u4e86\u7ed3\u5408DRL\u4e0e\u4f20\u7edf\u65b9\u6cd5\u7684\u6df7\u5408\u8def\u5f84\u89c4\u5212\u65b9\u5411\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u5f00\u653e\u6311\u6218\u548c\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.14909", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.14909", "abs": "https://arxiv.org/abs/2507.14909", "authors": ["Elio Grande"], "title": "The Endless Tuning. An Artificial Intelligence Design To Avoid Human Replacement and Trace Back Responsibilities", "comment": null, "summary": "The Endless Tuning is a design method for a reliable deployment of artificial\nintelligence based on a double mirroring process, which pursues both the goals\nof avoiding human replacement and filling the so-called responsibility gap\n(Matthias 2004). Originally depicted in (Fabris et al. 2024) and ensuing the\nrelational approach urged therein, it was then actualized in a protocol,\nimplemented in three prototypical applications regarding decision-making\nprocesses (respectively: loan granting, pneumonia diagnosis, and art style\nrecognition) and tested with such as many domain experts. Step by step\nillustrating the protocol, giving insights concretely showing a different voice\n(Gilligan 1993) in the ethics of artificial intelligence, a philosophical\naccount of technical choices (e.g., a reversed and hermeneutic deployment of\nXAI algorithms) will be provided in the present study together with the results\nof the experiments, focusing on user experience rather than statistical\naccuracy. Even thoroughly employing deep learning models, full control was\nperceived by the interviewees in the decision-making setting, while it appeared\nthat a bridge can be built between accountability and liability in case of\ndamage.", "AI": {"tldr": "\u300aEndless Tuning\u300b\u662f\u4e00\u79cd\u57fa\u4e8e\u53cc\u91cd\u955c\u50cf\u8fc7\u7a0b\u7684\u4eba\u5de5\u667a\u80fd\u53ef\u9760\u90e8\u7f72\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u65e8\u5728\u907f\u514d\u4eba\u7c7b\u88ab\u66ff\u4ee3\u5e76\u586b\u8865\u8d23\u4efb\u7f3a\u53e3\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u4e09\u4e2a\u539f\u578b\u5e94\u7528\uff08\u8d37\u6b3e\u5ba1\u6279\u3001\u80ba\u708e\u8bca\u65ad\u548c\u827a\u672f\u98ce\u683c\u8bc6\u522b\uff09\u6d4b\u8bd5\uff0c\u91cd\u70b9\u5173\u6ce8\u7528\u6237\u4f53\u9a8c\u800c\u975e\u7edf\u8ba1\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u4eba\u5de5\u667a\u80fd\u90e8\u7f72\u4e2d\u7684\u4eba\u7c7b\u66ff\u4ee3\u95ee\u9898\u548c\u8d23\u4efb\u7f3a\u53e3\uff08Matthias 2004\uff09\uff0c\u5e76\u63a2\u7d22\u4f26\u7406\u4e0e\u6280\u672f\u9009\u62e9\u7684\u5173\u7cfb\u3002", "method": "\u91c7\u7528\u53cc\u91cd\u955c\u50cf\u8fc7\u7a0b\uff0c\u7ed3\u5408\u53cd\u5411\u548c\u89e3\u91ca\u6027XAI\u7b97\u6cd5\uff0c\u901a\u8fc7\u4e09\u4e2a\u539f\u578b\u5e94\u7528\uff08\u8d37\u6b3e\u3001\u533b\u7597\u3001\u827a\u672f\uff09\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u7528\u6237\u5bf9\u51b3\u7b56\u8fc7\u7a0b\u6709\u5b8c\u5168\u63a7\u5236\u611f\uff0c\u540c\u65f6\u80fd\u5728\u8d23\u4efb\u4e0e\u6cd5\u5f8b\u4e49\u52a1\u4e4b\u95f4\u5efa\u7acb\u6865\u6881\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u6df1\u5ea6\u5b66\u4e60\u6027\u80fd\u7684\u540c\u65f6\uff0c\u63d0\u5347\u4e86\u7528\u6237\u4f53\u9a8c\u548c\u8d23\u4efb\u900f\u660e\u5ea6\uff0c\u4e3aAI\u4f26\u7406\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\u3002"}}
{"id": "2507.15474", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15474", "abs": "https://arxiv.org/abs/2507.15474", "authors": ["Charith Premachandra", "Achala Athukorala", "U-Xuan Tan"], "title": "All-UWB SLAM Using UWB Radar and UWB AOA", "comment": null, "summary": "There has been a growing interest in autonomous systems designed to operate\nin adverse conditions (e.g. smoke, dust), where the visible light spectrum\nfails. In this context, Ultra-wideband (UWB) radar is capable of penetrating\nthrough such challenging environmental conditions due to the lower frequency\ncomponents within its broad bandwidth. Therefore, UWB radar has emerged as a\npotential sensing technology for Simultaneous Localization and Mapping (SLAM)\nin vision-denied environments where optical sensors (e.g. LiDAR, Camera) are\nprone to failure. Existing approaches involving UWB radar as the primary\nexteroceptive sensor generally extract features in the environment, which are\nlater initialized as landmarks in a map. However, these methods are constrained\nby the number of distinguishable features in the environment. Hence, this paper\nproposes a novel method incorporating UWB Angle of Arrival (AOA) measurements\ninto UWB radar-based SLAM systems to improve the accuracy and scalability of\nSLAM in feature-deficient environments. The AOA measurements are obtained using\nUWB anchor-tag units which are dynamically deployed by the robot in featureless\nareas during mapping of the environment. This paper thoroughly discusses\nprevailing constraints associated with UWB AOA measurement units and presents\nsolutions to overcome them. Our experimental results show that integrating UWB\nAOA units with UWB radar enables SLAM in vision-denied feature-deficient\nenvironments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408UWB AOA\u6d4b\u91cf\u7684\u65b0\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347\u5728\u7279\u5f81\u7f3a\u5931\u73af\u5883\u4e2dUWB\u96f7\u8fbeSLAM\u7684\u7cbe\u5ea6\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u5728\u6076\u52a3\u73af\u5883\uff08\u5982\u70df\u96fe\u3001\u7070\u5c18\uff09\u4e2d\uff0c\u5149\u5b66\u4f20\u611f\u5668\u6613\u5931\u6548\uff0cUWB\u96f7\u8fbe\u56e0\u5176\u7a7f\u900f\u80fd\u529b\u6210\u4e3aSLAM\u7684\u6f5c\u5728\u6280\u672f\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u53d7\u9650\u4e8e\u73af\u5883\u7279\u5f81\u6570\u91cf\u3002", "method": "\u901a\u8fc7\u52a8\u6001\u90e8\u7f72UWB\u951a\u70b9-\u6807\u7b7e\u5355\u5143\u83b7\u53d6AOA\u6d4b\u91cf\uff0c\u5e76\u5c06\u5176\u6574\u5408\u5230UWB\u96f7\u8fbeSLAM\u7cfb\u7edf\u4e2d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7ed3\u5408UWB AOA\u5355\u5143\u53ef\u5728\u7279\u5f81\u7f3a\u5931\u7684\u89c6\u89c9\u53d7\u9650\u73af\u5883\u4e2d\u5b9e\u73b0SLAM\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86UWB AOA\u6d4b\u91cf\u5355\u5143\u7684\u7ea6\u675f\uff0c\u63d0\u5347\u4e86SLAM\u5728\u6076\u52a3\u73af\u5883\u4e2d\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2507.14912", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14912", "abs": "https://arxiv.org/abs/2507.14912", "authors": ["Ruhul Amin Khalil", "Kashif Ahmad", "Hazrat Ali"], "title": "Redefining Elderly Care with Agentic AI: Challenges and Opportunities", "comment": null, "summary": "The global ageing population necessitates new and emerging strategies for\ncaring for older adults. In this article, we explore the potential for\ntransformation in elderly care through Agentic Artificial Intelligence (AI),\npowered by Large Language Models (LLMs). We discuss the proactive and\nautonomous decision-making facilitated by Agentic AI in elderly care.\nPersonalized tracking of health, cognitive care, and environmental management,\nall aimed at enhancing independence and high-level living for older adults,\nrepresents important areas of application. With a potential for significant\ntransformation of elderly care, Agentic AI also raises profound concerns about\ndata privacy and security, decision independence, and access. We share key\ninsights to emphasize the need for ethical safeguards, privacy protections, and\ntransparent decision-making. Our goal in this article is to provide a balanced\ndiscussion of both the potential and the challenges associated with Agentic AI,\nand to provide insights into its responsible use in elderly care, to bring\nAgentic AI into harmony with the requirements and vulnerabilities specific to\nthe elderly. Finally, we identify the priorities for the academic research\ncommunities, to achieve human-centered advancements and integration of Agentic\nAI in elderly care. To the best of our knowledge, this is no existing study\nthat reviews the role of Agentic AI in elderly care. Hence, we address the\nliterature gap by analyzing the unique capabilities, applications, and\nlimitations of LLM-based Agentic AI in elderly care. We also provide a\ncompanion interactive dashboard at https://hazratali.github.io/agenticai/.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4ee3\u7406\u4eba\u5de5\u667a\u80fd\uff08Agentic AI\uff09\u5728\u8001\u5e74\u62a4\u7406\u4e2d\u7684\u6f5c\u529b\u4e0e\u6311\u6218\uff0c\u5f3a\u8c03\u5176\u4e2a\u6027\u5316\u5065\u5eb7\u8ffd\u8e2a\u3001\u8ba4\u77e5\u62a4\u7406\u548c\u73af\u5883\u7ba1\u7406\u7b49\u529f\u80fd\uff0c\u540c\u65f6\u63d0\u51fa\u6570\u636e\u9690\u79c1\u3001\u5b89\u5168\u6027\u548c\u4f26\u7406\u95ee\u9898\u3002", "motivation": "\u5168\u7403\u8001\u9f84\u5316\u8d8b\u52bf\u9700\u8981\u521b\u65b0\u7684\u8001\u5e74\u62a4\u7406\u7b56\u7565\uff0c\u4ee3\u7406AI\u56e0\u5176\u81ea\u4e3b\u51b3\u7b56\u80fd\u529b\u88ab\u89c6\u4e3a\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u5206\u6790\u4ee3\u7406AI\u5728\u8001\u5e74\u62a4\u7406\u4e2d\u7684\u5e94\u7528\uff0c\u8ba8\u8bba\u5176\u72ec\u7279\u80fd\u529b\u4e0e\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4f26\u7406\u4fdd\u969c\u548c\u900f\u660e\u51b3\u7b56\u7684\u5efa\u8bae\u3002", "result": "\u4ee3\u7406AI\u6709\u671b\u663e\u8457\u6539\u5584\u8001\u5e74\u62a4\u7406\uff0c\u4f46\u9700\u89e3\u51b3\u9690\u79c1\u3001\u5b89\u5168\u548c\u4f26\u7406\u95ee\u9898\u4ee5\u5b9e\u73b0\u8d1f\u8d23\u4efb\u7684\u5e94\u7528\u3002", "conclusion": "\u672c\u6587\u586b\u8865\u4e86\u4ee3\u7406AI\u5728\u8001\u5e74\u62a4\u7406\u9886\u57df\u7684\u7814\u7a76\u7a7a\u767d\uff0c\u547c\u5401\u5b66\u672f\u754c\u4f18\u5148\u63a8\u8fdb\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684AI\u6574\u5408\u3002"}}
{"id": "2507.15478", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15478", "abs": "https://arxiv.org/abs/2507.15478", "authors": ["Simon Kohaut", "Felix Divo", "Navid Hamid", "Benedict Flade", "Julian Eggert", "Devendra Singh Dhami", "Kristian Kersting"], "title": "The Constitutional Controller: Doubt-Calibrated Steering of Compliant Agents", "comment": null, "summary": "Ensuring reliable and rule-compliant behavior of autonomous agents in\nuncertain environments remains a fundamental challenge in modern robotics. Our\nwork shows how neuro-symbolic systems, which integrate probabilistic, symbolic\nwhite-box reasoning models with deep learning methods, offer a powerful\nsolution to this challenge. This enables the simultaneous consideration of\nexplicit rules and neural models trained on noisy data, combining the strength\nof structured reasoning with flexible representations. To this end, we\nintroduce the Constitutional Controller (CoCo), a novel framework designed to\nenhance the safety and reliability of agents by reasoning over deep\nprobabilistic logic programs representing constraints such as those found in\nshared traffic spaces. Furthermore, we propose the concept of self-doubt,\nimplemented as a probability density conditioned on doubt features such as\ntravel velocity, employed sensors, or health factors. In a real-world aerial\nmobility study, we demonstrate CoCo's advantages for intelligent autonomous\nsystems to learn appropriate doubts and navigate complex and uncertain\nenvironments safely and compliantly.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u795e\u7ecf\u7b26\u53f7\u7cfb\u7edf\u7684\u65b0\u6846\u67b6CoCo\uff0c\u901a\u8fc7\u6982\u7387\u903b\u8f91\u7a0b\u5e8f\u548c\u81ea\u6211\u6000\u7591\u673a\u5236\u63d0\u5347\u81ea\u4e3b\u4ee3\u7406\u5728\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002", "motivation": "\u89e3\u51b3\u81ea\u4e3b\u4ee3\u7406\u5728\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u53ef\u9760\u4e14\u5408\u89c4\u884c\u4e3a\u7684\u6311\u6218\u3002", "method": "\u7ed3\u5408\u6982\u7387\u7b26\u53f7\u63a8\u7406\u4e0e\u6df1\u5ea6\u5b66\u4e60\uff0c\u5f15\u5165Constitutional Controller\uff08CoCo\uff09\u6846\u67b6\u548c\u81ea\u6211\u6000\u7591\u673a\u5236\u3002", "result": "\u5728\u771f\u5b9e\u7a7a\u4e2d\u4ea4\u901a\u7814\u7a76\u4e2d\uff0cCoCo\u6210\u529f\u5e2e\u52a9\u667a\u80fd\u7cfb\u7edf\u5b66\u4e60\u9002\u5f53\u6000\u7591\u5e76\u5b89\u5168\u5bfc\u822a\u590d\u6742\u73af\u5883\u3002", "conclusion": "\u795e\u7ecf\u7b26\u53f7\u7cfb\u7edf\u548c\u81ea\u6211\u6000\u7591\u673a\u5236\u4e3a\u81ea\u4e3b\u4ee3\u7406\u5728\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u7684\u5b89\u5168\u8fd0\u884c\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.14962", "categories": ["cs.AI", "cs.CC", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.14962", "abs": "https://arxiv.org/abs/2507.14962", "authors": ["Johannes Schmidt", "Mohamed Maizia", "Victor Lagerkvist", "Johannes K. Fichte"], "title": "Complexity of Faceted Explanations in Propositional Abduction", "comment": "This is the author's self-archived copy including detailed proofs. To\n  appear in Theory and Practice of Logic Programming (TPLP), Proceedings of the\n  41st International Conference on Logic Programming (ICLP 2025)", "summary": "Abductive reasoning is a popular non-monotonic paradigm that aims to explain\nobserved symptoms and manifestations. It has many applications, such as\ndiagnosis and planning in artificial intelligence and database updates. In\npropositional abduction, we focus on specifying knowledge by a propositional\nformula. The computational complexity of tasks in propositional abduction has\nbeen systematically characterized - even with detailed classifications for\nBoolean fragments. Unsurprisingly, the most insightful reasoning problems\n(counting and enumeration) are computationally highly challenging. Therefore,\nwe consider reasoning between decisions and counting, allowing us to understand\nexplanations better while maintaining favorable complexity. We introduce facets\nto propositional abductions, which are literals that occur in some explanation\n(relevant) but not all explanations (dispensable). Reasoning with facets\nprovides a more fine-grained understanding of variability in explanations\n(heterogeneous). In addition, we consider the distance between two\nexplanations, enabling a better understanding of heterogeneity/homogeneity. We\ncomprehensively analyze facets of propositional abduction in various settings,\nincluding an almost complete characterization in Post's framework.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u547d\u9898\u6eaf\u56e0\u4e2d\u7684\u7ec6\u7c92\u5ea6\u63a8\u7406\uff0c\u5f15\u5165\u4e86\u201cfacet\u201d\u6982\u5ff5\u4ee5\u533a\u5206\u89e3\u91ca\u4e2d\u7684\u76f8\u5173\u6027\u548c\u53ef\u5ffd\u7565\u6027\uff0c\u5e76\u5206\u6790\u4e86\u4e0d\u540c\u8bbe\u7f6e\u4e0b\u7684\u590d\u6742\u6027\u3002", "motivation": "\u65e8\u5728\u66f4\u6df1\u5165\u7406\u89e3\u547d\u9898\u6eaf\u56e0\u4e2d\u89e3\u91ca\u7684\u53d8\u5f02\u6027\uff0c\u901a\u8fc7\u5f15\u5165facet\u548c\u89e3\u91ca\u95f4\u8ddd\u79bb\u6765\u63d0\u5347\u5bf9\u5f02\u8d28\u6027/\u540c\u8d28\u6027\u7684\u8ba4\u8bc6\u3002", "method": "\u5f15\u5165facet\u6982\u5ff5\uff0c\u5206\u6790\u5176\u5728\u547d\u9898\u6eaf\u56e0\u4e2d\u7684\u4f5c\u7528\uff0c\u5e76\u7ed3\u5408Post\u6846\u67b6\u8fdb\u884c\u590d\u6742\u6027\u5206\u7c7b\u3002", "result": "\u5728\u591a\u79cd\u8bbe\u7f6e\u4e0b\u5168\u9762\u5206\u6790\u4e86facet\uff0c\u5305\u62ec\u5728Post\u6846\u67b6\u4e2d\u7684\u51e0\u4e4e\u5b8c\u5168\u5206\u7c7b\u3002", "conclusion": "\u901a\u8fc7facet\u548c\u89e3\u91ca\u8ddd\u79bb\uff0c\u63d0\u4f9b\u4e86\u5bf9\u547d\u9898\u6eaf\u56e0\u89e3\u91ca\u53d8\u5f02\u6027\u66f4\u7ec6\u7c92\u5ea6\u7684\u7406\u89e3\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u826f\u597d\u7684\u8ba1\u7b97\u590d\u6742\u6027\u3002"}}
{"id": "2507.15484", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15484", "abs": "https://arxiv.org/abs/2507.15484", "authors": ["Jamie Bell"], "title": "Robots for Kiwifruit Harvesting and Pollination", "comment": null, "summary": "This research was a part of a project that developed mobile robots that\nperformed targeted pollen spraying and automated harvesting in pergola\nstructured kiwifruit orchards. Multiple kiwifruit detachment mechanisms were\ndesigned and field testing of one of the concepts showed that the mechanism\ncould reliably pick kiwifruit. Furthermore, this kiwifruit detachment mechanism\nwas able to reach over 80 percent of fruit in the cluttered kiwifruit canopy,\nwhereas the previous state of the art mechanism was only able to reach less\nthan 70 percent of the fruit. Artificial pollination was performed by detecting\nflowers and then spraying pollen in solution onto the detected flowers from a\nline of sprayers on a boom, while driving at up to 1.4 ms-1. In addition, the\nheight of the canopy was measured and the spray boom was moved up and down to\nkeep the boom close enough to the flowers for the spray to reach the flowers,\nwhile minimising collisions with the canopy. Mobile robot navigation was\nperformed using a 2D lidar in apple orchards and vineyards. Lidar navigation in\nkiwifruit orchards was more challenging because the pergola structure only\nprovides a small amount of data for the direction of rows, compared to the\namount of data from the overhead canopy, the undulating ground and other\nobjects in the orchards. Multiple methods are presented here for extracting\nstructure defining features from 3D lidar data in kiwifruit orchards. In\naddition, a 3D lidar navigation system -- which performed row following, row\nend detection and row end turns -- was tested for over 30 km of autonomous\ndriving in kiwifruit orchards. Computer vision algorithms for row detection and\nrow following were also tested. The computer vision algorithm worked as well as\nthe 3D lidar row following method in testing.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5f00\u53d1\u4e86\u7528\u4e8e\u7315\u7334\u6843\u679c\u56ed\u7684\u79fb\u52a8\u673a\u5668\u4eba\uff0c\u5b9e\u73b0\u4e86\u5b9a\u5411\u82b1\u7c89\u55b7\u6d12\u548c\u81ea\u52a8\u5316\u91c7\u6458\uff0c\u6539\u8fdb\u4e86\u679c\u5b9e\u91c7\u6458\u548c\u82b1\u7c89\u55b7\u6d12\u7684\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u7315\u7334\u6843\u679c\u56ed\u4e2d\u4eba\u5de5\u91c7\u6458\u548c\u82b1\u7c89\u55b7\u6d12\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u81ea\u52a8\u5316\u6c34\u5e73\u3002", "method": "\u8bbe\u8ba1\u4e86\u591a\u79cd\u679c\u5b9e\u91c7\u6458\u673a\u5236\uff0c\u5e76\u6d4b\u8bd5\u4e86\u5176\u4e2d\u4e00\u79cd\uff1b\u4f7f\u7528\u55b7\u96fe\u5668\u55b7\u6d12\u82b1\u7c89\uff1b\u5229\u75282D\u548c3D\u6fc0\u5149\u96f7\u8fbe\u8fdb\u884c\u5bfc\u822a\uff1b\u6d4b\u8bd5\u8ba1\u7b97\u673a\u89c6\u89c9\u7b97\u6cd5\u3002", "result": "\u91c7\u6458\u673a\u5236\u53ef\u8986\u76d680%\u4ee5\u4e0a\u7684\u679c\u5b9e\uff0c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff1b\u82b1\u7c89\u55b7\u6d12\u548c\u5bfc\u822a\u7cfb\u7edf\u5728\u679c\u56ed\u4e2d\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u79fb\u52a8\u673a\u5668\u4eba\u548c\u81ea\u52a8\u5316\u6280\u672f\u53ef\u663e\u8457\u63d0\u5347\u7315\u7334\u6843\u679c\u56ed\u7684\u751f\u4ea7\u6548\u7387\u3002"}}
{"id": "2507.14987", "categories": ["cs.AI", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14987", "abs": "https://arxiv.org/abs/2507.14987", "authors": ["Yi Zhang", "An Zhang", "XiuYu Zhang", "Leheng Sheng", "Yuxin Chen", "Zhenkai Liang", "Xiang Wang"], "title": "AlphaAlign: Incentivizing Safety Alignment with Extremely Simplified Reinforcement Learning", "comment": null, "summary": "Large language models (LLMs), despite possessing latent safety understanding\nfrom their vast pretraining data, remain vulnerable to generating harmful\ncontent and exhibit issues such as over-refusal and utility degradation after\nsafety alignment. Current safety alignment methods often result in superficial\nrefusal shortcuts or rely on intensive supervision for reasoning-based\napproaches, failing to fully leverage the model's intrinsic safety\nself-awareness. We propose \\textbf{AlphaAlign}, a simple yet effective pure\nreinforcement learning (RL) framework with verifiable safety reward designed to\nincentivize this latent safety awareness through proactive safety reasoning.}\nAlphaAlign employs a dual-reward system: a verifiable safety reward encourages\ncorrectly formatted and explicitly justified refusals for harmful queries while\npenalizing over-refusals, and a normalized helpfulness reward guides\nhigh-quality responses to benign inputs. This allows the model to develop\nproactive safety reasoning capabilities without depending on supervised\nsafety-specific reasoning data. AlphaAlign demonstrates three key advantages:\n(1) Simplicity and efficiency, requiring only binary prompt safety labels and\nminimal RL steps for substantial improvements. (2) Breaking the safety-utility\ntrade-off, by enhancing refusal of harmful content and reducing over-refusals,\nwhile simultaneously maintaining or even improving general task performance and\nrobustness to unseen jailbreaks. (3) Deep alignment, fostering proactive safety\nreasoning that generates explicit safety rationales rather than relying on\nshallow refusal patterns.", "AI": {"tldr": "AlphaAlign\u662f\u4e00\u4e2a\u57fa\u4e8e\u7eaf\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u9a8c\u8bc1\u7684\u5b89\u5168\u5956\u52b1\u6fc0\u53d1\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5185\u5728\u5b89\u5168\u81ea\u6211\u610f\u8bc6\uff0c\u89e3\u51b3\u73b0\u6709\u5b89\u5168\u5bf9\u9f50\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u5b89\u5168\u5bf9\u9f50\u65b9\u6cd5\u5b58\u5728\u6d45\u5c42\u62d2\u7edd\u6216\u4f9d\u8d56\u5bc6\u96c6\u76d1\u7763\u7684\u95ee\u9898\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u6a21\u578b\u7684\u5185\u5728\u5b89\u5168\u81ea\u6211\u610f\u8bc6\u3002", "method": "AlphaAlign\u91c7\u7528\u53cc\u5956\u52b1\u7cfb\u7edf\uff1a\u5b89\u5168\u5956\u52b1\u9f13\u52b1\u5bf9\u6709\u5bb3\u67e5\u8be2\u7684\u6b63\u786e\u62d2\u7edd\u548c\u660e\u786e\u7406\u7531\uff0c\u5e2e\u52a9\u5956\u52b1\u6307\u5bfc\u5bf9\u826f\u6027\u8f93\u5165\u7684\u9ad8\u8d28\u91cf\u54cd\u5e94\u3002", "result": "AlphaAlign\u5728\u7b80\u5316\u6d41\u7a0b\u3001\u6253\u7834\u5b89\u5168-\u6548\u7528\u6743\u8861\u53ca\u6df1\u5ea6\u5bf9\u9f50\u65b9\u9762\u8868\u73b0\u51fa\u663e\u8457\u4f18\u52bf\u3002", "conclusion": "AlphaAlign\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u6709\u6548\u63d0\u5347\u6a21\u578b\u7684\u5b89\u5168\u63a8\u7406\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u5347\u4efb\u52a1\u6027\u80fd\u3002"}}
{"id": "2507.15493", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15493", "abs": "https://arxiv.org/abs/2507.15493", "authors": ["Chilam Cheang", "Sijin Chen", "Zhongren Cui", "Yingdong Hu", "Liqun Huang", "Tao Kong", "Hang Li", "Yifeng Li", "Yuxiao Liu", "Xiao Ma", "Hao Niu", "Wenxuan Ou", "Wanli Peng", "Zeyu Ren", "Haixin Shi", "Jiawen Tian", "Hongtao Wu", "Xin Xiao", "Yuyang Xiao", "Jiafeng Xu", "Yichu Yang"], "title": "GR-3 Technical Report", "comment": "Tech report. Authors are listed in alphabetical order. Project page:\n  https://seed.bytedance.com/GR3/", "summary": "We report our recent progress towards building generalist robot policies, the\ndevelopment of GR-3. GR-3 is a large-scale vision-language-action (VLA) model.\nIt showcases exceptional capabilities in generalizing to novel objects,\nenvironments, and instructions involving abstract concepts. Furthermore, it can\nbe efficiently fine-tuned with minimal human trajectory data, enabling rapid\nand cost-effective adaptation to new settings. GR-3 also excels in handling\nlong-horizon and dexterous tasks, including those requiring bi-manual\nmanipulation and mobile movement, showcasing robust and reliable performance.\nThese capabilities are achieved through a multi-faceted training recipe that\nincludes co-training with web-scale vision-language data, efficient fine-tuning\nfrom human trajectory data collected via VR devices, and effective imitation\nlearning with robot trajectory data. In addition, we introduce ByteMini, a\nversatile bi-manual mobile robot designed with exceptional flexibility and\nreliability, capable of accomplishing a wide range of tasks when integrated\nwith GR-3. Through extensive real-world experiments, we show GR-3 surpasses the\nstate-of-the-art baseline method, $\\pi_0$, on a wide variety of challenging\ntasks. We hope GR-3 can serve as a step towards building generalist robots\ncapable of assisting humans in daily life.", "AI": {"tldr": "GR-3\u662f\u4e00\u79cd\u5927\u89c4\u6a21\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u5c55\u793a\u4e86\u5728\u6cdb\u5316\u65b0\u5bf9\u8c61\u3001\u73af\u5883\u548c\u62bd\u8c61\u6307\u4ee4\u65b9\u9762\u7684\u5353\u8d8a\u80fd\u529b\uff0c\u5e76\u80fd\u9ad8\u6548\u5fae\u8c03\u3002\u7ed3\u5408ByteMini\u673a\u5668\u4eba\uff0cGR-3\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u76ee\u6807\u662f\u6784\u5efa\u901a\u7528\u673a\u5668\u4eba\u7b56\u7565\uff0c\u4ee5\u8f85\u52a9\u4eba\u7c7b\u65e5\u5e38\u751f\u6d3b\u3002", "method": "\u901a\u8fc7\u591a\u6a21\u6001\u8bad\u7ec3\uff08\u5305\u62ec\u7f51\u7edc\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u6570\u636e\u3001VR\u8bbe\u5907\u6536\u96c6\u7684\u4eba\u7c7b\u8f68\u8ff9\u6570\u636e\u548c\u673a\u5668\u4eba\u8f68\u8ff9\u6570\u636e\uff09\u5b9e\u73b0\u3002", "result": "GR-3\u5728\u591a\u79cd\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u57fa\u7ebf\u65b9\u6cd5\u03c0\u2080\u3002", "conclusion": "GR-3\u662f\u8fc8\u5411\u901a\u7528\u673a\u5668\u4eba\u7684\u91cd\u8981\u4e00\u6b65\uff0c\u5c55\u793a\u4e86\u5176\u5728\u73b0\u5b9e\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.15013", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15013", "abs": "https://arxiv.org/abs/2507.15013", "authors": ["Xiaoyu Li", "Jin Wu", "Shaoyang Guo", "Haoran Shi", "Chanjin Zheng"], "title": "A Forced-Choice Neural Cognitive Diagnostic Model of Personality Testing", "comment": "15pages, 7 figures", "summary": "In the smart era, psychometric tests are becoming increasingly important for\npersonnel selection, career development, and mental health assessment.\nForced-choice tests are common in personality assessments because they require\nparticipants to select from closely related options, lowering the risk of\nresponse distortion. This study presents a deep learning-based Forced-Choice\nNeural Cognitive Diagnostic Model (FCNCD) that overcomes the limitations of\ntraditional models and is applicable to the three most common item block types\nfound in forced-choice tests. To account for the unidimensionality of items in\nforced-choice tests, we create interpretable participant and item parameters.\nWe model the interactions between participant and item features using\nmultilayer neural networks after mining them using nonlinear mapping. In\naddition, we use the monotonicity assumption to improve the interpretability of\nthe diagnostic results. The FCNCD's effectiveness is validated by experiments\non real-world and simulated datasets that show its accuracy, interpretability,\nand robustness.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u5f3a\u5236\u9009\u62e9\u795e\u7ecf\u8ba4\u77e5\u8bca\u65ad\u6a21\u578b\uff08FCNCD\uff09\uff0c\u7528\u4e8e\u6539\u8fdb\u4f20\u7edf\u6a21\u578b\u5728\u5f3a\u5236\u9009\u62e9\u6d4b\u8bd5\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u51c6\u786e\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u5728\u667a\u80fd\u65f6\u4ee3\uff0c\u5fc3\u7406\u6d4b\u91cf\u6d4b\u8bd5\u5728\u4eba\u5458\u9009\u62d4\u3001\u804c\u4e1a\u53d1\u5c55\u548c\u5fc3\u7406\u5065\u5eb7\u8bc4\u4f30\u4e2d\u8d8a\u6765\u8d8a\u91cd\u8981\u3002\u5f3a\u5236\u9009\u62e9\u6d4b\u8bd5\u56e0\u5176\u80fd\u964d\u4f4e\u56de\u7b54\u5931\u771f\u7684\u98ce\u9669\u800c\u88ab\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u4f20\u7edf\u6a21\u578b\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u901a\u8fc7\u975e\u7ebf\u6027\u6620\u5c04\u6316\u6398\u53c2\u4e0e\u8005\u548c\u9879\u76ee\u7279\u5f81\uff0c\u4f7f\u7528\u591a\u5c42\u795e\u7ecf\u7f51\u7edc\u5efa\u6a21\u5176\u4ea4\u4e92\uff0c\u5e76\u5229\u7528\u5355\u8c03\u6027\u5047\u8bbe\u63d0\u5347\u8bca\u65ad\u7ed3\u679c\u7684\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u5728\u771f\u5b9e\u548c\u6a21\u62df\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86FCNCD\u7684\u51c6\u786e\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "FCNCD\u6a21\u578b\u5728\u5f3a\u5236\u9009\u62e9\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u6a21\u578b\u7684\u4e0d\u8db3\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.15499", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15499", "abs": "https://arxiv.org/abs/2507.15499", "authors": ["Jongseok Lee", "Timo Birr", "Rudolph Triebel", "Tamim Asfour"], "title": "CLEVER: Stream-based Active Learning for Robust Semantic Perception from Human Instructions", "comment": "8 pages. Accepted to IEEE RAL", "summary": "We propose CLEVER, an active learning system for robust semantic perception\nwith Deep Neural Networks (DNNs). For data arriving in streams, our system\nseeks human support when encountering failures and adapts DNNs online based on\nhuman instructions. In this way, CLEVER can eventually accomplish the given\nsemantic perception tasks. Our main contribution is the design of a system that\nmeets several desiderata of realizing the aforementioned capabilities. The key\nenabler herein is our Bayesian formulation that encodes domain knowledge\nthrough priors. Empirically, we not only motivate CLEVER's design but further\ndemonstrate its capabilities with a user validation study as well as\nexperiments on humanoid and deformable objects. To our knowledge, we are the\nfirst to realize stream-based active learning on a real robot, providing\nevidence that the robustness of the DNN-based semantic perception can be\nimproved in practice. The project website can be accessed at\nhttps://sites.google.com/view/thecleversystem.", "AI": {"tldr": "CLEVER\u662f\u4e00\u4e2a\u57fa\u4e8e\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNN\uff09\u7684\u4e3b\u52a8\u5b66\u4e60\u7cfb\u7edf\uff0c\u901a\u8fc7\u5728\u7ebf\u83b7\u53d6\u4eba\u7c7b\u652f\u6301\u5e76\u9002\u5e94DNN\uff0c\u5b9e\u73b0\u9c81\u68d2\u7684\u8bed\u4e49\u611f\u77e5\u4efb\u52a1\u3002", "motivation": "\u89e3\u51b3\u6570\u636e\u6d41\u4e2dDNN\u8bed\u4e49\u611f\u77e5\u7684\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u4eba\u7c7b\u53cd\u9988\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u8d1d\u53f6\u65af\u516c\u5f0f\u7684\u7cfb\u7edf\uff0c\u5229\u7528\u5148\u9a8c\u77e5\u8bc6\u7f16\u7801\u9886\u57df\u77e5\u8bc6\uff0c\u5e76\u901a\u8fc7\u7528\u6237\u9a8c\u8bc1\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u80fd\u529b\u3002", "result": "\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u9996\u6b21\u5b9e\u73b0\u4e86\u57fa\u4e8e\u6d41\u7684\u4e3b\u52a8\u5b66\u4e60\uff0c\u8bc1\u660eDNN\u8bed\u4e49\u611f\u77e5\u7684\u9c81\u68d2\u6027\u53ef\u5b9e\u9645\u63d0\u5347\u3002", "conclusion": "CLEVER\u7cfb\u7edf\u901a\u8fc7\u7ed3\u5408\u4eba\u7c7b\u53cd\u9988\u548c\u8d1d\u53f6\u65af\u65b9\u6cd5\uff0c\u6709\u6548\u63d0\u5347\u4e86DNN\u8bed\u4e49\u611f\u77e5\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.15042", "categories": ["cs.AI", "cs.IR", "I.2.7; H.3.3; K.6.5"], "pdf": "https://arxiv.org/pdf/2507.15042", "abs": "https://arxiv.org/abs/2507.15042", "authors": ["Jerry Wang", "Fang Yu"], "title": "DeRAG: Black-box Adversarial Attacks on Multiple Retrieval-Augmented Generation Applications via Prompt Injection", "comment": "Accepted by KDD Workshop on Prompt Optimization 2025", "summary": "Adversarial prompt attacks can significantly alter the reliability of\nRetrieval-Augmented Generation (RAG) systems by re-ranking them to produce\nincorrect outputs. In this paper, we present a novel method that applies\nDifferential Evolution (DE) to optimize adversarial prompt suffixes for\nRAG-based question answering. Our approach is gradient-free, treating the RAG\npipeline as a black box and evolving a population of candidate suffixes to\nmaximize the retrieval rank of a targeted incorrect document to be closer to\nreal world scenarios. We conducted experiments on the BEIR QA datasets to\nevaluate attack success at certain retrieval rank thresholds under multiple\nretrieving applications. Our results demonstrate that DE-based prompt\noptimization attains competitive (and in some cases higher) success rates\ncompared to GGPP to dense retrievers and PRADA to sparse retrievers, while\nusing only a small number of tokens (<=5 tokens) in the adversarial suffix.\nFurthermore, we introduce a readability-aware suffix construction strategy,\nvalidated by a statistically significant reduction in MLM negative\nlog-likelihood with Welch's t-test. Through evaluations with a BERT-based\nadversarial suffix detector, we show that DE-generated suffixes evade\ndetection, yielding near-chance detection accuracy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5dee\u5206\u8fdb\u5316\uff08DE\uff09\u7684\u65b9\u6cd5\uff0c\u4f18\u5316\u5bf9\u6297\u6027\u63d0\u793a\u540e\u7f00\u4ee5\u653b\u51fbRAG\u7cfb\u7edf\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u653b\u51fb\u6210\u529f\u7387\u9ad8\u4e14\u9690\u853d\u6027\u5f3a\u3002", "motivation": "\u5bf9\u6297\u6027\u63d0\u793a\u653b\u51fb\u4f1a\u663e\u8457\u5f71\u54cdRAG\u7cfb\u7edf\u7684\u53ef\u9760\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u4e14\u9690\u853d\u7684\u653b\u51fb\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u68af\u5ea6\u65e0\u5173\u7684\u5dee\u5206\u8fdb\u5316\u7b97\u6cd5\uff0c\u5c06RAG\u7cfb\u7edf\u89c6\u4e3a\u9ed1\u7bb1\uff0c\u4f18\u5316\u5bf9\u6297\u6027\u63d0\u793a\u540e\u7f00\u4ee5\u63d0\u5347\u9519\u8bef\u6587\u6863\u7684\u68c0\u7d22\u6392\u540d\u3002", "result": "\u5728BEIR QA\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDE\u65b9\u6cd5\u653b\u51fb\u6210\u529f\u7387\u9ad8\uff0c\u4e14\u751f\u6210\u7684\u5bf9\u6297\u6027\u540e\u7f00\u9690\u853d\u6027\u5f3a\uff0c\u96be\u4ee5\u88ab\u68c0\u6d4b\u3002", "conclusion": "DE\u65b9\u6cd5\u5728\u5bf9\u6297\u6027\u63d0\u793a\u653b\u51fb\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3aRAG\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u65b0\u7684\u7814\u7a76\u89c6\u89d2\u3002"}}
{"id": "2507.15604", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15604", "abs": "https://arxiv.org/abs/2507.15604", "authors": ["Johannes Hartwig", "Philipp Lienhardt", "Dominik Henrich"], "title": "Estimation of Payload Inertial Parameters from Human Demonstrations by Hand Guiding", "comment": "Accepted for publication in Annals of Scientific Society for\n  Assembly, Handling and Industrial Robotics 2025 (to appear)", "summary": "As the availability of cobots increases, it is essential to address the needs\nof users with little to no programming knowledge to operate such systems\nefficiently. Programming concepts often use intuitive interaction modalities,\nsuch as hand guiding, to address this. When programming in-contact motions,\nsuch frameworks require knowledge of the robot tool's payload inertial\nparameters (PIP) in addition to the demonstrated velocities and forces to\nensure effective hybrid motion-force control. This paper aims to enable\nnon-expert users to program in-contact motions more efficiently by eliminating\nthe need for a dedicated PIP calibration, thereby enabling flexible robot tool\nchanges. Since demonstrated tasks generally also contain motions with\nnon-contact, our approach uses these parts to estimate the robot's PIP using\nestablished estimation techniques. The results show that the estimation of the\npayload's mass is accurate, whereas the center of mass and the inertia tensor\nare affected by noise and a lack of excitation. Overall, these findings show\nthe feasibility of PIP estimation during hand guiding but also highlight the\nneed for sufficient payload accelerations for an accurate estimation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u4e13\u7528\u8d1f\u8f7d\u60ef\u6027\u53c2\u6570\uff08PIP\uff09\u6821\u51c6\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u975e\u63a5\u89e6\u8fd0\u52a8\u90e8\u5206\u4f30\u8ba1PIP\uff0c\u4f7f\u975e\u4e13\u4e1a\u7528\u6237\u80fd\u66f4\u9ad8\u6548\u7f16\u7a0b\u63a5\u89e6\u8fd0\u52a8\u3002", "motivation": "\u968f\u7740\u534f\u4f5c\u673a\u5668\u4eba\u666e\u53ca\uff0c\u9700\u4e3a\u975e\u4e13\u4e1a\u7528\u6237\u63d0\u4f9b\u65e0\u9700\u7f16\u7a0b\u77e5\u8bc6\u7684\u64cd\u4f5c\u65b9\u5f0f\uff0c\u5c24\u5176\u662f\u63a5\u89e6\u8fd0\u52a8\u7f16\u7a0b\u4e2d\u907f\u514d\u4e13\u7528PIP\u6821\u51c6\u7684\u9700\u6c42\u3002", "method": "\u5229\u7528\u4efb\u52a1\u4e2d\u7684\u975e\u63a5\u89e6\u8fd0\u52a8\u90e8\u5206\uff0c\u901a\u8fc7\u73b0\u6709\u4f30\u8ba1\u6280\u672f\u4f30\u7b97PIP\uff0c\u907f\u514d\u4e13\u7528\u6821\u51c6\u3002", "result": "\u8d28\u91cf\u4f30\u8ba1\u51c6\u786e\uff0c\u4f46\u8d28\u5fc3\u548c\u60ef\u6027\u5f20\u91cf\u53d7\u566a\u58f0\u548c\u6fc0\u52b1\u4e0d\u8db3\u5f71\u54cd\u3002", "conclusion": "\u624b\u5f15\u5bfc\u4e2dPIP\u4f30\u8ba1\u53ef\u884c\uff0c\u4f46\u9700\u8db3\u591f\u8d1f\u8f7d\u52a0\u901f\u5ea6\u4ee5\u786e\u4fdd\u51c6\u786e\u6027\u3002"}}
{"id": "2507.15106", "categories": ["cs.AI", "cs.RO", "F.2.2"], "pdf": "https://arxiv.org/pdf/2507.15106", "abs": "https://arxiv.org/abs/2507.15106", "authors": ["Xia Xu", "Jochen Triesch"], "title": "From Kicking to Causality: Simulating Infant Agency Detection with a Robust Intrinsic Reward", "comment": "13 pages, 5 figures", "summary": "While human infants robustly discover their own causal efficacy, standard\nreinforcement learning agents remain brittle, as their reliance on\ncorrelation-based rewards fails in noisy, ecologically valid scenarios. To\naddress this, we introduce the Causal Action Influence Score (CAIS), a novel\nintrinsic reward rooted in causal inference. CAIS quantifies an action's\ninfluence by measuring the 1-Wasserstein distance between the learned\ndistribution of sensory outcomes conditional on that action, $p(h|a)$, and the\nbaseline outcome distribution, $p(h)$. This divergence provides a robust reward\nthat isolates the agent's causal impact from confounding environmental noise.\nWe test our approach in a simulated infant-mobile environment where\ncorrelation-based perceptual rewards fail completely when the mobile is\nsubjected to external forces. In stark contrast, CAIS enables the agent to\nfilter this noise, identify its influence, and learn the correct policy.\nFurthermore, the high-quality predictive model learned for CAIS allows our\nagent, when augmented with a surprise signal, to successfully reproduce the\n\"extinction burst\" phenomenon. We conclude that explicitly inferring causality\nis a crucial mechanism for developing a robust sense of agency, offering a\npsychologically plausible framework for more adaptive autonomous systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56e0\u679c\u63a8\u65ad\u7684Causal Action Influence Score (CAIS)\u5185\u5728\u5956\u52b1\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u6807\u51c6\u5f3a\u5316\u5b66\u4e60\u5728\u566a\u58f0\u73af\u5883\u4e2d\u7684\u8106\u5f31\u6027\u95ee\u9898\u3002", "motivation": "\u4eba\u7c7b\u5a74\u513f\u80fd\u6709\u6548\u53d1\u73b0\u81ea\u8eab\u56e0\u679c\u6548\u5e94\uff0c\u800c\u6807\u51c6\u5f3a\u5316\u5b66\u4e60\u4f9d\u8d56\u76f8\u5173\u6027\u5956\u52b1\uff0c\u5728\u566a\u58f0\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\u3002", "method": "CAIS\u901a\u8fc7\u8ba1\u7b97\u52a8\u4f5c\u5bf9\u611f\u5b98\u7ed3\u679c\u5206\u5e03\u7684\u5f71\u54cd\uff081-Wasserstein\u8ddd\u79bb\uff09\u6765\u91cf\u5316\u52a8\u4f5c\u7684\u56e0\u679c\u5f71\u54cd\u3002", "result": "\u5728\u6a21\u62df\u5a74\u513f-\u79fb\u52a8\u73af\u5883\u4e2d\uff0cCAIS\u80fd\u6709\u6548\u8fc7\u6ee4\u566a\u58f0\u5e76\u5b66\u4e60\u6b63\u786e\u7b56\u7565\uff0c\u540c\u65f6\u91cd\u73b0\u4e86\u201c\u6d88\u9000\u7206\u53d1\u201d\u73b0\u8c61\u3002", "conclusion": "\u663e\u5f0f\u63a8\u65ad\u56e0\u679c\u6027\u662f\u53d1\u5c55\u7a33\u5065\u4ee3\u7406\u611f\u7684\u5173\u952e\u673a\u5236\uff0c\u4e3a\u81ea\u9002\u5e94\u81ea\u4e3b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5fc3\u7406\u5b66\u5408\u7406\u7684\u6846\u67b6\u3002"}}
{"id": "2507.15607", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15607", "abs": "https://arxiv.org/abs/2507.15607", "authors": ["Yanbo Chen", "Yunzhe Tan", "Yaojia Wang", "Zhengzhe Xu", "Junbo Tan", "Xueqian Wang"], "title": "A Universal Vehicle-Trailer Navigation System with Neural Kinematics and Online Residual Learning", "comment": "8 pages, 10 figures", "summary": "Autonomous navigation of vehicle-trailer systems is crucial in environments\nlike airports, supermarkets, and concert venues, where various types of\ntrailers are needed to navigate with different payloads and conditions.\nHowever, accurately modeling such systems remains challenging, especially for\ntrailers with castor wheels. In this work, we propose a novel universal\nvehicle-trailer navigation system that integrates a hybrid nominal kinematic\nmodel--combining classical nonholonomic constraints for vehicles and neural\nnetwork-based trailer kinematics--with a lightweight online residual learning\nmodule to correct real-time modeling discrepancies and disturbances.\nAdditionally, we develop a model predictive control framework with a weighted\nmodel combination strategy that improves long-horizon prediction accuracy and\nensures safer motion planning. Our approach is validated through extensive\nreal-world experiments involving multiple trailer types and varying payload\nconditions, demonstrating robust performance without manual tuning or\ntrailer-specific calibration.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u901a\u7528\u8f66\u8f86-\u62d6\u8f66\u5bfc\u822a\u7cfb\u7edf\uff0c\u7ed3\u5408\u6df7\u5408\u8fd0\u52a8\u5b66\u6a21\u578b\u548c\u5728\u7ebf\u6b8b\u5dee\u5b66\u4e60\u6a21\u5757\uff0c\u63d0\u9ad8\u4e86\u5bfc\u822a\u7cbe\u5ea6\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u8f66\u8f86-\u62d6\u8f66\u7cfb\u7edf\u5728\u590d\u6742\u73af\u5883\uff08\u5982\u673a\u573a\u3001\u8d85\u5e02\uff09\u4e2d\u7684\u7cbe\u786e\u5efa\u6a21\u548c\u5bfc\u822a\u4ecd\u5177\u6311\u6218\u6027\uff0c\u5c24\u5176\u662f\u5e26\u811a\u8f6e\u7684\u62d6\u8f66\u3002", "method": "\u7ed3\u5408\u7ecf\u5178\u975e\u5b8c\u6574\u7ea6\u675f\u7684\u8f66\u8f86\u6a21\u578b\u548c\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u62d6\u8f66\u8fd0\u52a8\u5b66\uff0c\u5e76\u5f15\u5165\u5728\u7ebf\u6b8b\u5dee\u5b66\u4e60\u6a21\u5757\uff1b\u5f00\u53d1\u4e86\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u6846\u67b6\u3002", "result": "\u901a\u8fc7\u591a\u79cd\u62d6\u8f66\u548c\u8d1f\u8f7d\u6761\u4ef6\u7684\u771f\u5b9e\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u7cfb\u7edf\u8868\u73b0\u7a33\u5065\uff0c\u65e0\u9700\u624b\u52a8\u8c03\u6574\u6216\u62d6\u8f66\u7279\u5b9a\u6821\u51c6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u901a\u7528\u8f66\u8f86-\u62d6\u8f66\u5bfc\u822a\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u591a\u6837\u5316\u573a\u666f\u3002"}}
{"id": "2507.15120", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.15120", "abs": "https://arxiv.org/abs/2507.15120", "authors": ["Stefan Borgwardt", "Duy Nhu", "Gabriele R\u00f6ger"], "title": "Automated planning with ontologies under coherence update semantics", "comment": null, "summary": "Standard automated planning employs first-order formulas under closed-world\nsemantics to achieve a goal with a given set of actions from an initial state.\nWe follow a line of research that aims to incorporate background knowledge into\nautomated planning problems, for example, by means of ontologies, which are\nusually interpreted under open-world semantics. We present a new approach for\nplanning with DL-Lite ontologies that combines the advantages of ontology-based\naction conditions provided by explicit-input knowledge and action bases (eKABs)\nand ontology-aware action effects under the coherence update semantics. We show\nthat the complexity of the resulting formalism is not higher than that of\nprevious approaches and provide an implementation via a polynomial compilation\ninto classical planning. An evaluation of existing and new benchmarks examines\nthe performance of a planning system on different variants of our compilation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408DL-Lite\u672c\u4f53\u548c\u81ea\u52a8\u5316\u89c4\u5212\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u663e\u5f0f\u8f93\u5165\u77e5\u8bc6\u548c\u52a8\u4f5c\u57fa\u7840\uff08eKABs\uff09\u5b9e\u73b0\u672c\u4f53\u611f\u77e5\u7684\u52a8\u4f5c\u6548\u679c\uff0c\u590d\u6742\u5ea6\u672a\u589e\u52a0\uff0c\u5e76\u901a\u8fc7\u591a\u9879\u5f0f\u7f16\u8bd1\u5b9e\u73b0\u3002", "motivation": "\u5c06\u80cc\u666f\u77e5\u8bc6\uff08\u5982\u672c\u4f53\uff09\u878d\u5165\u81ea\u52a8\u5316\u89c4\u5212\u95ee\u9898\uff0c\u4ee5\u63d0\u5347\u89c4\u5212\u80fd\u529b\u3002", "method": "\u7ed3\u5408DL-Lite\u672c\u4f53\u548ceKABs\uff0c\u5229\u7528\u663e\u5f0f\u8f93\u5165\u77e5\u8bc6\u548c\u4e00\u81f4\u6027\u66f4\u65b0\u8bed\u4e49\u5b9e\u73b0\u672c\u4f53\u611f\u77e5\u52a8\u4f5c\u6548\u679c\u3002", "result": "\u65b0\u65b9\u6cd5\u7684\u590d\u6742\u5ea6\u4e0e\u4e4b\u524d\u65b9\u6cd5\u76f8\u5f53\uff0c\u5e76\u901a\u8fc7\u591a\u9879\u5f0f\u7f16\u8bd1\u5b9e\u73b0\u9ad8\u6548\u89c4\u5212\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6027\u80fd\u6d4b\u8bd5\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4e3a\u89c4\u5212\u95ee\u9898\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u7684\u80cc\u666f\u77e5\u8bc6\u652f\u6301\u3002"}}
{"id": "2507.15608", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15608", "abs": "https://arxiv.org/abs/2507.15608", "authors": ["Johannes Hartwig", "Fabian Viessmann", "Dominik Henrich"], "title": "Optimizing Force Signals from Human Demonstrations of In-Contact Motions", "comment": "Accepted for publication in Annals of Scientific Society for\n  Assembly, Handling and Industrial Robotics 2024 (to appear)", "summary": "For non-robot-programming experts, kinesthetic guiding can be an intuitive\ninput method, as robot programming of in-contact tasks is becoming more\nprominent. However, imprecise and noisy input signals from human demonstrations\npose problems when reproducing motions directly or using the signal as input\nfor machine learning methods. This paper explores optimizing force signals to\ncorrespond better to the human intention of the demonstrated signal. We compare\ndifferent signal filtering methods and propose a peak detection method for\ndealing with first-contact deviations in the signal. The evaluation of these\nmethods considers a specialized error criterion between the input and the\nhuman-intended signal. In addition, we analyze the critical parameters'\ninfluence on the filtering methods. The quality for an individual motion could\nbe increased by up to \\SI{20}{\\percent} concerning the error criterion. The\nproposed contribution can improve the usability of robot programming and the\ninteraction between humans and robots.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u4f18\u5316\u529b\u4fe1\u53f7\u4ee5\u66f4\u597d\u5730\u53cd\u6620\u4eba\u7c7b\u6f14\u793a\u610f\u56fe\uff0c\u6bd4\u8f83\u4e86\u4e0d\u540c\u4fe1\u53f7\u6ee4\u6ce2\u65b9\u6cd5\u5e76\u63d0\u51fa\u5cf0\u503c\u68c0\u6d4b\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u673a\u5668\u4eba\u7f16\u7a0b\u7684\u53ef\u7528\u6027\u3002", "motivation": "\u9488\u5bf9\u975e\u673a\u5668\u4eba\u7f16\u7a0b\u4e13\u5bb6\uff0c\u76f4\u89c2\u7684\u8f93\u5165\u65b9\u6cd5\uff08\u5982\u529b\u5f15\u5bfc\uff09\u5728\u63a5\u89e6\u4efb\u52a1\u7f16\u7a0b\u4e2d\u8d8a\u6765\u8d8a\u91cd\u8981\uff0c\u4f46\u4eba\u7c7b\u6f14\u793a\u4e2d\u7684\u4e0d\u7cbe\u786e\u548c\u566a\u58f0\u4fe1\u53f7\u4f1a\u76f4\u63a5\u5f71\u54cd\u8fd0\u52a8\u518d\u73b0\u6216\u673a\u5668\u5b66\u4e60\u8f93\u5165\u3002", "method": "\u6bd4\u8f83\u4e86\u4e0d\u540c\u4fe1\u53f7\u6ee4\u6ce2\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5cf0\u503c\u68c0\u6d4b\u65b9\u6cd5\u5904\u7406\u9996\u6b21\u63a5\u89e6\u504f\u5dee\uff0c\u5e76\u5206\u6790\u4e86\u5173\u952e\u53c2\u6570\u5bf9\u6ee4\u6ce2\u65b9\u6cd5\u7684\u5f71\u54cd\u3002", "result": "\u901a\u8fc7\u4f18\u5316\u529b\u4fe1\u53f7\uff0c\u5355\u4e2a\u8fd0\u52a8\u7684\u8bef\u5dee\u6807\u51c6\u53ef\u63d0\u5347\u9ad8\u8fbe20%\uff0c\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u7f16\u7a0b\u7684\u53ef\u7528\u6027\u548c\u4eba\u673a\u4ea4\u4e92\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u6539\u5584\u4e86\u529b\u4fe1\u53f7\u4e0e\u4eba\u7c7b\u610f\u56fe\u7684\u5339\u914d\uff0c\u4e3a\u673a\u5668\u4eba\u7f16\u7a0b\u548c\u4eba\u673a\u4ea4\u4e92\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.15140", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15140", "abs": "https://arxiv.org/abs/2507.15140", "authors": ["Mohammad Mashayekhi", "Sara Ahmadi Majd", "Arian AmirAmjadi", "Parsa Hosseini"], "title": "Clinical Semantic Intelligence (CSI): Emulating the Cognitive Framework of the Expert Clinician for Comprehensive Oral Disease Diagnosis", "comment": null, "summary": "The diagnosis of oral diseases presents a problematic clinical challenge,\ncharacterized by a wide spectrum of pathologies with overlapping\nsymptomatology. To address this, we developed Clinical Semantic Intelligence\n(CSI), a novel artificial intelligence framework that diagnoses 118 different\noral diseases by computationally modeling the cognitive processes of an expert\nclinician. Our core hypothesis is that moving beyond simple pattern matching to\nemulate expert reasoning is critical to building clinically useful diagnostic\naids.\n  CSI's architecture integrates a fine-tuned multimodal CLIP model with a\nspecialized ChatGLM-6B language model. This system executes a Hierarchical\nDiagnostic Reasoning Tree (HDRT), a structured framework that distills the\nsystematic, multi-step logic of differential diagnosis. The framework operates\nin two modes: a Fast Mode for rapid screening and a Standard Mode that\nleverages the full HDRT for an interactive and in-depth diagnostic workup.\n  To train and validate our system, we curated a primary dataset of 4,310\nimages, supplemented by an external hold-out set of 176 images for final\nvalidation. A clinically-informed augmentation strategy expanded our training\ndata to over 30,000 image-text pairs. On a 431-image internal test set, CSI's\nFast Mode achieved an accuracy of 73.4%, which increased to 89.5% with the\nHDRT-driven Standard Mode. The performance gain is directly attributable to the\nhierarchical reasoning process. Herein, we detail the architectural philosophy,\ndevelopment, and rigorous evaluation of the CSI framework.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u540d\u4e3aCSI\u7684\u4eba\u5de5\u667a\u80fd\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u62df\u4e13\u5bb6\u4e34\u5e8a\u533b\u751f\u7684\u8ba4\u77e5\u8fc7\u7a0b\uff0c\u8bca\u65ad118\u79cd\u53e3\u8154\u75be\u75c5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8bca\u65ad\u51c6\u786e\u6027\u3002", "motivation": "\u53e3\u8154\u75be\u75c5\u8bca\u65ad\u56e0\u75c7\u72b6\u91cd\u53e0\u800c\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u8981\u8d85\u8d8a\u7b80\u5355\u6a21\u5f0f\u5339\u914d\u7684\u4e13\u5bb6\u63a8\u7406\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408\u591a\u6a21\u6001CLIP\u6a21\u578b\u548cChatGLM-6B\u8bed\u8a00\u6a21\u578b\uff0c\u91c7\u7528\u5206\u5c42\u8bca\u65ad\u63a8\u7406\u6811\uff08HDRT\uff09\u8fdb\u884c\u5feb\u901f\u548c\u6807\u51c6\u6a21\u5f0f\u8bca\u65ad\u3002", "result": "\u5728431\u5f20\u5185\u90e8\u6d4b\u8bd5\u56fe\u50cf\u4e0a\uff0c\u5feb\u901f\u6a21\u5f0f\u51c6\u786e\u7387\u4e3a73.4%\uff0c\u6807\u51c6\u6a21\u5f0f\u63d0\u5347\u81f389.5%\u3002", "conclusion": "CSI\u6846\u67b6\u901a\u8fc7\u5206\u5c42\u63a8\u7406\u663e\u8457\u63d0\u5347\u8bca\u65ad\u6027\u80fd\uff0c\u4e3a\u4e34\u5e8a\u8bca\u65ad\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2507.15649", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15649", "abs": "https://arxiv.org/abs/2507.15649", "authors": ["Haocheng Xu", "Haodong Zhang", "Zhenghan Chen", "Rong Xiong"], "title": "EMP: Executable Motion Prior for Humanoid Robot Standing Upper-body Motion Imitation", "comment": null, "summary": "To support humanoid robots in performing manipulation tasks, it is essential\nto study stable standing while accommodating upper-body motions. However, the\nlimited controllable range of humanoid robots in a standing position affects\nthe stability of the entire body. Thus we introduce a reinforcement learning\nbased framework for humanoid robots to imitate human upper-body motions while\nmaintaining overall stability. Our approach begins with designing a retargeting\nnetwork that generates a large-scale upper-body motion dataset for training the\nreinforcement learning (RL) policy, which enables the humanoid robot to track\nupper-body motion targets, employing domain randomization for enhanced\nrobustness. To avoid exceeding the robot's execution capability and ensure\nsafety and stability, we propose an Executable Motion Prior (EMP) module, which\nadjusts the input target movements based on the robot's current state. This\nadjustment improves standing stability while minimizing changes to motion\namplitude. We evaluate our framework through simulation and real-world tests,\ndemonstrating its practical applicability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u4f7f\u4eba\u5f62\u673a\u5668\u4eba\u6a21\u4eff\u4eba\u7c7b\u4e0a\u534a\u8eab\u52a8\u4f5c\u5e76\u4fdd\u6301\u6574\u4f53\u7a33\u5b9a\u6027\u3002", "motivation": "\u4eba\u5f62\u673a\u5668\u4eba\u5728\u7ad9\u7acb\u65f6\u53ef\u63a7\u8303\u56f4\u6709\u9650\uff0c\u5f71\u54cd\u5168\u8eab\u7a33\u5b9a\u6027\uff0c\u9700\u7814\u7a76\u5982\u4f55\u5728\u6267\u884c\u4e0a\u534a\u8eab\u52a8\u4f5c\u65f6\u4fdd\u6301\u7a33\u5b9a\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u91cd\u5b9a\u5411\u7f51\u7edc\u751f\u6210\u5927\u89c4\u6a21\u4e0a\u534a\u8eab\u52a8\u4f5c\u6570\u636e\u96c6\uff0c\u8bad\u7ec3\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u5e76\u7ed3\u5408\u9886\u57df\u968f\u673a\u5316\u63d0\u5347\u9c81\u68d2\u6027\uff1b\u63d0\u51fa\u53ef\u6267\u884c\u8fd0\u52a8\u5148\u9a8c\uff08EMP\uff09\u6a21\u5757\u8c03\u6574\u8f93\u5165\u76ee\u6807\u52a8\u4f5c\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u548c\u5b9e\u9645\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u6709\u6548\u63d0\u5347\u4eba\u5f62\u673a\u5668\u4eba\u5728\u6267\u884c\u4e0a\u534a\u8eab\u52a8\u4f5c\u65f6\u7684\u7a33\u5b9a\u6027\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2507.15143", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.15143", "abs": "https://arxiv.org/abs/2507.15143", "authors": ["Abderaouf Bahi", "Amel Ourici"], "title": "Can We Move Freely in NEOM's The Line? An Agent-Based Simulation of Human Mobility in a Futuristic Smart City", "comment": null, "summary": "This paper investigates the feasibility of human mobility in The Line, a\nproposed 170-kilometer linear smart city in NEOM, Saudi Arabia. To assess\nwhether citizens can move freely within this unprecedented urban topology, we\ndevelop a hybrid simulation framework that integrates agent-based modeling,\nreinforcement learning, supervised learning, and graph neural networks. The\nsimulation captures multi-modal transportation behaviors across 50 vertical\nlevels and varying density scenarios using both synthetic data and real-world\ntraces from high-density cities. Our experiments reveal that with the full\nAI-integrated architecture, agents achieved an average commute time of 7.8 to\n8.4 minutes, a satisfaction rate exceeding 89 percent, and a reachability index\nof over 91 percent, even during peak congestion periods. Ablation studies\nconfirmed that the removal of intelligent modules such as reinforcement\nlearning or graph neural networks significantly degrades performance, with\ncommute times increasing by up to 85 percent and reachability falling below 70\npercent. Environmental modeling further demonstrated low energy consumption and\nminimal CO2 emissions when electric modes are prioritized. The findings suggest\nthat freedom of movement is not only conceptually achievable in The Line, but\nalso operationally realistic if supported by adaptive AI systems, sustainable\ninfrastructure, and real-time feedback loops.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u6c99\u7279\u963f\u62c9\u4f2fNEOM\u7ebf\u6027\u667a\u80fd\u57ce\u5e02The Line\u4e2d\u4eba\u7c7b\u79fb\u52a8\u7684\u53ef\u884c\u6027\uff0c\u901a\u8fc7\u6df7\u5408\u6a21\u62df\u6846\u67b6\u9a8c\u8bc1\u4e86AI\u7cfb\u7edf\u652f\u6301\u4e0b\u7684\u81ea\u7531\u79fb\u52a8\u662f\u53ef\u884c\u7684\u3002", "motivation": "\u63a2\u7d22\u5728The Line\u8fd9\u79cd\u524d\u6240\u672a\u6709\u7684\u7ebf\u6027\u57ce\u5e02\u62d3\u6251\u4e2d\uff0c\u5c45\u6c11\u662f\u5426\u80fd\u5b9e\u73b0\u81ea\u7531\u79fb\u52a8\uff0c\u5e76\u8bc4\u4f30\u5176\u53ef\u884c\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u7ed3\u5408\u4ee3\u7406\u5efa\u6a21\u3001\u5f3a\u5316\u5b66\u4e60\u3001\u76d1\u7763\u5b66\u4e60\u548c\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u6df7\u5408\u6a21\u62df\u6846\u67b6\uff0c\u6a21\u62df\u591a\u6a21\u6001\u4ea4\u901a\u884c\u4e3a\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cAI\u7cfb\u7edf\u652f\u6301\u4e0b\uff0c\u5e73\u5747\u901a\u52e4\u65f6\u95f4\u4e3a7.8\u81f38.4\u5206\u949f\uff0c\u6ee1\u610f\u5ea6\u8d85\u8fc789%\uff0c\u53ef\u8fbe\u6027\u6307\u6570\u8d85\u8fc791%\u3002", "conclusion": "The Line\u4e2d\u7684\u81ea\u7531\u79fb\u52a8\u5728AI\u7cfb\u7edf\u3001\u53ef\u6301\u7eed\u57fa\u7840\u8bbe\u65bd\u548c\u5b9e\u65f6\u53cd\u9988\u652f\u6301\u4e0b\u662f\u53ef\u884c\u7684\u3002"}}
{"id": "2507.15677", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15677", "abs": "https://arxiv.org/abs/2507.15677", "authors": ["Huayue Liang", "Yanbo Chen", "Hongyang Cheng", "Yanzhao Yu", "Shoujie Li", "Junbo Tan", "Xueqian Wang", "Long Zeng"], "title": "Data-Driven MPC with Data Selection for Flexible Cable-Driven Robotic Arms", "comment": null, "summary": "Flexible cable-driven robotic arms (FCRAs) offer dexterous and compliant\nmotion. Still, the inherent properties of cables, such as resilience,\nhysteresis, and friction, often lead to particular difficulties in modeling and\ncontrol. This paper proposes a model predictive control (MPC) method that\nrelies exclusively on input-output data, without a physical model, to improve\nthe control accuracy of FCRAs. First, we develop an implicit model based on\ninput-output data and integrate it into an MPC optimization framework. Second,\na data selection algorithm (DSA) is introduced to filter the data that best\ncharacterize the system, thereby reducing the solution time per step to\napproximately 4 ms, which is an improvement of nearly 80%. Lastly, the\ninfluence of hyperparameters on tracking error is investigated through\nsimulation. The proposed method has been validated on a real FCRA platform,\nincluding five-point positioning accuracy tests, a five-point response tracking\ntest, and trajectory tracking for letter drawing. The results demonstrate that\nthe average positioning accuracy is approximately 2.070 mm. Moreover, compared\nto the PID method with an average tracking error of 1.418{\\deg}, the proposed\nmethod achieves an average tracking error of 0.541{\\deg}.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u8f93\u5165\u8f93\u51fa\u6570\u636e\u7684\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u65b9\u6cd5\uff0c\u65e0\u9700\u7269\u7406\u6a21\u578b\u5373\u53ef\u63d0\u9ad8\u67d4\u6027\u7535\u7f06\u9a71\u52a8\u673a\u68b0\u81c2\uff08FCRA\uff09\u7684\u63a7\u5236\u7cbe\u5ea6\uff0c\u5e76\u901a\u8fc7\u6570\u636e\u9009\u62e9\u7b97\u6cd5\uff08DSA\uff09\u4f18\u5316\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u67d4\u6027\u7535\u7f06\u9a71\u52a8\u673a\u68b0\u81c2\uff08FCRA\uff09\u7684\u7535\u7f06\u7279\u6027\uff08\u5982\u5f39\u6027\u3001\u8fdf\u6ede\u548c\u6469\u64e6\uff09\u5bfc\u81f4\u5efa\u6a21\u548c\u63a7\u5236\u56f0\u96be\uff0c\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u7269\u7406\u6a21\u578b\uff0c\u96be\u4ee5\u7cbe\u786e\u63a7\u5236\u3002", "method": "1. \u57fa\u4e8e\u8f93\u5165\u8f93\u51fa\u6570\u636e\u6784\u5efa\u9690\u5f0f\u6a21\u578b\u5e76\u96c6\u6210\u5230MPC\u6846\u67b6\uff1b2. \u5f15\u5165\u6570\u636e\u9009\u62e9\u7b97\u6cd5\uff08DSA\uff09\u4f18\u5316\u6570\u636e\uff0c\u51cf\u5c11\u6bcf\u6b65\u8ba1\u7b97\u65f6\u95f4\u81f3\u7ea64\u6beb\u79d2\uff1b3. \u901a\u8fc7\u4eff\u771f\u7814\u7a76\u8d85\u53c2\u6570\u5bf9\u8ddf\u8e2a\u8bef\u5dee\u7684\u5f71\u54cd\u3002", "result": "\u5728\u771f\u5b9eFCRA\u5e73\u53f0\u4e0a\u9a8c\u8bc1\uff0c\u5b9a\u4f4d\u7cbe\u5ea6\u7ea62.070\u6beb\u7c73\uff0c\u8ddf\u8e2a\u8bef\u5dee0.541\u5ea6\uff08\u4f18\u4e8ePID\u65b9\u6cd5\u76841.418\u5ea6\uff09\uff0c\u8ba1\u7b97\u6548\u7387\u63d0\u534780%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86FCRA\u7684\u63a7\u5236\u7cbe\u5ea6\u548c\u8ba1\u7b97\u6548\u7387\uff0c\u9002\u7528\u4e8e\u590d\u6742\u4efb\u52a1\u5982\u8f68\u8ff9\u8ddf\u8e2a\u3002"}}
{"id": "2507.15225", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15225", "abs": "https://arxiv.org/abs/2507.15225", "authors": ["Yichi Zhou", "Jianqiu Zhao", "Yongxin Zhang", "Bohan Wang", "Siran Wang", "Luoxin Chen", "Jiahui Wang", "Haowei Chen", "Allan Jie", "Xinbo Zhang", "Haocheng Wang", "Luong Trung", "Rong Ye", "Phan Nhat Hoang", "Huishuai Zhang", "Peng Sun", "Hang Li"], "title": "Solving Formal Math Problems by Decomposition and Iterative Reflection", "comment": null, "summary": "General-purpose Large Language Models (LLMs) have achieved remarkable success\nin intelligence, performing comparably to human experts on complex reasoning\ntasks such as coding and mathematical reasoning. However, generating formal\nproofs in specialized languages like Lean 4 remains a significant challenge for\nthese models, limiting their application in complex theorem proving and\nautomated verification. Current approaches typically require specializing\nmodels through fine-tuning on dedicated formal corpora, incurring high costs\nfor data collection and training. In this work, we introduce \\textbf{Delta\nProver}, an agent-based framework that orchestrates the interaction between a\ngeneral-purpose LLM and the Lean 4 proof environment. Delta Prover leverages\nthe reflection and reasoning capabilities of general-purpose LLMs to\ninteractively construct formal proofs in Lean 4, circumventing the need for\nmodel specialization. At its core, the agent integrates two novel,\ninterdependent components: an algorithmic framework for reflective\ndecomposition and iterative proof repair, and a custom Domain-Specific Language\n(DSL) built upon Lean 4 for streamlined subproblem management. \\textbf{Delta\nProver achieves a state-of-the-art 95.9\\% success rate on the miniF2F-test\nbenchmark, surpassing all existing approaches, including those requiring model\nspecialization.} Furthermore, Delta Prover exhibits a significantly stronger\ntest-time scaling law compared to standard Best-of-N proof strategies.\nCrucially, our findings demonstrate that general-purpose LLMs, when guided by\nan effective agentic structure, possess substantial untapped theorem-proving\ncapabilities. This presents a computationally efficient alternative to\nspecialized models for robust automated reasoning in formal environments.", "AI": {"tldr": "Delta Prover\u662f\u4e00\u4e2a\u57fa\u4e8e\u4ee3\u7406\u7684\u6846\u67b6\uff0c\u5229\u7528\u901a\u7528LLM\u4e0eLean 4\u8bc1\u660e\u73af\u5883\u4ea4\u4e92\uff0c\u65e0\u9700\u6a21\u578b\u4e13\u4e1a\u5316\u5373\u53ef\u9ad8\u6548\u751f\u6210\u5f62\u5f0f\u5316\u8bc1\u660e\uff0c\u5728miniF2F-test\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523095.9%\u7684\u6210\u529f\u7387\u3002", "motivation": "\u901a\u7528LLM\u5728\u5f62\u5f0f\u5316\u8bc1\u660e\uff08\u5982Lean 4\uff09\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u73b0\u6709\u65b9\u6cd5\u9700\u901a\u8fc7\u4e13\u4e1a\u5316\u6a21\u578b\u5b9e\u73b0\uff0c\u6210\u672c\u9ad8\u6602\u3002Delta Prover\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "Delta Prover\u7ed3\u5408\u4e86\u53cd\u5c04\u5206\u89e3\u4e0e\u8fed\u4ee3\u8bc1\u660e\u4fee\u590d\u7684\u7b97\u6cd5\u6846\u67b6\uff0c\u4ee5\u53ca\u57fa\u4e8eLean 4\u7684\u5b9a\u5236DSL\uff0c\u901a\u8fc7\u4ee3\u7406\u7ed3\u6784\u5f15\u5bfcLLM\u751f\u6210\u8bc1\u660e\u3002", "result": "\u5728miniF2F-test\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523095.9%\u7684\u6210\u529f\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u6d4b\u8bd5\u65f6\u6269\u5c55\u6027\u66f4\u5f3a\u3002", "conclusion": "\u901a\u7528LLM\u5728\u6709\u6548\u4ee3\u7406\u7ed3\u6784\u5f15\u5bfc\u4e0b\u5177\u5907\u5f3a\u5927\u5b9a\u7406\u8bc1\u660e\u6f5c\u529b\uff0c\u4e3a\u5f62\u5f0f\u5316\u73af\u5883\u4e2d\u7684\u81ea\u52a8\u5316\u63a8\u7406\u63d0\u4f9b\u4e86\u9ad8\u6548\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2507.15693", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15693", "abs": "https://arxiv.org/abs/2507.15693", "authors": ["Georges Chebly", "Spencer Little", "Nisal Perera", "Aliya Abedeen", "Ken Suzuki", "Donghyun Kim"], "title": "Strong, Accurate, and Low-Cost Robot Manipulator", "comment": null, "summary": "This paper presents Forte, a fully 3D-printable, 6-DoF robotic arm designed\nto achieve near industrial-grade performance - 0.63 kg payload, 0.467 m reach,\nand sub-millimeter repeatability - at a material cost under $215. As an\naccessible robot for broad applications across classroom education to AI\nexperiments, Forte pushes forward the performance limitations of existing\nlow-cost educational arms. We introduce a cost-effective mechanical design that\ncombines capstan-based cable drives, timing belts, simple tensioning\nmechanisms, and lightweight 3D-printed structures, along with topology\noptimization for structural stiffness. Through careful drivetrain engineering,\nwe minimize backlash and maintain control fidelity without relying on\nhigh-power electronics or expensive manufacturing processes. Experimental\nvalidation demonstrates that Forte achieves high repeatability and load\ncapacity, offering a compelling robotic platform for both classroom instruction\nand advanced robotics research.", "AI": {"tldr": "Forte\u662f\u4e00\u6b3e\u51683D\u6253\u5370\u30016\u81ea\u7531\u5ea6\u673a\u68b0\u81c2\uff0c\u4ee5\u4f4e\u4e8e215\u7f8e\u5143\u7684\u6210\u672c\u5b9e\u73b0\u63a5\u8fd1\u5de5\u4e1a\u7ea7\u6027\u80fd\uff080.63 kg\u8d1f\u8f7d\u30010.467 m\u5de5\u4f5c\u8303\u56f4\u3001\u4e9a\u6beb\u7c73\u7ea7\u91cd\u590d\u7cbe\u5ea6\uff09\uff0c\u9002\u7528\u4e8e\u6559\u80b2\u548cAI\u5b9e\u9a8c\u3002", "motivation": "\u7a81\u7834\u73b0\u6709\u4f4e\u6210\u672c\u6559\u80b2\u673a\u68b0\u81c2\u7684\u6027\u80fd\u9650\u5236\uff0c\u63d0\u4f9b\u4e00\u79cd\u7ecf\u6d4e\u5b9e\u60e0\u4e14\u9ad8\u6027\u80fd\u7684\u673a\u5668\u4eba\u5e73\u53f0\u3002", "method": "\u91c7\u7528\u57fa\u4e8ecapstan\u7684\u7535\u7f06\u9a71\u52a8\u3001\u540c\u6b65\u5e26\u3001\u7b80\u5355\u5f20\u7d27\u673a\u6784\u548c\u8f7b\u91cf\u53163D\u6253\u5370\u7ed3\u6784\uff0c\u7ed3\u5408\u62d3\u6251\u4f18\u5316\u63d0\u5347\u521a\u5ea6\uff0c\u5e76\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u4f20\u52a8\u7cfb\u7edf\u51cf\u5c11\u80cc\u9699\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u663e\u793aForte\u5177\u6709\u9ad8\u91cd\u590d\u7cbe\u5ea6\u548c\u8d1f\u8f7d\u80fd\u529b\u3002", "conclusion": "Forte\u4e3a\u8bfe\u5802\u6559\u5b66\u548c\u9ad8\u7ea7\u673a\u5668\u4eba\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6027\u80fd\u4e14\u7ecf\u6d4e\u5b9e\u60e0\u7684\u5e73\u53f0\u3002"}}
{"id": "2507.15239", "categories": ["cs.AI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.15239", "abs": "https://arxiv.org/abs/2507.15239", "authors": ["Qianchao Wang", "Yuxuan Ding", "Chuanzhen Jia", "Zhe Li", "Yaping Du"], "title": "Explainable Artificial Intelligence based Soft Evaluation Indicator for Arc Fault Diagnosis", "comment": null, "summary": "Novel AI-based arc fault diagnosis models have demonstrated outstanding\nperformance in terms of classification accuracy. However, an inherent problem\nis whether these models can actually be trusted to find arc faults. In this\nlight, this work proposes a soft evaluation indicator that explains the outputs\nof arc fault diagnosis models, by defining the the correct explanation of arc\nfaults and leveraging Explainable Artificial Intelligence and real arc fault\nexperiments. Meanwhile, a lightweight balanced neural network is proposed to\nguarantee competitive accuracy and soft feature extraction score. In our\nexperiments, several traditional machine learning methods and deep learning\nmethods across two arc fault datasets with different sample times and noise\nlevels are utilized to test the effectiveness of the soft evaluation indicator.\nThrough this approach, the arc fault diagnosis models are easy to understand\nand trust, allowing practitioners to make informed and trustworthy decisions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8f6f\u8bc4\u4f30\u6307\u6807\uff0c\u7ed3\u5408\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\u548c\u771f\u5b9e\u7535\u5f27\u6545\u969c\u5b9e\u9a8c\uff0c\u89e3\u91ca\u7535\u5f27\u6545\u969c\u8bca\u65ad\u6a21\u578b\u7684\u8f93\u51fa\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u5e73\u8861\u795e\u7ecf\u7f51\u7edc\u4ee5\u4fdd\u8bc1\u51c6\u786e\u6027\u548c\u7279\u5f81\u63d0\u53d6\u80fd\u529b\u3002", "motivation": "\u73b0\u6709AI\u7535\u5f27\u6545\u969c\u8bca\u65ad\u6a21\u578b\u867d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u53ef\u4fe1\u5ea6\u5b58\u7591\uff0c\u9700\u4e00\u79cd\u65b9\u6cd5\u89e3\u91ca\u6a21\u578b\u8f93\u51fa\u4ee5\u589e\u5f3a\u4fe1\u4efb\u3002", "method": "\u5b9a\u4e49\u7535\u5f27\u6545\u969c\u7684\u6b63\u786e\u89e3\u91ca\uff0c\u5229\u7528\u53ef\u89e3\u91caAI\u548c\u771f\u5b9e\u5b9e\u9a8c\uff0c\u63d0\u51fa\u8f6f\u8bc4\u4f30\u6307\u6807\u548c\u8f7b\u91cf\u7ea7\u5e73\u8861\u795e\u7ecf\u7f51\u7edc\u3002", "result": "\u901a\u8fc7\u591a\u6570\u636e\u96c6\u548c\u566a\u58f0\u6c34\u5e73\u6d4b\u8bd5\uff0c\u9a8c\u8bc1\u4e86\u8f6f\u8bc4\u4f30\u6307\u6807\u7684\u6709\u6548\u6027\uff0c\u6a21\u578b\u66f4\u6613\u7406\u89e3\u548c\u4fe1\u4efb\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u5347\u4e86\u7535\u5f27\u6545\u969c\u8bca\u65ad\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u4fe1\u5ea6\uff0c\u652f\u6301\u5b9e\u8df5\u8005\u505a\u51fa\u53ef\u9760\u51b3\u7b56\u3002"}}
{"id": "2507.15710", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15710", "abs": "https://arxiv.org/abs/2507.15710", "authors": ["Lu Huang", "Lingxiao Meng", "Jiankun Wang", "Xingjian Jing"], "title": "Selective Densification for Rapid Motion Planning in High Dimensions with Narrow Passages", "comment": null, "summary": "Sampling-based algorithms are widely used for motion planning in\nhigh-dimensional configuration spaces. However, due to low sampling efficiency,\ntheir performance often diminishes in complex configuration spaces with narrow\ncorridors. Existing approaches address this issue using handcrafted or learned\nheuristics to guide sampling toward useful regions. Unfortunately, these\nstrategies often lack generalizability to various problems or require extensive\nprior training. In this paper, we propose a simple yet efficient sampling-based\nplanning framework along with its bidirectional version that overcomes these\nissues by integrating different levels of planning granularity. Our approach\nprobes configuration spaces with uniform random samples at varying resolutions\nand explores these multi-resolution samples online with a bias towards sparse\nsamples when traveling large free configuration spaces. By seamlessly\ntransitioning between sparse and dense samples, our approach can navigate\ncomplex configuration spaces while maintaining planning speed and completeness.\nThe simulation results demonstrate that our approach outperforms several\nstate-of-the-art sampling-based planners in $\\mathbb{SE}(2)$, $\\mathbb{SE}(3)$,\nand $\\mathbb{R}^{14}$ with challenging terrains. Furthermore, experiments\nconducted with the Franka Emika Panda robot operating in a constrained\nworkspace provide additional evidence of the superiority of the proposed\nmethod.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u591a\u5206\u8fa8\u7387\u91c7\u6837\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u91c7\u6837\u5bc6\u5ea6\u89e3\u51b3\u590d\u6742\u914d\u7f6e\u7a7a\u95f4\u4e2d\u7684\u5bfc\u822a\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u91c7\u6837\u89c4\u5212\u7b97\u6cd5\u5728\u590d\u6742\u914d\u7f6e\u7a7a\u95f4\u4e2d\u6548\u7387\u4f4e\uff0c\u4e14\u542f\u53d1\u5f0f\u65b9\u6cd5\u7f3a\u4e4f\u901a\u7528\u6027\u6216\u9700\u5927\u91cf\u8bad\u7ec3\u3002", "method": "\u7ed3\u5408\u4e0d\u540c\u7c92\u5ea6\u7684\u89c4\u5212\uff0c\u52a8\u6001\u8c03\u6574\u7a00\u758f\u4e0e\u5bc6\u96c6\u91c7\u6837\uff0c\u63d0\u5347\u5bfc\u822a\u6548\u7387\u3002", "result": "\u5728\u591a\u79cd\u914d\u7f6e\u7a7a\u95f4\u548c\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u73af\u5883\u4e2d\u9ad8\u6548\u4e14\u901a\u7528\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002"}}
{"id": "2507.15253", "categories": ["cs.AI", "cs.LG", "cs.SI"], "pdf": "https://arxiv.org/pdf/2507.15253", "abs": "https://arxiv.org/abs/2507.15253", "authors": ["Zhaochen Guo", "Zhixiang Shen", "Xuanting Xie", "Liangjian Wen", "Zhao Kang"], "title": "Disentangling Homophily and Heterophily in Multimodal Graph Clustering", "comment": "Appear in ACM Multimedia 2025", "summary": "Multimodal graphs, which integrate unstructured heterogeneous data with\nstructured interconnections, offer substantial real-world utility but remain\ninsufficiently explored in unsupervised learning. In this work, we initiate the\nstudy of multimodal graph clustering, aiming to bridge this critical gap.\nThrough empirical analysis, we observe that real-world multimodal graphs often\nexhibit hybrid neighborhood patterns, combining both homophilic and\nheterophilic relationships. To address this challenge, we propose a novel\nframework -- \\textsc{Disentangled Multimodal Graph Clustering (DMGC)} -- which\ndecomposes the original hybrid graph into two complementary views: (1) a\nhomophily-enhanced graph that captures cross-modal class consistency, and (2)\nheterophily-aware graphs that preserve modality-specific inter-class\ndistinctions. We introduce a \\emph{Multimodal Dual-frequency Fusion} mechanism\nthat jointly filters these disentangled graphs through a dual-pass strategy,\nenabling effective multimodal integration while mitigating category confusion.\nOur self-supervised alignment objectives further guide the learning process\nwithout requiring labels. Extensive experiments on both multimodal and\nmulti-relational graph datasets demonstrate that DMGC achieves state-of-the-art\nperformance, highlighting its effectiveness and generalizability across diverse\nsettings. Our code is available at https://github.com/Uncnbb/DMGC.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u65e0\u76d1\u7763\u591a\u6a21\u6001\u56fe\u805a\u7c7b\u6846\u67b6DMGC\uff0c\u901a\u8fc7\u5206\u89e3\u6df7\u5408\u56fe\u5e76\u5f15\u5165\u53cc\u9891\u878d\u5408\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u591a\u6a21\u6001\u6570\u636e\u7684\u6709\u6548\u805a\u7c7b\u3002", "motivation": "\u591a\u6a21\u6001\u56fe\u5728\u73b0\u5b9e\u4e2d\u6709\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5176\u65e0\u76d1\u7763\u5b66\u4e60\u7814\u7a76\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u6df7\u5408\u90bb\u57df\u6a21\u5f0f\uff08\u540c\u8d28\u6027\u548c\u5f02\u8d28\u6027\u5e76\u5b58\uff09\u7684\u6311\u6218\u5c1a\u672a\u89e3\u51b3\u3002", "method": "DMGC\u5c06\u6df7\u5408\u56fe\u5206\u89e3\u4e3a\u540c\u8d28\u6027\u589e\u5f3a\u56fe\u548c\u5f02\u8d28\u6027\u611f\u77e5\u56fe\uff0c\u901a\u8fc7\u53cc\u9891\u878d\u5408\u673a\u5236\u548c\u81ea\u76d1\u7763\u5bf9\u9f50\u76ee\u6807\u5b9e\u73b0\u805a\u7c7b\u3002", "result": "\u5b9e\u9a8c\u8868\u660eDMGC\u5728\u591a\u6a21\u6001\u548c\u591a\u5173\u7cfb\u56fe\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "DMGC\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u56fe\u805a\u7c7b\u7684\u6311\u6218\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u9002\u7528\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.15716", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15716", "abs": "https://arxiv.org/abs/2507.15716", "authors": ["Ziyu Wan", "Lin Zhao"], "title": "DiffPF: Differentiable Particle Filtering with Generative Sampling via Conditional Diffusion Models", "comment": null, "summary": "This paper proposes DiffPF, a differentiable particle filter that leverages\ndiffusion models for state estimation in dynamic systems. Unlike conventional\ndifferentiable particle filters, which require importance weighting and\ntypically rely on predefined or low-capacity proposal distributions. DiffPF\nlearns a flexible posterior sampler by conditioning a diffusion model on\npredicted particles and the current observation. This enables accurate,\nequally-weighted sampling from complex, high-dimensional, and multimodal\nfiltering distributions. We evaluate DiffPF across a range of scenarios,\nincluding both unimodal and highly multimodal distributions, and test it on\nsimulated as well as real-world tasks, where it consistently outperforms\nexisting filtering baselines. In particular, DiffPF achieves an 82.8%\nimprovement in estimation accuracy on a highly multimodal global localization\nbenchmark, and a 26% improvement on the real-world KITTI visual odometry\nbenchmark, compared to state-of-the-art differentiable filters. To the best of\nour knowledge, DiffPF is the first method to integrate conditional diffusion\nmodels into particle filtering, enabling high-quality posterior sampling that\nproduces more informative particles and significantly improves state\nestimation.", "AI": {"tldr": "DiffPF\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u53ef\u5fae\u5206\u7c92\u5b50\u6ee4\u6ce2\u5668\uff0c\u7528\u4e8e\u52a8\u6001\u7cfb\u7edf\u7684\u72b6\u6001\u4f30\u8ba1\uff0c\u901a\u8fc7\u5b66\u4e60\u7075\u6d3b\u7684\u540e\u9a8c\u91c7\u6837\u5668\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u9ad8\u7ef4\u591a\u6a21\u6001\u5206\u5e03\u4e0b\u7684\u4f30\u8ba1\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edf\u53ef\u5fae\u5206\u7c92\u5b50\u6ee4\u6ce2\u5668\u4f9d\u8d56\u9884\u5b9a\u4e49\u6216\u4f4e\u5bb9\u91cf\u63d0\u8bae\u5206\u5e03\uff0c\u9650\u5236\u4e86\u5176\u5728\u590d\u6742\u5206\u5e03\u4e0b\u7684\u6027\u80fd\u3002DiffPF\u65e8\u5728\u901a\u8fc7\u6269\u6563\u6a21\u578b\u5b9e\u73b0\u66f4\u7075\u6d3b\u3001\u51c6\u786e\u7684\u540e\u9a8c\u91c7\u6837\u3002", "method": "DiffPF\u5229\u7528\u6269\u6563\u6a21\u578b\uff0c\u57fa\u4e8e\u9884\u6d4b\u7c92\u5b50\u548c\u5f53\u524d\u89c2\u6d4b\u6761\u4ef6\u5316\uff0c\u5b66\u4e60\u540e\u9a8c\u91c7\u6837\u5668\uff0c\u5b9e\u73b0\u9ad8\u7ef4\u591a\u6a21\u6001\u5206\u5e03\u4e0b\u7684\u7b49\u6743\u91cd\u91c7\u6837\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4efb\u52a1\u4e2d\uff0cDiffPF\u8868\u73b0\u4f18\u5f02\uff0c\u591a\u6a21\u6001\u5168\u5c40\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u4f30\u8ba1\u7cbe\u5ea6\u63d0\u534782.8%\uff0cKITTI\u89c6\u89c9\u91cc\u7a0b\u8ba1\u4efb\u52a1\u4e2d\u63d0\u534726%\u3002", "conclusion": "DiffPF\u9996\u6b21\u5c06\u6761\u4ef6\u6269\u6563\u6a21\u578b\u5f15\u5165\u7c92\u5b50\u6ee4\u6ce2\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u540e\u9a8c\u91c7\u6837\uff0c\u663e\u8457\u63d0\u5347\u4e86\u72b6\u6001\u4f30\u8ba1\u6027\u80fd\u3002"}}
{"id": "2507.15268", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.15268", "abs": "https://arxiv.org/abs/2507.15268", "authors": ["Junhyeong Lee", "Joon-Young Kim", "Heekyu Kim", "Inhyo Lee", "Seunghwa Ryu"], "title": "IM-Chat: A Multi-agent LLM-based Framework for Knowledge Transfer in Injection Molding Industry", "comment": null, "summary": "The injection molding industry faces critical challenges in preserving and\ntransferring field knowledge, particularly as experienced workers retire and\nmultilingual barriers hinder effective communication. This study introduces\nIM-Chat, a multi-agent framework based on large language models (LLMs),\ndesigned to facilitate knowledge transfer in injection molding. IM-Chat\nintegrates both limited documented knowledge (e.g., troubleshooting tables,\nmanuals) and extensive field data modeled through a data-driven process\ncondition generator that infers optimal manufacturing settings from\nenvironmental inputs such as temperature and humidity, enabling robust and\ncontext-aware task resolution. By adopting a retrieval-augmented generation\n(RAG) strategy and tool-calling agents within a modular architecture, IM-Chat\nensures adaptability without the need for fine-tuning. Performance was assessed\nacross 100 single-tool and 60 hybrid tasks for GPT-4o, GPT-4o-mini, and\nGPT-3.5-turbo by domain experts using a 10-point rubric focused on relevance\nand correctness, and was further supplemented by automated evaluation using\nGPT-4o guided by a domain-adapted instruction prompt. The evaluation results\nindicate that more capable models tend to achieve higher accuracy, particularly\nin complex, tool-integrated scenarios. Overall, these findings demonstrate the\nviability of multi-agent LLM systems for industrial knowledge workflows and\nestablish IM-Chat as a scalable and generalizable approach to AI-assisted\ndecision support in manufacturing.", "AI": {"tldr": "IM-Chat\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u6ce8\u5851\u884c\u4e1a\u77e5\u8bc6\u4f20\u9012\u7684\u6311\u6218\uff0c\u7ed3\u5408\u6587\u6863\u77e5\u8bc6\u548c\u73b0\u573a\u6570\u636e\uff0c\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548c\u5de5\u5177\u8c03\u7528\u5b9e\u73b0\u9002\u5e94\u6027\u5f3a\u7684\u4efb\u52a1\u89e3\u51b3\u3002", "motivation": "\u6ce8\u5851\u884c\u4e1a\u9762\u4e34\u7ecf\u9a8c\u5de5\u4eba\u9000\u4f11\u548c\u591a\u8bed\u8a00\u969c\u788d\u5bfc\u81f4\u7684\u77e5\u8bc6\u4f20\u9012\u56f0\u96be\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7684\u77e5\u8bc6\u8f6c\u79fb\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6IM-Chat\uff0c\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u548c\u5de5\u5177\u8c03\u7528\uff0c\u5229\u7528\u6570\u636e\u9a71\u52a8\u7684\u5de5\u827a\u6761\u4ef6\u751f\u6210\u5668\u63a8\u65ad\u6700\u4f18\u5236\u9020\u8bbe\u7f6e\u3002", "result": "\u8bc4\u4f30\u663e\u793a\uff0c\u80fd\u529b\u66f4\u5f3a\u7684\u6a21\u578b\uff08\u5982GPT-4o\uff09\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u9a8c\u8bc1\u4e86IM-Chat\u5728\u5de5\u4e1a\u77e5\u8bc6\u5de5\u4f5c\u6d41\u4e2d\u7684\u53ef\u884c\u6027\u3002", "conclusion": "IM-Chat\u4e3a\u5236\u9020\u4e1a\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u901a\u7528\u7684AI\u8f85\u52a9\u51b3\u7b56\u652f\u6301\u65b9\u6cd5\u3002"}}
{"id": "2507.15729", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.15729", "abs": "https://arxiv.org/abs/2507.15729", "authors": ["Jens V. R\u00fcppel", "Andrey Rudenko", "Tim Schreiter", "Martin Magnusson", "Achim J. Lilienthal"], "title": "Gaze-supported Large Language Model Framework for Bi-directional Human-Robot Interaction", "comment": "This paper has been accepted to the 34th IEEE International\n  Conference on Robot and Human Interactive Communication (RO-MAN), which will\n  be held in Eindhoven, Netherlands on August 25-29, 2025. Copyright 2025 IEEE.\n  Personal use of this material is permitted. Permission from IEEE must be\n  obtained for all other uses", "summary": "The rapid development of Large Language Models (LLMs) creates an exciting\npotential for flexible, general knowledge-driven Human-Robot Interaction (HRI)\nsystems for assistive robots. Existing HRI systems demonstrate great progress\nin interpreting and following user instructions, action generation, and robot\ntask solving. On the other hand, bi-directional, multi-modal, and context-aware\nsupport of the user in collaborative tasks still remains an open challenge. In\nthis paper, we present a gaze- and speech-informed interface to the assistive\nrobot, which is able to perceive the working environment from multiple vision\ninputs and support the dynamic user in their tasks. Our system is designed to\nbe modular and transferable to adapt to diverse tasks and robots, and it is\ncapable of real-time use of language-based interaction state representation and\nfast on board perception modules. Its development was supported by multiple\npublic dissemination events, contributing important considerations for improved\nrobustness and user experience. Furthermore, in two lab studies, we compare the\nperformance and user ratings of our system with those of a traditional scripted\nHRI pipeline. Our findings indicate that an LLM-based approach enhances\nadaptability and marginally improves user engagement and task execution metrics\nbut may produce redundant output, while a scripted pipeline is well suited for\nmore straightforward tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u4eba\u673a\u4ea4\u4e92\uff08HRI\uff09\u7cfb\u7edf\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u8f93\u5165\uff08\u5982\u89c6\u7ebf\u548c\u8bed\u97f3\uff09\u652f\u6301\u52a8\u6001\u7528\u6237\u4efb\u52a1\uff0c\u5e76\u5bf9\u6bd4\u4e86\u5176\u4e0e\u4f20\u7edf\u811a\u672c\u5316HRI\u7cfb\u7edf\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709HRI\u7cfb\u7edf\u5728\u53cc\u5411\u3001\u591a\u6a21\u6001\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u534f\u4f5c\u4efb\u52a1\u652f\u6301\u65b9\u9762\u4ecd\u5b58\u5728\u6311\u6218\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7LLM\u63d0\u5347\u4ea4\u4e92\u7684\u7075\u6d3b\u6027\u548c\u9002\u5e94\u6027\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u6a21\u5757\u5316\u3001\u53ef\u8fc1\u79fb\u7684\u7cfb\u7edf\uff0c\u7ed3\u5408\u591a\u89c6\u89c9\u8f93\u5165\u548c\u5b9e\u65f6\u8bed\u8a00\u4ea4\u4e92\u72b6\u6001\u8868\u793a\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u4e0e\u4f20\u7edf\u811a\u672c\u5316HRI\u7cfb\u7edf\u5bf9\u6bd4\u3002", "result": "LLM\u65b9\u6cd5\u589e\u5f3a\u4e86\u9002\u5e94\u6027\u5e76\u7565\u5fae\u63d0\u5347\u4e86\u7528\u6237\u53c2\u4e0e\u5ea6\u548c\u4efb\u52a1\u6267\u884c\u6548\u679c\uff0c\u4f46\u53ef\u80fd\u4ea7\u751f\u5197\u4f59\u8f93\u51fa\uff1b\u811a\u672c\u5316\u7cfb\u7edf\u66f4\u9002\u5408\u7b80\u5355\u4efb\u52a1\u3002", "conclusion": "LLM\u9a71\u52a8\u7684HRI\u7cfb\u7edf\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u4f46\u9700\u4f18\u5316\u5197\u4f59\u95ee\u9898\uff1b\u811a\u672c\u5316\u7cfb\u7edf\u5728\u7b80\u5355\u4efb\u52a1\u4e2d\u4ecd\u5177\u4f18\u52bf\u3002"}}
{"id": "2507.15330", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15330", "abs": "https://arxiv.org/abs/2507.15330", "authors": ["Hammad Atta", "Muhammad Zeeshan Baig", "Yasir Mehmood", "Nadeem Shahzad", "Ken Huang", "Muhammad Aziz Ul Haq", "Muhammad Awais", "Kamal Ahmed"], "title": "QSAF: A Novel Mitigation Framework for Cognitive Degradation in Agentic AI", "comment": null, "summary": "We introduce Cognitive Degradation as a novel vulnerability class in agentic\nAI systems. Unlike traditional adversarial external threats such as prompt\ninjection, these failures originate internally, arising from memory starvation,\nplanner recursion, context flooding, and output suppression. These systemic\nweaknesses lead to silent agent drift, logic collapse, and persistent\nhallucinations over time. To address this class of failures, we introduce the\nQorvex Security AI Framework for Behavioral & Cognitive Resilience (QSAF Domain\n10), a lifecycle-aware defense framework defined by a six-stage cognitive\ndegradation lifecycle. The framework includes seven runtime controls\n(QSAF-BC-001 to BC-007) that monitor agent subsystems in real time and trigger\nproactive mitigation through fallback routing, starvation detection, and memory\nintegrity enforcement. Drawing from cognitive neuroscience, we map agentic\narchitectures to human analogs, enabling early detection of fatigue,\nstarvation, and role collapse. By introducing a formal lifecycle and real-time\nmitigation controls, this work establishes Cognitive Degradation as a critical\nnew class of AI system vulnerability and proposes the first cross-platform\ndefense model for resilient agentic behavior.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684AI\u7cfb\u7edf\u6f0f\u6d1e\u7c7b\u522b\u2014\u2014\u8ba4\u77e5\u9000\u5316\uff0c\u5e76\u63d0\u51fa\u4e86Qorvex\u5b89\u5168AI\u6846\u67b6\uff08QSAF Domain 10\uff09\u6765\u5e94\u5bf9\u6b64\u7c7b\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u7684\u5916\u90e8\u5a01\u80c1\uff08\u5982\u63d0\u793a\u6ce8\u5165\uff09\u65e0\u6cd5\u8986\u76d6AI\u7cfb\u7edf\u5185\u90e8\u7684\u8ba4\u77e5\u9000\u5316\u95ee\u9898\uff0c\u5982\u5185\u5b58\u9965\u997f\u3001\u89c4\u5212\u9012\u5f52\u7b49\uff0c\u8fd9\u4e9b\u95ee\u9898\u4f1a\u5bfc\u81f4AI\u884c\u4e3a\u5f02\u5e38\u3002", "method": "\u901a\u8fc7\u516d\u9636\u6bb5\u8ba4\u77e5\u9000\u5316\u751f\u547d\u5468\u671f\u548c\u4e03\u79cd\u5b9e\u65f6\u63a7\u5236\u63aa\u65bd\uff08QSAF-BC-001\u81f3BC-007\uff09\uff0c\u76d1\u63a7\u5e76\u7f13\u89e3\u8ba4\u77e5\u9000\u5316\u95ee\u9898\u3002", "result": "QSAF\u6846\u67b6\u80fd\u591f\u5b9e\u65f6\u68c0\u6d4b\u5e76\u7f13\u89e3\u8ba4\u77e5\u9000\u5316\u95ee\u9898\uff0c\u63d0\u5347AI\u7cfb\u7edf\u7684\u884c\u4e3a\u7a33\u5b9a\u6027\u3002", "conclusion": "\u8ba4\u77e5\u9000\u5316\u662fAI\u7cfb\u7edf\u7684\u91cd\u8981\u6f0f\u6d1e\u7c7b\u522b\uff0cQSAF\u6846\u67b6\u4e3a\u8de8\u5e73\u53f0\u9632\u5fa1\u63d0\u4f9b\u4e86\u9996\u4e2a\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.15782", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15782", "abs": "https://arxiv.org/abs/2507.15782", "authors": ["Ruochu Yang", "Yu Zhou", "Fumin Zhang", "Mengxue Hou"], "title": "Interleaved LLM and Motion Planning for Generalized Multi-Object Collection in Large Scene Graphs", "comment": null, "summary": "Household robots have been a longstanding research topic, but they still lack\nhuman-like intelligence, particularly in manipulating open-set objects and\nnavigating large environments efficiently and accurately. To push this\nboundary, we consider a generalized multi-object collection problem in large\nscene graphs, where the robot needs to pick up and place multiple objects\nacross multiple locations in a long mission of multiple human commands. This\nproblem is extremely challenging since it requires long-horizon planning in a\nvast action-state space under high uncertainties. To this end, we propose a\nnovel interleaved LLM and motion planning algorithm Inter-LLM. By designing a\nmultimodal action cost similarity function, our algorithm can both reflect the\nhistory and look into the future to optimize plans, striking a good balance of\nquality and efficiency. Simulation experiments demonstrate that compared with\nlatest works, our algorithm improves the overall mission performance by 30% in\nterms of fulfilling human commands, maximizing mission success rates, and\nminimizing mission costs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684Inter-LLM\u7b97\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408LLM\u548c\u8fd0\u52a8\u89c4\u5212\uff0c\u89e3\u51b3\u4e86\u5bb6\u5ead\u673a\u5668\u4eba\u5728\u591a\u5bf9\u8c61\u6536\u96c6\u4efb\u52a1\u4e2d\u7684\u957f\u65f6\u89c4\u5212\u95ee\u9898\uff0c\u6027\u80fd\u63d0\u534730%\u3002", "motivation": "\u5bb6\u5ead\u673a\u5668\u4eba\u5728\u5904\u7406\u5f00\u653e\u96c6\u5bf9\u8c61\u548c\u9ad8\u6548\u5bfc\u822a\u65b9\u9762\u7f3a\u4e4f\u4eba\u7c7b\u667a\u80fd\uff0c\u9700\u8981\u89e3\u51b3\u591a\u5bf9\u8c61\u6536\u96c6\u4efb\u52a1\u7684\u957f\u65f6\u89c4\u5212\u6311\u6218\u3002", "method": "\u8bbe\u8ba1\u4e86\u591a\u6a21\u6001\u52a8\u4f5c\u6210\u672c\u76f8\u4f3c\u6027\u51fd\u6570\uff0c\u7ed3\u5408LLM\u548c\u8fd0\u52a8\u89c4\u5212\uff0c\u4f18\u5316\u5386\u53f2\u4e0e\u672a\u6765\u89c4\u5212\u3002", "result": "\u4eff\u771f\u5b9e\u9a8c\u663e\u793a\uff0c\u7b97\u6cd5\u5728\u5b8c\u6210\u4efb\u52a1\u3001\u6210\u529f\u7387\u548c\u6210\u672c\u65b9\u9762\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u534730%\u3002", "conclusion": "Inter-LLM\u7b97\u6cd5\u5728\u591a\u5bf9\u8c61\u6536\u96c6\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u89c4\u5212\u3002"}}
{"id": "2507.15351", "categories": ["cs.AI", "cs.ET", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.15351", "abs": "https://arxiv.org/abs/2507.15351", "authors": ["Zijian Zhao", "Sen Li"], "title": "One Step is Enough: Multi-Agent Reinforcement Learning based on One-Step Policy Optimization for Order Dispatch on Ride-Sharing Platforms", "comment": null, "summary": "On-demand ride-sharing platforms face the fundamental challenge of\ndynamically bundling passengers with diverse origins and destinations and\nmatching them with vehicles in real time, all under significant uncertainty.\nRecently, MARL has emerged as a promising solution for this problem, leveraging\ndecentralized learning to address the curse of dimensionality caused by the\nlarge number of agents in the ride-hailing market and the resulting expansive\nstate and action spaces. However, conventional MARL-based ride-sharing\napproaches heavily rely on the accurate estimation of Q-values or V-values,\nwhich becomes problematic in large-scale, highly uncertain environments.\nSpecifically, most of these approaches adopt an independent paradigm,\nexacerbating this issue, as each agent treats others as part of the\nenvironment, leading to unstable training and substantial estimation bias in\nvalue functions. To address these challenges, we propose two novel alternative\nmethods that bypass value function estimation. First, we adapt GRPO to\nride-sharing, replacing the PPO baseline with the group average reward to\neliminate critic estimation errors and reduce training bias. Second, inspired\nby GRPO's full utilization of group reward information, we customize the PPO\nframework for ride-sharing platforms and show that, under a homogeneous fleet,\nthe optimal policy can be trained using only one-step rewards - a method we\nterm One-Step Policy Optimization (OSPO). Experiments on a real-world Manhattan\nride-hailing dataset demonstrate that both GRPO and OSPO achieve superior\nperformance across most scenarios, efficiently optimizing pickup times and the\nnumber of served orders using simple MLP networks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u65b0\u65b9\u6cd5\uff08GRPO\u548cOSPO\uff09\u89e3\u51b3\u52a8\u6001\u62fc\u8f66\u5e73\u53f0\u4e2d\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\uff0c\u907f\u514d\u4f20\u7edf\u65b9\u6cd5\u5bf9\u4ef7\u503c\u51fd\u6570\u4f30\u8ba1\u7684\u4f9d\u8d56\u3002", "motivation": "\u52a8\u6001\u62fc\u8f66\u5e73\u53f0\u9762\u4e34\u4e58\u5ba2\u4e0e\u8f66\u8f86\u5b9e\u65f6\u5339\u914d\u7684\u9ad8\u7ef4\u4e0d\u786e\u5b9a\u6027\u6311\u6218\uff0c\u4f20\u7edfMARL\u65b9\u6cd5\u56e0\u4f9d\u8d56\u51c6\u786e\u4ef7\u503c\u51fd\u6570\u4f30\u8ba1\u800c\u8868\u73b0\u4e0d\u4f73\u3002", "method": "1. \u91c7\u7528GRPO\uff0c\u7528\u7ec4\u5e73\u5747\u5956\u52b1\u66ff\u4ee3PPO\u57fa\u7ebf\u4ee5\u51cf\u5c11\u4f30\u8ba1\u8bef\u5dee\uff1b2. \u63d0\u51faOSPO\uff0c\u5229\u7528\u4e00\u6b65\u5956\u52b1\u8bad\u7ec3\u6700\u4f18\u7b56\u7565\u3002", "result": "\u5728\u771f\u5b9e\u66fc\u54c8\u987f\u6570\u636e\u96c6\u4e0a\uff0cGRPO\u548cOSPO\u5728\u591a\u6570\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u4f18\u5316\u4e86\u63a5\u5ba2\u65f6\u95f4\u548c\u8ba2\u5355\u670d\u52a1\u91cf\u3002", "conclusion": "GRPO\u548cOSPO\u901a\u8fc7\u7ed5\u8fc7\u4ef7\u503c\u51fd\u6570\u4f30\u8ba1\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u7684\u62fc\u8f66\u5339\u914d\u95ee\u9898\u3002"}}
{"id": "2507.15833", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15833", "abs": "https://arxiv.org/abs/2507.15833", "authors": ["Ian Chuang", "Andrew Lee", "Dechen Gao", "Jinyu Zou", "Iman Soltani"], "title": "Look, Focus, Act: Efficient and Robust Robot Learning via Human Gaze and Foveated Vision Transformers", "comment": "13 pages, 10 figures", "summary": "Human vision is a highly active process driven by gaze, which directs\nattention and fixation to task-relevant regions and dramatically reduces visual\nprocessing. In contrast, robot learning systems typically rely on passive,\nuniform processing of raw camera images. In this work, we explore how\nincorporating human-like active gaze into robotic policies can enhance both\nefficiency and performance. We build on recent advances in foveated image\nprocessing and apply them to an Active Vision robot system that emulates both\nhuman head movement and eye tracking. Extending prior work on the AV-ALOHA\nrobot simulation platform, we introduce a framework for simultaneously\ncollecting eye-tracking data and robot demonstrations from a human operator as\nwell as a simulation benchmark and dataset for training robot policies that\nincorporate human gaze. Given the widespread use of Vision Transformers (ViTs)\nin robot learning, we integrate gaze information into ViTs using a foveated\npatch tokenization scheme inspired by recent work in image segmentation.\nCompared to uniform patch tokenization, this significantly reduces the number\nof tokens-and thus computation-without sacrificing visual fidelity near regions\nof interest. We also explore two approaches to gaze imitation and prediction\nfrom human data. The first is a two-stage model that predicts gaze to guide\nfoveation and action; the second integrates gaze into the action space,\nallowing the policy to jointly predict gaze and actions end-to-end. Our results\nshow that our method for foveated robot vision not only drastically reduces\ncomputational overhead, but also improves performance for high precision tasks\nand robustness to unseen distractors. Together, these findings suggest that\nhuman-inspired visual processing offers a useful inductive bias for robotic\nvision systems. https://ian-chuang.github.io/gaze-av-aloha/", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5c06\u4eba\u7c7b\u4e3b\u52a8\u6ce8\u89c6\u673a\u5236\u5f15\u5165\u673a\u5668\u4eba\u89c6\u89c9\u7cfb\u7edf\uff0c\u4ee5\u63d0\u9ad8\u6548\u7387\u548c\u6027\u80fd\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6ce8\u89c6\u6570\u636e\u7684\u673a\u5668\u4eba\u7b56\u7565\u6846\u67b6\u3002", "motivation": "\u4eba\u7c7b\u89c6\u89c9\u901a\u8fc7\u4e3b\u52a8\u6ce8\u89c6\u9ad8\u6548\u5904\u7406\u4efb\u52a1\u76f8\u5173\u533a\u57df\uff0c\u800c\u673a\u5668\u4eba\u901a\u5e38\u88ab\u52a8\u5904\u7406\u56fe\u50cf\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u6ce8\u89c6\u673a\u5236\u63d0\u5347\u673a\u5668\u4eba\u89c6\u89c9\u7cfb\u7edf\u7684\u6027\u80fd\u3002", "method": "\u7ed3\u5408\u6ce8\u89c6\u6570\u636e\u548c\u673a\u5668\u4eba\u6f14\u793a\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eVision Transformers\u7684\u6ce8\u89c6\u5f15\u5bfc\u89c6\u89c9\u5904\u7406\u6846\u67b6\uff0c\u5305\u62ec\u6ce8\u89c6\u9884\u6d4b\u548c\u52a8\u4f5c\u8054\u5408\u9884\u6d4b\u4e24\u79cd\u65b9\u6cd5\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u5f00\u9500\uff0c\u63d0\u5347\u4e86\u9ad8\u7cbe\u5ea6\u4efb\u52a1\u7684\u6027\u80fd\u548c\u5bf9\u672a\u77e5\u5e72\u6270\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u4eba\u7c7b\u542f\u53d1\u7684\u89c6\u89c9\u5904\u7406\u4e3a\u673a\u5668\u4eba\u89c6\u89c9\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u5f52\u7eb3\u504f\u7f6e\u3002"}}
{"id": "2507.15356", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15356", "abs": "https://arxiv.org/abs/2507.15356", "authors": ["Lu Guo", "Yixiang Shan", "Zhengbang Zhu", "Qifan Liang", "Lichang Song", "Ting Long", "Weinan Zhang", "Yi Chang"], "title": "RAD: Retrieval High-quality Demonstrations to Enhance Decision-making", "comment": null, "summary": "Offline reinforcement learning (RL) enables agents to learn policies from\nfixed datasets, avoiding costly or unsafe environment interactions. However,\nits effectiveness is often limited by dataset sparsity and the lack of\ntransition overlap between suboptimal and expert trajectories, which makes\nlong-horizon planning particularly challenging. Prior solutions based on\nsynthetic data augmentation or trajectory stitching often fail to generalize to\nnovel states and rely on heuristic stitching points. To address these\nchallenges, we propose Retrieval High-quAlity Demonstrations (RAD) for\ndecision-making, which combines non-parametric retrieval with diffusion-based\ngenerative modeling. RAD dynamically retrieves high-return states from the\noffline dataset as target states based on state similarity and return\nestimation, and plans toward them using a condition-guided diffusion model.\nSuch retrieval-guided generation enables flexible trajectory stitching and\nimproves generalization when encountered with underrepresented or\nout-of-distribution states. Extensive experiments confirm that RAD achieves\ncompetitive or superior performance compared to baselines across diverse\nbenchmarks, validating its effectiveness.", "AI": {"tldr": "\u63d0\u51faRAD\u65b9\u6cd5\uff0c\u7ed3\u5408\u975e\u53c2\u6570\u68c0\u7d22\u4e0e\u6269\u6563\u6a21\u578b\uff0c\u89e3\u51b3\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u7a00\u758f\u6570\u636e\u548c\u8f68\u8ff9\u62fc\u63a5\u95ee\u9898\uff0c\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u56e0\u6570\u636e\u96c6\u7a00\u758f\u548c\u8f68\u8ff9\u91cd\u53e0\u4e0d\u8db3\u5bfc\u81f4\u957f\u65f6\u89c4\u5212\u56f0\u96be\uff0c\u73b0\u6709\u65b9\u6cd5\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002", "method": "RAD\u901a\u8fc7\u68c0\u7d22\u9ad8\u8d28\u91cf\u72b6\u6001\u4f5c\u4e3a\u76ee\u6807\uff0c\u7ed3\u5408\u6269\u6563\u6a21\u578b\u751f\u6210\u8f68\u8ff9\uff0c\u5b9e\u73b0\u7075\u6d3b\u62fc\u63a5\u548c\u6cdb\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660eRAD\u5728\u591a\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "RAD\u6709\u6548\u89e3\u51b3\u4e86\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6311\u6218\uff0c\u63d0\u5347\u4e86\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\u3002"}}
{"id": "2507.15411", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15411", "abs": "https://arxiv.org/abs/2507.15411", "authors": ["Wissam Gherissi", "Mehdi Acheli", "Joyce El Haddad", "Daniela Grigori"], "title": "Predictive Process Monitoring Using Object-centric Graph Embeddings", "comment": "ICSOC Workshops 2024, Dec 2024, Tunis, Tunisia", "summary": "Object-centric predictive process monitoring explores and utilizes\nobject-centric event logs to enhance process predictions. The main challenge\nlies in extracting relevant information and building effective models. In this\npaper, we propose an end-to-end model that predicts future process behavior,\nfocusing on two tasks: next activity prediction and next event time. The\nproposed model employs a graph attention network to encode activities and their\nrelationships, combined with an LSTM network to handle temporal dependencies.\nEvaluated on one reallife and three synthetic event logs, the model\ndemonstrates competitive performance compared to state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\u548cLSTM\u7684\u7aef\u5230\u7aef\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u672a\u6765\u6d41\u7a0b\u884c\u4e3a\uff0c\u5305\u62ec\u4e0b\u4e00\u6d3b\u52a8\u548c\u4e0b\u4e00\u4e8b\u4ef6\u65f6\u95f4\u3002", "motivation": "\u5229\u7528\u5bf9\u8c61\u4e2d\u5fc3\u4e8b\u4ef6\u65e5\u5fd7\u63d0\u5347\u6d41\u7a0b\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "method": "\u7ed3\u5408\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\u7f16\u7801\u6d3b\u52a8\u53ca\u5176\u5173\u7cfb\uff0cLSTM\u5904\u7406\u65f6\u95f4\u4f9d\u8d56\u3002", "result": "\u5728\u771f\u5b9e\u548c\u5408\u6210\u4e8b\u4ef6\u65e5\u5fd7\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u6a21\u578b\u5728\u9884\u6d4b\u4efb\u52a1\u4e2d\u5177\u6709\u7ade\u4e89\u529b\uff0c\u4e3a\u6d41\u7a0b\u76d1\u63a7\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2507.15457", "categories": ["cs.AI", "I.2.8"], "pdf": "https://arxiv.org/pdf/2507.15457", "abs": "https://arxiv.org/abs/2507.15457", "authors": ["Orlenys L\u00f3pez-Pintado", "Jannis Rosenbaum", "Marlon Dumas"], "title": "Optimization of Activity Batching Policies in Business Processes", "comment": null, "summary": "In business processes, activity batching refers to packing multiple activity\ninstances for joint execution. Batching allows managers to trade off cost and\nprocessing effort against waiting time. Larger and less frequent batches may\nlower costs by reducing processing effort and amortizing fixed costs, but they\ncreate longer waiting times. In contrast, smaller and more frequent batches\nreduce waiting times but increase fixed costs and processing effort. A batching\npolicy defines how activity instances are grouped into batches and when each\nbatch is activated. This paper addresses the problem of discovering batching\npolicies that strike optimal trade-offs between waiting time, processing\neffort, and cost. The paper proposes a Pareto optimization approach that starts\nfrom a given set (possibly empty) of activity batching policies and generates\nalternative policies for each batched activity via intervention heuristics.\nEach heuristic identifies an opportunity to improve an activity's batching\npolicy with respect to a metric (waiting time, processing time, cost, or\nresource utilization) and an associated adjustment to the activity's batching\npolicy (the intervention). The impact of each intervention is evaluated via\nsimulation. The intervention heuristics are embedded in an optimization\nmeta-heuristic that triggers interventions to iteratively update the Pareto\nfront of the interventions identified so far. The paper considers three\nmeta-heuristics: hill-climbing, simulated annealing, and reinforcement\nlearning. An experimental evaluation compares the proposed approach based on\nintervention heuristics against the same (non-heuristic guided) meta-heuristics\nbaseline regarding convergence, diversity, and cycle time gain of\nPareto-optimal policies.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5e15\u7d2f\u6258\u4f18\u5316\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5e72\u9884\u542f\u53d1\u5f0f\u53d1\u73b0\u4e1a\u52a1\u8fc7\u7a0b\u4e2d\u6d3b\u52a8\u6279\u5904\u7406\u7684\u6700\u4f18\u7b56\u7565\uff0c\u5e73\u8861\u7b49\u5f85\u65f6\u95f4\u3001\u5904\u7406\u6210\u672c\u548c\u8d44\u6e90\u5229\u7528\u7387\u3002", "motivation": "\u4e1a\u52a1\u8fc7\u7a0b\u4e2d\uff0c\u6279\u5904\u7406\u7b56\u7565\u9700\u8981\u5728\u6210\u672c\u548c\u7b49\u5f85\u65f6\u95f4\u4e4b\u95f4\u6743\u8861\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u81ea\u52a8\u53d1\u73b0\u6700\u4f18\u7b56\u7565\u7684\u80fd\u529b\u3002", "method": "\u91c7\u7528\u5e15\u7d2f\u6258\u4f18\u5316\u65b9\u6cd5\uff0c\u7ed3\u5408\u5e72\u9884\u542f\u53d1\u5f0f\uff08\u5982\u722c\u5c71\u6cd5\u3001\u6a21\u62df\u9000\u706b\u548c\u5f3a\u5316\u5b66\u4e60\uff09\u751f\u6210\u548c\u8bc4\u4f30\u6279\u5904\u7406\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8e\u5e72\u9884\u542f\u53d1\u5f0f\u7684\u65b9\u6cd5\u5728\u6536\u655b\u6027\u3001\u591a\u6837\u6027\u548c\u5468\u671f\u65f6\u95f4\u589e\u76ca\u4e0a\u4f18\u4e8e\u975e\u542f\u53d1\u5f0f\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u53d1\u73b0\u6700\u4f18\u6279\u5904\u7406\u7b56\u7565\uff0c\u4e3a\u4e1a\u52a1\u8fc7\u7a0b\u4f18\u5316\u63d0\u4f9b\u65b0\u601d\u8def\u3002"}}
{"id": "2507.15509", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.15509", "abs": "https://arxiv.org/abs/2507.15509", "authors": ["Lei Chen", "Xuanle Zhao", "Zhixiong Zeng", "Jing Huang", "Yufeng Zhong", "Lin Ma"], "title": "Chart-R1: Chain-of-Thought Supervision and Reinforcement for Advanced Chart Reasoner", "comment": "technical report", "summary": "Recently, inspired by OpenAI-o1/o3 and Deepseek-R1, the R1-Style method based\non reinforcement learning fine-tuning has received widespread attention from\nthe community. Previous R1-Style methods mainly focus on mathematical reasoning\nand code intelligence. It is of great research significance to verify their\nadvantages on more general multimodal data. Chart is an important multimodal\ndata type with rich information, which brings important research challenges in\ncomplex reasoning. In this work, we introduce Chart-R1, a chart-domain\nvision-language model with reinforcement learning fine-tuning to enable complex\nchart reasoning. To support Chart-R1, we first propose a novel programmatic\ndata synthesis technology to generate high-quality step-by-step chart reasoning\ndata covering single- and multi-subcharts, which makes up for the lack of\nreasoning data in the chart domain. Then we develop a two-stage training\nstrategy: Chart-COT with step-by-step chain-of-thought supervision, and\nChart-RFT with numerically sensitive reinforcement fine-tuning. Chart-COT aims\nto decompose complex chart reasoning tasks into fine-grained, understandable\nsubtasks through step-by-step supervision, which lays a good foundation for\nimproving the reasoning level of reinforcement learning. Chart-RFT utilize the\ntypical group relative policy optimization strategy, in which a relatively soft\nreward is adopted for numerical response to emphasize the numerical sensitivity\nin the chart domain. We conduct extensive experiments on open-source benchmarks\nand self-built chart reasoning dataset (\\emph{i.e., ChartRQA}). Experimental\nresults show that Chart-R1 has significant advantages compared to chart-domain\nmethods, even comparable to open/closed source large-scale models (\\emph{e.g.,\nGPT-4o, Claude-3.5}).", "AI": {"tldr": "Chart-R1\u662f\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u7684\u56fe\u8868\u9886\u57df\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u7a0b\u5e8f\u5316\u6570\u636e\u5408\u6210\u548c\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff08Chart-COT\u548cChart-RFT\uff09\u5b9e\u73b0\u590d\u6742\u56fe\u8868\u63a8\u7406\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u9a8c\u8bc1R1-Style\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u6570\u636e\uff08\u5982\u56fe\u8868\uff09\u4e0a\u7684\u4f18\u52bf\uff0c\u586b\u8865\u56fe\u8868\u9886\u57df\u63a8\u7406\u6570\u636e\u7684\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u7a0b\u5e8f\u5316\u6570\u636e\u5408\u6210\u6280\u672f\u751f\u6210\u9ad8\u8d28\u91cf\u63a8\u7406\u6570\u636e\uff0c\u5e76\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff1aChart-COT\uff08\u9010\u6b65\u76d1\u7763\uff09\u548cChart-RFT\uff08\u6570\u503c\u654f\u611f\u7684\u5f3a\u5316\u5fae\u8c03\uff09\u3002", "result": "Chart-R1\u5728\u5f00\u6e90\u57fa\u51c6\u548c\u81ea\u5efa\u6570\u636e\u96c6\uff08ChartRQA\uff09\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u56fe\u8868\u9886\u57df\u65b9\u6cd5\uff0c\u751a\u81f3\u5ab2\u7f8e\u5927\u578b\u6a21\u578b\uff08\u5982GPT-4o\u3001Claude-3.5\uff09\u3002", "conclusion": "Chart-R1\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u548c\u6570\u636e\u5408\u6210\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u8868\u9886\u57df\u7684\u590d\u6742\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2507.15518", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.15518", "abs": "https://arxiv.org/abs/2507.15518", "authors": ["Sizhou Chen", "Shufan Jiang", "Chi Zhang", "Xiao-Lei Zhang", "Xuelong Li"], "title": "HAMLET: Hyperadaptive Agent-based Modeling for Live Embodied Theatrics", "comment": null, "summary": "Creating an immersive and interactive theatrical experience is a long-term\ngoal in the field of interactive narrative. The emergence of large language\nmodel (LLM) is providing a new path to achieve this goal. However, existing\nLLM-based drama generation methods often result in AI agents that lack\ninitiative and cannot interact with the physical environment. Furthermore,\nthese methods typically require detailed user input to drive the drama. These\nlimitations reduce the interactivity and immersion of online real-time\nperformance. To address the above challenges, we propose HAMLET, a multi-agent\nframework focused on drama creation and online performance. Given a simple\ntopic, the framework generates a narrative blueprint, guiding the subsequent\nimprovisational performance. During the online performance, each actor is given\nan autonomous mind. This means that actors can make independent decisions based\non their own background, goals, and emotional state. In addition to\nconversations with other actors, their decisions can also change the state of\nscene props through actions such as opening a letter or picking up a weapon.\nThe change is then broadcast to other related actors, updating what they know\nand care about, which in turn influences their next action. To evaluate the\nquality of drama performance, we designed an evaluation method to assess three\nprimary aspects, including character performance, narrative quality, and\ninteraction experience. The experimental evaluation shows that HAMLET can\ncreate expressive and coherent theatrical experiences. Our code, dataset and\nmodels are available at https://github.com/HAMLET-2025/HAMLET.", "AI": {"tldr": "HAMLET\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u81ea\u4e3b\u51b3\u7b56\u548c\u7269\u7406\u73af\u5883\u4ea4\u4e92\u63d0\u5347\u620f\u5267\u751f\u6210\u7684\u4e92\u52a8\u6027\u548c\u6c89\u6d78\u611f\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u620f\u5267\u751f\u6210\u65b9\u6cd5\u7f3a\u4e4f\u4e3b\u52a8\u6027\u548c\u7269\u7406\u73af\u5883\u4ea4\u4e92\u80fd\u529b\uff0c\u4e14\u4f9d\u8d56\u8be6\u7ec6\u7528\u6237\u8f93\u5165\uff0c\u9650\u5236\u4e86\u5b9e\u65f6\u8868\u6f14\u7684\u4e92\u52a8\u6027\u548c\u6c89\u6d78\u611f\u3002", "method": "HAMLET\u6846\u67b6\u901a\u8fc7\u751f\u6210\u53d9\u4e8b\u84dd\u56fe\uff0c\u8d4b\u4e88\u6f14\u5458\u81ea\u4e3b\u51b3\u7b56\u80fd\u529b\uff0c\u4f7f\u5176\u80fd\u57fa\u4e8e\u80cc\u666f\u3001\u76ee\u6807\u548c\u60c5\u611f\u72b6\u6001\u884c\u52a8\uff0c\u5e76\u901a\u8fc7\u6539\u53d8\u573a\u666f\u9053\u5177\u72b6\u6001\u5f71\u54cd\u5176\u4ed6\u6f14\u5458\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0cHAMLET\u80fd\u751f\u6210\u5bcc\u6709\u8868\u73b0\u529b\u548c\u8fde\u8d2f\u6027\u7684\u620f\u5267\u4f53\u9a8c\u3002", "conclusion": "HAMLET\u4e3a\u4ea4\u4e92\u5f0f\u620f\u5267\u521b\u4f5c\u548c\u5b9e\u65f6\u8868\u6f14\u63d0\u4f9b\u4e86\u65b0\u8def\u5f84\uff0c\u63d0\u5347\u4e86\u4e92\u52a8\u6027\u548c\u6c89\u6d78\u611f\u3002"}}
{"id": "2507.15521", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15521", "abs": "https://arxiv.org/abs/2507.15521", "authors": ["Cole Robertson", "Philip Wolff"], "title": "LLM world models are mental: Output layer evidence of brittle world model use in LLM mechanical reasoning", "comment": "Manuscript comprises 14 pages, 4 figures, 4 tables in the Technical\n  Appendix and Supplementary Material, and is under review at NeurIPS 2025", "summary": "Do large language models (LLMs) construct and manipulate internal world\nmodels, or do they rely solely on statistical associations represented as\noutput layer token probabilities? We adapt cognitive science methodologies from\nhuman mental models research to test LLMs on pulley system problems using\nTikZ-rendered stimuli. Study 1 examines whether LLMs can estimate mechanical\nadvantage (MA). State-of-the-art models performed marginally but significantly\nabove chance, and their estimates correlated significantly with ground-truth\nMA. Significant correlations between number of pulleys and model estimates\nsuggest that models employed a pulley counting heuristic, without necessarily\nsimulating pulley systems to derive precise values. Study 2 tested this by\nprobing whether LLMs represent global features crucial to MA estimation. Models\nevaluated a functionally connected pulley system against a fake system with\nrandomly placed components. Without explicit cues, models identified the\nfunctional system as having greater MA with F1=0.8, suggesting LLMs could\nrepresent systems well enough to differentiate jumbled from functional systems.\nStudy 3 built on this by asking LLMs to compare functional systems with matched\nsystems which were connected up but which transferred no force to the weight;\nLLMs identified the functional system with F1=0.46, suggesting random guessing.\nInsofar as they may generalize, these findings are compatible with the notion\nthat LLMs manipulate internal world models, sufficient to exploit statistical\nassociations between pulley count and MA (Study 1), and to approximately\nrepresent system components' spatial relations (Study 2). However, they may\nlack the facility to reason over nuanced structural connectivity (Study 3). We\nconclude by advocating the utility of cognitive scientific methods to evaluate\nthe world-modeling capacities of artificial intelligence systems.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u662f\u5426\u6784\u5efa\u5185\u90e8\u4e16\u754c\u6a21\u578b\uff0c\u901a\u8fc7\u6ed1\u8f6e\u7cfb\u7edf\u95ee\u9898\u6d4b\u8bd5\u53d1\u73b0LLMs\u80fd\u5229\u7528\u7edf\u8ba1\u5173\u8054\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u590d\u6742\u7ed3\u6784\u7684\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u7814\u7a76LLMs\u662f\u5426\u4f9d\u8d56\u7edf\u8ba1\u5173\u8054\u6216\u6784\u5efa\u5185\u90e8\u4e16\u754c\u6a21\u578b\uff0c\u4ee5\u8bc4\u4f30\u5176\u8ba4\u77e5\u80fd\u529b\u3002", "method": "\u91c7\u7528\u8ba4\u77e5\u79d1\u5b66\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e09\u4e2a\u7814\u7a76\u6d4b\u8bd5LLMs\u5728\u6ed1\u8f6e\u7cfb\u7edf\u95ee\u9898\u4e2d\u7684\u8868\u73b0\u3002", "result": "LLMs\u80fd\u5229\u7528\u6ed1\u8f6e\u6570\u91cf\u542f\u53d1\u5f0f\u4f30\u8ba1\u673a\u68b0\u4f18\u52bf\uff08MA\uff09\uff0c\u5e76\u80fd\u533a\u5206\u529f\u80fd\u6027\u4e0e\u968f\u673a\u7cfb\u7edf\uff0c\u4f46\u5bf9\u590d\u6742\u7ed3\u6784\u63a8\u7406\u80fd\u529b\u6709\u9650\u3002", "conclusion": "LLMs\u53ef\u80fd\u90e8\u5206\u6784\u5efa\u5185\u90e8\u4e16\u754c\u6a21\u578b\uff0c\u4f46\u8ba4\u77e5\u79d1\u5b66\u65b9\u6cd5\u6709\u52a9\u4e8e\u8fdb\u4e00\u6b65\u8bc4\u4f30\u5176\u80fd\u529b\u3002"}}
{"id": "2507.15532", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15532", "abs": "https://arxiv.org/abs/2507.15532", "authors": ["Kasper Engelen", "Guillermo A. P\u00e9rez", "Marnix Suilen"], "title": "Data-Efficient Safe Policy Improvement Using Parametric Structure", "comment": "Accepted at ECAI 2025", "summary": "Safe policy improvement (SPI) is an offline reinforcement learning problem in\nwhich a new policy that reliably outperforms the behavior policy with high\nconfidence needs to be computed using only a dataset and the behavior policy.\nMarkov decision processes (MDPs) are the standard formalism for modeling\nenvironments in SPI. In many applications, additional information in the form\nof parametric dependencies between distributions in the transition dynamics is\navailable. We make SPI more data-efficient by leveraging these dependencies\nthrough three contributions: (1) a parametric SPI algorithm that exploits known\ncorrelations between distributions to more accurately estimate the transition\ndynamics using the same amount of data; (2) a preprocessing technique that\nprunes redundant actions from the environment through a game-based abstraction;\nand (3) a more advanced preprocessing technique, based on satisfiability modulo\ntheory (SMT) solving, that can identify more actions to prune. Empirical\nresults and an ablation study show that our techniques increase the data\nefficiency of SPI by multiple orders of magnitude while maintaining the same\nreliability guarantees.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u53c2\u6570\u4f9d\u8d56\u6027\u548c\u9884\u5904\u7406\u6280\u672f\u63d0\u5347\u6570\u636e\u6548\u7387\u3002", "motivation": "\u5728\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u5982\u4f55\u5229\u7528\u6709\u9650\u6570\u636e\u63d0\u5347\u7b56\u7565\u6027\u80fd\u662f\u4e00\u4e2a\u5173\u952e\u95ee\u9898\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u53c2\u6570\u4f9d\u8d56\u6027\u548c\u9884\u5904\u7406\u6280\u672f\u63d0\u9ad8\u6570\u636e\u6548\u7387\u3002", "method": "1. \u63d0\u51fa\u53c2\u6570\u5316SPI\u7b97\u6cd5\uff0c\u5229\u7528\u5206\u5e03\u95f4\u7684\u76f8\u5173\u6027\u66f4\u51c6\u786e\u4f30\u8ba1\u8f6c\u79fb\u52a8\u6001\uff1b2. \u57fa\u4e8e\u535a\u5f08\u62bd\u8c61\u9884\u5904\u7406\u6280\u672f\u526a\u679d\u5197\u4f59\u52a8\u4f5c\uff1b3. \u57fa\u4e8eSMT\u6c42\u89e3\u7684\u8fdb\u9636\u9884\u5904\u7406\u6280\u672f\u8fdb\u4e00\u6b65\u526a\u679d\u52a8\u4f5c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8fd9\u4e9b\u6280\u672f\u5c06SPI\u7684\u6570\u636e\u6548\u7387\u63d0\u5347\u591a\u4e2a\u6570\u91cf\u7ea7\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u9760\u6027\u3002", "conclusion": "\u901a\u8fc7\u53c2\u6570\u4f9d\u8d56\u6027\u548c\u9884\u5904\u7406\u6280\u672f\uff0c\u672c\u6587\u663e\u8457\u63d0\u5347\u4e86\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7684\u6570\u636e\u6548\u7387\u3002"}}
{"id": "2507.15581", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15581", "abs": "https://arxiv.org/abs/2507.15581", "authors": ["Ekaterina Goliakova", "Xavier Renard", "Marie-Jeanne Lesot", "Thibault Laugel", "Christophe Marsala", "Marcin Detyniecki"], "title": "Metric assessment protocol in the context of answer fluctuation on MCQ tasks", "comment": null, "summary": "Using multiple-choice questions (MCQs) has become a standard for assessing\nLLM capabilities efficiently. A variety of metrics can be employed for this\ntask. However, previous research has not conducted a thorough assessment of\nthem. At the same time, MCQ evaluation suffers from answer fluctuation: models\nproduce different results given slight changes in prompts. We suggest a metric\nassessment protocol in which evaluation methodologies are analyzed through\ntheir connection with fluctuation rates, as well as original performance. Our\nresults show that there is a strong link between existing metrics and the\nanswer changing, even when computed without any additional prompt variants. A\nnovel metric, worst accuracy, demonstrates the highest association on the\nprotocol.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30\u591a\u9009\u95ee\u9898\uff08MCQ\uff09\u6307\u6807\u7684\u65b0\u534f\u8bae\uff0c\u5206\u6790\u4e86\u73b0\u6709\u6307\u6807\u4e0e\u7b54\u6848\u6ce2\u52a8\u7387\u7684\u5173\u7cfb\uff0c\u5e76\u53d1\u73b0\u6700\u5dee\u51c6\u786e\u7387\uff08worst accuracy\uff09\u5177\u6709\u6700\u9ad8\u7684\u5173\u8054\u6027\u3002", "motivation": "\u591a\u9009\u95ee\u9898\uff08MCQ\uff09\u662f\u8bc4\u4f30LLM\u80fd\u529b\u7684\u5e38\u7528\u65b9\u6cd5\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u672a\u5145\u5206\u8bc4\u4f30\u5176\u6307\u6807\uff0c\u4e14\u5b58\u5728\u7b54\u6848\u6ce2\u52a8\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u6307\u6807\u8bc4\u4f30\u534f\u8bae\uff0c\u5206\u6790\u8bc4\u4f30\u65b9\u6cd5\u4e0e\u7b54\u6848\u6ce2\u52a8\u7387\u53ca\u539f\u59cb\u6027\u80fd\u7684\u5173\u7cfb\u3002", "result": "\u73b0\u6709\u6307\u6807\u4e0e\u7b54\u6848\u6ce2\u52a8\u7387\u6709\u5f3a\u5173\u8054\uff0c\u6700\u5dee\u51c6\u786e\u7387\u8868\u73b0\u6700\u4f73\u3002", "conclusion": "\u6700\u5dee\u51c6\u786e\u7387\u662f\u4e00\u79cd\u6709\u6548\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u53ef\u7528\u4e8e\u6539\u8fdbLLM\u7684MCQ\u8bc4\u4f30\u3002"}}
{"id": "2507.15618", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15618", "abs": "https://arxiv.org/abs/2507.15618", "authors": ["Weiyu Ma", "Jiwen Jiang", "Haobo Fu", "Haifeng Zhang"], "title": "TacticCraft: Natural Language-Driven Tactical Adaptation for StarCraft II", "comment": null, "summary": "We present an adapter-based approach for tactical conditioning of StarCraft\nII AI agents. Current agents, while powerful, lack the ability to adapt their\nstrategies based on high-level tactical directives. Our method freezes a\npre-trained policy network (DI-Star) and attaches lightweight adapter modules\nto each action head, conditioned on a tactical tensor that encodes strategic\npreferences. By training these adapters with KL divergence constraints, we\nensure the policy maintains core competencies while exhibiting tactical\nvariations. Experimental results show our approach successfully modulates agent\nbehavior across tactical dimensions including aggression, expansion patterns,\nand technology preferences, while maintaining competitive performance. Our\nmethod enables flexible tactical control with minimal computational overhead,\noffering practical strategy customization for complex real-time strategy games.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u9002\u914d\u5668\u7684StarCraft II AI\u6218\u672f\u8c03\u63a7\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f7b\u91cf\u7ea7\u9002\u914d\u6a21\u5757\u5b9e\u73b0\u7b56\u7565\u8c03\u6574\uff0c\u4fdd\u6301\u6838\u5fc3\u80fd\u529b\u7684\u540c\u65f6\u652f\u6301\u6218\u672f\u53d8\u5316\u3002", "motivation": "\u73b0\u6709AI\u4ee3\u7406\u7f3a\u4e4f\u57fa\u4e8e\u9ad8\u5c42\u6218\u672f\u6307\u4ee4\u7684\u9002\u5e94\u80fd\u529b\uff0c\u9700\u4e00\u79cd\u7075\u6d3b\u4e14\u9ad8\u6548\u7684\u65b9\u6cd5\u5b9e\u73b0\u6218\u672f\u8c03\u63a7\u3002", "method": "\u51bb\u7ed3\u9884\u8bad\u7ec3\u7b56\u7565\u7f51\u7edc\uff08DI-Star\uff09\uff0c\u4e3a\u6bcf\u4e2a\u52a8\u4f5c\u5934\u9644\u52a0\u8f7b\u91cf\u7ea7\u9002\u914d\u6a21\u5757\uff0c\u901a\u8fc7KL\u6563\u5ea6\u7ea6\u675f\u8bad\u7ec3\u9002\u914d\u5668\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u8c03\u63a7\u4ee3\u7406\u884c\u4e3a\uff08\u5982\u4fb5\u7565\u6027\u3001\u6269\u5f20\u6a21\u5f0f\u548c\u6280\u672f\u504f\u597d\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4ee5\u4f4e\u8ba1\u7b97\u5f00\u9500\u5b9e\u73b0\u7075\u6d3b\u6218\u672f\u63a7\u5236\uff0c\u9002\u7528\u4e8e\u590d\u6742\u5373\u65f6\u6218\u7565\u6e38\u620f\u7684\u7b56\u7565\u5b9a\u5236\u3002"}}
{"id": "2507.15676", "categories": ["cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2507.15676", "abs": "https://arxiv.org/abs/2507.15676", "authors": ["Reza Vatankhah Barenji", "Sina Khoshgoftar"], "title": "Agentic AI for autonomous anomaly management in complex systems", "comment": null, "summary": "This paper explores the potential of agentic AI in autonomously detecting and\nresponding to anomalies within complex systems, emphasizing its ability to\ntransform traditional, human-dependent anomaly management methods.", "AI": {"tldr": "\u63a2\u8ba8\u4ee3\u7406AI\u5728\u590d\u6742\u7cfb\u7edf\u4e2d\u81ea\u4e3b\u68c0\u6d4b\u548c\u54cd\u5e94\u5f02\u5e38\u7684\u6f5c\u529b\uff0c\u5f3a\u8c03\u5176\u6539\u53d8\u4f20\u7edf\u4f9d\u8d56\u4eba\u5de5\u7684\u5f02\u5e38\u7ba1\u7406\u65b9\u6cd5\u7684\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u5f02\u5e38\u7ba1\u7406\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\uff0c\u6548\u7387\u4f4e\u4e14\u6210\u672c\u9ad8\uff0c\u4ee3\u7406AI\u6709\u671b\u5b9e\u73b0\u81ea\u4e3b\u5316\uff0c\u63d0\u5347\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "\u7814\u7a76\u4ee3\u7406AI\u7684\u81ea\u4e3b\u68c0\u6d4b\u548c\u54cd\u5e94\u673a\u5236\uff0c\u5206\u6790\u5176\u5728\u590d\u6742\u7cfb\u7edf\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u3002", "result": "\u4ee3\u7406AI\u80fd\u591f\u6709\u6548\u8bc6\u522b\u548c\u5e94\u5bf9\u5f02\u5e38\uff0c\u663e\u8457\u51cf\u5c11\u4eba\u5de5\u5e72\u9884\u9700\u6c42\u3002", "conclusion": "\u4ee3\u7406AI\u5728\u5f02\u5e38\u7ba1\u7406\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u4f18\u5316\u5176\u81ea\u4e3b\u6027\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2507.15743", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15743", "abs": "https://arxiv.org/abs/2507.15743", "authors": ["Elahe Vedadi", "David Barrett", "Natalie Harris", "Ellery Wulczyn", "Shashir Reddy", "Roma Ruparel", "Mike Schaekermann", "Tim Strother", "Ryutaro Tanno", "Yash Sharma", "Jihyeon Lee", "C\u00edan Hughes", "Dylan Slack", "Anil Palepu", "Jan Freyberg", "Khaled Saab", "Valentin Li\u00e9vin", "Wei-Hung Weng", "Tao Tu", "Yun Liu", "Nenad Tomasev", "Kavita Kulkarni", "S. Sara Mahdavi", "Kelvin Guu", "Jo\u00eblle Barral", "Dale R. Webster", "James Manyika", "Avinatan Hassidim", "Katherine Chou", "Yossi Matias", "Pushmeet Kohli", "Adam Rodman", "Vivek Natarajan", "Alan Karthikesalingam", "David Stutz"], "title": "Towards physician-centered oversight of conversational diagnostic AI", "comment": null, "summary": "Recent work has demonstrated the promise of conversational AI systems for\ndiagnostic dialogue. However, real-world assurance of patient safety means that\nproviding individual diagnoses and treatment plans is considered a regulated\nactivity by licensed professionals. Furthermore, physicians commonly oversee\nother team members in such activities, including nurse practitioners (NPs) or\nphysician assistants/associates (PAs). Inspired by this, we propose a framework\nfor effective, asynchronous oversight of the Articulate Medical Intelligence\nExplorer (AMIE) AI system. We propose guardrailed-AMIE (g-AMIE), a multi-agent\nsystem that performs history taking within guardrails, abstaining from\nindividualized medical advice. Afterwards, g-AMIE conveys assessments to an\noverseeing primary care physician (PCP) in a clinician cockpit interface. The\nPCP provides oversight and retains accountability of the clinical decision.\nThis effectively decouples oversight from intake and can thus happen\nasynchronously. In a randomized, blinded virtual Objective Structured Clinical\nExamination (OSCE) of text consultations with asynchronous oversight, we\ncompared g-AMIE to NPs/PAs or a group of PCPs under the same guardrails. Across\n60 scenarios, g-AMIE outperformed both groups in performing high-quality\nintake, summarizing cases, and proposing diagnoses and management plans for the\noverseeing PCP to review. This resulted in higher quality composite decisions.\nPCP oversight of g-AMIE was also more time-efficient than standalone PCP\nconsultations in prior work. While our study does not replicate existing\nclinical practices and likely underestimates clinicians' capabilities, our\nresults demonstrate the promise of asynchronous oversight as a feasible\nparadigm for diagnostic AI systems to operate under expert human oversight for\nenhancing real-world care.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3ag-AMIE\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u7528\u4e8e\u5728\u533b\u7597\u8bca\u65ad\u5bf9\u8bdd\u4e2d\u5b9e\u73b0\u5f02\u6b65\u76d1\u7763\uff0c\u786e\u4fddAI\u7cfb\u7edf\u5728\u533b\u751f\u76d1\u7763\u4e0b\u5b89\u5168\u8fd0\u884c\u3002", "motivation": "\u73b0\u6709AI\u8bca\u65ad\u7cfb\u7edf\u7f3a\u4e4f\u6709\u6548\u7684\u76d1\u7763\u673a\u5236\uff0c\u800c\u533b\u7597\u8bca\u65ad\u9700\u7531\u6301\u724c\u4e13\u4e1a\u4eba\u5458\u8d1f\u8d23\u3002\u8bba\u6587\u65e8\u5728\u89e3\u51b3AI\u7cfb\u7edf\u5728\u533b\u7597\u9886\u57df\u7684\u76d1\u7763\u95ee\u9898\u3002", "method": "\u63d0\u51fag-AMIE\u7cfb\u7edf\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u5b8c\u6210\u75c5\u53f2\u91c7\u96c6\uff0c\u5e76\u5728\u533b\u751f\u76d1\u7763\u4e0b\u63d0\u4f9b\u8bca\u65ad\u5efa\u8bae\u3002\u901a\u8fc7\u865a\u62dfOSCE\u5b9e\u9a8c\u6bd4\u8f83g-AMIE\u4e0eNPs/PAs\u548cPCPs\u7684\u8868\u73b0\u3002", "result": "g-AMIE\u572860\u4e2a\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u4e8eNPs/PAs\u548cPCPs\uff0c\u80fd\u9ad8\u6548\u5b8c\u6210\u75c5\u53f2\u91c7\u96c6\u3001\u75c5\u4f8b\u603b\u7ed3\u548c\u8bca\u65ad\u5efa\u8bae\uff0c\u4e14\u76d1\u7763\u65f6\u95f4\u66f4\u77ed\u3002", "conclusion": "\u5f02\u6b65\u76d1\u7763\u662f\u4e00\u79cd\u53ef\u884c\u7684\u6a21\u5f0f\uff0c\u53ef\u589e\u5f3aAI\u7cfb\u7edf\u5728\u533b\u7597\u8bca\u65ad\u4e2d\u7684\u5b9e\u7528\u6027\uff0c\u540c\u65f6\u786e\u4fdd\u533b\u751f\u76d1\u7763\u548c\u8d23\u4efb\u5f52\u5c5e\u3002"}}
{"id": "2507.15758", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.15758", "abs": "https://arxiv.org/abs/2507.15758", "authors": ["Xingyu Wu", "Yuchen Yan", "Shangke Lyu", "Linjuan Wu", "Yiwen Qiu", "Yongliang Shen", "Weiming Lu", "Jian Shao", "Jun Xiao", "Yueting Zhuang"], "title": "LAPO: Internalizing Reasoning Efficiency via Length-Adaptive Policy Optimization", "comment": "GitHub:https://github.com/zju-real/lapo;\n  Project:https://zju-real.github.io/lapo", "summary": "Large reasoning models have achieved remarkable performance through extended\nchain-of-thought sequences, yet this computational freedom leads to excessive\ntoken generation even for simple problems. We present Length-Adaptive Policy\nOptimization (LAPO), a novel framework that transforms reasoning length control\nfrom an external constraint into an intrinsic model capability. Unlike existing\napproaches that impose rigid limits or rely on post-hoc interventions, LAPO\nenables models to internalize an understanding of appropriate reasoning depth\nthrough a two-stage reinforcement learning process. In the first stage, models\nlearn natural reasoning patterns by discovering the statistical distribution of\nsuccessful solution lengths. The second stage leverages these patterns as\nmeta-cognitive guidance, embedding them directly within the model's reasoning\ncontext to ensure inference-time flexibility. Experiments on mathematical\nreasoning benchmarks demonstrate that LAPO reduces token usage by up to 40.9\\%\nwhile improving accuracy by 2.3\\%. Our analysis reveals that models trained\nwith LAPO develop emergent abilities to allocate computational resources based\non problem complexity, achieving efficient reasoning without sacrificing\nquality.", "AI": {"tldr": "LAPO\u6846\u67b6\u901a\u8fc7\u4e24\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\uff0c\u4f7f\u6a21\u578b\u5185\u5316\u63a8\u7406\u957f\u5ea6\u7684\u9002\u5e94\u6027\uff0c\u51cf\u5c1140.9%\u7684token\u4f7f\u7528\u5e76\u63d0\u53472.3%\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u7b80\u5355\u95ee\u9898\u4e0a\u751f\u6210\u8fc7\u591atoken\u7684\u95ee\u9898\uff0c\u5c06\u63a8\u7406\u957f\u5ea6\u63a7\u5236\u4ece\u5916\u90e8\u7ea6\u675f\u8f6c\u5316\u4e3a\u6a21\u578b\u5185\u5728\u80fd\u529b\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\uff1a\u7b2c\u4e00\u9636\u6bb5\u5b66\u4e60\u6210\u529f\u89e3\u7684\u957f\u5ea6\u5206\u5e03\uff0c\u7b2c\u4e8c\u9636\u6bb5\u5c06\u5176\u4f5c\u4e3a\u5143\u8ba4\u77e5\u6307\u5bfc\u5d4c\u5165\u63a8\u7406\u4e0a\u4e0b\u6587\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\uff0cLAPO\u51cf\u5c1140.9%\u7684token\u4f7f\u7528\uff0c\u540c\u65f6\u63d0\u9ad82.3%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "LAPO\u4f7f\u6a21\u578b\u80fd\u6839\u636e\u95ee\u9898\u590d\u6742\u5ea6\u5206\u914d\u8ba1\u7b97\u8d44\u6e90\uff0c\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\u800c\u4e0d\u727a\u7272\u8d28\u91cf\u3002"}}
{"id": "2507.15761", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15761", "abs": "https://arxiv.org/abs/2507.15761", "authors": ["Jingyi Zheng", "Zifan Peng", "Yule Liu", "Junfeng Wang", "Yifan Liao", "Wenhan Dong", "Xinlei He"], "title": "GasAgent: A Multi-Agent Framework for Automated Gas Optimization in Smart Contracts", "comment": null, "summary": "Smart contracts are trustworthy, immutable, and automatically executed\nprograms on the blockchain. Their execution requires the Gas mechanism to\nensure efficiency and fairness. However, due to non-optimal coding practices,\nmany contracts contain Gas waste patterns that need to be optimized. Existing\nsolutions mostly rely on manual discovery, which is inefficient, costly to\nmaintain, and difficult to scale. Recent research uses large language models\n(LLMs) to explore new Gas waste patterns. However, it struggles to remain\ncompatible with existing patterns, often produces redundant patterns, and\nrequires manual validation/rewriting. To address this gap, we present GasAgent,\nthe first multi-agent system for smart contract Gas optimization that combines\ncompatibility with existing patterns and automated discovery/validation of new\npatterns, enabling end-to-end optimization. GasAgent consists of four\nspecialized agents, Seeker, Innovator, Executor, and Manager, that collaborate\nin a closed loop to identify, validate, and apply Gas-saving improvements.\nExperiments on 100 verified real-world contracts demonstrate that GasAgent\nsuccessfully optimizes 82 contracts, achieving an average deployment Gas\nsavings of 9.97%. In addition, our evaluation confirms its compatibility with\nexisting tools and validates the effectiveness of each module through ablation\nstudies. To assess broader usability, we further evaluate 500 contracts\ngenerated by five representative LLMs across 10 categories and find that\nGasAgent optimizes 79.8% of them, with deployment Gas savings ranging from\n4.79% to 13.93%, showing its usability as the optimization layer for\nLLM-assisted smart contract development.", "AI": {"tldr": "GasAgent\u662f\u4e00\u4e2a\u591a\u4ee3\u7406\u7cfb\u7edf\uff0c\u7528\u4e8e\u667a\u80fd\u5408\u7ea6Gas\u4f18\u5316\uff0c\u7ed3\u5408\u73b0\u6709\u6a21\u5f0f\u517c\u5bb9\u6027\u548c\u81ea\u52a8\u5316\u53d1\u73b0/\u9a8c\u8bc1\u65b0\u6a21\u5f0f\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u4f18\u5316\u3002", "motivation": "\u73b0\u6709Gas\u4f18\u5316\u65b9\u6848\u4f9d\u8d56\u624b\u52a8\u53d1\u73b0\uff0c\u6548\u7387\u4f4e\u4e14\u96be\u4ee5\u6269\u5c55\uff0c\u800c\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7814\u7a76\u5b58\u5728\u517c\u5bb9\u6027\u5dee\u548c\u5197\u4f59\u95ee\u9898\u3002", "method": "GasAgent\u7531\u56db\u4e2a\u4e13\u4e1a\u4ee3\u7406\uff08Seeker\u3001Innovator\u3001Executor\u3001Manager\uff09\u534f\u4f5c\uff0c\u95ed\u73af\u8bc6\u522b\u3001\u9a8c\u8bc1\u548c\u5e94\u7528Gas\u8282\u7701\u6539\u8fdb\u3002", "result": "\u5728100\u4e2a\u771f\u5b9e\u5408\u7ea6\u4e0a\u4f18\u531682\u4e2a\uff0c\u5e73\u5747\u8282\u77019.97%\u90e8\u7f72Gas\uff1b\u5728500\u4e2aLLM\u751f\u6210\u5408\u7ea6\u4e2d\u4f18\u531679.8%\uff0c\u8282\u77014.79%-13.93% Gas\u3002", "conclusion": "GasAgent\u4f5c\u4e3aLLM\u8f85\u52a9\u667a\u80fd\u5408\u7ea6\u5f00\u53d1\u7684\u4f18\u5316\u5c42\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u548c\u9ad8\u6548\u6027\u3002"}}
{"id": "2507.15770", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15770", "abs": "https://arxiv.org/abs/2507.15770", "authors": ["Yifan Shen", "Zihan Zhao", "Xiao Xue", "Yuwei Guo", "Qun Ma", "Deyu Zhou", "Ming Zhang"], "title": "A Framework for Analyzing Abnormal Emergence in Service Ecosystems Through LLM-based Agent Intention Mining", "comment": null, "summary": "With the rise of service computing, cloud computing, and IoT, service\necosystems are becoming increasingly complex. The intricate interactions among\nintelligent agents make abnormal emergence analysis challenging, as traditional\ncausal methods focus on individual trajectories. Large language models offer\nnew possibilities for Agent-Based Modeling (ABM) through Chain-of-Thought (CoT)\nreasoning to reveal agent intentions. However, existing approaches remain\nlimited to microscopic and static analysis. This paper introduces a framework:\nEmergence Analysis based on Multi-Agent Intention (EAMI), which enables dynamic\nand interpretable emergence analysis. EAMI first employs a dual-perspective\nthought track mechanism, where an Inspector Agent and an Analysis Agent extract\nagent intentions under bounded and perfect rationality. Then, k-means\nclustering identifies phase transition points in group intentions, followed by\na Intention Temporal Emergence diagram for dynamic analysis. The experiments\nvalidate EAMI in complex online-to-offline (O2O) service system and the\nStanford AI Town experiment, with ablation studies confirming its\neffectiveness, generalizability, and efficiency. This framework provides a\nnovel paradigm for abnormal emergence and causal analysis in service\necosystems. The code is available at\nhttps://anonymous.4open.science/r/EAMI-B085.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u610f\u56fe\u7684\u52a8\u6001\u53ef\u89e3\u91ca\u6d8c\u73b0\u5206\u6790\u6846\u67b6EAMI\uff0c\u901a\u8fc7\u53cc\u89c6\u89d2\u601d\u7ef4\u8ffd\u8e2a\u548ck-means\u805a\u7c7b\u5206\u6790\u7fa4\u4f53\u610f\u56fe\u7684\u76f8\u53d8\u70b9\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5728\u590d\u6742\u670d\u52a1\u7cfb\u7edf\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u968f\u7740\u670d\u52a1\u8ba1\u7b97\u3001\u4e91\u8ba1\u7b97\u548c\u7269\u8054\u7f51\u7684\u53d1\u5c55\uff0c\u670d\u52a1\u751f\u6001\u7cfb\u7edf\u65e5\u76ca\u590d\u6742\uff0c\u4f20\u7edf\u56e0\u679c\u65b9\u6cd5\u96be\u4ee5\u5206\u6790\u667a\u80fd\u4f53\u95f4\u7684\u5f02\u5e38\u6d8c\u73b0\u73b0\u8c61\uff0c\u9700\u8981\u52a8\u6001\u4e14\u53ef\u89e3\u91ca\u7684\u65b0\u65b9\u6cd5\u3002", "method": "EAMI\u6846\u67b6\u91c7\u7528\u53cc\u89c6\u89d2\u601d\u7ef4\u8ffd\u8e2a\u673a\u5236\uff08\u68c0\u67e5\u8005\u667a\u80fd\u4f53\u548c\u5206\u6790\u667a\u80fd\u4f53\uff09\u63d0\u53d6\u610f\u56fe\uff0c\u7ed3\u5408k-means\u805a\u7c7b\u548c\u610f\u56fe\u65f6\u5e8f\u6d8c\u73b0\u56fe\u8fdb\u884c\u52a8\u6001\u5206\u6790\u3002", "result": "\u5b9e\u9a8c\u5728\u590d\u6742O2O\u670d\u52a1\u7cfb\u7edf\u548cStanford AI Town\u4e2d\u9a8c\u8bc1\u4e86EAMI\u7684\u6709\u6548\u6027\u3001\u901a\u7528\u6027\u548c\u6548\u7387\u3002", "conclusion": "EAMI\u4e3a\u670d\u52a1\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u5f02\u5e38\u6d8c\u73b0\u548c\u56e0\u679c\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\u3002"}}
{"id": "2507.15796", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15796", "abs": "https://arxiv.org/abs/2507.15796", "authors": ["Nuria Rodr\u00edguez-Barroso", "Mario Garc\u00eda-M\u00e1rquez", "M. Victoria Luz\u00f3n", "Francisco Herrera"], "title": "Challenges of Trustworthy Federated Learning: What's Done, Current Trends and Remaining Work", "comment": null, "summary": "In recent years, the development of Trustworthy Artificial Intelligence (TAI)\nhas emerged as a critical objective in the deployment of AI systems across\nsensitive and high-risk domains. TAI frameworks articulate a comprehensive set\nof ethical, legal, and technical requirements to ensure that AI technologies\nare aligned with human values, rights, and societal expectations. Among the\nvarious AI paradigms, Federated Learning (FL) presents a promising solution to\npressing privacy concerns. However, aligning FL with the rest of the\nrequirements of TAI presents a series of challenges, most of which arise from\nits inherently distributed nature. In this work, we adopt the requirements TAI\nas a guiding structure to systematically analyze the challenges of adapting FL\nto TAI. Specifically, we classify and examine the key obstacles to aligning FL\nwith TAI, providing a detailed exploration of what has been done, the trends,\nand the remaining work within each of the identified challenges.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u5982\u4f55\u6ee1\u8db3\u53ef\u4fe1\u4eba\u5de5\u667a\u80fd\uff08TAI\uff09\u7684\u8981\u6c42\uff0c\u5206\u6790\u4e86\u5176\u6311\u6218\u4e0e\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u968f\u7740AI\u5728\u654f\u611f\u548c\u9ad8\u98ce\u9669\u9886\u57df\u7684\u5e94\u7528\u589e\u52a0\uff0c\u786e\u4fdd\u5176\u7b26\u5408\u4f26\u7406\u3001\u6cd5\u5f8b\u548c\u6280\u672f\u8981\u6c42\u7684\u9700\u6c42\u65e5\u76ca\u8feb\u5207\u3002\u8054\u90a6\u5b66\u4e60\u56e0\u5176\u9690\u79c1\u4fdd\u62a4\u7279\u6027\u88ab\u89c6\u4e3a\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u5176\u5206\u5e03\u5f0f\u7279\u6027\u4e0eTAI\u7684\u5176\u4ed6\u8981\u6c42\u5b58\u5728\u51b2\u7a81\u3002", "method": "\u4ee5TAI\u8981\u6c42\u4e3a\u6846\u67b6\uff0c\u7cfb\u7edf\u5206\u6790FL\u4e0eTAI\u5bf9\u9f50\u7684\u6311\u6218\uff0c\u5206\u7c7b\u5e76\u63a2\u8ba8\u5173\u952e\u969c\u788d\u3001\u73b0\u6709\u7814\u7a76\u3001\u8d8b\u52bf\u53ca\u672a\u89e3\u51b3\u95ee\u9898\u3002", "result": "\u7814\u7a76\u603b\u7ed3\u4e86FL\u5728\u6ee1\u8db3TAI\u8981\u6c42\u65f6\u7684\u4e3b\u8981\u6311\u6218\uff0c\u5305\u62ec\u9690\u79c1\u4fdd\u62a4\u4e0e\u6a21\u578b\u6027\u80fd\u7684\u5e73\u8861\u3001\u6570\u636e\u5206\u5e03\u4e0d\u5747\u7b49\u95ee\u9898\u3002", "conclusion": "FL\u5728TAI\u6846\u67b6\u4e0b\u4ecd\u9700\u89e3\u51b3\u591a\u65b9\u9762\u7684\u6311\u6218\uff0c\u672a\u6765\u7814\u7a76\u9700\u8fdb\u4e00\u6b65\u63a2\u7d22\u5176\u4e0eTAI\u7684\u5168\u9762\u5bf9\u9f50\u3002"}}
{"id": "2507.15842", "categories": ["cs.AI", "stat.ME", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.15842", "abs": "https://arxiv.org/abs/2507.15842", "authors": ["Sara LaPlante", "Emilija Perkovi\u0107"], "title": "Identifying Conditional Causal Effects in MPDAGs", "comment": "67 pages, 8 figures", "summary": "We consider identifying a conditional causal effect when a graph is known up\nto a maximally oriented partially directed acyclic graph (MPDAG). An MPDAG\nrepresents an equivalence class of graphs that is restricted by background\nknowledge and where all variables in the causal model are observed. We provide\nthree results that address identification in this setting: an identification\nformula when the conditioning set is unaffected by treatment, a generalization\nof the well-known do calculus to the MPDAG setting, and an algorithm that is\ncomplete for identifying these conditional effects.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5728\u5df2\u77e5\u6700\u5927\u5b9a\u5411\u90e8\u5206\u6709\u5411\u65e0\u73af\u56fe\uff08MPDAG\uff09\u7684\u60c5\u51b5\u4e0b\uff0c\u5982\u4f55\u8bc6\u522b\u6761\u4ef6\u56e0\u679c\u6548\u5e94\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e09\u79cd\u7ed3\u679c\uff1a\u4e0d\u53d7\u6cbb\u7597\u5f71\u54cd\u7684\u6761\u4ef6\u4e0b\u8bc6\u522b\u516c\u5f0f\u3001MPDAG\u73af\u5883\u4e0bdo calculus\u7684\u63a8\u5e7f\uff0c\u4ee5\u53ca\u4e00\u4e2a\u5b8c\u6574\u7684\u6761\u4ef6\u6548\u5e94\u8bc6\u522b\u7b97\u6cd5\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u89e3\u51b3\u5728\u80cc\u666f\u77e5\u8bc6\u9650\u5236\u548c\u6240\u6709\u53d8\u91cf\u53ef\u89c2\u6d4b\u7684\u60c5\u51b5\u4e0b\uff0c\u5982\u4f55\u4eceMPDAG\u4e2d\u8bc6\u522b\u6761\u4ef6\u56e0\u679c\u6548\u5e94\u7684\u95ee\u9898\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u63d0\u51fa\u4e00\u4e2a\u8bc6\u522b\u516c\u5f0f\uff08\u5f53\u6761\u4ef6\u96c6\u4e0d\u53d7\u6cbb\u7597\u5f71\u54cd\u65f6\uff09\u3001\u5c06do calculus\u63a8\u5e7f\u5230MPDAG\u73af\u5883\uff0c\u4ee5\u53ca\u5f00\u53d1\u4e00\u4e2a\u5b8c\u6574\u7684\u6761\u4ef6\u6548\u5e94\u8bc6\u522b\u7b97\u6cd5\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u63d0\u51fa\u7684\u516c\u5f0f\u548c\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u8bc6\u522bMPDAG\u4e2d\u7684\u6761\u4ef6\u56e0\u679c\u6548\u5e94\u3002", "conclusion": "\u7ed3\u8bba\u662f\uff0c\u8fd9\u4e9b\u7ed3\u679c\u4e3a\u5728MPDAG\u6846\u67b6\u4e0b\u8bc6\u522b\u6761\u4ef6\u56e0\u679c\u6548\u5e94\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2507.15844", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.15844", "abs": "https://arxiv.org/abs/2507.15844", "authors": ["Shangke Lyu", "Linjuan Wu", "Yuchen Yan", "Xingyu Wu", "Hao Li", "Yongliang Shen", "Peisheng Jiang", "Weiming Lu", "Jun Xiao", "Yueting Zhuang"], "title": "Hierarchical Budget Policy Optimization for Adaptive Reasoning", "comment": "Code: https://github.com/zju-real/hbpo Project\n  Page:https://zju-real.github.io/hbpo/", "summary": "Large reasoning models achieve remarkable performance through extensive\nchain-of-thought generation, yet exhibit significant computational inefficiency\nby applying uniform reasoning strategies regardless of problem complexity. We\npresent Hierarchical Budget Policy Optimization (HBPO), a reinforcement\nlearning framework that enables models to learn problem-specific reasoning\ndepths without sacrificing capability. HBPO addresses the fundamental challenge\nof exploration space collapse in efficiency-oriented training, where penalties\non long output length systematically bias models away from necessary long\nreasoning paths. Through hierarchical budget exploration, our approach\npartitions rollout samples into multiple subgroups with distinct token budgets,\naiming to enable efficient resource allocation while preventing degradation of\ncapability. We introduce differentiated reward mechanisms that create\nbudget-aware incentives aligned with the complexity of the problem, allowing\nmodels to discover natural correspondences between task requirements and\ncomputational effort. Extensive experiments demonstrate that HBPO reduces\naverage token usage by up to 60.6% while improving accuracy by 3.14% across\nfour reasoning benchmarks. Unlike existing methods that impose external\nconstraints or rely on discrete mode selection, HBPO exhibits emergent adaptive\nbehavior where models automatically adjust reasoning depth based on problem\ncomplexity. Our results suggest that reasoning efficiency and capability are\nnot inherently conflicting, and can be simultaneously optimized through\nappropriately structured hierarchical training that preserves exploration\ndiversity.", "AI": {"tldr": "HBPO\u662f\u4e00\u79cd\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u9884\u7b97\u63a2\u7d22\u548c\u5dee\u5f02\u5316\u5956\u52b1\u673a\u5236\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u6839\u636e\u95ee\u9898\u590d\u6742\u5ea6\u81ea\u9002\u5e94\u8c03\u6574\u63a8\u7406\u6df1\u5ea6\uff0c\u663e\u8457\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u540c\u65f6\u4fdd\u6301\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u63a8\u7406\u6a21\u578b\u5728\u7edf\u4e00\u63a8\u7406\u7b56\u7565\u4e0b\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\uff0c\u540c\u65f6\u907f\u514d\u56e0\u6548\u7387\u4f18\u5316\u5bfc\u81f4\u7684\u957f\u63a8\u7406\u8def\u5f84\u504f\u5dee\u3002", "method": "\u91c7\u7528\u5206\u5c42\u9884\u7b97\u63a2\u7d22\uff0c\u5c06\u6837\u672c\u5206\u4e3a\u4e0d\u540c\u9884\u7b97\u5b50\u7ec4\uff0c\u7ed3\u5408\u5dee\u5f02\u5316\u5956\u52b1\u673a\u5236\uff0c\u5b9e\u73b0\u8d44\u6e90\u9ad8\u6548\u5206\u914d\u3002", "result": "\u5728\u56db\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5e73\u5747\u4ee4\u724c\u4f7f\u7528\u51cf\u5c1160.6%\uff0c\u51c6\u786e\u7387\u63d0\u53473.14%\u3002", "conclusion": "HBPO\u8868\u660e\u63a8\u7406\u6548\u7387\u548c\u80fd\u529b\u53ef\u4ee5\u540c\u65f6\u4f18\u5316\uff0c\u5173\u952e\u5728\u4e8e\u5206\u5c42\u8bad\u7ec3\u4fdd\u6301\u63a2\u7d22\u591a\u6837\u6027\u3002"}}
{"id": "2507.15851", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15851", "abs": "https://arxiv.org/abs/2507.15851", "authors": ["Lingyu Li", "Yang Yao", "Yixu Wang", "Chubo Li", "Yan Teng", "Yingchun Wang"], "title": "The Other Mind: How Language Models Exhibit Human Temporal Cognition", "comment": "12 pages, 9 figures, 4 tables", "summary": "As Large Language Models (LLMs) continue to advance, they exhibit certain\ncognitive patterns similar to those of humans that are not directly specified\nin training data. This study investigates this phenomenon by focusing on\ntemporal cognition in LLMs. Leveraging the similarity judgment task, we find\nthat larger models spontaneously establish a subjective temporal reference\npoint and adhere to the Weber-Fechner law, whereby the perceived distance\nlogarithmically compresses as years recede from this reference point. To\nuncover the mechanisms behind this behavior, we conducted multiple analyses\nacross neuronal, representational, and informational levels. We first identify\na set of temporal-preferential neurons and find that this group exhibits\nminimal activation at the subjective reference point and implements a\nlogarithmic coding scheme convergently found in biological systems. Probing\nrepresentations of years reveals a hierarchical construction process, where\nyears evolve from basic numerical values in shallow layers to abstract temporal\norientation in deep layers. Finally, using pre-trained embedding models, we\nfound that the training corpus itself possesses an inherent, non-linear\ntemporal structure, which provides the raw material for the model's internal\nconstruction. In discussion, we propose an experientialist perspective for\nunderstanding these findings, where the LLMs' cognition is viewed as a\nsubjective construction of the external world by its internal representational\nsystem. This nuanced perspective implies the potential emergence of alien\ncognitive frameworks that humans cannot intuitively predict, pointing toward a\ndirection for AI alignment that focuses on guiding internal constructions. Our\ncode is available at https://TheOtherMind.github.io.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u65f6\u95f4\u8ba4\u77e5\u4e0a\u8868\u73b0\u51fa\u7c7b\u4f3c\u4eba\u7c7b\u7684\u81ea\u53d1\u884c\u4e3a\uff0c\u5305\u62ec\u4e3b\u89c2\u65f6\u95f4\u53c2\u8003\u70b9\u7684\u5efa\u7acb\u548c\u5bf9\u97e6\u4f2f-\u8d39\u5e0c\u7eb3\u5b9a\u5f8b\u7684\u9075\u5faa\u3002", "motivation": "\u63a2\u8ba8LLMs\u5728\u672a\u7ecf\u660e\u786e\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u5982\u4f55\u8868\u73b0\u51fa\u7c7b\u4f3c\u4eba\u7c7b\u7684\u65f6\u95f4\u8ba4\u77e5\u6a21\u5f0f\u3002", "method": "\u901a\u8fc7\u76f8\u4f3c\u6027\u5224\u65ad\u4efb\u52a1\uff0c\u5206\u6790\u795e\u7ecf\u5143\u3001\u8868\u5f81\u548c\u4fe1\u606f\u5c42\u9762\u7684\u673a\u5236\uff0c\u8bc6\u522b\u65f6\u95f4\u504f\u597d\u795e\u7ecf\u5143\u5e76\u7814\u7a76\u5176\u6fc0\u6d3b\u6a21\u5f0f\u3002", "result": "\u53d1\u73b0LLMs\u81ea\u53d1\u5f62\u6210\u4e3b\u89c2\u65f6\u95f4\u53c2\u8003\u70b9\uff0c\u5e76\u91c7\u7528\u5bf9\u6570\u7f16\u7801\u65b9\u5f0f\uff1b\u8bad\u7ec3\u8bed\u6599\u672c\u8eab\u5177\u6709\u975e\u7ebf\u6027\u65f6\u95f4\u7ed3\u6784\u3002", "conclusion": "\u63d0\u51fa\u4ece\u7ecf\u9a8c\u4e3b\u4e49\u89c6\u89d2\u7406\u89e3LLMs\u7684\u8ba4\u77e5\u884c\u4e3a\uff0c\u5f3a\u8c03AI\u5bf9\u9f50\u9700\u5173\u6ce8\u5185\u90e8\u8868\u5f81\u7cfb\u7edf\u7684\u5f15\u5bfc\u3002"}}
{"id": "2507.15855", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15855", "abs": "https://arxiv.org/abs/2507.15855", "authors": ["Yichen Huang", "Lin F. Yang"], "title": "Gemini 2.5 Pro Capable of Winning Gold at IMO 2025", "comment": null, "summary": "The International Mathematical Olympiad (IMO) poses uniquely challenging\nproblems requiring deep insight, creativity, and formal reasoning. While Large\nLanguage Models (LLMs) perform well on mathematical benchmarks like AIME, they\nstruggle with Olympiad-level tasks. We use Google's Gemini 2.5 Pro on the newly\nreleased IMO 2025 problems, avoiding data contamination. With pipeline design\nand prompt engineering, 5 (out of 6) problems are solved correctly (up to a\ncaveat discussed below), highlighting the importance of finding the optimal way\nof using powerful models.", "AI": {"tldr": "Gemini 2.5 Pro\u6210\u529f\u89e3\u51b3\u4e86IMO 2025\u4e2d\u76845/6\u95ee\u9898\uff0c\u5c55\u793a\u4e86\u4f18\u5316\u4f7f\u7528\u5f3a\u5927\u6a21\u578b\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u89e3\u51b3\u56fd\u9645\u6570\u5b66\u5965\u6797\u5339\u514b\uff08IMO\uff09\u7b49\u9ad8\u96be\u5ea6\u6570\u5b66\u95ee\u9898\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u4f7f\u7528Google\u7684Gemini 2.5 Pro\u6a21\u578b\uff0c\u901a\u8fc7\u7ba1\u9053\u8bbe\u8ba1\u548c\u63d0\u793a\u5de5\u7a0b\u907f\u514d\u6570\u636e\u6c61\u67d3\u3002", "result": "\u5728IMO 2025\u76846\u4e2a\u95ee\u9898\u4e2d\uff0c\u6210\u529f\u89e3\u51b3\u4e865\u4e2a\uff08\u5b58\u5728\u4e00\u4e2a\u4f8b\u5916\uff09\u3002", "conclusion": "\u4f18\u5316\u6a21\u578b\u4f7f\u7528\u65b9\u6cd5\u5bf9\u89e3\u51b3\u9ad8\u96be\u5ea6\u6570\u5b66\u95ee\u9898\u81f3\u5173\u91cd\u8981\u3002"}}
