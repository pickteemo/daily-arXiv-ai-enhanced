{"id": "2507.08100", "categories": ["cs.RO", "cond-mat.soft", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.08100", "abs": "https://arxiv.org/abs/2507.08100", "authors": ["Lucy Liu", "Justin Werfel", "Federico Toschi", "L. Mahadevan"], "title": "Noise-Enabled Goal Attainment in Crowded Collectives", "comment": null, "summary": "In crowded environments, individuals must navigate around other occupants to\nreach their destinations. Understanding and controlling traffic flows in these\nspaces is relevant to coordinating robot swarms and designing infrastructure\nfor dense populations. Here, we combine simulations, theory, and robotic\nexperiments to study how noisy motion can disrupt traffic jams and enable flow\nas agents travel to individual goals. Above a critical noise level, large jams\ndo not persist. From this observation, we analytically approximate the goal\nattainment rate as a function of the noise level, then solve for the optimal\nagent density and noise level that maximize the swarm's goal attainment rate.\nWe perform robotic experiments to corroborate our simulated and theoretical\nresults. Finally, we compare simple, local navigation approaches with a\nsophisticated but computationally costly central planner. A simple reactive\nscheme performs well up to moderate densities and is far more computationally\nefficient than a planner, suggesting lessons for real-world problems.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u6a21\u62df\u3001\u7406\u8bba\u548c\u673a\u5668\u4eba\u5b9e\u9a8c\uff0c\u63a2\u8ba8\u566a\u58f0\u8fd0\u52a8\u5982\u4f55\u6253\u7834\u4ea4\u901a\u62e5\u5835\u5e76\u4f18\u5316\u7fa4\u4f53\u76ee\u6807\u8fbe\u6210\u7387\u3002", "motivation": "\u5728\u62e5\u6324\u73af\u5883\u4e2d\uff0c\u7406\u89e3\u5e76\u63a7\u5236\u4ea4\u901a\u6d41\u5bf9\u673a\u5668\u4eba\u7fa4\u4f53\u534f\u8c03\u548c\u57fa\u7840\u8bbe\u65bd\u8bbe\u8ba1\u81f3\u5173\u91cd\u8981\u3002", "method": "\u7ed3\u5408\u6a21\u62df\u3001\u7406\u8bba\u548c\u673a\u5668\u4eba\u5b9e\u9a8c\uff0c\u5206\u6790\u566a\u58f0\u6c34\u5e73\u5bf9\u4ea4\u901a\u6d41\u7684\u5f71\u54cd\uff0c\u5e76\u4f18\u5316\u7fa4\u4f53\u76ee\u6807\u8fbe\u6210\u7387\u3002", "result": "\u8d85\u8fc7\u4e34\u754c\u566a\u58f0\u6c34\u5e73\u65f6\uff0c\u5927\u62e5\u5835\u4e0d\u4f1a\u6301\u7eed\uff1b\u7b80\u5355\u53cd\u5e94\u65b9\u6848\u5728\u4e2d\u7b49\u5bc6\u5ea6\u4e0b\u8868\u73b0\u826f\u597d\u4e14\u8ba1\u7b97\u9ad8\u6548\u3002", "conclusion": "\u7b80\u5355\u53cd\u5e94\u65b9\u6848\u5728\u73b0\u5b9e\u95ee\u9898\u4e2d\u66f4\u5177\u5b9e\u7528\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2507.08112", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.08112", "abs": "https://arxiv.org/abs/2507.08112", "authors": ["Lamiaa H. Zain", "Hossam H. Ammar", "Raafat E. Shalaby"], "title": "Imitation Learning for Obstacle Avoidance Using End-to-End CNN-Based Sensor Fusion", "comment": null, "summary": "Obstacle avoidance is crucial for mobile robots' navigation in both known and\nunknown environments. This research designs, trains, and tests two custom\nConvolutional Neural Networks (CNNs), using color and depth images from a depth\ncamera as inputs. Both networks adopt sensor fusion to produce an output: the\nmobile robot's angular velocity, which serves as the robot's steering command.\nA newly obtained visual dataset for navigation was collected in diverse\nenvironments with varying lighting conditions and dynamic obstacles. During\ndata collection, a communication link was established over Wi-Fi between a\nremote server and the robot, using Robot Operating System (ROS) topics.\nVelocity commands were transmitted from the server to the robot, enabling\nsynchronized recording of visual data and the corresponding steering commands.\nVarious evaluation metrics, such as Mean Squared Error, Variance Score, and\nFeed-Forward time, provided a clear comparison between the two networks and\nclarified which one to use for the application.", "AI": {"tldr": "\u7814\u7a76\u8bbe\u8ba1\u5e76\u6d4b\u8bd5\u4e86\u4e24\u79cd\u81ea\u5b9a\u4e49CNN\uff0c\u7528\u4e8e\u79fb\u52a8\u673a\u5668\u4eba\u7684\u907f\u969c\u5bfc\u822a\uff0c\u901a\u8fc7\u4f20\u611f\u5668\u878d\u5408\u8f93\u51fa\u89d2\u901f\u5ea6\u4f5c\u4e3a\u8f6c\u5411\u6307\u4ee4\u3002", "motivation": "\u79fb\u52a8\u673a\u5668\u4eba\u5728\u5df2\u77e5\u548c\u672a\u77e5\u73af\u5883\u4e2d\u7684\u907f\u969c\u5bfc\u822a\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u7528\u6df1\u5ea6\u76f8\u673a\u7684\u5f69\u8272\u548c\u6df1\u5ea6\u56fe\u50cf\u4f5c\u4e3a\u8f93\u5165\uff0c\u8bbe\u8ba1\u5e76\u8bad\u7ec3\u4e24\u79cdCNN\uff0c\u901a\u8fc7\u4f20\u611f\u5668\u878d\u5408\u751f\u6210\u89d2\u901f\u5ea6\u8f93\u51fa\u3002", "result": "\u5728\u591a\u6837\u5316\u73af\u5883\u4e2d\u6536\u96c6\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7\u591a\u79cd\u8bc4\u4f30\u6307\u6807\u6bd4\u8f83\u4e24\u79cd\u7f51\u7edc\u7684\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u660e\u786e\u4e86\u54ea\u79cd\u7f51\u7edc\u66f4\u9002\u5408\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2507.08224", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.08224", "abs": "https://arxiv.org/abs/2507.08224", "authors": ["Chan Young Park", "Jillian Fisher", "Marius Memmel", "Dipika Khullar", "Andy Yun", "Abhishek Gupta", "Yejin Choi"], "title": "Making VLMs More Robot-Friendly: Self-Critical Distillation of Low-Level Procedural Reasoning", "comment": "Code Available: https://github.com/chan0park/SelfReVision", "summary": "Large language models (LLMs) have shown promise in robotic procedural\nplanning, yet their human-centric reasoning often omits the low-level, grounded\ndetails needed for robotic execution. Vision-language models (VLMs) offer a\npath toward more perceptually grounded plans, but current methods either rely\non expensive, large-scale models or are constrained to narrow simulation\nsettings. We introduce SelfReVision, a lightweight and scalable\nself-improvement framework for vision-language procedural planning.\nSelfReVision enables small VLMs to iteratively critique, revise, and verify\ntheir own plans-without external supervision or teacher models-drawing\ninspiration from chain-of-thought prompting and self-instruct paradigms.\nThrough this self-distillation loop, models generate higher-quality,\nexecution-ready plans that can be used both at inference and for continued\nfine-tuning. Using models varying from 3B to 72B, our results show that\nSelfReVision not only boosts performance over weak base VLMs but also\noutperforms models 100X the size, yielding improved control in downstream\nembodied tasks.", "AI": {"tldr": "SelfReVision\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u53ef\u6269\u5c55\u7684\u81ea\u6211\u6539\u8fdb\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u673a\u5668\u4eba\u7a0b\u5e8f\u89c4\u5212\u4e2d\u7684\u8868\u73b0\uff0c\u65e0\u9700\u5916\u90e8\u76d1\u7763\u6216\u6559\u5e08\u6a21\u578b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u673a\u5668\u4eba\u7a0b\u5e8f\u89c4\u5212\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u7f3a\u4e4f\u4f4e\u5c42\u6b21\u7ec6\u8282\uff1b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u867d\u80fd\u63d0\u4f9b\u66f4\u63a5\u5730\u6c14\u7684\u89c4\u5212\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u7684\u5927\u6a21\u578b\u6216\u5c40\u9650\u4e8e\u6a21\u62df\u73af\u5883\u3002", "method": "SelfReVision\u901a\u8fc7\u81ea\u6211\u6279\u8bc4\u3001\u4fee\u8ba2\u548c\u9a8c\u8bc1\u7684\u5faa\u73af\uff0c\u5229\u7528\u94fe\u5f0f\u601d\u7ef4\u63d0\u793a\u548c\u81ea\u6211\u6307\u5bfc\u8303\u5f0f\uff0c\u4f7f\u5c0f\u578bVLMs\u751f\u6210\u66f4\u9ad8\u8d28\u91cf\u7684\u3001\u53ef\u6267\u884c\u7684\u89c4\u5212\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSelfReVision\u4e0d\u4ec5\u63d0\u5347\u4e86\u57fa\u7840VLMs\u7684\u6027\u80fd\uff0c\u8fd8\u4f18\u4e8e\u89c4\u6a21\u5927100\u500d\u7684\u6a21\u578b\uff0c\u5e76\u5728\u4e0b\u6e38\u5177\u4f53\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "SelfReVision\u4e3a\u8f7b\u91cf\u7ea7VLMs\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u81ea\u6211\u6539\u8fdb\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c4\u5212\u8d28\u91cf\u548c\u6267\u884c\u6548\u679c\u3002"}}
{"id": "2507.08262", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.08262", "abs": "https://arxiv.org/abs/2507.08262", "authors": ["Wenbo Cui", "Chengyang Zhao", "Yuhui Chen", "Haoran Li", "Zhizheng Zhang", "Dongbin Zhao", "He Wang"], "title": "CL3R: 3D Reconstruction and Contrastive Learning for Enhanced Robotic Manipulation Representations", "comment": null, "summary": "Building a robust perception module is crucial for visuomotor policy\nlearning. While recent methods incorporate pre-trained 2D foundation models\ninto robotic perception modules to leverage their strong semantic\nunderstanding, they struggle to capture 3D spatial information and generalize\nacross diverse camera viewpoints. These limitations hinder the policy's\neffectiveness, especially in fine-grained robotic manipulation scenarios. To\naddress these challenges, we propose CL3R, a novel 3D pre-training framework\ndesigned to enhance robotic manipulation policies. Our method integrates both\nspatial awareness and semantic understanding by employing a point cloud Masked\nAutoencoder to learn rich 3D representations while leveraging pre-trained 2D\nfoundation models through contrastive learning for efficient semantic knowledge\ntransfer. Additionally, we propose a 3D visual representation pre-training\nframework for robotic tasks. By unifying coordinate systems across datasets and\nintroducing random fusion of multi-view point clouds, we mitigate camera view\nambiguity and improve generalization, enabling robust perception from novel\nviewpoints at test time. Extensive experiments in both simulation and the real\nworld demonstrate the superiority of our method, highlighting its effectiveness\nin visuomotor policy learning for robotic manipulation.", "AI": {"tldr": "CL3R\u662f\u4e00\u79cd\u65b0\u578b3D\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u7a7a\u95f4\u611f\u77e5\u548c\u8bed\u4e49\u7406\u89e3\uff0c\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u7684\u611f\u77e5\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u9884\u8bad\u7ec3\u76842D\u57fa\u7840\u6a21\u578b\uff0c\u4f46\u7f3a\u4e4f3D\u7a7a\u95f4\u4fe1\u606f\u548c\u5bf9\u591a\u89c6\u89d2\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u9650\u5236\u4e86\u673a\u5668\u4eba\u7cbe\u7ec6\u64cd\u4f5c\u7684\u6548\u679c\u3002", "method": "\u91c7\u7528\u70b9\u4e91\u63a9\u7801\u81ea\u7f16\u7801\u5668\u5b66\u4e603D\u8868\u793a\uff0c\u5e76\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u5229\u7528\u9884\u8bad\u7ec32D\u6a21\u578b\u7684\u8bed\u4e49\u77e5\u8bc6\u3002\u7edf\u4e00\u5750\u6807\u7cfb\u5e76\u878d\u5408\u591a\u89c6\u89d2\u70b9\u4e91\u4ee5\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCL3R\u5728\u4eff\u771f\u548c\u73b0\u5b9e\u573a\u666f\u4e2d\u5747\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u611f\u77e5\u80fd\u529b\u3002", "conclusion": "CL3R\u901a\u8fc7\u7ed3\u54083D\u7a7a\u95f4\u548c\u8bed\u4e49\u4fe1\u606f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u611f\u77e5\u6311\u6218\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.08001", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.08001", "abs": "https://arxiv.org/abs/2507.08001", "authors": ["Shengyi Xie"], "title": "Human Creativity and AI", "comment": null, "summary": "With the advancement of science and technology, the philosophy of creativity\nhas undergone significant reinterpretation. This paper investigates\ncontemporary research in the fields of psychology, cognitive neuroscience, and\nthe philosophy of creativity, particularly in the context of the development of\nartificial intelligence (AI) techniques. It aims to address the central\nquestion: Can AI exhibit creativity? The paper reviews the historical\nperspectives on the philosophy of creativity and explores the influence of\npsychological advancements on the study of creativity. Furthermore, it analyzes\nvarious definitions of creativity and examines the responses of naturalism and\ncognitive neuroscience to the concept of creativity.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8AI\u662f\u5426\u80fd\u5c55\u73b0\u521b\u9020\u529b\uff0c\u7ed3\u5408\u5fc3\u7406\u5b66\u3001\u8ba4\u77e5\u795e\u7ecf\u79d1\u5b66\u548c\u54f2\u5b66\u89c6\u89d2\uff0c\u5206\u6790\u521b\u9020\u529b\u7684\u5b9a\u4e49\u53ca\u5176\u5728AI\u6280\u672f\u53d1\u5c55\u4e2d\u7684\u5f71\u54cd\u3002", "motivation": "\u968f\u7740\u79d1\u6280\u8fdb\u6b65\uff0c\u521b\u9020\u529b\u54f2\u5b66\u88ab\u91cd\u65b0\u8be0\u91ca\uff0c\u7814\u7a76AI\u662f\u5426\u5177\u5907\u521b\u9020\u529b\u6210\u4e3a\u6838\u5fc3\u95ee\u9898\u3002", "method": "\u56de\u987e\u5386\u53f2\u89c6\u89d2\uff0c\u7ed3\u5408\u5fc3\u7406\u5b66\u548c\u8ba4\u77e5\u795e\u7ecf\u79d1\u5b66\u8fdb\u5c55\uff0c\u5206\u6790\u521b\u9020\u529b\u7684\u5b9a\u4e49\u53ca\u81ea\u7136\u4e3b\u4e49\u4e0e\u8ba4\u77e5\u795e\u7ecf\u79d1\u5b66\u7684\u56de\u5e94\u3002", "result": "\u901a\u8fc7\u591a\u5b66\u79d1\u89c6\u89d2\u63a2\u8ba8AI\u521b\u9020\u529b\u7684\u53ef\u80fd\u6027\uff0c\u63ed\u793a\u6280\u672f\u4e0e\u54f2\u5b66\u7684\u4ea4\u7ec7\u5173\u7cfb\u3002", "conclusion": "AI\u521b\u9020\u529b\u7684\u7814\u7a76\u9700\u7ed3\u5408\u54f2\u5b66\u3001\u5fc3\u7406\u5b66\u548c\u795e\u7ecf\u79d1\u5b66\uff0c\u672a\u6765\u53ef\u80fd\u8fdb\u4e00\u6b65\u63a8\u52a8\u6280\u672f\u4e0e\u4eba\u7c7b\u7406\u89e3\u7684\u878d\u5408\u3002"}}
{"id": "2507.08303", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.08303", "abs": "https://arxiv.org/abs/2507.08303", "authors": ["Yang Zhang", "Zhanxiang Cao", "Buqing Nie", "Haoyang Li", "Yue Gao"], "title": "Learning Robust Motion Skills via Critical Adversarial Attacks for Humanoid Robots", "comment": "10 pages, 9 figures", "summary": "Humanoid robots show significant potential in daily tasks. However,\nreinforcement learning-based motion policies often suffer from robustness\ndegradation due to the sim-to-real dynamics gap, thereby affecting the agility\nof real robots. In this work, we propose a novel robust adversarial training\nparadigm designed to enhance the robustness of humanoid motion policies in real\nworlds. The paradigm introduces a learnable adversarial attack network that\nprecisely identifies vulnerabilities in motion policies and applies targeted\nperturbations, forcing the motion policy to enhance its robustness against\nperturbations through dynamic adversarial training. We conduct experiments on\nthe Unitree G1 humanoid robot for both perceptive locomotion and whole-body\ncontrol tasks. The results demonstrate that our proposed method significantly\nenhances the robot's motion robustness in real world environments, enabling\nsuccessful traversal of challenging terrains and highly agile whole-body\ntrajectory tracking.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u9c81\u68d2\u5bf9\u6297\u8bad\u7ec3\u8303\u5f0f\uff0c\u901a\u8fc7\u52a8\u6001\u5bf9\u6297\u8bad\u7ec3\u589e\u5f3a\u4eba\u5f62\u673a\u5668\u4eba\u8fd0\u52a8\u7b56\u7565\u7684\u9c81\u68d2\u6027\uff0c\u663e\u8457\u63d0\u5347\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u8fd0\u52a8\u6027\u80fd\u3002", "motivation": "\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u8fd0\u52a8\u7b56\u7565\u56e0\u4eff\u771f\u4e0e\u73b0\u5b9e\u7684\u52a8\u529b\u5b66\u5dee\u5f02\u5bfc\u81f4\u9c81\u68d2\u6027\u4e0b\u964d\uff0c\u5f71\u54cd\u771f\u5b9e\u673a\u5668\u4eba\u7684\u654f\u6377\u6027\u3002", "method": "\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u5bf9\u6297\u653b\u51fb\u7f51\u7edc\uff0c\u7cbe\u51c6\u8bc6\u522b\u8fd0\u52a8\u7b56\u7565\u7684\u8106\u5f31\u70b9\u5e76\u65bd\u52a0\u9488\u5bf9\u6027\u6270\u52a8\uff0c\u901a\u8fc7\u52a8\u6001\u5bf9\u6297\u8bad\u7ec3\u589e\u5f3a\u9c81\u68d2\u6027\u3002", "result": "\u5728Unitree G1\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u8fd0\u52a8\u9c81\u68d2\u6027\uff0c\u6210\u529f\u5e94\u5bf9\u590d\u6742\u5730\u5f62\u548c\u9ad8\u654f\u6377\u6027\u5168\u8eab\u8f68\u8ff9\u8ddf\u8e2a\u4efb\u52a1\u3002", "conclusion": "\u63d0\u51fa\u7684\u5bf9\u6297\u8bad\u7ec3\u8303\u5f0f\u6709\u6548\u89e3\u51b3\u4e86\u4eff\u771f\u4e0e\u73b0\u5b9e\u95f4\u7684\u9c81\u68d2\u6027\u5dee\u8ddd\uff0c\u4e3a\u4eba\u5f62\u673a\u5668\u4eba\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u9ad8\u6027\u80fd\u8fd0\u52a8\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2507.08046", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08046", "abs": "https://arxiv.org/abs/2507.08046", "authors": ["Sishi Xiong", "Dakai Wang", "Yu Zhao", "Jie Zhang", "Changzai Pan", "Haowei He", "Xiangyu Li", "Wenhan Chang", "Zhongjiang He", "Shuangyong Song", "Yongxiang Li"], "title": "TableReasoner: Advancing Table Reasoning Framework with Large Language Models", "comment": null, "summary": "The paper presents our system developed for table question answering (TQA).\nTQA tasks face challenges due to the characteristics of real-world tabular\ndata, such as large size, incomplete column semantics, and entity ambiguity. To\naddress these issues, we propose a large language model (LLM)-powered and\nprogramming-based table reasoning framework, named TableReasoner. It models a\ntable using the schema that combines structural and semantic representations,\nenabling holistic understanding and efficient processing of large tables. We\ndesign a multi-step schema linking plan to derive a focused table schema that\nretains only query-relevant information, eliminating ambiguity and alleviating\nhallucinations. This focused table schema provides precise and sufficient table\ndetails for query refinement and programming. Furthermore, we integrate the\nreasoning workflow into an iterative thinking architecture, allowing\nincremental cycles of thinking, reasoning and reflection. Our system achieves\nfirst place in both subtasks of SemEval-2025 Task 8.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u7f16\u7a0b\u7684\u8868\u95ee\u7b54\u7cfb\u7edfTableReasoner\uff0c\u901a\u8fc7\u7ed3\u5408\u7ed3\u6784\u548c\u8bed\u4e49\u8868\u793a\u7684\u8868\u6a21\u5f0f\u89e3\u51b3\u5927\u89c4\u6a21\u3001\u4e0d\u5b8c\u6574\u5217\u8bed\u4e49\u548c\u5b9e\u4f53\u6b67\u4e49\u7b49\u95ee\u9898\uff0c\u5e76\u5728SemEval-2025 Task 8\u4e2d\u53d6\u5f97\u7b2c\u4e00\u540d\u3002", "motivation": "\u8868\u95ee\u7b54\u4efb\u52a1\u9762\u4e34\u5927\u89c4\u6a21\u3001\u4e0d\u5b8c\u6574\u5217\u8bed\u4e49\u548c\u5b9e\u4f53\u6b67\u4e49\u7b49\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5168\u9762\u7406\u89e3\u8868\u5185\u5bb9\u5e76\u9ad8\u6548\u5904\u7406\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faTableReasoner\u6846\u67b6\uff0c\u7ed3\u5408\u7ed3\u6784\u548c\u8bed\u4e49\u8868\u793a\u7684\u8868\u6a21\u5f0f\uff0c\u8bbe\u8ba1\u591a\u6b65\u6a21\u5f0f\u94fe\u63a5\u8ba1\u5212\u4ee5\u805a\u7126\u67e5\u8be2\u76f8\u5173\u4fe1\u606f\uff0c\u5e76\u96c6\u6210\u8fed\u4ee3\u601d\u8003\u67b6\u6784\u5b9e\u73b0\u589e\u91cf\u63a8\u7406\u3002", "result": "\u7cfb\u7edf\u5728SemEval-2025 Task 8\u7684\u4e24\u4e2a\u5b50\u4efb\u52a1\u4e2d\u5747\u53d6\u5f97\u7b2c\u4e00\u540d\u3002", "conclusion": "TableReasoner\u901a\u8fc7\u7ed3\u5408\u8868\u6a21\u5f0f\u548c\u591a\u6b65\u63a8\u7406\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8868\u95ee\u7b54\u4e2d\u7684\u6311\u6218\uff0c\u5e76\u5c55\u793a\u4e86\u5353\u8d8a\u6027\u80fd\u3002"}}
{"id": "2507.08349", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.08349", "abs": "https://arxiv.org/abs/2507.08349", "authors": ["Junhui Wang", "Yan Qiao", "Chao Gao", "Naiqi Wu"], "title": "Joint Optimization-based Targetless Extrinsic Calibration for Multiple LiDARs and GNSS-Aided INS of Ground Vehicles", "comment": null, "summary": "Accurate extrinsic calibration between multiple LiDAR sensors and a\nGNSS-aided inertial navigation system (GINS) is essential for achieving\nreliable sensor fusion in intelligent mining environments. Such calibration\nenables vehicle-road collaboration by aligning perception data from\nvehicle-mounted sensors to a unified global reference frame. However, existing\nmethods often depend on artificial targets, overlapping fields of view, or\nprecise trajectory estimation, which are assumptions that may not hold in\npractice. Moreover, the planar motion of mining vehicles leads to observability\nissues that degrade calibration performance. This paper presents a targetless\nextrinsic calibration method that aligns multiple onboard LiDAR sensors to the\nGINS coordinate system without requiring overlapping sensor views or external\ntargets. The proposed approach introduces an observation model based on the\nknown installation height of the GINS unit to constrain unobservable\ncalibration parameters under planar motion. A joint optimization framework is\ndeveloped to refine both the extrinsic parameters and GINS trajectory by\nintegrating multiple constraints derived from geometric correspondences and\nmotion consistency. The proposed method is applicable to heterogeneous LiDAR\nconfigurations, including both mechanical and solid-state sensors. Extensive\nexperiments on simulated and real-world datasets demonstrate the accuracy,\nrobustness, and practical applicability of the approach under diverse sensor\nsetups.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65e0\u9700\u5916\u90e8\u76ee\u6807\u6216\u91cd\u53e0\u89c6\u91ce\u7684\u591aLiDAR\u4e0eGINS\u7684\u5916\u53c2\u6807\u5b9a\u65b9\u6cd5\uff0c\u901a\u8fc7\u51e0\u4f55\u5bf9\u5e94\u548c\u8fd0\u52a8\u4e00\u81f4\u6027\u4f18\u5316\u53c2\u6570\u3002", "motivation": "\u667a\u80fd\u91c7\u77ff\u73af\u5883\u4e2d\uff0c\u591aLiDAR\u4e0eGINS\u7684\u7cbe\u786e\u5916\u53c2\u6807\u5b9a\u5bf9\u4f20\u611f\u5668\u878d\u5408\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u76ee\u6807\u6216\u91cd\u53e0\u89c6\u91ce\uff0c\u4e14\u5e73\u9762\u8fd0\u52a8\u5bfc\u81f4\u53ef\u89c2\u6d4b\u6027\u95ee\u9898\u3002", "method": "\u57fa\u4e8eGINS\u5b89\u88c5\u9ad8\u5ea6\u7684\u89c2\u6d4b\u6a21\u578b\u7ea6\u675f\u4e0d\u53ef\u89c2\u6d4b\u53c2\u6570\uff0c\u7ed3\u5408\u51e0\u4f55\u5bf9\u5e94\u548c\u8fd0\u52a8\u4e00\u81f4\u6027\uff0c\u5f00\u53d1\u8054\u5408\u4f18\u5316\u6846\u67b6\u3002", "result": "\u6a21\u62df\u548c\u771f\u5b9e\u6570\u636e\u96c6\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u548c\u9002\u7528\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u5f02\u6784LiDAR\u914d\u7f6e\uff0c\u89e3\u51b3\u4e86\u5e73\u9762\u8fd0\u52a8\u4e0b\u7684\u6807\u5b9a\u95ee\u9898\u3002"}}
{"id": "2507.08207", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08207", "abs": "https://arxiv.org/abs/2507.08207", "authors": ["Zhengye Han", "Quanyan Zhu"], "title": "A Dynamic Stackelberg Game Framework for Agentic AI Defense Against LLM Jailbreaking", "comment": null, "summary": "As large language models (LLMs) are increasingly deployed in critical\napplications, the challenge of jailbreaking, where adversaries manipulate the\nmodels to bypass safety mechanisms, has become a significant concern. This\npaper presents a dynamic Stackelberg game framework to model the interactions\nbetween attackers and defenders in the context of LLM jailbreaking. The\nframework treats the prompt-response dynamics as a sequential extensive-form\ngame, where the defender, as the leader, commits to a strategy while\nanticipating the attacker's optimal responses. We propose a novel agentic AI\nsolution, the \"Purple Agent,\" which integrates adversarial exploration and\ndefensive strategies using Rapidly-exploring Random Trees (RRT). The Purple\nAgent actively simulates potential attack trajectories and intervenes\nproactively to prevent harmful outputs. This approach offers a principled\nmethod for analyzing adversarial dynamics and provides a foundation for\nmitigating the risk of jailbreaking.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001Stackelberg\u535a\u5f08\u6846\u67b6\uff0c\u7528\u4e8e\u5efa\u6a21\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8d8a\u72f1\u653b\u51fb\u4e2d\u653b\u51fb\u8005\u4e0e\u9632\u5fa1\u8005\u7684\u4ea4\u4e92\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a\u201c\u7d2b\u8272\u4ee3\u7406\u201d\u7684AI\u89e3\u51b3\u65b9\u6848\uff0c\u7ed3\u5408\u5bf9\u6297\u63a2\u7d22\u548c\u9632\u5fa1\u7b56\u7565\u3002", "motivation": "\u968f\u7740LLM\u5728\u5173\u952e\u5e94\u7528\u4e2d\u7684\u90e8\u7f72\u589e\u52a0\uff0c\u8d8a\u72f1\u653b\u51fb\uff08\u7ed5\u8fc7\u5b89\u5168\u673a\u5236\uff09\u6210\u4e3a\u91cd\u8981\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u7cfb\u7edf\u5316\u7684\u9632\u5fa1\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u52a8\u6001Stackelberg\u535a\u5f08\u6846\u67b6\uff0c\u5c06\u63d0\u793a-\u54cd\u5e94\u52a8\u6001\u5efa\u6a21\u4e3a\u5e8f\u8d2f\u6269\u5c55\u535a\u5f08\uff0c\u63d0\u51fa\u201c\u7d2b\u8272\u4ee3\u7406\u201d\u7ed3\u5408RRT\u7b97\u6cd5\u8fdb\u884c\u5bf9\u6297\u63a2\u7d22\u548c\u9632\u5fa1\u3002", "result": "\u7d2b\u8272\u4ee3\u7406\u80fd\u4e3b\u52a8\u6a21\u62df\u653b\u51fb\u8f68\u8ff9\u5e76\u5e72\u9884\uff0c\u9632\u6b62\u6709\u5bb3\u8f93\u51fa\uff0c\u4e3a\u5206\u6790\u5bf9\u6297\u52a8\u6001\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u7f13\u89e3LLM\u8d8a\u72f1\u98ce\u9669\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u65b9\u6cd5\uff0c\u5177\u6709\u7406\u8bba\u548c\u5b9e\u8df5\u610f\u4e49\u3002"}}
{"id": "2507.08364", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.08364", "abs": "https://arxiv.org/abs/2507.08364", "authors": ["Deteng Zhang", "Junjie Zhang", "Yan Sun", "Tao Li", "Hao Yin", "Hongzhao Xie", "Jie Yin"], "title": "Towards Robust Sensor-Fusion Ground SLAM: A Comprehensive Benchmark and A Resilient Framework", "comment": "This paper has already been accepted to IROS2025. 8 pages", "summary": "Considerable advancements have been achieved in SLAM methods tailored for\nstructured environments, yet their robustness under challenging corner cases\nremains a critical limitation. Although multi-sensor fusion approaches\nintegrating diverse sensors have shown promising performance improvements, the\nresearch community faces two key barriers: On one hand, the lack of\nstandardized and configurable benchmarks that systematically evaluate SLAM\nalgorithms under diverse degradation scenarios hinders comprehensive\nperformance assessment. While on the other hand, existing SLAM frameworks\nprimarily focus on fusing a limited set of sensor types, without effectively\naddressing adaptive sensor selection strategies for varying environmental\nconditions.\n  To bridge these gaps, we make three key contributions: First, we introduce\nM3DGR dataset: a sensor-rich benchmark with systematically induced degradation\npatterns including visual challenge, LiDAR degeneracy, wheel slippage and GNSS\ndenial. Second, we conduct a comprehensive evaluation of forty SLAM systems on\nM3DGR, providing critical insights into their robustness and limitations under\nchallenging real-world conditions. Third, we develop a resilient modular\nmulti-sensor fusion framework named Ground-Fusion++, which demonstrates robust\nperformance by coupling GNSS, RGB-D, LiDAR, IMU (Inertial Measurement Unit) and\nwheel odometry. Codes and datasets are publicly available.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86M3DGR\u6570\u636e\u96c6\u548cGround-Fusion++\u6846\u67b6\uff0c\u586b\u8865\u4e86SLAM\u7b97\u6cd5\u5728\u590d\u6742\u73af\u5883\u4e0b\u8bc4\u4f30\u548c\u81ea\u9002\u5e94\u4f20\u611f\u5668\u878d\u5408\u7684\u7a7a\u767d\u3002", "motivation": "\u5f53\u524dSLAM\u65b9\u6cd5\u5728\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u6781\u7aef\u6761\u4ef6\u4e0b\u9c81\u68d2\u6027\u4e0d\u8db3\uff0c\u4e14\u7f3a\u4e4f\u6807\u51c6\u5316\u8bc4\u4f30\u548c\u591a\u4f20\u611f\u5668\u81ea\u9002\u5e94\u878d\u5408\u7b56\u7565\u3002", "method": "1. \u63d0\u51faM3DGR\u6570\u636e\u96c6\uff0c\u5305\u542b\u591a\u79cd\u4f20\u611f\u5668\u9000\u5316\u573a\u666f\uff1b2. \u8bc4\u4f3040\u79cdSLAM\u7cfb\u7edf\uff1b3. \u5f00\u53d1Ground-Fusion++\u6846\u67b6\uff0c\u878d\u5408\u591a\u79cd\u4f20\u611f\u5668\u3002", "result": "M3DGR\u6570\u636e\u96c6\u548cGround-Fusion++\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86SLAM\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u6027\u80fd\u3002", "conclusion": "\u8bba\u6587\u901a\u8fc7\u6570\u636e\u96c6\u548c\u6846\u67b6\u7684\u8d21\u732e\uff0c\u63a8\u52a8\u4e86SLAM\u5728\u6311\u6218\u6027\u73af\u5883\u4e2d\u7684\u7814\u7a76\u548c\u5e94\u7528\u3002"}}
{"id": "2507.08208", "categories": ["cs.AI", "cs.GT"], "pdf": "https://arxiv.org/pdf/2507.08208", "abs": "https://arxiv.org/abs/2507.08208", "authors": ["Quanyan Zhu"], "title": "Reasoning and Behavioral Equilibria in LLM-Nash Games: From Mindsets to Actions", "comment": null, "summary": "We introduce the LLM-Nash framework, a game-theoretic model where agents\nselect reasoning prompts to guide decision-making via Large Language Models\n(LLMs). Unlike classical games that assume utility-maximizing agents with full\nrationality, this framework captures bounded rationality by modeling the\nreasoning process explicitly. Equilibrium is defined over the prompt space,\nwith actions emerging as the behavioral output of LLM inference. This approach\nenables the study of cognitive constraints, mindset expressiveness, and\nepistemic learning. Through illustrative examples, we show how reasoning\nequilibria can diverge from classical Nash outcomes, offering a new foundation\nfor strategic interaction in LLM-enabled systems.", "AI": {"tldr": "LLM-Nash\u6846\u67b6\u901a\u8fc7\u6e38\u620f\u7406\u8bba\u6a21\u578b\u7814\u7a76LLM\u9a71\u52a8\u7684\u51b3\u7b56\uff0c\u5173\u6ce8\u6709\u9650\u7406\u6027\u4e0b\u7684\u63a8\u7406\u5747\u8861\u3002", "motivation": "\u4f20\u7edf\u535a\u5f08\u8bba\u5047\u8bbe\u5b8c\u5168\u7406\u6027\uff0c\u800cLLM-Nash\u6846\u67b6\u65e8\u5728\u6355\u6349\u6709\u9650\u7406\u6027\uff0c\u7814\u7a76\u63a8\u7406\u8fc7\u7a0b\u5bf9\u51b3\u7b56\u7684\u5f71\u54cd\u3002", "method": "\u6a21\u578b\u5728\u63d0\u793a\u7a7a\u95f4\u4e2d\u5b9a\u4e49\u5747\u8861\uff0c\u901a\u8fc7LLM\u63a8\u7406\u751f\u6210\u884c\u4e3a\u8f93\u51fa\uff0c\u5206\u6790\u8ba4\u77e5\u7ea6\u675f\u548c\u601d\u7ef4\u8868\u8fbe\u3002", "result": "\u63a8\u7406\u5747\u8861\u4e0e\u4f20\u7edf\u7eb3\u4ec0\u5747\u8861\u5b58\u5728\u5dee\u5f02\uff0c\u4e3aLLM\u7cfb\u7edf\u7684\u6218\u7565\u4e92\u52a8\u63d0\u4f9b\u65b0\u89c6\u89d2\u3002", "conclusion": "LLM-Nash\u6846\u67b6\u4e3a\u7814\u7a76\u6709\u9650\u7406\u6027\u4e0b\u7684\u6218\u7565\u4e92\u52a8\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\uff0c\u62d3\u5c55\u4e86\u535a\u5f08\u8bba\u7684\u5e94\u7528\u8303\u56f4\u3002"}}
{"id": "2507.08366", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08366", "abs": "https://arxiv.org/abs/2507.08366", "authors": ["Ghaith El-Dalahmeh", "Mohammad Reza Jabbarpour", "Bao Quoc Vo", "Ryszard Kowalczyk"], "title": "Intelligent Control of Spacecraft Reaction Wheel Attitude Using Deep Reinforcement Learning", "comment": null, "summary": "Reliable satellite attitude control is essential for the success of space\nmissions, particularly as satellites increasingly operate autonomously in\ndynamic and uncertain environments. Reaction wheels (RWs) play a pivotal role\nin attitude control, and maintaining control resilience during RW faults is\ncritical to preserving mission objectives and system stability. However,\ntraditional Proportional Derivative (PD) controllers and existing deep\nreinforcement learning (DRL) algorithms such as TD3, PPO, and A2C often fall\nshort in providing the real time adaptability and fault tolerance required for\nautonomous satellite operations. This study introduces a DRL-based control\nstrategy designed to improve satellite resilience and adaptability under fault\nconditions. Specifically, the proposed method integrates Twin Delayed Deep\nDeterministic Policy Gradient (TD3) with Hindsight Experience Replay (HER) and\nDimension Wise Clipping (DWC) referred to as TD3-HD to enhance learning in\nsparse reward environments and maintain satellite stability during RW failures.\nThe proposed approach is benchmarked against PD control and leading DRL\nalgorithms. Experimental results show that TD3-HD achieves significantly lower\nattitude error, improved angular velocity regulation, and enhanced stability\nunder fault conditions. These findings underscore the proposed method potential\nas a powerful, fault tolerant, onboard AI solution for autonomous satellite\nattitude control.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u7684\u536b\u661f\u59ff\u6001\u63a7\u5236\u7b56\u7565TD3-HD\uff0c\u7ed3\u5408TD3\u3001HER\u548cDWC\uff0c\u663e\u8457\u63d0\u5347\u4e86\u536b\u661f\u5728\u6545\u969c\u6761\u4ef6\u4e0b\u7684\u9002\u5e94\u6027\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u536b\u661f\u5728\u52a8\u6001\u548c\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u81ea\u4e3b\u8fd0\u884c\u65f6\uff0c\u4f20\u7edfPD\u63a7\u5236\u5668\u548c\u73b0\u6709DRL\u7b97\u6cd5\uff08\u5982TD3\u3001PPO\u3001A2C\uff09\u65e0\u6cd5\u6ee1\u8db3\u5b9e\u65f6\u9002\u5e94\u6027\u548c\u5bb9\u9519\u9700\u6c42\u3002", "method": "\u6574\u5408TD3\u3001HER\u548cDWC\uff08\u79f0\u4e3aTD3-HD\uff09\uff0c\u4ee5\u589e\u5f3a\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e2d\u7684\u5b66\u4e60\u80fd\u529b\uff0c\u5e76\u5728\u53cd\u5e94\u8f6e\u6545\u969c\u65f6\u4fdd\u6301\u536b\u661f\u7a33\u5b9a\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cTD3-HD\u5728\u59ff\u6001\u8bef\u5dee\u3001\u89d2\u901f\u5ea6\u8c03\u8282\u548c\u6545\u969c\u6761\u4ef6\u4e0b\u7684\u7a33\u5b9a\u6027\u65b9\u9762\u663e\u8457\u4f18\u4e8ePD\u63a7\u5236\u5668\u548c\u5176\u4ed6DRL\u7b97\u6cd5\u3002", "conclusion": "TD3-HD\u662f\u4e00\u79cd\u5f3a\u5927\u7684\u3001\u5bb9\u9519\u7684\u673a\u8f7dAI\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u81ea\u4e3b\u536b\u661f\u59ff\u6001\u63a7\u5236\u3002"}}
{"id": "2507.08210", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08210", "abs": "https://arxiv.org/abs/2507.08210", "authors": ["Fryderyk Mantiuk", "Hanqi Zhou", "Charley M. Wu"], "title": "From Curiosity to Competence: How World Models Interact with the Dynamics of Exploration", "comment": null, "summary": "What drives an agent to explore the world while also maintaining control over\nthe environment? From a child at play to scientists in the lab, intelligent\nagents must balance curiosity (the drive to seek knowledge) with competence\n(the drive to master and control the environment). Bridging cognitive theories\nof intrinsic motivation with reinforcement learning, we ask how evolving\ninternal representations mediate the trade-off between curiosity (novelty or\ninformation gain) and competence (empowerment). We compare two model-based\nagents using handcrafted state abstractions (Tabular) or learning an internal\nworld model (Dreamer). The Tabular agent shows curiosity and competence guide\nexploration in distinct patterns, while prioritizing both improves exploration.\nThe Dreamer agent reveals a two-way interaction between exploration and\nrepresentation learning, mirroring the developmental co-evolution of curiosity\nand competence. Our findings formalize adaptive exploration as a balance\nbetween pursuing the unknown and the controllable, offering insights for\ncognitive theories and efficient reinforcement learning.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u667a\u80fd\u4f53\u5982\u4f55\u5e73\u8861\u597d\u5947\u5fc3\uff08\u63a2\u7d22\u672a\u77e5\uff09\u4e0e\u80fd\u529b\uff08\u63a7\u5236\u73af\u5883\uff09\uff0c\u901a\u8fc7\u6bd4\u8f83\u4e24\u79cd\u6a21\u578b\u9a71\u52a8\u4ee3\u7406\uff08Tabular\u548cDreamer\uff09\uff0c\u63ed\u793a\u4e86\u63a2\u7d22\u4e0e\u8868\u5f81\u5b66\u4e60\u7684\u53cc\u5411\u4e92\u52a8\u3002", "motivation": "\u7814\u7a76\u667a\u80fd\u4f53\u5982\u4f55\u5728\u63a2\u7d22\u4e16\u754c\u7684\u540c\u65f6\u4fdd\u6301\u5bf9\u73af\u5883\u7684\u63a7\u5236\uff0c\u7ed3\u5408\u8ba4\u77e5\u7406\u8bba\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u63a2\u8ba8\u597d\u5947\u5fc3\u4e0e\u80fd\u529b\u4e4b\u95f4\u7684\u6743\u8861\u3002", "method": "\u6bd4\u8f83\u4e24\u79cd\u6a21\u578b\u9a71\u52a8\u4ee3\u7406\uff1a\u57fa\u4e8e\u624b\u5de5\u72b6\u6001\u62bd\u8c61\u7684Tabular\u4ee3\u7406\u548c\u5b66\u4e60\u5185\u90e8\u4e16\u754c\u6a21\u578b\u7684Dreamer\u4ee3\u7406\uff0c\u5206\u6790\u5176\u63a2\u7d22\u884c\u4e3a\u3002", "result": "Tabular\u4ee3\u7406\u663e\u793a\u597d\u5947\u5fc3\u548c\u80fd\u529b\u5f15\u5bfc\u63a2\u7d22\u7684\u4e0d\u540c\u6a21\u5f0f\uff0c\u800cDreamer\u4ee3\u7406\u63ed\u793a\u4e86\u63a2\u7d22\u4e0e\u8868\u5f81\u5b66\u4e60\u7684\u53cc\u5411\u4e92\u52a8\u3002", "conclusion": "\u7814\u7a76\u5f62\u5f0f\u5316\u4e86\u9002\u5e94\u6027\u63a2\u7d22\u4f5c\u4e3a\u8ffd\u6c42\u672a\u77e5\u4e0e\u53ef\u63a7\u4e4b\u95f4\u7684\u5e73\u8861\uff0c\u4e3a\u8ba4\u77e5\u7406\u8bba\u548c\u9ad8\u6548\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002"}}
{"id": "2507.08420", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.08420", "abs": "https://arxiv.org/abs/2507.08420", "authors": ["Haitian Wang", "Hezam Albaqami", "Xinyu Wang", "Muhammad Ibrahim", "Zainy M. Malakan", "Abdullah M. Algamdi", "Mohammed H. Alghamdi", "Ajmal Mian"], "title": "LiDAR, GNSS and IMU Sensor Alignment through Dynamic Time Warping to Construct 3D City Maps", "comment": "Preparing to submit to International Journal of Applied Earth\n  Observation and Geoinformation", "summary": "LiDAR-based 3D mapping suffers from cumulative drift causing global\nmisalignment, particularly in GNSS-constrained environments. To address this,\nwe propose a unified framework that fuses LiDAR, GNSS, and IMU data for\nhigh-resolution city-scale mapping. The method performs velocity-based temporal\nalignment using Dynamic Time Warping and refines GNSS and IMU signals via\nextended Kalman filtering. Local maps are built using Normal Distributions\nTransform-based registration and pose graph optimization with loop closure\ndetection, while global consistency is enforced using GNSS-constrained anchors\nfollowed by fine registration of overlapping segments. We also introduce a\nlarge-scale multimodal dataset captured in Perth, Western Australia to\nfacilitate future research in this direction. Our dataset comprises 144{,}000\nframes acquired with a 128-channel Ouster LiDAR, synchronized RTK-GNSS\ntrajectories, and MEMS-IMU measurements across 21 urban loops. To assess\ngeometric consistency, we evaluated our method using alignment metrics based on\nroad centerlines and intersections to capture both global and local accuracy.\nOur method reduces the average global alignment error from 3.32\\,m to 1.24\\,m,\nachieving a 61.4\\% improvement. The constructed high-fidelity map supports a\nwide range of applications, including smart city planning, geospatial data\nintegration, infrastructure monitoring, and GPS-free navigation. Our method,\nand dataset together establish a new benchmark for evaluating 3D city mapping\nin GNSS-constrained environments. The dataset and code will be released\npublicly.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u878d\u5408LiDAR\u3001GNSS\u548cIMU\u6570\u636e\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3LiDAR 3D\u5efa\u56fe\u4e2d\u7684\u7d2f\u79ef\u6f02\u79fb\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5168\u5c40\u5bf9\u9f50\u7cbe\u5ea6\u3002", "motivation": "LiDAR 3D\u5efa\u56fe\u5728GNSS\u53d7\u9650\u73af\u5883\u4e2d\u5b58\u5728\u7d2f\u79ef\u6f02\u79fb\u95ee\u9898\uff0c\u5bfc\u81f4\u5168\u5c40\u9519\u4f4d\u3002", "method": "\u7ed3\u5408\u52a8\u6001\u65f6\u95f4\u89c4\u6574\uff08DTW\uff09\u548c\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\uff08EKF\uff09\u8fdb\u884c\u6570\u636e\u878d\u5408\uff0c\u4f7f\u7528NDT\u914d\u51c6\u548c\u4f4d\u59ff\u56fe\u4f18\u5316\u6784\u5efa\u5c40\u90e8\u5730\u56fe\uff0c\u5e76\u901a\u8fc7GNSS\u7ea6\u675f\u951a\u70b9\u548c\u91cd\u53e0\u6bb5\u7cbe\u7ec6\u914d\u51c6\u786e\u4fdd\u5168\u5c40\u4e00\u81f4\u6027\u3002", "result": "\u5168\u5c40\u5bf9\u9f50\u8bef\u5dee\u4ece3.32\u7c73\u964d\u81f31.24\u7c73\uff0c\u63d0\u534761.4%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ca\u914d\u5957\u6570\u636e\u96c6\u4e3aGNSS\u53d7\u9650\u73af\u5883\u4e0b\u76843D\u57ce\u5e02\u5efa\u56fe\u8bbe\u7acb\u4e86\u65b0\u57fa\u51c6\uff0c\u652f\u6301\u667a\u80fd\u57ce\u5e02\u89c4\u5212\u7b49\u591a\u79cd\u5e94\u7528\u3002"}}
{"id": "2507.08216", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08216", "abs": "https://arxiv.org/abs/2507.08216", "authors": ["Rodrigo Castellano Ontiveros", "Francesco Giannini", "Marco Gori", "Giuseppe Marra", "Michelangelo Diligenti"], "title": "Grounding Methods for Neural-Symbolic AI", "comment": null, "summary": "A large class of Neural-Symbolic (NeSy) methods employs a machine learner to\nprocess the input entities, while relying on a reasoner based on First-Order\nLogic to represent and process more complex relationships among the entities. A\nfundamental role for these methods is played by the process of logic grounding,\nwhich determines the relevant substitutions for the logic rules using a\n(sub)set of entities. Some NeSy methods use an exhaustive derivation of all\npossible substitutions, preserving the full expressive power of the logic\nknowledge. This leads to a combinatorial explosion in the number of ground\nformulas to consider and, therefore, strongly limits their scalability. Other\nmethods rely on heuristic-based selective derivations, which are generally more\ncomputationally efficient, but lack a justification and provide no guarantees\nof preserving the information provided to and returned by the reasoner. Taking\ninspiration from multi-hop symbolic reasoning, this paper proposes a\nparametrized family of grounding methods generalizing classic Backward\nChaining. Different selections within this family allow us to obtain commonly\nemployed grounding methods as special cases, and to control the trade-off\nbetween expressiveness and scalability of the reasoner. The experimental\nresults show that the selection of the grounding criterion is often as\nimportant as the NeSy method itself.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53c2\u6570\u5316\u7684\u903b\u8f91\u63a5\u5730\u65b9\u6cd5\uff0c\u5e73\u8861\u4e86\u8868\u8fbe\u80fd\u529b\u548c\u53ef\u6269\u5c55\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u63a5\u5730\u6807\u51c6\u9009\u62e9\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u89e3\u51b3\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u4e2d\u903b\u8f91\u63a5\u5730\u5bfc\u81f4\u7684\u7ec4\u5408\u7206\u70b8\u6216\u542f\u53d1\u5f0f\u65b9\u6cd5\u7f3a\u4e4f\u7406\u8bba\u4fdd\u8bc1\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u8df3\u7b26\u53f7\u63a8\u7406\u7684\u53c2\u6570\u5316\u63a5\u5730\u65b9\u6cd5\uff0c\u6269\u5c55\u4e86\u7ecf\u5178\u7684\u540e\u5411\u94fe\u5f0f\u63a8\u7406\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u63a5\u5730\u6807\u51c6\u7684\u9009\u62e9\u5bf9\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u7684\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u53c2\u6570\u5316\u63a5\u5730\u65b9\u6cd5\u80fd\u591f\u7075\u6d3b\u63a7\u5236\u8868\u8fbe\u80fd\u529b\u548c\u53ef\u6269\u5c55\u6027\uff0c\u4e3a\u795e\u7ecf\u7b26\u53f7\u63a8\u7406\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.08572", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.08572", "abs": "https://arxiv.org/abs/2507.08572", "authors": ["Juraj Gavura", "Michal Vavrecka", "Igor Farkas", "Connor Gade"], "title": "Robotic Calibration Based on Haptic Feedback Improves Sim-to-Real Transfer", "comment": "ICANN 2025", "summary": "When inverse kinematics (IK) is adopted to control robotic arms in\nmanipulation tasks, there is often a discrepancy between the end effector (EE)\nposition of the robot model in the simulator and the physical EE in reality. In\nmost robotic scenarios with sim-to-real transfer, we have information about\njoint positions in both simulation and reality, but the EE position is only\navailable in simulation. We developed a novel method to overcome this\ndifficulty based on haptic feedback calibration, using a touchscreen in front\nof the robot that provides information on the EE position in the real\nenvironment. During the calibration procedure, the robot touches specific\npoints on the screen, and the information is stored. In the next stage, we\nbuild a transformation function from the data based on linear transformation\nand neural networks that is capable of outputting all missing variables from\nany partial input (simulated/real joint/EE position). Our results demonstrate\nthat a fully nonlinear neural network model performs best, significantly\nreducing positioning errors.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u89e6\u89c9\u53cd\u9988\u6821\u51c6\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u89e6\u6478\u5c4f\u83b7\u53d6\u771f\u5b9e\u73af\u5883\u4e2d\u673a\u5668\u4eba\u672b\u7aef\u6267\u884c\u5668\u4f4d\u7f6e\uff0c\u663e\u8457\u51cf\u5c11\u5b9a\u4f4d\u8bef\u5dee\u3002", "motivation": "\u89e3\u51b3\u4eff\u771f\u4e0e\u73b0\u5b9e\u4e2d\u673a\u5668\u4eba\u672b\u7aef\u6267\u884c\u5668\u4f4d\u7f6e\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u4ec5\u6709\u4eff\u771f\u4e2d\u672b\u7aef\u6267\u884c\u5668\u4f4d\u7f6e\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u5229\u7528\u89e6\u6478\u5c4f\u6536\u96c6\u673a\u5668\u4eba\u672b\u7aef\u6267\u884c\u5668\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u4f4d\u7f6e\u6570\u636e\uff0c\u57fa\u4e8e\u7ebf\u6027\u53d8\u6362\u548c\u795e\u7ecf\u7f51\u7edc\u6784\u5efa\u8f6c\u6362\u51fd\u6570\u3002", "result": "\u5b8c\u5168\u975e\u7ebf\u6027\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u5b9a\u4f4d\u8bef\u5dee\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u89e6\u89c9\u53cd\u9988\u6821\u51c6\u6709\u6548\u89e3\u51b3\u4e86\u4eff\u771f\u5230\u73b0\u5b9e\u8f6c\u79fb\u4e2d\u7684\u672b\u7aef\u6267\u884c\u5668\u4f4d\u7f6e\u4e0d\u4e00\u81f4\u95ee\u9898\u3002"}}
{"id": "2507.08217", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08217", "abs": "https://arxiv.org/abs/2507.08217", "authors": ["Atit Pokharel", "Ratun Rahman", "Thomas Morris", "Dinh C. Nguyen"], "title": "Quantum Federated Learning for Multimodal Data: A Modality-Agnostic Approach", "comment": "This paper was presented at BEAM with CVPR 2025", "summary": "Quantum federated learning (QFL) has been recently introduced to enable a\ndistributed privacy-preserving quantum machine learning (QML) model training\nacross quantum processors (clients). Despite recent research efforts, existing\nQFL frameworks predominantly focus on unimodal systems, limiting their\napplicability to real-world tasks that often naturally involve multiple\nmodalities. To fill this significant gap, we present for the first time a novel\nmultimodal approach specifically tailored for the QFL setting with the\nintermediate fusion using quantum entanglement. Furthermore, to address a major\nbottleneck in multimodal QFL, where the absence of certain modalities during\ntraining can degrade model performance, we introduce a Missing Modality\nAgnostic (MMA) mechanism that isolates untrained quantum circuits, ensuring\nstable training without corrupted states. Simulation results demonstrate that\nthe proposed multimodal QFL method with MMA yields an improvement in accuracy\nof 6.84% in independent and identically distributed (IID) and 7.25% in non-IID\ndata distributions compared to the state-of-the-art methods.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u91cf\u5b50\u8054\u90a6\u5b66\u4e60\uff08QFL\uff09\u7684\u591a\u6a21\u6001\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u4e86\u7f3a\u5931\u6a21\u6001\u65e0\u5173\uff08MMA\uff09\u673a\u5236\uff0c\u4ee5\u89e3\u51b3\u8bad\u7ec3\u4e2d\u6a21\u6001\u7f3a\u5931\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "motivation": "\u73b0\u6709QFL\u6846\u67b6\u4e3b\u8981\u9488\u5bf9\u5355\u6a21\u6001\u7cfb\u7edf\uff0c\u9650\u5236\u4e86\u5176\u5728\u73b0\u5b9e\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u91cf\u5b50\u7ea0\u7f20\u7684\u4e2d\u95f4\u878d\u5408\u591a\u6a21\u6001QFL\u65b9\u6cd5\uff0c\u5e76\u8bbe\u8ba1\u4e86MMA\u673a\u5236\u4ee5\u9694\u79bb\u672a\u8bad\u7ec3\u7684\u91cf\u5b50\u7535\u8def\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728IID\u548c\u975eIID\u6570\u636e\u5206\u5e03\u4e0b\u5206\u522b\u63d0\u9ad8\u4e866.84%\u548c7.25%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86\u591a\u6a21\u6001QFL\u7684\u7a7a\u767d\uff0c\u5e76\u901a\u8fc7MMA\u673a\u5236\u63d0\u5347\u4e86\u6a21\u578b\u7a33\u5b9a\u6027\u3002"}}
{"id": "2507.08656", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.08656", "abs": "https://arxiv.org/abs/2507.08656", "authors": ["Aravind Elanjimattathil Vijayan", "Andrei Cramariuc", "Mattia Risiglione", "Christian Gehring", "Marco Hutter"], "title": "Multi-critic Learning for Whole-body End-effector Twist Tracking", "comment": null, "summary": "Learning whole-body control for locomotion and arm motions in a single policy\nhas challenges, as the two tasks have conflicting goals. For instance,\nefficient locomotion typically favors a horizontal base orientation, while\nend-effector tracking may benefit from base tilting to extend reachability.\nAdditionally, current Reinforcement Learning (RL) approaches using a pose-based\ntask specification lack the ability to directly control the end-effector\nvelocity, making smoothly executing trajectories very challenging. To address\nthese limitations, we propose an RL-based framework that allows for dynamic,\nvelocity-aware whole-body end-effector control. Our method introduces a\nmulti-critic actor architecture that decouples the reward signals for\nlocomotion and manipulation, simplifying reward tuning and allowing the policy\nto resolve task conflicts more effectively. Furthermore, we design a\ntwist-based end-effector task formulation that can track both discrete poses\nand motion trajectories. We validate our approach through a set of simulation\nand hardware experiments using a quadruped robot equipped with a robotic arm.\nThe resulting controller can simultaneously walk and move its end-effector and\nshows emergent whole-body behaviors, where the base assists the arm in\nextending the workspace, despite a lack of explicit formulations.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u5b9e\u73b0\u52a8\u6001\u3001\u901f\u5ea6\u611f\u77e5\u7684\u5168\u8eab\u672b\u7aef\u6267\u884c\u5668\u63a7\u5236\uff0c\u89e3\u51b3\u884c\u8d70\u548c\u624b\u81c2\u52a8\u4f5c\u7684\u51b2\u7a81\u76ee\u6807\u3002", "motivation": "\u884c\u8d70\u548c\u624b\u81c2\u52a8\u4f5c\u7684\u76ee\u6807\u5b58\u5728\u51b2\u7a81\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u76f4\u63a5\u63a7\u5236\u672b\u7aef\u6267\u884c\u5668\u901f\u5ea6\uff0c\u5bfc\u81f4\u8f68\u8ff9\u6267\u884c\u56f0\u96be\u3002", "method": "\u91c7\u7528\u591a\u8bc4\u8bba\u5bb6\u6f14\u5458\u67b6\u6784\u89e3\u8026\u884c\u8d70\u548c\u64cd\u4f5c\u7684\u5956\u52b1\u4fe1\u53f7\uff0c\u8bbe\u8ba1\u57fa\u4e8e\u626d\u8f6c\u7684\u672b\u7aef\u6267\u884c\u5668\u4efb\u52a1\u516c\u5f0f\u3002", "result": "\u5728\u56db\u8db3\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\uff0c\u63a7\u5236\u5668\u80fd\u540c\u65f6\u884c\u8d70\u548c\u79fb\u52a8\u672b\u7aef\u6267\u884c\u5668\uff0c\u5e76\u8868\u73b0\u51fa\u5168\u8eab\u534f\u540c\u884c\u4e3a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u4efb\u52a1\u51b2\u7a81\uff0c\u5b9e\u73b0\u4e86\u52a8\u6001\u5168\u8eab\u63a7\u5236\u3002"}}
{"id": "2507.08249", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.08249", "abs": "https://arxiv.org/abs/2507.08249", "authors": ["Bill Marino", "Ari Juels"], "title": "Giving AI Agents Access to Cryptocurrency and Smart Contracts Creates New Vectors of AI Harm", "comment": null, "summary": "There is growing interest in giving AI agents access to cryptocurrencies as\nwell as to the smart contracts that transact them. But doing so, this position\npaper argues, could lead to formidable new vectors of AI harm. To support this\nargument, we first examine the unique properties of cryptocurrencies and smart\ncontracts that could lead to these new vectors of harm. Next, we describe each\nof these new vectors of harm in detail. Finally, we conclude with a call for\nmore technical research aimed at preventing and mitigating these harms and,\nthereby making it safer to endow AI agents with cryptocurrencies and smart\ncontracts.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u8d4b\u4e88AI\u4ee3\u7406\u52a0\u5bc6\u8d27\u5e01\u548c\u667a\u80fd\u5408\u7ea6\u8bbf\u95ee\u6743\u9650\u53ef\u80fd\u5e26\u6765\u7684\u65b0\u578b\u5371\u5bb3\uff0c\u5e76\u547c\u5401\u66f4\u591a\u6280\u672f\u7814\u7a76\u4ee5\u9884\u9632\u548c\u51cf\u8f7b\u8fd9\u4e9b\u5371\u5bb3\u3002", "motivation": "\u968f\u7740AI\u4ee3\u7406\u5bf9\u52a0\u5bc6\u8d27\u5e01\u548c\u667a\u80fd\u5408\u7ea6\u7684\u8bbf\u95ee\u9700\u6c42\u589e\u52a0\uff0c\u7814\u7a76\u5176\u6f5c\u5728\u5371\u5bb3\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5206\u6790\u4e86\u52a0\u5bc6\u8d27\u5e01\u548c\u667a\u80fd\u5408\u7ea6\u7684\u72ec\u7279\u6027\u8d28\uff0c\u5e76\u8be6\u7ec6\u63cf\u8ff0\u4e86\u53ef\u80fd\u7684\u65b0\u578b\u5371\u5bb3\u5411\u91cf\u3002", "result": "\u63ed\u793a\u4e86\u65b0\u578bAI\u5371\u5bb3\u7684\u53ef\u80fd\u6027\uff0c\u5f3a\u8c03\u4e86\u9884\u9632\u63aa\u65bd\u7684\u5fc5\u8981\u6027\u3002", "conclusion": "\u547c\u5401\u52a0\u5f3a\u6280\u672f\u7814\u7a76\u4ee5\u786e\u4fddAI\u4ee3\u7406\u5b89\u5168\u4f7f\u7528\u52a0\u5bc6\u8d27\u5e01\u548c\u667a\u80fd\u5408\u7ea6\u3002"}}
{"id": "2507.08726", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.08726", "abs": "https://arxiv.org/abs/2507.08726", "authors": ["Yuekun Wu", "Yik Lung Pang", "Andrea Cavallaro", "Changjae Oh"], "title": "Learning human-to-robot handovers through 3D scene reconstruction", "comment": "8 pages, 6 figures, 2 table", "summary": "Learning robot manipulation policies from raw, real-world image data requires\na large number of robot-action trials in the physical environment. Although\ntraining using simulations offers a cost-effective alternative, the visual\ndomain gap between simulation and robot workspace remains a major limitation.\nGaussian Splatting visual reconstruction methods have recently provided new\ndirections for robot manipulation by generating realistic environments. In this\npaper, we propose the first method for learning supervised-based robot\nhandovers solely from RGB images without the need of real-robot training or\nreal-robot data collection. The proposed policy learner, Human-to-Robot\nHandover using Sparse-View Gaussian Splatting (H2RH-SGS), leverages sparse-view\nGaussian Splatting reconstruction of human-to-robot handover scenes to generate\nrobot demonstrations containing image-action pairs captured with a camera\nmounted on the robot gripper. As a result, the simulated camera pose changes in\nthe reconstructed scene can be directly translated into gripper pose changes.\nWe train a robot policy on demonstrations collected with 16 household objects\nand {\\em directly} deploy this policy in the real environment. Experiments in\nboth Gaussian Splatting reconstructed scene and real-world human-to-robot\nhandover experiments demonstrate that H2RH-SGS serves as a new and effective\nrepresentation for the human-to-robot handover task.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7a00\u758f\u89c6\u56fe\u9ad8\u65af\u6cfc\u6e85\u91cd\u5efa\u7684\u65b9\u6cd5H2RH-SGS\uff0c\u4ec5\u4eceRGB\u56fe\u50cf\u5b66\u4e60\u673a\u5668\u4eba\u4ea4\u63a5\u7b56\u7565\uff0c\u65e0\u9700\u771f\u5b9e\u673a\u5668\u4eba\u8bad\u7ec3\u6216\u6570\u636e\u6536\u96c6\u3002", "motivation": "\u89e3\u51b3\u4ece\u771f\u5b9e\u56fe\u50cf\u6570\u636e\u5b66\u4e60\u673a\u5668\u4eba\u64cd\u4f5c\u7b56\u7565\u65f6\u7684\u9ad8\u6210\u672c\u95ee\u9898\uff0c\u4ee5\u53ca\u4eff\u771f\u4e0e\u771f\u5b9e\u73af\u5883\u95f4\u7684\u89c6\u89c9\u57df\u5dee\u8ddd\u3002", "method": "\u5229\u7528\u7a00\u758f\u89c6\u56fe\u9ad8\u65af\u6cfc\u6e85\u91cd\u5efa\u4eba\u7c7b-\u673a\u5668\u4eba\u4ea4\u63a5\u573a\u666f\uff0c\u751f\u6210\u56fe\u50cf-\u52a8\u4f5c\u5bf9\uff0c\u5e76\u901a\u8fc7\u6a21\u62df\u76f8\u673a\u59ff\u6001\u53d8\u5316\u8bad\u7ec3\u673a\u5668\u4eba\u7b56\u7565\u3002", "result": "\u572816\u79cd\u5bb6\u5ead\u7269\u54c1\u4e0a\u8bad\u7ec3\u7684\u7b56\u7565\u53ef\u76f4\u63a5\u90e8\u7f72\u5230\u771f\u5b9e\u73af\u5883\u4e2d\uff0c\u5b9e\u9a8c\u8bc1\u660e\u4e86H2RH-SGS\u7684\u6709\u6548\u6027\u3002", "conclusion": "H2RH-SGS\u4e3a\u4eba\u7c7b-\u673a\u5668\u4eba\u4ea4\u63a5\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u9ad8\u6548\u8868\u793a\u65b9\u6cd5\u3002"}}
{"id": "2507.08264", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08264", "abs": "https://arxiv.org/abs/2507.08264", "authors": ["Abhinav Sood", "Kazjon Grace", "Stephen Wan", "Cecile Paris"], "title": "Abductive Computational Systems: Creative Abduction and Future Directions", "comment": "Published in the 16th International Conference on Computational\n  Creativity, ICCC25. Accepted Paper in\n  https://computationalcreativity.net/iccc25/wp-content/uploads/papers/iccc25-sood2025abductive.pdf", "summary": "Abductive reasoning, reasoning for inferring explanations for observations,\nis often mentioned in scientific, design-related and artistic contexts, but its\nunderstanding varies across these domains. This paper reviews how abductive\nreasoning is discussed in epistemology, science and design, and then analyses\nhow various computational systems use abductive reasoning. Our analysis shows\nthat neither theoretical accounts nor computational implementations of\nabductive reasoning adequately address generating creative hypotheses.\nTheoretical frameworks do not provide a straightforward model for generating\ncreative abductive hypotheses, computational systems largely implement\nsyllogistic forms of abductive reasoning. We break down abductive computational\nsystems into components and conclude by identifying specific directions for\nfuture research that could advance the state of creative abductive reasoning in\ncomputational systems.", "AI": {"tldr": "\u672c\u6587\u56de\u987e\u4e86\u6eaf\u56e0\u63a8\u7406\u5728\u8ba4\u8bc6\u8bba\u3001\u79d1\u5b66\u548c\u8bbe\u8ba1\u4e2d\u7684\u8ba8\u8bba\uff0c\u5206\u6790\u4e86\u8ba1\u7b97\u7cfb\u7edf\u5982\u4f55\u5e94\u7528\u6eaf\u56e0\u63a8\u7406\uff0c\u53d1\u73b0\u7406\u8bba\u548c\u8ba1\u7b97\u5b9e\u73b0\u5747\u672a\u80fd\u5145\u5206\u652f\u6301\u521b\u9020\u6027\u5047\u8bbe\u7684\u751f\u6210\u3002", "motivation": "\u63a2\u8ba8\u6eaf\u56e0\u63a8\u7406\u5728\u4e0d\u540c\u9886\u57df\u7684\u7406\u89e3\u5dee\u5f02\uff0c\u5e76\u8bc4\u4f30\u5176\u8ba1\u7b97\u5b9e\u73b0\u7684\u5c40\u9650\u6027\u3002", "method": "\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\u548c\u8ba1\u7b97\u7cfb\u7edf\u5206\u6790\uff0c\u5206\u89e3\u6eaf\u56e0\u63a8\u7406\u7684\u7ec4\u6210\u90e8\u5206\u3002", "result": "\u7406\u8bba\u548c\u8ba1\u7b97\u7cfb\u7edf\u5747\u672a\u6709\u6548\u652f\u6301\u521b\u9020\u6027\u6eaf\u56e0\u5047\u8bbe\u7684\u751f\u6210\u3002", "conclusion": "\u63d0\u51fa\u672a\u6765\u7814\u7a76\u65b9\u5411\u4ee5\u6539\u8fdb\u8ba1\u7b97\u7cfb\u7edf\u4e2d\u7684\u521b\u9020\u6027\u6eaf\u56e0\u63a8\u7406\u3002"}}
{"id": "2507.08270", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.08270", "abs": "https://arxiv.org/abs/2507.08270", "authors": ["Zeyang Sha", "Hanling Tian", "Zhuoer Xu", "Shiwen Cui", "Changhua Meng", "Weiqiang Wang"], "title": "Agent Safety Alignment via Reinforcement Learning", "comment": null, "summary": "The emergence of autonomous Large Language Model (LLM) agents capable of tool\nusage has introduced new safety risks that go beyond traditional conversational\nmisuse. These agents, empowered to execute external functions, are vulnerable\nto both user-initiated threats (e.g., adversarial prompts) and tool-initiated\nthreats (e.g., malicious outputs from compromised tools). In this paper, we\npropose the first unified safety-alignment framework for tool-using agents,\nenabling models to handle both channels of threat via structured reasoning and\nsandboxed reinforcement learning. We introduce a tri-modal taxonomy, including\nbenign, malicious, and sensitive for both user prompts and tool responses, and\ndefine a policy-driven decision model. Our framework employs a custom-designed\nsandbox environment that simulates real-world tool execution and allows\nfine-grained reward shaping. Through extensive evaluations on public and\nself-built benchmarks, including Agent SafetyBench, InjecAgent, and BFCL, we\ndemonstrate that our safety-aligned agents significantly improve resistance to\nsecurity threats while preserving strong utility on benign tasks. Our results\nshow that safety and effectiveness can be jointly optimized, laying the\ngroundwork for trustworthy deployment of autonomous LLM agents.", "AI": {"tldr": "\u63d0\u51fa\u9996\u4e2a\u7edf\u4e00\u7684\u5b89\u5168\u5bf9\u9f50\u6846\u67b6\uff0c\u7528\u4e8e\u5de5\u5177\u4f7f\u7528\u578bLLM\u4ee3\u7406\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u63a8\u7406\u548c\u6c99\u76d2\u5f3a\u5316\u5b66\u4e60\u5e94\u5bf9\u7528\u6237\u548c\u5de5\u5177\u7684\u53cc\u91cd\u5a01\u80c1\u3002", "motivation": "\u81ea\u4e3bLLM\u4ee3\u7406\u7684\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u5e26\u6765\u4e86\u65b0\u7684\u5b89\u5168\u98ce\u9669\uff0c\u5305\u62ec\u7528\u6237\u548c\u5de5\u5177\u4e24\u65b9\u9762\u7684\u5a01\u80c1\uff0c\u9700\u8981\u7edf\u4e00\u7684\u5b89\u5168\u5bf9\u9f50\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e09\u6a21\u6001\u5206\u7c7b\u6cd5\uff08\u826f\u6027\u3001\u6076\u610f\u3001\u654f\u611f\uff09\u548c\u7b56\u7565\u9a71\u52a8\u51b3\u7b56\u6a21\u578b\uff0c\u7ed3\u5408\u6c99\u76d2\u73af\u5883\u548c\u7ec6\u7c92\u5ea6\u5956\u52b1\u5851\u9020\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5b89\u5168\u5bf9\u9f50\u4ee3\u7406\u663e\u8457\u63d0\u5347\u5bf9\u5a01\u80c1\u7684\u62b5\u6297\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u826f\u6027\u4efb\u52a1\u7684\u9ad8\u6548\u6027\u3002", "conclusion": "\u5b89\u5168\u4e0e\u6548\u80fd\u53ef\u534f\u540c\u4f18\u5316\uff0c\u4e3a\u81ea\u4e3bLLM\u4ee3\u7406\u7684\u53ef\u4fe1\u90e8\u7f72\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2507.08306", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.08306", "abs": "https://arxiv.org/abs/2507.08306", "authors": ["Inclusion AI", ":", "Fudong Wang", "Jiajia Liu", "Jingdong Chen", "Jun Zhou", "Kaixiang Ji", "Lixiang Ru", "Qingpei Guo", "Ruobing Zheng", "Tianqi Li", "Yi Yuan", "Yifan Mao", "Yuting Xiao", "Ziping Ma"], "title": "M2-Reasoning: Empowering MLLMs with Unified General and Spatial Reasoning", "comment": "31pages, 14 figures", "summary": "Recent advancements in Multimodal Large Language Models (MLLMs), particularly\nthrough Reinforcement Learning with Verifiable Rewards (RLVR), have\nsignificantly enhanced their reasoning abilities. However, a critical gap\npersists: these models struggle with dynamic spatial interactions, a capability\nessential for real-world applications. To bridge this gap, we introduce\nM2-Reasoning-7B, a model designed to excel in both general and spatial\nreasoning. Our approach integrates two key innovations: (1) a novel data\npipeline that generates 294.2K high-quality data samples (168K for cold-start\nfine-tuning and 126.2K for RLVR), which feature logically coherent reasoning\ntrajectories and have undergone comprehensive assessment; and (2) a dynamic\nmulti-task training strategy with step-wise optimization to mitigate conflicts\nbetween data, and task-specific rewards for delivering tailored incentive\nsignals. This combination of curated data and advanced training allows\nM2-Reasoning-7B to set a new state-of-the-art (SOTA) across 8 benchmarks,\nshowcasing superior performance in both general and spatial reasoning domains.", "AI": {"tldr": "M2-Reasoning-7B\u6a21\u578b\u901a\u8fc7\u521b\u65b0\u7684\u6570\u636e\u7ba1\u9053\u548c\u52a8\u6001\u591a\u4efb\u52a1\u8bad\u7ec3\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u901a\u7528\u548c\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u73b0\u6709MLLMs\u5728\u52a8\u6001\u7a7a\u95f4\u4ea4\u4e92\u80fd\u529b\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002", "method": "1. \u6784\u5efa\u5305\u542b29.4\u4e07\u9ad8\u8d28\u91cf\u6837\u672c\u7684\u6570\u636e\u7ba1\u9053\uff1b2. \u91c7\u7528\u52a8\u6001\u591a\u4efb\u52a1\u8bad\u7ec3\u7b56\u7565\u548c\u4efb\u52a1\u7279\u5b9a\u5956\u52b1\u3002", "result": "M2-Reasoning-7B\u57288\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u65b0SOTA\uff0c\u5c24\u5176\u5728\u7a7a\u95f4\u63a8\u7406\u9886\u57df\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u8be5\u6a21\u578b\u901a\u8fc7\u6570\u636e\u4f18\u5316\u548c\u8bad\u7ec3\u7b56\u7565\u521b\u65b0\uff0c\u6210\u529f\u586b\u8865\u4e86MLLMs\u5728\u52a8\u6001\u7a7a\u95f4\u63a8\u7406\u4e0a\u7684\u80fd\u529b\u7f3a\u53e3\u3002"}}
{"id": "2507.08392", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.08392", "abs": "https://arxiv.org/abs/2507.08392", "authors": ["Asma Yamani", "Malak Baslyman", "Moataz Ahmed"], "title": "Multi-Agent LLMs as Ethics Advocates in AI-Based Systems", "comment": null, "summary": "Incorporating ethics into the requirement elicitation process is essential\nfor creating ethically aligned systems. Although eliciting manual ethics\nrequirements is effective, it requires diverse input from multiple\nstakeholders, which can be challenging due to time and resource constraints.\nMoreover, it is often given a low priority in the requirements elicitation\nprocess. This study proposes a framework for generating ethics requirements\ndrafts by introducing an ethics advocate agent in a multi-agent LLM setting.\nThis agent critiques and provides input on ethical issues based on the system\ndescription. The proposed framework is evaluated through two case studies from\ndifferent contexts, demonstrating that it captures the majority of ethics\nrequirements identified by researchers during 30-minute interviews and\nintroduces several additional relevant requirements. However, it also\nhighlights reliability issues in generating ethics requirements, emphasizing\nthe need for human feedback in this sensitive domain. We believe this work can\nfacilitate the broader adoption of ethics in the requirements engineering\nprocess, ultimately leading to more ethically aligned products.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u591a\u667a\u80fd\u4f53LLM\u73af\u5883\u4e2d\u5f15\u5165\u4f26\u7406\u5021\u5bfc\u4ee3\u7406\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u751f\u6210\u4f26\u7406\u9700\u6c42\u8349\u6848\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u624b\u52a8\u83b7\u53d6\u4f26\u7406\u9700\u6c42\u6548\u7387\u4f4e\u4e14\u8d44\u6e90\u6d88\u8017\u5927\uff0c\u5bfc\u81f4\u5176\u5728\u9700\u6c42\u83b7\u53d6\u8fc7\u7a0b\u4e2d\u4f18\u5148\u7ea7\u8f83\u4f4e\u3002", "method": "\u5728\u591a\u667a\u80fd\u4f53LLM\u73af\u5883\u4e2d\u5f15\u5165\u4f26\u7406\u5021\u5bfc\u4ee3\u7406\uff0c\u6839\u636e\u7cfb\u7edf\u63cf\u8ff0\u63d0\u4f9b\u4f26\u7406\u95ee\u9898\u7684\u6279\u8bc4\u548c\u5efa\u8bae\u3002", "result": "\u6846\u67b6\u80fd\u6355\u6349\u5927\u90e8\u5206\u7814\u7a76\u8005\u901a\u8fc7\u8bbf\u8c08\u8bc6\u522b\u7684\u4f26\u7406\u9700\u6c42\uff0c\u5e76\u5f15\u5165\u989d\u5916\u76f8\u5173\u9700\u6c42\uff0c\u4f46\u5b58\u5728\u53ef\u9760\u6027\u95ee\u9898\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u52a9\u4e8e\u63a8\u52a8\u4f26\u7406\u9700\u6c42\u5728\u5de5\u7a0b\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u4ecd\u9700\u4eba\u7c7b\u53cd\u9988\u4ee5\u786e\u4fdd\u53ef\u9760\u6027\u3002"}}
{"id": "2507.08454", "categories": ["cs.AI", "cs.LG", "cs.LO", "68T27, 03B05", "I.2.3; F.4.1"], "pdf": "https://arxiv.org/pdf/2507.08454", "abs": "https://arxiv.org/abs/2507.08454", "authors": ["Tobias Geibinger", "Reijo Jaakkola", "Antti Kuusisto", "Xinghan Liu", "Miikka Vilander"], "title": "Why this and not that? A Logic-based Framework for Contrastive Explanations", "comment": "20 pages, accepted to JELIA 2025", "summary": "We define several canonical problems related to contrastive explanations,\neach answering a question of the form ''Why P but not Q?''. The problems\ncompute causes for both P and Q, explicitly comparing their differences. We\ninvestigate the basic properties of our definitions in the setting of\npropositional logic. We show, inter alia, that our framework captures a\ncardinality-minimal version of existing contrastive explanations in the\nliterature. Furthermore, we provide an extensive analysis of the computational\ncomplexities of the problems. We also implement the problems for CNF-formulas\nusing answer set programming and present several examples demonstrating how\nthey work in practice.", "AI": {"tldr": "\u8bba\u6587\u5b9a\u4e49\u4e86\u4e0e\u5bf9\u6bd4\u89e3\u91ca\u76f8\u5173\u7684\u51e0\u4e2a\u5178\u578b\u95ee\u9898\uff0c\u7814\u7a76\u5176\u5728\u547d\u9898\u903b\u8f91\u4e2d\u7684\u57fa\u672c\u6027\u8d28\uff0c\u5e76\u5206\u6790\u4e86\u8ba1\u7b97\u590d\u6742\u6027\uff0c\u540c\u65f6\u901a\u8fc7\u5b9e\u4f8b\u5c55\u793a\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "motivation": "\u7814\u7a76\u5bf9\u6bd4\u89e3\u91ca\uff08\u56de\u7b54\u201c\u4e3a\u4ec0\u4e48P\u800c\u4e0d\u662fQ\uff1f\u201d\uff09\u7684\u89c4\u8303\u95ee\u9898\uff0c\u586b\u8865\u73b0\u6709\u6587\u732e\u4e2d\u7684\u7a7a\u767d\u3002", "method": "\u5728\u547d\u9898\u903b\u8f91\u4e2d\u5b9a\u4e49\u95ee\u9898\uff0c\u5206\u6790\u8ba1\u7b97\u590d\u6742\u6027\uff0c\u5e76\u4f7f\u7528\u7b54\u6848\u96c6\u7f16\u7a0b\u5b9e\u73b0CNF\u516c\u5f0f\u7684\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u6846\u67b6\u80fd\u591f\u6355\u6349\u73b0\u6709\u5bf9\u6bd4\u89e3\u91ca\u7684\u6700\u5c0f\u57fa\u6570\u7248\u672c\uff0c\u5e76\u63d0\u4f9b\u4e86\u8ba1\u7b97\u590d\u6742\u6027\u7684\u8be6\u7ec6\u5206\u6790\u3002", "conclusion": "\u8bba\u6587\u63d0\u51fa\u7684\u6846\u67b6\u548c\u65b9\u6cd5\u4e3a\u5bf9\u6bd4\u89e3\u91ca\u95ee\u9898\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u9645\u5e94\u7528\u5de5\u5177\u3002"}}
{"id": "2507.08501", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08501", "abs": "https://arxiv.org/abs/2507.08501", "authors": ["Keying Yang", "Hao Wang", "Kai Yang"], "title": "From Language to Logic: A Bi-Level Framework for Structured Reasoning", "comment": null, "summary": "Structured reasoning over natural language inputs remains a core challenge in\nartificial intelligence, as it requires bridging the gap between unstructured\nlinguistic expressions and formal logical representations. In this paper, we\npropose a novel \\textbf{bi-level framework} that maps language to logic through\na two-stage process: high-level task abstraction and low-level logic\ngeneration. At the upper level, a large language model (LLM) parses natural\nlanguage queries into intermediate structured representations specifying the\nproblem type, objectives, decision variables, and symbolic constraints. At the\nlower level, the LLM uses these representations to generate symbolic workflows\nor executable reasoning programs for accurate and interpretable decision\nmaking. The framework supports modular reasoning, enforces explicit\nconstraints, and generalizes across domains such as mathematical problem\nsolving, question answering, and logical inference. We further optimize the\nframework with an end-to-end {bi-level} optimization approach that jointly\nrefines both the high-level abstraction and low-level logic generation stages.\nExperiments on multiple realistic reasoning benchmarks demonstrate that our\napproach significantly outperforms existing baselines in accuracy, with\naccuracy gains reaching as high as 40\\%. Moreover, the bi-level design enhances\ntransparency and error traceability, offering a promising step toward\ntrustworthy and systematic reasoning with LLMs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u5c42\u6846\u67b6\uff0c\u901a\u8fc7\u4efb\u52a1\u62bd\u8c61\u548c\u903b\u8f91\u751f\u6210\u4e24\u9636\u6bb5\u5c06\u81ea\u7136\u8bed\u8a00\u6620\u5c04\u4e3a\u903b\u8f91\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406\u51c6\u786e\u6027\u548c\u900f\u660e\u5ea6\u3002", "motivation": "\u89e3\u51b3\u81ea\u7136\u8bed\u8a00\u8f93\u5165\u4e0e\u5f62\u5f0f\u903b\u8f91\u8868\u793a\u4e4b\u95f4\u7684\u9e3f\u6c9f\uff0c\u5b9e\u73b0\u66f4\u51c6\u786e\u3001\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u3002", "method": "\u91c7\u7528\u53cc\u5c42\u6846\u67b6\uff1a\u9ad8\u5c42\u4efb\u52a1\u62bd\u8c61\uff08LLM\u89e3\u6790\u81ea\u7136\u8bed\u8a00\u4e3a\u7ed3\u6784\u5316\u8868\u793a\uff09\u548c\u4f4e\u5c42\u903b\u8f91\u751f\u6210\uff08LLM\u751f\u6210\u7b26\u53f7\u5316\u5de5\u4f5c\u6d41\uff09\u3002\u901a\u8fc7\u7aef\u5230\u7aef\u53cc\u5c42\u4f18\u5316\u8054\u5408\u4f18\u5316\u4e24\u9636\u6bb5\u3002", "result": "\u5728\u591a\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u51c6\u786e\u7387\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\uff08\u6700\u9ad8\u63d0\u534740%\uff09\uff0c\u540c\u65f6\u589e\u5f3a\u900f\u660e\u5ea6\u548c\u9519\u8bef\u53ef\u8ffd\u6eaf\u6027\u3002", "conclusion": "\u53cc\u5c42\u6846\u67b6\u4e3aLLM\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u4fe1\u8d56\u3001\u7cfb\u7edf\u5316\u7684\u63a8\u7406\u65b9\u6cd5\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2507.08529", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.08529", "abs": "https://arxiv.org/abs/2507.08529", "authors": ["Mingda Zhang", "Na Zhao", "Jianglong Qin", "Guoyu Ye", "Ruixiang Tang"], "title": "A Multi-granularity Concept Sparse Activation and Hierarchical Knowledge Graph Fusion Framework for Rare Disease Diagnosis", "comment": "10 pages,3 figures", "summary": "Despite advances from medical large language models in healthcare,\nrare-disease diagnosis remains hampered by insufficient\nknowledge-representation depth, limited concept understanding, and constrained\nclinical reasoning. We propose a framework that couples multi-granularity\nsparse activation of medical concepts with a hierarchical knowledge graph. Four\ncomplementary matching algorithms, diversity control, and a five-level fallback\nstrategy enable precise concept activation, while a three-layer knowledge graph\n(taxonomy, clinical features, instances) provides structured, up-to-date\ncontext. Experiments on the BioASQ rare-disease QA set show BLEU gains of 0.09,\nROUGE gains of 0.05, and accuracy gains of 0.12, with peak accuracy of 0.89\napproaching the 0.90 clinical threshold. Expert evaluation confirms\nimprovements in information quality, reasoning, and professional expression,\nsuggesting our approach shortens the \"diagnostic odyssey\" for rare-disease\npatients.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u591a\u7c92\u5ea6\u7a00\u758f\u6fc0\u6d3b\u548c\u5206\u5c42\u77e5\u8bc6\u56fe\u8c31\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u7f55\u89c1\u75c5\u8bca\u65ad\u7684\u51c6\u786e\u6027\u548c\u4fe1\u606f\u8d28\u91cf\u3002", "motivation": "\u7f55\u89c1\u75c5\u8bca\u65ad\u56e0\u77e5\u8bc6\u8868\u793a\u6df1\u5ea6\u4e0d\u8db3\u3001\u6982\u5ff5\u7406\u89e3\u6709\u9650\u548c\u4e34\u5e8a\u63a8\u7406\u53d7\u9650\u800c\u8fdb\u5c55\u7f13\u6162\u3002", "method": "\u91c7\u7528\u591a\u7c92\u5ea6\u7a00\u758f\u6fc0\u6d3b\u533b\u5b66\u6982\u5ff5\uff0c\u7ed3\u5408\u5206\u5c42\u77e5\u8bc6\u56fe\u8c31\uff08\u5206\u7c7b\u3001\u4e34\u5e8a\u7279\u5f81\u3001\u5b9e\u4f8b\uff09\uff0c\u5e76\u4f7f\u7528\u56db\u79cd\u5339\u914d\u7b97\u6cd5\u3001\u591a\u6837\u6027\u63a7\u5236\u548c\u4e94\u7ea7\u56de\u9000\u7b56\u7565\u3002", "result": "\u5728BioASQ\u7f55\u89c1\u75c5QA\u96c6\u4e0a\uff0cBLEU\u63d0\u53470.09\uff0cROUGE\u63d0\u53470.05\uff0c\u51c6\u786e\u7387\u63d0\u53470.12\uff0c\u5cf0\u503c\u51c6\u786e\u7387\u8fbe0.89\u3002\u4e13\u5bb6\u8bc4\u4f30\u663e\u793a\u4fe1\u606f\u8d28\u91cf\u3001\u63a8\u7406\u548c\u4e13\u4e1a\u8868\u8fbe\u5747\u6709\u6539\u5584\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u7f29\u77ed\u4e86\u7f55\u89c1\u75c5\u60a3\u8005\u7684\u8bca\u65ad\u5468\u671f\uff0c\u63a5\u8fd1\u4e34\u5e8a\u9608\u503c\uff080.90\uff09\u3002"}}
{"id": "2507.08575", "categories": ["cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.08575", "abs": "https://arxiv.org/abs/2507.08575", "authors": ["Kalana Wijegunarathna", "Kristin Stock", "Christopher B. Jones"], "title": "Large Multi-modal Model Cartographic Map Comprehension for Textual Locality Georeferencing", "comment": null, "summary": "Millions of biological sample records collected in the last few centuries\narchived in natural history collections are un-georeferenced. Georeferencing\ncomplex locality descriptions associated with these collection samples is a\nhighly labour-intensive task collection agencies struggle with. None of the\nexisting automated methods exploit maps that are an essential tool for\ngeoreferencing complex relations. We present preliminary experiments and\nresults of a novel method that exploits multi-modal capabilities of recent\nLarge Multi-Modal Models (LMM). This method enables the model to visually\ncontextualize spatial relations it reads in the locality description. We use a\ngrid-based approach to adapt these auto-regressive models for this task in a\nzero-shot setting. Our experiments conducted on a small manually annotated\ndataset show impressive results for our approach ($\\sim$1 km Average distance\nerror) compared to uni-modal georeferencing with Large Language Models and\nexisting georeferencing tools. The paper also discusses the findings of the\nexperiments in light of an LMM's ability to comprehend fine-grained maps.\nMotivated by these results, a practical framework is proposed to integrate this\nmethod into a georeferencing workflow.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMM\uff09\u8fdb\u884c\u5730\u7406\u53c2\u8003\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u89c6\u89c9\u5316\u7a7a\u95f4\u5173\u7cfb\u63d0\u5347\u590d\u6742\u5730\u70b9\u63cf\u8ff0\u7684\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "motivation": "\u89e3\u51b3\u81ea\u7136\u5386\u53f2\u6536\u85cf\u4e2d\u5927\u91cf\u672a\u5730\u7406\u53c2\u8003\u7684\u751f\u7269\u6837\u672c\u8bb0\u5f55\u95ee\u9898\uff0c\u73b0\u6709\u81ea\u52a8\u5316\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u5730\u56fe\u4fe1\u606f\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u7f51\u683c\u7684\u65b9\u6cd5\uff0c\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u5229\u7528LMM\u7684\u591a\u6a21\u6001\u80fd\u529b\uff0c\u7ed3\u5408\u6587\u672c\u63cf\u8ff0\u548c\u5730\u56fe\u8fdb\u884c\u5730\u7406\u53c2\u8003\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u8be5\u65b9\u6cd5\u5e73\u5747\u8ddd\u79bb\u8bef\u5dee\u7ea61\u516c\u91cc\uff0c\u4f18\u4e8e\u5355\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u548c\u73b0\u6709\u5de5\u5177\u3002", "conclusion": "LMM\u80fd\u591f\u7406\u89e3\u7ec6\u7c92\u5ea6\u5730\u56fe\uff0c\u63d0\u51fa\u5c06\u5176\u6574\u5408\u5230\u5730\u7406\u53c2\u8003\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u7684\u5b9e\u7528\u6846\u67b6\u3002"}}
{"id": "2507.08603", "categories": ["cs.AI", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.08603", "abs": "https://arxiv.org/abs/2507.08603", "authors": ["Yonghua Hei", "Yibo Yan", "Shuliang Liu", "Huiyu Zhou", "Linfeng Zhang", "Xuming Hu"], "title": "Unlocking Speech Instruction Data Potential with Query Rewriting", "comment": "ACL 2025 Findings", "summary": "End-to-end Large Speech Language Models~(\\textbf{LSLMs}) demonstrate strong\npotential in response latency and speech comprehension capabilities, showcasing\ngeneral intelligence across speech understanding tasks. However, the ability to\nfollow speech instructions has not been fully realized due to the lack of\ndatasets and heavily biased training tasks. Leveraging the rich ASR datasets,\nprevious approaches have used Large Language Models~(\\textbf{LLMs}) to continue\nthe linguistic information of speech to construct speech instruction datasets.\nYet, due to the gap between LLM-generated results and real human responses, the\ncontinuation methods further amplify these shortcomings. Given the high costs\nof collecting and annotating speech instruction datasets by humans, using\nspeech synthesis to construct large-scale speech instruction datasets has\nbecome a balanced and robust alternative. Although modern\nText-To-Speech~(\\textbf{TTS}) models have achieved near-human-level synthesis\nquality, it is challenging to appropriately convert out-of-distribution text\ninstruction to speech due to the limitations of the training data distribution\nin TTS models. To address this issue, we propose a query rewriting framework\nwith multi-LLM knowledge fusion, employing multiple agents to annotate and\nvalidate the synthesized speech, making it possible to construct high-quality\nspeech instruction datasets without relying on human annotation. Experiments\nshow that this method can transform text instructions into distributions more\nsuitable for TTS models for speech synthesis through zero-shot rewriting,\nincreasing data usability from 72\\% to 93\\%. It also demonstrates unique\nadvantages in rewriting tasks that require complex knowledge and\ncontext-related abilities.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591aLLM\u77e5\u8bc6\u878d\u5408\u7684\u67e5\u8be2\u91cd\u5199\u6846\u67b6\uff0c\u7528\u4e8e\u6784\u5efa\u9ad8\u8d28\u91cf\u8bed\u97f3\u6307\u4ee4\u6570\u636e\u96c6\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6570\u636e\u53ef\u7528\u6027\u3002", "motivation": "\u5f53\u524d\u8bed\u97f3\u6307\u4ee4\u6570\u636e\u96c6\u7f3a\u4e4f\u4e14\u8bad\u7ec3\u4efb\u52a1\u504f\u5dee\u5927\uff0cLLM\u751f\u6210\u7684\u8bed\u97f3\u6307\u4ee4\u4e0e\u771f\u5b9e\u4eba\u7c7b\u54cd\u5e94\u5b58\u5728\u5dee\u8ddd\uff0c\u4eba\u5de5\u6807\u6ce8\u6210\u672c\u9ad8\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u591aLLM\u77e5\u8bc6\u878d\u5408\u7684\u67e5\u8be2\u91cd\u5199\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u6807\u6ce8\u548c\u9a8c\u8bc1\u5408\u6210\u8bed\u97f3\uff0c\u5c06\u6587\u672c\u6307\u4ee4\u8f6c\u6362\u4e3a\u66f4\u9002\u5408TTS\u6a21\u578b\u5408\u6210\u7684\u5206\u5e03\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u96f6\u6837\u672c\u91cd\u5199\u5c06\u6570\u636e\u53ef\u7528\u6027\u4ece72%\u63d0\u5347\u81f393%\uff0c\u5e76\u5728\u590d\u6742\u77e5\u8bc6\u548c\u4e0a\u4e0b\u6587\u76f8\u5173\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6784\u5efa\u9ad8\u8d28\u91cf\u8bed\u97f3\u6307\u4ee4\u6570\u636e\u96c6\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.08619", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08619", "abs": "https://arxiv.org/abs/2507.08619", "authors": ["Soheyl Massoudi", "Mark Fuge"], "title": "Agentic Large Language Models for Conceptual Systems Engineering and Design", "comment": "32 pages, 3 figures", "summary": "Early-stage engineering design involves complex, iterative reasoning, yet\nexisting large language model (LLM) workflows struggle to maintain task\ncontinuity and generate executable models. We evaluate whether a structured\nmulti-agent system (MAS) can more effectively manage requirements extraction,\nfunctional decomposition, and simulator code generation than a simpler\ntwo-agent system (2AS). The target application is a solar-powered water\nfiltration system as described in a cahier des charges. We introduce the\nDesign-State Graph (DSG), a JSON-serializable representation that bundles\nrequirements, physical embodiments, and Python-based physics models into graph\nnodes. A nine-role MAS iteratively builds and refines the DSG, while the 2AS\ncollapses the process to a Generator-Reflector loop. Both systems run a total\nof 60 experiments (2 LLMs - Llama 3.3 70B vs reasoning-distilled DeepSeek R1\n70B x 2 agent configurations x 3 temperatures x 5 seeds). We report a JSON\nvalidity, requirement coverage, embodiment presence, code compatibility,\nworkflow completion, runtime, and graph size. Across all runs, both MAS and 2AS\nmaintained perfect JSON integrity and embodiment tagging. Requirement coverage\nremained minimal (less than 20\\%). Code compatibility peaked at 100\\% under\nspecific 2AS settings but averaged below 50\\% for MAS. Only the\nreasoning-distilled model reliably flagged workflow completion. Powered by\nDeepSeek R1 70B, the MAS generated more granular DSGs (average 5-6 nodes)\nwhereas 2AS mode-collapsed. Structured multi-agent orchestration enhanced\ndesign detail. Reasoning-distilled LLM improved completion rates, yet low\nrequirements and fidelity gaps in coding persisted.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08MAS\uff09\u5728\u65e9\u671f\u5de5\u7a0b\u8bbe\u8ba1\u4e2d\u7684\u8868\u73b0\uff0c\u6bd4\u8f83\u4e86\u5176\u4e0e\u53cc\u667a\u80fd\u4f53\u7cfb\u7edf\uff082AS\uff09\u5728\u4efb\u52a1\u8fde\u7eed\u6027\u3001\u53ef\u6267\u884c\u6a21\u578b\u751f\u6210\u7b49\u65b9\u9762\u7684\u5dee\u5f02\u3002\u7ed3\u679c\u663e\u793aMAS\u5728\u7ec6\u8282\u751f\u6210\u4e0a\u66f4\u4f18\uff0c\u4f46\u9700\u6c42\u8986\u76d6\u7387\u548c\u4ee3\u7801\u517c\u5bb9\u6027\u4ecd\u6709\u4e0d\u8db3\u3002", "motivation": "\u65e9\u671f\u5de5\u7a0b\u8bbe\u8ba1\u6d89\u53ca\u590d\u6742\u7684\u8fed\u4ee3\u63a8\u7406\uff0c\u73b0\u6709\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5de5\u4f5c\u6d41\u96be\u4ee5\u4fdd\u6301\u4efb\u52a1\u8fde\u7eed\u6027\u5e76\u751f\u6210\u53ef\u6267\u884c\u6a21\u578b\u3002\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u7ed3\u6784\u5316\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u662f\u5426\u80fd\u66f4\u6709\u6548\u5730\u7ba1\u7406\u9700\u6c42\u63d0\u53d6\u3001\u529f\u80fd\u5206\u89e3\u548c\u6a21\u62df\u4ee3\u7801\u751f\u6210\u3002", "method": "\u7814\u7a76\u5f15\u5165\u4e86\u8bbe\u8ba1\u72b6\u6001\u56fe\uff08DSG\uff09\uff0c\u5e76\u901a\u8fc7\u4e5d\u89d2\u8272MAS\u548c\u53cc\u89d2\u82722AS\u8fdb\u884c\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u4e86\u4e24\u79cdLLM\uff08Llama 3.3 70B\u548cDeepSeek R1 70B\uff09\u5728\u4e0d\u540c\u914d\u7f6e\u4e0b\u7684\u8868\u73b0\u3002", "result": "MAS\u5728\u7ec6\u8282\u751f\u6210\u4e0a\u4f18\u4e8e2AS\uff0c\u4f46\u9700\u6c42\u8986\u76d6\u7387\u4f4e\uff08<20%\uff09\u3002\u4ee3\u7801\u517c\u5bb9\u6027\u5728\u7279\u5b9a2AS\u8bbe\u7f6e\u4e0b\u8fbe\u5230100%\uff0c\u4f46MAS\u5e73\u5747\u4f4e\u4e8e50%\u3002\u63a8\u7406\u84b8\u998f\u6a21\u578b\u80fd\u53ef\u9760\u6807\u8bb0\u5de5\u4f5c\u6d41\u5b8c\u6210\u3002", "conclusion": "\u7ed3\u6784\u5316\u591a\u667a\u80fd\u4f53\u534f\u8c03\u63d0\u5347\u4e86\u8bbe\u8ba1\u7ec6\u8282\uff0c\u4f46\u9700\u6c42\u8986\u76d6\u7387\u548c\u4ee3\u7801\u4fdd\u771f\u5ea6\u4ecd\u9700\u6539\u8fdb\u3002\u63a8\u7406\u84b8\u998fLLM\u63d0\u9ad8\u4e86\u5b8c\u6210\u7387\uff0c\u4f46\u6574\u4f53\u8868\u73b0\u4ecd\u6709\u5c40\u9650\u3002"}}
{"id": "2507.08649", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08649", "abs": "https://arxiv.org/abs/2507.08649", "authors": ["Xingguang Ji", "Yahui Liu", "Qi Wang", "Jingyuan Zhang", "Yang Yue", "Rui Shi", "Chenxi Sun", "Fuzheng Zhang", "Guorui Zhou", "Kun Gai"], "title": "Leanabell-Prover-V2: Verifier-integrated Reasoning for Formal Theorem Proving via Reinforcement Learning", "comment": "23 pages, 13 figures", "summary": "We introduce our Leanabell-Prover-V2, a 7B large language models (LLMs) that\ncan produce formal theorem proofs in Lean 4, with verifier-integrated Long\nChain-of-Thoughts (CoT). Following our previous work Leanabell-Prover-V1, we\ncontinual to choose to posttrain existing strong prover models for further\nperformance improvement. In our V2 version, we mainly upgrade the Reinforcement\nLearning (RL) with feedback provided by the Lean 4 verifier. Crucially,\nverifier feedback, such as indicating success or detailing specific errors,\nallows the LLM to become ``self-aware'' of the correctness of its own reasoning\nprocess and learn to reflexively correct errors. Leanabell-Prover-V2 directly\noptimizes LLM reasoning trajectories with multi-turn verifier interactions,\ntogether with feedback token masking for stable RL training and a simple reward\nstrategy. Experiments show that Leanabell-Prover-V2 improves performance by\n3.2% (pass@128) with Kimina-Prover-Preview-Distill-7B and 2.0% (pass@128) with\nDeepSeek-Prover-V2-7B on the MiniF2F test set. The source codes, curated data\nand models are available at:\nhttps://github.com/Leanabell-LM/Leanabell-Prover-V2.", "AI": {"tldr": "Leanabell-Prover-V2\u662f\u4e00\u4e2a7B\u53c2\u6570\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u7528\u4e8e\u751f\u6210Lean 4\u4e2d\u7684\u5f62\u5f0f\u5316\u5b9a\u7406\u8bc1\u660e\uff0c\u901a\u8fc7\u9a8c\u8bc1\u5668\u96c6\u6210\u7684\u957f\u94fe\u601d\u7ef4\uff08CoT\uff09\u548c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4f18\u5316\u6027\u80fd\u3002", "motivation": "\u5728V1\u7248\u672c\u7684\u57fa\u7840\u4e0a\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\uff0c\u901a\u8fc7\u9a8c\u8bc1\u5668\u53cd\u9988\u5b9e\u73b0\u81ea\u6211\u4fee\u6b63\u548c\u4f18\u5316\u3002", "method": "\u91c7\u7528\u9a8c\u8bc1\u5668\u53cd\u9988\u7684\u5f3a\u5316\u5b66\u4e60\uff0c\u7ed3\u5408\u53cd\u9988\u4ee4\u724c\u63a9\u7801\u548c\u7b80\u5355\u5956\u52b1\u7b56\u7565\uff0c\u4f18\u5316\u591a\u8f6e\u9a8c\u8bc1\u5668\u4ea4\u4e92\u7684\u63a8\u7406\u8f68\u8ff9\u3002", "result": "\u5728MiniF2F\u6d4b\u8bd5\u96c6\u4e0a\uff0c\u6027\u80fd\u5206\u522b\u63d0\u5347\u4e863.2%\uff08Kimina-Prover-Preview-Distill-7B\uff09\u548c2.0%\uff08DeepSeek-Prover-V2-7B\uff09\u3002", "conclusion": "Leanabell-Prover-V2\u901a\u8fc7\u9a8c\u8bc1\u5668\u53cd\u9988\u548c\u5f3a\u5316\u5b66\u4e60\u663e\u8457\u63d0\u5347\u4e86\u5f62\u5f0f\u5316\u5b9a\u7406\u8bc1\u660e\u7684\u6027\u80fd\u3002"}}
{"id": "2507.08664", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08664", "abs": "https://arxiv.org/abs/2507.08664", "authors": ["Haoran Sun", "Shaoning Zeng"], "title": "Introspection of Thought Helps AI Agents", "comment": null, "summary": "AI Agents rely on Large Language Models (LLMs) and Multimodal-LLMs (MLLMs) to\nperform interpretation and inference in text and image tasks without\npost-training, where LLMs and MLLMs play the most critical role and determine\nthe initial ability and limitations of AI Agents. Usually, AI Agents utilize\nsophisticated prompt engineering and external reasoning framework to obtain a\npromising interaction with LLMs, e.g., Chain-of-Thought, Iteration of Thought\nand Image-of-Thought. However, they are still constrained by the inherent\nlimitations of LLM in understanding natural language, and the iterative\nreasoning process will generate a large amount of inference cost. To this end,\nwe propose a novel AI Agent Reasoning Framework with Introspection of Thought\n(INoT) by designing a new LLM-Read code in prompt. It enables LLM to execute\nprogrammatic dialogue reasoning processes following the code in prompt.\nTherefore, self-denial and reflection occur within LLM instead of outside LLM,\nwhich can reduce token cost effectively. Through our experiments on six\nbenchmarks for three different tasks, the effectiveness of INoT is verified,\nwith an average improvement of 7.95\\% in performance, exceeding the baselines.\nFurthermore, the token cost of INoT is lower on average than the best\nperforming method at baseline by 58.3\\%. In addition, we demonstrate the\nversatility of INoT in image interpretation and inference through verification\nexperiments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aINoT\u7684\u65b0\u578bAI\u4ee3\u7406\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u8bbe\u8ba1\u65b0\u7684LLM-Read\u4ee3\u7801\u63d0\u793a\uff0c\u4f7fLLM\u80fd\u591f\u6267\u884c\u7a0b\u5e8f\u5316\u5bf9\u8bdd\u63a8\u7406\uff0c\u4ece\u800c\u51cf\u5c11\u63a8\u7406\u6210\u672c\u5e76\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709AI\u4ee3\u7406\u4f9d\u8d56LLMs\u548cMLLMs\u8fdb\u884c\u63a8\u7406\uff0c\u4f46\u53d7\u9650\u4e8e\u5176\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u7684\u56fa\u6709\u7f3a\u9677\uff0c\u4e14\u8fed\u4ee3\u63a8\u7406\u8fc7\u7a0b\u6210\u672c\u9ad8\u6602\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cdLLM-Read\u4ee3\u7801\u63d0\u793a\uff0c\u4f7fLLM\u80fd\u591f\u5728\u63d0\u793a\u4e2d\u6267\u884c\u7a0b\u5e8f\u5316\u5bf9\u8bdd\u63a8\u7406\uff0c\u5b9e\u73b0\u81ea\u6211\u5426\u5b9a\u548c\u53cd\u601d\uff0c\u51cf\u5c11\u5916\u90e8\u63a8\u7406\u6210\u672c\u3002", "result": "\u5728\u516d\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cINoT\u5e73\u5747\u6027\u80fd\u63d0\u53477.95%\uff0c\u4e14\u63a8\u7406\u6210\u672c\u6bd4\u57fa\u7ebf\u6700\u4f73\u65b9\u6cd5\u4f4e58.3%\u3002", "conclusion": "INoT\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86AI\u4ee3\u7406\u7684\u63a8\u7406\u80fd\u529b\u548c\u6548\u7387\uff0c\u5e76\u5728\u56fe\u50cf\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u5176\u901a\u7528\u6027\u3002"}}
{"id": "2507.08705", "categories": ["cs.AI", "I.2.5; I.2.1; I.2.7; I.2.11"], "pdf": "https://arxiv.org/pdf/2507.08705", "abs": "https://arxiv.org/abs/2507.08705", "authors": ["Philip Osborne", "Danilo S. Carvalho", "Andr\u00e9 Freitas"], "title": "elsciRL: Integrating Language Solutions into Reinforcement Learning Problem Settings", "comment": "6 pages, 1 figure, 3 tables, 11 Appendix pages, submitted to EMNLP\n  2025 Call for System Demonstrations", "summary": "We present elsciRL, an open-source Python library to facilitate the\napplication of language solutions on reinforcement learning problems. We\ndemonstrate the potential of our software by extending the Language Adapter\nwith Self-Completing Instruction framework defined in (Osborne, 2024) with the\nuse of LLMs. Our approach can be re-applied to new applications with minimal\nsetup requirements. We provide a novel GUI that allows a user to provide text\ninput for an LLM to generate instructions which it can then self-complete.\nEmpirical results indicate that these instructions \\textit{can} improve a\nreinforcement learning agent's performance. Therefore, we present this work to\naccelerate the evaluation of language solutions on reward based environments to\nenable new opportunities for scientific discovery.", "AI": {"tldr": "elsciRL\u662f\u4e00\u4e2a\u5f00\u6e90\u7684Python\u5e93\uff0c\u65e8\u5728\u5c06\u8bed\u8a00\u89e3\u51b3\u65b9\u6848\u5e94\u7528\u4e8e\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\uff0c\u901a\u8fc7\u7ed3\u5408LLMs\u6269\u5c55\u4e86\u73b0\u6709\u6846\u67b6\uff0c\u5e76\u63d0\u4f9b\u4e86\u7528\u6237\u53cb\u597d\u7684GUI\u3002", "motivation": "\u52a0\u901f\u8bed\u8a00\u89e3\u51b3\u65b9\u6848\u5728\u57fa\u4e8e\u5956\u52b1\u7684\u73af\u5883\u4e2d\u7684\u8bc4\u4f30\uff0c\u4ee5\u4fc3\u8fdb\u79d1\u5b66\u53d1\u73b0\u7684\u65b0\u673a\u4f1a\u3002", "method": "\u6269\u5c55\u4e86Language Adapter with Self-Completing Instruction\u6846\u67b6\uff0c\u7ed3\u5408LLMs\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2aGUI\uff0c\u7528\u6237\u53ef\u4ee5\u901a\u8fc7\u6587\u672c\u8f93\u5165\u751f\u6210\u6307\u4ee4\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u751f\u6210\u7684\u6307\u4ee4\u80fd\u591f\u63d0\u5347\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u7684\u6027\u80fd\u3002", "conclusion": "elsciRL\u4e3a\u8bed\u8a00\u89e3\u51b3\u65b9\u6848\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u4fbf\u6377\u5de5\u5177\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.08715", "categories": ["cs.AI", "cs.SY", "eess.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.08715", "abs": "https://arxiv.org/abs/2507.08715", "authors": ["Paul Saves", "Jasper Bussemaker", "R\u00e9mi Lafage", "Thierry Lefebvre", "Nathalie Bartoli", "Youssef Diouane", "Joseph Morlier"], "title": "System-of-systems Modeling and Optimization: An Integrated Framework for Intermodal Mobility", "comment": null, "summary": "For developing innovative systems architectures, modeling and optimization\ntechniques have been central to frame the architecting process and define the\noptimization and modeling problems. In this context, for system-of-systems the\nuse of efficient dedicated approaches (often physics-based simulations) is\nhighly recommended to reduce the computational complexity of the targeted\napplications. However, exploring novel architectures using such dedicated\napproaches might pose challenges for optimization algorithms, including\nincreased evaluation costs and potential failures. To address these challenges,\nsurrogate-based optimization algorithms, such as Bayesian optimization\nutilizing Gaussian process models have emerged.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5728\u7cfb\u7edf\u67b6\u6784\u5f00\u53d1\u4e2d\uff0c\u4f7f\u7528\u57fa\u4e8e\u7269\u7406\u7684\u6a21\u62df\u65b9\u6cd5\u53ef\u80fd\u5e26\u6765\u7684\u8ba1\u7b97\u590d\u6742\u6027\u548c\u4f18\u5316\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u4ee3\u7406\u7684\u4f18\u5316\u7b97\u6cd5\uff08\u5982\u8d1d\u53f6\u65af\u4f18\u5316\uff09\u4f5c\u4e3a\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u5728\u7cfb\u7edf\u67b6\u6784\u5f00\u53d1\u4e2d\uff0c\u4f20\u7edf\u7684\u57fa\u4e8e\u7269\u7406\u7684\u6a21\u62df\u65b9\u6cd5\u867d\u7136\u9ad8\u6548\uff0c\u4f46\u53ef\u80fd\u5bfc\u81f4\u8ba1\u7b97\u590d\u6742\u6027\u589e\u52a0\u548c\u4f18\u5316\u7b97\u6cd5\u5931\u8d25\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u9ad8\u6548\u7684\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4f7f\u7528\u57fa\u4e8e\u4ee3\u7406\u7684\u4f18\u5316\u7b97\u6cd5\uff0c\u7279\u522b\u662f\u8d1d\u53f6\u65af\u4f18\u5316\u548c\u9ad8\u65af\u8fc7\u7a0b\u6a21\u578b\uff0c\u4ee5\u5e94\u5bf9\u4f20\u7edf\u65b9\u6cd5\u7684\u6311\u6218\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u57fa\u4e8e\u4ee3\u7406\u7684\u4f18\u5316\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u5e76\u63d0\u9ad8\u4f18\u5316\u6548\u7387\u3002", "conclusion": "\u57fa\u4e8e\u4ee3\u7406\u7684\u4f18\u5316\u7b97\u6cd5\uff08\u5982\u8d1d\u53f6\u65af\u4f18\u5316\uff09\u662f\u89e3\u51b3\u7cfb\u7edf\u67b6\u6784\u5f00\u53d1\u4e2d\u8ba1\u7b97\u590d\u6742\u6027\u548c\u4f18\u5316\u6311\u6218\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
