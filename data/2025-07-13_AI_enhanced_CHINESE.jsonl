{"id": "2507.07142", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.07142", "abs": "https://arxiv.org/abs/2507.07142", "authors": ["Quanjie Qiu", "MengCheng Lau"], "title": "g2o vs. Ceres: Optimizing Scan Matching in Cartographer SLAM", "comment": null, "summary": "This article presents a comparative analysis of g2o and Ceres solvers in\nenhancing scan matching performance within the Cartographer framework.\nCartographer, a widely-used library for Simultaneous Localization and Mapping\n(SLAM), relies on optimization algorithms to refine pose estimates and improve\nmap accuracy. The research aims to evaluate the performance, efficiency, and\naccuracy of the g2o solver in comparison to the Ceres solver, which is the\ndefault in Cartographer. In our experiments comparing Ceres and g2o within\nCartographer, Ceres outperformed g2o in terms of speed, convergence efficiency,\nand overall map clarity. Ceres required fewer iterations and less time to\nconverge, producing more accurate and well-defined maps, especially in\nreal-world mapping scenarios with the AgileX LIMO robot. However, g2o excelled\nin localized obstacle detection, highlighting its value in specific situations.", "AI": {"tldr": "\u6bd4\u8f83g2o\u548cCeres\u5728Cartographer\u6846\u67b6\u4e2d\u4f18\u5316\u626b\u63cf\u5339\u914d\u6027\u80fd\u7684\u8868\u73b0\uff0cCeres\u5728\u901f\u5ea6\u3001\u6536\u655b\u6548\u7387\u548c\u5730\u56fe\u6e05\u6670\u5ea6\u4e0a\u4f18\u4e8eg2o\uff0c\u800cg2o\u5728\u5c40\u90e8\u969c\u788d\u7269\u68c0\u6d4b\u4e2d\u8868\u73b0\u66f4\u597d\u3002", "motivation": "\u8bc4\u4f30g2o\u4e0eCeres\u5728SLAM\u5e93Cartographer\u4e2d\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u4ee5\u4f18\u5316\u59ff\u6001\u4f30\u8ba1\u548c\u5730\u56fe\u7cbe\u5ea6\u3002", "method": "\u5728Cartographer\u6846\u67b6\u4e2d\u5bf9\u6bd4g2o\u548cCeres\u7684\u6027\u80fd\u3001\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u4f7f\u7528AgileX LIMO\u673a\u5668\u4eba\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "Ceres\u5728\u901f\u5ea6\u3001\u6536\u655b\u6548\u7387\u548c\u5730\u56fe\u6e05\u6670\u5ea6\u4e0a\u4f18\u4e8eg2o\uff0c\u4f46g2o\u5728\u5c40\u90e8\u969c\u788d\u7269\u68c0\u6d4b\u4e2d\u8868\u73b0\u66f4\u4f73\u3002", "conclusion": "Ceres\u66f4\u9002\u5408Cartographer\u4e2d\u7684\u5168\u5c40\u4f18\u5316\u4efb\u52a1\uff0c\u800cg2o\u5728\u7279\u5b9a\u573a\u666f\uff08\u5982\u5c40\u90e8\u969c\u788d\u7269\u68c0\u6d4b\uff09\u4e2d\u66f4\u5177\u4f18\u52bf\u3002"}}
{"id": "2507.07221", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.07221", "abs": "https://arxiv.org/abs/2507.07221", "authors": ["Nam Gyun Kim", "William E. Heap", "Yimeng Qin", "Elvy B. Yao", "Jee-Hwan Ryu", "Allison M. Okamura"], "title": "Self-Wearing Adaptive Garments via Soft Robotic Unfurling", "comment": null, "summary": "Robotic dressing assistance has the potential to improve the quality of life\nfor individuals with limited mobility. Existing solutions predominantly rely on\nrigid robotic manipulators, which have challenges in handling deformable\ngarments and ensuring safe physical interaction with the human body. Prior\nrobotic dressing methods require excessive operation times, complex control\nstrategies, and constrained user postures, limiting their practicality and\nadaptability. This paper proposes a novel soft robotic dressing system, the\nSelf-Wearing Adaptive Garment (SWAG), which uses an unfurling and growth\nmechanism to facilitate autonomous dressing. Unlike traditional approaches,the\nSWAG conforms to the human body through an unfurling based deployment method,\neliminating skin-garment friction and enabling a safer and more efficient\ndressing process. We present the working principles of the SWAG, introduce its\ndesign and fabrication, and demonstrate its performance in dressing assistance.\nThe proposed system demonstrates effective garment application across various\ngarment configurations, presenting a promising alternative to conventional\nrobotic dressing assistance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u8f6f\u673a\u5668\u4eba\u7a7f\u8863\u7cfb\u7edfSWAG\uff0c\u901a\u8fc7\u5c55\u5f00\u548c\u751f\u957f\u673a\u5236\u5b9e\u73b0\u81ea\u4e3b\u7a7f\u8863\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u521a\u6027\u673a\u5668\u4eba\u5904\u7406\u53d8\u5f62\u8863\u7269\u548c\u786e\u4fdd\u5b89\u5168\u4e92\u52a8\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u7a7f\u8863\u65b9\u6848\u4f9d\u8d56\u521a\u6027\u673a\u68b0\u81c2\uff0c\u64cd\u4f5c\u65f6\u95f4\u957f\u3001\u63a7\u5236\u590d\u6742\u4e14\u9650\u5236\u7528\u6237\u59ff\u52bf\uff0c\u5b9e\u7528\u6027\u5dee\u3002SWAG\u65e8\u5728\u63d0\u4f9b\u66f4\u5b89\u5168\u9ad8\u6548\u7684\u7a7f\u8863\u8f85\u52a9\u3002", "method": "\u91c7\u7528\u5c55\u5f00\u5f0f\u90e8\u7f72\u65b9\u6cd5\uff0cSWAG\u8d34\u5408\u4eba\u4f53\uff0c\u51cf\u5c11\u76ae\u80a4\u4e0e\u8863\u7269\u6469\u64e6\uff0c\u8bbe\u8ba1\u5e76\u5c55\u793a\u4e86\u5176\u5de5\u4f5c\u539f\u7406\u548c\u6027\u80fd\u3002", "result": "SWAG\u5728\u4e0d\u540c\u8863\u7269\u914d\u7f6e\u4e0b\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u4f20\u7edf\u673a\u5668\u4eba\u7a7f\u8863\u8f85\u52a9\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "SWAG\u901a\u8fc7\u8f6f\u673a\u5668\u4eba\u6280\u672f\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u5b89\u5168\u7684\u7a7f\u8863\u8f85\u52a9\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.07225", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.07225", "abs": "https://arxiv.org/abs/2507.07225", "authors": ["Yimeng Qin", "Jared Grinberg", "William Heap", "Allison M. Okamura"], "title": "3D Steering and Localization in Pipes and Burrows using an Externally Steered Soft Growing Robot", "comment": null, "summary": "Navigation and inspection in confined environments, such as tunnels and\npipes, pose significant challenges for existing robots due to limitations in\nmaneuverability and adaptability to varying geometries. Vine robots, which are\nsoft growing continuum robots that extend their length through soft material\neversion at their tip, offer unique advantages due to their ability to navigate\ntight spaces, adapt to complex paths, and minimize friction. However, existing\nvine robot designs struggle with navigation in manmade and natural passageways,\nwith branches and sharp 3D turns. In this letter, we introduce a steerable vine\nrobot specifically designed for pipe and burrow environments. The robot\nfeatures a simple tubular body and an external tip mount that steers the vine\nrobot in three degrees of freedom by changing the growth direction and, when\nnecessary, bracing against the wall of the pipe or burrow. Our external tip\nsteering approach enables: (1) active branch selection in 3D space with a\nmaximum steerable angle of 51.7{\\deg}, (2) navigation of pipe networks with\nradii as small as 2.5 cm, (3) a compliant tip enabling navigation of sharp\nturns, and (4) real-time 3D localization in GPS-denied environments using\ntip-mounted sensors and continuum body odometry. We describe the forward\nkinematics, characterize steerability, and demonstrate the system in a 3D pipe\nsystem as well as a natural animal burrow.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u53ef\u8f6c\u5411\u7684\u85e4\u8513\u673a\u5668\u4eba\uff0c\u4e13\u4e3a\u7ba1\u9053\u548c\u6d1e\u7a74\u73af\u5883\u8bbe\u8ba1\uff0c\u5177\u6709\u4e3b\u52a8\u5206\u652f\u9009\u62e9\u3001\u5c0f\u534a\u5f84\u5bfc\u822a\u3001\u7075\u6d3b\u8f6c\u5411\u548c\u5b9e\u65f6\u5b9a\u4f4d\u529f\u80fd\u3002", "motivation": "\u73b0\u6709\u85e4\u8513\u673a\u5668\u4eba\u5728\u4eba\u9020\u548c\u81ea\u7136\u901a\u9053\u4e2d\u5bfc\u822a\u56f0\u96be\uff0c\u5c24\u5176\u662f\u5728\u5206\u652f\u548c\u6025\u8f6c\u5f2f\u5904\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5916\u90e8\u5c16\u7aef\u5b89\u88c5\u8bbe\u8ba1\uff0c\u901a\u8fc7\u6539\u53d8\u751f\u957f\u65b9\u5411\u548c\u652f\u6491\u7ba1\u9053\u58c1\u5b9e\u73b0\u4e09\u81ea\u7531\u5ea6\u8f6c\u5411\u3002", "result": "\u673a\u5668\u4eba\u5b9e\u73b0\u4e8651.7\u5ea6\u7684\u6700\u5927\u8f6c\u5411\u89d2\u30012.5\u5398\u7c73\u5c0f\u534a\u5f84\u5bfc\u822a\u3001\u7075\u6d3b\u8f6c\u5411\u548c\u5b9e\u65f63D\u5b9a\u4f4d\u3002", "conclusion": "\u8be5\u8bbe\u8ba1\u663e\u8457\u63d0\u5347\u4e86\u85e4\u8513\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u7ba1\u9053\u548c\u6d1e\u7a74\u5e94\u7528\u3002"}}
{"id": "2507.07299", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.07299", "abs": "https://arxiv.org/abs/2507.07299", "authors": ["Sonia Raychaudhuri", "Enrico Cancelli", "Tommaso Campari", "Lamberto Ballan", "Manolis Savva", "Angel X. Chang"], "title": "LangNavBench: Evaluation of Natural Language Understanding in Semantic Navigation", "comment": null, "summary": "Recent progress in large vision-language models has driven improvements in\nlanguage-based semantic navigation, where an embodied agent must reach a target\nobject described in natural language. Despite these advances, we still lack a\nclear, language-focused benchmark for testing how well such agents ground the\nwords in their instructions. We address this gap with LangNav, an open-set\ndataset specifically created to test an agent's ability to locate objects\ndescribed at different levels of detail, from broad category names to fine\nattributes and object-object relations. Every description in LangNav was\nmanually checked, yielding a lower error rate than existing lifelong- and\nsemantic-navigation datasets. On top of LangNav we build LangNavBench, a\nbenchmark that measures how well current semantic-navigation methods understand\nand act on these descriptions while moving toward their targets. LangNavBench\nallows us to systematically compare models on their handling of attributes,\nspatial and relational cues, and category hierarchies, offering the first\nthorough, language-centric evaluation of embodied navigation systems. We also\npresent Multi-Layered Feature Map (MLFM), a method that builds a queryable\nmulti-layered semantic map, particularly effective when dealing with small\nobjects or instructions involving spatial relations. MLFM outperforms\nstate-of-the-art mapping-based navigation baselines on the LangNav dataset.", "AI": {"tldr": "LangNav\u662f\u4e00\u4e2a\u4e13\u6ce8\u4e8e\u8bed\u8a00\u7406\u89e3\u7684\u8bed\u4e49\u5bfc\u822a\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u6d4b\u8bd5\u667a\u80fd\u4f53\u5bf9\u4e0d\u540c\u7ec6\u8282\u5c42\u6b21\u63cf\u8ff0\u7684\u7269\u4f53\u5b9a\u4f4d\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u4e86LangNavBench\u57fa\u51c6\u548cMLFM\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u8bed\u4e49\u5bfc\u822a\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u8bed\u8a00\u7406\u89e3\u7684\u7cfb\u7edf\u8bc4\u4f30\uff0cLangNav\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u4e86LangNav\u6570\u636e\u96c6\u548cLangNavBench\u57fa\u51c6\uff0c\u5e76\u5f00\u53d1\u4e86MLFM\u65b9\u6cd5\uff0c\u6784\u5efa\u53ef\u67e5\u8be2\u7684\u591a\u5c42\u8bed\u4e49\u5730\u56fe\u3002", "result": "MLFM\u5728LangNav\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u57fa\u4e8e\u5730\u56fe\u7684\u5bfc\u822a\u65b9\u6cd5\u3002", "conclusion": "LangNav\u548cMLFM\u4e3a\u8bed\u8a00\u9a71\u52a8\u7684\u8bed\u4e49\u5bfc\u822a\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u548c\u6539\u8fdb\u65b9\u6cd5\u3002"}}
{"id": "2507.07115", "categories": ["cs.AI", "cs.MA", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.07115", "abs": "https://arxiv.org/abs/2507.07115", "authors": ["Javal Vyas", "Mehmet Mercangoz"], "title": "Autonomous Control Leveraging LLMs: An Agentic Framework for Next-Generation Industrial Automation", "comment": null, "summary": "The increasing complexity of modern chemical processes, coupled with\nworkforce shortages and intricate fault scenarios, demands novel automation\nparadigms that blend symbolic reasoning with adaptive control. In this work, we\nintroduce a unified agentic framework that leverages large language models\n(LLMs) for both discrete fault-recovery planning and continuous process control\nwithin a single architecture. We adopt Finite State Machines (FSMs) as\ninterpretable operating envelopes: an LLM-driven planning agent proposes\nrecovery sequences through the FSM, a Simulation Agent executes and checks each\ntransition, and a Validator-Reprompting loop iteratively refines invalid plans.\nIn Case Study 1, across 180 randomly generated FSMs of varying sizes (4-25\nstates, 4-300 transitions), GPT-4o and GPT-4o-mini achieve 100% valid-path\nsuccess within five reprompts-outperforming open-source LLMs in both accuracy\nand latency. In Case Study 2, the same framework modulates dual-heater inputs\non a laboratory TCLab platform (and its digital twin) to maintain a target\naverage temperature under persistent asymmetric disturbances. Compared to\nclassical PID control, our LLM-based controller attains similar performance,\nwhile ablation of the prompting loop reveals its critical role in handling\nnonlinear dynamics. We analyze key failure modes-such as instruction following\nlapses and coarse ODE approximations. Our results demonstrate that, with\nstructured feedback and modular agents, LLMs can unify high-level symbolic\nplanningand low-level continuous control, paving the way towards resilient,\nlanguage-driven automation in chemical engineering.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7b26\u53f7\u63a8\u7406\u4e0e\u81ea\u9002\u5e94\u63a7\u5236\u7684\u7edf\u4e00\u4ee3\u7406\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5b9e\u73b0\u79bb\u6563\u6545\u969c\u6062\u590d\u89c4\u5212\u548c\u8fde\u7eed\u8fc7\u7a0b\u63a7\u5236\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u4ee3\u5316\u5b66\u8fc7\u7a0b\u590d\u6742\u6027\u589e\u52a0\uff0c\u52b3\u52a8\u529b\u77ed\u7f3a\u548c\u6545\u969c\u573a\u666f\u590d\u6742\uff0c\u9700\u8981\u65b0\u7684\u81ea\u52a8\u5316\u8303\u5f0f\u3002", "method": "\u91c7\u7528\u6709\u9650\u72b6\u6001\u673a\uff08FSMs\uff09\u4f5c\u4e3a\u53ef\u89e3\u91ca\u7684\u64cd\u4f5c\u6846\u67b6\uff0c\u7ed3\u5408LLM\u9a71\u52a8\u7684\u89c4\u5212\u4ee3\u7406\u3001\u6a21\u62df\u4ee3\u7406\u548c\u9a8c\u8bc1-\u91cd\u65b0\u63d0\u793a\u5faa\u73af\u3002", "result": "\u5728\u6848\u4f8b\u7814\u7a76\u4e2d\uff0cGPT-4o\u548cGPT-4o-mini\u5728180\u4e2a\u968f\u673a\u751f\u6210\u7684FSMs\u4e2d\u5b9e\u73b0\u4e86100%\u7684\u6709\u6548\u8def\u5f84\u6210\u529f\u7387\uff1b\u5728\u53cc\u52a0\u70ed\u5668\u63a7\u5236\u4e2d\uff0cLLM\u63a7\u5236\u5668\u6027\u80fd\u63a5\u8fd1\u7ecf\u5178PID\u63a7\u5236\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u6784\u5316\u53cd\u9988\u548c\u6a21\u5757\u5316\u4ee3\u7406\uff0cLLMs\u53ef\u4ee5\u7edf\u4e00\u9ad8\u7ea7\u7b26\u53f7\u89c4\u5212\u548c\u4f4e\u7ea7\u8fde\u7eed\u63a7\u5236\uff0c\u4e3a\u5316\u5b66\u5de5\u7a0b\u4e2d\u7684\u5f39\u6027\u81ea\u52a8\u5316\u94fa\u5e73\u9053\u8def\u3002"}}
{"id": "2507.07315", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.07315", "abs": "https://arxiv.org/abs/2507.07315", "authors": ["Ricardo Vega", "Cameron Nowzari"], "title": "Classifying Emergence in Robot Swarms: An Observer-Dependent Approach", "comment": "25 pages, 3 tables, 8 figures", "summary": "Emergence and swarms are widely discussed topics, yet no consensus exists on\ntheir formal definitions. This lack of agreement makes it difficult not only\nfor new researchers to grasp these concepts, but also for experts who may use\nthe same terms to mean different things. Many attempts have been made to\nobjectively define 'swarm' or 'emergence,' with recent work highlighting the\nrole of the external observer. Still, several researchers argue that once an\nobserver's vantage point (e.g., scope, resolution, context) is established, the\nterms can be made objective or measured quantitatively. In this note, we\npropose a framework to discuss these ideas rigorously by separating externally\nobservable states from latent, unobservable ones. This allows us to compare and\ncontrast existing definitions of swarms and emergence on common ground. We\nargue that these concepts are ultimately subjective-shaped less by the system\nitself than by the perception and tacit knowledge of the observer.\nSpecifically, we suggest that a 'swarm' is not defined by its group behavior\nalone, but by the process generating that behavior. Our broader goal is to\nsupport the design and deployment of robotic swarm systems, highlighting the\ncritical distinction between multi-robot systems and true swarms.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u4e2a\u6846\u67b6\uff0c\u901a\u8fc7\u533a\u5206\u5916\u90e8\u53ef\u89c2\u5bdf\u72b6\u6001\u4e0e\u6f5c\u5728\u4e0d\u53ef\u89c2\u5bdf\u72b6\u6001\uff0c\u4e25\u683c\u8ba8\u8bba'\u7fa4\u4f53'\u548c'\u6d8c\u73b0'\u7684\u5b9a\u4e49\uff0c\u5f3a\u8c03\u8fd9\u4e9b\u6982\u5ff5\u7684\u4e3b\u89c2\u6027\u3002", "motivation": "\u7531\u4e8e\u5bf9'\u7fa4\u4f53'\u548c'\u6d8c\u73b0'\u7f3a\u4e4f\u5171\u8bc6\uff0c\u65b0\u7814\u7a76\u8005\u96be\u4ee5\u7406\u89e3\u8fd9\u4e9b\u6982\u5ff5\uff0c\u4e13\u5bb6\u4e5f\u53ef\u80fd\u56e0\u672f\u8bed\u6b67\u4e49\u4ea7\u751f\u8bef\u89e3\u3002\u8bba\u6587\u65e8\u5728\u63d0\u4f9b\u4e00\u4e2a\u5171\u540c\u8ba8\u8bba\u7684\u57fa\u7840\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u6846\u67b6\uff0c\u5206\u79bb\u5916\u90e8\u53ef\u89c2\u5bdf\u72b6\u6001\u4e0e\u6f5c\u5728\u4e0d\u53ef\u89c2\u5bdf\u72b6\u6001\uff0c\u5e76\u57fa\u4e8e\u6b64\u5bf9\u6bd4\u73b0\u6709\u5b9a\u4e49\u3002", "result": "\u8ba4\u4e3a'\u7fa4\u4f53'\u548c'\u6d8c\u73b0'\u7684\u5b9a\u4e49\u66f4\u591a\u53d6\u51b3\u4e8e\u89c2\u5bdf\u8005\u7684\u89c6\u89d2\u548c\u9690\u6027\u77e5\u8bc6\uff0c\u800c\u975e\u7cfb\u7edf\u672c\u8eab\u3002", "conclusion": "\u5f3a\u8c03'\u7fa4\u4f53'\u7684\u5b9a\u4e49\u5e94\u5173\u6ce8\u884c\u4e3a\u751f\u6210\u8fc7\u7a0b\u800c\u975e\u884c\u4e3a\u672c\u8eab\uff0c\u4e3a\u673a\u5668\u4eba\u7fa4\u4f53\u7cfb\u7edf\u7684\u8bbe\u8ba1\u548c\u90e8\u7f72\u63d0\u4f9b\u7406\u8bba\u652f\u6301\u3002"}}
{"id": "2507.07134", "categories": ["cs.AI", "cs.LG", "I.2.10"], "pdf": "https://arxiv.org/pdf/2507.07134", "abs": "https://arxiv.org/abs/2507.07134", "authors": ["Mridula Vijendran", "Shuang Chen", "Jingjing Deng", "Hubert P. H. Shum"], "title": "BOOST: Out-of-Distribution-Informed Adaptive Sampling for Bias Mitigation in Stylistic Convolutional Neural Networks", "comment": "18 pages, 7 figures, 3 tables", "summary": "The pervasive issue of bias in AI presents a significant challenge to\npainting classification, and is getting more serious as these systems become\nincreasingly integrated into tasks like art curation and restoration. Biases,\noften arising from imbalanced datasets where certain artistic styles dominate,\ncompromise the fairness and accuracy of model predictions, i.e., classifiers\nare less accurate on rarely seen paintings. While prior research has made\nstrides in improving classification performance, it has largely overlooked the\ncritical need to address these underlying biases, that is, when dealing with\nout-of-distribution (OOD) data. Our insight highlights the necessity of a more\nrobust approach to bias mitigation in AI models for art classification on\nbiased training data. We propose a novel OOD-informed model bias adaptive\nsampling method called BOOST (Bias-Oriented OOD Sampling and Tuning). It\naddresses these challenges by dynamically adjusting temperature scaling and\nsampling probabilities, thereby promoting a more equitable representation of\nall classes. We evaluate our proposed approach to the KaoKore and PACS\ndatasets, focusing on the model's ability to reduce class-wise bias. We further\npropose a new metric, Same-Dataset OOD Detection Score (SODC), designed to\nassess class-wise separation and per-class bias reduction. Our method\ndemonstrates the ability to balance high performance with fairness, making it a\nrobust solution for unbiasing AI models in the art domain.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBOOST\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u6e29\u5ea6\u7f29\u653e\u548c\u91c7\u6837\u6982\u7387\uff0c\u89e3\u51b3AI\u827a\u672f\u5206\u7c7b\u4e2d\u7684\u504f\u89c1\u95ee\u9898\uff0c\u5e76\u5728KaoKore\u548cPACS\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "AI\u827a\u672f\u5206\u7c7b\u4e2d\u56e0\u6570\u636e\u96c6\u4e0d\u5e73\u8861\u5bfc\u81f4\u7684\u504f\u89c1\u95ee\u9898\u65e5\u76ca\u4e25\u91cd\uff0c\u73b0\u6709\u7814\u7a76\u591a\u5ffd\u89c6\u5bf9\u5206\u5e03\u5916\u6570\u636e\u7684\u5904\u7406\uff0c\u4e9f\u9700\u66f4\u9c81\u68d2\u7684\u504f\u89c1\u7f13\u89e3\u65b9\u6cd5\u3002", "method": "\u63d0\u51faBOOST\u65b9\u6cd5\uff0c\u52a8\u6001\u8c03\u6574\u6e29\u5ea6\u7f29\u653e\u548c\u91c7\u6837\u6982\u7387\uff0c\u5e76\u5f15\u5165\u65b0\u6307\u6807SODC\u8bc4\u4f30\u7c7b\u522b\u5206\u79bb\u548c\u504f\u89c1\u51cf\u5c11\u3002", "result": "BOOST\u5728KaoKore\u548cPACS\u6570\u636e\u96c6\u4e0a\u6709\u6548\u5e73\u8861\u4e86\u6027\u80fd\u4e0e\u516c\u5e73\u6027\uff0c\u51cf\u5c11\u4e86\u7c7b\u522b\u504f\u89c1\u3002", "conclusion": "BOOST\u4e3a\u827a\u672f\u9886\u57df\u7684AI\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9c81\u68d2\u7684\u504f\u89c1\u7f13\u89e3\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.07327", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.07327", "abs": "https://arxiv.org/abs/2507.07327", "authors": ["Brian B. Vuong", "Josie Davidson", "Sangheui Cheon", "Kyujin Cho", "Allison M. Okamura"], "title": "Effects of Wrist-Worn Haptic Feedback on Force Accuracy and Task Speed during a Teleoperated Robotic Surgery Task", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Previous work has shown that the addition of haptic feedback to the hands can\nimprove awareness of tool-tissue interactions and enhance performance of\nteleoperated tasks in robot-assisted minimally invasive surgery. However,\nhand-based haptic feedback occludes direct interaction with the manipulanda of\nsurgeon console in teleoperated surgical robots. We propose relocating haptic\nfeedback to the wrist using a wearable haptic device so that haptic feedback\nmechanisms do not need to be integrated into the manipulanda. However, it is\nunknown if such feedback will be effective, given that it is not co-located\nwith the finger movements used for manipulation. To test if relocated haptic\nfeedback improves force application during teleoperated tasks using da Vinci\nResearch Kit (dVRK) surgical robot, participants learned to palpate a phantom\ntissue to desired forces. A soft pneumatic wrist-worn haptic device with an\nanchoring system renders tool-tissue interaction forces to the wrist of the\nuser. Participants performed the palpation task with and without wrist-worn\nhaptic feedback and were evaluated for the accuracy of applied forces.\nParticipants demonstrated statistically significant lower force error when\nwrist-worn haptic feedback was provided. Participants also performed the\npalpation task with longer movement times when provided wrist-worn haptic\nfeedback, indicating that the haptic feedback may have caused participants to\noperate at a different point in the speed-accuracy tradeoff curve.", "AI": {"tldr": "\u5c06\u89e6\u89c9\u53cd\u9988\u4ece\u624b\u90e8\u79fb\u81f3\u8155\u90e8\uff0c\u901a\u8fc7\u53ef\u7a7f\u6234\u8bbe\u5907\u63d0\u4f9b\u89e6\u89c9\u53cd\u9988\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u529b\u5e94\u7528\u8bef\u5dee\uff0c\u4f46\u589e\u52a0\u4e86\u64cd\u4f5c\u65f6\u95f4\u3002", "motivation": "\u624b\u90e8\u89e6\u89c9\u53cd\u9988\u4f1a\u5e72\u6270\u624b\u672f\u673a\u5668\u4eba\u64cd\u7eb5\u5668\u7684\u76f4\u63a5\u64cd\u4f5c\uff0c\u56e0\u6b64\u7814\u7a76\u8155\u90e8\u89e6\u89c9\u53cd\u9988\u7684\u53ef\u884c\u6027\u3002", "method": "\u4f7f\u7528\u8f6f\u6c14\u52a8\u624b\u8155\u89e6\u89c9\u8bbe\u5907\uff0c\u6d4b\u8bd5\u53c2\u4e0e\u8005\u5728\u6709\u65e0\u89e6\u89c9\u53cd\u9988\u4e0b\u5b8c\u6210\u7ec4\u7ec7\u89e6\u8bca\u4efb\u52a1\u7684\u529b\u5e94\u7528\u51c6\u786e\u6027\u3002", "result": "\u63d0\u4f9b\u8155\u90e8\u89e6\u89c9\u53cd\u9988\u65f6\uff0c\u53c2\u4e0e\u8005\u7684\u529b\u8bef\u5dee\u663e\u8457\u964d\u4f4e\uff0c\u4f46\u64cd\u4f5c\u65f6\u95f4\u5ef6\u957f\u3002", "conclusion": "\u8155\u90e8\u89e6\u89c9\u53cd\u9988\u80fd\u6709\u6548\u63d0\u5347\u529b\u5e94\u7528\u51c6\u786e\u6027\uff0c\u4f46\u53ef\u80fd\u5f71\u54cd\u64cd\u4f5c\u901f\u5ea6\u3002"}}
{"id": "2507.07203", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.07203", "abs": "https://arxiv.org/abs/2507.07203", "authors": ["Minkyung Kim", "Junsik Kim", "Hwidong Bae", "Woongcheol Yang", "Sangdon Park", "Sohee Bae"], "title": "State-Inference-Based Prompting for Natural Language Trading with Game NPCs", "comment": "9 pages main content, 4 pages appendix, 3 figures. Accepted to the\n  KDD 2025 Workshop on Prompt Optimization", "summary": "Large Language Models enable dynamic game interactions but struggle with\nrule-governed trading systems. Current implementations suffer from rule\nviolations, such as item hallucinations and calculation errors, that erode\nplayer trust. Here, State-Inference-Based Prompting (SIBP) enables reliable\ntrading through autonomous dialogue state inference and context-specific rule\nadherence. The approach decomposes trading into six states within a unified\nprompt framework, implementing context-aware item referencing and\nplaceholder-based price calculations. Evaluation across 100 trading dialogues\ndemonstrates >97% state compliance, >95% referencing accuracy, and 99.7%\ncalculation precision. SIBP maintains computational efficiency while\noutperforming baseline approaches, establishing a practical foundation for\ntrustworthy NPC interactions in commercial games.", "AI": {"tldr": "SIBP\u65b9\u6cd5\u901a\u8fc7\u72b6\u6001\u63a8\u7406\u548c\u4e0a\u4e0b\u6587\u89c4\u5219\u9075\u5b88\uff0c\u89e3\u51b3\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u89c4\u5219\u4ea4\u6613\u7cfb\u7edf\u4e2d\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u51c6\u786e\u6027\u548c\u4fe1\u4efb\u5ea6\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u89c4\u5219\u4ea4\u6613\u7cfb\u7edf\u4e2d\u5b58\u5728\u89c4\u5219\u8fdd\u53cd\u95ee\u9898\uff08\u5982\u7269\u54c1\u5e7b\u89c9\u548c\u8ba1\u7b97\u9519\u8bef\uff09\uff0c\u5bfc\u81f4\u73a9\u5bb6\u4fe1\u4efb\u4e0b\u964d\u3002", "method": "\u63d0\u51faState-Inference-Based Prompting\uff08SIBP\uff09\uff0c\u5c06\u4ea4\u6613\u5206\u89e3\u4e3a\u516d\u4e2a\u72b6\u6001\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u7269\u54c1\u5f15\u7528\u548c\u5360\u4f4d\u7b26\u4ef7\u683c\u8ba1\u7b97\u5b9e\u73b0\u89c4\u5219\u9075\u5b88\u3002", "result": "\u5728100\u6b21\u4ea4\u6613\u5bf9\u8bdd\u4e2d\uff0cSIBP\u5b9e\u73b0\u4e86>97%\u7684\u72b6\u6001\u9075\u5b88\u7387\u3001>95%\u7684\u5f15\u7528\u51c6\u786e\u7387\u548c99.7%\u7684\u8ba1\u7b97\u7cbe\u5ea6\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "SIBP\u4e3a\u5546\u4e1a\u6e38\u620f\u4e2d\u53ef\u4fe1\u8d56\u7684NPC\u4ea4\u4e92\u63d0\u4f9b\u4e86\u5b9e\u7528\u57fa\u7840\uff0c\u540c\u65f6\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2507.07356", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.07356", "abs": "https://arxiv.org/abs/2507.07356", "authors": ["Kangning Yin", "Weishuai Zeng", "Ke Fan", "Zirui Wang", "Qiang Zhang", "Zheng Tian", "Jingbo Wang", "Jiangmiao Pang", "Weinan Zhang"], "title": "UniTracker: Learning Universal Whole-Body Motion Tracker for Humanoid Robots", "comment": "10 pages, 5 figures", "summary": "Humanoid robots must achieve diverse, robust, and generalizable whole-body\ncontrol to operate effectively in complex, human-centric environments. However,\nexisting methods, particularly those based on teacher-student frameworks often\nsuffer from a loss of motion diversity during policy distillation and exhibit\nlimited generalization to unseen behaviors. In this work, we present\nUniTracker, a simplified yet powerful framework that integrates a Conditional\nVariational Autoencoder (CVAE) into the student policy to explicitly model the\nlatent diversity of human motion. By leveraging a learned CVAE prior, our\nmethod enables the student to retain expressive motion characteristics while\nimproving robustness and adaptability under partial observations. The result is\na single policy capable of tracking a wide spectrum of whole-body motions with\nhigh fidelity and stability. Comprehensive experiments in both simulation and\nreal-world deployments demonstrate that UniTracker significantly outperforms\nMLP-based DAgger baselines in motion quality, generalization to unseen\nreferences, and deployment robustness, offering a practical and scalable\nsolution for expressive humanoid control.", "AI": {"tldr": "UniTracker\u662f\u4e00\u4e2a\u57fa\u4e8eCVAE\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u4eba\u5f62\u673a\u5668\u4eba\u7684\u5168\u8eab\u63a7\u5236\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u8fd0\u52a8\u591a\u6837\u6027\u548c\u6cdb\u5316\u6027\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5e08\u751f\u6846\u67b6\u7684\u65b9\u6cd5\u5728\u7b56\u7565\u84b8\u998f\u8fc7\u7a0b\u4e2d\u4f1a\u635f\u5931\u8fd0\u52a8\u591a\u6837\u6027\uff0c\u4e14\u5bf9\u672a\u89c1\u884c\u4e3a\u7684\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002", "method": "\u901a\u8fc7\u5c06CVAE\u96c6\u6210\u5230\u5b66\u751f\u7b56\u7565\u4e2d\uff0c\u663e\u5f0f\u5efa\u6a21\u4eba\u7c7b\u8fd0\u52a8\u7684\u6f5c\u5728\u591a\u6837\u6027\uff0c\u5229\u7528CVAE\u5148\u9a8c\u4fdd\u7559\u8fd0\u52a8\u7279\u6027\u5e76\u63d0\u5347\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cUniTracker\u5728\u8fd0\u52a8\u8d28\u91cf\u3001\u6cdb\u5316\u80fd\u529b\u548c\u90e8\u7f72\u9c81\u68d2\u6027\u4e0a\u663e\u8457\u4f18\u4e8eMLP-based DAgger\u57fa\u7ebf\u3002", "conclusion": "UniTracker\u4e3a\u8868\u8fbe\u6027\u4eba\u5f62\u63a7\u5236\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.07217", "categories": ["cs.AI", "cs.LG", "cs.LO", "I.2.4; I.2.7; J.4"], "pdf": "https://arxiv.org/pdf/2507.07217", "abs": "https://arxiv.org/abs/2507.07217", "authors": ["Zili Wang", "Frank Montabon", "Kristin Yvonne Rozier"], "title": "Neurosymbolic Feature Extraction for Identifying Forced Labor in Supply Chains", "comment": null, "summary": "Supply chain networks are complex systems that are challenging to analyze;\nthis problem is exacerbated when there are illicit activities involved in the\nsupply chain, such as counterfeit parts, forced labor, or human trafficking.\nWhile machine learning (ML) can find patterns in complex systems like supply\nchains, traditional ML techniques require large training data sets. However,\nillicit supply chains are characterized by very sparse data, and the data that\nis available is often (purposely) corrupted or unreliable in order to hide the\nnature of the activities. We need to be able to automatically detect new\npatterns that correlate with such illegal activity over complex, even temporal\ndata, without requiring large training data sets. We explore neurosymbolic\nmethods for identifying instances of illicit activity in supply chains and\ncompare the effectiveness of manual and automated feature extraction from news\narticles accurately describing illicit activities uncovered by authorities. We\npropose a question tree approach for querying a large language model (LLM) to\nidentify and quantify the relevance of articles. This enables a systematic\nevaluation of the differences between human and machine classification of news\narticles related to forced labor in supply chains.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5229\u7528\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u68c0\u6d4b\u4f9b\u5e94\u94fe\u4e2d\u7684\u975e\u6cd5\u6d3b\u52a8\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u7a00\u758f\u4e14\u4e0d\u53ef\u9760\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u95ee\u9898\u6811\u65b9\u6cd5\uff0c\u6bd4\u8f83\u4eba\u5de5\u4e0e\u673a\u5668\u5206\u7c7b\u65b0\u95fb\u6587\u7ae0\u7684\u6548\u679c\u3002", "motivation": "\u4f9b\u5e94\u94fe\u7f51\u7edc\u590d\u6742\u4e14\u6d89\u53ca\u975e\u6cd5\u6d3b\u52a8\u65f6\u5206\u6790\u96be\u5ea6\u5927\uff0c\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u8bad\u7ec3\u6570\u636e\uff0c\u800c\u975e\u6cd5\u4f9b\u5e94\u94fe\u6570\u636e\u7a00\u758f\u4e14\u4e0d\u53ef\u9760\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u81ea\u52a8\u68c0\u6d4b\u975e\u6cd5\u6d3b\u52a8\u6a21\u5f0f\u3002", "method": "\u91c7\u7528\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\uff0c\u7ed3\u5408LLM\u548c\u95ee\u9898\u6811\u65b9\u6cd5\uff0c\u4ece\u65b0\u95fb\u6587\u7ae0\u4e2d\u63d0\u53d6\u7279\u5f81\uff0c\u6bd4\u8f83\u4eba\u5de5\u4e0e\u673a\u5668\u5206\u7c7b\u7684\u5dee\u5f02\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u6027\u8bc4\u4f30\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4e0d\u4f9d\u8d56\u5927\u91cf\u8bad\u7ec3\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u8bc6\u522b\u975e\u6cd5\u6d3b\u52a8\u6a21\u5f0f\u3002", "conclusion": "\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u548cLLM\u7684\u7ed3\u5408\u4e3a\u7a00\u758f\u6570\u636e\u4e0b\u7684\u975e\u6cd5\u4f9b\u5e94\u94fe\u6d3b\u52a8\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2507.07370", "categories": ["cs.RO", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.07370", "abs": "https://arxiv.org/abs/2507.07370", "authors": ["Zhanhong Jiang", "Dylan Shah", "Hsin-Jung Yang", "Soumik Sarkar"], "title": "Data-driven Kinematic Modeling in Soft Robots: System Identification and Uncertainty Quantification", "comment": "6 pages; 6 figures; accepted at the 5th Modeling, Estimation and\n  Control Conference (MECC 2025)", "summary": "Precise kinematic modeling is critical in calibration and controller design\nfor soft robots, yet remains a challenging issue due to their highly nonlinear\nand complex behaviors. To tackle the issue, numerous data-driven machine\nlearning approaches have been proposed for modeling nonlinear dynamics.\nHowever, these models suffer from prediction uncertainty that can negatively\naffect modeling accuracy, and uncertainty quantification for kinematic modeling\nin soft robots is underexplored. In this work, using limited simulation and\nreal-world data, we first investigate multiple linear and nonlinear machine\nlearning models commonly used for kinematic modeling of soft robots. The\nresults reveal that nonlinear ensemble methods exhibit the most robust\ngeneralization performance. We then develop a conformal kinematic modeling\nframework for soft robots by utilizing split conformal prediction to quantify\npredictive position uncertainty, ensuring distribution-free prediction\nintervals with a theoretical guarantee.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5171\u5f62\u9884\u6d4b\u7684\u8f6f\u4f53\u673a\u5668\u4eba\u8fd0\u52a8\u5b66\u5efa\u6a21\u6846\u67b6\uff0c\u7528\u4e8e\u91cf\u5316\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\uff0c\u63d0\u5347\u5efa\u6a21\u7cbe\u5ea6\u3002", "motivation": "\u8f6f\u4f53\u673a\u5668\u4eba\u8fd0\u52a8\u5b66\u5efa\u6a21\u56e0\u9ad8\u5ea6\u975e\u7ebf\u6027\u548c\u590d\u6742\u884c\u4e3a\u800c\u5177\u6709\u6311\u6218\u6027\uff0c\u73b0\u6709\u6570\u636e\u9a71\u52a8\u6a21\u578b\u5b58\u5728\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\uff0c\u5f71\u54cd\u5efa\u6a21\u7cbe\u5ea6\u3002", "method": "\u7814\u7a76\u4e86\u591a\u79cd\u7ebf\u6027\u548c\u975e\u7ebf\u6027\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u53d1\u73b0\u975e\u7ebf\u6027\u96c6\u6210\u65b9\u6cd5\u6cdb\u5316\u6027\u80fd\u6700\u4f73\uff1b\u5f00\u53d1\u4e86\u57fa\u4e8e\u5171\u5f62\u9884\u6d4b\u7684\u5efa\u6a21\u6846\u67b6\uff0c\u91cf\u5316\u9884\u6d4b\u4f4d\u7f6e\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u975e\u7ebf\u6027\u96c6\u6210\u65b9\u6cd5\u8868\u73b0\u6700\u4f18\uff1b\u5171\u5f62\u9884\u6d4b\u6846\u67b6\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\u7684\u9884\u6d4b\u533a\u95f4\u3002", "conclusion": "\u5171\u5f62\u9884\u6d4b\u6846\u67b6\u80fd\u6709\u6548\u91cf\u5316\u8f6f\u4f53\u673a\u5668\u4eba\u8fd0\u52a8\u5b66\u5efa\u6a21\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u63d0\u5347\u6a21\u578b\u53ef\u9760\u6027\u3002"}}
{"id": "2507.07257", "categories": ["cs.AI", "astro-ph.IM", "cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.07257", "abs": "https://arxiv.org/abs/2507.07257", "authors": ["Licong Xu", "Milind Sarkar", "Anto I. Lonappan", "\u00cd\u00f1igo Zubeldia", "Pablo Villanueva-Domingo", "Santiago Casas", "Christian Fidler", "Chetana Amancharla", "Ujjwal Tiwari", "Adrian Bayer", "Chadi Ait Ekiou", "Miles Cranmer", "Adrian Dimitrov", "James Fergusson", "Kahaan Gandhi", "Sven Krippendorf", "Andrew Laverick", "Julien Lesgourgues", "Antony Lewis", "Thomas Meier", "Blake Sherwin", "Kristen Surrao", "Francisco Villaescusa-Navarro", "Chi Wang", "Xueqing Xu", "Boris Bolliet"], "title": "Open Source Planning & Control System with Language Agents for Autonomous Scientific Discovery", "comment": "Accepted contribution to the ICML 2025 Workshop on Machine Learning\n  for Astrophysics. Code: https://github.com/CMBAgents/cmbagent; Videos:\n  https://www.youtube.com/@cmbagent; HuggingFace:\n  https://huggingface.co/spaces/astropilot-ai/cmbagent; Cloud:\n  https://cmbagent.cloud", "summary": "We present a multi-agent system for automation of scientific research tasks,\ncmbagent. The system is formed by about 30 Large Language Model (LLM) agents\nand implements a Planning & Control strategy to orchestrate the agentic\nworkflow, with no human-in-the-loop at any point. Each agent specializes in a\ndifferent task (performing retrieval on scientific papers and codebases,\nwriting code, interpreting results, critiquing the output of other agents) and\nthe system is able to execute code locally. We successfully apply cmbagent to\ncarry out a PhD level cosmology task (the measurement of cosmological\nparameters using supernova data) and evaluate its performance on two benchmark\nsets, finding superior performance over state-of-the-art LLMs. The source code\nis available on GitHub, demonstration videos are also available, and the system\nis deployed on HuggingFace and will be available on the cloud.", "AI": {"tldr": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edfcmbagent\u901a\u8fc730\u4e2aLLM\u4ee3\u7406\u5b9e\u73b0\u79d1\u7814\u4efb\u52a1\u81ea\u52a8\u5316\uff0c\u91c7\u7528\u89c4\u5212\u4e0e\u63a7\u5236\u7b56\u7565\uff0c\u6210\u529f\u5b8c\u6210\u5b87\u5b99\u5b66\u535a\u58eb\u7ea7\u4efb\u52a1\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709LLM\u3002", "motivation": "\u89e3\u51b3\u79d1\u7814\u4efb\u52a1\u81ea\u52a8\u5316\u9700\u6c42\uff0c\u51cf\u5c11\u4eba\u5de5\u5e72\u9884\uff0c\u63d0\u9ad8\u6548\u7387\u3002", "method": "\u753130\u4e2aLLM\u4ee3\u7406\u7ec4\u6210\uff0c\u5404\u53f8\u5176\u804c\uff0c\u91c7\u7528\u89c4\u5212\u4e0e\u63a7\u5236\u7b56\u7565\uff0c\u652f\u6301\u672c\u5730\u4ee3\u7801\u6267\u884c\u3002", "result": "\u5728\u5b87\u5b99\u5b66\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u6027\u80fd\u8d85\u8d8a\u73b0\u6709LLM\uff0c\u4ee3\u7801\u5f00\u6e90\u5e76\u90e8\u7f72\u4e8eHuggingFace\u548c\u4e91\u7aef\u3002", "conclusion": "cmbagent\u5c55\u793a\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728\u79d1\u7814\u81ea\u52a8\u5316\u4e2d\u7684\u6f5c\u529b\uff0c\u672a\u6765\u53ef\u6269\u5c55\u81f3\u66f4\u591a\u9886\u57df\u3002"}}
{"id": "2507.07376", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.07376", "abs": "https://arxiv.org/abs/2507.07376", "authors": ["Hengrui Liu", "Yi Feng", "Qilong Zhang"], "title": "PILOC: A Pheromone Inverse Guidance Mechanism and Local-Communication Framework for Dynamic Target Search of Multi-Agent in Unknown Environments", "comment": null, "summary": "Multi-Agent Search and Rescue (MASAR) plays a vital role in disaster\nresponse, exploration, and reconnaissance. However, dynamic and unknown\nenvironments pose significant challenges due to target unpredictability and\nenvironmental uncertainty. To tackle these issues, we propose PILOC, a\nframework that operates without global prior knowledge, leveraging local\nperception and communication. It introduces a pheromone inverse guidance\nmechanism to enable efficient coordination and dynamic target localization.\nPILOC promotes decentralized cooperation through local communication,\nsignificantly reducing reliance on global channels. Unlike conventional\nheuristics, the pheromone mechanism is embedded into the observation space of\nDeep Reinforcement Learning (DRL), supporting indirect agent coordination based\non environmental cues. We further integrate this strategy into a DRL-based\nmulti-agent architecture and conduct extensive experiments. Results show that\ncombining local communication with pheromone-based guidance significantly\nboosts search efficiency, adaptability, and system robustness. Compared to\nexisting methods, PILOC performs better under dynamic and\ncommunication-constrained scenarios, offering promising directions for future\nMASAR applications.", "AI": {"tldr": "PILOC\u6846\u67b6\u901a\u8fc7\u5c40\u90e8\u611f\u77e5\u548c\u901a\u4fe1\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u641c\u7d22\u6551\u63f4\u4e2d\u7684\u52a8\u6001\u548c\u672a\u77e5\u73af\u5883\u95ee\u9898\uff0c\u7ed3\u5408\u4fe1\u606f\u7d20\u673a\u5236\u548c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u63d0\u5347\u6548\u7387\u3002", "motivation": "\u52a8\u6001\u548c\u672a\u77e5\u73af\u5883\u4e2d\u7684\u76ee\u6807\u4e0d\u53ef\u9884\u6d4b\u6027\u548c\u73af\u5883\u4e0d\u786e\u5b9a\u6027\u5bf9\u591a\u667a\u80fd\u4f53\u641c\u7d22\u6551\u63f4\u6784\u6210\u6311\u6218\uff0c\u9700\u8981\u65e0\u9700\u5168\u5c40\u5148\u9a8c\u77e5\u8bc6\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faPILOC\u6846\u67b6\uff0c\u5229\u7528\u5c40\u90e8\u611f\u77e5\u548c\u901a\u4fe1\uff0c\u5f15\u5165\u4fe1\u606f\u7d20\u9006\u5411\u5f15\u5bfc\u673a\u5236\uff0c\u7ed3\u5408\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u95f4\u63a5\u534f\u8c03\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cPILOC\u5728\u52a8\u6001\u548c\u901a\u4fe1\u53d7\u9650\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u63d0\u5347\u641c\u7d22\u6548\u7387\u548c\u7cfb\u7edf\u9c81\u68d2\u6027\u3002", "conclusion": "PILOC\u4e3a\u672a\u6765\u591a\u667a\u80fd\u4f53\u641c\u7d22\u6551\u63f4\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u81ea\u9002\u5e94\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.07302", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.07302", "abs": "https://arxiv.org/abs/2507.07302", "authors": ["Ashish Kumar"], "title": "Application of LLMs to Multi-Robot Path Planning and Task Allocation", "comment": null, "summary": "Efficient exploration is a well known problem in deep reinforcement learning\nand this problem is exacerbated in multi-agent reinforcement learning due the\nintrinsic complexities of such algorithms. There are several approaches to\nefficiently explore an environment to learn to solve tasks by multi-agent\noperating in that environment, of which, the idea of expert exploration is\ninvestigated in this work. More specifically, this work investigates the\napplication of large-language models as expert planners for efficient\nexploration in planning based tasks for multiple agents.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u4e13\u5bb6\u89c4\u5212\u5668\u4ee5\u5b9e\u73b0\u9ad8\u6548\u63a2\u7d22\u7684\u65b9\u6cd5\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u63a2\u7d22\u95ee\u9898\u56e0\u5176\u590d\u6742\u6027\u800c\u66f4\u5177\u6311\u6218\u6027\uff0c\u9700\u8981\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u4efb\u52a1\u3002", "method": "\u7814\u7a76\u91c7\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u4e13\u5bb6\u89c4\u5212\u5668\uff0c\u7528\u4e8e\u591a\u667a\u80fd\u4f53\u5728\u89c4\u5212\u4efb\u52a1\u4e2d\u7684\u9ad8\u6548\u63a2\u7d22\u3002", "result": "\u672a\u660e\u786e\u63d0\u53ca\u5177\u4f53\u5b9e\u9a8c\u7ed3\u679c\uff0c\u4f46\u63a2\u8ba8\u4e86\u65b9\u6cd5\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u591a\u667a\u80fd\u4f53\u89c4\u5212\u4efb\u52a1\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u53ef\u4f5c\u4e3a\u9ad8\u6548\u63a2\u7d22\u7684\u5de5\u5177\u3002"}}
{"id": "2507.07444", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.07444", "abs": "https://arxiv.org/abs/2507.07444", "authors": ["Korbinian Moller", "Rafael Neher", "Marvin Seegert", "Johannes Betz"], "title": "Towards Safe Autonomous Driving: A Real-Time Safeguarding Concept for Motion Planning Algorithms", "comment": "7 pages, submitted to the IEEE ICVES 2025, Coventry, UK", "summary": "Ensuring the functional safety of motion planning modules in autonomous\nvehicles remains a critical challenge, especially when dealing with complex or\nlearning-based software. Online verification has emerged as a promising\napproach to monitor such systems at runtime, yet its integration into embedded\nreal-time environments remains limited. This work presents a safeguarding\nconcept for motion planning that extends prior approaches by introducing a time\nsafeguard. While existing methods focus on geometric and dynamic feasibility,\nour approach additionally monitors the temporal consistency of planning outputs\nto ensure timely system response. A prototypical implementation on a real-time\noperating system evaluates trajectory candidates using constraint-based\nfeasibility checks and cost-based plausibility metrics. Preliminary results\nshow that the safeguarding module operates within real-time bounds and\neffectively detects unsafe trajectories. However, the full integration of the\ntime safeguard logic and fallback strategies is ongoing. This study contributes\na modular and extensible framework for runtime trajectory verification and\nhighlights key aspects for deployment on automotive-grade hardware. Future work\nincludes completing the safeguarding logic and validating its effectiveness\nthrough hardware-in-the-loop simulations and vehicle-based testing. The code is\navailable at: https://github.com/TUM-AVS/motion-planning-supervisor", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u8fd0\u52a8\u89c4\u5212\u6a21\u5757\u7684\u5b89\u5168\u4fdd\u969c\u6982\u5ff5\uff0c\u901a\u8fc7\u5f15\u5165\u65f6\u95f4\u76d1\u63a7\u6269\u5c55\u4e86\u73b0\u6709\u65b9\u6cd5\uff0c\u786e\u4fdd\u7cfb\u7edf\u54cd\u5e94\u7684\u53ca\u65f6\u6027\u3002", "motivation": "\u89e3\u51b3\u590d\u6742\u6216\u57fa\u4e8e\u5b66\u4e60\u7684\u8f6f\u4ef6\u4e2d\u8fd0\u52a8\u89c4\u5212\u529f\u80fd\u5b89\u5168\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u5d4c\u5165\u5f0f\u5b9e\u65f6\u73af\u5883\u4e2d\u7684\u5728\u7ebf\u9a8c\u8bc1\u95ee\u9898\u3002", "method": "\u5728\u5b9e\u65f6\u64cd\u4f5c\u7cfb\u7edf\u4e0a\u5b9e\u73b0\u539f\u578b\uff0c\u901a\u8fc7\u7ea6\u675f\u53ef\u884c\u6027\u68c0\u67e5\u548c\u57fa\u4e8e\u6210\u672c\u7684\u5408\u7406\u6027\u8bc4\u4f30\u8f68\u8ff9\u5019\u9009\u3002", "result": "\u521d\u6b65\u7ed3\u679c\u8868\u660e\uff0c\u5b89\u5168\u4fdd\u969c\u6a21\u5757\u5728\u5b9e\u65f6\u8303\u56f4\u5185\u8fd0\u884c\uff0c\u5e76\u80fd\u6709\u6548\u68c0\u6d4b\u4e0d\u5b89\u5168\u8f68\u8ff9\u3002", "conclusion": "\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6a21\u5757\u5316\u548c\u53ef\u6269\u5c55\u7684\u8fd0\u884c\u65f6\u8f68\u8ff9\u9a8c\u8bc1\u6846\u67b6\uff0c\u672a\u6765\u5de5\u4f5c\u5305\u62ec\u5b8c\u5584\u903b\u8f91\u5e76\u901a\u8fc7\u786c\u4ef6\u5728\u73af\u548c\u8f66\u8f86\u6d4b\u8bd5\u9a8c\u8bc1\u3002"}}
{"id": "2507.07306", "categories": ["cs.AI", "cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.07306", "abs": "https://arxiv.org/abs/2507.07306", "authors": ["Yichen Lu", "Wei Dai", "Jiaen Liu", "Ching Wing Kwok", "Zongheng Wu", "Xudong Xiao", "Ao Sun", "Sheng Fu", "Jianyuan Zhan", "Yian Wang", "Takatomo Saito", "Sicheng Lai"], "title": "ViDove: A Translation Agent System with Multimodal Context and Memory-Augmented Reasoning", "comment": null, "summary": "LLM-based translation agents have achieved highly human-like translation\nresults and are capable of handling longer and more complex contexts with\ngreater efficiency. However, they are typically limited to text-only inputs. In\nthis paper, we introduce ViDove, a translation agent system designed for\nmultimodal input. Inspired by the workflow of human translators, ViDove\nleverages visual and contextual background information to enhance the\ntranslation process. Additionally, we integrate a multimodal memory system and\nlong-short term memory modules enriched with domain-specific knowledge,\nenabling the agent to perform more accurately and adaptively in real-world\nscenarios. As a result, ViDove achieves significantly higher translation\nquality in both subtitle generation and general translation tasks, with a 28%\nimprovement in BLEU scores and a 15% improvement in SubER compared to previous\nstate-of-the-art baselines. Moreover, we introduce DoveBench, a new benchmark\nfor long-form automatic video subtitling and translation, featuring 17 hours of\nhigh-quality, human-annotated data. Our code is available here:\nhttps://github.com/pigeonai-org/ViDove", "AI": {"tldr": "ViDove\u662f\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u8f93\u5165\u7684\u7ffb\u8bd1\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\u63d0\u5347\u7ffb\u8bd1\u8d28\u91cf\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u73b0\u6709LLM\u7ffb\u8bd1\u4ee3\u7406\u4ec5\u9650\u4e8e\u6587\u672c\u8f93\u5165\uff0c\u65e0\u6cd5\u5229\u7528\u591a\u6a21\u6001\u4fe1\u606f\uff0cViDove\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "ViDove\u91c7\u7528\u591a\u6a21\u6001\u8bb0\u5fc6\u7cfb\u7edf\u548c\u957f\u77ed\u65f6\u8bb0\u5fc6\u6a21\u5757\uff0c\u7ed3\u5408\u9886\u57df\u77e5\u8bc6\uff0c\u4f18\u5316\u7ffb\u8bd1\u8fc7\u7a0b\u3002", "result": "ViDove\u5728\u5b57\u5e55\u751f\u6210\u548c\u7ffb\u8bd1\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0cBLEU\u5206\u6570\u63d0\u534728%\uff0cSubER\u63d0\u534715%\u3002", "conclusion": "ViDove\u901a\u8fc7\u591a\u6a21\u6001\u8f93\u5165\u663e\u8457\u63d0\u5347\u7ffb\u8bd1\u8d28\u91cf\uff0c\u5e76\u63a8\u51fa\u65b0\u57fa\u51c6DoveBench\u3002"}}
{"id": "2507.07467", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.07467", "abs": "https://arxiv.org/abs/2507.07467", "authors": ["Juyeop Han", "Lukas Lao Beyer", "Guilherme V. Cavalheiro", "Sertac Karaman"], "title": "SCREP: Scene Coordinate Regression and Evidential Learning-based Perception-Aware Trajectory Generation", "comment": "8 pages, 7 figures, 3 tables", "summary": "Autonomous flight in GPS denied indoor spaces requires trajectories that keep\nvisual localization error tightly bounded across varied missions. Whereas\nvisual inertial odometry (VIO) accumulates drift over time, scene coordinate\nregression (SCR) yields drift-free, high accuracy absolute pose estimation. We\npresent a perception-aware framework that couples an evidential learning-based\nSCR pose estimator with a receding horizon trajectory optimizer. The optimizer\nsteers the onboard camera toward pixels whose uncertainty predicts reliable\nscene coordinates, while a fixed-lag smoother fuses the low rate SCR stream\nwith high rate IMU data to close the perception control loop in real time. In\nsimulation, our planner reduces translation (rotation) mean error by 54% / 15%\n(40% / 31%) relative to yaw fixed and forward-looking baselines, respectively.\nMoreover, hardware in the loop experiment validates the feasibility of our\nproposed framework.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u573a\u666f\u5750\u6807\u56de\u5f52\uff08SCR\uff09\u548c\u8f68\u8ff9\u4f18\u5316\u7684\u611f\u77e5\u611f\u77e5\u6846\u67b6\uff0c\u7528\u4e8eGPS\u62d2\u7edd\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u98de\u884c\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5b9a\u4f4d\u8bef\u5dee\u3002", "motivation": "\u5728GPS\u62d2\u7edd\u7684\u5ba4\u5185\u73af\u5883\u4e2d\uff0c\u89c6\u89c9\u60ef\u6027\u91cc\u7a0b\u8ba1\uff08VIO\uff09\u4f1a\u968f\u65f6\u95f4\u7d2f\u79ef\u6f02\u79fb\uff0c\u800cSCR\u80fd\u63d0\u4f9b\u65e0\u6f02\u79fb\u7684\u9ad8\u7cbe\u5ea6\u7edd\u5bf9\u59ff\u6001\u4f30\u8ba1\u3002", "method": "\u6846\u67b6\u7ed3\u5408\u4e86\u57fa\u4e8e\u8bc1\u636e\u5b66\u4e60\u7684SCR\u59ff\u6001\u4f30\u8ba1\u5668\u548c\u540e\u9000\u6c34\u5e73\u8f68\u8ff9\u4f18\u5316\u5668\uff0c\u4f18\u5316\u5668\u5f15\u5bfc\u76f8\u673a\u671d\u5411\u4e0d\u786e\u5b9a\u6027\u4f4e\u7684\u50cf\u7d20\uff0c\u540c\u65f6\u56fa\u5b9a\u6ede\u540e\u5e73\u6ed1\u5668\u878d\u5408SCR\u548cIMU\u6570\u636e\u3002", "result": "\u4eff\u771f\u4e2d\uff0c\u89c4\u5212\u5668\u5c06\u5e73\u79fb\uff08\u65cb\u8f6c\uff09\u5e73\u5747\u8bef\u5dee\u5206\u522b\u964d\u4f4e\u4e8654%/15%\uff0840%/31%\uff09\uff0c\u786c\u4ef6\u5728\u73af\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u51cf\u5c11\u4e86\u89c6\u89c9\u5b9a\u4f4d\u8bef\u5dee\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u611f\u77e5\u63a7\u5236\u95ed\u73af\u3002"}}
{"id": "2507.07341", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.07341", "abs": "https://arxiv.org/abs/2507.07341", "authors": ["Sarah Ball", "Greg Gluch", "Shafi Goldwasser", "Frauke Kreuter", "Omer Reingold", "Guy N. Rothblum"], "title": "On the Impossibility of Separating Intelligence from Judgment: The Computational Intractability of Filtering for AI Alignment", "comment": null, "summary": "With the increased deployment of large language models (LLMs), one concern is\ntheir potential misuse for generating harmful content. Our work studies the\nalignment challenge, with a focus on filters to prevent the generation of\nunsafe information. Two natural points of intervention are the filtering of the\ninput prompt before it reaches the model, and filtering the output after\ngeneration. Our main results demonstrate computational challenges in filtering\nboth prompts and outputs. First, we show that there exist LLMs for which there\nare no efficient prompt filters: adversarial prompts that elicit harmful\nbehavior can be easily constructed, which are computationally indistinguishable\nfrom benign prompts for any efficient filter. Our second main result identifies\na natural setting in which output filtering is computationally intractable. All\nof our separation results are under cryptographic hardness assumptions. In\naddition to these core findings, we also formalize and study relaxed mitigation\napproaches, demonstrating further computational barriers. We conclude that\nsafety cannot be achieved by designing filters external to the LLM internals\n(architecture and weights); in particular, black-box access to the LLM will not\nsuffice. Based on our technical results, we argue that an aligned AI system's\nintelligence cannot be separated from its judgment.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u751f\u6210\u6709\u5bb3\u5185\u5bb9\u7684\u8fc7\u6ee4\u95ee\u9898\uff0c\u53d1\u73b0\u8f93\u5165\u548c\u8f93\u51fa\u8fc7\u6ee4\u5747\u5b58\u5728\u8ba1\u7b97\u6311\u6218\uff0c\u5e76\u6307\u51fa\u5916\u90e8\u8fc7\u6ee4\u5668\u65e0\u6cd5\u5b8c\u5168\u4fdd\u969c\u5b89\u5168\u6027\u3002", "motivation": "\u968f\u7740LLMs\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u9632\u6b62\u5176\u751f\u6210\u6709\u5bb3\u5185\u5bb9\u6210\u4e3a\u91cd\u8981\u95ee\u9898\u3002\u8bba\u6587\u65e8\u5728\u63a2\u8ba8\u8fc7\u6ee4\u6280\u672f\u7684\u6709\u6548\u6027\u53ca\u5176\u8ba1\u7b97\u9650\u5236\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\uff0c\u7814\u7a76\u8f93\u5165\u63d0\u793a\uff08prompt\uff09\u548c\u8f93\u51fa\u8fc7\u6ee4\u7684\u8ba1\u7b97\u590d\u6742\u6027\uff0c\u5e76\u57fa\u4e8e\u5bc6\u7801\u5b66\u5047\u8bbe\u8bc1\u660e\u5176\u4e0d\u53ef\u884c\u6027\u3002", "result": "\u53d1\u73b0\u5bf9\u6297\u6027\u63d0\u793a\u53ef\u7ed5\u8fc7\u9ad8\u6548\u8fc7\u6ee4\u5668\uff0c\u4e14\u8f93\u51fa\u8fc7\u6ee4\u5728\u7279\u5b9a\u60c5\u51b5\u4e0b\u8ba1\u7b97\u4e0d\u53ef\u884c\u3002\u5916\u90e8\u8fc7\u6ee4\u5668\u65e0\u6cd5\u786e\u4fdd\u5b89\u5168\u6027\u3002", "conclusion": "\u5b89\u5168\u6027\u9700\u5185\u7f6e\u4e8eLLMs\u7684\u8bbe\u8ba1\u4e2d\uff0c\u9ed1\u76d2\u8bbf\u95ee\u4e0d\u8db3\uff0c\u667a\u80fd\u4e0e\u5224\u65ad\u4e0d\u53ef\u5206\u5272\u3002"}}
{"id": "2507.07661", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.07661", "abs": "https://arxiv.org/abs/2507.07661", "authors": ["Daria Trinitatova", "Dzmitry Tsetserukou"], "title": "FiDTouch: A 3D Wearable Haptic Display for the Finger Pad", "comment": "Accepted to the IEEE World Haptics Conference 2025 (IEEE WHC 2025), 7\n  pages, 8 figures, 3 tables", "summary": "The applications of fingertip haptic devices have spread to various fields\nfrom revolutionizing virtual reality and medical training simulations to\nfacilitating remote robotic operations, proposing great potential for enhancing\nuser experiences, improving training outcomes, and new forms of interaction. In\nthis work, we present FiDTouch, a 3D wearable haptic device that delivers\ncutaneous stimuli to the finger pad, such as contact, pressure, encounter, skin\nstretch, and vibrotactile feedback. The application of a tiny inverted Delta\nrobot in the mechanism design allows providing accurate contact and fast\nchanging dynamic stimuli to the finger pad surface. The performance of the\ndeveloped display was evaluated in a two-stage user study of the perception of\nstatic spatial contact stimuli and skin stretch stimuli generated on the finger\npad. The proposed display, by providing users with precise touch and force\nstimuli, can enhance user immersion and efficiency in the fields of\nhuman-computer and human-robot interactions.", "AI": {"tldr": "FiDTouch\u662f\u4e00\u79cd3D\u53ef\u7a7f\u6234\u89e6\u89c9\u8bbe\u5907\uff0c\u901a\u8fc7\u5fae\u578b\u5012\u7f6eDelta\u673a\u5668\u4eba\u63d0\u4f9b\u7cbe\u786e\u7684\u63a5\u89e6\u548c\u52a8\u6001\u523a\u6fc0\uff0c\u63d0\u5347\u4eba\u673a\u4ea4\u4e92\u4f53\u9a8c\u3002", "motivation": "\u6307\u5c16\u89e6\u89c9\u8bbe\u5907\u5728\u865a\u62df\u73b0\u5b9e\u3001\u533b\u7597\u57f9\u8bad\u548c\u8fdc\u7a0b\u673a\u5668\u4eba\u64cd\u4f5c\u7b49\u9886\u57df\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\uff0c\u4f46\u73b0\u6709\u8bbe\u5907\u96be\u4ee5\u63d0\u4f9b\u7cbe\u786e\u7684\u52a8\u6001\u89e6\u89c9\u53cd\u9988\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8e\u5fae\u578b\u5012\u7f6eDelta\u673a\u5668\u4eba\u76843D\u53ef\u7a7f\u6234\u89e6\u89c9\u8bbe\u5907FiDTouch\uff0c\u53ef\u63d0\u4f9b\u63a5\u89e6\u3001\u538b\u529b\u3001\u76ae\u80a4\u62c9\u4f38\u548c\u632f\u52a8\u53cd\u9988\u3002\u901a\u8fc7\u4e24\u9636\u6bb5\u7528\u6237\u7814\u7a76\u8bc4\u4f30\u5176\u6027\u80fd\u3002", "result": "FiDTouch\u80fd\u591f\u7cbe\u786e\u63d0\u4f9b\u9759\u6001\u7a7a\u95f4\u63a5\u89e6\u548c\u76ae\u80a4\u62c9\u4f38\u523a\u6fc0\uff0c\u663e\u8457\u63d0\u5347\u7528\u6237\u6c89\u6d78\u611f\u548c\u4ea4\u4e92\u6548\u7387\u3002", "conclusion": "FiDTouch\u901a\u8fc7\u7cbe\u786e\u89e6\u89c9\u53cd\u9988\uff0c\u4e3a\u4eba\u673a\u4ea4\u4e92\u548c\u673a\u5668\u4eba\u64cd\u4f5c\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.07355", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.07355", "abs": "https://arxiv.org/abs/2507.07355", "authors": ["Haoyue Bai", "Haoyu Wang", "Nanxu Gong", "Xinyuan Wang", "Wangyang Ying", "Haifeng Chen", "Yanjie Fu"], "title": "Supply Chain Optimization via Generative Simulation and Iterative Decision Policies", "comment": null, "summary": "High responsiveness and economic efficiency are critical objectives in supply\nchain transportation, both of which are influenced by strategic decisions on\nshipping mode. An integrated framework combining an efficient simulator with an\nintelligent decision-making algorithm can provide an observable, low-risk\nenvironment for transportation strategy design. An ideal simulation-decision\nframework must (1) generalize effectively across various settings, (2) reflect\nfine-grained transportation dynamics, (3) integrate historical experience with\npredictive insights, and (4) maintain tight integration between simulation\nfeedback and policy refinement. We propose Sim-to-Dec framework to satisfy\nthese requirements. Specifically, Sim-to-Dec consists of a generative\nsimulation module, which leverages autoregressive modeling to simulate\ncontinuous state changes, reducing dependence on handcrafted domain-specific\nrules and enhancing robustness against data fluctuations; and a history-future\ndual-aware decision model, refined iteratively through end-to-end optimization\nwith simulator interactions. Extensive experiments conducted on three\nreal-world datasets demonstrate that Sim-to-Dec significantly improves timely\ndelivery rates and profit.", "AI": {"tldr": "Sim-to-Dec\u6846\u67b6\u7ed3\u5408\u751f\u6210\u6a21\u62df\u6a21\u5757\u548c\u53cc\u611f\u77e5\u51b3\u7b56\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4f9b\u5e94\u94fe\u8fd0\u8f93\u7684\u53ca\u65f6\u4ea4\u4ed8\u7387\u548c\u5229\u6da6\u3002", "motivation": "\u4f9b\u5e94\u94fe\u8fd0\u8f93\u4e2d\u7684\u9ad8\u54cd\u5e94\u6027\u548c\u7ecf\u6d4e\u6548\u7387\u53d7\u8fd0\u8f93\u6a21\u5f0f\u6218\u7565\u51b3\u7b56\u5f71\u54cd\uff0c\u9700\u4e00\u79cd\u53ef\u89c2\u5bdf\u3001\u4f4e\u98ce\u9669\u7684\u7b56\u7565\u8bbe\u8ba1\u73af\u5883\u3002", "method": "\u63d0\u51faSim-to-Dec\u6846\u67b6\uff0c\u5305\u62ec\u751f\u6210\u6a21\u62df\u6a21\u5757\uff08\u5229\u7528\u81ea\u56de\u5f52\u5efa\u6a21\uff09\u548c\u53cc\u611f\u77e5\u51b3\u7b56\u6a21\u578b\uff08\u901a\u8fc7\u7aef\u5230\u7aef\u4f18\u5316\u8fed\u4ee3\u6539\u8fdb\uff09\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSim-to-Dec\u663e\u8457\u63d0\u9ad8\u4e86\u53ca\u65f6\u4ea4\u4ed8\u7387\u548c\u5229\u6da6\u3002", "conclusion": "Sim-to-Dec\u6ee1\u8db3\u901a\u7528\u6027\u3001\u7ec6\u7c92\u5ea6\u52a8\u6001\u3001\u5386\u53f2\u4e0e\u9884\u6d4b\u7ed3\u5408\u53ca\u6a21\u62df\u53cd\u9988\u4e0e\u7b56\u7565\u4f18\u5316\u7684\u7d27\u5bc6\u96c6\u6210\u8981\u6c42\u3002"}}
{"id": "2507.07714", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07714", "abs": "https://arxiv.org/abs/2507.07714", "authors": ["Julio Garrido", "Javier Vales", "Diego Silva-Mu\u00f1iz", "Enrique Riveiro", "Pablo L\u00f3pez-Matencio", "Josu\u00e9 Rivera-Andrade"], "title": "Adaptive Gaussian Mixture Models-based Anomaly Detection for under-constrained Cable-Driven Parallel Robots", "comment": "14 pages, 8 figures, 1 table, to be submitted to Advanced Intelligent\n  Systems", "summary": "Cable-Driven Parallel Robots (CDPRs) are increasingly used for load\nmanipulation tasks involving predefined toolpaths with intermediate stops. At\neach stop, where the platform maintains a fixed pose and the motors keep the\ncables under tension, the system must evaluate whether it is safe to proceed by\ndetecting anomalies that could compromise performance (e.g., wind gusts or\ncable impacts). This paper investigates whether anomalies can be detected using\nonly motor torque data, without additional sensors. It introduces an adaptive,\nunsupervised outlier detection algorithm based on Gaussian Mixture Models\n(GMMs) to identify anomalies from torque signals. The method starts with a\nbrief calibration period, just a few seconds, during which a GMM is fit on\nknown anomaly-free data. Real-time torque measurements are then evaluated using\nMahalanobis distance from the GMM, with statistically derived thresholds\ntriggering anomaly flags. Model parameters are periodically updated using the\nlatest segments identified as anomaly-free to adapt to changing conditions.\nValidation includes 14 long-duration test sessions simulating varied wind\nintensities. The proposed method achieves a 100% true positive rate and 95.4%\naverage true negative rate, with 1-second detection latency. Comparative\nevaluation against power threshold and non-adaptive GMM methods indicates\nhigher robustness to drift and environmental variation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ad8\u65af\u6df7\u5408\u6a21\u578b\uff08GMM\uff09\u7684\u81ea\u9002\u5e94\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\u7b97\u6cd5\uff0c\u4ec5\u901a\u8fc7\u7535\u673a\u626d\u77e9\u6570\u636e\u68c0\u6d4b\u7535\u7f06\u9a71\u52a8\u5e76\u8054\u673a\u5668\u4eba\uff08CDPRs\uff09\u7684\u5f02\u5e38\u3002", "motivation": "CDPRs\u5728\u8d1f\u8f7d\u64cd\u7eb5\u4efb\u52a1\u4e2d\u9700\u68c0\u6d4b\u5f02\u5e38\u4ee5\u786e\u4fdd\u5b89\u5168\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u989d\u5916\u4f20\u611f\u5668\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u4ec5\u7528\u626d\u77e9\u6570\u636e\u5b9e\u73b0\u9ad8\u6548\u5f02\u5e38\u68c0\u6d4b\u7684\u53ef\u80fd\u6027\u3002", "method": "\u91c7\u7528GMM\u62df\u5408\u65e0\u5f02\u5e38\u6570\u636e\uff0c\u901a\u8fc7\u9a6c\u6c0f\u8ddd\u79bb\u5b9e\u65f6\u8bc4\u4f30\u626d\u77e9\u4fe1\u53f7\uff0c\u52a8\u6001\u66f4\u65b0\u6a21\u578b\u53c2\u6570\u4ee5\u9002\u5e94\u73af\u5883\u53d8\u5316\u3002", "result": "\u572814\u6b21\u957f\u65f6\u95f4\u6d4b\u8bd5\u4e2d\uff0c\u65b9\u6cd5\u5b9e\u73b0\u4e86100%\u7684\u771f\u9633\u6027\u7387\u548c95.4%\u7684\u5e73\u5747\u771f\u9634\u6027\u7387\uff0c\u68c0\u6d4b\u5ef6\u8fdf\u4e3a1\u79d2\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u65e0\u989d\u5916\u4f20\u611f\u5668\u7684\u60c5\u51b5\u4e0b\uff0c\u8868\u73b0\u51fa\u5bf9\u6f02\u79fb\u548c\u73af\u5883\u53d8\u5316\u7684\u9ad8\u9c81\u68d2\u6027\uff0c\u4f18\u4e8e\u4f20\u7edf\u9608\u503c\u548c\u975e\u81ea\u9002\u5e94GMM\u65b9\u6cd5\u3002"}}
{"id": "2507.07426", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.07426", "abs": "https://arxiv.org/abs/2507.07426", "authors": ["Zerui Yang", "Yuwei Wan", "Yinqiao Li", "Yudai Matsuda", "Tong Xie", "Linqi Song"], "title": "DrugMCTS: a drug repurposing framework combining multi-agent, RAG and Monte Carlo Tree Search", "comment": null, "summary": "Recent advances in large language models have demonstrated considerable\npotential in scientific domains such as drug discovery. However, their\neffectiveness remains constrained when reasoning extends beyond the knowledge\nacquired during pretraining. Conventional approaches, such as fine-tuning or\nretrieval-augmented generation, face limitations in either imposing high\ncomputational overhead or failing to fully exploit structured scientific data.\nTo overcome these challenges, we propose DrugMCTS, a novel framework that\nsynergistically integrates RAG, multi-agent collaboration, and Monte Carlo Tree\nSearch for drug repurposing. The framework employs five specialized agents\ntasked with retrieving and analyzing molecular and protein information, thereby\nenabling structured and iterative reasoning. Without requiring domain-specific\nfine-tuning, DrugMCTS empowers Qwen2.5-7B-Instruct to outperform Deepseek-R1 by\nover 20\\%. Extensive experiments on the DrugBank and KIBA datasets demonstrate\nthat DrugMCTS achieves substantially higher recall and robustness compared to\nboth general-purpose LLMs and deep learning baselines. Our results highlight\nthe importance of structured reasoning, agent-based collaboration, and\nfeedback-driven search mechanisms in advancing LLM applications for drug\ndiscovery.", "AI": {"tldr": "DrugMCTS\u7ed3\u5408RAG\u3001\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u548c\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u9886\u57df\u5fae\u8c03\u7684\u836f\u7269\u91cd\u5b9a\u4f4d\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u79d1\u5b66\u9886\u57df\uff08\u5982\u836f\u7269\u53d1\u73b0\uff09\u7684\u6f5c\u529b\u53d7\u9650\u4e8e\u9884\u8bad\u7ec3\u77e5\u8bc6\u5916\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f20\u7edf\u65b9\u6cd5\u5b58\u5728\u8ba1\u7b97\u5f00\u9500\u5927\u6216\u672a\u80fd\u5145\u5206\u5229\u7528\u7ed3\u6784\u5316\u79d1\u5b66\u6570\u636e\u7684\u95ee\u9898\u3002", "method": "DrugMCTS\u6846\u67b6\u6574\u5408RAG\u3001\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u548c\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff0c\u901a\u8fc7\u4e94\u4e2a\u4e13\u4e1a\u667a\u80fd\u4f53\u68c0\u7d22\u548c\u5206\u6790\u5206\u5b50\u4e0e\u86cb\u767d\u8d28\u4fe1\u606f\uff0c\u5b9e\u73b0\u7ed3\u6784\u5316\u8fed\u4ee3\u63a8\u7406\u3002", "result": "\u5728DrugBank\u548cKIBA\u6570\u636e\u96c6\u4e0a\uff0cDrugMCTS\u7684\u53ec\u56de\u7387\u548c\u9c81\u68d2\u6027\u663e\u8457\u4f18\u4e8e\u901a\u7528LLM\u548c\u6df1\u5ea6\u5b66\u4e60\u57fa\u7ebf\uff0c\u6027\u80fd\u63d0\u5347\u8d85\u8fc720%\u3002", "conclusion": "\u7ed3\u6784\u5316\u63a8\u7406\u3001\u667a\u80fd\u4f53\u534f\u4f5c\u548c\u53cd\u9988\u9a71\u52a8\u641c\u7d22\u673a\u5236\u5bf9\u63a8\u8fdbLLM\u5728\u836f\u7269\u53d1\u73b0\u4e2d\u7684\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2507.07718", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.07718", "abs": "https://arxiv.org/abs/2507.07718", "authors": ["Alberto Rota", "Ke Fan", "Elena De Momi"], "title": "Implementation and Assessment of an Augmented Training Curriculum for Surgical Robotics", "comment": null, "summary": "The integration of high-level assistance algorithms in surgical robotics\ntraining curricula may be beneficial in establishing a more comprehensive and\nrobust skillset for aspiring surgeons, improving their clinical performance as\na consequence. This work presents the development and validation of a\nhaptic-enhanced Virtual Reality simulator for surgical robotics training,\nfeaturing 8 surgical tasks that the trainee can interact with thanks to the\nembedded physics engine. This virtual simulated environment is augmented by the\nintroduction of high-level haptic interfaces for robotic assistance that aim at\nre-directing the motion of the trainee's hands and wrists toward targets or\naway from obstacles, and providing a quantitative performance score after the\nexecution of each training exercise.An experimental study shows that the\nintroduction of enhanced robotic assistance into a surgical robotics training\ncurriculum improves performance during the training process and, crucially,\npromotes the transfer of the acquired skills to an unassisted surgical\nscenario, like the clinical one.", "AI": {"tldr": "\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u89e6\u89c9\u589e\u5f3a\u7684\u865a\u62df\u73b0\u5b9e\u624b\u672f\u673a\u5668\u4eba\u8bad\u7ec3\u6a21\u62df\u5668\uff0c\u901a\u8fc7\u5f15\u5165\u9ad8\u7ea7\u89e6\u89c9\u8f85\u52a9\u7b97\u6cd5\uff0c\u63d0\u5347\u5b66\u5458\u6280\u80fd\u5e76\u4fc3\u8fdb\u6280\u80fd\u5411\u65e0\u8f85\u52a9\u4e34\u5e8a\u573a\u666f\u7684\u8f6c\u79fb\u3002", "motivation": "\u901a\u8fc7\u6574\u5408\u9ad8\u7ea7\u8f85\u52a9\u7b97\u6cd5\u5230\u624b\u672f\u673a\u5668\u4eba\u8bad\u7ec3\u8bfe\u7a0b\u4e2d\uff0c\u5e2e\u52a9\u5b66\u5458\u5efa\u7acb\u66f4\u5168\u9762\u548c\u7a33\u5065\u7684\u6280\u80fd\uff0c\u4ece\u800c\u63d0\u5347\u4e34\u5e8a\u8868\u73b0\u3002", "method": "\u5f00\u53d1\u5e76\u9a8c\u8bc1\u4e86\u4e00\u79cd\u89e6\u89c9\u589e\u5f3a\u7684\u865a\u62df\u73b0\u5b9e\u6a21\u62df\u5668\uff0c\u5305\u542b8\u4e2a\u624b\u672f\u4efb\u52a1\uff0c\u901a\u8fc7\u7269\u7406\u5f15\u64ce\u548c\u9ad8\u7ea7\u89e6\u89c9\u63a5\u53e3\u63d0\u4f9b\u8fd0\u52a8\u5f15\u5bfc\u548c\u6027\u80fd\u8bc4\u5206\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5f15\u5165\u589e\u5f3a\u673a\u5668\u4eba\u8f85\u52a9\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u8868\u73b0\uff0c\u5e76\u4fc3\u8fdb\u4e86\u6280\u80fd\u5411\u65e0\u8f85\u52a9\u4e34\u5e8a\u573a\u666f\u7684\u8f6c\u79fb\u3002", "conclusion": "\u89e6\u89c9\u589e\u5f3a\u7684\u865a\u62df\u73b0\u5b9e\u6a21\u62df\u5668\u7ed3\u5408\u9ad8\u7ea7\u8f85\u52a9\u7b97\u6cd5\uff0c\u6709\u6548\u63d0\u5347\u4e86\u624b\u672f\u673a\u5668\u4eba\u8bad\u7ec3\u7684\u6548\u679c\u548c\u6280\u80fd\u8f6c\u79fb\u80fd\u529b\u3002"}}
{"id": "2507.07445", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.07445", "abs": "https://arxiv.org/abs/2507.07445", "authors": ["Weihao Tan", "Changjiu Jiang", "Yu Duan", "Mingcong Lei", "Jiageng Li", "Yitian Hong", "Xinrun Wang", "Bo An"], "title": "StarDojo: Benchmarking Open-Ended Behaviors of Agentic Multimodal LLMs in Production-Living Simulations with Stardew Valley", "comment": "Project website: https://weihaotan.github.io/StarDojo", "summary": "Autonomous agents navigating human society must master both production\nactivities and social interactions, yet existing benchmarks rarely evaluate\nthese skills simultaneously. To bridge this gap, we introduce StarDojo, a novel\nbenchmark based on Stardew Valley, designed to assess AI agents in open-ended\nproduction-living simulations. In StarDojo, agents are tasked to perform\nessential livelihood activities such as farming and crafting, while\nsimultaneously engaging in social interactions to establish relationships\nwithin a vibrant community. StarDojo features 1,000 meticulously curated tasks\nacross five key domains: farming, crafting, exploration, combat, and social\ninteractions. Additionally, we provide a compact subset of 100 representative\ntasks for efficient model evaluation. The benchmark offers a unified,\nuser-friendly interface that eliminates the need for keyboard and mouse\ncontrol, supports all major operating systems, and enables the parallel\nexecution of multiple environment instances, making it particularly well-suited\nfor evaluating the most capable foundation agents, powered by multimodal large\nlanguage models (MLLMs). Extensive evaluations of state-of-the-art MLLMs agents\ndemonstrate substantial limitations, with the best-performing model, GPT-4.1,\nachieving only a 12.7% success rate, primarily due to challenges in visual\nunderstanding, multimodal reasoning and low-level manipulation. As a\nuser-friendly environment and benchmark, StarDojo aims to facilitate further\nresearch towards robust, open-ended agents in complex production-living\nenvironments.", "AI": {"tldr": "StarDojo\u662f\u4e00\u4e2a\u57fa\u4e8eStardew Valley\u7684\u65b0\u578b\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30AI\u4ee3\u7406\u5728\u5f00\u653e\u5f0f\u751f\u4ea7\u751f\u6d3b\u6a21\u62df\u4e2d\u7684\u8868\u73b0\uff0c\u6db5\u76d6\u519c\u4e1a\u3001\u624b\u5de5\u827a\u3001\u63a2\u7d22\u3001\u6218\u6597\u548c\u793e\u4ea4\u4e92\u52a8\u7b49\u9886\u57df\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5f88\u5c11\u540c\u65f6\u8bc4\u4f30\u751f\u4ea7\u6d3b\u52a8\u548c\u793e\u4f1a\u4e92\u52a8\u80fd\u529b\uff0cStarDojo\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u8bbe\u8ba1\u4e861,000\u4e2a\u4efb\u52a1\u548c100\u4e2a\u4ee3\u8868\u6027\u5b50\u96c6\uff0c\u63d0\u4f9b\u7edf\u4e00\u63a5\u53e3\u652f\u6301\u591a\u73af\u5883\u5e76\u884c\u6267\u884c\uff0c\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u4ee3\u7406\u7684\u80fd\u529b\u3002", "result": "\u5f53\u524d\u6700\u5148\u8fdb\u7684MLLMs\u4ee3\u7406\u8868\u73b0\u6709\u9650\uff0cGPT-4.1\u6210\u529f\u7387\u4ec512.7%\uff0c\u4e3b\u8981\u56e0\u89c6\u89c9\u7406\u89e3\u3001\u591a\u6a21\u6001\u63a8\u7406\u548c\u4f4e\u7ea7\u64cd\u4f5c\u56f0\u96be\u3002", "conclusion": "StarDojo\u65e8\u5728\u63a8\u52a8\u590d\u6742\u751f\u4ea7\u751f\u6d3b\u73af\u5883\u4e2d\u7a33\u5065\u5f00\u653e\u5f0f\u4ee3\u7406\u7684\u7814\u7a76\u3002"}}
{"id": "2507.07724", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.07724", "abs": "https://arxiv.org/abs/2507.07724", "authors": ["Thiemen Siemensma", "Niels de Boer", "Bahar Haghighat"], "title": "Distributed Surface Inspection via Operational Modal Analysis by a Swarm of Miniaturized Vibration-Sensing Robots", "comment": null, "summary": "Robot swarms offer the potential to serve a variety of distributed sensing\napplications. An interesting real-world application that stands to benefit\nsignificantly from deployment of swarms is structural monitoring, where\ntraditional sensor networks face challenges in structural coverage due to their\nstatic nature. This paper investigates the deployment of a swarm of\nminiaturized vibration sensing robots to inspect and localize structural\ndamages on a surface section within a high-fidelity simulation environment. In\nparticular, we consider a 1 m x 1 m x 3 mm steel surface section and utilize\nfinite element analysis using Abaqus to obtain realistic structural vibration\ndata. The resulting vibration data is imported into the physics-based robotic\nsimulator Webots, where we simulate the dynamics of our surface inspecting\nrobot swarm. We employ (i) Gaussian process estimators to guide the robots'\nexploration as they collect vibration samples across the surface and (ii)\noperational modal analysis to detect structural damages by estimating and\ncomparing existing and intact structural vibration patterns. We analyze the\ninfluence of exploration radii on estimation uncertainty and assess the\neffectiveness of our method across 10 randomized scenarios, where the number,\nlocations, surface area, and depth of structural damages vary. Our simulation\nstudies validate the efficacy of our miniaturized robot swarm for\nvibration-based structural inspection.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5229\u7528\u5fae\u578b\u632f\u52a8\u4f20\u611f\u673a\u5668\u4eba\u7fa4\u4f53\u5728\u6a21\u62df\u73af\u5883\u4e2d\u68c0\u6d4b\u548c\u5b9a\u4f4d\u7ed3\u6784\u635f\u4f24\u7684\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u9759\u6001\u4f20\u611f\u5668\u7f51\u7edc\u5728\u7ed3\u6784\u8986\u76d6\u4e0a\u5b58\u5728\u5c40\u9650\u6027\uff0c\u800c\u673a\u5668\u4eba\u7fa4\u4f53\u80fd\u63d0\u4f9b\u5206\u5e03\u5f0f\u611f\u77e5\uff0c\u9002\u7528\u4e8e\u7ed3\u6784\u76d1\u6d4b\u3002", "method": "\u7ed3\u5408\u6709\u9650\u5143\u5206\u6790\uff08Abaqus\uff09\u548c\u673a\u5668\u4eba\u6a21\u62df\u5668\uff08Webots\uff09\uff0c\u4f7f\u7528\u9ad8\u65af\u8fc7\u7a0b\u4f30\u8ba1\u5668\u5f15\u5bfc\u673a\u5668\u4eba\u63a2\u7d22\uff0c\u5e76\u901a\u8fc7\u6a21\u6001\u5206\u6790\u68c0\u6d4b\u635f\u4f24\u3002", "result": "\u572810\u79cd\u968f\u673a\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5206\u6790\u4e86\u63a2\u7d22\u534a\u5f84\u5bf9\u4f30\u8ba1\u4e0d\u786e\u5b9a\u6027\u7684\u5f71\u54cd\u3002", "conclusion": "\u6a21\u62df\u7814\u7a76\u8bc1\u5b9e\u4e86\u5fae\u578b\u673a\u5668\u4eba\u7fa4\u4f53\u5728\u632f\u52a8\u7ed3\u6784\u68c0\u6d4b\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2507.07544", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07544", "abs": "https://arxiv.org/abs/2507.07544", "authors": ["Oliver Eberle", "Thomas McGee", "Hamza Giaffar", "Taylor Webb", "Ida Momennejad"], "title": "Position: We Need An Algorithmic Understanding of Generative AI", "comment": "Accepted at ICML 2025 as a Spotlight Position Paper", "summary": "What algorithms do LLMs actually learn and use to solve problems? Studies\naddressing this question are sparse, as research priorities are focused on\nimproving performance through scale, leaving a theoretical and empirical gap in\nunderstanding emergent algorithms. This position paper proposes AlgEval: a\nframework for systematic research into the algorithms that LLMs learn and use.\nAlgEval aims to uncover algorithmic primitives, reflected in latent\nrepresentations, attention, and inference-time compute, and their algorithmic\ncomposition to solve task-specific problems. We highlight potential\nmethodological paths and a case study toward this goal, focusing on emergent\nsearch algorithms. Our case study illustrates both the formation of top-down\nhypotheses about candidate algorithms, and bottom-up tests of these hypotheses\nvia circuit-level analysis of attention patterns and hidden states. The\nrigorous, systematic evaluation of how LLMs actually solve tasks provides an\nalternative to resource-intensive scaling, reorienting the field toward a\nprincipled understanding of underlying computations. Such algorithmic\nexplanations offer a pathway to human-understandable interpretability, enabling\ncomprehension of the model's internal reasoning performance measures. This can\nin turn lead to more sample-efficient methods for training and improving\nperformance, as well as novel architectures for end-to-end and multi-agent\nsystems.", "AI": {"tldr": "AlgEval\u662f\u4e00\u4e2a\u7814\u7a76\u6846\u67b6\uff0c\u65e8\u5728\u63ed\u793aLLMs\u5b66\u4e60\u548c\u4f7f\u7528\u7684\u7b97\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u6f5c\u5728\u8868\u793a\u3001\u6ce8\u610f\u529b\u548c\u63a8\u7406\u8ba1\u7b97\uff0c\u4ee5\u7406\u89e3\u5176\u4efb\u52a1\u89e3\u51b3\u65b9\u5f0f\u3002", "motivation": "\u5f53\u524d\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u901a\u8fc7\u89c4\u6a21\u63d0\u5347\u6027\u80fd\uff0c\u800c\u5bf9LLMs\u5b66\u4e60\u7684\u7b97\u6cd5\u7f3a\u4e4f\u7406\u8bba\u548c\u5b9e\u8bc1\u7406\u89e3\uff0cAlgEval\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "AlgEval\u7ed3\u5408\u81ea\u4e0a\u800c\u4e0b\u7684\u5047\u8bbe\u548c\u81ea\u4e0b\u800c\u4e0a\u7684\u7535\u8def\u7ea7\u5206\u6790\uff08\u5982\u6ce8\u610f\u529b\u6a21\u5f0f\u548c\u9690\u85cf\u72b6\u6001\uff09\uff0c\u7814\u7a76LLMs\u7684\u7b97\u6cd5\u3002", "result": "\u6848\u4f8b\u7814\u7a76\u8868\u660e\u4e86\u5019\u9009\u7b97\u6cd5\u7684\u5f62\u6210\u548c\u9a8c\u8bc1\uff0c\u4e3a\u7406\u89e3LLMs\u7684\u5185\u90e8\u8ba1\u7b97\u63d0\u4f9b\u4e86\u7cfb\u7edf\u65b9\u6cd5\u3002", "conclusion": "AlgEval\u4e3aLLMs\u7684\u7b97\u6cd5\u89e3\u91ca\u63d0\u4f9b\u4e86\u8def\u5f84\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\u548c\u8bbe\u8ba1\u65b0\u578b\u67b6\u6784\u3002"}}
{"id": "2507.07745", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.07745", "abs": "https://arxiv.org/abs/2507.07745", "authors": ["Eleni Konstantinidou", "Nikolaos Kounalakis", "Nikolaos Efstathopoulos", "Dimitrios Papageorgiou"], "title": "On the capabilities of LLMs for classifying and segmenting time series of fruit picking motions into primitive actions", "comment": "This paper is a Late Breaking Results report and it will be presented\n  through a poster at the 34th IEEE International Conference on Robot and Human\n  Interactive Communication (ROMAN), 2025 at Eindhoven, the Netherlands", "summary": "Despite their recent introduction to human society, Large Language Models\n(LLMs) have significantly affected the way we tackle mental challenges in our\neveryday lives. From optimizing our linguistic communication to assisting us in\nmaking important decisions, LLMs, such as ChatGPT, are notably reducing our\ncognitive load by gradually taking on an increasing share of our mental\nactivities. In the context of Learning by Demonstration (LbD), classifying and\nsegmenting complex motions into primitive actions, such as pushing, pulling,\ntwisting etc, is considered to be a key-step towards encoding a task. In this\nwork, we investigate the capabilities of LLMs to undertake this task,\nconsidering a finite set of predefined primitive actions found in fruit picking\noperations. By utilizing LLMs instead of simple supervised learning or analytic\nmethods, we aim at making the method easily applicable and deployable in a\nreal-life scenario. Three different fine-tuning approaches are investigated,\ncompared on datasets captured kinesthetically, using a UR10e robot, during a\nfruit-picking scenario.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5b66\u4e60\u548c\u6f14\u793a\uff08LbD\uff09\u4e2d\u5206\u7c7b\u548c\u5206\u5272\u590d\u6742\u52a8\u4f5c\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u6c34\u679c\u91c7\u6458\u4efb\u52a1\u4e2d\u3002", "motivation": "LLMs\u5982ChatGPT\u5df2\u663e\u8457\u5f71\u54cd\u4eba\u7c7b\u5904\u7406\u65e5\u5e38\u5fc3\u7406\u6311\u6218\u7684\u65b9\u5f0f\uff0c\u672c\u6587\u65e8\u5728\u63a2\u7d22\u5176\u5728LbD\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u4ee5\u51cf\u8f7b\u8ba4\u77e5\u8d1f\u62c5\u5e76\u63d0\u9ad8\u5b9e\u7528\u6027\u3002", "method": "\u7814\u7a76\u4e86\u4e09\u79cd\u4e0d\u540c\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u5e76\u5728\u4f7f\u7528UR10e\u673a\u5668\u4eba\u91c7\u96c6\u7684\u6c34\u679c\u91c7\u6458\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "result": "\u901a\u8fc7LLMs\u66ff\u4ee3\u4f20\u7edf\u76d1\u7763\u5b66\u4e60\u6216\u5206\u6790\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u65b9\u6cd5\u7684\u6613\u7528\u6027\u548c\u5b9e\u9645\u90e8\u7f72\u80fd\u529b\u3002", "conclusion": "LLMs\u5728LbD\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u80fd\u591f\u6709\u6548\u5206\u7c7b\u548c\u5206\u5272\u590d\u6742\u52a8\u4f5c\uff0c\u9002\u7528\u4e8e\u73b0\u5b9e\u573a\u666f\u3002"}}
{"id": "2507.07576", "categories": ["cs.AI", "cs.LG", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.07576", "abs": "https://arxiv.org/abs/2507.07576", "authors": ["Mohamed Siala", "Jordi Planes", "Joao Marques-Silva"], "title": "On Trustworthy Rule-Based Models and Explanations", "comment": null, "summary": "A task of interest in machine learning (ML) is that of ascribing explanations\nto the predictions made by ML models. Furthermore, in domains deemed high risk,\nthe rigor of explanations is paramount. Indeed, incorrect explanations can and\nwill mislead human decision makers. As a result, and even if interpretability\nis acknowledged as an elusive concept, so-called interpretable models are\nemployed ubiquitously in high-risk uses of ML and data mining (DM). This is the\ncase for rule-based ML models, which encompass decision trees, diagrams, sets\nand lists. This paper relates explanations with well-known undesired facets of\nrule-based ML models, which include negative overlap and several forms of\nredundancy. The paper develops algorithms for the analysis of these undesired\nfacets of rule-based systems, and concludes that well-known and widely used\ntools for learning rule-based ML models will induce rule sets that exhibit one\nor more negative facets.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u673a\u5668\u5b66\u4e60\u4e2d\u89e3\u91ca\u9884\u6d4b\u7684\u91cd\u8981\u6027\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u98ce\u9669\u9886\u57df\uff0c\u5e76\u5206\u6790\u4e86\u57fa\u4e8e\u89c4\u5219\u7684\u6a21\u578b\u4e2d\u7684\u8d1f\u9762\u7279\u5f81\uff08\u5982\u5197\u4f59\u548c\u91cd\u53e0\uff09\u3002", "motivation": "\u5728\u9ad8\u98ce\u9669\u9886\u57df\uff0c\u9519\u8bef\u7684\u89e3\u91ca\u53ef\u80fd\u8bef\u5bfc\u51b3\u7b56\u8005\uff0c\u56e0\u6b64\u9700\u8981\u4e25\u683c\u7684\u89e3\u91ca\u65b9\u6cd5\u3002\u57fa\u4e8e\u89c4\u5219\u7684\u6a21\u578b\u867d\u7136\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u5176\u8d1f\u9762\u7279\u5f81\u53ef\u80fd\u5f71\u54cd\u89e3\u91ca\u7684\u53ef\u9760\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u5206\u6790\u57fa\u4e8e\u89c4\u5219\u6a21\u578b\u4e2d\u8d1f\u9762\u7279\u5f81\u7684\u7b97\u6cd5\uff0c\u5305\u62ec\u8d1f\u91cd\u53e0\u548c\u5197\u4f59\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u4e8e\u89c4\u5219\u5b66\u4e60\u5de5\u5177\u751f\u6210\u7684\u89c4\u5219\u96c6\u5f80\u5f80\u8868\u73b0\u51fa\u4e00\u79cd\u6216\u591a\u79cd\u8d1f\u9762\u7279\u5f81\u3002", "conclusion": "\u57fa\u4e8e\u89c4\u5219\u7684\u6a21\u578b\u5728\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u9700\u8c28\u614e\u4f7f\u7528\uff0c\u4ee5\u907f\u514d\u8d1f\u9762\u7279\u5f81\u5bf9\u89e3\u91ca\u7684\u8bef\u5bfc\u3002"}}
{"id": "2507.07752", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.07752", "abs": "https://arxiv.org/abs/2507.07752", "authors": ["Thanh Nguyen Canh", "Bao Nguyen Quoc", "Haolan Zhang", "Bupesh Rethinam Veeraiah", "Xiem HoangVan", "Nak Young Chong"], "title": "IRAF-SLAM: An Illumination-Robust and Adaptive Feature-Culling Front-End for Visual SLAM in Challenging Environments", "comment": "In the European Conference on Mobile Robots 2025", "summary": "Robust Visual SLAM (vSLAM) is essential for autonomous systems operating in\nreal-world environments, where challenges such as dynamic objects, low texture,\nand critically, varying illumination conditions often degrade performance.\nExisting feature-based SLAM systems rely on fixed front-end parameters, making\nthem vulnerable to sudden lighting changes and unstable feature tracking. To\naddress these challenges, we propose ``IRAF-SLAM'', an Illumination-Robust and\nAdaptive Feature-Culling front-end designed to enhance vSLAM resilience in\ncomplex and challenging environments. Our approach introduces: (1) an image\nenhancement scheme to preprocess and adjust image quality under varying\nlighting conditions; (2) an adaptive feature extraction mechanism that\ndynamically adjusts detection sensitivity based on image entropy, pixel\nintensity, and gradient analysis; and (3) a feature culling strategy that\nfilters out unreliable feature points using density distribution analysis and a\nlighting impact factor. Comprehensive evaluations on the TUM-VI and European\nRobotics Challenge (EuRoC) datasets demonstrate that IRAF-SLAM significantly\nreduces tracking failures and achieves superior trajectory accuracy compared to\nstate-of-the-art vSLAM methods under adverse illumination conditions. These\nresults highlight the effectiveness of adaptive front-end strategies in\nimproving vSLAM robustness without incurring significant computational\noverhead. The implementation of IRAF-SLAM is publicly available at\nhttps://thanhnguyencanh. github.io/IRAF-SLAM/.", "AI": {"tldr": "IRAF-SLAM\u662f\u4e00\u79cd\u9488\u5bf9\u590d\u6742\u5149\u7167\u6761\u4ef6\u7684\u89c6\u89c9SLAM\u7cfb\u7edf\uff0c\u901a\u8fc7\u56fe\u50cf\u589e\u5f3a\u3001\u81ea\u9002\u5e94\u7279\u5f81\u63d0\u53d6\u548c\u7279\u5f81\u7b5b\u9009\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u52a8\u6001\u73af\u5883\u548c\u5149\u7167\u53d8\u5316\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709SLAM\u7cfb\u7edf\u5728\u5149\u7167\u53d8\u5316\u548c\u52a8\u6001\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9c81\u68d2\u7684\u524d\u7aef\u5904\u7406\u65b9\u6cd5\u6765\u63d0\u5347\u6027\u80fd\u3002", "method": "1. \u56fe\u50cf\u589e\u5f3a\u9884\u5904\u7406\uff1b2. \u57fa\u4e8e\u56fe\u50cf\u71b5\u3001\u50cf\u7d20\u5f3a\u5ea6\u548c\u68af\u5ea6\u7684\u81ea\u9002\u5e94\u7279\u5f81\u63d0\u53d6\uff1b3. \u901a\u8fc7\u5bc6\u5ea6\u5206\u5e03\u548c\u5149\u7167\u5f71\u54cd\u56e0\u5b50\u7b5b\u9009\u4e0d\u53ef\u9760\u7279\u5f81\u70b9\u3002", "result": "\u5728TUM-VI\u548cEuRoC\u6570\u636e\u96c6\u4e0a\uff0cIRAF-SLAM\u663e\u8457\u51cf\u5c11\u4e86\u8ddf\u8e2a\u5931\u8d25\uff0c\u8f68\u8ff9\u7cbe\u5ea6\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u81ea\u9002\u5e94\u524d\u7aef\u7b56\u7565\u80fd\u6709\u6548\u63d0\u5347SLAM\u9c81\u68d2\u6027\uff0c\u4e14\u8ba1\u7b97\u5f00\u9500\u4f4e\uff0c\u9002\u7528\u4e8e\u590d\u6742\u5149\u7167\u73af\u5883\u3002"}}
{"id": "2507.07595", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07595", "abs": "https://arxiv.org/abs/2507.07595", "authors": ["Zhixiang Su", "Di Wang", "Chunyan Miao"], "title": "Context Pooling: Query-specific Graph Pooling for Generic Inductive Link Prediction in Knowledge Graphs", "comment": null, "summary": "Recent investigations on the effectiveness of Graph Neural Network\n(GNN)-based models for link prediction in Knowledge Graphs (KGs) show that\nvanilla aggregation does not significantly impact the model performance. In\nthis paper, we introduce a novel method, named Context Pooling, to enhance\nGNN-based models' efficacy for link predictions in KGs. To our best of\nknowledge, Context Pooling is the first methodology that applies graph pooling\nin KGs. Additionally, Context Pooling is first-of-its-kind to enable the\ngeneration of query-specific graphs for inductive settings, where testing\nentities are unseen during training. Specifically, we devise two metrics,\nnamely neighborhood precision and neighborhood recall, to assess the neighbors'\nlogical relevance regarding the given queries, thereby enabling the subsequent\ncomprehensive identification of only the logically relevant neighbors for link\nprediction. Our method is generic and assessed by being applied to two\nstate-of-the-art (SOTA) models on three public transductive and inductive\ndatasets, achieving SOTA performance in 42 out of 48 settings.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aContext Pooling\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u57fa\u4e8eGNN\u7684\u77e5\u8bc6\u56fe\u8c31\u94fe\u63a5\u9884\u6d4b\u6027\u80fd\uff0c\u9996\u6b21\u5c06\u56fe\u6c60\u5316\u5e94\u7528\u4e8e\u77e5\u8bc6\u56fe\u8c31\uff0c\u5e76\u572842/48\u7684\u5b9e\u9a8c\u4e2d\u8fbe\u5230SOTA\u6548\u679c\u3002", "motivation": "\u73b0\u6709GNN\u6a21\u578b\u5728\u77e5\u8bc6\u56fe\u8c31\u94fe\u63a5\u9884\u6d4b\u4e2d\u8868\u73b0\u5e73\u5e73\uff0c\u5c24\u5176\u662fvanilla\u805a\u5408\u65b9\u6cd5\u6548\u679c\u6709\u9650\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faContext Pooling\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bbe\u8ba1\u90bb\u57df\u7cbe\u5ea6\u548c\u90bb\u57df\u53ec\u56de\u7387\u6307\u6807\uff0c\u7b5b\u9009\u903b\u8f91\u76f8\u5173\u90bb\u5c45\uff0c\u751f\u6210\u67e5\u8be2\u7279\u5b9a\u56fe\u3002", "result": "\u5728\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u5e94\u7528\u4e8e\u4e24\u4e2aSOTA\u6a21\u578b\uff0c42/48\u60c5\u51b5\u4e0b\u8fbe\u5230SOTA\u6027\u80fd\u3002", "conclusion": "Context Pooling\u663e\u8457\u63d0\u5347\u4e86GNN\u5728\u77e5\u8bc6\u56fe\u8c31\u94fe\u63a5\u9884\u6d4b\u4e2d\u7684\u8868\u73b0\uff0c\u5c24\u5176\u5728\u672a\u89c1\u5b9e\u4f53\u7684\u5f52\u7eb3\u8bbe\u7f6e\u4e2d\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2507.07794", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.07794", "abs": "https://arxiv.org/abs/2507.07794", "authors": ["Zhe Han", "Huanyu Tian", "Tom Vercauteren", "Da Liu", "Changsheng Li", "Xingguang Duan"], "title": "Collaborative Human-Robot Surgery for Mandibular Angle Split Osteotomy: Optical Tracking based Approach", "comment": null, "summary": "Mandibular Angle Split Osteotomy (MASO) is a significant procedure in oral\nand maxillofacial surgery. Despite advances in technique and instrumentation,\nits success still relies heavily on the surgeon's experience. In this work, a\nhuman-robot collaborative system is proposed to perform MASO according to a\npreoperative plan and under guidance of a surgeon. A task decomposition\nmethodology is used to divide the collaborative surgical procedure into three\nsubtasks: (1) positional control and (2) orientation control, both led by the\nrobot for precise alignment; and (3) force-control, managed by surgeon to\nensure safety. Additionally, to achieve patient tracking without the need for a\nskull clamp, an optical tracking system (OTS) is utilized. Movement of the\npatient mandibular is measured with an optical-based tracker mounted on a\ndental occlusal splint. A registration method and Robot-OTS calibration method\nare introduced to achieve reliable navigation within our framework. The\nexperiments of drilling were conducted on the realistic phantom model, which\ndemonstrated that the average error between the planned and actual drilling\npoints is 1.85mm.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4eba\u673a\u534f\u4f5c\u7cfb\u7edf\u7528\u4e8e\u4e0b\u988c\u89d2\u5288\u5f00\u622a\u9aa8\u672f\uff08MASO\uff09\uff0c\u901a\u8fc7\u4efb\u52a1\u5206\u89e3\u548c\u5149\u5b66\u8ddf\u8e2a\u7cfb\u7edf\uff08OTS\uff09\u5b9e\u73b0\u7cbe\u786e\u548c\u5b89\u5168\u7684\u624b\u672f\u64cd\u4f5c\u3002", "motivation": "\u5c3d\u7ba1\u6280\u672f\u548c\u5668\u68b0\u6709\u6240\u8fdb\u6b65\uff0cMASO\u7684\u6210\u529f\u4ecd\u9ad8\u5ea6\u4f9d\u8d56\u5916\u79d1\u533b\u751f\u7684\u7ecf\u9a8c\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u7cbe\u786e\u548c\u5b89\u5168\u7684\u65b9\u6cd5\u3002", "method": "\u5c06\u624b\u672f\u5206\u4e3a\u4e09\u4e2a\u5b50\u4efb\u52a1\uff1a\u673a\u5668\u4eba\u4e3b\u5bfc\u7684\u4f4d\u7f6e\u548c\u65b9\u5411\u63a7\u5236\uff0c\u4ee5\u53ca\u5916\u79d1\u533b\u751f\u4e3b\u5bfc\u7684\u529b\u63a7\u5236\uff1b\u4f7f\u7528OTS\u8fdb\u884c\u60a3\u8005\u8ddf\u8e2a\uff0c\u65e0\u9700\u9885\u9aa8\u5939\u3002", "result": "\u5728\u6a21\u578b\u5b9e\u9a8c\u4e2d\uff0c\u8ba1\u5212\u4e0e\u5b9e\u9645\u94bb\u5b54\u70b9\u7684\u5e73\u5747\u8bef\u5dee\u4e3a1.85mm\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u80fd\u591f\u63d0\u9ad8MASO\u7684\u7cbe\u786e\u6027\u548c\u5b89\u5168\u6027\uff0c\u51cf\u5c11\u5bf9\u5916\u79d1\u533b\u751f\u7ecf\u9a8c\u7684\u4f9d\u8d56\u3002"}}
{"id": "2507.07599", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.07599", "abs": "https://arxiv.org/abs/2507.07599", "authors": ["Sedigh Khademi", "Jim Black", "Christopher Palmer", "Muhammad Javed", "Hazel Clothier", "Jim Buttery", "Gerardo Luis Dimaguila"], "title": "Enhancing Vaccine Safety Surveillance: Extracting Vaccine Mentions from Emergency Department Triage Notes Using Fine-Tuned Large Language Models", "comment": "5 pages", "summary": "This study evaluates fine-tuned Llama 3.2 models for extracting\nvaccine-related information from emergency department triage notes to support\nnear real-time vaccine safety surveillance. Prompt engineering was used to\ninitially create a labeled dataset, which was then confirmed by human\nannotators. The performance of prompt-engineered models, fine-tuned models, and\na rule-based approach was compared. The fine-tuned Llama 3 billion parameter\nmodel outperformed other models in its accuracy of extracting vaccine names.\nModel quantization enabled efficient deployment in resource-constrained\nenvironments. Findings demonstrate the potential of large language models in\nautomating data extraction from emergency department notes, supporting\nefficient vaccine safety surveillance and early detection of emerging adverse\nevents following immunization issues.", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e86\u5fae\u8c03\u7684Llama 3.2\u6a21\u578b\u4ece\u6025\u8bca\u5206\u8bca\u8bb0\u5f55\u4e2d\u63d0\u53d6\u75ab\u82d7\u76f8\u5173\u4fe1\u606f\u7684\u80fd\u529b\uff0c\u4ee5\u652f\u6301\u8fd1\u5b9e\u65f6\u75ab\u82d7\u5b89\u5168\u76d1\u6d4b\u3002", "motivation": "\u901a\u8fc7\u81ea\u52a8\u5316\u6570\u636e\u63d0\u53d6\u63d0\u9ad8\u75ab\u82d7\u5b89\u5168\u76d1\u6d4b\u6548\u7387\uff0c\u65e9\u671f\u53d1\u73b0\u75ab\u82d7\u63a5\u79cd\u540e\u7684\u4e0d\u826f\u4e8b\u4ef6\u3002", "method": "\u4f7f\u7528\u63d0\u793a\u5de5\u7a0b\u521b\u5efa\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u5e76\u7531\u4eba\u5de5\u786e\u8ba4\uff1b\u6bd4\u8f83\u63d0\u793a\u5de5\u7a0b\u6a21\u578b\u3001\u5fae\u8c03\u6a21\u578b\u548c\u57fa\u4e8e\u89c4\u5219\u7684\u65b9\u6cd5\u3002", "result": "\u5fae\u8c03\u7684Llama 3B\u53c2\u6570\u6a21\u578b\u5728\u63d0\u53d6\u75ab\u82d7\u540d\u79f0\u7684\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\uff0c\u91cf\u5316\u6280\u672f\u4f7f\u5176\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u9ad8\u6548\u90e8\u7f72\u3002", "conclusion": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u5316\u6025\u8bca\u8bb0\u5f55\u6570\u636e\u63d0\u53d6\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u53ef\u652f\u6301\u9ad8\u6548\u7684\u75ab\u82d7\u5b89\u5168\u76d1\u6d4b\u3002"}}
{"id": "2507.07825", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.07825", "abs": "https://arxiv.org/abs/2507.07825", "authors": ["Leixin Chang", "Yuxuan Nai", "Hua Chen", "Liangjing Yang"], "title": "Beyond Robustness: Learning Unknown Dynamic Load Adaptation for Quadruped Locomotion on Rough Terrain", "comment": "Accepted to the 2025 IEEE International Conference on Robotics &\n  Automation (ICRA). 8 pages, 8 figures", "summary": "Unknown dynamic load carrying is one important practical application for\nquadruped robots. Such a problem is non-trivial, posing three major challenges\nin quadruped locomotion control. First, how to model or represent the dynamics\nof the load in a generic manner. Second, how to make the robot capture the\ndynamics without any external sensing. Third, how to enable the robot to\ninteract with load handling the mutual effect and stabilizing the load. In this\nwork, we propose a general load modeling approach called load characteristics\nmodeling to capture the dynamics of the load. We integrate this proposed\nmodeling technique and leverage recent advances in Reinforcement Learning (RL)\nbased locomotion control to enable the robot to infer the dynamics of load\nmovement and interact with the load indirectly to stabilize it and realize the\nsim-to-real deployment to verify its effectiveness in real scenarios. We\nconduct extensive comparative simulation experiments to validate the\neffectiveness and superiority of our proposed method. Results show that our\nmethod outperforms other methods in sudden load resistance, load stabilizing\nand locomotion with heavy load on rough terrain.\n\\href{https://leixinjonaschang.github.io/leggedloadadapt.github.io/}{Project\nPage}.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u8d1f\u8f7d\u7279\u6027\u5efa\u6a21\u65b9\u6cd5\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u63a7\u5236\uff0c\u4f7f\u56db\u8db3\u673a\u5668\u4eba\u80fd\u591f\u63a8\u65ad\u8d1f\u8f7d\u52a8\u6001\u5e76\u95f4\u63a5\u7a33\u5b9a\u8d1f\u8f7d\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u56db\u8db3\u673a\u5668\u4eba\u5728\u672a\u77e5\u52a8\u6001\u8d1f\u8f7d\u4e0b\u7684\u4e09\u4e2a\u4e3b\u8981\u6311\u6218\uff1a\u8d1f\u8f7d\u52a8\u6001\u7684\u901a\u7528\u5efa\u6a21\u3001\u65e0\u5916\u90e8\u611f\u77e5\u7684\u52a8\u6001\u6355\u83b7\u4ee5\u53ca\u8d1f\u8f7d\u4e0e\u673a\u5668\u4eba\u7684\u4ea4\u4e92\u7a33\u5b9a\u3002", "method": "\u63d0\u51fa\u8d1f\u8f7d\u7279\u6027\u5efa\u6a21\u65b9\u6cd5\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u63a7\u5236\u6280\u672f\uff0c\u5b9e\u73b0\u8d1f\u8f7d\u52a8\u6001\u63a8\u65ad\u548c\u95f4\u63a5\u4ea4\u4e92\u7a33\u5b9a\u3002", "result": "\u4eff\u771f\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u7a81\u52a0\u8f7d\u8377\u62b5\u6297\u3001\u8d1f\u8f7d\u7a33\u5b9a\u548c\u590d\u6742\u5730\u5f62\u91cd\u8f7d\u8fd0\u52a8\u65b9\u9762\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "conclusion": "\u8d1f\u8f7d\u7279\u6027\u5efa\u6a21\u4e0e\u5f3a\u5316\u5b66\u4e60\u7684\u7ed3\u5408\u4e3a\u56db\u8db3\u673a\u5668\u4eba\u5904\u7406\u672a\u77e5\u52a8\u6001\u8d1f\u8f7d\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.07619", "categories": ["cs.AI", "math.PR"], "pdf": "https://arxiv.org/pdf/2507.07619", "abs": "https://arxiv.org/abs/2507.07619", "authors": ["Marco Sangalli", "Thomas Krak", "Cassio de Campos"], "title": "Towards conservative inference in credal networks using belief functions: the case of credal chains", "comment": null, "summary": "This paper explores belief inference in credal networks using Dempster-Shafer\ntheory. By building on previous work, we propose a novel framework for\npropagating uncertainty through a subclass of credal networks, namely chains.\nThe proposed approach efficiently yields conservative intervals through belief\nand plausibility functions, combining computational speed with robust\nuncertainty representation. Key contributions include formalizing belief-based\ninference methods and comparing belief-based inference against classical\nsensitivity analysis. Numerical results highlight the advantages and\nlimitations of applying belief inference within this framework, providing\ninsights into its practical utility for chains and for credal networks in\ngeneral.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eDempster-Shafer\u7406\u8bba\u7684\u4fe1\u5ff5\u63a8\u7406\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u94fe\u5f0f\u4fe1\u7528\u7f51\u7edc\u4e2d\u4f20\u64ad\u4e0d\u786e\u5b9a\u6027\uff0c\u7ed3\u5408\u8ba1\u7b97\u901f\u5ea6\u4e0e\u9c81\u68d2\u7684\u8868\u793a\u65b9\u6cd5\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u5728\u4fe1\u7528\u7f51\u7edc\u4e2d\u9ad8\u6548\u4e14\u9c81\u68d2\u5730\u4f20\u64ad\u4e0d\u786e\u5b9a\u6027\uff0c\u7279\u522b\u662f\u9488\u5bf9\u94fe\u5f0f\u7ed3\u6784\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4fe1\u5ff5\u548c\u53ef\u4fe1\u5ea6\u51fd\u6570\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4fdd\u5b88\u533a\u95f4\u4f20\u64ad\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u4e0e\u7ecf\u5178\u654f\u611f\u6027\u5206\u6790\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u94fe\u5f0f\u4fe1\u7528\u7f51\u7edc\u4e2d\u5177\u6709\u4f18\u52bf\uff0c\u4f46\u4e5f\u5b58\u5728\u5c40\u9650\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u94fe\u5f0f\u4fe1\u7528\u7f51\u7edc\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u4fe1\u5ff5\u63a8\u7406\u65b9\u6cd5\uff0c\u5e76\u5bf9\u5176\u5728\u66f4\u5e7f\u6cdb\u4fe1\u7528\u7f51\u7edc\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u542f\u793a\u3002"}}
{"id": "2507.07845", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.07845", "abs": "https://arxiv.org/abs/2507.07845", "authors": ["David Warutumo", "Ciira wa Maina"], "title": "Perceptual Distortions and Autonomous Representation Learning in a Minimal Robotic System", "comment": "2 authors, 23 pages, 11 figures", "summary": "Autonomous agents, particularly in the field of robotics, rely on sensory\ninformation to perceive and navigate their environment. However, these sensory\ninputs are often imperfect, leading to distortions in the agent's internal\nrepresentation of the world. This paper investigates the nature of these\nperceptual distortions and how they influence autonomous representation\nlearning using a minimal robotic system. We utilize a simulated two-wheeled\nrobot equipped with distance sensors and a compass, operating within a simple\nsquare environment. Through analysis of the robot's sensor data during random\nexploration, we demonstrate how a distorted perceptual space emerges. Despite\nthese distortions, we identify emergent structures within the perceptual space\nthat correlate with the physical environment, revealing how the robot\nautonomously learns a structured representation for navigation without explicit\nspatial information. This work contributes to the understanding of embodied\ncognition, minimal agency, and the role of perception in self-generated\nnavigation strategies in artificial life.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u611f\u77e5\u5931\u771f\u5bf9\u81ea\u4e3b\u673a\u5668\u4eba\u8868\u793a\u5b66\u4e60\u7684\u5f71\u54cd\uff0c\u901a\u8fc7\u6a21\u62df\u673a\u5668\u4eba\u5b9e\u9a8c\u5c55\u793a\u4e86\u5931\u771f\u611f\u77e5\u7a7a\u95f4\u4e2d\u7684\u6d8c\u73b0\u7ed3\u6784\u3002", "motivation": "\u63a2\u7d22\u81ea\u4e3b\u673a\u5668\u4eba\u5982\u4f55\u5728\u611f\u77e5\u5931\u771f\u60c5\u51b5\u4e0b\u5b66\u4e60\u73af\u5883\u8868\u793a\uff0c\u4ee5\u7406\u89e3\u611f\u77e5\u5728\u5bfc\u822a\u7b56\u7565\u4e2d\u7684\u4f5c\u7528\u3002", "method": "\u4f7f\u7528\u6a21\u62df\u4e24\u8f6e\u673a\u5668\u4eba\uff0c\u914d\u5907\u8ddd\u79bb\u4f20\u611f\u5668\u548c\u6307\u5357\u9488\uff0c\u5728\u65b9\u5f62\u73af\u5883\u4e2d\u968f\u673a\u63a2\u7d22\u5e76\u5206\u6790\u4f20\u611f\u5668\u6570\u636e\u3002", "result": "\u53d1\u73b0\u611f\u77e5\u7a7a\u95f4\u4e2d\u6d8c\u73b0\u7684\u7ed3\u6784\u4e0e\u7269\u7406\u73af\u5883\u76f8\u5173\uff0c\u673a\u5668\u4eba\u80fd\u81ea\u4e3b\u5b66\u4e60\u5bfc\u822a\u8868\u793a\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5177\u8eab\u8ba4\u77e5\u548c\u6700\u5c0f\u4ee3\u7406\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\uff0c\u63ed\u793a\u4e86\u611f\u77e5\u5728\u81ea\u4e3b\u5bfc\u822a\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2507.07644", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.07644", "abs": "https://arxiv.org/abs/2507.07644", "authors": ["Fedor Rodionov", "Abdelrahman Eldesokey", "Michael Birsak", "John Femiani", "Bernard Ghanem", "Peter Wonka"], "title": "PlanQA: A Benchmark for Spatial Reasoning in LLMs using Structured Representations", "comment": "25 pages, 18 figures. Diagnostic benchmark for spatial reasoning in\n  LLMs. Project page: https://OldDelorean.github.io/PlanQA/", "summary": "We introduce PlanQA, a diagnostic benchmark for evaluating geometric and\nspatial reasoning in large-language models (LLMs). PlanQA is grounded in\nstructured representations of indoor scenes, such as kitchens, living rooms,\nand bedrooms, encoded in a symbolic format (e.g., JSON, XML layouts). The\nbenchmark includes diverse question types that test not only metric and\ntopological reasoning (e.g., distance, visibility, shortest paths) but also\ninterior design constraints such as affordance, clearance, balance, and\nusability. Our results across a variety of frontier open-source and commercial\nLLMs show that while models may succeed in shallow queries, they often fail to\nsimulate physical constraints, preserve spatial coherence, or generalize under\nlayout perturbation. PlanQA uncovers a clear blind spot in today's LLMs: they\ndo not consistently reason about real-world layouts. We hope that this\nbenchmark inspires new work on language models that can accurately infer and\nmanipulate spatial and geometric properties in practical settings.", "AI": {"tldr": "PlanQA\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u51e0\u4f55\u548c\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u7684\u8bca\u65ad\u57fa\u51c6\uff0c\u57fa\u4e8e\u5ba4\u5185\u573a\u666f\u7684\u7ed3\u6784\u5316\u8868\u793a\uff0c\u63ed\u793aLLM\u5728\u771f\u5b9e\u4e16\u754c\u5e03\u5c40\u63a8\u7406\u4e2d\u7684\u76f2\u70b9\u3002", "motivation": "\u5f53\u524dLLM\u5728\u51e0\u4f55\u548c\u7a7a\u95f4\u63a8\u7406\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5728\u6a21\u62df\u7269\u7406\u7ea6\u675f\u548c\u4fdd\u6301\u7a7a\u95f4\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0cPlanQA\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "PlanQA\u4f7f\u7528\u7b26\u53f7\u5316\u683c\u5f0f\uff08\u5982JSON\u3001XML\uff09\u7f16\u7801\u5ba4\u5185\u573a\u666f\uff0c\u8bbe\u8ba1\u591a\u6837\u5316\u95ee\u9898\u7c7b\u578b\u6d4b\u8bd5\u5ea6\u91cf\u3001\u62d3\u6251\u63a8\u7406\u53ca\u8bbe\u8ba1\u7ea6\u675f\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cLLM\u5728\u6d45\u5c42\u67e5\u8be2\u4e2d\u8868\u73b0\u5c1a\u53ef\uff0c\u4f46\u5728\u7269\u7406\u7ea6\u675f\u6a21\u62df\u3001\u7a7a\u95f4\u4e00\u81f4\u6027\u4fdd\u6301\u53ca\u5e03\u5c40\u6270\u52a8\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "PlanQA\u63ed\u793a\u4e86LLM\u5728\u771f\u5b9e\u4e16\u754c\u5e03\u5c40\u63a8\u7406\u4e2d\u7684\u76f2\u70b9\uff0c\u4e3a\u672a\u6765\u5f00\u53d1\u80fd\u51c6\u786e\u63a8\u7406\u548c\u64cd\u4f5c\u7a7a\u95f4\u51e0\u4f55\u5c5e\u6027\u7684\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2507.07846", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.07846", "abs": "https://arxiv.org/abs/2507.07846", "authors": ["Kavindie Katuwandeniya", "Samith Rajapaksha Jayasekara Widhanapathirana"], "title": "ROS Help Desk: GenAI Powered, User-Centric Framework for ROS Error Diagnosis and Debugging", "comment": null, "summary": "As the robotics systems increasingly integrate into daily life, from smart\nhome assistants to the new-wave of industrial automation systems (Industry\n4.0), there's an increasing need to bridge the gap between complex robotic\nsystems and everyday users. The Robot Operating System (ROS) is a flexible\nframework often utilised in writing robot software, providing tools and\nlibraries for building complex robotic systems. However, ROS's distributed\narchitecture and technical messaging system create barriers for understanding\nrobot status and diagnosing errors. This gap can lead to extended maintenance\ndowntimes, as users with limited ROS knowledge may struggle to quickly diagnose\nand resolve system issues. Moreover, this deficit in expertise often delays\nproactive maintenance and troubleshooting, further increasing the frequency and\nduration of system interruptions. ROS Help Desk provides intuitive error\nexplanations and debugging support, dynamically customized to users of varying\nexpertise levels. It features user-centric debugging tools that simplify error\ndiagnosis, implements proactive error detection capabilities to reduce\ndowntime, and integrates multimodal data processing for comprehensive system\nstate understanding across multi-sensor data (e.g., lidar, RGB). Testing\nqualitatively and quantitatively with artificially induced errors demonstrates\nthe system's ability to proactively and accurately diagnose problems,\nultimately reducing maintenance time and fostering more effective human-robot\ncollaboration.", "AI": {"tldr": "ROS Help Desk\u901a\u8fc7\u76f4\u89c2\u7684\u9519\u8bef\u89e3\u91ca\u548c\u8c03\u8bd5\u652f\u6301\uff0c\u51cf\u5c11\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u7ef4\u62a4\u65f6\u95f4\uff0c\u63d0\u5347\u4eba\u673a\u534f\u4f5c\u6548\u7387\u3002", "motivation": "\u968f\u7740\u673a\u5668\u4eba\u7cfb\u7edf\u878d\u5165\u65e5\u5e38\u751f\u6d3b\u548c\u5de5\u4e1a\u81ea\u52a8\u5316\uff0cROS\u7684\u590d\u6742\u67b6\u6784\u548c\u6280\u672f\u6d88\u606f\u7cfb\u7edf\u5bf9\u666e\u901a\u7528\u6237\u6784\u6210\u969c\u788d\uff0c\u5bfc\u81f4\u7ef4\u62a4\u56f0\u96be\u548c\u7cfb\u7edf\u4e2d\u65ad\u3002", "method": "ROS Help Desk\u63d0\u4f9b\u7528\u6237\u53cb\u597d\u7684\u8c03\u8bd5\u5de5\u5177\u3001\u4e3b\u52a8\u9519\u8bef\u68c0\u6d4b\u529f\u80fd\uff0c\u5e76\u6574\u5408\u591a\u6a21\u6001\u6570\u636e\u5904\u7406\uff0c\u4ee5\u7b80\u5316\u9519\u8bef\u8bca\u65ad\u3002", "result": "\u6d4b\u8bd5\u8868\u660e\uff0c\u7cfb\u7edf\u80fd\u4e3b\u52a8\u51c6\u786e\u8bca\u65ad\u95ee\u9898\uff0c\u663e\u8457\u51cf\u5c11\u7ef4\u62a4\u65f6\u95f4\u3002", "conclusion": "ROS Help Desk\u6709\u6548\u964d\u4f4e\u4e86\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u7ef4\u62a4\u96be\u5ea6\uff0c\u4fc3\u8fdb\u4e86\u66f4\u9ad8\u6548\u7684\u4eba\u673a\u534f\u4f5c\u3002"}}
{"id": "2507.07723", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07723", "abs": "https://arxiv.org/abs/2507.07723", "authors": ["Chengtao Jian", "Kai Yang", "Ye Ouyang", "Xiaozhou Ye"], "title": "Stable Preference Optimization for LLMs: A Bilevel Approach Beyond Direct Preference Optimization", "comment": null, "summary": "Direct Preference Optimization (DPO) has emerged as a popular and efficient\nalternative to reward modeling and reinforcement learning for aligning language\nmodels with human preferences. Despite its empirical success, the theoretical\nproperties and intrinsic limitations of DPO remain underexplored. In this work,\nwe first present a comprehensive analysis of DPO's dynamics from a probability\nevolution perspective. Our analysis reveals that DPO is highly sensitive to\ninitialization. It also tends to misallocate probability mass, which can\ninadvertently shift probability toward irrelevant or undesired responses. This\nmisallocation may unintentionally reinforce model bias, thereby compromising\nboth the stability of model alignment and the consistency with intended\npreferences. Motivated by these theoretical findings, we propose a\ntheoretically grounded bilevel optimization framework that tightly integrate\nsupervised fine-tuning with an enhanced DPO objective a.k.a. stable preference\noptimization. Our approach introduces a principled regularization scheme to\nexplicitly encourage absolute probability improvement for preferred outputs,\nwhile maintaining stable optimization dynamics. Experiments on challenging\nreasoning and summarization benchmarks elucidate that our method consistently\nimproves reasoning accuracy and better aligns output distributions with\nintended preferences, outperforming standard DPO. Stable preference\noptimization provides new insights into the design of preference-based\nalignment objectives and opens up new avenues towards more reliable and\ninterpretable language model alignment.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u7684\u7406\u8bba\u5c40\u9650\u6027\u548c\u52a8\u6001\u7279\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cc\u5c42\u4f18\u5316\u7684\u7a33\u5b9a\u504f\u597d\u4f18\u5316\u65b9\u6cd5\uff0c\u4ee5\u6539\u8fdb\u6a21\u578b\u5bf9\u9f50\u7684\u7a33\u5b9a\u6027\u548c\u4e00\u81f4\u6027\u3002", "motivation": "\u5c3d\u7ba1DPO\u5728\u5b9e\u8bc1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5176\u7406\u8bba\u7279\u6027\u548c\u5185\u5728\u5c40\u9650\u6027\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002\u7814\u7a76\u53d1\u73b0DPO\u5bf9\u521d\u59cb\u5316\u654f\u611f\u4e14\u5bb9\u6613\u9519\u8bef\u5206\u914d\u6982\u7387\u8d28\u91cf\uff0c\u53ef\u80fd\u65e0\u610f\u4e2d\u5f3a\u5316\u6a21\u578b\u504f\u89c1\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cc\u5c42\u4f18\u5316\u7684\u7a33\u5b9a\u504f\u597d\u4f18\u5316\u6846\u67b6\uff0c\u7ed3\u5408\u76d1\u7763\u5fae\u8c03\u548c\u6539\u8fdb\u7684DPO\u76ee\u6807\uff0c\u5f15\u5165\u6b63\u5219\u5316\u65b9\u6848\u4ee5\u660e\u786e\u9f13\u52b1\u5bf9\u504f\u597d\u8f93\u51fa\u7684\u7edd\u5bf9\u6982\u7387\u63d0\u5347\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u63a8\u7406\u548c\u6458\u8981\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u51c6\u786e\u6027\uff0c\u5e76\u66f4\u597d\u5730\u5bf9\u9f50\u8f93\u51fa\u5206\u5e03\u4e0e\u9884\u671f\u504f\u597d\uff0c\u4f18\u4e8e\u6807\u51c6DPO\u3002", "conclusion": "\u7a33\u5b9a\u504f\u597d\u4f18\u5316\u4e3a\u504f\u597d\u5bf9\u9f50\u76ee\u6807\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\uff0c\u5e76\u4e3a\u66f4\u53ef\u9760\u548c\u53ef\u89e3\u91ca\u7684\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2507.07872", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07872", "abs": "https://arxiv.org/abs/2507.07872", "authors": ["Daniel Betschinske", "Steven Peters"], "title": "Improving AEBS Validation Through Objective Intervention Classification Leveraging the Prediction Divergence Principle", "comment": "This work has been accepted for publication at the 2025 IEEE\n  International Automated Vehicle Validation Conference (IAVVC)", "summary": "The safety validation of automatic emergency braking system (AEBS) requires\naccurately distinguishing between false positive (FP) and true positive (TP)\nsystem activations. While simulations allow straightforward differentiation by\ncomparing scenarios with and without interventions, analyzing activations from\nopen-loop resimulations - such as those from field operational testing (FOT) -\nis more complex. This complexity arises from scenario parameter uncertainty and\nthe influence of driver interventions in the recorded data. Human labeling is\nfrequently used to address these challenges, relying on subjective assessments\nof intervention necessity or situational criticality, potentially introducing\nbiases and limitations. This work proposes a rule-based classification approach\nleveraging the Prediction Divergence Principle (PDP) to address those issues.\nApplied to a simplified AEBS, the proposed method reveals key strengths,\nlimitations, and system requirements for effective implementation. The findings\nsuggest that combining this approach with human labeling may enhance the\ntransparency and consistency of classification, thereby improving the overall\nvalidation process. While the rule set for classification derived in this work\nadopts a conservative approach, the paper outlines future directions for\nrefinement and broader applicability. Finally, this work highlights the\npotential of such methods to complement existing practices, paving the way for\nmore reliable and reproducible AEBS validation frameworks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c4\u5219\u7684\u5206\u7c7b\u65b9\u6cd5\uff08PDP\uff09\uff0c\u7528\u4e8e\u533a\u5206\u81ea\u52a8\u7d27\u6025\u5236\u52a8\u7cfb\u7edf\uff08AEBS\uff09\u7684\u8bef\u62a5\u548c\u771f\u62a5\uff0c\u4ee5\u63d0\u9ad8\u9a8c\u8bc1\u7684\u900f\u660e\u5ea6\u548c\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u4eba\u5de5\u6807\u6ce8\uff09\u5b58\u5728\u4e3b\u89c2\u6027\u548c\u504f\u5dee\uff0c\u96be\u4ee5\u51c6\u786e\u533a\u5206AEBS\u7684\u8bef\u62a5\u548c\u771f\u62a5\uff0c\u5c24\u5176\u662f\u5728\u5f00\u653e\u5faa\u73af\u91cd\u6a21\u62df\uff08\u5982FOT\uff09\u4e2d\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u9884\u6d4b\u5206\u6b67\u539f\u5219\uff08PDP\uff09\u7684\u89c4\u5219\u5206\u7c7b\u65b9\u6cd5\uff0c\u7ed3\u5408\u7b80\u5316AEBS\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u8be5\u65b9\u6cd5\u5c55\u793a\u4e86\u5176\u4f18\u52bf\u548c\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u7cfb\u7edf\u9700\u6c42\uff1b\u7ed3\u5408\u4eba\u5de5\u6807\u6ce8\u53ef\u63d0\u5347\u5206\u7c7b\u7684\u900f\u660e\u5ea6\u548c\u4e00\u81f4\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aAEBS\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u8865\u5145\u5de5\u5177\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u4f18\u5316\u548c\u6269\u5c55\u5e94\u7528\u8303\u56f4\u3002"}}
{"id": "2507.07743", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.07743", "abs": "https://arxiv.org/abs/2507.07743", "authors": ["Phil\u00e9mon Beghin", "Anne-Emmanuelle Ceulemans", "Fran\u00e7ois Glineur"], "title": "Identification of Violin Reduction via Contour Lines Classification", "comment": null, "summary": "The first violins appeared in late 16th-century Italy. Over the next 200\nyears, they spread across Europe and luthiers of various royal courts, eager to\nexperiment with new techniques, created a highly diverse family of instruments.\nAround 1750, size standards were introduced to unify violin making for\norchestras and conservatories. Instruments that fell between two standards were\nthen reduced to a smaller size by luthiers. These reductions have an impact on\nseveral characteristics of violins, in particular on the contour lines, i.e.\nlines of constant altitude, which look more like a U for non reduced\ninstruments and a V for reduced ones. While such differences are observed by\nexperts, they have not been studied quantitatively.\n  This paper presents a method for classifying violins as reduced or\nnon-reduced based on their contour lines. We study a corpus of 25 instruments\nwhose 3D geometric meshes were acquired via photogrammetry. For each\ninstrument, we extract 10-20 contour lines regularly spaced every millimetre.\nEach line is fitted with a parabola-like curve (with an equation of the type y\n= alpha*abs(x)**beta) depending on two parameters, describing how open (beta)\nand how vertically stretched (alpha) the curve is. We compute additional\nfeatures from those parameters, using regressions and counting how many values\nfall under some threshold. We also deal with outliers and non equal numbers of\nlevels, and eventually obtain a numerical profile for each instrument.\n  We then apply classification methods to assess whether geometry alone can\npredict size reduction. We find that distinguishing between reduced and non\nreduced instruments is feasible to some degree, taking into account that a\nwhole spectrum of more or less transformed violins exists, for which it is more\ndifficult to quantify the reduction. We also find the opening parameter beta to\nbe the most predictive.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8f6e\u5ed3\u7ebf\u5206\u7c7b\u5c0f\u63d0\u7434\u662f\u5426\u88ab\u7f29\u5c0f\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u51e0\u4f55\u7279\u5f81\u5206\u6790\u5b9e\u73b0\u4e86\u5bf9\u7f29\u5c0f\u4e0e\u975e\u7f29\u5c0f\u4e50\u5668\u7684\u533a\u5206\u3002", "motivation": "\u5386\u53f2\u4e0a\u5c0f\u63d0\u7434\u5236\u4f5c\u6807\u51c6\u7684\u53d8\u5316\u5bfc\u81f4\u90e8\u5206\u4e50\u5668\u88ab\u7f29\u5c0f\uff0c\u4f46\u5176\u51e0\u4f55\u7279\u5f81\u5dee\u5f02\u672a\u88ab\u91cf\u5316\u7814\u7a76\u3002", "method": "\u5229\u7528\u6444\u5f71\u6d4b\u91cf\u83b7\u53d625\u628a\u5c0f\u63d0\u7434\u76843D\u51e0\u4f55\u7f51\u683c\uff0c\u63d0\u53d6\u8f6e\u5ed3\u7ebf\u5e76\u62df\u5408\u629b\u7269\u7ebf\u66f2\u7ebf\uff0c\u8ba1\u7b97\u53c2\u6570\u7279\u5f81\u540e\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u51e0\u4f55\u7279\u5f81\u53ef\u4ee5\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u533a\u5206\u7f29\u5c0f\u4e0e\u975e\u7f29\u5c0f\u4e50\u5668\uff0c\u5176\u4e2d\u5f00\u53e3\u53c2\u6570\u03b2\u6700\u5177\u9884\u6d4b\u6027\u3002", "conclusion": "\u51e0\u4f55\u65b9\u6cd5\u53ef\u7528\u4e8e\u5c0f\u63d0\u7434\u7f29\u5c0f\u5206\u7c7b\uff0c\u4f46\u9700\u8003\u8651\u4e50\u5668\u53d8\u5f62\u7684\u591a\u6837\u6027\u3002"}}
{"id": "2507.07980", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.07980", "abs": "https://arxiv.org/abs/2507.07980", "authors": ["Wanjia Fu", "Hongyu Li", "Ivy X. He", "Stefanie Tellex", "Srinath Sridhar"], "title": "UniTac: Whole-Robot Touch Sensing Without Tactile Sensors", "comment": null, "summary": "Robots can better interact with humans and unstructured environments through\ntouch sensing. However, most commercial robots are not equipped with tactile\nskins, making it challenging to achieve even basic touch-sensing functions,\nsuch as contact localization. We present UniTac, a data-driven whole-body\ntouch-sensing approach that uses only proprioceptive joint sensors and does not\nrequire the installation of additional sensors. Our approach enables a robot\nequipped solely with joint sensors to localize contacts. Our goal is to\ndemocratize touch sensing and provide an off-the-shelf tool for HRI researchers\nto provide their robots with touch-sensing capabilities. We validate our\napproach on two platforms: the Franka robot arm and the Spot quadruped. On\nFranka, we can localize contact to within 8.0 centimeters, and on Spot, we can\nlocalize to within 7.2 centimeters at around 2,000 Hz on an RTX 3090 GPU\nwithout adding any additional sensors to the robot. Project website:\nhttps://ivl.cs.brown.edu/research/unitac.", "AI": {"tldr": "UniTac\u662f\u4e00\u79cd\u4ec5\u4f7f\u7528\u672c\u4f53\u611f\u89c9\u5173\u8282\u4f20\u611f\u5668\u7684\u6570\u636e\u9a71\u52a8\u5168\u8eab\u89e6\u89c9\u611f\u77e5\u65b9\u6cd5\uff0c\u65e0\u9700\u989d\u5916\u4f20\u611f\u5668\u5373\u53ef\u5b9e\u73b0\u63a5\u89e6\u5b9a\u4f4d\u3002", "motivation": "\u5546\u4e1a\u673a\u5668\u4eba\u901a\u5e38\u7f3a\u4e4f\u89e6\u89c9\u76ae\u80a4\uff0c\u96be\u4ee5\u5b9e\u73b0\u57fa\u672c\u89e6\u89c9\u529f\u80fd\uff0c\u5982\u63a5\u89e6\u5b9a\u4f4d\u3002UniTac\u65e8\u5728\u666e\u53ca\u89e6\u89c9\u611f\u77e5\uff0c\u4e3aHRI\u7814\u7a76\u63d0\u4f9b\u73b0\u6210\u5de5\u5177\u3002", "method": "\u5229\u7528\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u4ec5\u4f9d\u8d56\u5173\u8282\u4f20\u611f\u5668\u5b9e\u73b0\u63a5\u89e6\u5b9a\u4f4d\uff0c\u65e0\u9700\u989d\u5916\u786c\u4ef6\u3002", "result": "\u5728Franka\u673a\u68b0\u81c2\u548cSpot\u56db\u8db3\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\uff0c\u63a5\u89e6\u5b9a\u4f4d\u7cbe\u5ea6\u5206\u522b\u4e3a8.0\u5398\u7c73\u548c7.2\u5398\u7c73\uff0c\u9891\u7387\u8fbe2000Hz\u3002", "conclusion": "UniTac\u4e3a\u673a\u5668\u4eba\u89e6\u89c9\u611f\u77e5\u63d0\u4f9b\u4e86\u4e00\u79cd\u4f4e\u6210\u672c\u3001\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5e73\u53f0\u3002"}}
{"id": "2507.07787", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.07787", "abs": "https://arxiv.org/abs/2507.07787", "authors": ["Elizabeth Hilliard", "Akshaya Jagadeesh", "Alex Cook", "Steele Billings", "Nicholas Skytland", "Alicia Llewellyn", "Jackson Paull", "Nathan Paull", "Nolan Kurylo", "Keatra Nesbitt", "Robert Gruenewald", "Anthony Jantzi", "Omar Chavez"], "title": "Measuring AI Alignment with Human Flourishing", "comment": null, "summary": "This paper introduces the Flourishing AI Benchmark (FAI Benchmark), a novel\nevaluation framework that assesses AI alignment with human flourishing across\nseven dimensions: Character and Virtue, Close Social Relationships, Happiness\nand Life Satisfaction, Meaning and Purpose, Mental and Physical Health,\nFinancial and Material Stability, and Faith and Spirituality. Unlike\ntraditional benchmarks that focus on technical capabilities or harm prevention,\nthe FAI Benchmark measures AI performance on how effectively models contribute\nto the flourishing of a person across these dimensions. The benchmark evaluates\nhow effectively LLM AI systems align with current research models of holistic\nhuman well-being through a comprehensive methodology that incorporates 1,229\nobjective and subjective questions. Using specialized judge Large Language\nModels (LLMs) and cross-dimensional evaluation, the FAI Benchmark employs\ngeometric mean scoring to ensure balanced performance across all flourishing\ndimensions. Initial testing of 28 leading language models reveals that while\nsome models approach holistic alignment (with the highest-scoring models\nachieving 72/100), none are acceptably aligned across all dimensions,\nparticularly in Faith and Spirituality, Character and Virtue, and Meaning and\nPurpose. This research establishes a framework for developing AI systems that\nactively support human flourishing rather than merely avoiding harm, offering\nsignificant implications for AI development, ethics, and evaluation.", "AI": {"tldr": "FAI Benchmark\u8bc4\u4f30AI\u5728\u4e03\u4e2a\u7ef4\u5ea6\u4e0a\u5bf9\u4eba\u7c7b\u7e41\u8363\u7684\u8d21\u732e\uff0c\u53d1\u73b0\u5f53\u524d\u6a21\u578b\u5728\u4fe1\u4ef0\u4e0e\u7075\u6027\u3001\u54c1\u5fb7\u4e0e\u7f8e\u5fb7\u3001\u610f\u4e49\u4e0e\u76ee\u7684\u65b9\u9762\u8868\u73b0\u4e0d\u8db3\u3002", "motivation": "\u4f20\u7edfAI\u8bc4\u4f30\u4ec5\u5173\u6ce8\u6280\u672f\u80fd\u529b\u6216\u5371\u5bb3\u9884\u9632\uff0c\u800cFAI Benchmark\u65e8\u5728\u8861\u91cfAI\u5bf9\u4eba\u7c7b\u5168\u9762\u7e41\u8363\u7684\u8d21\u732e\u3002", "method": "\u901a\u8fc71,229\u4e2a\u4e3b\u5ba2\u89c2\u95ee\u9898\uff0c\u7ed3\u5408\u4e13\u4e1aLLM\u8bc4\u4f30\u548c\u51e0\u4f55\u5e73\u5747\u8bc4\u5206\uff0c\u5bf928\u4e2a\u9886\u5148\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "\u6700\u9ad8\u5206\u6a21\u578b\u4ec572/100\uff0c\u65e0\u6a21\u578b\u5728\u6240\u6709\u7ef4\u5ea6\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u5c24\u5176\u5728\u4fe1\u4ef0\u4e0e\u7075\u6027\u3001\u54c1\u5fb7\u4e0e\u7f8e\u5fb7\u3001\u610f\u4e49\u4e0e\u76ee\u7684\u65b9\u9762\u3002", "conclusion": "FAI Benchmark\u4e3a\u5f00\u53d1\u652f\u6301\u4eba\u7c7b\u7e41\u8363\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6846\u67b6\uff0c\u5bf9AI\u53d1\u5c55\u3001\u4f26\u7406\u548c\u8bc4\u4f30\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2507.07818", "categories": ["cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07818", "abs": "https://arxiv.org/abs/2507.07818", "authors": ["Lu Xu", "Jiaqian Yu", "Xiongfeng Peng", "Yiwei Chen", "Weiming Li", "Jaewook Yoo", "Sunghyun Chunag", "Dongwook Lee", "Daehyun Ji", "Chao Zhang"], "title": "MoSE: Skill-by-Skill Mixture-of-Expert Learning for Autonomous Driving", "comment": null, "summary": "Recent studies show large language models (LLMs) and vision language models\n(VLMs) trained using web-scale data can empower end-to-end autonomous driving\nsystems for a better generalization and interpretation. Specifically, by\ndynamically routing inputs to specialized subsets of parameters, the\nMixture-of-Experts (MoE) technique enables general LLMs or VLMs to achieve\nsubstantial performance improvements while maintaining computational\nefficiency. However, general MoE models usually demands extensive training data\nand complex optimization. In this work, inspired by the learning process of\nhuman drivers, we propose a skill-oriented MoE, called MoSE, which mimics human\ndrivers' learning process and reasoning process, skill-by-skill and\nstep-by-step. We propose a skill-oriented routing mechanism that begins with\ndefining and annotating specific skills, enabling experts to identify the\nnecessary driving competencies for various scenarios and reasoning tasks,\nthereby facilitating skill-by-skill learning. Further align the driving process\nto multi-step planning in human reasoning and end-to-end driving models, we\nbuild a hierarchical skill dataset and pretrain the router to encourage the\nmodel to think step-by-step. Unlike multi-round dialogs, MoSE integrates\nvaluable auxiliary tasks (e.g.\\ description, reasoning, planning) in one single\nforward process without introducing any extra computational cost. With less\nthan 3B sparsely activated parameters, our model outperforms several 8B+\nparameters on CODA AD corner case reasoning task. Compared to existing methods\nbased on open-source models and data, our approach achieves state-of-the-art\nperformance with significantly reduced activated model size (at least by\n$62.5\\%$) with a single-turn conversation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6280\u80fd\u5bfc\u5411\u7684\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\uff08MoSE\uff09\uff0c\u6a21\u4eff\u4eba\u7c7b\u9a7e\u9a76\u5458\u7684\u5b66\u4e60\u548c\u63a8\u7406\u8fc7\u7a0b\uff0c\u901a\u8fc7\u6280\u80fd\u5206\u6b65\u5b66\u4e60\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u6027\u80fd\uff0c\u540c\u65f6\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\uff08MoE\uff09\u9700\u8981\u5927\u91cf\u6570\u636e\u548c\u590d\u6742\u4f18\u5316\uff0c\u800c\u4eba\u7c7b\u9a7e\u9a76\u5458\u7684\u5b66\u4e60\u8fc7\u7a0b\u66f4\u5177\u6548\u7387\u3002MoSE\u65e8\u5728\u901a\u8fc7\u6280\u80fd\u5bfc\u5411\u7684\u8def\u7531\u673a\u5236\uff0c\u6a21\u4eff\u4eba\u7c7b\u7684\u5b66\u4e60\u548c\u63a8\u7406\u65b9\u5f0f\uff0c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "method": "MoSE\u901a\u8fc7\u5b9a\u4e49\u548c\u6807\u6ce8\u7279\u5b9a\u6280\u80fd\uff0c\u6784\u5efa\u5206\u5c42\u6280\u80fd\u6570\u636e\u96c6\uff0c\u5e76\u9884\u8bad\u7ec3\u8def\u7531\u5668\uff0c\u5b9e\u73b0\u6280\u80fd\u5206\u6b65\u5b66\u4e60\u548c\u591a\u6b65\u89c4\u5212\u63a8\u7406\u3002\u6a21\u578b\u5728\u5355\u6b21\u524d\u5411\u8fc7\u7a0b\u4e2d\u6574\u5408\u8f85\u52a9\u4efb\u52a1\uff0c\u65e0\u9700\u989d\u5916\u8ba1\u7b97\u6210\u672c\u3002", "result": "MoSE\u5728CODA AD\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u591a\u4e2a8B+\u53c2\u6570\u6a21\u578b\uff0c\u6fc0\u6d3b\u53c2\u6570\u91cf\u51cf\u5c11\u81f3\u5c1162.5%\uff0c\u4e14\u6027\u80fd\u8fbe\u5230\u5f53\u524d\u6700\u4f18\u3002", "conclusion": "MoSE\u901a\u8fc7\u6a21\u4eff\u4eba\u7c7b\u5b66\u4e60\u8fc7\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u6a21\u578b\u7684\u6027\u80fd\u548c\u6548\u7387\uff0c\u4e3a\u8f7b\u91cf\u5316\u6a21\u578b\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.07820", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07820", "abs": "https://arxiv.org/abs/2507.07820", "authors": ["Eunsu Baek", "Keondo Park", "Jeonggil Ko", "Min-hwan Oh", "Taesik Gong", "Hyung-Sin Kim"], "title": "AI Should Sense Better, Not Just Scale Bigger: Adaptive Sensing as a Paradigm Shift", "comment": null, "summary": "Current AI advances largely rely on scaling neural models and expanding\ntraining datasets to achieve generalization and robustness. Despite notable\nsuccesses, this paradigm incurs significant environmental, economic, and\nethical costs, limiting sustainability and equitable access. Inspired by\nbiological sensory systems, where adaptation occurs dynamically at the input\n(e.g., adjusting pupil size, refocusing vision)--we advocate for adaptive\nsensing as a necessary and foundational shift. Adaptive sensing proactively\nmodulates sensor parameters (e.g., exposure, sensitivity, multimodal\nconfigurations) at the input level, significantly mitigating covariate shifts\nand improving efficiency. Empirical evidence from recent studies demonstrates\nthat adaptive sensing enables small models (e.g., EfficientNet-B0) to surpass\nsubstantially larger models (e.g., OpenCLIP-H) trained with significantly more\ndata and compute. We (i) outline a roadmap for broadly integrating adaptive\nsensing into real-world applications spanning humanoid, healthcare, autonomous\nsystems, agriculture, and environmental monitoring, (ii) critically assess\ntechnical and ethical integration challenges, and (iii) propose targeted\nresearch directions, such as standardized benchmarks, real-time adaptive\nalgorithms, multimodal integration, and privacy-preserving methods.\nCollectively, these efforts aim to transition the AI community toward\nsustainable, robust, and equitable artificial intelligence systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u81ea\u9002\u5e94\u611f\u77e5\u4f5c\u4e3aAI\u53d1\u5c55\u7684\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u4f20\u611f\u5668\u53c2\u6570\u63d0\u5347\u6548\u7387\u548c\u9c81\u68d2\u6027\uff0c\u51cf\u5c11\u5bf9\u5927\u89c4\u6a21\u6a21\u578b\u548c\u6570\u636e\u7684\u4f9d\u8d56\u3002", "motivation": "\u5f53\u524dAI\u4f9d\u8d56\u5927\u89c4\u6a21\u6a21\u578b\u548c\u6570\u636e\u96c6\uff0c\u5e26\u6765\u73af\u5883\u3001\u7ecf\u6d4e\u548c\u4f26\u7406\u6210\u672c\uff0c\u9650\u5236\u4e86\u53ef\u6301\u7eed\u6027\u548c\u516c\u5e73\u6027\u3002\u751f\u7269\u611f\u5b98\u7cfb\u7edf\u7684\u52a8\u6001\u9002\u5e94\u6027\u542f\u53d1\u4e86\u8fd9\u4e00\u7814\u7a76\u3002", "method": "\u63d0\u51fa\u81ea\u9002\u5e94\u611f\u77e5\u65b9\u6cd5\uff0c\u52a8\u6001\u8c03\u6574\u4f20\u611f\u5668\u53c2\u6570\uff08\u5982\u66dd\u5149\u3001\u7075\u654f\u5ea6\u3001\u591a\u6a21\u6001\u914d\u7f6e\uff09\uff0c\u4ee5\u5e94\u5bf9\u534f\u53d8\u91cf\u504f\u79fb\u5e76\u63d0\u5347\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u81ea\u9002\u5e94\u611f\u77e5\u80fd\u4f7f\u5c0f\u6a21\u578b\uff08\u5982EfficientNet-B0\uff09\u8d85\u8d8a\u66f4\u5927\u6a21\u578b\uff08\u5982OpenCLIP-H\uff09\u7684\u6027\u80fd\u3002", "conclusion": "\u8bba\u6587\u547c\u5401\u5c06\u81ea\u9002\u5e94\u611f\u77e5\u6574\u5408\u5230\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u5e76\u63d0\u51fa\u7814\u7a76\u65b9\u5411\uff08\u5982\u6807\u51c6\u5316\u57fa\u51c6\u3001\u5b9e\u65f6\u7b97\u6cd5\u3001\u9690\u79c1\u4fdd\u62a4\u65b9\u6cd5\uff09\uff0c\u4ee5\u63a8\u52a8\u53ef\u6301\u7eed\u3001\u9c81\u68d2\u548c\u516c\u5e73\u7684AI\u7cfb\u7edf\u3002"}}
{"id": "2507.07857", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.07857", "abs": "https://arxiv.org/abs/2507.07857", "authors": ["Samuel Reyd", "Ada Diaconescu", "Jean-Louis Dessalles"], "title": "Searching for actual causes: Approximate algorithms with adjustable precision", "comment": null, "summary": "Causality has gained popularity in recent years. It has helped improve the\nperformance, reliability, and interpretability of machine learning models.\nHowever, recent literature on explainable artificial intelligence (XAI) has\nfaced criticism. The classical XAI and causality literature focuses on\nunderstanding which factors contribute to which consequences. While such\nknowledge is valuable for researchers and engineers, it is not what non-expert\nusers expect as explanations. Instead, these users often await facts that cause\nthe target consequences, i.e., actual causes. Formalizing this notion is still\nan open problem. Additionally, identifying actual causes is reportedly an\nNP-complete problem, and there are too few practical solutions to approximate\nformal definitions. We propose a set of algorithms to identify actual causes\nwith a polynomial complexity and an adjustable level of precision and\nexhaustiveness. Our experiments indicate that the algorithms (1) identify\ncauses for different categories of systems that are not handled by existing\napproaches (i.e., non-boolean, black-box, and stochastic systems), (2) can be\nadjusted to gain more precision and exhaustiveness with more computation time.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u9879\u5f0f\u590d\u6742\u5ea6\u7684\u7b97\u6cd5\uff0c\u7528\u4e8e\u8bc6\u522b\u5b9e\u9645\u539f\u56e0\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406\u7684\u975e\u5e03\u5c14\u3001\u9ed1\u76d2\u548c\u968f\u673a\u7cfb\u7edf\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\uff08XAI\uff09\u548c\u56e0\u679c\u6027\u7814\u7a76\u672a\u80fd\u6ee1\u8db3\u975e\u4e13\u5bb6\u7528\u6237\u5bf9\u89e3\u91ca\u7684\u9700\u6c42\uff0c\u4e14\u8bc6\u522b\u5b9e\u9645\u539f\u56e0\u662f\u4e00\u4e2aNP\u5b8c\u5168\u95ee\u9898\uff0c\u7f3a\u4e4f\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u7ec4\u591a\u9879\u5f0f\u590d\u6742\u5ea6\u7684\u7b97\u6cd5\uff0c\u53ef\u8c03\u6574\u7cbe\u5ea6\u548c\u5168\u9762\u6027\uff0c\u9002\u7528\u4e8e\u975e\u5e03\u5c14\u3001\u9ed1\u76d2\u548c\u968f\u673a\u7cfb\u7edf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7b97\u6cd5\u80fd\u8bc6\u522b\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406\u7684\u7cfb\u7edf\u539f\u56e0\uff0c\u4e14\u53ef\u901a\u8fc7\u589e\u52a0\u8ba1\u7b97\u65f6\u95f4\u63d0\u9ad8\u7cbe\u5ea6\u548c\u5168\u9762\u6027\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u4e3a\u89e3\u51b3\u5b9e\u9645\u539f\u56e0\u8bc6\u522b\u95ee\u9898\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.07893", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.07893", "abs": "https://arxiv.org/abs/2507.07893", "authors": ["Mingda Zhang", "Na Zhao", "Jianglong Qing", "Qing xu", "Kaiwen Pan", "Ting luo"], "title": "An Integrated Framework of Prompt Engineering and Multidimensional Knowledge Graphs for Legal Dispute Analysis", "comment": "15 pages,3 figures", "summary": "The rapid development of artificial intelligence has positioned large\nlanguage models as fundamental components of intelligent legal systems.\nHowever, these models face significant limitations in legal dispute analysis,\nincluding insufficient legal knowledge representation, limited concept\nunderstanding, and reasoning deficiencies. This research proposes an enhanced\nframework integrating prompt engineering with multidimensional knowledge\ngraphs. The framework introduces a three-stage hierarchical prompt structure\ncomprising task definition, knowledge background, and reasoning guidance,\nsupplemented by legal-specific reasoning templates and dynamic optimization\nmechanisms. A three-layer knowledge graph architecture is constructed with\nlegal classification ontology, representation, and instance layers. Four\ncomplementary methods enable precise legal concept retrieval: direct legal norm\ncode matching, domain-specific semantic vector similarity, ontology-based path\nreasoning, and specialized lexical segmentation. These components integrate\nwith web search technology to establish a knowledge-enhanced framework for\nlegal decision-making. Experimental results demonstrate significant performance\nimprovements in legal dispute analysis, enabling accurate legal application\nanalysis for complex cases while exhibiting nuanced understanding of judicial\ndecision-making logic, providing a novel technical approach for implementing\nintelligent legal assistance systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u63d0\u793a\u5de5\u7a0b\u548c\u591a\u7ef4\u77e5\u8bc6\u56fe\u8c31\u7684\u589e\u5f3a\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6cd5\u5f8b\u7ea0\u7eb7\u5206\u6790\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6cd5\u5f8b\u7ea0\u7eb7\u5206\u6790\u4e2d\u5b58\u5728\u6cd5\u5f8b\u77e5\u8bc6\u8868\u793a\u4e0d\u8db3\u3001\u6982\u5ff5\u7406\u89e3\u6709\u9650\u548c\u63a8\u7406\u7f3a\u9677\u7b49\u95ee\u9898\uff0c\u4e9f\u9700\u6539\u8fdb\u3002", "method": "\u63d0\u51fa\u4e09\u9636\u6bb5\u5206\u5c42\u63d0\u793a\u7ed3\u6784\uff08\u4efb\u52a1\u5b9a\u4e49\u3001\u77e5\u8bc6\u80cc\u666f\u3001\u63a8\u7406\u6307\u5bfc\uff09\u548c\u4e09\u5c42\u6b21\u77e5\u8bc6\u56fe\u8c31\u67b6\u6784\uff08\u5206\u7c7b\u672c\u4f53\u3001\u8868\u793a\u5c42\u3001\u5b9e\u4f8b\u5c42\uff09\uff0c\u7ed3\u5408\u56db\u79cd\u7cbe\u51c6\u6cd5\u5f8b\u6982\u5ff5\u68c0\u7d22\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u6cd5\u5f8b\u7ea0\u7eb7\u5206\u6790\u7684\u6027\u80fd\uff0c\u80fd\u51c6\u786e\u5206\u6790\u590d\u6742\u6848\u4ef6\u7684\u6cd5\u5f8b\u9002\u7528\uff0c\u5e76\u6df1\u5165\u7406\u89e3\u53f8\u6cd5\u51b3\u7b56\u903b\u8f91\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u667a\u80fd\u6cd5\u5f8b\u8f85\u52a9\u7cfb\u7edf\u7684\u5b9e\u73b0\u63d0\u4f9b\u4e86\u65b0\u9896\u7684\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2507.07931", "categories": ["cs.AI", "cs.CY", "I.2.0; K.4.1"], "pdf": "https://arxiv.org/pdf/2507.07931", "abs": "https://arxiv.org/abs/2507.07931", "authors": ["Hans Gundlach", "Jayson Lynch", "Neil Thompson"], "title": "Meek Models Shall Inherit the Earth", "comment": "13 pages, 9 figures, longer version of the paper presented at TAIG\n  ICML 2025", "summary": "The past decade has seen incredible scaling of AI systems by a few companies,\nleading to inequality in AI model performance. This paper argues that, contrary\nto prevailing intuition, the diminishing returns to compute scaling will lead\nto a convergence of AI model capabilities. In other words, meek models (those\nwith limited computation budget) shall inherit the earth, approaching the\nperformance level of the best models overall. We develop a model illustrating\nthat under a fixed-distribution next-token objective, the marginal capability\nreturns to raw compute shrink substantially. Given current scaling practices,\nwe argue that these diminishing returns are strong enough that even companies\nthat can scale their models exponentially faster than other organizations will\neventually have little advantage in capabilities. As part of our argument, we\ngive several reasons that proxies like training loss differences capture\nimportant capability measures using evidence from benchmark data and\ntheoretical performance models. In addition, we analyze empirical data on the\ncapability difference of AI models over time. Finally, in light of the\nincreasing ability of meek models, we argue that AI strategy and policy require\nreexamination, and we outline the areas this shift will affect.", "AI": {"tldr": "\u8bba\u6587\u8ba4\u4e3a\uff0c\u968f\u7740\u8ba1\u7b97\u89c4\u6a21\u6536\u76ca\u9012\u51cf\uff0cAI\u6a21\u578b\u80fd\u529b\u5c06\u8d8b\u540c\uff0c\u8d44\u6e90\u6709\u9650\u7684\u6a21\u578b\u5c06\u63a5\u8fd1\u6700\u4f73\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u63a2\u8ba8AI\u6a21\u578b\u6027\u80fd\u4e0d\u5e73\u7b49\u95ee\u9898\uff0c\u63d0\u51fa\u8ba1\u7b97\u89c4\u6a21\u6536\u76ca\u9012\u51cf\u5c06\u5bfc\u81f4\u80fd\u529b\u8d8b\u540c\u7684\u89c2\u70b9\u3002", "method": "\u5efa\u7acb\u6a21\u578b\u5206\u6790\u8ba1\u7b97\u89c4\u6a21\u8fb9\u9645\u6536\u76ca\u9012\u51cf\uff0c\u7ed3\u5408\u57fa\u51c6\u6570\u636e\u548c\u7406\u8bba\u6a21\u578b\u9a8c\u8bc1\u80fd\u529b\u5dee\u5f02\u3002", "result": "\u53d1\u73b0\u8d44\u6e90\u6709\u9650\u7684\u6a21\u578b\u6027\u80fd\u5c06\u63a5\u8fd1\u6700\u4f73\u6a21\u578b\uff0c\u8ba1\u7b97\u4f18\u52bf\u9010\u6e10\u6d88\u5931\u3002", "conclusion": "\u5efa\u8bae\u91cd\u65b0\u5ba1\u89c6AI\u6218\u7565\u4e0e\u653f\u7b56\uff0c\u4ee5\u9002\u5e94\u6a21\u578b\u80fd\u529b\u8d8b\u540c\u7684\u8d8b\u52bf\u3002"}}
{"id": "2507.07935", "categories": ["cs.AI", "cs.CY", "econ.GN", "q-fin.EC"], "pdf": "https://arxiv.org/pdf/2507.07935", "abs": "https://arxiv.org/abs/2507.07935", "authors": ["Kiran Tomlinson", "Sonia Jaffe", "Will Wang", "Scott Counts", "Siddharth Suri"], "title": "Working with AI: Measuring the Occupational Implications of Generative AI", "comment": "40 pages", "summary": "Given the rapid adoption of generative AI and its potential to impact a wide\nrange of tasks, understanding the effects of AI on the economy is one of\nsociety's most important questions. In this work, we take a step toward that\ngoal by analyzing the work activities people do with AI, how successfully and\nbroadly those activities are done, and combine that with data on what\noccupations do those activities. We analyze a dataset of 200k anonymized and\nprivacy-scrubbed conversations between users and Microsoft Bing Copilot, a\npublicly available generative AI system. We find the most common work\nactivities people seek AI assistance for involve gathering information and\nwriting, while the most common activities that AI itself is performing are\nproviding information and assistance, writing, teaching, and advising.\nCombining these activity classifications with measurements of task success and\nscope of impact, we compute an AI applicability score for each occupation. We\nfind the highest AI applicability scores for knowledge work occupation groups\nsuch as computer and mathematical, and office and administrative support, as\nwell as occupations such as sales whose work activities involve providing and\ncommunicating information. Additionally, we characterize the types of work\nactivities performed most successfully, how wage and education correlate with\nAI applicability, and how real-world usage compares to predictions of\noccupational AI impact.", "AI": {"tldr": "\u7814\u7a76\u5206\u6790\u4e86\u751f\u6210\u5f0fAI\u5bf9\u5de5\u4f5c\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u4fe1\u606f\u6536\u96c6\u548c\u5199\u4f5c\u662f\u6700\u5e38\u89c1\u7684AI\u8f85\u52a9\u4efb\u52a1\uff0c\u77e5\u8bc6\u578b\u804c\u4e1a\u7684AI\u9002\u7528\u6027\u6700\u9ad8\u3002", "motivation": "\u7406\u89e3AI\u5bf9\u7ecf\u6d4e\u7684\u5f71\u54cd\u662f\u793e\u4f1a\u7684\u5173\u952e\u95ee\u9898\uff0c\u7814\u7a76\u901a\u8fc7\u5206\u6790AI\u8f85\u52a9\u7684\u5de5\u4f5c\u6d3b\u52a8\u548c\u804c\u4e1a\u9002\u7528\u6027\u8fc8\u51fa\u7b2c\u4e00\u6b65\u3002", "method": "\u5206\u6790\u4e8620\u4e07\u6761\u7528\u6237\u4e0eMicrosoft Bing Copilot\u7684\u533f\u540d\u5bf9\u8bdd\u6570\u636e\uff0c\u7ed3\u5408\u804c\u4e1a\u6d3b\u52a8\u5206\u7c7b\u548c\u4efb\u52a1\u6210\u529f\u5ea6\u91cf\uff0c\u8ba1\u7b97\u804c\u4e1a\u7684AI\u9002\u7528\u6027\u5206\u6570\u3002", "result": "\u4fe1\u606f\u63d0\u4f9b\u3001\u5199\u4f5c\u3001\u6559\u5b66\u548c\u54a8\u8be2\u662fAI\u6700\u5e38\u89c1\u7684\u6d3b\u52a8\uff1b\u8ba1\u7b97\u673a\u3001\u6570\u5b66\u3001\u884c\u653f\u652f\u6301\u548c\u9500\u552e\u7b49\u804c\u4e1a\u7684AI\u9002\u7528\u6027\u6700\u9ad8\u3002", "conclusion": "\u77e5\u8bc6\u578b\u804c\u4e1a\u548c\u6d89\u53ca\u4fe1\u606f\u6c9f\u901a\u7684\u804c\u4e1a\u6700\u6613\u53d7AI\u5f71\u54cd\uff0c\u7814\u7a76\u4e3aAI\u5bf9\u804c\u4e1a\u7684\u6f5c\u5728\u5f71\u54cd\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u4f9d\u636e\u3002"}}
