{"id": "2507.19555", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19555", "abs": "https://arxiv.org/abs/2507.19555", "authors": ["Rajat Khanda", "Mohammad Baqar", "Sambuddha Chakrabarti", "Satyasaran Changdar"], "title": "Extending Group Relative Policy Optimization to Continuous Control: A Theoretical Framework for Robotic Reinforcement Learning", "comment": "13 pages, 2 figures", "summary": "Group Relative Policy Optimization (GRPO) has shown promise in discrete\naction spaces by eliminating value function dependencies through group-based\nadvantage estimation. However, its application to continuous control remains\nunexplored, limiting its utility in robotics where continuous actions are\nessential. This paper presents a theoretical framework extending GRPO to\ncontinuous control environments, addressing challenges in high-dimensional\naction spaces, sparse rewards, and temporal dynamics. Our approach introduces\ntrajectory-based policy clustering, state-aware advantage estimation, and\nregularized policy updates designed for robotic applications. We provide\ntheoretical analysis of convergence properties and computational complexity,\nestablishing a foundation for future empirical validation in robotic systems\nincluding locomotion and manipulation tasks.", "AI": {"tldr": "GRPO\u6269\u5c55\u5230\u8fde\u7eed\u63a7\u5236\u73af\u5883\uff0c\u63d0\u51fa\u8f68\u8ff9\u805a\u7c7b\u3001\u72b6\u6001\u611f\u77e5\u4f18\u52bf\u4f30\u8ba1\u548c\u6b63\u5219\u5316\u7b56\u7565\u66f4\u65b0\uff0c\u9002\u7528\u4e8e\u673a\u5668\u4eba\u4efb\u52a1\u3002", "motivation": "GRPO\u5728\u79bb\u6563\u52a8\u4f5c\u7a7a\u95f4\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u8fde\u7eed\u63a7\u5236\u4e2d\u5c1a\u672a\u63a2\u7d22\uff0c\u800c\u673a\u5668\u4eba\u9886\u57df\u9700\u8981\u8fde\u7eed\u52a8\u4f5c\u3002", "method": "\u5f15\u5165\u8f68\u8ff9\u805a\u7c7b\u3001\u72b6\u6001\u611f\u77e5\u4f18\u52bf\u4f30\u8ba1\u548c\u6b63\u5219\u5316\u7b56\u7565\u66f4\u65b0\uff0c\u89e3\u51b3\u9ad8\u7ef4\u52a8\u4f5c\u7a7a\u95f4\u3001\u7a00\u758f\u5956\u52b1\u548c\u65f6\u5e8f\u52a8\u6001\u95ee\u9898\u3002", "result": "\u63d0\u4f9b\u7406\u8bba\u5206\u6790\uff0c\u8bc1\u660e\u6536\u655b\u6027\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u4e3a\u540e\u7eed\u673a\u5668\u4eba\u4efb\u52a1\u9a8c\u8bc1\u5960\u5b9a\u57fa\u7840\u3002", "conclusion": "GRPO\u6269\u5c55\u81f3\u8fde\u7eed\u63a7\u5236\u53ef\u884c\uff0c\u4e3a\u673a\u5668\u4eba\u5e94\u7528\u63d0\u4f9b\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2507.19642", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.19642", "abs": "https://arxiv.org/abs/2507.19642", "authors": ["Ahmad Suleman", "Misha Urooj Khan", "Zeeshan Kaleem", "Ali H. Alenezi", "Iqra Shabbir Sinem Coleri", "Chau Yuen"], "title": "Reward-Augmented Reinforcement Learning for Continuous Control in Precision Autonomous Parking via Policy Optimization Methods", "comment": null, "summary": "Autonomous parking (AP) represents a critical yet complex subset of\nintelligent vehicle automation, characterized by tight spatial constraints,\nfrequent close-range obstacle interactions, and stringent safety margins.\nHowever, conventional rule-based and model-predictive methods often lack the\nadaptability and generalization needed to handle the nonlinear and\nenvironment-dependent complexities of AP. To address these limitations, we\npropose a reward-augmented learning framework for AP (RARLAP), that mitigates\nthe inherent complexities of continuous-domain control by leveraging structured\nreward design to induce smooth and adaptable policy behavior, trained entirely\nwithin a high-fidelity Unity-based custom 3D simulation environment. We\nsystematically design and assess three structured reward strategies: goal-only\nreward (GOR), dense proximity reward (DPR), and milestone-augmented reward\n(MAR), each integrated with both on-policy and off-policy optimization\nparadigms. Empirical evaluations demonstrate that the on-policy MAR achieves a\n91\\% success rate, yielding smoother trajectories and more robust behavior,\nwhile GOR and DPR fail to guide effective learning. Convergence and trajectory\nanalyses demonstrate that the proposed framework enhances policy adaptability,\naccelerates training, and improves safety in continuous control. Overall,\nRARLAP establishes that reward augmentation effectively addresses complex\nautonomous parking challenges, enabling scalable and efficient policy\noptimization with both on- and off-policy methods. To support reproducibility,\nthe code accompanying this paper is publicly available.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5956\u52b1\u589e\u5f3a\u5b66\u4e60\u6846\u67b6\uff08RARLAP\uff09\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u5956\u52b1\u8bbe\u8ba1\u89e3\u51b3\u81ea\u4e3b\u6cca\u8f66\u7684\u590d\u6742\u6027\u95ee\u9898\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u663e\u8457\u63d0\u5347\u4e86\u7b56\u7565\u9002\u5e94\u6027\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u89c4\u5219\u548c\u6a21\u578b\u9884\u6d4b\u7684\u65b9\u6cd5\u5728\u81ea\u4e3b\u6cca\u8f66\u4e2d\u7f3a\u4e4f\u9002\u5e94\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u65e0\u6cd5\u5904\u7406\u975e\u7ebf\u6027\u53ca\u73af\u5883\u4f9d\u8d56\u7684\u590d\u6742\u6027\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e09\u79cd\u7ed3\u6784\u5316\u5956\u52b1\u7b56\u7565\uff08GOR\u3001DPR\u3001MAR\uff09\uff0c\u7ed3\u5408\u4e86\u540c\u7b56\u7565\u548c\u5f02\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\uff0c\u5e76\u5728\u9ad8\u4fdd\u771fUnity\u4eff\u771f\u73af\u5883\u4e2d\u8bad\u7ec3\u3002", "result": "\u540c\u7b56\u7565MAR\u5b9e\u73b0\u4e8691%\u7684\u6210\u529f\u7387\uff0c\u8f68\u8ff9\u66f4\u5e73\u6ed1\u4e14\u884c\u4e3a\u66f4\u9c81\u68d2\uff0c\u800cGOR\u548cDPR\u672a\u80fd\u6709\u6548\u5f15\u5bfc\u5b66\u4e60\u3002", "conclusion": "\u5956\u52b1\u589e\u5f3a\u5b66\u4e60\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u4e3b\u6cca\u8f66\u7684\u590d\u6742\u6027\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u7b56\u7565\u4f18\u5316\u7684\u53ef\u6269\u5c55\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2507.19647", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.19647", "abs": "https://arxiv.org/abs/2507.19647", "authors": ["Amin Banayeeanzade", "Fatemeh Bahrani", "Yutai Zhou", "Erdem B\u0131y\u0131k"], "title": "GABRIL: Gaze-Based Regularization for Mitigating Causal Confusion in Imitation Learning", "comment": "IROS 2025 camera-ready version. First two authors contributed equally", "summary": "Imitation Learning (IL) is a widely adopted approach which enables agents to\nlearn from human expert demonstrations by framing the task as a supervised\nlearning problem. However, IL often suffers from causal confusion, where agents\nmisinterpret spurious correlations as causal relationships, leading to poor\nperformance in testing environments with distribution shift. To address this\nissue, we introduce GAze-Based Regularization in Imitation Learning (GABRIL), a\nnovel method that leverages the human gaze data gathered during the data\ncollection phase to guide the representation learning in IL. GABRIL utilizes a\nregularization loss which encourages the model to focus on causally relevant\nfeatures identified through expert gaze and consequently mitigates the effects\nof confounding variables. We validate our approach in Atari environments and\nthe Bench2Drive benchmark in CARLA by collecting human gaze datasets and\napplying our method in both domains. Experimental results show that the\nimprovement of GABRIL over behavior cloning is around 179% more than the same\nnumber for other baselines in the Atari and 76% in the CARLA setup. Finally, we\nshow that our method provides extra explainability when compared to regular IL\nagents.", "AI": {"tldr": "GABRIL\u5229\u7528\u4eba\u7c7b\u6ce8\u89c6\u6570\u636e\u6539\u8fdb\u6a21\u4eff\u5b66\u4e60\uff0c\u901a\u8fc7\u6b63\u5219\u5316\u635f\u5931\u51cf\u5c11\u56e0\u679c\u6df7\u6dc6\uff0c\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u6a21\u4eff\u5b66\u4e60\u5e38\u56e0\u56e0\u679c\u6df7\u6dc6\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u9700\u89e3\u51b3\u6b64\u95ee\u9898\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u6ce8\u89c6\u6570\u636e\u7684\u6b63\u5219\u5316\u635f\u5931\uff0c\u5f15\u5bfc\u6a21\u578b\u5173\u6ce8\u56e0\u679c\u76f8\u5173\u7279\u5f81\u3002", "result": "\u5728Atari\u548cCARLA\u4e2d\uff0cGABRIL\u5206\u522b\u6bd4\u57fa\u7ebf\u63d0\u5347179%\u548c76%\u3002", "conclusion": "GABRIL\u6709\u6548\u51cf\u5c11\u56e0\u679c\u6df7\u6dc6\uff0c\u63d0\u5347\u6027\u80fd\u5e76\u63d0\u4f9b\u989d\u5916\u89e3\u91ca\u6027\u3002"}}
{"id": "2507.19652", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.19652", "abs": "https://arxiv.org/abs/2507.19652", "authors": ["Mattia Risiglione", "Abdelrahman Abdalla", "Victor Barasuol", "Kim Tien Ly", "Ioannis Havoutis", "Claudio Semini"], "title": "RAKOMO: Reachability-Aware K-Order Markov Path Optimization for Quadrupedal Loco-Manipulation", "comment": null, "summary": "Legged manipulators, such as quadrupeds equipped with robotic arms, require\nmotion planning techniques that account for their complex kinematic constraints\nin order to perform manipulation tasks both safely and effectively. However,\ntrajectory optimization methods often face challenges due to the hybrid\ndynamics introduced by contact discontinuities, and tend to neglect leg\nlimitations during planning for computational reasons. In this work, we propose\nRAKOMO, a path optimization technique that integrates the strengths of K-Order\nMarkov Optimization (KOMO) with a kinematically-aware criterion based on the\nreachable region defined as reachability margin. We leverage a neural-network\nto predict the margin and optimize it by incorporating it in the standard KOMO\nformulation. This approach enables rapid convergence of gradient-based motion\nplanning -- commonly tailored for continuous systems -- while adapting it\neffectively to legged manipulators, successfully executing loco-manipulation\ntasks. We benchmark RAKOMO against a baseline KOMO approach through a set of\nsimulations for pick-and-place tasks with the HyQReal quadruped robot equipped\nwith a Kinova Gen3 robotic arm.", "AI": {"tldr": "RAKOMO\u662f\u4e00\u79cd\u7ed3\u5408K-Order Markov Optimization\uff08KOMO\uff09\u548c\u57fa\u4e8e\u53ef\u8fbe\u6027\u8fb9\u754c\u7684\u8fd0\u52a8\u89c4\u5212\u6280\u672f\uff0c\u7528\u4e8e\u89e3\u51b3\u56db\u8db3\u673a\u5668\u4eba\u64cd\u7eb5\u4efb\u52a1\u4e2d\u7684\u8fd0\u52a8\u89c4\u5212\u95ee\u9898\u3002", "motivation": "\u56db\u8db3\u673a\u5668\u4eba\u64cd\u7eb5\u4efb\u52a1\u4e2d\u7684\u8fd0\u52a8\u89c4\u5212\u9762\u4e34\u590d\u6742\u8fd0\u52a8\u5b66\u7ea6\u675f\u548c\u63a5\u89e6\u4e0d\u8fde\u7eed\u6027\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u5e38\u56e0\u8ba1\u7b97\u539f\u56e0\u5ffd\u7565\u817f\u90e8\u9650\u5236\u3002", "method": "\u63d0\u51faRAKOMO\uff0c\u7ed3\u5408KOMO\u548c\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u7684\u53ef\u8fbe\u6027\u8fb9\u754c\uff0c\u4f18\u5316\u8fd0\u52a8\u89c4\u5212\u3002", "result": "RAKOMO\u5728\u4eff\u771f\u5b9e\u9a8c\u4e2d\u6210\u529f\u6267\u884c\u4e86\u56db\u8db3\u673a\u5668\u4ebaHyQReal\u7684\u62fe\u53d6\u4efb\u52a1\uff0c\u4f18\u4e8e\u57fa\u7ebfKOMO\u65b9\u6cd5\u3002", "conclusion": "RAKOMO\u6709\u6548\u89e3\u51b3\u4e86\u56db\u8db3\u673a\u5668\u4eba\u64cd\u7eb5\u4efb\u52a1\u7684\u8fd0\u52a8\u89c4\u5212\u95ee\u9898\uff0c\u5177\u6709\u5feb\u901f\u6536\u655b\u548c\u9002\u5e94\u6027\u5f3a\u7684\u7279\u70b9\u3002"}}
{"id": "2507.19489", "categories": ["cs.AI", "cs.CV", "cs.HC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.19489", "abs": "https://arxiv.org/abs/2507.19489", "authors": ["Simone Bendazzoli", "Sanna Persson", "Mehdi Astaraki", "Sebastian Pettersson", "Vitali Grozman", "Rodrigo Moreno"], "title": "MAIA: A Collaborative Medical AI Platform for Integrated Healthcare Innovation", "comment": "26 pages, 12 figures", "summary": "The integration of Artificial Intelligence (AI) into clinical workflows\nrequires robust collaborative platforms that are able to bridge the gap between\ntechnical innovation and practical healthcare applications. This paper\nintroduces MAIA (Medical Artificial Intelligence Assistant), an open-source\nplatform designed to facilitate interdisciplinary collaboration among\nclinicians, researchers, and AI developers. Built on Kubernetes, MAIA offers a\nmodular, scalable environment with integrated tools for data management, model\ndevelopment, annotation, deployment, and clinical feedback. Key features\ninclude project isolation, CI/CD automation, integration with high-computing\ninfrastructures and in clinical workflows. MAIA supports real-world use cases\nin medical imaging AI, with deployments in both academic and clinical\nenvironments. By promoting collaborations and interoperability, MAIA aims to\naccelerate the translation of AI research into impactful clinical solutions\nwhile promoting reproducibility, transparency, and user-centered design. We\nshowcase the use of MAIA with different projects, both at KTH Royal Institute\nof Technology and Karolinska University Hospital.", "AI": {"tldr": "MAIA\u662f\u4e00\u4e2a\u5f00\u6e90\u5e73\u53f0\uff0c\u65e8\u5728\u4fc3\u8fdb\u4e34\u5e8a\u3001\u7814\u7a76\u548cAI\u5f00\u53d1\u8005\u4e4b\u95f4\u7684\u534f\u4f5c\uff0c\u52a0\u901fAI\u5728\u533b\u7597\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u89e3\u51b3AI\u6280\u672f\u4e0e\u5b9e\u9645\u533b\u7597\u5e94\u7528\u4e4b\u95f4\u7684\u8131\u8282\u95ee\u9898\uff0c\u63a8\u52a8\u8de8\u5b66\u79d1\u5408\u4f5c\u3002", "method": "\u57fa\u4e8eKubernetes\u6784\u5efa\uff0c\u63d0\u4f9b\u6a21\u5757\u5316\u3001\u53ef\u6269\u5c55\u7684\u73af\u5883\uff0c\u96c6\u6210\u6570\u636e\u7ba1\u7406\u3001\u6a21\u578b\u5f00\u53d1\u548c\u90e8\u7f72\u5de5\u5177\u3002", "result": "\u6210\u529f\u5e94\u7528\u4e8e\u533b\u5b66\u5f71\u50cfAI\u9879\u76ee\uff0c\u652f\u6301\u5b66\u672f\u548c\u4e34\u5e8a\u73af\u5883\u4e2d\u7684\u5b9e\u9645\u7528\u4f8b\u3002", "conclusion": "MAIA\u901a\u8fc7\u4fc3\u8fdb\u534f\u4f5c\u548c\u4e92\u64cd\u4f5c\u6027\uff0c\u52a0\u901fAI\u7814\u7a76\u5411\u4e34\u5e8a\u89e3\u51b3\u65b9\u6848\u7684\u8f6c\u5316\u3002"}}
{"id": "2507.19701", "categories": ["cs.RO", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.19701", "abs": "https://arxiv.org/abs/2507.19701", "authors": ["Haichuan Li", "Tomi Westerlund"], "title": "PhysVarMix: Physics-Informed Variational Mixture Model for Multi-Modal Trajectory Prediction", "comment": null, "summary": "Accurate prediction of future agent trajectories is a critical challenge for\nensuring safe and efficient autonomous navigation, particularly in complex\nurban environments characterized by multiple plausible future scenarios. In\nthis paper, we present a novel hybrid approach that integrates learning-based\nwith physics-based constraints to address the multi-modality inherent in\ntrajectory prediction. Our method employs a variational Bayesian mixture model\nto effectively capture the diverse range of potential future behaviors, moving\nbeyond traditional unimodal assumptions. Unlike prior approaches that\npredominantly treat trajectory prediction as a data-driven regression task, our\nframework incorporates physical realism through sector-specific boundary\nconditions and Model Predictive Control (MPC)-based smoothing. These\nconstraints ensure that predicted trajectories are not only data-consistent but\nalso physically plausible, adhering to kinematic and dynamic principles.\nFurthermore, our method produces interpretable and diverse trajectory\npredictions, enabling enhanced downstream decision-making and planning in\nautonomous driving systems. We evaluate our approach on two benchmark datasets,\ndemonstrating superior performance compared to existing methods. Comprehensive\nablation studies validate the contributions of each component and highlight\ntheir synergistic impact on prediction accuracy and reliability. By balancing\ndata-driven insights with physics-informed constraints, our approach offers a\nrobust and scalable solution for navigating the uncertainties of real-world\nurban environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5b66\u4e60\u4e0e\u7269\u7406\u7ea6\u675f\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u7528\u4e8e\u591a\u6a21\u6001\u8f68\u8ff9\u9884\u6d4b\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u590d\u6742\u57ce\u5e02\u73af\u5883\u4e2d\u591a\u6a21\u6001\u8f68\u8ff9\u9884\u6d4b\u7684\u6311\u6218\uff0c\u786e\u4fdd\u9884\u6d4b\u7684\u7269\u7406\u5408\u7406\u6027\u548c\u6570\u636e\u4e00\u81f4\u6027\u3002", "method": "\u4f7f\u7528\u53d8\u5206\u8d1d\u53f6\u65af\u6df7\u5408\u6a21\u578b\u6355\u6349\u591a\u6a21\u6001\u884c\u4e3a\uff0c\u7ed3\u5408\u7269\u7406\u7ea6\u675f\u548cMPC\u5e73\u6ed1\u3002", "result": "\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u7ec4\u4ef6\u7684\u534f\u540c\u4f5c\u7528\u3002", "conclusion": "\u901a\u8fc7\u5e73\u8861\u6570\u636e\u9a71\u52a8\u548c\u7269\u7406\u7ea6\u675f\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9c81\u68d2\u4e14\u53ef\u6269\u5c55\u7684\u8f68\u8ff9\u9884\u6d4b\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.19543", "categories": ["cs.AI", "cs.MA", "I.2.11; I.2.7"], "pdf": "https://arxiv.org/pdf/2507.19543", "abs": "https://arxiv.org/abs/2507.19543", "authors": ["Maria Emilia Mazzolenis", "Ruirui Zhang"], "title": "Agent WARPP: Workflow Adherence via Runtime Parallel Personalization", "comment": "Accepted at the ICML 2025 Workshop on Multi-Agent Systems in the Era\n  of Foundation Models: Opportunities, Challenges, and Futures. Code repo:\n  https://github.com/emiliamazzo/WARPP/", "summary": "Large language models (LLMs) are increasingly applied in task-oriented\ndialogue (TOD) systems but often struggle with long, conditional workflows that\ninvolve external tool calls and depend on user-specific information. We present\nWorkflow Adherence via Runtime Parallel Personalization, or WARPP, a\ntraining-free, modular framework that combines multi-agent orchestration with\nruntime personalization to improve workflow adherence in LLM-based systems. By\ndynamically pruning conditional branches based on user attributes, the\nframework reduces reasoning overhead and narrows tool selection at runtime.\nWARPP deploys a parallelized architecture where a dedicated Personalizer agent\noperates alongside modular, domain-specific agents to dynamically tailor\nexecution paths in real time. The framework is evaluated across five\nrepresentative user intents of varying complexity within three domains:\nbanking, flights, and healthcare. Our evaluation leverages synthetic datasets\nand LLM-powered simulated users to test scenarios with conditional\ndependencies. Our results demonstrate that WARPP outperforms both the\nnon-personalized method and the ReAct baseline, achieving increasingly larger\ngains in parameter fidelity and tool accuracy as intent complexity grows, while\nalso reducing average token usage, without any additional training.", "AI": {"tldr": "WARPP\u662f\u4e00\u4e2a\u65e0\u9700\u8bad\u7ec3\u3001\u6a21\u5757\u5316\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u7f16\u6392\u548c\u8fd0\u884c\u65f6\u4e2a\u6027\u5316\uff0c\u63d0\u5347\u57fa\u4e8eLLM\u7684\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\u7cfb\u7edf\u5728\u590d\u6742\u5de5\u4f5c\u6d41\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u4efb\u52a1\u5bfc\u5411\u5bf9\u8bdd\u7cfb\u7edf\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u5728\u5904\u7406\u6d89\u53ca\u5916\u90e8\u5de5\u5177\u8c03\u7528\u548c\u7528\u6237\u7279\u5b9a\u4fe1\u606f\u7684\u590d\u6742\u6761\u4ef6\u5de5\u4f5c\u6d41\u65f6\u8868\u73b0\u4e0d\u4f73\u3002", "method": "WARPP\u91c7\u7528\u5e76\u884c\u67b6\u6784\uff0c\u7ed3\u5408\u4e2a\u6027\u5316\u4ee3\u7406\u548c\u9886\u57df\u7279\u5b9a\u4ee3\u7406\uff0c\u52a8\u6001\u526a\u679d\u6761\u4ef6\u5206\u652f\u4ee5\u4f18\u5316\u6267\u884c\u8def\u5f84\u3002", "result": "\u5728\u94f6\u884c\u3001\u822a\u73ed\u548c\u533b\u7597\u4e09\u4e2a\u9886\u57df\u7684\u8bc4\u4f30\u4e2d\uff0cWARPP\u5728\u53c2\u6570\u4fdd\u771f\u5ea6\u548c\u5de5\u5177\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u975e\u4e2a\u6027\u5316\u65b9\u6cd5\u548cReAct\u57fa\u7ebf\uff0c\u540c\u65f6\u51cf\u5c11\u4ee4\u724c\u4f7f\u7528\u3002", "conclusion": "WARPP\u901a\u8fc7\u8fd0\u884c\u65f6\u4e2a\u6027\u5316\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u5de5\u4f5c\u6d41\u7684\u5904\u7406\u80fd\u529b\uff0c\u4e14\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002"}}
{"id": "2507.19742", "categories": ["cs.RO", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.19742", "abs": "https://arxiv.org/abs/2507.19742", "authors": ["Yanbin Li", "Canran Xiao", "Hongyang He", "Shenghai Yuan", "Zong Ke", "Jiajie Yu", "Zixiong Qin", "Zhiguo Zhang", "Wenzheng Chi", "Wei Zhang"], "title": "DOA: A Degeneracy Optimization Agent with Adaptive Pose Compensation Capability based on Deep Reinforcement Learning", "comment": "10 pages,9 figures", "summary": "Particle filter-based 2D-SLAM is widely used in indoor localization tasks due\nto its efficiency. However, indoor environments such as long straight corridors\ncan cause severe degeneracy problems in SLAM. In this paper, we use Proximal\nPolicy Optimization (PPO) to train an adaptive degeneracy optimization agent\n(DOA) to address degeneracy problem. We propose a systematic methodology to\naddress three critical challenges in traditional supervised learning\nframeworks: (1) data acquisition bottlenecks in degenerate dataset, (2)\ninherent quality deterioration of training samples, and (3) ambiguity in\nannotation protocol design. We design a specialized reward function to guide\nthe agent in developing perception capabilities for degenerate environments.\nUsing the output degeneracy factor as a reference weight, the agent can\ndynamically adjust the contribution of different sensors to pose optimization.\nSpecifically, the observation distribution is shifted towards the motion model\ndistribution, with the step size determined by a linear interpolation formula\nrelated to the degeneracy factor. In addition, we employ a transfer learning\nmodule to endow the agent with generalization capabilities across different\nenvironments and address the inefficiency of training in degenerate\nenvironments. Finally, we conduct ablation studies to demonstrate the\nrationality of our model design and the role of transfer learning. We also\ncompare the proposed DOA with SOTA methods to prove its superior degeneracy\ndetection and optimization capabilities across various environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8ePPO\u7684\u81ea\u9002\u5e94\u9000\u5316\u4f18\u5316\u4ee3\u7406\uff08DOA\uff09\uff0c\u7528\u4e8e\u89e3\u51b3SLAM\u5728\u957f\u76f4\u8d70\u5eca\u7b49\u73af\u5883\u4e2d\u7684\u9000\u5316\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5956\u52b1\u51fd\u6570\u548c\u8fc1\u79fb\u5b66\u4e60\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5ba4\u5185\u73af\u5883\uff08\u5982\u957f\u76f4\u8d70\u5eca\uff09\u4f1a\u5bfc\u81f4SLAM\u7684\u4e25\u91cd\u9000\u5316\u95ee\u9898\uff0c\u4f20\u7edf\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u5728\u6570\u636e\u83b7\u53d6\u3001\u6837\u672c\u8d28\u91cf\u548c\u6807\u6ce8\u534f\u8bae\u8bbe\u8ba1\u4e0a\u5b58\u5728\u6311\u6218\u3002", "method": "\u4f7f\u7528PPO\u8bad\u7ec3DOA\uff0c\u8bbe\u8ba1\u5956\u52b1\u51fd\u6570\u5f15\u5bfc\u4ee3\u7406\u611f\u77e5\u9000\u5316\u73af\u5883\uff0c\u52a8\u6001\u8c03\u6574\u4f20\u611f\u5668\u8d21\u732e\uff0c\u5e76\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660eDOA\u5728\u9000\u5316\u68c0\u6d4b\u548c\u4f18\u5316\u80fd\u529b\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u8fc1\u79fb\u5b66\u4e60\u663e\u8457\u63d0\u5347\u6548\u7387\u3002", "conclusion": "DOA\u80fd\u6709\u6548\u89e3\u51b3SLAM\u9000\u5316\u95ee\u9898\uff0c\u5177\u6709\u6cdb\u5316\u6027\u548c\u9ad8\u6548\u6027\u3002"}}
{"id": "2507.19593", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.19593", "abs": "https://arxiv.org/abs/2507.19593", "authors": ["Vince Trencsenyi", "Agnieszka Mensfelt", "Kostas Stathis"], "title": "Hypergames: Modeling Misaligned Perceptions and Nested Beliefs for Multi-agent Systems", "comment": null, "summary": "Classical game-theoretic models typically assume rational agents, complete\ninformation, and common knowledge of payoffs - assumptions that are often\nviolated in real-world MAS characterized by uncertainty, misaligned\nperceptions, and nested beliefs. To overcome these limitations, researchers\nhave proposed extensions that incorporate models of cognitive constraints,\nsubjective beliefs, and heterogeneous reasoning. Among these, hypergame theory\nextends the classical paradigm by explicitly modeling agents' subjective\nperceptions of the strategic scenario, known as perceptual games, in which\nagents may hold divergent beliefs about the structure, payoffs, or available\nactions. We present a systematic review of agent-compatible applications of\nhypergame theory, examining how its descriptive capabilities have been adapted\nto dynamic and interactive MAS contexts. We analyze 44 selected studies from\ncybersecurity, robotics, social simulation, communications, and general\ngame-theoretic modeling. Building on a formal introduction to hypergame theory\nand its two major extensions - hierarchical hypergames and HNF - we develop\nagent-compatibility criteria and an agent-based classification framework to\nassess integration patterns and practical applicability. Our analysis reveals\nprevailing tendencies, including the prevalence of hierarchical and graph-based\nmodels in deceptive reasoning and the simplification of extensive theoretical\nframeworks in practical applications. We identify structural gaps, including\nthe limited adoption of HNF-based models, the lack of formal hypergame\nlanguages, and unexplored opportunities for modeling human-agent and\nagent-agent misalignment. By synthesizing trends, challenges, and open research\ndirections, this review provides a new roadmap for applying hypergame theory to\nenhance the realism and effectiveness of strategic modeling in dynamic\nmulti-agent environments.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u8d85\u535a\u5f08\u7406\u8bba\u5728\u52a8\u6001\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\uff0c\u5206\u6790\u4e8644\u9879\u7814\u7a76\uff0c\u63d0\u51fa\u4e86\u667a\u80fd\u4f53\u517c\u5bb9\u6027\u6807\u51c6\u548c\u5206\u7c7b\u6846\u67b6\uff0c\u5e76\u6307\u51fa\u4e86\u5f53\u524d\u7814\u7a76\u7684\u5c40\u9650\u4e0e\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u4f20\u7edf\u535a\u5f08\u8bba\u5047\u8bbe\u7406\u6027\u3001\u5b8c\u5168\u4fe1\u606f\u548c\u5171\u540c\u77e5\u8bc6\uff0c\u4f46\u73b0\u5b9e\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5e38\u5b58\u5728\u4e0d\u786e\u5b9a\u6027\u548c\u8ba4\u77e5\u5dee\u5f02\u3002\u8d85\u535a\u5f08\u7406\u8bba\u901a\u8fc7\u5efa\u6a21\u4e3b\u89c2\u611f\u77e5\u6e38\u620f\u6765\u5f25\u8865\u8fd9\u4e9b\u4e0d\u8db3\u3002", "method": "\u7cfb\u7edf\u56de\u987e44\u9879\u7814\u7a76\uff0c\u63d0\u51fa\u667a\u80fd\u4f53\u517c\u5bb9\u6027\u6807\u51c6\u548c\u5206\u7c7b\u6846\u67b6\uff0c\u5206\u6790\u8d85\u535a\u5f08\u7406\u8bba\u53ca\u5176\u6269\u5c55\uff08\u5982\u5206\u5c42\u8d85\u535a\u5f08\u548cHNF\uff09\u7684\u5e94\u7528\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5206\u5c42\u548c\u56fe\u6a21\u578b\u5728\u6b3a\u9a97\u63a8\u7406\u4e2d\u5360\u4e3b\u5bfc\uff0c\u4f46HNF\u6a21\u578b\u5e94\u7528\u6709\u9650\uff0c\u7f3a\u4e4f\u5f62\u5f0f\u5316\u8bed\u8a00\uff0c\u4e14\u4eba\u673a/\u673a\u673a\u8ba4\u77e5\u504f\u5dee\u5efa\u6a21\u672a\u5145\u5206\u63a2\u7d22\u3002", "conclusion": "\u672c\u6587\u4e3a\u8d85\u535a\u5f08\u7406\u8bba\u5728\u52a8\u6001\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u8def\u7ebf\u56fe\uff0c\u5f3a\u8c03\u9700\u589e\u5f3a\u7406\u8bba\u6846\u67b6\u7684\u5b9e\u7528\u6027\u548c\u5efa\u6a21\u80fd\u529b\u3002"}}
{"id": "2507.19760", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.19760", "abs": "https://arxiv.org/abs/2507.19760", "authors": ["Alberto Confente", "Takanori Jin", "Taisuke Kobayashi", "Julio Rogelio Guadarrama-Olvera", "Gordon Cheng"], "title": "Skin-Machine Interface with Multimodal Contact Motion Classifier", "comment": "8 pages, 8 figures (accepted in Humanoids2025)", "summary": "This paper proposes a novel framework for utilizing skin sensors as a new\noperation interface of complex robots. The skin sensors employed in this study\npossess the capability to quantify multimodal tactile information at multiple\ncontact points. The time-series data generated from these sensors is\nanticipated to facilitate the classification of diverse contact motions\nexhibited by an operator. By mapping the classification results with robot\nmotion primitives, a diverse range of robot motions can be generated by\naltering the manner in which the skin sensors are interacted with. In this\npaper, we focus on a learning-based contact motion classifier employing\nrecurrent neural networks. This classifier is a pivotal factor in the success\nof this framework. Furthermore, we elucidate the requisite conditions for\nsoftware-hardware designs. Firstly, multimodal sensing and its comprehensive\nencoding significantly contribute to the enhancement of classification accuracy\nand learning stability. Utilizing all modalities simultaneously as inputs to\nthe classifier proves to be an effective approach. Secondly, it is essential to\nmount the skin sensors on a flexible and compliant support to enable the\nactivation of three-axis accelerometers. These accelerometers are capable of\nmeasuring horizontal tactile information, thereby enhancing the correlation\nwith other modalities. Furthermore, they serve to absorb the noises generated\nby the robot's movements during deployment. Through these discoveries, the\naccuracy of the developed classifier surpassed 95 %, enabling the dual-arm\nmobile manipulator to execute a diverse range of tasks via the Skin-Machine\nInterface. https://youtu.be/UjUXT4Z4BC8", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u76ae\u80a4\u4f20\u611f\u5668\u4f5c\u4e3a\u590d\u6742\u673a\u5668\u4eba\u64cd\u4f5c\u754c\u9762\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u89e6\u89c9\u4fe1\u606f\u5206\u7c7b\u5b9e\u73b0\u591a\u6837\u5316\u673a\u5668\u4eba\u52a8\u4f5c\u3002", "motivation": "\u63a2\u7d22\u76ae\u80a4\u4f20\u611f\u5668\u4f5c\u4e3a\u65b0\u578b\u64cd\u4f5c\u754c\u9762\u7684\u6f5c\u529b\uff0c\u4ee5\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u591a\u6837\u6027\u548c\u7075\u6d3b\u6027\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u7684\u63a5\u89e6\u52a8\u4f5c\u5206\u7c7b\u5668\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u4f20\u611f\u548c\u67d4\u6027\u652f\u6491\u8bbe\u8ba1\u3002", "result": "\u5206\u7c7b\u5668\u51c6\u786e\u7387\u8d85\u8fc795%\uff0c\u6210\u529f\u9a71\u52a8\u53cc\u81c2\u79fb\u52a8\u673a\u68b0\u81c2\u5b8c\u6210\u591a\u6837\u5316\u4efb\u52a1\u3002", "conclusion": "\u591a\u6a21\u6001\u4f20\u611f\u548c\u67d4\u6027\u652f\u6491\u8bbe\u8ba1\u662f\u63d0\u5347\u5206\u7c7b\u5668\u6027\u80fd\u7684\u5173\u952e\uff0c\u76ae\u80a4\u4f20\u611f\u5668\u754c\u9762\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.19608", "categories": ["cs.AI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.19608", "abs": "https://arxiv.org/abs/2507.19608", "authors": ["Jiawen Qi", "Chang Gao", "Zhaochun Ren", "Qinyu Chen"], "title": "DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference", "comment": null, "summary": "Deploying Large Language Models (LLMs) on edge devices remains challenging\ndue to their quadratically increasing computations with the sequence length.\nExisting studies for dynamic attention pruning are designed for hardware with\nmassively parallel computation capabilities, such as GPUs or TPUs, and aim at\nlong context lengths (e.g., 64K), making them unsuitable for edge scenarios. We\npresent DeltaLLM, a training-free framework that exploits temporal sparsity in\nattention patterns to enable efficient LLM inference across both the prefilling\nand decoding stages, on resource-constrained edge devices. DeltaLLM introduces\nan accuracy- and memory-aware delta matrix construction strategy that\nintroduces temporal sparsity, and a context-aware hybrid attention mechanism\nthat combines full attention in a local context window with delta approximation\noutside it to increase accuracy. We evaluate our framework on the\nedge-device-friendly BitNet-b1.58-2B-4T model and Llama3.2-1B-Instruct model\nacross diverse language tasks. The results show that on BitNet, our framework\nincreases the attention sparsity from 0% to 60% during the prefilling stage\nwith slight accuracy improvement on the WG task, and 0% to 57% across both the\nprefilling and decoding stages, with even higher F1 score from 29.63 to 30.97\non SQuAD-v2 task. On the Llama model, it can also achieve up to 60% sparsity\nduring the prefilling stage and around 57% across both stages with negligible\naccuracy drop. These results demonstrate that DeltaLLM offers a promising\nsolution for efficient edge deployment, requiring no fine-tuning and seamlessly\nintegrating with existing inference pipelines.", "AI": {"tldr": "DeltaLLM\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u52a8\u6001\u6ce8\u610f\u529b\u526a\u679d\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u6ce8\u610f\u529b\u6a21\u5f0f\u7684\u65f6\u95f4\u7a00\u758f\u6027\uff0c\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b0\u9ad8\u6548LLM\u63a8\u7406\u3002", "motivation": "\u89e3\u51b3\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u65f6\u56e0\u5e8f\u5217\u957f\u5ea6\u589e\u52a0\u800c\u8ba1\u7b97\u91cf\u5267\u589e\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u65f6\u95f4\u7a00\u758f\u6027\u7684delta\u77e9\u9635\u6784\u5efa\u7b56\u7565\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u6df7\u5408\u6ce8\u610f\u529b\u673a\u5236\u3002", "result": "\u5728BitNet\u548cLlama\u6a21\u578b\u4e0a\uff0c\u6ce8\u610f\u529b\u7a00\u758f\u6027\u663e\u8457\u63d0\u5347\uff08\u6700\u9ad860%\uff09\uff0c\u4e14\u7cbe\u5ea6\u635f\u5931\u53ef\u5ffd\u7565\u3002", "conclusion": "DeltaLLM\u4e3a\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u9ad8\u6548LLM\u90e8\u7f72\u63d0\u4f9b\u4e86\u65e0\u9700\u5fae\u8c03\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.19817", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.19817", "abs": "https://arxiv.org/abs/2507.19817", "authors": ["Ziyin Xiong", "Yinghan Chen", "Puhao Li", "Yixin Zhu", "Tengyu Liu", "Siyuan Huang"], "title": "Ag2x2: Robust Agent-Agnostic Visual Representations for Zero-Shot Bimanual Manipulation", "comment": "Accepted to IROS 2025, oral presentation. Project page link:\n  https://ziyin-xiong.github.io/ag2x2.github.io/", "summary": "Bimanual manipulation, fundamental to human daily activities, remains a\nchallenging task due to its inherent complexity of coordinated control. Recent\nadvances have enabled zero-shot learning of single-arm manipulation skills\nthrough agent-agnostic visual representations derived from human videos;\nhowever, these methods overlook crucial agent-specific information necessary\nfor bimanual coordination, such as end-effector positions. We propose Ag2x2, a\ncomputational framework for bimanual manipulation through coordination-aware\nvisual representations that jointly encode object states and hand motion\npatterns while maintaining agent-agnosticism. Extensive experiments demonstrate\nthat Ag2x2 achieves a 73.5% success rate across 13 diverse bimanual tasks from\nBi-DexHands and PerAct2, including challenging scenarios with deformable\nobjects like ropes. This performance outperforms baseline methods and even\nsurpasses the success rate of policies trained with expert-engineered rewards.\nFurthermore, we show that representations learned through Ag2x2 can be\neffectively leveraged for imitation learning, establishing a scalable pipeline\nfor skill acquisition without expert supervision. By maintaining robust\nperformance across diverse tasks without human demonstrations or engineered\nrewards, Ag2x2 represents a step toward scalable learning of complex bimanual\nrobotic skills.", "AI": {"tldr": "Ag2x2\u6846\u67b6\u901a\u8fc7\u534f\u8c03\u611f\u77e5\u89c6\u89c9\u8868\u793a\u5b9e\u73b0\u53cc\u624b\u673a\u5668\u4eba\u64cd\u4f5c\uff0c\u65e0\u9700\u4e13\u5bb6\u76d1\u7763\uff0c\u6027\u80fd\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u53cc\u624b\u673a\u5668\u4eba\u64cd\u4f5c\u56e0\u5176\u534f\u8c03\u63a7\u5236\u7684\u590d\u6742\u6027\u800c\u5177\u6709\u6311\u6218\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u4e86\u5173\u952e\u7684\u624b\u90e8\u4f4d\u7f6e\u4fe1\u606f\u3002", "method": "\u63d0\u51faAg2x2\u6846\u67b6\uff0c\u8054\u5408\u7f16\u7801\u7269\u4f53\u72b6\u6001\u548c\u624b\u90e8\u8fd0\u52a8\u6a21\u5f0f\uff0c\u4fdd\u6301\u4e0e\u4ee3\u7406\u65e0\u5173\u6027\u3002", "result": "\u572813\u79cd\u53cc\u624b\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u8fbe\u523073.5%\u6210\u529f\u7387\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u548c\u4e13\u5bb6\u5956\u52b1\u7b56\u7565\u3002", "conclusion": "Ag2x2\u4e3a\u590d\u6742\u53cc\u624b\u673a\u5668\u4eba\u6280\u80fd\u7684\u53ef\u6269\u5c55\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2507.19672", "categories": ["cs.AI", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.19672", "abs": "https://arxiv.org/abs/2507.19672", "authors": ["Haoran Lu", "Luyang Fang", "Ruidong Zhang", "Xinliang Li", "Jiazhang Cai", "Huimin Cheng", "Lin Tang", "Ziyu Liu", "Zeliang Sun", "Tao Wang", "Yingchuan Zhang", "Arif Hassan Zidan", "Jinwen Xu", "Jincheng Yu", "Meizhi Yu", "Hanqi Jiang", "Xilin Gong", "Weidi Luo", "Bolun Sun", "Yongkai Chen", "Terry Ma", "Shushan Wu", "Yifan Zhou", "Junhao Chen", "Haotian Xiang", "Jing Zhang", "Afrar Jahin", "Wei Ruan", "Ke Deng", "Yi Pan", "Peilong Wang", "Jiahui Li", "Zhengliang Liu", "Lu Zhang", "Lin Zhao", "Wei Liu", "Dajiang Zhu", "Xin Xing", "Fei Dou", "Wei Zhang", "Chao Huang", "Rongjie Liu", "Mengrui Zhang", "Yiwen Liu", "Xiaoxiao Sun", "Qin Lu", "Zhen Xiang", "Wenxuan Zhong", "Tianming Liu", "Ping Ma"], "title": "Alignment and Safety in Large Language Models: Safety Mechanisms, Training Paradigms, and Emerging Challenges", "comment": "119 pages, 10 figures, 7 tables", "summary": "Due to the remarkable capabilities and growing impact of large language\nmodels (LLMs), they have been deeply integrated into many aspects of society.\nThus, ensuring their alignment with human values and intentions has emerged as\na critical challenge. This survey provides a comprehensive overview of\npractical alignment techniques, training protocols, and empirical findings in\nLLM alignment. We analyze the development of alignment methods across diverse\nparadigms, characterizing the fundamental trade-offs between core alignment\nobjectives. Our analysis shows that while supervised fine-tuning enables basic\ninstruction-following, preference-based methods offer more flexibility for\naligning with nuanced human intent. We discuss state-of-the-art techniques,\nincluding Direct Preference Optimization (DPO), Constitutional AI,\nbrain-inspired methods, and alignment uncertainty quantification (AUQ),\nhighlighting their approaches to balancing quality and efficiency. We review\nexisting evaluation frameworks and benchmarking datasets, emphasizing\nlimitations such as reward misspecification, distributional robustness, and\nscalable oversight. We summarize strategies adopted by leading AI labs to\nillustrate the current state of practice. We conclude by outlining open\nproblems in oversight, value pluralism, robustness, and continuous alignment.\nThis survey aims to inform both researchers and practitioners navigating the\nevolving landscape of LLM alignment.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5bf9\u9f50\u7684\u5b9e\u7528\u6280\u672f\u3001\u8bad\u7ec3\u534f\u8bae\u548c\u5b9e\u8bc1\u7814\u7a76\uff0c\u5206\u6790\u4e86\u4e0d\u540c\u8303\u5f0f\u4e0b\u7684\u5bf9\u9f50\u65b9\u6cd5\u53ca\u5176\u6838\u5fc3\u76ee\u6807\u95f4\u7684\u6743\u8861\uff0c\u5e76\u8ba8\u8bba\u4e86\u524d\u6cbf\u6280\u672f\u548c\u73b0\u6709\u8bc4\u4f30\u6846\u67b6\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u80fd\u529b\u7684\u663e\u8457\u63d0\u5347\u53ca\u5176\u5bf9\u793e\u4f1a\u5f71\u54cd\u7684\u6269\u5927\uff0c\u786e\u4fdd\u5176\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u548c\u610f\u56fe\u7684\u5bf9\u9f50\u6210\u4e3a\u5173\u952e\u6311\u6218\u3002", "method": "\u5206\u6790\u4e86\u76d1\u7763\u5fae\u8c03\u3001\u57fa\u4e8e\u504f\u597d\u7684\u65b9\u6cd5\u7b49\u5bf9\u9f50\u6280\u672f\uff0c\u5305\u62ecDPO\u3001Constitutional AI\u7b49\u524d\u6cbf\u65b9\u6cd5\uff0c\u5e76\u63a2\u8ba8\u4e86\u8bc4\u4f30\u6846\u67b6\u548c\u6570\u636e\u96c6\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u76d1\u7763\u5fae\u8c03\u53ef\u5b9e\u73b0\u57fa\u672c\u6307\u4ee4\u9075\u5faa\uff0c\u800c\u57fa\u4e8e\u504f\u597d\u7684\u65b9\u6cd5\u80fd\u66f4\u7075\u6d3b\u5730\u5bf9\u9f50\u590d\u6742\u4eba\u7c7b\u610f\u56fe\u3002", "conclusion": "\u603b\u7ed3\u4e86\u5f53\u524d\u5b9e\u8df5\u4e2d\u7684\u7b56\u7565\uff0c\u5e76\u63d0\u51fa\u4e86\u76d1\u7763\u3001\u4ef7\u503c\u591a\u5143\u6027\u3001\u9c81\u68d2\u6027\u548c\u6301\u7eed\u5bf9\u9f50\u7b49\u5f00\u653e\u95ee\u9898\uff0c\u65e8\u5728\u4e3a\u7814\u7a76\u8005\u548c\u5b9e\u8df5\u8005\u63d0\u4f9b\u6307\u5bfc\u3002"}}
{"id": "2507.19829", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.19829", "abs": "https://arxiv.org/abs/2507.19829", "authors": ["Chuan Cao", "Xiaoning Wang", "Wenqian Xi", "Han Zhang", "Weidong Chen", "Jingchuan Wang"], "title": "A 4D Radar Camera Extrinsic Calibration Tool Based on 3D Uncertainty Perspective N Points", "comment": null, "summary": "4D imaging radar is a type of low-cost millimeter-wave radar(costing merely\n10-20$\\%$ of lidar systems) capable of providing range, azimuth, elevation, and\nDoppler velocity information. Accurate extrinsic calibration between\nmillimeter-wave radar and camera systems is critical for robust multimodal\nperception in robotics, yet remains challenging due to inherent sensor noise\ncharacteristics and complex error propagation. This paper presents a systematic\ncalibration framework to address critical challenges through a spatial 3d\nuncertainty-aware PnP algorithm (3DUPnP) that explicitly models spherical\ncoordinate noise propagation in radar measurements, then compensating for\nnon-zero error expectations during coordinate transformations. Finally,\nexperimental validation demonstrates significant performance improvements over\nstate-of-the-art CPnP baseline, including improved consistency in simulations\nand enhanced precision in physical experiments. This study provides a robust\ncalibration solution for robotic systems equipped with millimeter-wave radar\nand cameras, tailored specifically for autonomous driving and robotic\nperception applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7a7a\u95f43D\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684PnP\u7b97\u6cd5\uff083DUPnP\uff09\uff0c\u7528\u4e8e\u6beb\u7c73\u6ce2\u96f7\u8fbe\u4e0e\u76f8\u673a\u7cfb\u7edf\u7684\u5916\u53c2\u6807\u5b9a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6807\u5b9a\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u6beb\u7c73\u6ce2\u96f7\u8fbe\u4e0e\u76f8\u673a\u7cfb\u7edf\u7684\u5916\u53c2\u6807\u5b9a\u5bf9\u673a\u5668\u4eba\u591a\u6a21\u6001\u611f\u77e5\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u4f20\u611f\u5668\u566a\u58f0\u548c\u590d\u6742\u8bef\u5dee\u4f20\u64ad\uff0c\u6807\u5b9a\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u63d0\u51fa3DUPnP\u7b97\u6cd5\uff0c\u663e\u5f0f\u5efa\u6a21\u96f7\u8fbe\u6d4b\u91cf\u4e2d\u7684\u7403\u5750\u6807\u566a\u58f0\u4f20\u64ad\uff0c\u5e76\u5728\u5750\u6807\u53d8\u6362\u4e2d\u8865\u507f\u975e\u96f6\u8bef\u5dee\u671f\u671b\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0c3DUPnP\u5728\u4eff\u771f\u548c\u7269\u7406\u5b9e\u9a8c\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709CPnP\u57fa\u7ebf\uff0c\u6807\u5b9a\u7cbe\u5ea6\u548c\u4e00\u81f4\u6027\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u914d\u5907\u6beb\u7c73\u6ce2\u96f7\u8fbe\u548c\u76f8\u673a\u7684\u673a\u5668\u4eba\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u9c81\u68d2\u7684\u6807\u5b9a\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u548c\u673a\u5668\u4eba\u611f\u77e5\u5e94\u7528\u3002"}}
{"id": "2507.19703", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19703", "abs": "https://arxiv.org/abs/2507.19703", "authors": ["Peter V. Coveney", "Sauro Succi"], "title": "The wall confronting large language models", "comment": null, "summary": "We show that the scaling laws which determine the performance of large\nlanguage models (LLMs) severely limit their ability to improve the uncertainty\nof their predictions. As a result, raising their reliability to meet the\nstandards of scientific inquiry is intractable by any reasonable measure. We\nargue that the very mechanism which fuels much of the learning power of LLMs,\nnamely the ability to generate non-Gaussian output distributions from Gaussian\ninput ones, might well be at the roots of their propensity to produce error\npileup, ensuing information catastrophes and degenerative AI behaviour. This\ntension between learning and accuracy is a likely candidate mechanism\nunderlying the observed low values of the scaling components. It is\nsubstantially compounded by the deluge of spurious correlations pointed out by\nCalude and Longo which rapidly increase in any data set merely as a function of\nits size, regardless of its nature. The fact that a degenerative AI pathway is\na very probable feature of the LLM landscape does not mean that it must\ninevitably arise in all future AI research. Its avoidance, which we also\ndiscuss in this paper, necessitates putting a much higher premium on insight\nand understanding of the structural characteristics of the problems being\ninvestigated.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u6027\u80fd\u53d7\u9650\u4e8e\u5176\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u7684\u6539\u8fdb\u80fd\u529b\uff0c\u96be\u4ee5\u6ee1\u8db3\u79d1\u5b66\u7814\u7a76\u7684\u53ef\u9760\u6027\u6807\u51c6\u3002\u5176\u5b66\u4e60\u673a\u5236\u53ef\u80fd\u5bfc\u81f4\u9519\u8bef\u7d2f\u79ef\u548c\u4fe1\u606f\u707e\u96be\u3002", "motivation": "\u63a2\u8ba8LLMs\u5728\u63d0\u5347\u9884\u6d4b\u53ef\u9760\u6027\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u63ed\u793a\u5176\u5b66\u4e60\u673a\u5236\u4e0e\u51c6\u786e\u6027\u4e4b\u95f4\u7684\u51b2\u7a81\u3002", "method": "\u5206\u6790LLMs\u7684\u7f29\u653e\u5b9a\u5f8b\u53ca\u5176\u5bf9\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u7684\u5f71\u54cd\uff0c\u63a2\u8ba8\u975e\u9ad8\u65af\u8f93\u51fa\u5206\u5e03\u4e0e\u9519\u8bef\u7d2f\u79ef\u7684\u5173\u7cfb\u3002", "result": "LLMs\u7684\u5b66\u4e60\u673a\u5236\u53ef\u80fd\u5bfc\u81f4\u4fe1\u606f\u707e\u96be\u548c\u9000\u5316AI\u884c\u4e3a\uff0c\u4e14\u6570\u636e\u89c4\u6a21\u589e\u957f\u4f1a\u52a0\u5267\u865a\u5047\u76f8\u5173\u6027\u3002", "conclusion": "\u4e3a\u907f\u514d\u9000\u5316AI\u8def\u5f84\uff0c\u9700\u66f4\u91cd\u89c6\u5bf9\u95ee\u9898\u7ed3\u6784\u7279\u5f81\u7684\u6df1\u5165\u7406\u89e3\u548c\u6d1e\u5bdf\u3002"}}
{"id": "2507.19831", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.19831", "abs": "https://arxiv.org/abs/2507.19831", "authors": ["Zaar Khizar", "Johann Laconte", "Roland Lenain", "Romuald Aufrere"], "title": "Feeling the Force: A Nuanced Physics-based Traversability Sensor for Navigation in Unstructured Vegetation", "comment": null, "summary": "In many applications, robots are increasingly deployed in unstructured and\nnatural environments where they encounter various types of vegetation.\nVegetation presents unique challenges as a traversable obstacle, where the\nmechanical properties of the plants can influence whether a robot can safely\ncollide with and overcome the obstacle. A more nuanced approach is required to\nassess the safety and traversability of these obstacles, as collisions can\nsometimes be safe and necessary for navigating through dense or unavoidable\nvegetation. This paper introduces a novel sensor designed to directly measure\nthe applied forces exerted by vegetation on a robot: by directly capturing the\npush-back forces, our sensor provides a detailed understanding of the\ninteractions between the robot and its surroundings. We demonstrate the\nsensor's effectiveness through experimental validations, showcasing its ability\nto measure subtle force variations. This force-based approach provides a\nquantifiable metric that can inform navigation decisions and serve as a\nfoundation for developing future learning algorithms.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u4f20\u611f\u5668\uff0c\u7528\u4e8e\u76f4\u63a5\u6d4b\u91cf\u690d\u88ab\u5bf9\u673a\u5668\u4eba\u65bd\u52a0\u7684\u529b\uff0c\u4ee5\u8bc4\u4f30\u5176\u5b89\u5168\u6027\u548c\u53ef\u7a7f\u8d8a\u6027\u3002", "motivation": "\u673a\u5668\u4eba\u5728\u975e\u7ed3\u6784\u5316\u81ea\u7136\u73af\u5883\u4e2d\u5de5\u4f5c\u65f6\uff0c\u690d\u88ab\u4f5c\u4e3a\u53ef\u7a7f\u8d8a\u969c\u788d\u7269\u5177\u6709\u72ec\u7279\u7684\u673a\u68b0\u7279\u6027\uff0c\u9700\u8981\u66f4\u7ec6\u81f4\u7684\u65b9\u6cd5\u6765\u8bc4\u4f30\u5176\u5b89\u5168\u6027\u548c\u53ef\u7a7f\u8d8a\u6027\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u578b\u4f20\u611f\u5668\uff0c\u76f4\u63a5\u6355\u83b7\u690d\u88ab\u5bf9\u673a\u5668\u4eba\u7684\u53cd\u4f5c\u7528\u529b\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "result": "\u4f20\u611f\u5668\u80fd\u591f\u7cbe\u786e\u6d4b\u91cf\u7ec6\u5fae\u7684\u529b\u53d8\u5316\uff0c\u4e3a\u5bfc\u822a\u51b3\u7b56\u63d0\u4f9b\u91cf\u5316\u6307\u6807\u3002", "conclusion": "\u8fd9\u79cd\u57fa\u4e8e\u529b\u7684\u65b9\u6cd5\u4e3a\u672a\u6765\u5b66\u4e60\u7b97\u6cd5\u7684\u5f00\u53d1\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u5e76\u6709\u52a9\u4e8e\u673a\u5668\u4eba\u66f4\u5b89\u5168\u5730\u7a7f\u8d8a\u690d\u88ab\u73af\u5883\u3002"}}
{"id": "2507.19725", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19725", "abs": "https://arxiv.org/abs/2507.19725", "authors": ["Leonardo Villalobos-Arias", "Grant Forbes", "Jianxun Wang", "David L Roberts", "Arnav Jhala"], "title": "Minding Motivation: The Effect of Intrinsic Motivation on Agent Behaviors", "comment": "11 pages, 7 figures, 3 tables", "summary": "Games are challenging for Reinforcement Learning~(RL) agents due to their\nreward-sparsity, as rewards are only obtainable after long sequences of\ndeliberate actions. Intrinsic Motivation~(IM) methods -- which introduce\nexploration rewards -- are an effective solution to reward-sparsity. However,\nIM also causes an issue known as `reward hacking' where the agent optimizes for\nthe new reward at the expense of properly playing the game. The larger problem\nis that reward hacking itself is largely unknown; there is no answer to\nwhether, and to what extent, IM rewards change the behavior of RL agents. This\nstudy takes a first step by empirically evaluating the impact on behavior of\nthree IM techniques on the MiniGrid game-like environment. We compare these IM\nmodels with Generalized Reward Matching~(GRM), a method that can be used with\nany intrinsic reward function to guarantee optimality. Our results suggest that\nIM causes noticeable change by increasing the initial rewards, but also\naltering the way the agent plays; and that GRM mitigated reward hacking in some\nscenarios.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5185\u5728\u52a8\u673a\uff08IM\uff09\u65b9\u6cd5\u5728\u7a00\u758f\u5956\u52b1\u6e38\u620f\u4e2d\u5bf9\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4ee3\u7406\u884c\u4e3a\u7684\u5f71\u54cd\uff0c\u53d1\u73b0IM\u53ef\u80fd\u5bfc\u81f4\u5956\u52b1\u6ee5\u7528\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u5e7f\u4e49\u5956\u52b1\u5339\u914d\uff08GRM\uff09\u65b9\u6cd5\u90e8\u5206\u7f13\u89e3\u4e86\u8fd9\u4e00\u95ee\u9898\u3002", "motivation": "\u7531\u4e8e\u6e38\u620f\u4e2d\u5956\u52b1\u7a00\u758f\uff0cRL\u4ee3\u7406\u96be\u4ee5\u5b66\u4e60\u3002IM\u65b9\u6cd5\u901a\u8fc7\u5f15\u5165\u63a2\u7d22\u5956\u52b1\u89e3\u51b3\u4e86\u8fd9\u4e00\u95ee\u9898\uff0c\u4f46\u53ef\u80fd\u5bfc\u81f4\u5956\u52b1\u6ee5\u7528\u884c\u4e3a\uff0c\u76ee\u524d\u5bf9\u5176\u5f71\u54cd\u5c1a\u4e0d\u6e05\u695a\u3002", "method": "\u5728MiniGrid\u73af\u5883\u4e2d\uff0c\u5b9e\u8bc1\u8bc4\u4f30\u4e86\u4e09\u79cdIM\u6280\u672f\u5bf9RL\u4ee3\u7406\u884c\u4e3a\u7684\u5f71\u54cd\uff0c\u5e76\u4e0eGRM\u65b9\u6cd5\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "IM\u663e\u8457\u6539\u53d8\u4e86\u4ee3\u7406\u7684\u884c\u4e3a\uff0c\u589e\u52a0\u4e86\u521d\u59cb\u5956\u52b1\u4f46\u4e5f\u5bfc\u81f4\u5956\u52b1\u6ee5\u7528\uff1bGRM\u5728\u67d0\u4e9b\u573a\u666f\u4e2d\u7f13\u89e3\u4e86\u8fd9\u4e00\u95ee\u9898\u3002", "conclusion": "IM\u5bf9RL\u4ee3\u7406\u884c\u4e3a\u6709\u663e\u8457\u5f71\u54cd\uff0cGRM\u662f\u4e00\u79cd\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2507.19851", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.19851", "abs": "https://arxiv.org/abs/2507.19851", "authors": ["Ye Wang", "Haodong Jing", "Yang Liao", "Yongqiang Ma", "Nanning Zheng"], "title": "PlaneHEC: Efficient Hand-Eye Calibration for Multi-view Robotic Arm via Any Point Cloud Plane Detection", "comment": "Accepted by 2025 IEEE International Conference on Robotics &\n  Automation (ICRA)", "summary": "Hand-eye calibration is an important task in vision-guided robotic systems\nand is crucial for determining the transformation matrix between the camera\ncoordinate system and the robot end-effector. Existing methods, for multi-view\nrobotic systems, usually rely on accurate geometric models or manual\nassistance, generalize poorly, and can be very complicated and inefficient.\nTherefore, in this study, we propose PlaneHEC, a generalized hand-eye\ncalibration method that does not require complex models and can be accomplished\nusing only depth cameras, which achieves the optimal and fastest calibration\nresults using arbitrary planar surfaces like walls and tables. PlaneHEC\nintroduces hand-eye calibration equations based on planar constraints, which\nmakes it strongly interpretable and generalizable. PlaneHEC also uses a\ncomprehensive solution that starts with a closed-form solution and improves it\nwithiterative optimization, which greatly improves accuracy. We comprehensively\nevaluated the performance of PlaneHEC in both simulated and real-world\nenvironments and compared the results with other point-cloud-based calibration\nmethods, proving its superiority. Our approach achieves universal and fast\ncalibration with an innovative design of computational models, providing a\nstrong contribution to the development of multi-agent systems and embodied\nintelligence.", "AI": {"tldr": "PlaneHEC\u662f\u4e00\u79cd\u65e0\u9700\u590d\u6742\u6a21\u578b\u3001\u4ec5\u9700\u6df1\u5ea6\u76f8\u673a\u7684\u901a\u7528\u624b\u773c\u6807\u5b9a\u65b9\u6cd5\uff0c\u5229\u7528\u4efb\u610f\u5e73\u9762\u8868\u9762\u5b9e\u73b0\u6700\u4f18\u4e14\u6700\u5feb\u7684\u6807\u5b9a\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u7cbe\u786e\u51e0\u4f55\u6a21\u578b\u6216\u4eba\u5de5\u8f85\u52a9\uff0c\u6cdb\u5316\u6027\u5dee\u4e14\u590d\u6742\u4f4e\u6548\uff0c\u56e0\u6b64\u63d0\u51faPlaneHEC\u4ee5\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u5e73\u9762\u7ea6\u675f\u8bbe\u8ba1\u624b\u773c\u6807\u5b9a\u65b9\u7a0b\uff0c\u7ed3\u5408\u95ed\u5f0f\u89e3\u548c\u8fed\u4ee3\u4f18\u5316\u63d0\u9ad8\u7cbe\u5ea6\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86PlaneHEC\u7684\u4f18\u8d8a\u6027\uff0c\u4f18\u4e8e\u5176\u4ed6\u70b9\u4e91\u6807\u5b9a\u65b9\u6cd5\u3002", "conclusion": "PlaneHEC\u901a\u8fc7\u521b\u65b0\u8ba1\u7b97\u6a21\u578b\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u4e86\u901a\u7528\u4e14\u5feb\u901f\u7684\u6807\u5b9a\uff0c\u5bf9\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u548c\u5177\u8eab\u667a\u80fd\u53d1\u5c55\u6709\u91cd\u8981\u8d21\u732e\u3002"}}
{"id": "2507.19726", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.19726", "abs": "https://arxiv.org/abs/2507.19726", "authors": ["Yuzhang Xie", "Xu Han", "Ran Xu", "Xiao Hu", "Jiaying Lu", "Carl Yang"], "title": "HypKG: Hypergraph-based Knowledge Graph Contextualization for Precision Healthcare", "comment": "Extended version of paper accepted at the 24th International Semantic\n  Web Conference (ISWC 2025), Main Tracks, Research Track, Oral", "summary": "Knowledge graphs (KGs) are important products of the semantic web, which are\nwidely used in various application domains. Healthcare is one of such domains\nwhere KGs are intensively used, due to the high requirement for knowledge\naccuracy and interconnected nature of healthcare data. However, KGs storing\ngeneral factual information often lack the ability to account for important\ncontexts of the knowledge such as the status of specific patients, which are\ncrucial in precision healthcare. Meanwhile, electronic health records (EHRs)\nprovide rich personal data, including various diagnoses and medications, which\nprovide natural contexts for general KGs. In this paper, we propose HypKG, a\nframework that integrates patient information from EHRs into KGs to generate\ncontextualized knowledge representations for accurate healthcare predictions.\nUsing advanced entity-linking techniques, we connect relevant knowledge from\ngeneral KGs with patient information from EHRs, and then utilize a hypergraph\nmodel to \"contextualize\" the knowledge with the patient information. Finally,\nwe employ hypergraph transformers guided by downstream prediction tasks to\njointly learn proper contextualized representations for both KGs and patients,\nfully leveraging existing knowledge in KGs and patient contexts in EHRs. In\nexperiments using a large biomedical KG and two real-world EHR datasets, HypKG\ndemonstrates significant improvements in healthcare prediction tasks across\nmultiple evaluation metrics. Additionally, by integrating external contexts,\nHypKG can learn to adjust the representations of entities and relations in KG,\npotentially improving the quality and real-world utility of knowledge.", "AI": {"tldr": "HypKG\u6846\u67b6\u901a\u8fc7\u6574\u5408\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHRs\uff09\u548c\u77e5\u8bc6\u56fe\u8c31\uff08KGs\uff09\uff0c\u751f\u6210\u60c5\u5883\u5316\u77e5\u8bc6\u8868\u793a\uff0c\u63d0\u5347\u533b\u7597\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u901a\u7528\u77e5\u8bc6\u56fe\u8c31\u7f3a\u4e4f\u5bf9\u60a3\u8005\u7279\u5b9a\u60c5\u5883\u7684\u8003\u8651\uff0c\u800c\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u4e2a\u4eba\u6570\u636e\uff0c\u4e24\u8005\u7ed3\u5408\u53ef\u63d0\u5347\u7cbe\u51c6\u533b\u7597\u7684\u6548\u679c\u3002", "method": "\u91c7\u7528\u5b9e\u4f53\u94fe\u63a5\u6280\u672f\u5c06\u901a\u7528KGs\u4e0eEHRs\u4e2d\u7684\u60a3\u8005\u4fe1\u606f\u8fde\u63a5\uff0c\u5e76\u5229\u7528\u8d85\u56fe\u6a21\u578b\u548c\u8d85\u56fe\u53d8\u6362\u5668\u5b66\u4e60\u60c5\u5883\u5316\u8868\u793a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cHypKG\u5728\u591a\u4e2a\u8bc4\u4f30\u6307\u6807\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u533b\u7597\u9884\u6d4b\u4efb\u52a1\u7684\u6027\u80fd\u3002", "conclusion": "HypKG\u901a\u8fc7\u6574\u5408\u5916\u90e8\u60c5\u5883\uff0c\u4f18\u5316\u4e86\u77e5\u8bc6\u56fe\u8c31\u7684\u8868\u793a\uff0c\u63d0\u5347\u4e86\u77e5\u8bc6\u7684\u8d28\u91cf\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2507.19854", "categories": ["cs.RO", "cs.HC", "68T05, 68T07, 68T40", "I.2.6; I.2.9; I.2.7; I.2.10; H.5.2"], "pdf": "https://arxiv.org/pdf/2507.19854", "abs": "https://arxiv.org/abs/2507.19854", "authors": ["Anjali R. Menon", "Rohit K. Sharma", "Priya Singh", "Chengyu Wang", "Aurora M. Ferreira", "Mateja Novak"], "title": "Think, Act, Learn: A Framework for Autonomous Robotic Agents using Closed-Loop Large Language Models", "comment": "13 pages, 7 figures", "summary": "The integration of Large Language Models (LLMs) into robotics has unlocked\nunprecedented capabilities in high-level task planning. However, most current\nsystems operate in an open-loop fashion, where LLMs act as one-shot planners,\nrendering them brittle and unable to adapt to unforeseen circumstances in\ndynamic physical environments. To overcome this limitation, this paper\nintroduces the \"Think, Act, Learn\" (T-A-L) framework, a novel architecture that\nenables an embodied agent to autonomously learn and refine its policies through\ncontinuous interaction. Our framework establishes a closed-loop cycle where an\nLLM first \"thinks\" by decomposing high-level commands into actionable plans.\nThe robot then \"acts\" by executing these plans while gathering rich, multimodal\nsensory feedback. Critically, the \"learn\" module processes this feedback to\nfacilitate LLM-driven self-reflection, allowing the agent to perform causal\nanalysis on its failures and generate corrective strategies. These insights are\nstored in an experiential memory to guide future planning cycles. We\ndemonstrate through extensive experiments in both simulation and the real world\nthat our T-A-L agent significantly outperforms baseline methods, including\nopen-loop LLMs, Behavioral Cloning, and traditional Reinforcement Learning. Our\nframework achieves over a 97% success rate on complex, long-horizon tasks,\nconverges to a stable policy in an average of just 9 trials, and exhibits\nremarkable generalization to unseen tasks. This work presents a significant\nstep towards developing more robust, adaptive, and truly autonomous robotic\nagents.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u601d\u8003\u3001\u884c\u52a8\u3001\u5b66\u4e60\u201d\uff08T-A-L\uff09\u7684\u95ed\u73af\u6846\u67b6\uff0c\u901a\u8fc7\u6301\u7eed\u4ea4\u4e92\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u81ea\u4e3b\u5b66\u4e60\u548c\u4f18\u5316\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u52a8\u6001\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\u548c\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u673a\u5668\u4eba\u4efb\u52a1\u89c4\u5212\u4e2d\u591a\u4e3a\u5f00\u73af\u7cfb\u7edf\uff0c\u65e0\u6cd5\u9002\u5e94\u52a8\u6001\u73af\u5883\u4e2d\u7684\u7a81\u53d1\u60c5\u51b5\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u95ed\u73af\u6846\u67b6\u6765\u589e\u5f3a\u5176\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u3002", "method": "T-A-L\u6846\u67b6\u901a\u8fc7\u4e09\u4e2a\u6a21\u5757\u5b9e\u73b0\u95ed\u73af\uff1aLLM\u5206\u89e3\u4efb\u52a1\u4e3a\u53ef\u6267\u884c\u8ba1\u5212\uff08\u601d\u8003\uff09\uff0c\u673a\u5668\u4eba\u6267\u884c\u8ba1\u5212\u5e76\u6536\u96c6\u53cd\u9988\uff08\u884c\u52a8\uff09\uff0c\u4ee5\u53ca\u901a\u8fc7\u53cd\u9988\u8fdb\u884c\u81ea\u6211\u53cd\u601d\u548c\u7b56\u7565\u4f18\u5316\uff08\u5b66\u4e60\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cT-A-L\u6846\u67b6\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u6210\u529f\u7387\u8d85\u8fc797%\uff0c\u5e73\u5747\u4ec5\u97009\u6b21\u8bd5\u9a8c\u5373\u53ef\u6536\u655b\uff0c\u5e76\u80fd\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u4efb\u52a1\u3002", "conclusion": "T-A-L\u6846\u67b6\u4e3a\u5f00\u53d1\u66f4\u9c81\u68d2\u3001\u81ea\u9002\u5e94\u548c\u771f\u6b63\u81ea\u4e3b\u7684\u673a\u5668\u4eba\u4ee3\u7406\u63d0\u4f9b\u4e86\u91cd\u8981\u8fdb\u5c55\u3002"}}
{"id": "2507.19733", "categories": ["cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2507.19733", "abs": "https://arxiv.org/abs/2507.19733", "authors": ["Alec Scully", "Cameron Stockton", "Forrest Hare"], "title": "Integrating Activity Predictions in Knowledge Graphs", "comment": "7 pages. 18 figures. Semantic Technology for Intelligence, Defense,\n  and Security (STIDS 2024)", "summary": "We argue that ontology-structured knowledge graphs can play a crucial role in\ngenerating predictions about future events. By leveraging the semantic\nframework provided by Basic Formal Ontology (BFO) and Common Core Ontologies\n(CCO), we demonstrate how data such as the movements of a fishing vessel can be\norganized in and retrieved from a knowledge graph. These query results are then\nused to create Markov chain models, allowing us to predict future states based\non the vessel's history. To fully support this process, we introduce the term\n`spatiotemporal instant' to complete the necessary structural semantics.\nAdditionally, we critique the prevailing ontological model of probability,\nwhich conflates probability with likelihood and relies on the problematic\nconcept of modal measurements: measurements of future entities. We propose an\nalternative view, where probabilities are treated as being about process\nprofiles, which better captures the dynamics of real world phenomena. Finally,\nwe demonstrate how our Markov chain based probability calculations can be\nseamlessly integrated back into the knowledge graph, enabling further analysis\nand decision-making. Keywords: predictive analytics, ontology, Markov chains,\nprobability, Basic Formal Ontology (BFO), knowledge graphs, SPARQL.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u5229\u7528\u672c\u4f53\u7ed3\u6784\u77e5\u8bc6\u56fe\u8c31\u9884\u6d4b\u672a\u6765\u4e8b\u4ef6\uff0c\u7ed3\u5408BFO\u548cCCO\u6846\u67b6\u7ec4\u7ec7\u6570\u636e\uff0c\u901a\u8fc7\u9a6c\u5c14\u53ef\u592b\u94fe\u6a21\u578b\u9884\u6d4b\u672a\u6765\u72b6\u6001\uff0c\u5e76\u63d0\u51fa\u201c\u65f6\u7a7a\u5b9e\u4f8b\u201d\u6982\u5ff5\u5b8c\u5584\u8bed\u4e49\u7ed3\u6784\u3002\u540c\u65f6\u6279\u5224\u73b0\u6709\u6982\u7387\u6a21\u578b\uff0c\u63d0\u51fa\u57fa\u4e8e\u8fc7\u7a0b\u8f6e\u5ed3\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5e76\u5c06\u6982\u7387\u8ba1\u7b97\u65e0\u7f1d\u96c6\u6210\u56de\u77e5\u8bc6\u56fe\u8c31\u3002", "motivation": "\u63a2\u7d22\u672c\u4f53\u7ed3\u6784\u77e5\u8bc6\u56fe\u8c31\u5728\u9884\u6d4b\u672a\u6765\u4e8b\u4ef6\u4e2d\u7684\u6f5c\u529b\uff0c\u89e3\u51b3\u73b0\u6709\u6982\u7387\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u63d0\u4f9b\u66f4\u52a8\u6001\u7684\u9884\u6d4b\u65b9\u6cd5\u3002", "method": "\u5229\u7528BFO\u548cCCO\u6846\u67b6\u7ec4\u7ec7\u6570\u636e\uff0c\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\uff0c\u901a\u8fc7\u9a6c\u5c14\u53ef\u592b\u94fe\u6a21\u578b\u9884\u6d4b\u672a\u6765\u72b6\u6001\uff0c\u5f15\u5165\u201c\u65f6\u7a7a\u5b9e\u4f8b\u201d\u6982\u5ff5\u5b8c\u5584\u8bed\u4e49\u7ed3\u6784\u3002", "result": "\u6210\u529f\u5c55\u793a\u4e86\u5982\u4f55\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u548c\u9a6c\u5c14\u53ef\u592b\u94fe\u6a21\u578b\u9884\u6d4b\u672a\u6765\u4e8b\u4ef6\uff0c\u5e76\u63d0\u51fa\u66f4\u5408\u7406\u7684\u6982\u7387\u6a21\u578b\u3002", "conclusion": "\u672c\u4f53\u7ed3\u6784\u77e5\u8bc6\u56fe\u8c31\u7ed3\u5408\u9a6c\u5c14\u53ef\u592b\u94fe\u6a21\u578b\u80fd\u6709\u6548\u9884\u6d4b\u672a\u6765\u4e8b\u4ef6\uff0c\u63d0\u51fa\u7684\u6982\u7387\u6a21\u578b\u66f4\u8d34\u8fd1\u73b0\u5b9e\u52a8\u6001\uff0c\u4e3a\u51b3\u7b56\u5206\u6790\u63d0\u4f9b\u652f\u6301\u3002"}}
{"id": "2507.19860", "categories": ["cs.RO", "cs.MA", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.19860", "abs": "https://arxiv.org/abs/2507.19860", "authors": ["Haoze Dong", "Meng Guo", "Chengyi He", "Zhongkui Li"], "title": "Homotopy-aware Multi-agent Navigation via Distributed Model Predictive Control", "comment": null, "summary": "Multi-agent trajectory planning requires ensuring both safety and efficiency,\nyet deadlocks remain a significant challenge, especially in obstacle-dense\nenvironments. Such deadlocks frequently occur when multiple agents attempt to\ntraverse the same long and narrow corridor simultaneously. To address this, we\npropose a novel distributed trajectory planning framework that bridges the gap\nbetween global path and local trajectory cooperation. At the global level, a\nhomotopy-aware optimal path planning algorithm is proposed, which fully\nleverages the topological structure of the environment. A reference path is\nchosen from distinct homotopy classes by considering both its spatial and\ntemporal properties, leading to improved coordination among agents globally. At\nthe local level, a model predictive control-based trajectory optimization\nmethod is used to generate dynamically feasible and collision-free\ntrajectories. Additionally, an online replanning strategy ensures its\nadaptability to dynamic environments. Simulations and experiments validate the\neffectiveness of our approach in mitigating deadlocks. Ablation studies\ndemonstrate that by incorporating time-aware homotopic properties into the\nunderlying global paths, our method can significantly reduce deadlocks and\nimprove the average success rate from 4%-13% to over 90% in randomly generated\ndense scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5e03\u5f0f\u8f68\u8ff9\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u5168\u5c40\u8def\u5f84\u89c4\u5212\u548c\u5c40\u90e8\u8f68\u8ff9\u4f18\u5316\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u5728\u5bc6\u96c6\u73af\u5883\u4e2d\u7684\u6b7b\u9501\u95ee\u9898\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u8f68\u8ff9\u89c4\u5212\u5728\u5bc6\u96c6\u73af\u5883\u4e2d\u6613\u53d1\u751f\u6b7b\u9501\uff0c\u5c24\u5176\u662f\u5728\u72ed\u7a84\u8d70\u5eca\u4e2d\u3002", "method": "\u5168\u5c40\u5c42\u9762\u4f7f\u7528\u540c\u4f26\u611f\u77e5\u7684\u6700\u4f18\u8def\u5f84\u89c4\u5212\u7b97\u6cd5\uff0c\u5c40\u90e8\u5c42\u9762\u91c7\u7528\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u4f18\u5316\u8f68\u8ff9\uff0c\u5e76\u7ed3\u5408\u5728\u7ebf\u91cd\u89c4\u5212\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u6b7b\u9501\uff0c\u6210\u529f\u7387\u4ece4%-13%\u63d0\u5347\u81f390%\u4ee5\u4e0a\u3002", "conclusion": "\u7ed3\u5408\u65f6\u95f4\u548c\u7a7a\u95f4\u7279\u6027\u7684\u5168\u5c40\u8def\u5f84\u89c4\u5212\u80fd\u6709\u6548\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u6b7b\u9501\u95ee\u9898\u3002"}}
{"id": "2507.19749", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19749", "abs": "https://arxiv.org/abs/2507.19749", "authors": ["Lin Ren", "Guohui Xiao", "Guilin Qi", "Yishuai Geng", "Haohan Xue"], "title": "Can LLMs Solve ASP Problems? Insights from a Benchmarking Study (Extended Version)", "comment": "Accepted for publication at the 22nd International Conference on\n  Principles of Knowledge Representation and Reasoning (KR 2025). The code is\n  available at https://github.com/HomuraT/ASPBench", "summary": "Answer Set Programming (ASP) is a powerful paradigm for non-monotonic\nreasoning. Recently, large language models (LLMs) have demonstrated promising\ncapabilities in logical reasoning. Despite this potential, current evaluations\nof LLM capabilities in ASP are often limited. Existing works normally employ\noverly simplified ASP programs, do not support negation, disjunction, or\nmultiple answer sets. Furthermore, there is a lack of benchmarks that introduce\ntasks specifically designed for ASP solving. To bridge this gap, we introduce\nASPBench, a comprehensive ASP benchmark, including three ASP specific tasks:\nASP entailment, answer set verification, and answer set computation. Our\nextensive evaluations on ASPBench reveal that while 14 state-of-the-art LLMs,\nincluding \\emph{deepseek-r1}, \\emph{o4-mini}, and\n\\emph{gemini-2.5-flash-thinking}, perform relatively well on the first two\nsimpler tasks, they struggle with answer set computation, which is the core of\nASP solving. These findings offer insights into the current limitations of LLMs\nin ASP solving. This highlights the need for new approaches that integrate\nsymbolic reasoning capabilities more effectively. The code and dataset are\navailable at https://github.com/HomuraT/ASPBench.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86ASPBench\uff0c\u4e00\u4e2a\u9488\u5bf9\u7b54\u6848\u96c6\u7f16\u7a0b\uff08ASP\uff09\u7684\u5168\u9762\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728ASP\u6838\u5fc3\u4efb\u52a1\u4e0a\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5f53\u524d\u5bf9LLM\u5728ASP\u4e2d\u80fd\u529b\u7684\u8bc4\u4f30\u8fc7\u4e8e\u7b80\u5316\uff0c\u7f3a\u4e4f\u652f\u6301\u5426\u5b9a\u3001\u6790\u53d6\u6216\u591a\u7b54\u6848\u96c6\u7684\u6d4b\u8bd5\uff0c\u4e14\u7f3a\u5c11\u4e13\u95e8\u9488\u5bf9ASP\u8bbe\u8ba1\u7684\u4efb\u52a1\u57fa\u51c6\u3002", "method": "\u63d0\u51faASPBench\uff0c\u5305\u542b\u4e09\u4e2aASP\u7279\u5b9a\u4efb\u52a1\uff1aASP\u8574\u542b\u3001\u7b54\u6848\u96c6\u9a8c\u8bc1\u548c\u7b54\u6848\u96c6\u8ba1\u7b97\uff0c\u5e76\u5bf914\u79cd\u5148\u8fdbLLM\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "LLM\u5728\u524d\u4e24\u4e2a\u7b80\u5355\u4efb\u52a1\u4e0a\u8868\u73b0\u8f83\u597d\uff0c\u4f46\u5728\u6838\u5fc3\u7684\u7b54\u6848\u96c6\u8ba1\u7b97\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86LLM\u5728ASP\u89e3\u51b3\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5f3a\u8c03\u9700\u8981\u66f4\u6709\u6548\u5730\u6574\u5408\u7b26\u53f7\u63a8\u7406\u80fd\u529b\u7684\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2507.19883", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.19883", "abs": "https://arxiv.org/abs/2507.19883", "authors": ["Ahmed Abouelazm", "Mohammad Mahmoud", "Conrad Walter", "Oleksandr Shchetsura", "Erne Hussong", "Helen Gremmelmaier", "J. Marius Z\u00f6llner"], "title": "Bridging Simulation and Usability: A User-Friendly Framework for Scenario Generation in CARLA", "comment": "Paper is accepted in IEEE International Automated Vehicle Validation\n  Conference (IAVVC 2025)", "summary": "Autonomous driving promises safer roads, reduced congestion, and improved\nmobility, yet validating these systems across diverse conditions remains a\nmajor challenge. Real-world testing is expensive, time-consuming, and sometimes\nunsafe, making large-scale validation impractical. In contrast, simulation\nenvironments offer a scalable and cost-effective alternative for rigorous\nverification and validation. A critical component of the validation process is\nscenario generation, which involves designing and configuring traffic scenarios\nto evaluate autonomous systems' responses to various events and uncertainties.\nHowever, existing scenario generation tools often require programming\nknowledge, limiting accessibility for non-technical users. To address this\nlimitation, we present an interactive, no-code framework for scenario\ngeneration. Our framework features a graphical interface that enables users to\ncreate, modify, save, load, and execute scenarios without needing coding\nexpertise or detailed simulation knowledge. Unlike script-based tools such as\nScenic or ScenarioRunner, our approach lowers the barrier to entry and supports\na broader user base. Central to our framework is a graph-based scenario\nrepresentation that facilitates structured management, supports both manual and\nautomated generation, and enables integration with deep learning-based scenario\nand behavior generation methods. In automated mode, the framework can randomly\nsample parameters such as actor types, behaviors, and environmental conditions,\nallowing the generation of diverse and realistic test datasets. By simplifying\nthe scenario generation process, this framework supports more efficient testing\nworkflows and increases the accessibility of simulation-based validation for\nresearchers, engineers, and policymakers.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u7f16\u7a0b\u7684\u4ea4\u4e92\u5f0f\u573a\u666f\u751f\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u9a8c\u8bc1\uff0c\u901a\u8fc7\u56fe\u5f62\u754c\u9762\u964d\u4f4e\u4f7f\u7528\u95e8\u69db\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u9a8c\u8bc1\u9700\u8981\u5927\u89c4\u6a21\u573a\u666f\u6d4b\u8bd5\uff0c\u4f46\u73b0\u6709\u5de5\u5177\u4f9d\u8d56\u7f16\u7a0b\u77e5\u8bc6\uff0c\u9650\u5236\u4e86\u975e\u6280\u672f\u7528\u6237\u7684\u4f7f\u7528\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u56fe\u5f62\u7684\u4ea4\u4e92\u5f0f\u6846\u67b6\uff0c\u652f\u6301\u624b\u52a8\u548c\u81ea\u52a8\u751f\u6210\u573a\u666f\uff0c\u5e76\u53ef\u96c6\u6210\u6df1\u5ea6\u5b66\u4e60\u751f\u6210\u65b9\u6cd5\u3002", "result": "\u6846\u67b6\u80fd\u591f\u9ad8\u6548\u751f\u6210\u591a\u6837\u5316\u7684\u6d4b\u8bd5\u573a\u666f\uff0c\u63d0\u5347\u9a8c\u8bc1\u7684\u53ef\u8bbf\u95ee\u6027\u548c\u6548\u7387\u3002", "conclusion": "\u8be5\u6846\u67b6\u7b80\u5316\u4e86\u573a\u666f\u751f\u6210\u6d41\u7a0b\uff0c\u4e3a\u7814\u7a76\u4eba\u5458\u3001\u5de5\u7a0b\u5e08\u548c\u653f\u7b56\u5236\u5b9a\u8005\u63d0\u4f9b\u4e86\u66f4\u4fbf\u6377\u7684\u9a8c\u8bc1\u5de5\u5177\u3002"}}
{"id": "2507.19788", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19788", "abs": "https://arxiv.org/abs/2507.19788", "authors": ["Rifny Rachman", "Josh Tingey", "Richard Allmendinger", "Pradyumn Shukla", "Wei Pan"], "title": "Reinforcement Learning for Multi-Objective Multi-Echelon Supply Chain Optimisation", "comment": null, "summary": "This study develops a generalised multi-objective, multi-echelon supply chain\noptimisation model with non-stationary markets based on a Markov decision\nprocess, incorporating economic, environmental, and social considerations. The\nmodel is evaluated using a multi-objective reinforcement learning (RL) method,\nbenchmarked against an originally single-objective RL algorithm modified with\nweighted sum using predefined weights, and a multi-objective evolutionary\nalgorithm (MOEA)-based approach. We conduct experiments on varying network\ncomplexities, mimicking typical real-world challenges using a customisable\nsimulator. The model determines production and delivery quantities across\nsupply chain routes to achieve near-optimal trade-offs between competing\nobjectives, approximating Pareto front sets. The results demonstrate that the\nprimary approach provides the most balanced trade-off between optimality,\ndiversity, and density, further enhanced with a shared experience buffer that\nallows knowledge transfer among policies. In complex settings, it achieves up\nto 75\\% higher hypervolume than the MOEA-based method and generates solutions\nthat are approximately eleven times denser, signifying better robustness, than\nthose produced by the modified single-objective RL method. Moreover, it ensures\nstable production and inventory levels while minimising demand loss.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u7684\u5e7f\u4e49\u591a\u76ee\u6807\u3001\u591a\u5c42\u6b21\u4f9b\u5e94\u94fe\u4f18\u5316\u6a21\u578b\uff0c\u7ed3\u5408\u7ecf\u6d4e\u3001\u73af\u5883\u548c\u793e\u4f1a\u56e0\u7d20\uff0c\u5e76\u901a\u8fc7\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u8fdb\u884c\u8bc4\u4f30\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u975e\u7a33\u6001\u5e02\u573a\u4e0b\u7684\u4f9b\u5e94\u94fe\u4f18\u5316\u95ee\u9898\uff0c\u540c\u65f6\u517c\u987e\u7ecf\u6d4e\u3001\u73af\u5883\u548c\u793e\u4f1a\u76ee\u6807\u3002", "method": "\u91c7\u7528\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u65b9\u6cd5\uff0c\u5e76\u4e0e\u52a0\u6743\u548c\u5355\u76ee\u6807RL\u7b97\u6cd5\u53ca\u591a\u76ee\u6807\u8fdb\u5316\u7b97\u6cd5\uff08MOEA\uff09\u8fdb\u884c\u5bf9\u6bd4\u3002\u901a\u8fc7\u81ea\u5b9a\u4e49\u6a21\u62df\u5668\u5728\u4e0d\u540c\u7f51\u7edc\u590d\u6742\u5ea6\u4e0b\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u4e3b\u8981\u65b9\u6cd5\u5728\u6700\u4f18\u6027\u3001\u591a\u6837\u6027\u548c\u5bc6\u5ea6\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u6bd4MOEA\u65b9\u6cd5\u9ad8\u51fa75%\u7684\u8d85\u4f53\u79ef\uff0c\u4e14\u89e3\u51b3\u65b9\u6848\u5bc6\u5ea6\u662f\u5355\u76ee\u6807RL\u65b9\u6cd5\u768411\u500d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u73af\u5883\u4e0b\u80fd\u5b9e\u73b0\u7a33\u5b9a\u7684\u751f\u4ea7\u548c\u5e93\u5b58\u6c34\u5e73\uff0c\u540c\u65f6\u6700\u5c0f\u5316\u9700\u6c42\u635f\u5931\uff0c\u8868\u73b0\u51fa\u66f4\u597d\u7684\u9c81\u68d2\u6027\u548c\u5e73\u8861\u6027\u3002"}}
{"id": "2507.19914", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.19914", "abs": "https://arxiv.org/abs/2507.19914", "authors": ["Akram Khairi", "Hussain Sajwani", "Abdallah Mohammad Alkilany", "Laith AbuAssi", "Mohamad Halwani", "Islam Mohamed Zaid", "Ahmed Awadalla", "Dewald Swart", "Abdulla Ayyad", "Yahya Zweiri"], "title": "High-Speed Event Vision-Based Tactile Roller Sensor for Large Surface Measurements", "comment": "14 pages, 11 figures", "summary": "Inspecting large-scale industrial surfaces like aircraft fuselages for\nquality control requires capturing their precise 3D surface geometry at high\nresolution. Vision-based tactile sensors (VBTSs) offer high local resolution\nbut require slow 'press-and-lift' measurements stitched for large areas.\nApproaches with sliding or roller/belt VBTS designs provide measurements\ncontinuity. However, they face significant challenges respectively: sliding\nstruggles with friction/wear and both approaches are speed-limited by\nconventional camera frame rates and motion blur, making large-area scanning\ntime consuming. Thus, a rapid, continuous, high-resolution method is needed. We\nintroduce a novel tactile sensor integrating a neuromorphic camera in a rolling\nmechanism to achieve this. Leveraging its high temporal resolution and\nrobustness to motion blur, our system uses a modified event-based multi-view\nstereo approach for 3D reconstruction. We demonstrate state-of-the-art scanning\nspeeds up to 0.5 m/s, achieving Mean Absolute Error below 100 microns -- 11\ntimes faster than prior continuous tactile sensing methods. A multi-reference\nBayesian fusion strategy enhances accuracy (reducing MAE by 25.2\\% compared to\nEMVS) and mitigates curvature errors. We also validate high-speed feature\nrecognition via Braille reading 2.6 times faster than previous approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u89e6\u89c9\u4f20\u611f\u5668\uff0c\u7ed3\u5408\u795e\u7ecf\u5f62\u6001\u76f8\u673a\u548c\u6eda\u52a8\u673a\u5236\uff0c\u5b9e\u73b0\u5feb\u901f\u3001\u8fde\u7eed\u3001\u9ad8\u5206\u8fa8\u7387\u76843D\u8868\u9762\u626b\u63cf\u3002", "motivation": "\u73b0\u6709\u89e6\u89c9\u4f20\u611f\u5668\u5728\u5927\u9762\u79ef\u626b\u63cf\u4e2d\u5b58\u5728\u901f\u5ea6\u6162\u3001\u5206\u8fa8\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u9700\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u795e\u7ecf\u5f62\u6001\u76f8\u673a\u548c\u6eda\u52a8\u673a\u5236\uff0c\u7ed3\u5408\u4e8b\u4ef6\u9a71\u52a8\u7684\u591a\u89c6\u89d2\u7acb\u4f53\u89c6\u89c9\u65b9\u6cd5\u8fdb\u884c3D\u91cd\u5efa\uff0c\u5e76\u4f7f\u7528\u8d1d\u53f6\u65af\u878d\u5408\u7b56\u7565\u63d0\u5347\u7cbe\u5ea6\u3002", "result": "\u626b\u63cf\u901f\u5ea6\u8fbe0.5 m/s\uff0c\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u4f4e\u4e8e100\u5fae\u7c73\uff0c\u6bd4\u73b0\u6709\u65b9\u6cd5\u5feb11\u500d\uff1b\u7279\u5f81\u8bc6\u522b\u901f\u5ea6\u63d0\u53472.6\u500d\u3002", "conclusion": "\u65b0\u578b\u4f20\u611f\u5668\u663e\u8457\u63d0\u5347\u4e86\u626b\u63cf\u901f\u5ea6\u548c\u7cbe\u5ea6\uff0c\u9002\u7528\u4e8e\u5de5\u4e1a\u8868\u9762\u68c0\u6d4b\u3002"}}
{"id": "2507.19882", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19882", "abs": "https://arxiv.org/abs/2507.19882", "authors": ["Xinshu Li", "Ruoyu Wang", "Erdun Gao", "Mingming Gong", "Lina Yao"], "title": "Causality-aligned Prompt Learning via Diffusion-based Counterfactual Generation", "comment": null, "summary": "Prompt learning has garnered attention for its efficiency over traditional\nmodel training and fine-tuning. However, existing methods, constrained by\ninadequate theoretical foundations, encounter difficulties in achieving\ncausally invariant prompts, ultimately falling short of capturing robust\nfeatures that generalize effectively across categories. To address these\nchallenges, we introduce the $\\textit{\\textbf{DiCap}}$ model, a theoretically\ngrounded $\\textbf{Di}$ffusion-based $\\textbf{C}$ounterf$\\textbf{a}$ctual\n$\\textbf{p}$rompt learning framework, which leverages a diffusion process to\niteratively sample gradients from the marginal and conditional distributions of\nthe causal model, guiding the generation of counterfactuals that satisfy the\nminimal sufficiency criterion. Grounded in rigorous theoretical derivations,\nthis approach guarantees the identifiability of counterfactual outcomes while\nimposing strict bounds on estimation errors. We further employ a contrastive\nlearning framework that leverages the generated counterfactuals, thereby\nenabling the refined extraction of prompts that are precisely aligned with the\ncausal features of the data. Extensive experimental results demonstrate that\nour method performs excellently across tasks such as image classification,\nimage-text retrieval, and visual question answering, with particularly strong\nadvantages in unseen categories.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u53cd\u4e8b\u5b9e\u63d0\u793a\u5b66\u4e60\u6846\u67b6DiCap\uff0c\u901a\u8fc7\u7406\u8bba\u63a8\u5bfc\u751f\u6210\u56e0\u679c\u4e0d\u53d8\u63d0\u793a\uff0c\u63d0\u5347\u8de8\u7c7b\u522b\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5\u56e0\u7f3a\u4e4f\u7406\u8bba\u652f\u6301\uff0c\u96be\u4ee5\u751f\u6210\u56e0\u679c\u4e0d\u53d8\u63d0\u793a\uff0c\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002", "method": "DiCap\u5229\u7528\u6269\u6563\u8fc7\u7a0b\u4ece\u56e0\u679c\u6a21\u578b\u7684\u8fb9\u9645\u548c\u6761\u4ef6\u5206\u5e03\u4e2d\u8fed\u4ee3\u91c7\u6837\u68af\u5ea6\uff0c\u751f\u6210\u6ee1\u8db3\u6700\u5c0f\u5145\u5206\u6027\u51c6\u5219\u7684\u53cd\u4e8b\u5b9e\uff0c\u5e76\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\u4f18\u5316\u63d0\u793a\u63d0\u53d6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDiCap\u5728\u56fe\u50cf\u5206\u7c7b\u3001\u56fe\u6587\u68c0\u7d22\u548c\u89c6\u89c9\u95ee\u7b54\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728\u672a\u89c1\u7c7b\u522b\u4e0a\u4f18\u52bf\u663e\u8457\u3002", "conclusion": "DiCap\u901a\u8fc7\u7406\u8bba\u9a71\u52a8\u7684\u53cd\u4e8b\u5b9e\u63d0\u793a\u751f\u6210\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.19947", "categories": ["cs.RO", "cs.CL", "cs.IT", "cs.LG", "cs.SY", "eess.SY", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.19947", "abs": "https://arxiv.org/abs/2507.19947", "authors": ["Supawich Sitdhipol", "Waritwong Sukprasongdee", "Ekapol Chuangsuwanich", "Rina Tse"], "title": "Spatial Language Likelihood Grounding Network for Bayesian Fusion of Human-Robot Observations", "comment": "Accepted to the 2025 IEEE International Conference on Systems, Man,\n  and Cybernetics (SMC)", "summary": "Fusing information from human observations can help robots overcome sensing\nlimitations in collaborative tasks. However, an uncertainty-aware fusion\nframework requires a grounded likelihood representing the uncertainty of human\ninputs. This paper presents a Feature Pyramid Likelihood Grounding Network\n(FP-LGN) that grounds spatial language by learning relevant map image features\nand their relationships with spatial relation semantics. The model is trained\nas a probability estimator to capture aleatoric uncertainty in human language\nusing three-stage curriculum learning. Results showed that FP-LGN matched\nexpert-designed rules in mean Negative Log-Likelihood (NLL) and demonstrated\ngreater robustness with lower standard deviation. Collaborative sensing results\ndemonstrated that the grounded likelihood successfully enabled\nuncertainty-aware fusion of heterogeneous human language observations and robot\nsensor measurements, achieving significant improvements in human-robot\ncollaborative task performance.", "AI": {"tldr": "FP-LGN\u6a21\u578b\u901a\u8fc7\u4e09\u9636\u6bb5\u8bfe\u7a0b\u5b66\u4e60\uff0c\u5c06\u4eba\u7c7b\u7a7a\u95f4\u8bed\u8a00\u4e0e\u5730\u56fe\u7279\u5f81\u5173\u8054\uff0c\u5b9e\u73b0\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u4fe1\u606f\u878d\u5408\uff0c\u63d0\u5347\u4eba\u673a\u534f\u4f5c\u4efb\u52a1\u8868\u73b0\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u611f\u77e5\u5c40\u9650\uff0c\u901a\u8fc7\u878d\u5408\u4eba\u7c7b\u89c2\u5bdf\u4fe1\u606f\u63d0\u5347\u534f\u4f5c\u4efb\u52a1\u6027\u80fd\u3002", "method": "\u63d0\u51faFP-LGN\u6a21\u578b\uff0c\u5b66\u4e60\u5730\u56fe\u7279\u5f81\u4e0e\u7a7a\u95f4\u8bed\u8a00\u5173\u7cfb\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u8bfe\u7a0b\u5b66\u4e60\u4f30\u8ba1\u4e0d\u786e\u5b9a\u6027\u3002", "result": "FP-LGN\u5728NLL\u6307\u6807\u4e0a\u5ab2\u7f8e\u4e13\u5bb6\u89c4\u5219\uff0c\u9c81\u68d2\u6027\u66f4\u5f3a\uff0c\u663e\u8457\u63d0\u5347\u4eba\u673a\u534f\u4f5c\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "FP-LGN\u6210\u529f\u5b9e\u73b0\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u4fe1\u606f\u878d\u5408\uff0c\u4e3a\u4eba\u673a\u534f\u4f5c\u4efb\u52a1\u63d0\u4f9b\u6709\u6548\u652f\u6301\u3002"}}
{"id": "2507.19960", "categories": ["cs.AI", "I.2.0; K.2; K.4.0"], "pdf": "https://arxiv.org/pdf/2507.19960", "abs": "https://arxiv.org/abs/2507.19960", "authors": ["Olivia Guest"], "title": "What Does 'Human-Centred AI' Mean?", "comment": null, "summary": "While it seems sensible that human-centred artificial intelligence (AI) means\ncentring \"human behaviour and experience,\" it cannot be any other way. AI, I\nargue, is usefully seen as a relationship between technology and humans where\nit appears that artifacts can perform, to a greater or lesser extent, human\ncognitive labour. This is evinced using examples that juxtapose technology with\ncognition, inter alia: abacus versus mental arithmetic; alarm clock versus\nknocker-upper; camera versus vision; and sweatshop versus tailor. Using novel\ndefinitions and analyses, sociotechnical relationships can be analysed into\nvarying types of: displacement (harmful), enhancement (beneficial), and/or\nreplacement (neutral) of human cognitive labour. Ultimately, all AI implicates\nhuman cognition; no matter what. Obfuscation of cognition in the AI context --\nfrom clocks to artificial neural networks -- results in distortion, in slowing\ncritical engagement, perverting cognitive science, and indeed in limiting our\nability to truly centre humans and humanity in the engineering of AI systems.\nTo even begin to de-fetishise AI, we must look the human-in-the-loop in the\neyes.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u672c\u8d28\u4e0a\u662f\u6280\u672f\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u7684\u5173\u7cfb\uff0c\u5206\u6790\u4e86AI\u5bf9\u4eba\u7c7b\u8ba4\u77e5\u52b3\u52a8\u7684\u66ff\u4ee3\u3001\u589e\u5f3a\u6216\u53d6\u4ee3\uff0c\u5e76\u5f3a\u8c03\u5ffd\u89c6\u8fd9\u79cd\u5173\u7cfb\u4f1a\u626d\u66f2\u8ba4\u77e5\u79d1\u5b66\u548c\u9650\u5236AI\u7cfb\u7edf\u7684\u771f\u6b63\u4eba\u6027\u5316\u8bbe\u8ba1\u3002", "motivation": "\u65e8\u5728\u6f84\u6e05\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684AI\u7684\u6838\u5fc3\u662f\u6280\u672f\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u7684\u5173\u7cfb\uff0c\u907f\u514d\u56e0\u5ffd\u89c6\u8fd9\u4e00\u5173\u7cfb\u800c\u5bfc\u81f4\u7684\u8ba4\u77e5\u79d1\u5b66\u626d\u66f2\u548cAI\u8bbe\u8ba1\u504f\u5dee\u3002", "method": "\u901a\u8fc7\u5bf9\u6bd4\u6280\u672f\uff08\u5982\u7b97\u76d8\u3001\u95f9\u949f\u3001\u76f8\u673a\uff09\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u52b3\u52a8\uff08\u5982\u5fc3\u7b97\u3001\u4eba\u5de5\u53eb\u9192\u3001\u89c6\u89c9\uff09\uff0c\u63d0\u51fa\u65b0\u7684\u5b9a\u4e49\u548c\u5206\u6790\u6846\u67b6\uff0c\u5c06\u793e\u4f1a\u6280\u672f\u5173\u7cfb\u5206\u4e3a\u66ff\u4ee3\uff08\u6709\u5bb3\uff09\u3001\u589e\u5f3a\uff08\u6709\u76ca\uff09\u6216\u53d6\u4ee3\uff08\u4e2d\u6027\uff09\u4e09\u7c7b\u3002", "result": "\u63ed\u793a\u4e86\u6240\u6709AI\u90fd\u6d89\u53ca\u4eba\u7c7b\u8ba4\u77e5\uff0c\u5ffd\u89c6\u8fd9\u4e00\u70b9\u4f1a\u963b\u788d\u6279\u5224\u6027\u601d\u8003\u3001\u626d\u66f2\u8ba4\u77e5\u79d1\u5b66\uff0c\u5e76\u9650\u5236AI\u7cfb\u7edf\u7684\u4eba\u6027\u5316\u8bbe\u8ba1\u3002", "conclusion": "\u5f3a\u8c03\u5fc5\u987b\u6b63\u89c6AI\u4e2d\u7684\u4eba\u7c7b\u8ba4\u77e5\u89d2\u8272\uff0c\u624d\u80fd\u771f\u6b63\u5b9e\u73b0\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684AI\u8bbe\u8ba1\uff0c\u907f\u514d\u6280\u672f\u5d07\u62dc\u5e26\u6765\u7684\u8d1f\u9762\u5f71\u54cd\u3002"}}
{"id": "2507.19975", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.19975", "abs": "https://arxiv.org/abs/2507.19975", "authors": ["Aude Billard", "Alin Albu-Schaeffer", "Michael Beetz", "Wolfram Burgard", "Peter Corke", "Matei Ciocarlie", "Ravinder Dahiya", "Danica Kragic", "Ken Goldberg", "Yukie Nagai", "Davide Scaramuzza"], "title": "A roadmap for AI in robotics", "comment": null, "summary": "AI technologies, including deep learning, large-language models have gone\nfrom one breakthrough to the other. As a result, we are witnessing growing\nexcitement in robotics at the prospect of leveraging the potential of AI to\ntackle some of the outstanding barriers to the full deployment of robots in our\ndaily lives. However, action and sensing in the physical world pose greater and\ndifferent challenges than analysing data in isolation. As the development and\napplication of AI in robotic products advances, it is important to reflect on\nwhich technologies, among the vast array of network architectures and learning\nmodels now available in the AI field, are most likely to be successfully\napplied to robots; how they can be adapted to specific robot designs, tasks,\nenvironments; which challenges must be overcome. This article offers an\nassessment of what AI for robotics has achieved since the 1990s and proposes a\nshort- and medium-term research roadmap listing challenges and promises. These\nrange from keeping up-to-date large datasets, representatives of a diversity of\ntasks robots may have to perform, and of environments they may encounter, to\ndesigning AI algorithms tailored specifically to robotics problems but generic\nenough to apply to a wide range of applications and transfer easily to a\nvariety of robotic platforms. For robots to collaborate effectively with\nhumans, they must predict human behavior without relying on bias-based\nprofiling. Explainability and transparency in AI-driven robot control are not\noptional but essential for building trust, preventing misuse, and attributing\nresponsibility in accidents. We close on what we view as the primary long-term\nchallenges, that is, to design robots capable of lifelong learning, while\nguaranteeing safe deployment and usage, and sustainable computational costs.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86AI\u5728\u673a\u5668\u4eba\u9886\u57df\u7684\u6210\u5c31\uff0c\u5e76\u63d0\u51fa\u4e86\u77ed\u671f\u548c\u4e2d\u671f\u7684\u7814\u7a76\u8def\u7ebf\u56fe\uff0c\u6db5\u76d6\u6570\u636e\u96c6\u66f4\u65b0\u3001\u7b97\u6cd5\u8bbe\u8ba1\u3001\u4eba\u673a\u534f\u4f5c\u53ca\u5b89\u5168\u6027\u7b49\u6311\u6218\u3002", "motivation": "\u63a2\u8ba8\u5982\u4f55\u5229\u7528AI\u6280\u672f\u89e3\u51b3\u673a\u5668\u4eba\u90e8\u7f72\u4e2d\u7684\u969c\u788d\uff0c\u5e76\u53cd\u601d\u54ea\u4e9bAI\u6280\u672f\u6700\u9002\u5408\u673a\u5668\u4eba\u5e94\u7528\u3002", "method": "\u8bc4\u4f30\u81ea1990\u5e74\u4ee3\u4ee5\u6765AI\u5728\u673a\u5668\u4eba\u9886\u57df\u7684\u8fdb\u5c55\uff0c\u5e76\u63d0\u51fa\u7814\u7a76\u8def\u7ebf\u56fe\uff0c\u5305\u62ec\u6570\u636e\u96c6\u3001\u7b97\u6cd5\u8bbe\u8ba1\u3001\u4eba\u673a\u534f\u4f5c\u548c\u5b89\u5168\u6027\u7b49\u65b9\u9762\u3002", "result": "\u63d0\u51fa\u4e86\u673a\u5668\u4ebaAI\u7814\u7a76\u7684\u77ed\u671f\u548c\u4e2d\u671f\u76ee\u6807\uff0c\u5f3a\u8c03\u6570\u636e\u96c6\u591a\u6837\u6027\u3001\u7b97\u6cd5\u901a\u7528\u6027\u3001\u900f\u660e\u6027\u548c\u5b89\u5168\u6027\u3002", "conclusion": "\u957f\u671f\u6311\u6218\u5728\u4e8e\u8bbe\u8ba1\u5177\u5907\u7ec8\u8eab\u5b66\u4e60\u80fd\u529b\u3001\u5b89\u5168\u90e8\u7f72\u548c\u53ef\u6301\u7eed\u8ba1\u7b97\u6210\u672c\u7684\u673a\u5668\u4eba\u3002"}}
{"id": "2507.19973", "categories": ["cs.AI", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.19973", "abs": "https://arxiv.org/abs/2507.19973", "authors": ["Ebrahim Rasromani", "Stella K. Kang", "Yanqi Xu", "Beisong Liu", "Garvit Luhadia", "Wan Fung Chui", "Felicia L. Pasadyn", "Yu Chih Hung", "Julie Y. An", "Edwin Mathieu", "Zehui Gu", "Carlos Fernandez-Granda", "Ammar A. Javed", "Greg D. Sacks", "Tamas Gonda", "Chenchan Huang", "Yiqiu Shen"], "title": "Leveraging Fine-Tuned Large Language Models for Interpretable Pancreatic Cystic Lesion Feature Extraction and Risk Categorization", "comment": null, "summary": "Background: Manual extraction of pancreatic cystic lesion (PCL) features from\nradiology reports is labor-intensive, limiting large-scale studies needed to\nadvance PCL research. Purpose: To develop and evaluate large language models\n(LLMs) that automatically extract PCL features from MRI/CT reports and assign\nrisk categories based on guidelines. Materials and Methods: We curated a\ntraining dataset of 6,000 abdominal MRI/CT reports (2005-2024) from 5,134\npatients that described PCLs. Labels were generated by GPT-4o using\nchain-of-thought (CoT) prompting to extract PCL and main pancreatic duct\nfeatures. Two open-source LLMs were fine-tuned using QLoRA on GPT-4o-generated\nCoT data. Features were mapped to risk categories per institutional guideline\nbased on the 2017 ACR White Paper. Evaluation was performed on 285 held-out\nhuman-annotated reports. Model outputs for 100 cases were independently\nreviewed by three radiologists. Feature extraction was evaluated using exact\nmatch accuracy, risk categorization with macro-averaged F1 score, and\nradiologist-model agreement with Fleiss' Kappa. Results: CoT fine-tuning\nimproved feature extraction accuracy for LLaMA (80% to 97%) and DeepSeek (79%\nto 98%), matching GPT-4o (97%). Risk categorization F1 scores also improved\n(LLaMA: 0.95; DeepSeek: 0.94), closely matching GPT-4o (0.97), with no\nstatistically significant differences. Radiologist inter-reader agreement was\nhigh (Fleiss' Kappa = 0.888) and showed no statistically significant difference\nwith the addition of DeepSeek-FT-CoT (Fleiss' Kappa = 0.893) or GPT-CoT\n(Fleiss' Kappa = 0.897), indicating that both models achieved agreement levels\non par with radiologists. Conclusion: Fine-tuned open-source LLMs with CoT\nsupervision enable accurate, interpretable, and efficient phenotyping for\nlarge-scale PCL research, achieving performance comparable to GPT-4o.", "AI": {"tldr": "\u901a\u8fc7\u5fae\u8c03\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u76d1\u7763\uff0c\u5b9e\u73b0\u4e86\u4eceMRI/CT\u62a5\u544a\u4e2d\u81ea\u52a8\u63d0\u53d6\u80f0\u817a\u56ca\u6027\u75c5\u53d8\uff08PCL\uff09\u7279\u5f81\u5e76\u5206\u7c7b\u98ce\u9669\uff0c\u6027\u80fd\u63a5\u8fd1GPT-4o\u3002", "motivation": "\u624b\u52a8\u63d0\u53d6PCL\u7279\u5f81\u8017\u65f6\u4e14\u9650\u5236\u5927\u89c4\u6a21\u7814\u7a76\uff0c\u9700\u81ea\u52a8\u5316\u5de5\u5177\u652f\u6301\u3002", "method": "\u4f7f\u7528GPT-4o\u751f\u6210\u7684CoT\u6570\u636e\u5fae\u8c03LLaMA\u548cDeepSeek\u6a21\u578b\uff0c\u57fa\u4e8eACR\u6307\u5357\u5206\u7c7b\u98ce\u9669\uff0c\u8bc4\u4f30\u91c7\u7528\u51c6\u786e\u7387\u548cF1\u5206\u6570\u3002", "result": "\u5fae\u8c03\u540eLLaMA\u548cDeepSeek\u7684\u51c6\u786e\u7387\u5206\u522b\u63d0\u5347\u81f397%\u548c98%\uff0c\u98ce\u9669\u5206\u7c7bF1\u5206\u6570\u8fbe0.95\u548c0.94\uff0c\u4e0eGPT-4o\u65e0\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "\u5fae\u8c03\u5f00\u6e90LLM\u7ed3\u5408CoT\u76d1\u7763\u53ef\u5b9e\u73b0\u9ad8\u6548\u3001\u51c6\u786e\u7684PCL\u7279\u5f81\u63d0\u53d6\uff0c\u6027\u80fd\u5ab2\u7f8eGPT-4o\u3002"}}
{"id": "2507.19983", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.19983", "abs": "https://arxiv.org/abs/2507.19983", "authors": ["Yuhong Deng", "Chao Tang", "Cunjun Yu", "Linfeng Li", "David Hsu"], "title": "CLASP: General-Purpose Clothes Manipulation with Semantic Keypoints", "comment": null, "summary": "Clothes manipulation, such as folding or hanging, is a critical capability\nfor home service robots. Despite recent advances, most existing methods remain\nlimited to specific tasks and clothes types, due to the complex,\nhigh-dimensional geometry of clothes. This paper presents CLothes mAnipulation\nwith Semantic keyPoints (CLASP), which aims at general-purpose clothes\nmanipulation over different clothes types, T-shirts, shorts, skirts, long\ndresses, ... , as well as different tasks, folding, flattening, hanging, ... .\nThe core idea of CLASP is semantic keypoints -- e.g., ''left sleeve'', ''right\nshoulder'', etc. -- a sparse spatial-semantic representation that is salient\nfor both perception and action. Semantic keypoints of clothes can be reliably\nextracted from RGB-D images and provide an effective intermediate\nrepresentation of clothes manipulation policies. CLASP uses semantic keypoints\nto bridge high-level task planning and low-level action execution. At the high\nlevel, it exploits vision language models (VLMs) to predict task plans over the\nsemantic keypoints. At the low level, it executes the plans with the help of a\nsimple pre-built manipulation skill library. Extensive simulation experiments\nshow that CLASP outperforms state-of-the-art baseline methods on multiple tasks\nacross diverse clothes types, demonstrating strong performance and\ngeneralization. Further experiments with a Franka dual-arm system on four\ndistinct tasks -- folding, flattening, hanging, and placing -- confirm CLASP's\nperformance on a real robot.", "AI": {"tldr": "CLASP\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u4e49\u5173\u952e\u70b9\u7684\u901a\u7528\u8863\u7269\u64cd\u4f5c\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u8863\u7269\u7c7b\u578b\u548c\u4efb\u52a1\uff0c\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u9884\u6784\u5efa\u6280\u80fd\u5e93\u5b9e\u73b0\u9ad8\u6548\u64cd\u4f5c\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u8863\u7269\u64cd\u4f5c\u65b9\u6cd5\u5c40\u9650\u4e8e\u7279\u5b9a\u4efb\u52a1\u548c\u8863\u7269\u7c7b\u578b\u7684\u95ee\u9898\uff0c\u5229\u7528\u8bed\u4e49\u5173\u952e\u70b9\u5b9e\u73b0\u901a\u7528\u6027\u3002", "method": "\u4f7f\u7528\u8bed\u4e49\u5173\u952e\u70b9\u4f5c\u4e3a\u4e2d\u95f4\u8868\u793a\uff0c\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4efb\u52a1\u89c4\u5212\uff0c\u5e76\u901a\u8fc7\u9884\u6784\u5efa\u6280\u80fd\u5e93\u6267\u884c\u64cd\u4f5c\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\uff0cCLASP\u5728\u591a\u79cd\u4efb\u52a1\u548c\u8863\u7269\u7c7b\u578b\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "CLASP\u901a\u8fc7\u8bed\u4e49\u5173\u952e\u70b9\u5b9e\u73b0\u4e86\u901a\u7528\u8863\u7269\u64cd\u4f5c\uff0c\u5c55\u793a\u4e86\u5f3a\u6cdb\u5316\u80fd\u529b\u548c\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.19974", "categories": ["cs.AI", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.19974", "abs": "https://arxiv.org/abs/2507.19974", "authors": ["Tongjie Li", "Jianhua Zhang", "Li Yu", "Yuxiang Zhang", "Yunlong Cai", "Fan Xu", "Guangyi Liu"], "title": "Digital Twin Channel-Enabled Online Resource Allocation for 6G: Principle, Architecture and Application", "comment": null, "summary": "Emerging applications such as holographic communication, autonomous driving,\nand the industrial Internet of Things impose stringent requirements on\nflexible, low-latency, and reliable resource allocation in 6G networks.\nConventional methods, which rely on statistical modeling, have proven effective\nin general contexts but may fail to achieve optimal performance in specific and\ndynamic environments. Furthermore, acquiring real-time channel state\ninformation (CSI) typically requires excessive pilot overhead. To address these\nchallenges, a digital twin channel (DTC)-enabled online optimization framework\nis proposed, in which DTC is employed to predict CSI based on environmental\nsensing. The predicted CSI is then utilized by lightweight game-theoretic\nalgorithms to perform online resource allocation in a timely and efficient\nmanner. Simulation results based on a digital replica of a realistic industrial\nworkshop demonstrate that the proposed method achieves throughput improvements\nof up to 11.5\\% compared with pilot-based ideal CSI schemes, validating its\neffectiveness for scalable, low-overhead, and environment-aware communication\nin future 6G networks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6570\u5b57\u5b6a\u751f\u4fe1\u9053\uff08DTC\uff09\u7684\u5728\u7ebf\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8e6G\u7f51\u7edc\u4e2d\u9ad8\u6548\u3001\u4f4e\u5ef6\u8fdf\u7684\u8d44\u6e90\u5206\u914d\u3002", "motivation": "6G\u7f51\u7edc\u4e2d\u65b0\u5174\u5e94\u7528\u5bf9\u7075\u6d3b\u3001\u4f4e\u5ef6\u8fdf\u548c\u53ef\u9760\u8d44\u6e90\u5206\u914d\u7684\u9700\u6c42\uff0c\u4f20\u7edf\u7edf\u8ba1\u5efa\u6a21\u65b9\u6cd5\u5728\u52a8\u6001\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u5b9e\u65f6\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\uff08CSI\uff09\u83b7\u53d6\u6210\u672c\u9ad8\u3002", "method": "\u5229\u7528DTC\u9884\u6d4bCSI\uff0c\u7ed3\u5408\u8f7b\u91cf\u7ea7\u535a\u5f08\u8bba\u7b97\u6cd5\u8fdb\u884c\u5728\u7ebf\u8d44\u6e90\u5206\u914d\u3002", "result": "\u4eff\u771f\u663e\u793a\uff0c\u76f8\u6bd4\u57fa\u4e8e\u5bfc\u9891\u7684\u7406\u60f3CSI\u65b9\u6848\uff0c\u541e\u5410\u91cf\u63d0\u5347\u9ad8\u8fbe11.5%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a6G\u7f51\u7edc\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u4f4e\u5f00\u9500\u4e14\u73af\u5883\u611f\u77e5\u7684\u901a\u4fe1\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.19999", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.19999", "abs": "https://arxiv.org/abs/2507.19999", "authors": ["Laura Treers", "Daniel Soto", "Joonha Hwang", "Michael A. D. Goodisman", "Daniel I. Goldman"], "title": "Robot Excavation and Manipulation of Geometrically Cohesive Granular Media", "comment": null, "summary": "Construction throughout history typically assumes that its blueprints and\nbuilding blocks are pre-determined. However, recent work suggests that\nalternative approaches can enable new paradigms for structure formation.\nAleatory architectures, or those which rely on the properties of their granular\nbuilding blocks rather than pre-planned design or computation, have thus far\nrelied on human intervention for their creation. We imagine that robotic swarms\ncould be valuable to create such aleatory structures by manipulating and\nforming structures from entangled granular materials. To discover principles by\nwhich robotic systems can effectively manipulate soft matter, we develop a\nrobophysical model for interaction with geometrically cohesive granular media\ncomposed of u-shape particles. This robotic platform uses environmental signals\nto autonomously coordinate excavation, transport, and deposition of material.\nWe test the effect of substrate initial conditions by characterizing robot\nperformance in two different material compaction states and observe as much as\na 75% change in transported mass depending on initial substrate compressive\nloading. These discrepancies suggest the functional role that material\nproperties such as packing and cohesion/entanglement play in excavation and\nconstruction. To better understand these material properties, we develop an\napparatus for tensile testing of the geometrically cohesive substrates, which\nreveals how entangled material strength responds strongly to initial\ncompressive loading. These results explain the variation observed in robotic\nperformance and point to future directions for better understanding robotic\ninteraction mechanics with entangled materials.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5229\u7528\u673a\u5668\u4eba\u7fa4\u4f53\u64cd\u7eb5\u9897\u7c92\u6750\u6599\u6784\u5efa\u975e\u786e\u5b9a\u6027\u7ed3\u6784\u7684\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6750\u6599\u521d\u59cb\u6761\u4ef6\u5bf9\u673a\u5668\u4eba\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u4f20\u7edf\u5efa\u7b51\u4f9d\u8d56\u9884\u8bbe\u84dd\u56fe\u548c\u6750\u6599\uff0c\u800c\u7814\u7a76\u63a2\u7d22\u4e86\u901a\u8fc7\u9897\u7c92\u6750\u6599\u7684\u81ea\u7ec4\u7ec7\u7279\u6027\u5b9e\u73b0\u975e\u786e\u5b9a\u6027\u7ed3\u6784\u6784\u5efa\u7684\u53ef\u80fd\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u673a\u5668\u4eba\u7269\u7406\u6a21\u578b\uff0c\u7528\u4e8e\u4e0e\u51e0\u4f55\u7c98\u6027\u9897\u7c92\u4ecb\u8d28\uff08U\u5f62\u9897\u7c92\uff09\u4ea4\u4e92\uff0c\u5e76\u901a\u8fc7\u73af\u5883\u4fe1\u53f7\u81ea\u4e3b\u534f\u8c03\u6316\u6398\u3001\u8fd0\u8f93\u548c\u6c89\u79ef\u6750\u6599\u3002", "result": "\u5b9e\u9a8c\u53d1\u73b0\uff0c\u6750\u6599\u521d\u59cb\u538b\u7f29\u72b6\u6001\u5bf9\u673a\u5668\u4eba\u8fd0\u8f93\u8d28\u91cf\u5f71\u54cd\u663e\u8457\uff08\u9ad8\u8fbe75%\u53d8\u5316\uff09\uff0c\u63ed\u793a\u4e86\u6750\u6599\u7279\u6027\uff08\u5982\u5806\u79ef\u548c\u7ea0\u7f20\uff09\u5728\u6316\u6398\u548c\u6784\u5efa\u4e2d\u7684\u529f\u80fd\u4f5c\u7528\u3002", "conclusion": "\u7814\u7a76\u4e3a\u7406\u89e3\u673a\u5668\u4eba\u4e0e\u7ea0\u7f20\u6750\u6599\u7684\u4ea4\u4e92\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2507.20000", "categories": ["cs.AI", "cs.DL"], "pdf": "https://arxiv.org/pdf/2507.20000", "abs": "https://arxiv.org/abs/2507.20000", "authors": ["Renaud Fabre", "Daniel Egret", "Patrice Bellot"], "title": "Matching Game Preferences Through Dialogical Large Language Models: A Perspective", "comment": "28 pages, 1 figure. Published in Applied Sciences", "summary": "This perspective paper explores the future potential of \"conversational\nintelligence\" by examining how Large Language Models (LLMs) could be combined\nwith GRAPHYP's network system to better understand human conversations and\npreferences. Using recent research and case studies, we propose a conceptual\nframework that could make AI rea-soning transparent and traceable, allowing\nhumans to see and understand how AI reaches its conclusions. We present the\nconceptual perspective of \"Matching Game Preferences through Dialogical Large\nLanguage Models (D-LLMs),\" a proposed system that would allow multiple users to\nshare their different preferences through structured conversations. This\napproach envisions personalizing LLMs by embedding individual user preferences\ndirectly into how the model makes decisions. The proposed D-LLM framework would\nrequire three main components: (1) reasoning processes that could analyze\ndifferent search experiences and guide performance, (2) classification systems\nthat would identify user preference patterns, and (3) dialogue approaches that\ncould help humans resolve conflicting information. This perspective framework\naims to create an interpretable AI system where users could examine,\nunderstand, and combine the different human preferences that influence AI\nresponses, detected through GRAPHYP's search experience networks. The goal of\nthis perspective is to envision AI systems that would not only provide answers\nbut also show users how those answers were reached, making artificial\nintelligence more transparent and trustworthy for human decision-making.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e0eGRAPHYP\u7f51\u7edc\u7cfb\u7edf\u7ed3\u5408\uff0c\u901a\u8fc7\u5bf9\u8bdd\u5f0f\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08D-LLMs\uff09\u5b9e\u73b0\u900f\u660e\u5316AI\u63a8\u7406\uff0c\u4ee5\u66f4\u597d\u5730\u7406\u89e3\u548c\u5339\u914d\u7528\u6237\u504f\u597d\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63d0\u5347AI\u7cfb\u7edf\u7684\u900f\u660e\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4f7f\u5176\u4e0d\u4ec5\u63d0\u4f9b\u7b54\u6848\uff0c\u8fd8\u80fd\u5c55\u793a\u63a8\u7406\u8fc7\u7a0b\uff0c\u4ece\u800c\u589e\u5f3a\u7528\u6237\u4fe1\u4efb\u3002", "method": "\u63d0\u51faD-LLM\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u63a8\u7406\u8fc7\u7a0b\u3001\u504f\u597d\u5206\u7c7b\u7cfb\u7edf\u548c\u5bf9\u8bdd\u65b9\u6cd5\uff0c\u901a\u8fc7GRAPHYP\u7f51\u7edc\u5206\u6790\u7528\u6237\u504f\u597d\u3002", "result": "\u6982\u5ff5\u6027\u6846\u67b6\u5c55\u793a\u4e86\u5982\u4f55\u901a\u8fc7\u7ed3\u6784\u5316\u5bf9\u8bdd\u5b9e\u73b0\u4e2a\u6027\u5316LLMs\uff0c\u5e76\u652f\u6301\u7528\u6237\u7406\u89e3\u548c\u6574\u5408AI\u7684\u51b3\u7b56\u4f9d\u636e\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6784\u5efa\u900f\u660e\u3001\u53ef\u89e3\u91ca\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u6709\u671b\u63d0\u5347AI\u5728\u51b3\u7b56\u652f\u6301\u4e2d\u7684\u53ef\u4fe1\u5ea6\u3002"}}
{"id": "2507.20002", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.20002", "abs": "https://arxiv.org/abs/2507.20002", "authors": ["Peiyao Hou", "Danning Sun", "Meng Wang", "Yuzhe Huang", "Zeyu Zhang", "Hangxin Liu", "Wanlin Li", "Ziyuan Jiao"], "title": "SuperMag: Vision-based Tactile Data Guided High-resolution Tactile Shape Reconstruction for Magnetic Tactile Sensors", "comment": "7 pages, 7 figures; accepted by IROS 2025", "summary": "Magnetic-based tactile sensors (MBTS) combine the advantages of compact\ndesign and high-frequency operation but suffer from limited spatial resolution\ndue to their sparse taxel arrays. This paper proposes SuperMag, a tactile shape\nreconstruction method that addresses this limitation by leveraging\nhigh-resolution vision-based tactile sensor (VBTS) data to supervise MBTS\nsuper-resolution. Co-designed, open-source VBTS and MBTS with identical contact\nmodules enable synchronized data collection of high-resolution shapes and\nmagnetic signals via a symmetric calibration setup. We frame tactile shape\nreconstruction as a conditional generative problem, employing a conditional\nvariational auto-encoder to infer high-resolution shapes from low-resolution\nMBTS inputs. The MBTS achieves a sampling frequency of 125 Hz, whereas the\nshape reconstruction sustains an inference time within 2.5 ms. This\ncross-modality synergy advances tactile perception of the MBTS, potentially\nunlocking its new capabilities in high-precision robotic tasks.", "AI": {"tldr": "SuperMag\u5229\u7528\u9ad8\u5206\u8fa8\u7387\u89c6\u89c9\u89e6\u89c9\u4f20\u611f\u5668\u6570\u636e\u76d1\u7763\u78c1\u89e6\u89c9\u4f20\u611f\u5668\u7684\u8d85\u5206\u8fa8\u7387\u91cd\u5efa\uff0c\u63d0\u5347\u5176\u7a7a\u95f4\u5206\u8fa8\u7387\u3002", "motivation": "\u78c1\u89e6\u89c9\u4f20\u611f\u5668\uff08MBTS\uff09\u8bbe\u8ba1\u7d27\u51d1\u4e14\u9ad8\u9891\u8fd0\u884c\uff0c\u4f46\u7a00\u758f\u7684\u89e6\u89c9\u9635\u5217\u9650\u5236\u4e86\u5176\u7a7a\u95f4\u5206\u8fa8\u7387\u3002", "method": "\u901a\u8fc7\u540c\u6b65\u6536\u96c6\u9ad8\u5206\u8fa8\u7387\u89c6\u89c9\u89e6\u89c9\u4f20\u611f\u5668\uff08VBTS\uff09\u548cMBTS\u6570\u636e\uff0c\u91c7\u7528\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668\u4ece\u4f4e\u5206\u8fa8\u7387MBTS\u8f93\u5165\u63a8\u65ad\u9ad8\u5206\u8fa8\u7387\u5f62\u72b6\u3002", "result": "MBTS\u91c7\u6837\u9891\u7387\u8fbe125 Hz\uff0c\u5f62\u72b6\u91cd\u5efa\u63a8\u7406\u65f6\u95f4\u4f4e\u4e8e2.5 ms\uff0c\u663e\u8457\u63d0\u5347\u89e6\u89c9\u611f\u77e5\u80fd\u529b\u3002", "conclusion": "\u8de8\u6a21\u6001\u534f\u540c\u4e3aMBTS\u89e3\u9501\u4e86\u9ad8\u7cbe\u5ea6\u673a\u5668\u4eba\u4efb\u52a1\u7684\u65b0\u6f5c\u529b\u3002"}}
{"id": "2507.20010", "categories": ["cs.AI", "cs.GT", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.20010", "abs": "https://arxiv.org/abs/2507.20010", "authors": ["M\u00fcge Fidan", "Esra Erdem"], "title": "Finding Personalized Good-Enough Solutions to Unsatisfiable Stable Roommates Problems", "comment": null, "summary": "The Stable Roommates problems are characterized by the preferences of agents\nover other agents as roommates. A solution is a partition of the agents into\npairs that are acceptable to each other (i.e., they are in the preference lists\nof each other), and the matching is stable (i.e., there do not exist any two\nagents who prefer each other to their roommates, and thus block the matching).\nMotivated by real-world applications, and considering that stable roommates\nproblems do not always have solutions, we continue our studies to compute\n\"good-enough\" matchings. In addition to the agents' habits and habitual\npreferences, we consider their networks of preferred friends, and introduce a\nmethod to generate personalized solutions to stable roommates problems. We\nillustrate the usefulness of our method with examples and empirical\nevaluations.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u7a33\u5b9a\u5ba4\u53cb\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4ee3\u7406\u4e60\u60ef\u548c\u504f\u597d\u7f51\u7edc\u7684\u4e2a\u6027\u5316\u5339\u914d\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edf\u95ee\u9898\u4e2d\u53ef\u80fd\u65e0\u89e3\u7684\u60c5\u51b5\u3002", "motivation": "\u73b0\u5b9e\u5e94\u7528\u4e2d\uff0c\u7a33\u5b9a\u5ba4\u53cb\u95ee\u9898\u5e76\u4e0d\u603b\u662f\u6709\u89e3\uff0c\u56e0\u6b64\u9700\u8981\u5bfb\u627e\u201c\u8db3\u591f\u597d\u201d\u7684\u5339\u914d\u65b9\u6848\u3002", "method": "\u7ed3\u5408\u4ee3\u7406\u7684\u4e60\u60ef\u3001\u504f\u597d\u53ca\u5176\u793e\u4ea4\u7f51\u7edc\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u751f\u6210\u4e2a\u6027\u5316\u5339\u914d\u7684\u65b9\u6cd5\u3002", "result": "\u901a\u8fc7\u793a\u4f8b\u548c\u5b9e\u8bc1\u8bc4\u4f30\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u89e3\u51b3\u7a33\u5b9a\u5ba4\u53cb\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u884c\u7684\u4e2a\u6027\u5316\u5339\u914d\u65b9\u6848\u3002"}}
{"id": "2507.20021", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.20021", "abs": "https://arxiv.org/abs/2507.20021", "authors": ["Matin Aghaei", "Mohammad Ali Alomrani", "Yingxue Zhang", "Mahdi Biparva"], "title": "When Engineering Outruns Intelligence: A Re-evaluation of Instruction-Guided Navigation", "comment": null, "summary": "Large language models (LLMs) are often credited with recent leaps in\nObjectGoal Navigation, yet the extent to which they improve planning remains\nunclear. We revisit this question on the HM3D-v1 validation split. First, we\nstrip InstructNav of its Dynamic Chain-of-Navigation prompt, open-vocabulary\nGLEE detector and Intuition saliency map, and replace them with a simple\nDistance-Weighted Frontier Explorer (DWFE). This geometry-only heuristic raises\nSuccess from 58.0% to 61.1% and lifts SPL from 20.9% to 36.0% over 2 000\nvalidation episodes, outperforming all previous training-free baselines.\nSecond, we add a lightweight language prior (SHF); on a 200-episode subset this\nyields a further +2% Success and +0.9% SPL while shortening paths by five steps\non average. Qualitative trajectories confirm the trend: InstructNav back-tracks\nand times-out, DWFE reaches the goal after a few islands, and SHF follows an\nalmost straight route. Our results indicate that frontier geometry, not\nemergent LLM reasoning, drives most reported gains, and suggest that\nmetric-aware prompts or offline semantic graphs are necessary before\nattributing navigation success to \"LLM intelligence.\"", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u76ee\u6807\u5bfc\u822a\u4efb\u52a1\u4e2d\uff0c\u51e0\u4f55\u542f\u53d1\u5f0f\u65b9\u6cd5\uff08DWFE\uff09\u6bd4\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u66f4\u80fd\u63d0\u5347\u6027\u80fd\uff0c\u800c\u8f7b\u91cf\u7ea7\u8bed\u8a00\u5148\u9a8c\uff08SHF\uff09\u8fdb\u4e00\u6b65\u4f18\u5316\u4e86\u8def\u5f84\u89c4\u5212\u3002", "motivation": "\u63a2\u8ba8\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u76ee\u6807\u5bfc\u822a\u4efb\u52a1\u4e2d\u662f\u5426\u771f\u6b63\u63d0\u5347\u4e86\u89c4\u5212\u80fd\u529b\uff0c\u8fd8\u662f\u51e0\u4f55\u542f\u53d1\u5f0f\u65b9\u6cd5\u66f4\u5173\u952e\u3002", "method": "\u901a\u8fc7\u79fb\u9664InstructNav\u7684\u52a8\u6001\u5bfc\u822a\u63d0\u793a\u3001\u5f00\u653e\u8bcd\u6c47\u68c0\u6d4b\u5668\u548c\u663e\u8457\u6027\u56fe\uff0c\u66ff\u6362\u4e3a\u7b80\u5355\u7684\u51e0\u4f55\u542f\u53d1\u5f0f\u65b9\u6cd5\uff08DWFE\uff09\uff0c\u5e76\u52a0\u5165\u8f7b\u91cf\u7ea7\u8bed\u8a00\u5148\u9a8c\uff08SHF\uff09\u8fdb\u884c\u5bf9\u6bd4\u5b9e\u9a8c\u3002", "result": "DWFE\u5c06\u6210\u529f\u7387\u4ece58.0%\u63d0\u5347\u81f361.1%\uff0cSPL\u4ece20.9%\u63d0\u5347\u81f336.0%\uff1bSHF\u8fdb\u4e00\u6b65\u589e\u52a02%\u6210\u529f\u7387\u548c0.9% SPL\uff0c\u5e76\u7f29\u77ed\u8def\u5f84\u3002", "conclusion": "\u51e0\u4f55\u542f\u53d1\u5f0f\u65b9\u6cd5\u800c\u975eLLM\u9a71\u52a8\u4e86\u6027\u80fd\u63d0\u5347\uff0c\u672a\u6765\u9700\u7ed3\u5408\u5ea6\u91cf\u611f\u77e5\u63d0\u793a\u6216\u79bb\u7ebf\u8bed\u4e49\u56fe\u624d\u80fd\u4f53\u73b0LLM\u7684\u667a\u80fd\u8d21\u732e\u3002"}}
{"id": "2507.20067", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.20067", "abs": "https://arxiv.org/abs/2507.20067", "authors": ["Sarat Chandra Bobbili", "Ujwal Dinesha", "Dheeraj Narasimha", "Srinivas Shakkottai"], "title": "PITA: Preference-Guided Inference-Time Alignment for LLM Post-Training", "comment": null, "summary": "Inference-time alignment enables large language models (LLMs) to generate\noutputs aligned with end-user preferences without further training. Recent\npost-training methods achieve this by using small guidance models to modify\ntoken generation during inference. These methods typically optimize a reward\nfunction KL-regularized by the original LLM taken as the reference policy. A\ncritical limitation, however, is their dependence on a pre-trained reward\nmodel, which requires fitting to human preference feedback--a potentially\nunstable process. In contrast, we introduce PITA, a novel framework that\nintegrates preference feedback directly into the LLM's token generation,\neliminating the need for a reward model. PITA learns a small preference-based\nguidance policy to modify token probabilities at inference time without LLM\nfine-tuning, reducing computational cost and bypassing the pre-trained reward\nmodel dependency. The problem is framed as identifying an underlying preference\ndistribution, solved through stochastic search and iterative refinement of the\npreference-based guidance model. We evaluate PITA across diverse tasks,\nincluding mathematical reasoning and sentiment classification, demonstrating\nits effectiveness in aligning LLM outputs with user preferences.", "AI": {"tldr": "PITA\u6846\u67b6\u901a\u8fc7\u76f4\u63a5\u6574\u5408\u504f\u597d\u53cd\u9988\u5230LLM\u7684token\u751f\u6210\u4e2d\uff0c\u65e0\u9700\u9884\u8bad\u7ec3\u5956\u52b1\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u63a8\u7406\u65f6\u5bf9\u9f50\u7528\u6237\u504f\u597d\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u9884\u8bad\u7ec3\u5956\u52b1\u6a21\u578b\uff0c\u53ef\u80fd\u4e0d\u7a33\u5b9a\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\uff0cPITA\u65e8\u5728\u6d88\u9664\u8fd9\u4e00\u4f9d\u8d56\u3002", "method": "PITA\u901a\u8fc7\u5b66\u4e60\u5c0f\u578b\u504f\u597d\u5f15\u5bfc\u7b56\u7565\uff0c\u5728\u63a8\u7406\u65f6\u4fee\u6539token\u6982\u7387\uff0c\u65e0\u9700\u5fae\u8c03LLM\uff0c\u91c7\u7528\u968f\u673a\u641c\u7d22\u548c\u8fed\u4ee3\u4f18\u5316\u3002", "result": "\u5728\u6570\u5b66\u63a8\u7406\u548c\u60c5\u611f\u5206\u7c7b\u7b49\u4efb\u52a1\u4e2d\uff0cPITA\u6709\u6548\u5bf9\u9f50LLM\u8f93\u51fa\u4e0e\u7528\u6237\u504f\u597d\u3002", "conclusion": "PITA\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u7a33\u5b9a\u7684\u63a8\u7406\u65f6\u5bf9\u9f50\u65b9\u6cd5\uff0c\u51cf\u5c11\u4e86\u5bf9\u9884\u8bad\u7ec3\u5956\u52b1\u6a21\u578b\u7684\u4f9d\u8d56\u3002"}}
{"id": "2507.20034", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.20034", "abs": "https://arxiv.org/abs/2507.20034", "authors": ["Aviad Golan", "Gregory Zin", "Zahra Ahmed", "Emily Bates", "Toby Bell", "Pol Francesch Huc", "Samuel Y. W. Low", "Juergen Bosse", "Simone D'Amico"], "title": "Digital and Robotic Twinning for Validation of Proximity Operations and Formation Flying", "comment": "23 pages, 12 figures. 2025 Astrodynamics Specialist Conference", "summary": "In spacecraft Rendezvous, Proximity Operations (RPO), and Formation Flying\n(FF), the Guidance Navigation and Control (GNC) system is safety-critical and\nmust meet strict performance requirements. However, validating such systems is\nchallenging due to the complexity of the space environment, necessitating a\nverification and validation (V&V) process that bridges simulation and\nreal-world behavior. The key contribution of this paper is a unified,\nend-to-end digital and robotic twinning framework that enables software- and\nhardware-in-the-loop testing for multi-modal GNC systems. The robotic twin\nincludes three testbeds at Stanford's Space Rendezvous Laboratory (SLAB): the\nGNSS and Radiofrequency Autonomous Navigation Testbed for Distributed Space\nSystems (GRAND) to validate RF-based navigation techniques, and the Testbed for\nRendezvous and Optical Navigation (TRON) and Optical Stimulator (OS) to\nvalidate vision-based methods. The test article for this work is an integrated\nmulti-modal GNC software stack for RPO and FF developed at SLAB. This paper\nintroduces the hybrid framework and summarizes calibration and error\ncharacterization for the robotic twin. Then, the GNC stack's performance and\nrobustness is characterized using the integrated digital and robotic twinning\npipeline for a full-range RPO mission scenario in Low-Earth Orbit (LEO). The\nresults shown in the paper demonstrate consistency between digital and robotic\ntwins, validating the hybrid twinning pipeline as a reliable framework for\nrealistic assessment and verification of GNC systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u6570\u5b57\u548c\u673a\u5668\u4eba\u5b6a\u751f\u6846\u67b6\uff0c\u7528\u4e8e\u9a8c\u8bc1\u591a\u6a21\u6001GNC\u7cfb\u7edf\uff0c\u901a\u8fc7\u4eff\u771f\u548c\u5b9e\u9645\u6d4b\u8bd5\u7ed3\u5408\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u4f4e\u5730\u7403\u8f68\u9053RPO\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u7531\u4e8e\u592a\u7a7a\u73af\u5883\u7684\u590d\u6742\u6027\uff0c\u9a8c\u8bc1GNC\u7cfb\u7edf\u7684\u6027\u80fd\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u8fde\u63a5\u4eff\u771f\u548c\u5b9e\u9645\u884c\u4e3a\u7684V&V\u6d41\u7a0b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u6570\u5b57\u548c\u673a\u5668\u4eba\u5b6a\u751f\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u8f6f\u4ef6\u548c\u786c\u4ef6\u5728\u73af\u6d4b\u8bd5\uff0c\u4f7f\u7528\u4e86\u4e09\u4e2a\u6d4b\u8bd5\u5e73\u53f0\uff08GRAND\u3001TRON\u548cOS\uff09\u6765\u9a8c\u8bc1RF\u548c\u89c6\u89c9\u5bfc\u822a\u6280\u672f\u3002", "result": "\u6570\u5b57\u548c\u673a\u5668\u4eba\u5b6a\u751f\u7ed3\u679c\u4e00\u81f4\uff0c\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u5728GNC\u7cfb\u7edf\u8bc4\u4f30\u548c\u9a8c\u8bc1\u4e2d\u7684\u53ef\u9760\u6027\u3002", "conclusion": "\u8be5\u6df7\u5408\u5b6a\u751f\u6846\u67b6\u4e3aGNC\u7cfb\u7edf\u7684\u771f\u5b9e\u8bc4\u4f30\u548c\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u65b9\u6cd5\u3002"}}
{"id": "2507.20143", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.20143", "abs": "https://arxiv.org/abs/2507.20143", "authors": ["Zhonghan Ge", "Yuanyang Zhu", "Chunlin Chen"], "title": "Concept Learning for Cooperative Multi-Agent Reinforcement Learning", "comment": "IEEE-China Conference on System Simulation Technology and its\n  Applications, 2025", "summary": "Despite substantial progress in applying neural networks (NN) to multi-agent\nreinforcement learning (MARL) areas, they still largely suffer from a lack of\ntransparency and interoperability. However, its implicit cooperative mechanism\nis not yet fully understood due to black-box networks. In this work, we study\nan interpretable value decomposition framework via concept bottleneck models,\nwhich promote trustworthiness by conditioning credit assignment on an\nintermediate level of human-like cooperation concepts. To address this problem,\nwe propose a novel value-based method, named Concepts learning for Multi-agent\nQ-learning (CMQ), that goes beyond the current performance-vs-interpretability\ntrade-off by learning interpretable cooperation concepts. CMQ represents each\ncooperation concept as a supervised vector, as opposed to existing models where\nthe information flowing through their end-to-end mechanism is concept-agnostic.\nIntuitively, using individual action value conditioning on global state\nembeddings to represent each concept allows for extra cooperation\nrepresentation capacity. Empirical evaluations on the StarCraft II\nmicromanagement challenge and level-based foraging (LBF) show that CMQ achieves\nsuperior performance compared with the state-of-the-art counterparts. The\nresults also demonstrate that CMQ provides more cooperation concept\nrepresentation capturing meaningful cooperation modes, and supports test-time\nconcept interventions for detecting potential biases of cooperation mode and\nidentifying spurious artifacts that impact cooperation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6982\u5ff5\u74f6\u9888\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u503c\u5206\u89e3\u6846\u67b6\uff08CMQ\uff09\uff0c\u7528\u4e8e\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff0c\u901a\u8fc7\u663e\u5f0f\u5b66\u4e60\u5408\u4f5c\u6982\u5ff5\u63d0\u5347\u900f\u660e\u5ea6\u548c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u795e\u7ecf\u7f51\u7edc\u5728\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u7f3a\u4e4f\u900f\u660e\u6027\u548c\u4e92\u64cd\u4f5c\u6027\uff0c\u5176\u9690\u542b\u7684\u5408\u4f5c\u673a\u5236\u96be\u4ee5\u7406\u89e3\u3002", "method": "\u63d0\u51faCMQ\u65b9\u6cd5\uff0c\u901a\u8fc7\u76d1\u7763\u5411\u91cf\u8868\u793a\u5408\u4f5c\u6982\u5ff5\uff0c\u5229\u7528\u5168\u5c40\u72b6\u6001\u5d4c\u5165\u7684\u6761\u4ef6\u5316\u52a8\u4f5c\u503c\u589e\u5f3a\u5408\u4f5c\u8868\u793a\u80fd\u529b\u3002", "result": "\u5728StarCraft II\u548cLBF\u4efb\u52a1\u4e2d\uff0cCMQ\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u80fd\u6355\u6349\u6709\u610f\u4e49\u7684\u5408\u4f5c\u6a21\u5f0f\u3002", "conclusion": "CMQ\u7a81\u7834\u4e86\u6027\u80fd\u4e0e\u53ef\u89e3\u91ca\u6027\u7684\u6743\u8861\uff0c\u652f\u6301\u6d4b\u8bd5\u65f6\u6982\u5ff5\u5e72\u9884\uff0c\u6709\u52a9\u4e8e\u68c0\u6d4b\u5408\u4f5c\u504f\u5dee\u548c\u4f2a\u5f71\u3002"}}
{"id": "2507.20049", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.20049", "abs": "https://arxiv.org/abs/2507.20049", "authors": ["Frederico Belmonte Klein", "Zhaoyuan Wan", "Huawei Wang", "Ruoli Wang"], "title": "A real-time full-chain wearable sensor-based musculoskeletal simulation: an OpenSim-ROS Integration", "comment": "11 pages, 10 figures", "summary": "Musculoskeletal modeling and simulations enable the accurate description and\nanalysis of the movement of biological systems with applications such as\nrehabilitation assessment, prosthesis, and exoskeleton design. However, the\nwidespread usage of these techniques is limited by costly sensors,\nlaboratory-based setups, computationally demanding processes, and the use of\ndiverse software tools that often lack seamless integration. In this work, we\naddress these limitations by proposing an integrated, real-time framework for\nmusculoskeletal modeling and simulations that leverages OpenSimRT, the robotics\noperating system (ROS), and wearable sensors. As a proof-of-concept, we\ndemonstrate that this framework can reasonably well describe inverse kinematics\nof both lower and upper body using either inertial measurement units or\nfiducial markers. Additionally, we show that it can effectively estimate\ninverse dynamics of the ankle joint and muscle activations of major lower limb\nmuscles during daily activities, including walking, squatting and sit to stand,\nstand to sit when combined with pressure insoles. We believe this work lays the\ngroundwork for further studies with more complex real-time and wearable\nsensor-based human movement analysis systems and holds potential to advance\ntechnologies in rehabilitation, robotics and exoskeleton designs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eOpenSimRT\u3001ROS\u548c\u53ef\u7a7f\u6234\u4f20\u611f\u5668\u7684\u5b9e\u65f6\u96c6\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u808c\u9aa8\u5efa\u6a21\u4e0e\u4eff\u771f\u4e2d\u7684\u9ad8\u6210\u672c\u3001\u590d\u6742\u6027\u548c\u8f6f\u4ef6\u96c6\u6210\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u808c\u9aa8\u5efa\u6a21\u4e0e\u4eff\u771f\u6280\u672f\u56e0\u4f20\u611f\u5668\u6210\u672c\u9ad8\u3001\u5b9e\u9a8c\u5ba4\u8bbe\u7f6e\u590d\u6742\u3001\u8ba1\u7b97\u91cf\u5927\u4ee5\u53ca\u8f6f\u4ef6\u5de5\u5177\u7f3a\u4e4f\u96c6\u6210\u800c\u5e94\u7528\u53d7\u9650\u3002", "method": "\u7ed3\u5408OpenSimRT\u3001ROS\u548c\u53ef\u7a7f\u6234\u4f20\u611f\u5668\uff0c\u5f00\u53d1\u5b9e\u65f6\u96c6\u6210\u6846\u67b6\uff0c\u9a8c\u8bc1\u5176\u7528\u4e8e\u63cf\u8ff0\u4e0a\u4e0b\u80a2\u9006\u8fd0\u52a8\u5b66\u548c\u8e1d\u5173\u8282\u9006\u52a8\u529b\u5b66\u7684\u80fd\u529b\u3002", "result": "\u6846\u67b6\u80fd\u6709\u6548\u63cf\u8ff0\u9006\u8fd0\u52a8\u5b66\uff0c\u5e76\u4f30\u8ba1\u8e1d\u5173\u8282\u9006\u52a8\u529b\u5b66\u53ca\u4e0b\u80a2\u808c\u8089\u6fc0\u6d3b\u72b6\u6001\uff0c\u9002\u7528\u4e8e\u65e5\u5e38\u6d3b\u52a8\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u590d\u6742\u5b9e\u65f6\u808c\u9aa8\u5206\u6790\u7cfb\u7edf\u5960\u5b9a\u57fa\u7840\uff0c\u6709\u671b\u63a8\u52a8\u5eb7\u590d\u3001\u673a\u5668\u4eba\u548c\u5916\u9aa8\u9abc\u8bbe\u8ba1\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2507.20150", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.20150", "abs": "https://arxiv.org/abs/2507.20150", "authors": ["Xingcheng Xu"], "title": "The Policy Cliff: A Theoretical Analysis of Reward-Policy Maps in Large Language Models", "comment": null, "summary": "Reinforcement learning (RL) plays a crucial role in shaping the behavior of\nlarge language and reasoning models (LLMs/LRMs). However, it often produces\nbrittle and unstable policies, leading to critical failures such as spurious\nreasoning, deceptive alignment, and instruction disobedience that undermine the\ntrustworthiness and safety of LLMs/LRMs. Currently, these issues lack a unified\ntheoretical explanation and are typically addressed using ad-hoc heuristics.\nThis paper presents a rigorous mathematical framework for analyzing the\nstability of the mapping from a reward function to the optimal policy. We show\nthat policy brittleness often stems from non-unique optimal actions, a common\noccurrence when multiple valid traces exist in a reasoning task. This\ntheoretical lens provides a unified explanation for a range of seemingly\ndisparate failures, reframing them as rational outcomes of optimizing rewards\nthat may be incomplete or noisy, especially in the presence of action\ndegeneracy. We extend this analysis from the fundamental single-reward setting\nto the more realistic multi-reward RL across diverse domains, showing how\nstability is governed by an \"effective reward\" aggregation mechanism. We also\nprove that entropy regularization restores policy stability at the cost of\nincreased stochasticity. Our framework provides a unified explanation for\nrecent empirical findings on deceptive reasoning, instruction-following\ntrade-offs, and RLHF-induced sophistry, and is further validated through\nperturbation experiments in multi-reward RL. This work advances\npolicy-stability analysis from empirical heuristics towards a principled\ntheory, offering essential insights for designing safer and more trustworthy AI\nsystems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u6570\u5b66\u6846\u67b6\uff0c\u5206\u6790\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4e2d\u5956\u52b1\u51fd\u6570\u5230\u6700\u4f18\u7b56\u7565\u7684\u7a33\u5b9a\u6027\uff0c\u89e3\u91ca\u4e86\u653f\u7b56\u8106\u5f31\u6027\u7684\u6839\u6e90\uff0c\u5e76\u9a8c\u8bc1\u4e86\u71b5\u6b63\u5219\u5316\u5bf9\u7a33\u5b9a\u6027\u7684\u5f71\u54cd\u3002", "motivation": "\u5f53\u524dRL\u5728\u5927\u578b\u8bed\u8a00\u548c\u63a8\u7406\u6a21\u578b\uff08LLMs/LRMs\uff09\u4e2d\u5e38\u4ea7\u751f\u8106\u5f31\u548c\u4e0d\u7a33\u5b9a\u7684\u7b56\u7565\uff0c\u5bfc\u81f4\u865a\u5047\u63a8\u7406\u3001\u6b3a\u9a97\u6027\u5bf9\u9f50\u548c\u6307\u4ee4\u4e0d\u670d\u4ece\u7b49\u95ee\u9898\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u7406\u8bba\u89e3\u91ca\u3002", "method": "\u901a\u8fc7\u6570\u5b66\u6846\u67b6\u5206\u6790\u5956\u52b1\u51fd\u6570\u5230\u6700\u4f18\u7b56\u7565\u7684\u6620\u5c04\u7a33\u5b9a\u6027\uff0c\u7814\u7a76\u975e\u552f\u4e00\u6700\u4f18\u52a8\u4f5c\u7684\u5f71\u54cd\uff0c\u5e76\u6269\u5c55\u5230\u591a\u5956\u52b1RL\u548c\u71b5\u6b63\u5219\u5316\u7684\u4f5c\u7528\u3002", "result": "\u63ed\u793a\u4e86\u653f\u7b56\u8106\u5f31\u6027\u6e90\u4e8e\u975e\u552f\u4e00\u6700\u4f18\u52a8\u4f5c\uff0c\u71b5\u6b63\u5219\u5316\u53ef\u6062\u590d\u7a33\u5b9a\u6027\u4f46\u589e\u52a0\u968f\u673a\u6027\uff0c\u6846\u67b6\u7edf\u4e00\u89e3\u91ca\u4e86\u591a\u79cd\u5931\u8d25\u73b0\u8c61\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8bbe\u8ba1\u66f4\u5b89\u5168\u3001\u53ef\u4fe1\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\uff0c\u5c06\u653f\u7b56\u7a33\u5b9a\u6027\u5206\u6790\u4ece\u7ecf\u9a8c\u542f\u53d1\u63d0\u5347\u5230\u539f\u5219\u6027\u7406\u8bba\u3002"}}
{"id": "2507.20217", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.20217", "abs": "https://arxiv.org/abs/2507.20217", "authors": ["Wei Cui", "Haoyu Wang", "Wenkang Qin", "Yijie Guo", "Gang Han", "Wen Zhao", "Jiahang Cao", "Zhang Zhang", "Jiaru Zhong", "Jingkai Sun", "Pihai Sun", "Shuai Shi", "Botuo Jiang", "Jiahao Ma", "Jiaxu Wang", "Hao Cheng", "Zhichao Liu", "Yang Wang", "Zheng Zhu", "Guan Huang", "Jian Tang", "Qiang Zhang"], "title": "Humanoid Occupancy: Enabling A Generalized Multimodal Occupancy Perception System on Humanoid Robots", "comment": "Tech Report", "summary": "Humanoid robot technology is advancing rapidly, with manufacturers\nintroducing diverse heterogeneous visual perception modules tailored to\nspecific scenarios. Among various perception paradigms, occupancy-based\nrepresentation has become widely recognized as particularly suitable for\nhumanoid robots, as it provides both rich semantic and 3D geometric information\nessential for comprehensive environmental understanding. In this work, we\npresent Humanoid Occupancy, a generalized multimodal occupancy perception\nsystem that integrates hardware and software components, data acquisition\ndevices, and a dedicated annotation pipeline. Our framework employs advanced\nmulti-modal fusion techniques to generate grid-based occupancy outputs encoding\nboth occupancy status and semantic labels, thereby enabling holistic\nenvironmental understanding for downstream tasks such as task planning and\nnavigation. To address the unique challenges of humanoid robots, we overcome\nissues such as kinematic interference and occlusion, and establish an effective\nsensor layout strategy. Furthermore, we have developed the first panoramic\noccupancy dataset specifically for humanoid robots, offering a valuable\nbenchmark and resource for future research and development in this domain. The\nnetwork architecture incorporates multi-modal feature fusion and temporal\ninformation integration to ensure robust perception. Overall, Humanoid\nOccupancy delivers effective environmental perception for humanoid robots and\nestablishes a technical foundation for standardizing universal visual modules,\npaving the way for the widespread deployment of humanoid robots in complex\nreal-world scenarios.", "AI": {"tldr": "Humanoid Occupancy\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u5360\u7528\u611f\u77e5\u7cfb\u7edf\uff0c\u7ed3\u5408\u786c\u4ef6\u548c\u8f6f\u4ef6\uff0c\u4e3a\u4eff\u4eba\u673a\u5668\u4eba\u63d0\u4f9b\u5168\u9762\u7684\u73af\u5883\u7406\u89e3\u3002", "motivation": "\u4eff\u4eba\u673a\u5668\u4eba\u9700\u8981\u4e30\u5bcc\u7684\u8bed\u4e49\u548c3D\u51e0\u4f55\u4fe1\u606f\u4ee5\u652f\u6301\u73af\u5883\u7406\u89e3\uff0c\u800c\u73b0\u6709\u7684\u5360\u7528\u8868\u793a\u65b9\u6cd5\u88ab\u8ba4\u4e3a\u662f\u6700\u9002\u5408\u7684\u3002", "method": "\u7cfb\u7edf\u91c7\u7528\u591a\u6a21\u6001\u878d\u5408\u6280\u672f\u548c\u7f51\u683c\u5360\u7528\u8f93\u51fa\uff0c\u89e3\u51b3\u8fd0\u52a8\u5e72\u6270\u548c\u906e\u6321\u95ee\u9898\uff0c\u5e76\u5f00\u53d1\u4e86\u9996\u4e2a\u5168\u666f\u5360\u7528\u6570\u636e\u96c6\u3002", "result": "Humanoid Occupancy\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u73af\u5883\u611f\u77e5\uff0c\u4e3a\u4eff\u4eba\u673a\u5668\u4eba\u5728\u590d\u6742\u573a\u666f\u4e2d\u7684\u90e8\u7f72\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u4eff\u4eba\u673a\u5668\u4eba\u7684\u901a\u7528\u89c6\u89c9\u6a21\u5757\u6807\u51c6\u5316\u63d0\u4f9b\u4e86\u6280\u672f\u57fa\u7840\uff0c\u63a8\u52a8\u4e86\u5176\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u3002"}}
{"id": "2507.20199", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.20199", "abs": "https://arxiv.org/abs/2507.20199", "authors": ["Shijie Shang", "Ruosi Wan", "Yue Peng", "Yutong Wu", "Xiong-hui Chen", "Jie Yan", "Xiangyu Zhang"], "title": "StepFun-Prover Preview: Let's Think and Verify Step by Step", "comment": "25 pages, 4 figures", "summary": "We present StepFun-Prover Preview, a large language model designed for formal\ntheorem proving through tool-integrated reasoning. Using a reinforcement\nlearning pipeline that incorporates tool-based interactions, StepFun-Prover can\nachieve strong performance in generating Lean 4 proofs with minimal sampling.\nOur approach enables the model to emulate human-like problem-solving strategies\nby iteratively refining proofs based on real-time environment feedback. On the\nminiF2F-test benchmark, StepFun-Prover achieves a pass@1 success rate of\n$70.0\\%$. Beyond advancing benchmark performance, we introduce an end-to-end\ntraining framework for developing tool-integrated reasoning models, offering a\npromising direction for automated theorem proving and Math AI assistant.", "AI": {"tldr": "StepFun-Prover Preview\u662f\u4e00\u4e2a\u7528\u4e8e\u5f62\u5f0f\u5316\u5b9a\u7406\u8bc1\u660e\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u5de5\u5177\u96c6\u6210\u63a8\u7406\u5b9e\u73b0\u9ad8\u6548Lean 4\u8bc1\u660e\u751f\u6210\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u5de5\u5177\u96c6\u6210\u63a8\u7406\u63d0\u5347\u81ea\u52a8\u5b9a\u7406\u8bc1\u660e\u7684\u6027\u80fd\uff0c\u5e76\u6a21\u62df\u4eba\u7c7b\u95ee\u9898\u89e3\u51b3\u7b56\u7565\u3002", "method": "\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u7ba1\u9053\uff0c\u7ed3\u5408\u5de5\u5177\u4ea4\u4e92\uff0c\u901a\u8fc7\u5b9e\u65f6\u73af\u5883\u53cd\u9988\u8fed\u4ee3\u4f18\u5316\u8bc1\u660e\u3002", "result": "\u5728miniF2F-test\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cpass@1\u6210\u529f\u7387\u8fbe\u523070.0%\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u8bad\u7ec3\u6846\u67b6\uff0c\u4e3a\u81ea\u52a8\u5b9a\u7406\u8bc1\u660e\u548c\u6570\u5b66AI\u52a9\u624b\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2507.20282", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.20282", "abs": "https://arxiv.org/abs/2507.20282", "authors": ["Yifan Zhang", "Dianye Huang", "Nassir Navab", "Zhongliang Jiang"], "title": "Tactile-Guided Robotic Ultrasound: Mapping Preplanned Scan Paths for Intercostal Imaging", "comment": "Accepted by IROS2025, video link: https://youtu.be/SBwpFVzEhAg", "summary": "Medical ultrasound (US) imaging is widely used in clinical examinations due\nto its portability, real-time capability, and radiation-free nature. To address\ninter- and intra-operator variability, robotic ultrasound systems have gained\nincreasing attention. However, their application in challenging intercostal\nimaging remains limited due to the lack of an effective scan path generation\nmethod within the constrained acoustic window. To overcome this challenge, we\nexplore the potential of tactile cues for characterizing subcutaneous rib\nstructures as an alternative signal for ultrasound segmentation-free bone\nsurface point cloud extraction. Compared to 2D US images, 1D tactile-related\nsignals offer higher processing efficiency and are less susceptible to acoustic\nnoise and artifacts. By leveraging robotic tracking data, a sparse tactile\npoint cloud is generated through a few scans along the rib, mimicking human\npalpation. To robustly map the scanning trajectory into the intercostal space,\nthe sparse tactile bone location point cloud is first interpolated to form a\ndenser representation. This refined point cloud is then registered to an\nimage-based dense bone surface point cloud, enabling accurate scan path mapping\nfor individual patients. Additionally, to ensure full coverage of the object of\ninterest, we introduce an automated tilt angle adjustment method to visualize\nstructures beneath the bone. To validate the proposed method, we conducted\ncomprehensive experiments on four distinct phantoms. The final scanning\nwaypoint mapping achieved Mean Nearest Neighbor Distance (MNND) and Hausdorff\ndistance (HD) errors of 3.41 mm and 3.65 mm, respectively, while the\nreconstructed object beneath the bone had errors of 0.69 mm and 2.2 mm compared\nto the CT ground truth.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89e6\u89c9\u4fe1\u53f7\u7684\u673a\u5668\u4eba\u8d85\u58f0\u626b\u63cf\u8def\u5f84\u751f\u6210\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u808b\u95f4\u6210\u50cf\u4e2d\u7684\u6311\u6218\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u8d85\u58f0\u7cfb\u7edf\u5728\u808b\u95f4\u6210\u50cf\u4e2d\u56e0\u7f3a\u4e4f\u6709\u6548\u626b\u63cf\u8def\u5f84\u751f\u6210\u65b9\u6cd5\u800c\u53d7\u9650\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528\u89e6\u89c9\u4fe1\u53f7\u63d0\u53d6\u808b\u9aa8\u8868\u9762\u70b9\u4e91\uff0c\u901a\u8fc7\u7a00\u758f\u70b9\u4e91\u63d2\u503c\u548c\u914d\u51c6\u6280\u672f\u751f\u6210\u626b\u63cf\u8def\u5f84\uff0c\u5e76\u5f15\u5165\u81ea\u52a8\u503e\u659c\u89d2\u5ea6\u8c03\u6574\u65b9\u6cd5\u3002", "result": "\u626b\u63cf\u8def\u5f84\u6620\u5c04\u7684\u5e73\u5747\u6700\u8fd1\u90bb\u8ddd\u79bb\u548cHausdorff\u8ddd\u79bb\u8bef\u5dee\u5206\u522b\u4e3a3.41 mm\u548c3.65 mm\uff0c\u91cd\u5efa\u5bf9\u8c61\u7684\u8bef\u5dee\u4e3a0.69 mm\u548c2.2 mm\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u808b\u95f4\u6210\u50cf\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u51c6\u786e\u6027\uff0c\u4e3a\u673a\u5668\u4eba\u8d85\u58f0\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.20226", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.20226", "abs": "https://arxiv.org/abs/2507.20226", "authors": ["Shuyang Guo", "Wenjin Xie", "Ping Lu", "Ting Deng", "Richong Zhang", "Jianxin Li", "Xiangping Huang", "Zhongyi Liu"], "title": "Improving Subgraph Matching by Combining Algorithms and Graph Neural Networks", "comment": null, "summary": "Homomorphism is a key mapping technique between graphs that preserves their\nstructure. Given a graph and a pattern, the subgraph homomorphism problem\ninvolves finding a mapping from the pattern to the graph, ensuring that\nadjacent vertices in the pattern are mapped to adjacent vertices in the graph.\nUnlike subgraph isomorphism, which requires a one-to-one mapping, homomorphism\nallows multiple vertices in the pattern to map to the same vertex in the graph,\nmaking it more complex. We propose HFrame, the first graph neural network-based\nframework for subgraph homomorphism, which integrates traditional algorithms\nwith machine learning techniques. We demonstrate that HFrame outperforms\nstandard graph neural networks by being able to distinguish more graph pairs\nwhere the pattern is not homomorphic to the graph. Additionally, we provide a\ngeneralization error bound for HFrame. Through experiments on both real-world\nand synthetic graphs, we show that HFrame is up to 101.91 times faster than\nexact matching algorithms and achieves an average accuracy of 0.962.", "AI": {"tldr": "HFrame\u662f\u4e00\u4e2a\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u5b50\u56fe\u540c\u6001\u6846\u67b6\uff0c\u7ed3\u5408\u4f20\u7edf\u7b97\u6cd5\u4e0e\u673a\u5668\u5b66\u4e60\u6280\u672f\uff0c\u6027\u80fd\u4f18\u4e8e\u6807\u51c6\u56fe\u795e\u7ecf\u7f51\u7edc\uff0c\u5e76\u5728\u901f\u5ea6\u548c\u51c6\u786e\u6027\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u5b50\u56fe\u540c\u6001\u95ee\u9898\u6bd4\u5b50\u56fe\u540c\u6784\u66f4\u590d\u6742\uff0c\u4f20\u7edf\u65b9\u6cd5\u6548\u7387\u4f4e\uff0c\u9700\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u63d0\u5347\u6027\u80fd\u3002", "method": "\u63d0\u51faHFrame\u6846\u67b6\uff0c\u7ed3\u5408\u56fe\u795e\u7ecf\u7f51\u7edc\u4e0e\u4f20\u7edf\u7b97\u6cd5\uff0c\u7528\u4e8e\u5b50\u56fe\u540c\u6001\u95ee\u9898\u3002", "result": "HFrame\u5728\u533a\u5206\u975e\u540c\u6001\u56fe\u5bf9\u65f6\u8868\u73b0\u66f4\u4f18\uff0c\u901f\u5ea6\u6bd4\u7cbe\u786e\u5339\u914d\u7b97\u6cd5\u5feb101.91\u500d\uff0c\u5e73\u5747\u51c6\u786e\u7387\u8fbe0.962\u3002", "conclusion": "HFrame\u4e3a\u5b50\u56fe\u540c\u6001\u95ee\u9898\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.20293", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.20293", "abs": "https://arxiv.org/abs/2507.20293", "authors": ["Stepan Dergachev", "Konstantin Yakovlev"], "title": "Decentralized Uncertainty-Aware Multi-Agent Collision Avoidance With Model Predictive Path Integral", "comment": "This is a pre-print of the paper accepted to IROS2025. It contains 8\n  pages, 4 figures and 1 table. The supplementary video available at\n  https://youtu.be/_D4zDYJ4KCk", "summary": "Decentralized multi-agent navigation under uncertainty is a complex task that\narises in numerous robotic applications. It requires collision avoidance\nstrategies that account for both kinematic constraints, sensing and action\nexecution noise. In this paper, we propose a novel approach that integrates the\nModel Predictive Path Integral (MPPI) with a probabilistic adaptation of\nOptimal Reciprocal Collision Avoidance. Our method ensures safe and efficient\nmulti-agent navigation by incorporating probabilistic safety constraints\ndirectly into the MPPI sampling process via a Second-Order Cone Programming\nformulation. This approach enables agents to operate independently using local\nnoisy observations while maintaining safety guarantees. We validate our\nalgorithm through extensive simulations with differential-drive robots and\nbenchmark it against state-of-the-art methods, including ORCA-DD and B-UAVC.\nResults demonstrate that our approach outperforms them while achieving high\nsuccess rates, even in densely populated environments. Additionally, validation\nin the Gazebo simulator confirms its practical applicability to robotic\nplatforms.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408MPPI\u548c\u6982\u7387ORCA\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u591a\u667a\u80fd\u4f53\u5bfc\u822a\uff0c\u786e\u4fdd\u5b89\u5168\u9ad8\u6548\u3002", "motivation": "\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u5bfc\u822a\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u548c\u78b0\u649e\u907f\u514d\u95ee\u9898\uff0c\u9002\u5e94\u8fd0\u52a8\u7ea6\u675f\u548c\u566a\u58f0\u3002", "method": "\u5c06MPPI\u4e0e\u6982\u7387ORCA\u7ed3\u5408\uff0c\u901a\u8fc7SOCP\u7ea6\u675f\u786e\u4fdd\u5b89\u5168\u6027\u3002", "result": "\u5728\u5bc6\u96c6\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u4e8eORCA-DD\u548cB-UAVC\uff0c\u6210\u529f\u7387\u9ad8\u3002", "conclusion": "\u65b9\u6cd5\u5b9e\u7528\u4e14\u9ad8\u6548\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u673a\u5668\u4eba\u5e73\u53f0\u3002"}}
{"id": "2507.20230", "categories": ["cs.AI", "cs.CV", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.20230", "abs": "https://arxiv.org/abs/2507.20230", "authors": ["Yufan Chen", "Ching Ting Leung", "Bowen Yu", "Jianwei Sun", "Yong Huang", "Linyan Li", "Hao Chen", "Hanyu Gao"], "title": "A Multi-Agent System for Information Extraction from the Chemical Literature", "comment": null, "summary": "To fully expedite AI-powered chemical research, high-quality chemical\ndatabases are the cornerstone. Automatic extraction of chemical information\nfrom the literature is essential for constructing reaction databases, but it is\ncurrently limited by the multimodality and style variability of chemical\ninformation. In this work, we developed a multimodal large language model\n(MLLM)-based multi-agent system for automatic chemical information extraction.\nWe used the MLLM's strong reasoning capability to understand the structure of\ncomplex chemical graphics, decompose the extraction task into sub-tasks and\ncoordinate a set of specialized agents to solve them. Our system achieved an F1\nscore of 80.8% on a benchmark dataset of complex chemical reaction graphics\nfrom the literature, surpassing the previous state-of-the-art model (F1 score:\n35.6%) by a significant margin. Additionally, it demonstrated consistent\nimprovements in key sub-tasks, including molecular image recognition, reaction\nimage parsing, named entity recognition and text-based reaction extraction.\nThis work is a critical step toward automated chemical information extraction\ninto structured datasets, which will be a strong promoter of AI-driven chemical\nresearch.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u7528\u4e8e\u81ea\u52a8\u63d0\u53d6\u5316\u5b66\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u5316\u5b66\u53cd\u5e94\u56fe\u5f62\u7684\u63d0\u53d6\u6027\u80fd\u3002", "motivation": "\u9ad8\u8d28\u91cf\u5316\u5b66\u6570\u636e\u5e93\u662fAI\u9a71\u52a8\u5316\u5b66\u7814\u7a76\u7684\u57fa\u77f3\uff0c\u4f46\u5f53\u524d\u5316\u5b66\u4fe1\u606f\u7684\u591a\u6a21\u6001\u548c\u98ce\u683c\u591a\u6837\u6027\u9650\u5236\u4e86\u81ea\u52a8\u63d0\u53d6\u6280\u672f\u7684\u53d1\u5c55\u3002", "method": "\u5229\u7528MLLM\u7684\u5f3a\u5927\u63a8\u7406\u80fd\u529b\u7406\u89e3\u590d\u6742\u5316\u5b66\u56fe\u5f62\u7ed3\u6784\uff0c\u5c06\u63d0\u53d6\u4efb\u52a1\u5206\u89e3\u4e3a\u5b50\u4efb\u52a1\uff0c\u5e76\u901a\u8fc7\u534f\u8c03\u591a\u4e2a\u4e13\u7528\u667a\u80fd\u4f53\u89e3\u51b3\u3002", "result": "\u5728\u590d\u6742\u5316\u5b66\u53cd\u5e94\u56fe\u5f62\u7684\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0c\u7cfb\u7edfF1\u5f97\u5206\u4e3a80.8%\uff0c\u663e\u8457\u8d85\u8d8a\u4e4b\u524d\u7684\u6700\u4f73\u6a21\u578b\uff0835.6%\uff09\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u662f\u5b9e\u73b0\u5316\u5b66\u4fe1\u606f\u81ea\u52a8\u63d0\u53d6\u4e3a\u7ed3\u6784\u5316\u6570\u636e\u96c6\u7684\u5173\u952e\u4e00\u6b65\uff0c\u5c06\u6709\u529b\u63a8\u52a8AI\u9a71\u52a8\u7684\u5316\u5b66\u7814\u7a76\u3002"}}
{"id": "2507.20370", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.20370", "abs": "https://arxiv.org/abs/2507.20370", "authors": ["Michele Grimaldi", "Carlo Cernicchiaro", "Sebastian Realpe Rua", "Alaaeddine El-Masri-El-Chaarani", "Markus Buchholz", "Loizos Michael", "Pere Ridao Rodriguez", "Ignacio Carlucho", "Yvan R. Petillot"], "title": "Advancing Shared and Multi-Agent Autonomy in Underwater Missions: Integrating Knowledge Graphs and Retrieval-Augmented Generation", "comment": null, "summary": "Robotic platforms have become essential for marine operations by providing\nregular and continuous access to offshore assets, such as underwater\ninfrastructure inspection, environmental monitoring, and resource exploration.\nHowever, the complex and dynamic nature of underwater environments,\ncharacterized by limited visibility, unpredictable currents, and communication\nconstraints, presents significant challenges that demand advanced autonomy\nwhile ensuring operator trust and oversight. Central to addressing these\nchallenges are knowledge representation and reasoning techniques, particularly\nknowledge graphs and retrieval-augmented generation (RAG) systems, that enable\nrobots to efficiently structure, retrieve, and interpret complex environmental\ndata. These capabilities empower robotic agents to reason, adapt, and respond\neffectively to changing conditions. The primary goal of this work is to\ndemonstrate both multi-agent autonomy and shared autonomy, where multiple\nrobotic agents operate independently while remaining connected to a human\nsupervisor. We show how a RAG-powered large language model, augmented with\nknowledge graph data and domain taxonomy, enables autonomous multi-agent\ndecision-making and facilitates seamless human-robot interaction, resulting in\n100\\% mission validation and behavior completeness. Finally, ablation studies\nreveal that without structured knowledge from the graph and/or taxonomy, the\nLLM is prone to hallucinations, which can compromise decision quality.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u6c34\u4e0b\u673a\u5668\u4eba\u5e73\u53f0\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u51b3\u7b56\u95ee\u9898\uff0c\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u548cRAG\u6280\u672f\u5b9e\u73b0\u591a\u667a\u80fd\u4f53\u81ea\u4e3b\u4e0e\u5171\u4eab\u81ea\u4e3b\uff0c\u663e\u8457\u63d0\u5347\u4efb\u52a1\u5b8c\u6210\u7387\u548c\u51b3\u7b56\u8d28\u91cf\u3002", "motivation": "\u6c34\u4e0b\u73af\u5883\u7684\u590d\u6742\u6027\u548c\u52a8\u6001\u6027\u5bf9\u673a\u5668\u4eba\u81ea\u4e3b\u6027\u63d0\u51fa\u9ad8\u8981\u6c42\uff0c\u9700\u786e\u4fdd\u64cd\u4f5c\u8005\u4fe1\u4efb\u4e0e\u76d1\u7763\uff0c\u56e0\u6b64\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u77e5\u8bc6\u8868\u793a\u4e0e\u63a8\u7406\u6280\u672f\u63d0\u5347\u673a\u5668\u4eba\u9002\u5e94\u80fd\u529b\u3002", "method": "\u7ed3\u5408\u77e5\u8bc6\u56fe\u8c31\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7cfb\u7edf\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8fdb\u884c\u591a\u667a\u80fd\u4f53\u81ea\u4e3b\u51b3\u7b56\uff0c\u5e76\u652f\u6301\u4eba\u673a\u4ea4\u4e92\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86100%\u7684\u4efb\u52a1\u9a8c\u8bc1\u548c\u884c\u4e3a\u5b8c\u6574\u6027\uff0c\u800c\u7f3a\u5c11\u7ed3\u6784\u5316\u77e5\u8bc6\u4f1a\u5bfc\u81f4LLM\u4ea7\u751f\u5e7b\u89c9\uff0c\u5f71\u54cd\u51b3\u7b56\u8d28\u91cf\u3002", "conclusion": "\u77e5\u8bc6\u56fe\u8c31\u548cRAG\u6280\u672f\u80fd\u6709\u6548\u63d0\u5347\u6c34\u4e0b\u673a\u5668\u4eba\u7684\u81ea\u4e3b\u6027\u548c\u51b3\u7b56\u53ef\u9760\u6027\uff0c\u652f\u6301\u591a\u667a\u80fd\u4f53\u4e0e\u4eba\u7c7b\u534f\u540c\u5de5\u4f5c\u3002"}}
{"id": "2507.20280", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.20280", "abs": "https://arxiv.org/abs/2507.20280", "authors": ["Keyan Ding", "Jing Yu", "Junjie Huang", "Yuchen Yang", "Qiang Zhang", "Huajun Chen"], "title": "SciToolAgent: A Knowledge Graph-Driven Scientific Agent for Multi-Tool Integration", "comment": "21 pages, 6 figures", "summary": "Scientific research increasingly relies on specialized computational tools,\nyet effectively utilizing these tools demands substantial domain expertise.\nWhile Large Language Models (LLMs) show promise in tool automation, they\nstruggle to seamlessly integrate and orchestrate multiple tools for complex\nscientific workflows. Here, we present SciToolAgent, an LLM-powered agent that\nautomates hundreds of scientific tools across biology, chemistry, and materials\nscience. At its core, SciToolAgent leverages a scientific tool knowledge graph\nthat enables intelligent tool selection and execution through graph-based\nretrieval-augmented generation. The agent also incorporates a comprehensive\nsafety-checking module to ensure responsible and ethical tool usage. Extensive\nevaluations on a curated benchmark demonstrate that SciToolAgent significantly\noutperforms existing approaches. Case studies in protein engineering, chemical\nreactivity prediction, chemical synthesis, and metal-organic framework\nscreening further demonstrate SciToolAgent's capability to automate complex\nscientific workflows, making advanced research tools accessible to both experts\nand non-experts.", "AI": {"tldr": "SciToolAgent\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4ee3\u7406\uff0c\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u548c\u5b89\u5168\u6027\u68c0\u67e5\u6a21\u5757\uff0c\u81ea\u52a8\u5316\u79d1\u5b66\u5de5\u5177\u7684\u4f7f\u7528\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u79d1\u5b66\u5de5\u5177\u7684\u4f7f\u7528\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\uff0c\u800c\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5de5\u5177\u96c6\u6210\u548c\u590d\u6742\u5de5\u4f5c\u6d41\u4e2d\u8868\u73b0\u4e0d\u8db3\u3002", "method": "\u5229\u7528\u79d1\u5b66\u5de5\u5177\u77e5\u8bc6\u56fe\u8c31\u5b9e\u73b0\u667a\u80fd\u5de5\u5177\u9009\u62e9\u548c\u6267\u884c\uff0c\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548c\u5b89\u5168\u6027\u68c0\u67e5\u6a21\u5757\u3002", "result": "\u5728\u591a\u4e2a\u79d1\u5b66\u9886\u57df\uff08\u5982\u751f\u7269\u5b66\u3001\u5316\u5b66\u3001\u6750\u6599\u79d1\u5b66\uff09\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u80fd\u81ea\u52a8\u5316\u590d\u6742\u5de5\u4f5c\u6d41\u3002", "conclusion": "SciToolAgent\u4f7f\u9ad8\u7ea7\u7814\u7a76\u5de5\u5177\u5bf9\u4e13\u5bb6\u548c\u975e\u4e13\u5bb6\u90fd\u66f4\u6613\u7528\uff0c\u63a8\u52a8\u4e86\u79d1\u5b66\u7814\u7a76\u7684\u81ea\u52a8\u5316\u3002"}}
{"id": "2507.20382", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.20382", "abs": "https://arxiv.org/abs/2507.20382", "authors": ["Yuyou Zhang", "Radu Corcodel", "Ding Zhao"], "title": "Bipedalism for Quadrupedal Robots: Versatile Loco-Manipulation through Risk-Adaptive Reinforcement Learning", "comment": "Humanoids 2025", "summary": "Loco-manipulation of quadrupedal robots has broadened robotic applications,\nbut using legs as manipulators often compromises locomotion, while mounting\narms complicates the system. To mitigate this issue, we introduce bipedalism\nfor quadrupedal robots, thus freeing the front legs for versatile interactions\nwith the environment. We propose a risk-adaptive distributional Reinforcement\nLearning (RL) framework designed for quadrupedal robots walking on their hind\nlegs, balancing worst-case conservativeness with optimal performance in this\ninherently unstable task. During training, the adaptive risk preference is\ndynamically adjusted based on the uncertainty of the return, measured by the\ncoefficient of variation of the estimated return distribution. Extensive\nexperiments in simulation show our method's superior performance over\nbaselines. Real-world deployment on a Unitree Go2 robot further demonstrates\nthe versatility of our policy, enabling tasks like cart pushing, obstacle\nprobing, and payload transport, while showcasing robustness against challenging\ndynamics and external disturbances.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u8db3\u884c\u8d70\u7684\u56db\u8db3\u673a\u5668\u4eba\u65b9\u6cd5\uff0c\u901a\u8fc7\u91ca\u653e\u524d\u817f\u5b9e\u73b0\u591a\u529f\u80fd\u4ea4\u4e92\uff0c\u5e76\u91c7\u7528\u98ce\u9669\u81ea\u9002\u5e94\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5e73\u8861\u7a33\u5b9a\u6027\u548c\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u56db\u8db3\u673a\u5668\u4eba\u4f7f\u7528\u817f\u4f5c\u4e3a\u64cd\u7eb5\u5668\u65f6\u5f71\u54cd\u8fd0\u52a8\u80fd\u529b\u7684\u95ee\u9898\uff0c\u540c\u65f6\u907f\u514d\u5b89\u88c5\u989d\u5916\u673a\u68b0\u81c2\u7684\u590d\u6742\u6027\u3002", "method": "\u5f15\u5165\u53cc\u8db3\u884c\u8d70\u6a21\u5f0f\uff0c\u91c7\u7528\u57fa\u4e8e\u98ce\u9669\u81ea\u9002\u5e94\u7684\u5206\u5e03\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u52a8\u6001\u8c03\u6574\u98ce\u9669\u504f\u597d\u4ee5\u5e94\u5bf9\u4e0d\u7a33\u5b9a\u4efb\u52a1\u3002", "result": "\u4eff\u771f\u548c\u5b9e\u7269\u5b9e\u9a8c\uff08Unitree Go2\u673a\u5668\u4eba\uff09\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\uff0c\u5b9e\u73b0\u4e86\u63a8\u8f66\u3001\u63a2\u6d4b\u969c\u788d\u7269\u548c\u8fd0\u8f93\u8d1f\u8f7d\u7b49\u4efb\u52a1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u591a\u529f\u80fd\u4ea4\u4e92\u548c\u52a8\u6001\u7a33\u5b9a\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u9002\u7528\u4e8e\u590d\u6742\u73af\u5883\u548c\u5916\u90e8\u5e72\u6270\u4e0b\u7684\u4efb\u52a1\u3002"}}
{"id": "2507.20322", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.20322", "abs": "https://arxiv.org/abs/2507.20322", "authors": ["Manish Verma", "Vivek Sharma", "Vishal Singh"], "title": "Artificial Intelligence In Patent And Market Intelligence: A New Paradigm For Technology Scouting", "comment": "Page 4-Figure 1 and Page 11-Figure 2 . A preprint describing a system\n  for AI-powered technology scouting", "summary": "This paper presents the development of an AI powered software platform that\nleverages advanced large language models (LLMs) to transform technology\nscouting and solution discovery in industrial R&D. Traditional approaches to\nsolving complex research and development challenges are often time consuming,\nmanually driven, and heavily dependent on domain specific expertise. These\nmethods typically involve navigating fragmented sources such as patent\nrepositories, commercial product catalogs, and competitor data, leading to\ninefficiencies and incomplete insights. The proposed platform utilizes cutting\nedge LLM capabilities including semantic understanding, contextual reasoning,\nand cross-domain knowledge extraction to interpret problem statements and\nretrieve high-quality, sustainable solutions. The system processes unstructured\npatent texts, such as claims and technical descriptions, and systematically\nextracts potential innovations aligned with the given problem context. These\nsolutions are then algorithmically organized under standardized technical\ncategories and subcategories to ensure clarity and relevance across\ninterdisciplinary domains. In addition to patent analysis, the platform\nintegrates commercial intelligence by identifying validated market solutions\nand active organizations addressing similar challenges. This combined insight\nsourced from both intellectual property and real world product data enables R&D\nteams to assess not only technical novelty but also feasibility, scalability,\nand sustainability. The result is a comprehensive, AI driven scouting engine\nthat reduces manual effort, accelerates innovation cycles, and enhances\ndecision making in complex R&D environments.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684AI\u5e73\u53f0\uff0c\u7528\u4e8e\u5de5\u4e1a\u7814\u53d1\u4e2d\u7684\u6280\u672f\u641c\u7d22\u548c\u89e3\u51b3\u65b9\u6848\u53d1\u73b0\uff0c\u63d0\u5347\u6548\u7387\u548c\u51b3\u7b56\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edf\u6280\u672f\u641c\u7d22\u65b9\u6cd5\u8017\u65f6\u3001\u4f9d\u8d56\u4eba\u5de5\u548c\u9886\u57df\u4e13\u5bb6\uff0c\u4e14\u4fe1\u606f\u6765\u6e90\u5206\u6563\uff0c\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u548c\u6d1e\u5bdf\u4e0d\u5b8c\u6574\u3002", "method": "\u5229\u7528LLM\u7684\u8bed\u4e49\u7406\u89e3\u3001\u4e0a\u4e0b\u6587\u63a8\u7406\u548c\u8de8\u9886\u57df\u77e5\u8bc6\u63d0\u53d6\u80fd\u529b\uff0c\u5206\u6790\u4e13\u5229\u6587\u672c\u548c\u5546\u4e1a\u6570\u636e\uff0c\u7cfb\u7edf\u5316\u63d0\u53d6\u548c\u7ec4\u7ec7\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u5e73\u53f0\u51cf\u5c11\u4e86\u4eba\u5de5\u5de5\u4f5c\u91cf\uff0c\u52a0\u901f\u4e86\u521b\u65b0\u5468\u671f\uff0c\u5e76\u63d0\u5347\u4e86\u590d\u6742\u7814\u53d1\u73af\u5883\u4e2d\u7684\u51b3\u7b56\u80fd\u529b\u3002", "conclusion": "\u8be5AI\u9a71\u52a8\u7684\u5e73\u53f0\u4e3a\u5de5\u4e1a\u7814\u53d1\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u5168\u9762\u7684\u6280\u672f\u641c\u7d22\u548c\u89e3\u51b3\u65b9\u6848\u53d1\u73b0\u5de5\u5177\u3002"}}
{"id": "2507.20427", "categories": ["cs.RO", "J.2; I.2; I.6"], "pdf": "https://arxiv.org/pdf/2507.20427", "abs": "https://arxiv.org/abs/2507.20427", "authors": ["Mattia Piccinini", "Aniello Mungiello", "Georg Jank", "Gastone Pietro Rosati Papini", "Francesco Biral", "Johannes Betz"], "title": "Model-Structured Neural Networks to Control the Steering Dynamics of Autonomous Race Cars", "comment": "Accepted at the 2025 IEEE International Conference on Intelligent\n  Transportation Systems (ITSC)", "summary": "Autonomous racing has gained increasing attention in recent years, as a safe\nenvironment to accelerate the development of motion planning and control\nmethods for autonomous driving. Deep learning models, predominantly based on\nneural networks (NNs), have demonstrated significant potential in modeling the\nvehicle dynamics and in performing various tasks in autonomous driving.\nHowever, their black-box nature is critical in the context of autonomous\nracing, where safety and robustness demand a thorough understanding of the\ndecision-making algorithms. To address this challenge, this paper proposes\nMS-NN-steer, a new Model-Structured Neural Network for vehicle steering\ncontrol, integrating the prior knowledge of the nonlinear vehicle dynamics into\nthe neural architecture. The proposed controller is validated using real-world\ndata from the Abu Dhabi Autonomous Racing League (A2RL) competition, with\nfull-scale autonomous race cars. In comparison with general-purpose NNs,\nMS-NN-steer is shown to achieve better accuracy and generalization with small\ntraining datasets, while being less sensitive to the weights' initialization.\nAlso, MS-NN-steer outperforms the steering controller used by the A2RL winning\nteam. Our implementation is available open-source in a GitHub repository.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMS-NN-steer\u7684\u65b0\u578b\u6a21\u578b\u7ed3\u6784\u795e\u7ecf\u7f51\u7edc\uff0c\u7528\u4e8e\u8f66\u8f86\u8f6c\u5411\u63a7\u5236\uff0c\u901a\u8fc7\u5c06\u975e\u7ebf\u6027\u8f66\u8f86\u52a8\u529b\u5b66\u7684\u5148\u9a8c\u77e5\u8bc6\u878d\u5165\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u89e3\u51b3\u4e86\u81ea\u52a8\u9a7e\u9a76\u8d5b\u8f66\u4e2d\u5b89\u5168\u6027\u548c\u9c81\u68d2\u6027\u7684\u6311\u6218\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u8d5b\u8f66\u4f5c\u4e3a\u52a0\u901f\u81ea\u52a8\u9a7e\u9a76\u8fd0\u52a8\u89c4\u5212\u548c\u63a7\u5236\u65b9\u6cd5\u53d1\u5c55\u7684\u5b89\u5168\u73af\u5883\uff0c\u5176\u5b89\u5168\u6027\u548c\u9c81\u68d2\u6027\u8981\u6c42\u5bf9\u51b3\u7b56\u7b97\u6cd5\u6709\u6df1\u5165\u7406\u89e3\u3002\u7136\u800c\uff0c\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u9ed1\u76d2\u7279\u6027\u96be\u4ee5\u6ee1\u8db3\u8fd9\u4e00\u9700\u6c42\u3002", "method": "\u63d0\u51faMS-NN-steer\uff0c\u4e00\u79cd\u5c06\u975e\u7ebf\u6027\u8f66\u8f86\u52a8\u529b\u5b66\u5148\u9a8c\u77e5\u8bc6\u878d\u5165\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u7684\u6a21\u578b\u7ed3\u6784\u795e\u7ecf\u7f51\u7edc\uff0c\u7528\u4e8e\u8f66\u8f86\u8f6c\u5411\u63a7\u5236\u3002", "result": "\u5728\u963f\u5e03\u624e\u6bd4\u81ea\u52a8\u9a7e\u9a76\u8d5b\u8f66\u8054\u76df\uff08A2RL\uff09\u7ade\u8d5b\u7684\u771f\u5b9e\u6570\u636e\u9a8c\u8bc1\u4e2d\uff0cMS-NN-steer\u5728\u5c0f\u8bad\u7ec3\u6570\u636e\u96c6\u4e0b\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4e14\u5bf9\u6743\u91cd\u521d\u59cb\u5316\u4e0d\u654f\u611f\uff0c\u6027\u80fd\u4f18\u4e8eA2RL\u51a0\u519b\u56e2\u961f\u7684\u8f6c\u5411\u63a7\u5236\u5668\u3002", "conclusion": "MS-NN-steer\u5728\u81ea\u52a8\u9a7e\u9a76\u8d5b\u8f66\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u7ed3\u5408\u5148\u9a8c\u77e5\u8bc6\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u63d0\u5347\u4e86\u63a7\u5236\u5668\u7684\u6027\u80fd\u548c\u53ef\u9760\u6027\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u8d5b\u8f66\u9886\u57df\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.20333", "categories": ["cs.AI", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.20333", "abs": "https://arxiv.org/abs/2507.20333", "authors": ["Rachel S. Y. Teo", "Laziz U. Abdullaev", "Tan M. Nguyen"], "title": "The Blessing and Curse of Dimensionality in Safety Alignment", "comment": "Published as a conference paper at COLM 2025", "summary": "The focus on safety alignment in large language models (LLMs) has increased\nsignificantly due to their widespread adoption across different domains. The\nscale of LLMs play a contributing role in their success, and the growth in\nparameter count follows larger hidden dimensions. In this paper, we hypothesize\nthat while the increase in dimensions has been a key advantage, it may lead to\nemergent problems as well. These problems emerge as the linear structures in\nthe activation space can be exploited, in the form of activation engineering,\nto circumvent its safety alignment. Through detailed visualizations of linear\nsubspaces associated with different concepts, such as safety, across various\nmodel scales, we show that the curse of high-dimensional representations\nuniquely impacts LLMs. Further substantiating our claim, we demonstrate that\nprojecting the representations of the model onto a lower dimensional subspace\ncan preserve sufficient information for alignment while avoiding those linear\nstructures. Empirical results confirm that such dimensional reduction\nsignificantly reduces susceptibility to jailbreaking through representation\nengineering. Building on our empirical validations, we provide theoretical\ninsights into these linear jailbreaking methods relative to a model's hidden\ndimensions. Broadly speaking, our work posits that the high dimensions of a\nmodel's internal representations can be both a blessing and a curse in safety\nalignment.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5b89\u5168\u5bf9\u9f50\u4e2d\u9ad8\u7ef4\u8868\u793a\u7684\u53cc\u5203\u5251\u6548\u5e94\uff0c\u63d0\u51fa\u964d\u7ef4\u65b9\u6cd5\u4ee5\u51cf\u5c11\u7ebf\u6027\u7ed3\u6784\u88ab\u5229\u7528\u7684\u98ce\u9669\u3002", "motivation": "\u968f\u7740LLMs\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u9ad8\u7ef4\u8868\u793a\u867d\u5e26\u6765\u6027\u80fd\u4f18\u52bf\uff0c\u4f46\u4e5f\u53ef\u80fd\u88ab\u5229\u7528\uff08\u5982\u6fc0\u6d3b\u5de5\u7a0b\uff09\u7ed5\u8fc7\u5b89\u5168\u5bf9\u9f50\uff0c\u9700\u7814\u7a76\u5176\u5f71\u54cd\u53ca\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u53ef\u89c6\u5316\u4e0d\u540c\u6982\u5ff5\uff08\u5982\u5b89\u5168\uff09\u7684\u7ebf\u6027\u5b50\u7a7a\u95f4\uff0c\u5206\u6790\u9ad8\u7ef4\u8868\u793a\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u964d\u7ef4\u6295\u5f71\u65b9\u6cd5\u4ee5\u51cf\u5c11\u6f0f\u6d1e\u3002", "result": "\u5b9e\u8bc1\u8868\u660e\u964d\u7ef4\u53ef\u663e\u8457\u964d\u4f4e\u901a\u8fc7\u8868\u793a\u5de5\u7a0b\u7684\u8d8a\u72f1\u98ce\u9669\uff0c\u540c\u65f6\u4fdd\u7559\u8db3\u591f\u5bf9\u9f50\u4fe1\u606f\u3002", "conclusion": "\u9ad8\u7ef4\u8868\u793a\u65e2\u662f\u4f18\u52bf\u4e5f\u662f\u9690\u60a3\uff0c\u964d\u7ef4\u662f\u89e3\u51b3\u7ebf\u6027\u8d8a\u72f1\u95ee\u9898\u7684\u6709\u6548\u624b\u6bb5\u3002"}}
{"id": "2507.20445", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.20445", "abs": "https://arxiv.org/abs/2507.20445", "authors": ["Tianyu Li", "Hengbo Ma", "Sehoon Ha", "Kwonjoon Lee"], "title": "Learning Physical Interaction Skills from Human Demonstrations", "comment": null, "summary": "Learning physical interaction skills, such as dancing, handshaking, or\nsparring, remains a fundamental challenge for agents operating in human\nenvironments, particularly when the agent's morphology differs significantly\nfrom that of the demonstrator. Existing approaches often rely on handcrafted\nobjectives or morphological similarity, limiting their capacity for\ngeneralization. Here, we introduce a framework that enables agents with diverse\nembodiments to learn wholebbody interaction behaviors directly from human\ndemonstrations. The framework extracts a compact, transferable representation\nof interaction dynamics, called the Embedded Interaction Graph (EIG), which\ncaptures key spatiotemporal relationships between the interacting agents. This\ngraph is then used as an imitation objective to train control policies in\nphysics-based simulations, allowing the agent to generate motions that are both\nsemantically meaningful and physically feasible. We demonstrate BuddyImitation\non multiple agents, such as humans, quadrupedal robots with manipulators, or\nmobile manipulators and various interaction scenarios, including sparring,\nhandshaking, rock-paper-scissors, or dancing. Our results demonstrate a\npromising path toward coordinated behaviors across morphologically distinct\ncharacters via cross embodiment interaction learning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBuddyImitation\u7684\u6846\u67b6\uff0c\u901a\u8fc7Embedded Interaction Graph\uff08EIG\uff09\u4ece\u4eba\u7c7b\u6f14\u793a\u4e2d\u5b66\u4e60\u5168\u8eab\u4ea4\u4e92\u884c\u4e3a\uff0c\u9002\u7528\u4e8e\u5f62\u6001\u5404\u5f02\u7684\u667a\u80fd\u4f53\u3002", "motivation": "\u89e3\u51b3\u667a\u80fd\u4f53\u5728\u5f62\u6001\u4e0e\u6f14\u793a\u8005\u5dee\u5f02\u663e\u8457\u65f6\u5b66\u4e60\u7269\u7406\u4ea4\u4e92\u6280\u80fd\u7684\u6311\u6218\uff0c\u514b\u670d\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u624b\u5de5\u76ee\u6807\u6216\u5f62\u6001\u76f8\u4f3c\u6027\u7684\u9650\u5236\u3002", "method": "\u63d0\u53d6\u4ea4\u4e92\u52a8\u6001\u7684\u7d27\u51d1\u53ef\u8fc1\u79fb\u8868\u793aEIG\uff0c\u4f5c\u4e3a\u6a21\u4eff\u76ee\u6807\u8bad\u7ec3\u63a7\u5236\u7b56\u7565\uff0c\u751f\u6210\u8bed\u4e49\u660e\u786e\u4e14\u7269\u7406\u53ef\u884c\u7684\u52a8\u4f5c\u3002", "result": "\u5728\u591a\u79cd\u667a\u80fd\u4f53\u548c\u4ea4\u4e92\u573a\u666f\uff08\u5982\u683c\u6597\u3001\u63e1\u624b\u3001\u731c\u62f3\u3001\u8df3\u821e\uff09\u4e2d\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u4e3a\u5f62\u6001\u5dee\u5f02\u5927\u7684\u667a\u80fd\u4f53\u4e4b\u95f4\u7684\u534f\u8c03\u884c\u4e3a\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u524d\u666f\u7684\u5b66\u4e60\u8def\u5f84\u3002"}}
{"id": "2507.20342", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.20342", "abs": "https://arxiv.org/abs/2507.20342", "authors": ["Zhipeng Tang", "Sha Zhang", "Jiajun Deng", "Chenjie Wang", "Guoliang You", "Yuting Huang", "Xinrui Lin", "Yanyong Zhang"], "title": "VLMPlanner: Integrating Visual Language Models with Motion Planning", "comment": "8 pages, 3 figures, this paper has been accepted by ACM MM 2025", "summary": "Integrating large language models (LLMs) into autonomous driving motion\nplanning has recently emerged as a promising direction, offering enhanced\ninterpretability, better controllability, and improved generalization in rare\nand long-tail scenarios. However, existing methods often rely on abstracted\nperception or map-based inputs, missing crucial visual context, such as\nfine-grained road cues, accident aftermath, or unexpected obstacles, which are\nessential for robust decision-making in complex driving environments. To bridge\nthis gap, we propose VLMPlanner, a hybrid framework that combines a\nlearning-based real-time planner with a vision-language model (VLM) capable of\nreasoning over raw images. The VLM processes multi-view images to capture rich,\ndetailed visual information and leverages its common-sense reasoning\ncapabilities to guide the real-time planner in generating robust and safe\ntrajectories. Furthermore, we develop the Context-Adaptive Inference Gate\n(CAI-Gate) mechanism that enables the VLM to mimic human driving behavior by\ndynamically adjusting its inference frequency based on scene complexity,\nthereby achieving an optimal balance between planning performance and\ncomputational efficiency. We evaluate our approach on the large-scale,\nchallenging nuPlan benchmark, with comprehensive experimental results\ndemonstrating superior planning performance in scenarios with intricate road\nconditions and dynamic elements. Code will be available.", "AI": {"tldr": "VLMPlanner\u662f\u4e00\u4e2a\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u548c\u5b9e\u65f6\u89c4\u5212\u5668\u7684\u6df7\u5408\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u8fd0\u52a8\u89c4\u5212\uff0c\u901a\u8fc7\u591a\u89c6\u89d2\u56fe\u50cf\u6355\u6349\u7ec6\u8282\u89c6\u89c9\u4fe1\u606f\uff0c\u5e76\u52a8\u6001\u8c03\u6574\u63a8\u7406\u9891\u7387\u4ee5\u4f18\u5316\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u89c6\u89c9\u4e0a\u4e0b\u6587\uff08\u5982\u9053\u8def\u7ec6\u8282\u3001\u610f\u5916\u969c\u788d\uff09\u7684\u5145\u5206\u5229\u7528\uff0c\u5f71\u54cd\u4e86\u590d\u6742\u9a7e\u9a76\u73af\u5883\u4e2d\u7684\u51b3\u7b56\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51faVLMPlanner\u6846\u67b6\uff0c\u5229\u7528VLM\u5904\u7406\u591a\u89c6\u89d2\u56fe\u50cf\u5e76\u63a8\u7406\uff0c\u7ed3\u5408CAI-Gate\u673a\u5236\u52a8\u6001\u8c03\u6574\u63a8\u7406\u9891\u7387\u3002", "result": "\u5728nuPlan\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728\u590d\u6742\u9053\u8def\u6761\u4ef6\u548c\u52a8\u6001\u573a\u666f\u4e2d\u3002", "conclusion": "VLMPlanner\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u52a8\u6001\u63a8\u7406\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u7684\u9c81\u68d2\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2507.20509", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.20509", "abs": "https://arxiv.org/abs/2507.20509", "authors": ["Zhongchao Zhou", "Yuxi Lu", "Yaonan Zhu", "Yifan Zhao", "Bin He", "Liang He", "Wenwen Yu", "Yusuke Iwasawa"], "title": "LLMs-guided adaptive compensator: Bringing Adaptivity to Automatic Control Systems with Large Language Models", "comment": null, "summary": "With rapid advances in code generation, reasoning, and problem-solving, Large\nLanguage Models (LLMs) are increasingly applied in robotics. Most existing work\nfocuses on high-level tasks such as task decomposition. A few studies have\nexplored the use of LLMs in feedback controller design; however, these efforts\nare restricted to overly simplified systems, fixed-structure gain tuning, and\nlack real-world validation. To further investigate LLMs in automatic control,\nthis work targets a key subfield: adaptive control. Inspired by the framework\nof model reference adaptive control (MRAC), we propose an LLM-guided adaptive\ncompensator framework that avoids designing controllers from scratch. Instead,\nthe LLMs are prompted using the discrepancies between an unknown system and a\nreference system to design a compensator that aligns the response of the\nunknown system with that of the reference, thereby achieving adaptivity.\nExperiments evaluate five methods: LLM-guided adaptive compensator, LLM-guided\nadaptive controller, indirect adaptive control, learning-based adaptive\ncontrol, and MRAC, on soft and humanoid robots in both simulated and real-world\nenvironments. Results show that the LLM-guided adaptive compensator outperforms\ntraditional adaptive controllers and significantly reduces reasoning complexity\ncompared to the LLM-guided adaptive controller. The Lyapunov-based analysis and\nreasoning-path inspection demonstrate that the LLM-guided adaptive compensator\nenables a more structured design process by transforming mathematical\nderivation into a reasoning task, while exhibiting strong generalizability,\nadaptability, and robustness. This study opens a new direction for applying\nLLMs in the field of automatic control, offering greater deployability and\npracticality compared to vision-language models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u81ea\u9002\u5e94\u8865\u507f\u5668\u6846\u67b6\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u63a7\u5236\uff0c\u907f\u514d\u4e86\u4ece\u5934\u8bbe\u8ba1\u63a7\u5236\u5668\u7684\u590d\u6742\u6027\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u4f20\u7edf\u81ea\u9002\u5e94\u63a7\u5236\u5668\uff0c\u5e76\u663e\u8457\u964d\u4f4e\u4e86\u63a8\u7406\u590d\u6742\u5ea6\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u96c6\u4e2d\u4e8eLLM\u5728\u673a\u5668\u4eba\u9ad8\u7ea7\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\uff0c\u800c\u5728\u53cd\u9988\u63a7\u5236\u5668\u8bbe\u8ba1\u65b9\u9762\u7814\u7a76\u8f83\u5c11\u4e14\u5c40\u9650\u4e8e\u7b80\u5316\u7cfb\u7edf\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22LLM\u5728\u81ea\u9002\u5e94\u63a7\u5236\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\u3002", "method": "\u53d7\u6a21\u578b\u53c2\u8003\u81ea\u9002\u5e94\u63a7\u5236\uff08MRAC\uff09\u542f\u53d1\uff0c\u63d0\u51faLLM\u5f15\u5bfc\u7684\u81ea\u9002\u5e94\u8865\u507f\u5668\u6846\u67b6\uff0c\u5229\u7528\u672a\u77e5\u7cfb\u7edf\u4e0e\u53c2\u8003\u7cfb\u7edf\u7684\u5dee\u5f02\u8bbe\u8ba1\u8865\u507f\u5668\uff0c\u5b9e\u73b0\u54cd\u5e94\u5bf9\u9f50\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLLM\u5f15\u5bfc\u7684\u81ea\u9002\u5e94\u8865\u507f\u5668\u5728\u8f6f\u4f53\u548c\u7c7b\u4eba\u673a\u5668\u4eba\u4e0a\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u4e14\u63a8\u7406\u590d\u6742\u5ea6\u66f4\u4f4e\u3002Lyapunov\u5206\u6790\u9a8c\u8bc1\u4e86\u5176\u7ed3\u6784\u5316\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3aLLM\u5728\u81ea\u52a8\u63a7\u5236\u9886\u57df\u7684\u5e94\u7528\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\uff0c\u5177\u6709\u66f4\u5f3a\u7684\u90e8\u7f72\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2507.20377", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.20377", "abs": "https://arxiv.org/abs/2507.20377", "authors": ["Farshid Nooshi", "Suining He"], "title": "Multi-Agent Reinforcement Learning for Dynamic Mobility Resource Allocation with Hierarchical Adaptive Grouping", "comment": "5 pages, UrbComp 2025", "summary": "Allocating mobility resources (e.g., shared bikes/e-scooters, ride-sharing\nvehicles) is crucial for rebalancing the mobility demand and supply in the\nurban environments. We propose in this work a novel multi-agent reinforcement\nlearning named Hierarchical Adaptive Grouping-based Parameter Sharing (HAG-PS)\nfor dynamic mobility resource allocation. HAG-PS aims to address two important\nresearch challenges regarding multi-agent reinforcement learning for mobility\nresource allocation: (1) how to dynamically and adaptively share the mobility\nresource allocation policy (i.e., how to distribute mobility resources) across\nagents (i.e., representing the regional coordinators of mobility resources);\nand (2) how to achieve memory-efficient parameter sharing in an urban-scale\nsetting. To address the above challenges, we have provided following novel\ndesigns within HAG-PS. To enable dynamic and adaptive parameter sharing, we\nhave designed a hierarchical approach that consists of global and local\ninformation of the mobility resource states (e.g., distribution of mobility\nresources). We have developed an adaptive agent grouping approach in order to\nsplit or merge the groups of agents based on their relative closeness of\nencoded trajectories (i.e., states, actions, and rewards). We have designed a\nlearnable identity (ID) embeddings to enable agent specialization beyond simple\nparameter copy. We have performed extensive experimental studies based on\nreal-world NYC bike sharing data (a total of more than 1.2 million trips), and\ndemonstrated the superior performance (e.g., improved bike availability) of\nHAG-PS compared with other baseline approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHAG-PS\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u52a8\u6001\u5206\u914d\u57ce\u5e02\u79fb\u52a8\u8d44\u6e90\uff08\u5982\u5171\u4eab\u5355\u8f66/\u7535\u52a8\u6ed1\u677f\u8f66\uff09\uff0c\u89e3\u51b3\u4e86\u7b56\u7565\u52a8\u6001\u5171\u4eab\u548c\u5185\u5b58\u9ad8\u6548\u53c2\u6570\u5171\u4eab\u7684\u6311\u6218\u3002", "motivation": "\u57ce\u5e02\u79fb\u52a8\u8d44\u6e90\u5206\u914d\u9700\u8981\u5e73\u8861\u4f9b\u9700\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u52a8\u6001\u5171\u4eab\u7b56\u7565\u548c\u5185\u5b58\u6548\u7387\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u5206\u5c42\u81ea\u9002\u5e94\u5206\u7ec4\u53c2\u6570\u5171\u4eab\uff08HAG-PS\uff09\uff0c\u5305\u62ec\u5168\u5c40\u548c\u5c40\u90e8\u4fe1\u606f\u7684\u5206\u5c42\u8bbe\u8ba1\u3001\u81ea\u9002\u5e94\u4ee3\u7406\u5206\u7ec4\u548c\u53ef\u5b66\u4e60\u7684ID\u5d4c\u5165\u3002", "result": "\u57fa\u4e8e\u7ebd\u7ea6\u5171\u4eab\u5355\u8f66\u6570\u636e\u7684\u5b9e\u9a8c\u663e\u793a\uff0cHAG-PS\u5728\u8d44\u6e90\u53ef\u7528\u6027\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "HAG-PS\u6709\u6548\u89e3\u51b3\u4e86\u52a8\u6001\u8d44\u6e90\u5206\u914d\u4e2d\u7684\u7b56\u7565\u5171\u4eab\u548c\u5185\u5b58\u6548\u7387\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u57ce\u5e02\u79fb\u52a8\u8d44\u6e90\u7ba1\u7406\u7684\u6027\u80fd\u3002"}}
{"id": "2507.20516", "categories": ["cs.RO", "68T40", "I.2.9"], "pdf": "https://arxiv.org/pdf/2507.20516", "abs": "https://arxiv.org/abs/2507.20516", "authors": ["Xiaofeng Jin", "Ningbo Bu", "Shijie Wang", "Jianfei Ge", "Jiangjian Xiao", "Matteo Matteucci"], "title": "Large-Scale LiDAR-Inertial Dataset for Degradation-Robust High-Precision Mapping", "comment": "9 pages,7 figures, 6 tables", "summary": "This paper introduces a large-scale, high-precision LiDAR-Inertial Odometry\n(LIO) dataset, aiming to address the insufficient validation of LIO systems in\ncomplex real-world scenarios in existing research. The dataset covers four\ndiverse real-world environments spanning 60,000 to 750,000 square meters,\ncollected using a custom backpack-mounted platform equipped with multi-beam\nLiDAR, an industrial-grade IMU, and RTK-GNSS modules. The dataset includes long\ntrajectories, complex scenes, and high-precision ground truth, generated by\nfusing SLAM-based optimization with RTK-GNSS anchoring, and validated for\ntrajectory accuracy through the integration of oblique photogrammetry and\nRTK-GNSS. This dataset provides a comprehensive benchmark for evaluating the\ngeneralization ability of LIO systems in practical high-precision mapping\nscenarios.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u5927\u89c4\u6a21\u3001\u9ad8\u7cbe\u5ea6\u7684LiDAR-\u60ef\u6027\u91cc\u7a0b\u8ba1\uff08LIO\uff09\u6570\u636e\u96c6\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u7814\u7a76\u4e2dLIO\u7cfb\u7edf\u5728\u590d\u6742\u73b0\u5b9e\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e2d\u5bf9LIO\u7cfb\u7edf\u5728\u590d\u6742\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u9a8c\u8bc1\u4e0d\u8db3\uff0c\u7f3a\u4e4f\u5168\u9762\u7684\u57fa\u51c6\u6570\u636e\u96c6\u3002", "method": "\u4f7f\u7528\u81ea\u5b9a\u4e49\u80cc\u5305\u5f0f\u5e73\u53f0\uff0c\u914d\u5907\u591a\u5149\u675fLiDAR\u3001\u5de5\u4e1a\u7ea7IMU\u548cRTK-GNSS\u6a21\u5757\uff0c\u5728\u56db\u79cd\u591a\u6837\u5316\u7684\u73b0\u5b9e\u73af\u5883\u4e2d\u6536\u96c6\u6570\u636e\uff0c\u8986\u76d6\u9762\u79ef\u4ece60,000\u5230750,000\u5e73\u65b9\u7c73\u3002\u901a\u8fc7SLAM\u4f18\u5316\u4e0eRTK-GNSS\u951a\u5b9a\u878d\u5408\u751f\u6210\u9ad8\u7cbe\u5ea6\u5730\u9762\u771f\u5b9e\u503c\uff0c\u5e76\u901a\u8fc7\u503e\u659c\u6444\u5f71\u6d4b\u91cf\u548cRTK-GNSS\u9a8c\u8bc1\u8f68\u8ff9\u7cbe\u5ea6\u3002", "result": "\u6570\u636e\u96c6\u5305\u542b\u957f\u8f68\u8ff9\u3001\u590d\u6742\u573a\u666f\u548c\u9ad8\u7cbe\u5ea6\u5730\u9762\u771f\u5b9e\u503c\uff0c\u4e3aLIO\u7cfb\u7edf\u5728\u5b9e\u9645\u9ad8\u7cbe\u5ea6\u6d4b\u7ed8\u573a\u666f\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u63d0\u4f9b\u4e86\u5168\u9762\u57fa\u51c6\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u586b\u8865\u4e86LIO\u7cfb\u7edf\u9a8c\u8bc1\u7684\u7a7a\u767d\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u9ad8\u7cbe\u5ea6\u6d4b\u7ed8\u63d0\u4f9b\u4e86\u91cd\u8981\u652f\u6301\u3002"}}
{"id": "2507.20395", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.20395", "abs": "https://arxiv.org/abs/2507.20395", "authors": ["Hafsteinn Einarsson"], "title": "MazeEval: A Benchmark for Testing Sequential Decision-Making in Language Models", "comment": null, "summary": "As Large Language Models (LLMs) increasingly power autonomous agents in\nrobotics and embodied AI, understanding their spatial reasoning capabilities\nbecomes crucial for ensuring reliable real-world deployment. Despite advances\nin language understanding, current research lacks evaluation of how LLMs\nperform spatial navigation without visual cues, a fundamental requirement for\nagents operating with limited sensory information. This paper addresses this\ngap by introducing MazeEval, a benchmark designed to isolate and evaluate pure\nspatial reasoning in LLMs through coordinate-based maze navigation tasks. Our\nmethodology employs a function-calling interface where models navigate mazes of\nvarying complexity ($5\\times 5$ to $15\\times 15$ grids) using only coordinate\nfeedback and distance-to-wall information, excluding visual input to test\nfundamental spatial cognition. We evaluate eight state-of-the-art LLMs across\nidentical mazes in both English and Icelandic to assess cross-linguistic\ntransfer of spatial abilities. Our findings reveal striking disparities: while\nOpenAI's O3 achieves perfect navigation for mazes up to size $30\\times 30$,\nother models exhibit catastrophic failure beyond $9\\times 9$ mazes, with 100%\nof failures attributed to excessive looping behavior where models revisit a\ncell at least 10 times. We document a significant performance degradation in\nIcelandic, with models solving mazes 3-4 sizes smaller than in English,\nsuggesting spatial reasoning in LLMs emerges from linguistic patterns rather\nthan language-agnostic mechanisms. These results have important implications\nfor global deployment of LLM-powered autonomous systems, showing spatial\nintelligence remains fundamentally constrained by training data availability\nand highlighting the need for architectural innovations to achieve reliable\nnavigation across linguistic contexts.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86MazeEval\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30LLMs\u5728\u65e0\u89c6\u89c9\u7ebf\u7d22\u4e0b\u7684\u7eaf\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u53d1\u73b0\u4e0d\u540c\u6a21\u578b\u8868\u73b0\u5dee\u5f02\u663e\u8457\uff0c\u4e14\u8de8\u8bed\u8a00\u80fd\u529b\u53d7\u9650\u3002", "motivation": "\u968f\u7740LLMs\u5728\u81ea\u4e3b\u4ee3\u7406\u4e2d\u7684\u5e94\u7528\u589e\u52a0\uff0c\u7406\u89e3\u5176\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u5bf9\u5b9e\u9645\u90e8\u7f72\u81f3\u5173\u91cd\u8981\u3002\u5f53\u524d\u7814\u7a76\u7f3a\u4e4f\u5bf9LLMs\u5728\u65e0\u89c6\u89c9\u7ebf\u7d22\u4e0b\u7a7a\u95f4\u5bfc\u822a\u80fd\u529b\u7684\u8bc4\u4f30\u3002", "method": "\u901a\u8fc7\u5750\u6807\u53cd\u9988\u548c\u8ddd\u79bb\u4fe1\u606f\u8bbe\u8ba1\u8ff7\u5bab\u5bfc\u822a\u4efb\u52a1\uff0c\u8bc4\u4f308\u79cdLLMs\u5728\u82f1\u8bed\u548c\u51b0\u5c9b\u8bed\u4e2d\u7684\u8868\u73b0\u3002", "result": "OpenAI\u7684O3\u8868\u73b0\u6700\u4f73\uff0c\u800c\u5176\u4ed6\u6a21\u578b\u5728\u590d\u6742\u8ff7\u5bab\u4e2d\u8868\u73b0\u5d29\u6e83\uff0c\u8de8\u8bed\u8a00\u80fd\u529b\u663e\u8457\u4e0b\u964d\u3002", "conclusion": "LLMs\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u53d7\u8bad\u7ec3\u6570\u636e\u548c\u8bed\u8a00\u6a21\u5f0f\u9650\u5236\uff0c\u9700\u67b6\u6784\u521b\u65b0\u4ee5\u5b9e\u73b0\u8de8\u8bed\u8a00\u53ef\u9760\u5bfc\u822a\u3002"}}
{"id": "2507.20538", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.20538", "abs": "https://arxiv.org/abs/2507.20538", "authors": ["Gilhwan Kang", "Hogyun Kim", "Byunghee Choi", "Seokhwan Jeong", "Young-Sik Shin", "Younggun Cho"], "title": "Uni-Mapper: Unified Mapping Framework for Multi-modal LiDARs in Complex and Dynamic Environments", "comment": "18 pages, 14 figures", "summary": "The unification of disparate maps is crucial for enabling scalable robot\noperation across multiple sessions and collaborative multi-robot scenarios.\nHowever, achieving a unified map robust to sensor modalities and dynamic\nenvironments remains a challenging problem. Variations in LiDAR types and\ndynamic elements lead to differences in point cloud distribution and scene\nconsistency, hindering reliable descriptor generation and loop closure\ndetection essential for accurate map alignment. To address these challenges,\nthis paper presents Uni-Mapper, a dynamic-aware 3D point cloud map merging\nframework for multi-modal LiDAR systems. It comprises dynamic object removal,\ndynamic-aware loop closure, and multi-modal LiDAR map merging modules. A\nvoxel-wise free space hash map is built in a coarse-to-fine manner to identify\nand reject dynamic objects via temporal occupancy inconsistencies. The removal\nmodule is integrated with a LiDAR global descriptor, which encodes preserved\nstatic local features to ensure robust place recognition in dynamic\nenvironments. In the final stage, multiple pose graph optimizations are\nconducted for both intra-session and inter-map loop closures. We adopt a\ncentralized anchor-node strategy to mitigate intra-session drift errors during\nmap merging. In the final stage, centralized anchor-node-based pose graph\noptimization is performed to address intra- and inter-map loop closures for\nglobally consistent map merging. Our framework is evaluated on diverse\nreal-world datasets with dynamic objects and heterogeneous LiDARs, showing\nsuperior performance in loop detection across sensor modalities, robust mapping\nin dynamic environments, and accurate multi-map alignment over existing\nmethods. Project Page: https://sparolab.github.io/research/uni_mapper.", "AI": {"tldr": "Uni-Mapper\u662f\u4e00\u4e2a\u52a8\u6001\u611f\u77e5\u76843D\u70b9\u4e91\u5730\u56fe\u5408\u5e76\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u6a21\u6001LiDAR\u7cfb\u7edf\uff0c\u89e3\u51b3\u4e86\u52a8\u6001\u73af\u5883\u548c\u4f20\u611f\u5668\u5dee\u5f02\u5e26\u6765\u7684\u5730\u56fe\u7edf\u4e00\u6311\u6218\u3002", "motivation": "\u591a\u673a\u5668\u4eba\u534f\u4f5c\u548c\u591a\u4f1a\u8bdd\u64cd\u4f5c\u9700\u8981\u7edf\u4e00\u7684\u5730\u56fe\uff0c\u4f46\u52a8\u6001\u73af\u5883\u548c\u4e0d\u540cLiDAR\u7c7b\u578b\u5bfc\u81f4\u70b9\u4e91\u5206\u5e03\u548c\u573a\u666f\u4e00\u81f4\u6027\u5dee\u5f02\uff0c\u5f71\u54cd\u5730\u56fe\u5bf9\u9f50\u7684\u51c6\u786e\u6027\u3002", "method": "Uni-Mapper\u5305\u62ec\u52a8\u6001\u5bf9\u8c61\u79fb\u9664\u3001\u52a8\u6001\u611f\u77e5\u95ed\u73af\u548c\u591a\u6a21\u6001LiDAR\u5730\u56fe\u5408\u5e76\u6a21\u5757\uff0c\u91c7\u7528\u4f53\u7d20\u81ea\u7531\u7a7a\u95f4\u54c8\u5e0c\u56fe\u548c\u5168\u5c40\u63cf\u8ff0\u7b26\u4f18\u5316\u5730\u56fe\u5bf9\u9f50\u3002", "result": "\u5728\u52a8\u6001\u73af\u5883\u548c\u5f02\u6784LiDAR\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5b9e\u73b0\u4e86\u8de8\u4f20\u611f\u5668\u6a21\u6001\u7684\u95ed\u73af\u68c0\u6d4b\u548c\u51c6\u786e\u7684\u591a\u5730\u56fe\u5bf9\u9f50\u3002", "conclusion": "Uni-Mapper\u5728\u52a8\u6001\u73af\u5883\u548c\u591a\u6a21\u6001LiDAR\u7cfb\u7edf\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u5730\u56fe\u7edf\u4e00\u63d0\u4f9b\u4e86\u53ef\u9760\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.20444", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.20444", "abs": "https://arxiv.org/abs/2507.20444", "authors": ["Chengzhuo Han"], "title": "Enhancing QoS in Edge Computing through Federated Layering Techniques: A Pathway to Resilient AI Lifelong Learning Systems", "comment": null, "summary": "In the context of the rapidly evolving information technology landscape,\nmarked by the advent of 6G communication networks, we face an increased data\nvolume and complexity in network environments. This paper addresses these\nchallenges by focusing on Quality of Service (QoS) in edge computing\nframeworks. We propose a novel approach to enhance QoS through the development\nof General Artificial Intelligence Lifelong Learning Systems, with a special\nemphasis on Federated Layering Techniques (FLT). Our work introduces a\nfederated layering-based small model collaborative mechanism aimed at improving\nAI models' operational efficiency and response time in environments where\nresources are limited. This innovative method leverages the strengths of cloud\nand edge computing, incorporating a negotiation and debate mechanism among\nsmall AI models to enhance reasoning and decision-making processes. By\nintegrating model layering techniques with privacy protection measures, our\napproach ensures the secure transmission of model parameters while maintaining\nhigh efficiency in learning and reasoning capabilities. The experimental\nresults demonstrate that our strategy not only enhances learning efficiency and\nreasoning accuracy but also effectively protects the privacy of edge nodes.\nThis presents a viable solution for achieving resilient large model lifelong\nlearning systems, with a significant improvement in QoS for edge computing\nenvironments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8054\u90a6\u5206\u5c42\u6280\u672f\uff08FLT\uff09\u7684\u901a\u7528\u4eba\u5de5\u667a\u80fd\u7ec8\u8eab\u5b66\u4e60\u7cfb\u7edf\uff0c\u65e8\u5728\u63d0\u5347\u8fb9\u7f18\u8ba1\u7b97\u73af\u5883\u4e2d\u7684\u670d\u52a1\u8d28\u91cf\uff08QoS\uff09\u3002\u901a\u8fc7\u5c0f\u6a21\u578b\u534f\u4f5c\u673a\u5236\u548c\u9690\u79c1\u4fdd\u62a4\u63aa\u65bd\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u5b66\u4e60\u6548\u7387\u548c\u63a8\u7406\u51c6\u786e\u6027\u3002", "motivation": "\u968f\u77406G\u901a\u4fe1\u7f51\u7edc\u7684\u53d1\u5c55\uff0c\u7f51\u7edc\u73af\u5883\u4e2d\u7684\u6570\u636e\u91cf\u548c\u590d\u6742\u6027\u6025\u5267\u589e\u52a0\uff0c\u4e9f\u9700\u63d0\u5347\u8fb9\u7f18\u8ba1\u7b97\u4e2d\u7684QoS\u3002", "method": "\u91c7\u7528\u8054\u90a6\u5206\u5c42\u6280\u672f\u548c\u5c0f\u6a21\u578b\u534f\u4f5c\u673a\u5236\uff0c\u7ed3\u5408\u4e91\u4e0e\u8fb9\u7f18\u8ba1\u7b97\u7684\u4f18\u52bf\uff0c\u5e76\u5f15\u5165\u6a21\u578b\u95f4\u7684\u534f\u5546\u4e0e\u8fa9\u8bba\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u5b66\u4e60\u6548\u7387\u548c\u63a8\u7406\u51c6\u786e\u6027\uff0c\u8fd8\u4fdd\u62a4\u4e86\u8fb9\u7f18\u8282\u70b9\u7684\u9690\u79c1\u3002", "conclusion": "\u8be5\u7b56\u7565\u4e3a\u6784\u5efa\u5f39\u6027\u7684\u5927\u6a21\u578b\u7ec8\u8eab\u5b66\u4e60\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u663e\u8457\u6539\u5584\u4e86\u8fb9\u7f18\u8ba1\u7b97\u73af\u5883\u7684QoS\u3002"}}
{"id": "2507.20589", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.20589", "abs": "https://arxiv.org/abs/2507.20589", "authors": ["Francisco J. Soler Mora", "Adri\u00e1n Peidr\u00f3 Vidal", "Marc Fabregat-Ja\u00e9n", "Luis Pay\u00e1 Castell\u00f3", "\u00d3scar Reinoso Garc\u00eda"], "title": "Methods for the Segmentation of Reticular Structures Using 3D LiDAR Data: A Comparative Evaluation", "comment": null, "summary": "Reticular structures form the backbone of major infrastructure like bridges,\npylons, and airports, but their inspection and maintenance are costly and\nhazardous, often requiring human intervention. While prior research has focused\non fault detection via images or robotic platform design, the autonomous\nnavigation of robots within these structures is less explored. This study\naddresses that gap by proposing methods to detect navigable surfaces in truss\nstructures, enhancing the autonomy of climbing robots. The paper introduces\nseveral approaches for binary segmentation of navigable surfaces versus\nbackground from 3D point clouds of metallic trusses. These methods fall into\ntwo categories: analytical algorithms and deep learning models. The analytical\napproach features a custom algorithm that segments structures by analyzing the\neigendecomposition of planar patches in the point cloud. In parallel, advanced\ndeep learning models PointNet, PointNet++, MinkUNet34C, and PointTransformerV3\nare trained and evaluated for the same task. Comparative analysis shows that\nthe analytical algorithm offers easier parameter tuning and performance\ncomparable to deep learning models, which, while more computationally\nintensive, excel in segmentation accuracy. Notably, PointTransformerV3 achieves\na Mean Intersection Over Union (mIoU) of about 97%. The study demonstrates the\npromise of both analytical and deep learning methods for improving autonomous\nnavigation in complex truss environments. The results highlight the trade-offs\nbetween computational efficiency and segmentation performance, providing\nvaluable guidance for future research and practical applications in autonomous\ninfrastructure inspection and maintenance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u65b9\u6cd5\uff08\u89e3\u6790\u7b97\u6cd5\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff09\u7528\u4e8e\u6841\u67b6\u7ed3\u6784\u4e2d\u53ef\u5bfc\u822a\u8868\u9762\u7684\u68c0\u6d4b\uff0c\u63d0\u5347\u722c\u884c\u673a\u5668\u4eba\u7684\u81ea\u4e3b\u6027\u3002", "motivation": "\u6841\u67b6\u7ed3\u6784\u7684\u68c0\u67e5\u548c\u7ef4\u62a4\u6210\u672c\u9ad8\u4e14\u5371\u9669\uff0c\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u6545\u969c\u68c0\u6d4b\u6216\u673a\u5668\u4eba\u5e73\u53f0\u8bbe\u8ba1\uff0c\u800c\u81ea\u4e3b\u5bfc\u822a\u7814\u7a76\u8f83\u5c11\u3002", "method": "\u63d0\u51fa\u89e3\u6790\u7b97\u6cd5\u548c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08PointNet\u3001PointNet++\u3001MinkUNet34C\u3001PointTransformerV3\uff09\u5bf93D\u70b9\u4e91\u8fdb\u884c\u4e8c\u5206\u7c7b\u5206\u5272\u3002", "result": "\u89e3\u6790\u7b97\u6cd5\u53c2\u6570\u8c03\u6574\u7b80\u5355\u4e14\u6027\u80fd\u63a5\u8fd1\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u800c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08\u5982PointTransformerV3\uff09\u5728\u5206\u5272\u7cbe\u5ea6\u4e0a\u66f4\u4f18\uff08mIoU\u7ea697%\uff09\u3002", "conclusion": "\u7814\u7a76\u5c55\u793a\u4e86\u4e24\u79cd\u65b9\u6cd5\u5728\u590d\u6742\u6841\u67b6\u73af\u5883\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u81ea\u4e3b\u57fa\u7840\u8bbe\u65bd\u68c0\u67e5\u4e0e\u7ef4\u62a4\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002"}}
{"id": "2507.20451", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.20451", "abs": "https://arxiv.org/abs/2507.20451", "authors": ["Pritom Ray Nobin", "Imran Ahammad Rifat"], "title": "STARN-GAT: A Multi-Modal Spatio-Temporal Graph Attention Network for Accident Severity Prediction", "comment": "10 pages", "summary": "Accurate prediction of traffic accident severity is critical for improving\nroad safety, optimizing emergency response strategies, and informing the design\nof safer transportation infrastructure. However, existing approaches often\nstruggle to effectively model the intricate interdependencies among spatial,\ntemporal, and contextual variables that govern accident outcomes. In this\nstudy, we introduce STARN-GAT, a Multi-Modal Spatio-Temporal Graph Attention\nNetwork, which leverages adaptive graph construction and modality-aware\nattention mechanisms to capture these complex relationships. Unlike\nconventional methods, STARN-GAT integrates road network topology, temporal\ntraffic patterns, and environmental context within a unified attention-based\nframework. The model is evaluated on the Fatality Analysis Reporting System\n(FARS) dataset, achieving a Macro F1-score of 85 percent, ROC-AUC of 0.91, and\nrecall of 81 percent for severe incidents. To ensure generalizability within\nthe South Asian context, STARN-GAT is further validated on the ARI-BUET traffic\naccident dataset, where it attains a Macro F1-score of 0.84, recall of 0.78,\nand ROC-AUC of 0.89. These results demonstrate the model's effectiveness in\nidentifying high-risk cases and its potential for deployment in real-time,\nsafety-critical traffic management systems. Furthermore, the attention-based\narchitecture enhances interpretability, offering insights into contributing\nfactors and supporting trust in AI-assisted decision-making. Overall, STARN-GAT\nbridges the gap between advanced graph neural network techniques and practical\napplications in road safety analytics.", "AI": {"tldr": "STARN-GAT\u662f\u4e00\u79cd\u591a\u6a21\u6001\u65f6\u7a7a\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\uff0c\u7528\u4e8e\u9884\u6d4b\u4ea4\u901a\u4e8b\u6545\u4e25\u91cd\u6027\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u56fe\u6784\u5efa\u548c\u6a21\u6001\u611f\u77e5\u6ce8\u610f\u529b\u673a\u5236\u6355\u6349\u590d\u6742\u5173\u7cfb\uff0c\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5efa\u6a21\u4ea4\u901a\u4e8b\u6545\u4e25\u91cd\u6027\u7684\u7a7a\u95f4\u3001\u65f6\u95f4\u548c\u4e0a\u4e0b\u6587\u53d8\u91cf\u4e4b\u95f4\u7684\u590d\u6742\u4f9d\u8d56\u5173\u7cfb\uff0c\u9700\u8981\u66f4\u5148\u8fdb\u7684\u6a21\u578b\u6765\u63d0\u5347\u9884\u6d4b\u51c6\u786e\u6027\u548c\u5b9e\u7528\u6027\u3002", "method": "\u63d0\u51faSTARN-GAT\uff0c\u7ed3\u5408\u9053\u8def\u7f51\u7edc\u62d3\u6251\u3001\u65f6\u95f4\u4ea4\u901a\u6a21\u5f0f\u548c\u73af\u5883\u4e0a\u4e0b\u6587\uff0c\u91c7\u7528\u6ce8\u610f\u529b\u673a\u5236\u7edf\u4e00\u5efa\u6a21\u3002", "result": "\u5728FARS\u6570\u636e\u96c6\u4e0aMacro F1-score\u8fbe85%\uff0cROC-AUC\u4e3a0.91\uff1b\u5728ARI-BUET\u6570\u636e\u96c6\u4e0aMacro F1-score\u4e3a0.84\uff0cROC-AUC\u4e3a0.89\u3002", "conclusion": "STARN-GAT\u5728\u9884\u6d4b\u9ad8\u98ce\u9669\u4e8b\u6545\u548c\u63d0\u5347\u51b3\u7b56\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u4ea4\u901a\u7ba1\u7406\u7cfb\u7edf\u3002"}}
{"id": "2507.20622", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.20622", "abs": "https://arxiv.org/abs/2507.20622", "authors": ["Guangyan Chen", "Meiling Wang", "Te Cui", "Yao Mu", "Haoyang Lu", "Zicai Peng", "Mengxiao Hu", "Tianxing Zhou", "Mengyin Fu", "Yi Yang", "Yufeng Yue"], "title": "FMimic: Foundation Models are Fine-grained Action Learners from Human Videos", "comment": "accepted to International Journal of Robotics Research(IJRR)", "summary": "Visual imitation learning (VIL) provides an efficient and intuitive strategy\nfor robotic systems to acquire novel skills. Recent advancements in foundation\nmodels, particularly Vision Language Models (VLMs), have demonstrated\nremarkable capabilities in visual and linguistic reasoning for VIL tasks.\nDespite this progress, existing approaches primarily utilize these models for\nlearning high-level plans from human demonstrations, relying on pre-defined\nmotion primitives for executing physical interactions, which remains a major\nbottleneck for robotic systems. In this work, we present FMimic, a novel\nparadigm that harnesses foundation models to directly learn generalizable\nskills at even fine-grained action levels, using only a limited number of human\nvideos. Extensive experiments demonstrate that our FMimic delivers strong\nperformance with a single human video, and significantly outperforms all other\nmethods with five videos. Furthermore, our method exhibits significant\nimprovements of over 39% and 29% in RLBench multi-task experiments and\nreal-world manipulation tasks, respectively, and exceeds baselines by more than\n34% in high-precision tasks and 47% in long-horizon tasks.", "AI": {"tldr": "FMimic\u5229\u7528\u57fa\u7840\u6a21\u578b\u76f4\u63a5\u4ece\u5c11\u91cf\u4eba\u7c7b\u89c6\u9891\u4e2d\u5b66\u4e60\u7ec6\u7c92\u5ea6\u52a8\u4f5c\u6280\u80fd\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u6a21\u4eff\u5b66\u4e60\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u9884\u5b9a\u4e49\u52a8\u4f5c\u539f\u8bed\u6267\u884c\u7269\u7406\u4ea4\u4e92\uff0c\u9650\u5236\u4e86\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u7075\u6d3b\u6027\u3002FMimic\u65e8\u5728\u901a\u8fc7\u57fa\u7840\u6a21\u578b\u76f4\u63a5\u5b66\u4e60\u901a\u7528\u6280\u80fd\u3002", "method": "FMimic\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u76f4\u63a5\u4ece\u4eba\u7c7b\u89c6\u9891\u4e2d\u5b66\u4e60\u7ec6\u7c92\u5ea6\u52a8\u4f5c\uff0c\u65e0\u9700\u4f9d\u8d56\u9884\u5b9a\u4e49\u52a8\u4f5c\u3002", "result": "FMimic\u5728\u5355\u89c6\u9891\u548c\u4e94\u89c6\u9891\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u591a\u4efb\u52a1\u548c\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u5206\u522b\u63d0\u534739%\u548c29%\uff0c\u9ad8\u7cbe\u5ea6\u548c\u957f\u65f6\u4efb\u52a1\u4e2d\u8d85\u8fc7\u57fa\u7ebf34%\u548c47%\u3002", "conclusion": "FMimic\u5c55\u793a\u4e86\u57fa\u7840\u6a21\u578b\u5728\u89c6\u89c9\u6a21\u4eff\u5b66\u4e60\u4e2d\u7684\u6f5c\u529b\uff0c\u80fd\u591f\u76f4\u63a5\u4ece\u5c11\u91cf\u89c6\u9891\u4e2d\u5b66\u4e60\u901a\u7528\u6280\u80fd\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002"}}
{"id": "2507.20526", "categories": ["cs.AI", "cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.20526", "abs": "https://arxiv.org/abs/2507.20526", "authors": ["Andy Zou", "Maxwell Lin", "Eliot Jones", "Micha Nowak", "Mateusz Dziemian", "Nick Winter", "Alexander Grattan", "Valent Nathanael", "Ayla Croft", "Xander Davies", "Jai Patel", "Robert Kirk", "Nate Burnikell", "Yarin Gal", "Dan Hendrycks", "J. Zico Kolter", "Matt Fredrikson"], "title": "Security Challenges in AI Agent Deployment: Insights from a Large Scale Public Competition", "comment": null, "summary": "Recent advances have enabled LLM-powered AI agents to autonomously execute\ncomplex tasks by combining language model reasoning with tools, memory, and web\naccess. But can these systems be trusted to follow deployment policies in\nrealistic environments, especially under attack? To investigate, we ran the\nlargest public red-teaming competition to date, targeting 22 frontier AI agents\nacross 44 realistic deployment scenarios. Participants submitted 1.8 million\nprompt-injection attacks, with over 60,000 successfully eliciting policy\nviolations such as unauthorized data access, illicit financial actions, and\nregulatory noncompliance. We use these results to build the Agent Red Teaming\n(ART) benchmark - a curated set of high-impact attacks - and evaluate it across\n19 state-of-the-art models. Nearly all agents exhibit policy violations for\nmost behaviors within 10-100 queries, with high attack transferability across\nmodels and tasks. Importantly, we find limited correlation between agent\nrobustness and model size, capability, or inference-time compute, suggesting\nthat additional defenses are needed against adversarial misuse. Our findings\nhighlight critical and persistent vulnerabilities in today's AI agents. By\nreleasing the ART benchmark and accompanying evaluation framework, we aim to\nsupport more rigorous security assessment and drive progress toward safer agent\ndeployment.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86LLM\u9a71\u52a8\u7684AI\u4ee3\u7406\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u662f\u5426\u9075\u5faa\u90e8\u7f72\u653f\u7b56\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u7ea2\u961f\u7ade\u8d5b\u53d1\u73b0\u5176\u5b58\u5728\u4e25\u91cd\u6f0f\u6d1e\uff0c\u5e76\u63d0\u51fa\u4e86ART\u57fa\u51c6\u4ee5\u8bc4\u4f30\u5b89\u5168\u6027\u3002", "motivation": "\u63a2\u8ba8AI\u4ee3\u7406\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u7684\u5b89\u5168\u6027\uff0c\u5c24\u5176\u662f\u5728\u653b\u51fb\u4e0b\u662f\u5426\u9075\u5faa\u653f\u7b56\u3002", "method": "\u901a\u8fc7\u7ea2\u961f\u7ade\u8d5b\u6536\u96c6180\u4e07\u6b21\u63d0\u793a\u6ce8\u5165\u653b\u51fb\uff0c\u6784\u5efaART\u57fa\u51c6\uff0c\u5e76\u8bc4\u4f3019\u79cd\u6700\u5148\u8fdb\u6a21\u578b\u3002", "result": "\u51e0\u4e4e\u6240\u6709\u4ee3\u7406\u572810-100\u6b21\u67e5\u8be2\u5185\u51fa\u73b0\u653f\u7b56\u8fdd\u89c4\uff0c\u653b\u51fb\u5728\u4e0d\u540c\u6a21\u578b\u548c\u4efb\u52a1\u95f4\u9ad8\u5ea6\u53ef\u8f6c\u79fb\u3002", "conclusion": "AI\u4ee3\u7406\u5b58\u5728\u4e25\u91cd\u6f0f\u6d1e\uff0c\u9700\u989d\u5916\u9632\u5fa1\u63aa\u65bd\u3002ART\u57fa\u51c6\u65e8\u5728\u63a8\u52a8\u66f4\u4e25\u683c\u7684\u5b89\u5168\u8bc4\u4f30\u548c\u66f4\u5b89\u5168\u7684\u4ee3\u7406\u90e8\u7f72\u3002"}}
{"id": "2507.20784", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.20784", "abs": "https://arxiv.org/abs/2507.20784", "authors": ["Mohamed Sorour", "Mohamed Heshmat", "Khaled Elgeneidy", "P\u00e5l Johan From"], "title": "A Strawberry Harvesting Tool with Minimal Footprint", "comment": null, "summary": "In this paper, a novel prototype for harvesting table-top grown strawberries\nis presented, that is minimalist in its footprint interacting with the fruit.\nIn our methodology, a smooth trapper manipulates the stem into a precise groove\nlocation at which a distant laser beam is focused. The tool reaches\ntemperatures as high as 188{\\deg} Celsius and as such killing germs and\npreventing the spread of local plant diseases. The burnt stem wound preserves\nwater content and in turn the fruit shelf life. Cycle and cut times achieved\nare 5.56 and 2.88 seconds respectively in successful in-door harvesting\ndemonstration. Extensive experiments are performed to optimize the laser spot\ndiameter and lateral speed against the cutting time.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u8349\u8393\u91c7\u6458\u539f\u578b\uff0c\u901a\u8fc7\u6fc0\u5149\u5207\u5272\u830e\u90e8\uff0c\u51cf\u5c11\u63a5\u89e6\u5e76\u5ef6\u957f\u679c\u5b9e\u4fdd\u8d28\u671f\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u8349\u8393\u91c7\u6458\u4e2d\u63a5\u89e6\u6c61\u67d3\u548c\u4fdd\u8d28\u671f\u77ed\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u5e73\u6ed1\u5939\u5177\u5c06\u830e\u90e8\u5f15\u5bfc\u81f3\u7cbe\u786e\u4f4d\u7f6e\uff0c\u901a\u8fc7\u8fdc\u8ddd\u79bb\u6fc0\u5149\u5207\u5272\u5e76\u9ad8\u6e29\u6d88\u6bd2\u3002", "result": "\u6210\u529f\u5b9e\u73b0\u5ba4\u5185\u91c7\u6458\uff0c\u5207\u5272\u65f6\u95f4\u4e3a2.88\u79d2\uff0c\u5faa\u73af\u65f6\u95f4\u4e3a5.56\u79d2\u3002", "conclusion": "\u8be5\u539f\u578b\u9ad8\u6548\u4e14\u536b\u751f\uff0c\u663e\u8457\u63d0\u5347\u8349\u8393\u91c7\u6458\u8d28\u91cf\u548c\u4fdd\u8d28\u671f\u3002"}}
{"id": "2507.20541", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.20541", "abs": "https://arxiv.org/abs/2507.20541", "authors": ["Zishang Qiu", "Xinan Chen", "Long Chen", "Ruibin Bai"], "title": "MeLA: A Metacognitive LLM-Driven Architecture for Automatic Heuristic Design", "comment": null, "summary": "This paper introduces MeLA, a Metacognitive LLM-Driven Architecture that\npresents a new paradigm for Automatic Heuristic Design (AHD). Traditional\nevolutionary methods operate directly on heuristic code; in contrast, MeLA\nevolves the instructional prompts used to guide a Large Language Model (LLM) in\ngenerating these heuristics. This process of \"prompt evolution\" is driven by a\nnovel metacognitive framework where the system analyzes performance feedback to\nsystematically refine its generative strategy. MeLA's architecture integrates a\nproblem analyzer to construct an initial strategic prompt, an error diagnosis\nsystem to repair faulty code, and a metacognitive search engine that\niteratively optimizes the prompt based on heuristic effectiveness. In\ncomprehensive experiments across both benchmark and real-world problems, MeLA\nconsistently generates more effective and robust heuristics, significantly\noutperforming state-of-the-art methods. Ultimately, this research demonstrates\nthe profound potential of using cognitive science as a blueprint for AI\narchitecture, revealing that by enabling an LLM to metacognitively regulate its\nproblem-solving process, we unlock a more robust and interpretable path to AHD.", "AI": {"tldr": "MeLA\u662f\u4e00\u79cd\u57fa\u4e8e\u5143\u8ba4\u77e5\u7684LLM\u9a71\u52a8\u67b6\u6784\uff0c\u901a\u8fc7\u8fdb\u5316\u63d0\u793a\u800c\u975e\u76f4\u63a5\u64cd\u4f5c\u542f\u53d1\u5f0f\u4ee3\u7801\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u542f\u53d1\u5f0f\u8bbe\u8ba1\u7684\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u8fdb\u5316\u65b9\u6cd5\u76f4\u63a5\u64cd\u4f5c\u542f\u53d1\u5f0f\u4ee3\u7801\uff0c\u800cMeLA\u901a\u8fc7\u8fdb\u5316\u63d0\u793a\u6765\u6307\u5bfcLLM\u751f\u6210\u542f\u53d1\u5f0f\uff0c\u65e8\u5728\u63a2\u7d22\u66f4\u9ad8\u6548\u3001\u9c81\u68d2\u7684\u81ea\u52a8\u542f\u53d1\u5f0f\u8bbe\u8ba1\u65b9\u6cd5\u3002", "method": "MeLA\u91c7\u7528\u5143\u8ba4\u77e5\u6846\u67b6\uff0c\u5305\u62ec\u95ee\u9898\u5206\u6790\u5668\u3001\u9519\u8bef\u8bca\u65ad\u7cfb\u7edf\u548c\u5143\u8ba4\u77e5\u641c\u7d22\u5f15\u64ce\uff0c\u901a\u8fc7\u6027\u80fd\u53cd\u9988\u8fed\u4ee3\u4f18\u5316\u63d0\u793a\u3002", "result": "\u5728\u57fa\u51c6\u548c\u5b9e\u9645\u95ee\u9898\u6d4b\u8bd5\u4e2d\uff0cMeLA\u751f\u6210\u7684\u542f\u53d1\u5f0f\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5c06\u8ba4\u77e5\u79d1\u5b66\u878d\u5165AI\u67b6\u6784\uff0c\u901a\u8fc7\u5143\u8ba4\u77e5\u8c03\u63a7LLM\u7684\u95ee\u9898\u89e3\u51b3\u8fc7\u7a0b\uff0c\u4e3a\u81ea\u52a8\u542f\u53d1\u5f0f\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u548c\u53ef\u89e3\u91ca\u7684\u8def\u5f84\u3002"}}
{"id": "2507.20800", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.20800", "abs": "https://arxiv.org/abs/2507.20800", "authors": ["Vinil Polepalli"], "title": "LanternNet: A Novel Hub-and-Spoke System to Seek and Suppress Spotted Lanternfly Populations", "comment": null, "summary": "The invasive spotted lanternfly (SLF) poses a significant threat to\nagriculture and ecosystems, causing widespread damage. Current control methods,\nsuch as egg scraping, pesticides, and quarantines, prove labor-intensive,\nenvironmentally hazardous, and inadequate for long-term SLF suppression. This\nresearch introduces LanternNet, a novel autonomous robotic Hub-and-Spoke system\ndesigned for scalable detection and suppression of SLF populations. A central,\ntree-mimicking hub utilizes a YOLOv8 computer vision model for precise SLF\nidentification. Three specialized robotic spokes perform targeted tasks: pest\nneutralization, environmental monitoring, and navigation/mapping. Field\ndeployment across multiple infested sites over 5 weeks demonstrated\nLanternNet's efficacy. Quantitative analysis revealed significant reductions (p\n< 0.01, paired t-tests) in SLF populations and corresponding improvements in\ntree health indicators across the majority of test sites. Compared to\nconventional methods, LanternNet offers substantial cost advantages and\nimproved scalability. Furthermore, the system's adaptability for enhanced\nautonomy and targeting of other invasive species presents significant potential\nfor broader ecological impact. LanternNet demonstrates the transformative\npotential of integrating robotics and AI for advanced invasive species\nmanagement and improved environmental outcomes.", "AI": {"tldr": "LanternNet\u662f\u4e00\u79cd\u65b0\u578b\u81ea\u4e3b\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u7528\u4e8e\u68c0\u6d4b\u548c\u6291\u5236\u6591\u70b9\u706f\u7b3c\u8747\uff08SLF\uff09\uff0c\u663e\u8457\u51cf\u5c11\u866b\u5bb3\u5e76\u6539\u5584\u6811\u6728\u5065\u5eb7\uff0c\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u5177\u6210\u672c\u6548\u76ca\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u6591\u70b9\u706f\u7b3c\u8747\u5bf9\u519c\u4e1a\u548c\u751f\u6001\u7cfb\u7edf\u6784\u6210\u91cd\u5927\u5a01\u80c1\uff0c\u73b0\u6709\u63a7\u5236\u65b9\u6cd5\u6548\u7387\u4f4e\u4e14\u5bf9\u73af\u5883\u6709\u5bb3\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "LanternNet\u91c7\u7528\u4e2d\u5fc3-\u8f90\u5c04\u5f0f\u7cfb\u7edf\u8bbe\u8ba1\uff0c\u4e2d\u5fc3\u4f7f\u7528YOLOv8\u6a21\u578b\u8bc6\u522bSLF\uff0c\u4e09\u4e2a\u673a\u5668\u4eba\u8f90\u5c04\u81c2\u5206\u522b\u8d1f\u8d23\u706d\u866b\u3001\u73af\u5883\u76d1\u6d4b\u548c\u5bfc\u822a/\u7ed8\u56fe\u3002", "result": "\u5b9e\u5730\u6d4b\u8bd5\u663e\u793a\uff0cLanternNet\u663e\u8457\u51cf\u5c11\u4e86SLF\u79cd\u7fa4\uff08p < 0.01\uff09\uff0c\u5e76\u6539\u5584\u4e86\u6811\u6728\u5065\u5eb7\u6307\u6807\uff0c\u4e14\u6210\u672c\u66f4\u4f4e\u3001\u53ef\u6269\u5c55\u6027\u66f4\u5f3a\u3002", "conclusion": "LanternNet\u5c55\u793a\u4e86\u673a\u5668\u4eba\u4e0eAI\u7ed3\u5408\u5728\u5165\u4fb5\u7269\u79cd\u7ba1\u7406\u4e2d\u7684\u6f5c\u529b\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u751f\u6001\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2507.20566", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.20566", "abs": "https://arxiv.org/abs/2507.20566", "authors": ["Jiajun Liu", "Wenjun Ke", "Peng Wang", "Yao He", "Ziyu Shang", "Guozheng Li", "Zijie Xu", "Ke Ji"], "title": "Unlearning of Knowledge Graph Embedding via Preference Optimization", "comment": null, "summary": "Existing knowledge graphs (KGs) inevitably contain outdated or erroneous\nknowledge that needs to be removed from knowledge graph embedding (KGE) models.\nTo address this challenge, knowledge unlearning can be applied to eliminate\nspecific information while preserving the integrity of the remaining knowledge\nin KGs. Existing unlearning methods can generally be categorized into exact\nunlearning and approximate unlearning. However, exact unlearning requires high\ntraining costs while approximate unlearning faces two issues when applied to\nKGs due to the inherent connectivity of triples: (1) It fails to fully remove\ntargeted information, as forgetting triples can still be inferred from\nremaining ones. (2) It focuses on local data for specific removal, which\nweakens the remaining knowledge in the forgetting boundary. To address these\nissues, we propose GraphDPO, a novel approximate unlearning framework based on\ndirect preference optimization (DPO). Firstly, to effectively remove forgetting\ntriples, we reframe unlearning as a preference optimization problem, where the\nmodel is trained by DPO to prefer reconstructed alternatives over the original\nforgetting triples. This formulation penalizes reliance on forgettable\nknowledge, mitigating incomplete forgetting caused by KG connectivity.\nMoreover, we introduce an out-boundary sampling strategy to construct\npreference pairs with minimal semantic overlap, weakening the connection\nbetween forgetting and retained knowledge. Secondly, to preserve boundary\nknowledge, we introduce a boundary recall mechanism that replays and distills\nrelevant information both within and across time steps. We construct eight\nunlearning datasets across four popular KGs with varying unlearning rates.\nExperiments show that GraphDPO outperforms state-of-the-art baselines by up to\n10.1% in MRR_Avg and 14.0% in MRR_F1.", "AI": {"tldr": "GraphDPO\u662f\u4e00\u79cd\u57fa\u4e8e\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u7684\u77e5\u8bc6\u9057\u5fd8\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u77e5\u8bc6\u56fe\u8c31\u5d4c\u5165\u6a21\u578b\u4e2d\u6709\u6548\u79fb\u9664\u7279\u5b9a\u4fe1\u606f\uff0c\u540c\u65f6\u4fdd\u62a4\u5269\u4f59\u77e5\u8bc6\u7684\u5b8c\u6574\u6027\u3002", "motivation": "\u73b0\u6709\u77e5\u8bc6\u56fe\u8c31\uff08KGs\uff09\u53ef\u80fd\u5305\u542b\u8fc7\u65f6\u6216\u9519\u8bef\u7684\u77e5\u8bc6\uff0c\u9700\u8981\u4ece\u77e5\u8bc6\u56fe\u8c31\u5d4c\u5165\uff08KGE\uff09\u6a21\u578b\u4e2d\u79fb\u9664\u3002\u73b0\u6709\u9057\u5fd8\u65b9\u6cd5\u5b58\u5728\u9ad8\u6210\u672c\u6216\u4fe1\u606f\u79fb\u9664\u4e0d\u5b8c\u5168\u7684\u95ee\u9898\u3002", "method": "GraphDPO\u5c06\u9057\u5fd8\u95ee\u9898\u91cd\u6784\u4e3a\u504f\u597d\u4f18\u5316\u95ee\u9898\uff0c\u901a\u8fc7DPO\u8bad\u7ec3\u6a21\u578b\u504f\u597d\u91cd\u6784\u7684\u66ff\u4ee3\u4e09\u5143\u7ec4\u800c\u975e\u539f\u59cb\u9057\u5fd8\u4e09\u5143\u7ec4\uff0c\u5e76\u5f15\u5165\u8fb9\u754c\u5916\u91c7\u6837\u7b56\u7565\u548c\u8fb9\u754c\u56de\u5fc6\u673a\u5236\u3002", "result": "\u5728\u56db\u4e2a\u6d41\u884c\u77e5\u8bc6\u56fe\u8c31\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGraphDPO\u5728MRR_Avg\u548cMRR_F1\u4e0a\u5206\u522b\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u5347\u4e8610.1%\u548c14.0%\u3002", "conclusion": "GraphDPO\u901a\u8fc7\u504f\u597d\u4f18\u5316\u548c\u8fb9\u754c\u4fdd\u62a4\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u77e5\u8bc6\u56fe\u8c31\u4e2d\u4fe1\u606f\u79fb\u9664\u4e0d\u5b8c\u5168\u548c\u5269\u4f59\u77e5\u8bc6\u53d7\u635f\u7684\u95ee\u9898\u3002"}}
{"id": "2507.20832", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.20832", "abs": "https://arxiv.org/abs/2507.20832", "authors": ["Mihai Pomarlan", "Stefano De Giorgis", "Rachel Ringe", "Maria M. Hedblom", "Nikolaos Tsiogkas"], "title": "Hanging Around: Cognitive Inspired Reasoning for Reactive Robotics", "comment": "This article is published online with Open Access by IOS Press and\n  distributed under the terms of the Creative Commons Attribution\n  Non-Commercial License 4.0 (CC BY-NC 4.0)", "summary": "Situationally-aware artificial agents operating with competence in natural\nenvironments face several challenges: spatial awareness, object affordance\ndetection, dynamic changes and unpredictability. A critical challenge is the\nagent's ability to identify and monitor environmental elements pertinent to its\nobjectives. Our research introduces a neurosymbolic modular architecture for\nreactive robotics. Our system combines a neural component performing object\nrecognition over the environment and image processing techniques such as\noptical flow, with symbolic representation and reasoning. The reasoning system\nis grounded in the embodied cognition paradigm, via integrating image schematic\nknowledge in an ontological structure. The ontology is operatively used to\ncreate queries for the perception system, decide on actions, and infer\nentities' capabilities derived from perceptual data. The combination of\nreasoning and image processing allows the agent to focus its perception for\nnormal operation as well as discover new concepts for parts of objects involved\nin particular interactions. The discovered concepts allow the robot to\nautonomously acquire training data and adjust its subsymbolic perception to\nrecognize the parts, as well as making planning for more complex tasks feasible\nby focusing search on those relevant object parts. We demonstrate our approach\nin a simulated world, in which an agent learns to recognize parts of objects\ninvolved in support relations. While the agent has no concept of handle\ninitially, by observing examples of supported objects hanging from a hook it\nlearns to recognize the parts involved in establishing support and becomes able\nto plan the establishment/destruction of the support relation. This underscores\nthe agent's capability to expand its knowledge through observation in a\nsystematic way, and illustrates the potential of combining deep reasoning\n[...].", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u795e\u7ecf\u7b26\u53f7\u6a21\u5757\u5316\u67b6\u6784\uff0c\u7528\u4e8e\u53cd\u5e94\u5f0f\u673a\u5668\u4eba\uff0c\u7ed3\u5408\u795e\u7ecf\u7ec4\u4ef6\u548c\u7b26\u53f7\u63a8\u7406\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u8bc6\u522b\u548c\u76d1\u63a7\u73af\u5883\u4e2d\u7684\u76f8\u5173\u5143\u7d20\uff0c\u5e76\u901a\u8fc7\u89c2\u5bdf\u5b66\u4e60\u65b0\u6982\u5ff5\u3002", "motivation": "\u89e3\u51b3\u4eba\u5de5\u4ee3\u7406\u5728\u81ea\u7136\u73af\u5883\u4e2d\u64cd\u4f5c\u65f6\u9762\u4e34\u7684\u6311\u6218\uff0c\u5982\u7a7a\u95f4\u611f\u77e5\u3001\u5bf9\u8c61\u529f\u80fd\u68c0\u6d4b\u548c\u52a8\u6001\u53d8\u5316\uff0c\u7279\u522b\u662f\u5982\u4f55\u8bc6\u522b\u548c\u76d1\u63a7\u4e0e\u5176\u76ee\u6807\u76f8\u5173\u7684\u73af\u5883\u5143\u7d20\u3002", "method": "\u91c7\u7528\u795e\u7ecf\u7b26\u53f7\u6a21\u5757\u5316\u67b6\u6784\uff0c\u7ed3\u5408\u795e\u7ecf\u7f51\u7edc\u7684\u7269\u4f53\u8bc6\u522b\u548c\u56fe\u50cf\u5904\u7406\u6280\u672f\uff08\u5982\u5149\u6d41\uff09\u4e0e\u7b26\u53f7\u8868\u793a\u548c\u63a8\u7406\u3002\u63a8\u7406\u7cfb\u7edf\u57fa\u4e8e\u5177\u8eab\u8ba4\u77e5\u8303\u5f0f\uff0c\u901a\u8fc7\u672c\u4f53\u7ed3\u6784\u6574\u5408\u56fe\u50cf\u6a21\u5f0f\u77e5\u8bc6\u3002", "result": "\u5728\u6a21\u62df\u4e16\u754c\u4e2d\uff0c\u4ee3\u7406\u901a\u8fc7\u5b66\u4e60\u8bc6\u522b\u6d89\u53ca\u652f\u6491\u5173\u7cfb\u7684\u7269\u4f53\u90e8\u5206\uff0c\u6210\u529f\u6269\u5c55\u5176\u77e5\u8bc6\uff0c\u5e76\u80fd\u591f\u89c4\u5212\u5efa\u7acb\u6216\u7834\u574f\u652f\u6491\u5173\u7cfb\u3002", "conclusion": "\u8be5\u67b6\u6784\u5c55\u793a\u4e86\u901a\u8fc7\u7cfb\u7edf\u89c2\u5bdf\u6269\u5c55\u77e5\u8bc6\u7684\u80fd\u529b\uff0c\u7ed3\u5408\u6df1\u5ea6\u63a8\u7406\u548c\u56fe\u50cf\u5904\u7406\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.20613", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.20613", "abs": "https://arxiv.org/abs/2507.20613", "authors": ["Te Zhang", "Yuheng Li", "Junxiang Wang", "Lujun Li"], "title": "Enhancing Large Multimodal Models with Adaptive Sparsity and KV Cache Compression", "comment": "6 pages", "summary": "Large multimodal models (LMMs) have advanced significantly by integrating\nvisual encoders with extensive language models, enabling robust reasoning\ncapabilities. However, compressing LMMs for deployment on edge devices remains\na critical challenge. In this work, we propose an adaptive search algorithm\nthat optimizes sparsity and KV cache compression to enhance LMM efficiency.\nUtilizing the Tree-structured Parzen Estimator, our method dynamically adjusts\npruning ratios and KV cache quantization bandwidth across different LMM layers,\nusing model performance as the optimization objective. This approach uniquely\ncombines pruning with key-value cache quantization and incorporates a fast\npruning technique that eliminates the need for additional fine-tuning or weight\nadjustments, achieving efficient compression without compromising accuracy.\nComprehensive evaluations on benchmark datasets, including LLaVA-1.5 7B and\n13B, demonstrate our method superiority over state-of-the-art techniques such\nas SparseGPT and Wanda across various compression levels. Notably, our\nframework automatic allocation of KV cache compression resources sets a new\nstandard in LMM optimization, delivering memory efficiency without sacrificing\nmuch performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u641c\u7d22\u7b97\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u7a00\u758f\u6027\u548cKV\u7f13\u5b58\u538b\u7f29\uff0c\u63d0\u5347\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMM\uff09\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u90e8\u7f72\u6548\u7387\u3002", "motivation": "\u5c3d\u7ba1LMM\u5728\u89c6\u89c9\u7f16\u7801\u5668\u548c\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u5176\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u538b\u7f29\u90e8\u7f72\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u91c7\u7528Tree-structured Parzen Estimator\u52a8\u6001\u8c03\u6574\u4e0d\u540c\u5c42\u7684\u526a\u679d\u6bd4\u4f8b\u548cKV\u7f13\u5b58\u91cf\u5316\u5e26\u5bbd\uff0c\u7ed3\u5408\u526a\u679d\u4e0eKV\u7f13\u5b58\u91cf\u5316\uff0c\u65e0\u9700\u989d\u5916\u5fae\u8c03\u3002", "result": "\u5728LLaVA-1.5 7B\u548c13B\u7b49\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8eSparseGPT\u548cWanda\uff0c\u5185\u5b58\u6548\u7387\u63d0\u5347\u4e14\u6027\u80fd\u635f\u5931\u5c0f\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aLMM\u4f18\u5316\u8bbe\u5b9a\u4e86\u65b0\u6807\u51c6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u538b\u7f29\u4e0e\u6027\u80fd\u7684\u5e73\u8861\u3002"}}
{"id": "2507.20850", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.20850", "abs": "https://arxiv.org/abs/2507.20850", "authors": ["Meiting Dang", "Yanping Wu", "Yafei Wang", "Dezong Zhao", "David Flynn", "Chongfeng Wei"], "title": "Free Energy-Inspired Cognitive Risk Integration for AV Navigation in Pedestrian-Rich Environments", "comment": "14 pages, 5 figures", "summary": "Recent advances in autonomous vehicle (AV) behavior planning have shown\nimpressive social interaction capabilities when interacting with other road\nusers. However, achieving human-like prediction and decision-making in\ninteractions with vulnerable road users remains a key challenge in complex\nmulti-agent interactive environments. Existing research focuses primarily on\ncrowd navigation for small mobile robots, which cannot be directly applied to\nAVs due to inherent differences in their decision-making strategies and dynamic\nboundaries. Moreover, pedestrians in these multi-agent simulations follow fixed\nbehavior patterns that cannot dynamically respond to AV actions. To overcome\nthese limitations, this paper proposes a novel framework for modeling\ninteractions between the AV and multiple pedestrians. In this framework, a\ncognitive process modeling approach inspired by the Free Energy Principle is\nintegrated into both the AV and pedestrian models to simulate more realistic\ninteraction dynamics. Specifically, the proposed pedestrian Cognitive-Risk\nSocial Force Model adjusts goal-directed and repulsive forces using a fused\nmeasure of cognitive uncertainty and physical risk to produce human-like\ntrajectories. Meanwhile, the AV leverages this fused risk to construct a\ndynamic, risk-aware adjacency matrix for a Graph Convolutional Network within a\nSoft Actor-Critic architecture, allowing it to make more reasonable and\ninformed decisions. Simulation results indicate that our proposed framework\neffectively improves safety, efficiency, and smoothness of AV navigation\ncompared to the state-of-the-art method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u7531\u80fd\u539f\u7406\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u4e0e\u884c\u4eba\u95f4\u7684\u4ea4\u4e92\u5efa\u6a21\uff0c\u63d0\u5347\u5b89\u5168\u6027\u548c\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u4e0e\u5f31\u52bf\u9053\u8def\u7528\u6237\u4ea4\u4e92\u4e2d\u7684\u4eba\u6027\u5316\u9884\u6d4b\u4e0e\u51b3\u7b56\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u8ba4\u77e5\u8fc7\u7a0b\u5efa\u6a21\u548c\u8ba4\u77e5-\u98ce\u9669\u793e\u4f1a\u529b\u6a21\u578b\uff0c\u5229\u7528\u56fe\u5377\u79ef\u7f51\u7edc\u548cSoft Actor-Critic\u67b6\u6784\u8fdb\u884c\u51b3\u7b56\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6846\u67b6\u5728\u5b89\u5168\u6027\u3001\u6548\u7387\u548c\u6d41\u7545\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u590d\u6742\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u73af\u5883\u63d0\u4f9b\u4e86\u66f4\u771f\u5b9e\u7684\u5efa\u6a21\u548c\u51b3\u7b56\u65b9\u6848\u3002"}}
{"id": "2507.20620", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.20620", "abs": "https://arxiv.org/abs/2507.20620", "authors": ["Lijian Li"], "title": "Complementarity-driven Representation Learning for Multi-modal Knowledge Graph Completion", "comment": null, "summary": "Multi-modal Knowledge Graph Completion (MMKGC) aims to uncover hidden world\nknowledge in multimodal knowledge graphs by leveraging both multimodal and\nstructural entity information. However, the inherent imbalance in multimodal\nknowledge graphs, where modality distributions vary across entities, poses\nchallenges in utilizing additional modality data for robust entity\nrepresentation. Existing MMKGC methods typically rely on attention or\ngate-based fusion mechanisms but overlook complementarity contained in\nmulti-modal data. In this paper, we propose a novel framework named Mixture of\nComplementary Modality Experts (MoCME), which consists of a\nComplementarity-guided Modality Knowledge Fusion (CMKF) module and an\nEntropy-guided Negative Sampling (EGNS) mechanism. The CMKF module exploits\nboth intra-modal and inter-modal complementarity to fuse multi-view and\nmulti-modal embeddings, enhancing representations of entities. Additionally, we\nintroduce an Entropy-guided Negative Sampling mechanism to dynamically\nprioritize informative and uncertain negative samples to enhance training\neffectiveness and model robustness. Extensive experiments on five benchmark\ndatasets demonstrate that our MoCME achieves state-of-the-art performance,\nsurpassing existing approaches.", "AI": {"tldr": "MoCME\u6846\u67b6\u901a\u8fc7\u4e92\u8865\u6027\u6a21\u6001\u77e5\u8bc6\u878d\u5408\u548c\u71b5\u5f15\u5bfc\u8d1f\u91c7\u6837\uff0c\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u77e5\u8bc6\u56fe\u8c31\u4e2d\u6a21\u6001\u5206\u5e03\u4e0d\u5e73\u8861\u7684\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u5b9e\u4f53\u8868\u793a\u548c\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u591a\u6a21\u6001\u77e5\u8bc6\u56fe\u8c31\u4e2d\u6a21\u6001\u5206\u5e03\u4e0d\u5e73\u8861\uff0c\u73b0\u6709\u65b9\u6cd5\u5ffd\u89c6\u591a\u6a21\u6001\u6570\u636e\u7684\u4e92\u8865\u6027\uff0c\u9650\u5236\u4e86\u5b9e\u4f53\u8868\u793a\u7684\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51faMoCME\u6846\u67b6\uff0c\u5305\u542b\u4e92\u8865\u6027\u5f15\u5bfc\u7684\u6a21\u6001\u77e5\u8bc6\u878d\u5408\u6a21\u5757\uff08CMKF\uff09\u548c\u71b5\u5f15\u5bfc\u8d1f\u91c7\u6837\u673a\u5236\uff08EGNS\uff09\uff0c\u4ee5\u878d\u5408\u591a\u6a21\u6001\u6570\u636e\u5e76\u4f18\u5316\u8bad\u7ec3\u3002", "result": "\u5728\u4e94\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMoCME\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "MoCME\u901a\u8fc7\u6709\u6548\u5229\u7528\u591a\u6a21\u6001\u4e92\u8865\u6027\u548c\u52a8\u6001\u8d1f\u91c7\u6837\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u7684\u6027\u80fd\u3002"}}
{"id": "2507.20861", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.20861", "abs": "https://arxiv.org/abs/2507.20861", "authors": ["Marco Faroni", "Carlo Odesco", "Andrea Zanchettin", "Paolo Rocco"], "title": "Uncertainty-aware Planning with Inaccurate Models for Robotized Liquid Handling", "comment": "Accepted at IEEE/RSJ IROS 2025", "summary": "Physics-based simulations and learning-based models are vital for complex\nrobotics tasks like deformable object manipulation and liquid handling.\nHowever, these models often struggle with accuracy due to epistemic uncertainty\nor the sim-to-real gap. For instance, accurately pouring liquid from one\ncontainer to another poses challenges, particularly when models are trained on\nlimited demonstrations and may perform poorly in novel situations. This paper\nproposes an uncertainty-aware Monte Carlo Tree Search (MCTS) algorithm designed\nto mitigate these inaccuracies. By incorporating estimates of model\nuncertainty, the proposed MCTS strategy biases the search towards actions with\nlower predicted uncertainty. This approach enhances the reliability of planning\nunder uncertain conditions. Applied to a liquid pouring task, our method\ndemonstrates improved success rates even with models trained on minimal data,\noutperforming traditional methods and showcasing its potential for robust\ndecision-making in robotics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684MCTS\u7b97\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u590d\u6742\u673a\u5668\u4eba\u4efb\u52a1\uff08\u5982\u6db2\u4f53\u503e\u5012\uff09\u4e2d\u7684\u51b3\u7b56\u53ef\u9760\u6027\u3002", "motivation": "\u7269\u7406\u4eff\u771f\u548c\u5b66\u4e60\u6a21\u578b\u5728\u590d\u6742\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u5b58\u5728\u51c6\u786e\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u9762\u5bf9\u65b0\u60c5\u51b5\u65f6\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u901a\u8fc7\u7ed3\u5408\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u6539\u8fdbMCTS\u7b97\u6cd5\uff0c\u4f7f\u5176\u504f\u5411\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u8f83\u4f4e\u7684\u52a8\u4f5c\u3002", "result": "\u5728\u6db2\u4f53\u503e\u5012\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5373\u4f7f\u57fa\u4e8e\u5c11\u91cf\u8bad\u7ec3\u6570\u636e\u4e5f\u80fd\u63d0\u9ad8\u6210\u529f\u7387\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u673a\u5668\u4eba\u51b3\u7b56\u63d0\u4f9b\u4e86\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\uff0c\u5c24\u5176\u5728\u4e0d\u786e\u5b9a\u6761\u4ef6\u4e0b\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2507.20641", "categories": ["cs.AI", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.20641", "abs": "https://arxiv.org/abs/2507.20641", "authors": ["Lijian Li"], "title": "Adaptive Fuzzy Time Series Forecasting via Partially Asymmetric Convolution and Sub-Sliding Window Fusion", "comment": null, "summary": "At present, state-of-the-art forecasting models are short of the ability to\ncapture spatio-temporal dependency and synthesize global information at the\nstage of learning. To address this issue, in this paper, through the adaptive\nfuzzified construction of temporal data, we propose a novel convolutional\narchitecture with partially asymmetric design based on the scheme of sliding\nwindow to realize accurate time series forecasting. First, the construction\nstrategy of traditional fuzzy time series is improved to further extract short\nand long term temporal interrelation, which enables every time node to\nautomatically possess corresponding global information and inner relationships\namong them in a restricted sliding window and the process does not require\nhuman involvement. Second, a bilateral Atrous algorithm is devised to reduce\ncalculation demand of the proposed model without sacrificing global\ncharacteristics of elements. And it also allows the model to avoid processing\nredundant information. Third, after the transformation of time series, a\npartially asymmetric convolutional architecture is designed to more flexibly\nmine data features by filters in different directions on feature maps, which\ngives the convolutional neural network (CNN) the ability to construct\nsub-windows within existing sliding windows to model at a more fine-grained\nlevel. And after obtaining the time series information at different levels, the\nmulti-scale features from different sub-windows will be sent to the\ncorresponding network layer for time series information fusion. Compared with\nother competitive modern models, the proposed method achieves state-of-the-art\nresults on most of popular time series datasets, which is fully verified by the\nexperimental results.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6ed1\u52a8\u7a97\u53e3\u7684\u90e8\u5206\u4e0d\u5bf9\u79f0\u5377\u79ef\u67b6\u6784\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u6a21\u7cca\u5316\u65f6\u95f4\u6570\u636e\uff0c\u63d0\u5347\u65f6\u7a7a\u4f9d\u8d56\u6355\u6349\u80fd\u529b\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u3002", "motivation": "\u73b0\u6709\u9884\u6d4b\u6a21\u578b\u5728\u65f6\u7a7a\u4f9d\u8d56\u6355\u6349\u548c\u5168\u5c40\u4fe1\u606f\u6574\u5408\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u6539\u8fdb\u4ee5\u63d0\u5347\u9884\u6d4b\u80fd\u529b\u3002", "method": "1. \u6539\u8fdb\u6a21\u7cca\u65f6\u95f4\u5e8f\u5217\u6784\u5efa\u7b56\u7565\uff0c\u81ea\u52a8\u63d0\u53d6\u77ed\u957f\u671f\u65f6\u95f4\u5173\u8054\uff1b2. \u8bbe\u8ba1\u53cc\u8fb9Atrous\u7b97\u6cd5\u964d\u4f4e\u8ba1\u7b97\u9700\u6c42\uff1b3. \u8bbe\u8ba1\u90e8\u5206\u4e0d\u5bf9\u79f0\u5377\u79ef\u67b6\u6784\uff0c\u7075\u6d3b\u6316\u6398\u6570\u636e\u7279\u5f81\u3002", "result": "\u5728\u4e3b\u6d41\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u6700\u4f18\u7ed3\u679c\u3002", "conclusion": "\u6240\u63d0\u65b9\u6cd5\u901a\u8fc7\u81ea\u9002\u5e94\u6a21\u7cca\u5316\u548c\u90e8\u5206\u4e0d\u5bf9\u79f0\u5377\u79ef\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u7cbe\u5ea6\u548c\u6548\u7387\u3002"}}
{"id": "2507.20870", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.20870", "abs": "https://arxiv.org/abs/2507.20870", "authors": ["Elena Merlo", "Marta Lagomarsino", "Arash Ajoudani"], "title": "A Human-in-the-loop Approach to Robot Action Replanning through LLM Common-Sense Reasoning", "comment": null, "summary": "To facilitate the wider adoption of robotics, accessible programming tools\nare required for non-experts. Observational learning enables intuitive human\nskills transfer through hands-on demonstrations, but relying solely on visual\ninput can be inefficient in terms of scalability and failure mitigation,\nespecially when based on a single demonstration. This paper presents a\nhuman-in-the-loop method for enhancing the robot execution plan, automatically\ngenerated based on a single RGB video, with natural language input to a Large\nLanguage Model (LLM). By including user-specified goals or critical task\naspects and exploiting the LLM common-sense reasoning, the system adjusts the\nvision-based plan to prevent potential failures and adapts it based on the\nreceived instructions. Experiments demonstrated the framework intuitiveness and\neffectiveness in correcting vision-derived errors and adapting plans without\nrequiring additional demonstrations. Moreover, interactive plan refinement and\nhallucination corrections promoted system robustness.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u89c6\u89c9\u548c\u81ea\u7136\u8bed\u8a00\u8f93\u5165\u7684\u4eba\u673a\u534f\u4f5c\u65b9\u6cd5\uff0c\u901a\u8fc7LLM\u589e\u5f3a\u673a\u5668\u4eba\u6267\u884c\u8ba1\u5212\uff0c\u63d0\u9ad8\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u4e3a\u975e\u4e13\u5bb6\u63d0\u4f9b\u66f4\u76f4\u89c2\u7684\u673a\u5668\u4eba\u7f16\u7a0b\u5de5\u5177\uff0c\u89e3\u51b3\u5355\u7eaf\u4f9d\u8d56\u89c6\u89c9\u8f93\u5165\u7684\u6269\u5c55\u6027\u548c\u9519\u8bef\u7f13\u89e3\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u5355\u6b21RGB\u89c6\u9891\u751f\u6210\u521d\u59cb\u8ba1\u5212\uff0c\u5229\u7528LLM\u7684\u81ea\u7136\u8bed\u8a00\u8f93\u5165\u8c03\u6574\u8ba1\u5212\uff0c\u9632\u6b62\u6f5c\u5728\u9519\u8bef\u5e76\u9002\u5e94\u65b0\u6307\u4ee4\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u7ea0\u6b63\u89c6\u89c9\u9519\u8bef\u5e76\u9002\u5e94\u8ba1\u5212\uff0c\u65e0\u9700\u989d\u5916\u6f14\u793a\uff0c\u4e14\u4ea4\u4e92\u5f0f\u4f18\u5316\u63d0\u9ad8\u4e86\u7cfb\u7edf\u9c81\u68d2\u6027\u3002", "conclusion": "\u7ed3\u5408\u89c6\u89c9\u548c\u8bed\u8a00\u8f93\u5165\u7684\u65b9\u6cd5\u4e3a\u975e\u4e13\u5bb6\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u673a\u5668\u4eba\u7f16\u7a0b\u5de5\u5177\uff0c\u589e\u5f3a\u4e86\u7cfb\u7edf\u7684\u9002\u5e94\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2507.20703", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.20703", "abs": "https://arxiv.org/abs/2507.20703", "authors": ["Aysu Bogatarkan", "Esra Erdem"], "title": "A General Framework for Dynamic MAPF using Multi-Shot ASP and Tunnels", "comment": null, "summary": "MAPF problem aims to find plans for multiple agents in an environment within\na given time, such that the agents do not collide with each other or obstacles.\nMotivated by the execution and monitoring of these plans, we study Dynamic MAPF\n(D-MAPF) problem, which allows changes such as agents entering/leaving the\nenvironment or obstacles being removed/moved. Considering the requirements of\nreal-world applications in warehouses with the presence of humans, we introduce\n1) a general definition for D-MAPF (applicable to variations of D-MAPF), 2) a\nnew framework to solve D-MAPF (utilizing multi-shot computation, and allowing\ndifferent methods to solve D-MAPF), and 3) a new ASP-based method to solve\nD-MAPF (combining advantages of replanning and repairing methods, with a novel\nconcept of tunnels to specify where agents can move). We have illustrated the\nstrengths and weaknesses of this method by experimental evaluations, from the\nperspectives of computational performance and quality of solutions.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u52a8\u6001\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\uff08D-MAPF\uff09\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u5b9a\u4e49\u3001\u65b0\u6846\u67b6\u548c\u57fa\u4e8eASP\u7684\u89e3\u51b3\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\u3002", "motivation": "\u73b0\u5b9e\u5e94\u7528\u4e2d\uff08\u5982\u4ed3\u5e93\u73af\u5883\uff09\uff0c\u667a\u80fd\u4f53\u7684\u52a8\u6001\u53d8\u5316\uff08\u5982\u8fdb\u51fa\u6216\u969c\u788d\u7269\u79fb\u52a8\uff09\u9700\u8981\u52a8\u6001\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\u3002", "method": "1) \u63d0\u51faD-MAPF\u7684\u901a\u7528\u5b9a\u4e49\uff1b2) \u8bbe\u8ba1\u591a\u9636\u6bb5\u8ba1\u7b97\u6846\u67b6\uff1b3) \u5f00\u53d1\u57fa\u4e8eASP\u7684\u65b0\u65b9\u6cd5\uff0c\u7ed3\u5408\u91cd\u89c4\u5212\u548c\u4fee\u590d\u7b56\u7565\uff0c\u5f15\u5165\u96a7\u9053\u6982\u5ff5\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u5728\u8ba1\u7b97\u6027\u80fd\u548c\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u4e0a\u7684\u4f18\u7f3a\u70b9\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u9002\u7528\u4e8e\u52a8\u6001\u73af\u5883\uff0c\u4e3aD-MAPF\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.20892", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.20892", "abs": "https://arxiv.org/abs/2507.20892", "authors": ["Sergey Bakulin", "Timur Akhtyamov", "Denis Fatykhov", "German Devchich", "Gonzalo Ferrer"], "title": "PixelNav: Towards Model-based Vision-Only Navigation with Topological Graphs", "comment": null, "summary": "This work proposes a novel hybrid approach for vision-only navigation of\nmobile robots, which combines advances of both deep learning approaches and\nclassical model-based planning algorithms. Today, purely data-driven end-to-end\nmodels are dominant solutions to this problem. Despite advantages such as\nflexibility and adaptability, the requirement of a large amount of training\ndata and limited interpretability are the main bottlenecks for their practical\napplications. To address these limitations, we propose a hierarchical system\nthat utilizes recent advances in model predictive control, traversability\nestimation, visual place recognition, and pose estimation, employing\ntopological graphs as a representation of the target environment. Using such a\ncombination, we provide a scalable system with a higher level of\ninterpretability compared to end-to-end approaches. Extensive real-world\nexperiments show the efficiency of the proposed method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u548c\u7ecf\u5178\u6a21\u578b\u89c4\u5212\u7b97\u6cd5\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u7528\u4e8e\u79fb\u52a8\u673a\u5668\u4eba\u7684\u7eaf\u89c6\u89c9\u5bfc\u822a\uff0c\u89e3\u51b3\u4e86\u7aef\u5230\u7aef\u6a21\u578b\u7684\u6570\u636e\u9700\u6c42\u548c\u53ef\u89e3\u91ca\u6027\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u7aef\u5230\u7aef\u6a21\u578b\u867d\u7136\u7075\u6d3b\uff0c\u4f46\u9700\u8981\u5927\u91cf\u8bad\u7ec3\u6570\u636e\u4e14\u53ef\u89e3\u91ca\u6027\u5dee\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u91c7\u7528\u5206\u5c42\u7cfb\u7edf\uff0c\u7ed3\u5408\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u3001\u53ef\u901a\u884c\u6027\u4f30\u8ba1\u3001\u89c6\u89c9\u5730\u70b9\u8bc6\u522b\u548c\u59ff\u6001\u4f30\u8ba1\uff0c\u4f7f\u7528\u62d3\u6251\u56fe\u8868\u793a\u76ee\u6807\u73af\u5883\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u9ad8\u6548\u4e14\u6bd4\u7aef\u5230\u7aef\u6a21\u578b\u66f4\u5177\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u6df7\u5408\u65b9\u6cd5\u5728\u53ef\u6269\u5c55\u6027\u548c\u53ef\u89e3\u91ca\u6027\u4e0a\u4f18\u4e8e\u7eaf\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u3002"}}
{"id": "2507.20711", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.20711", "abs": "https://arxiv.org/abs/2507.20711", "authors": ["Filip Cano", "Thomas A. Henzinger", "Konstantin Kueffner"], "title": "Algorithmic Fairness: A Runtime Perspective", "comment": "To appear in RV 2025", "summary": "Fairness in AI is traditionally studied as a static property evaluated once,\nover a fixed dataset. However, real-world AI systems operate sequentially, with\noutcomes and environments evolving over time. This paper proposes a framework\nfor analysing fairness as a runtime property. Using a minimal yet expressive\nmodel based on sequences of coin tosses with possibly evolving biases, we study\nthe problems of monitoring and enforcing fairness expressed in either toss\noutcomes or coin biases. Since there is no one-size-fits-all solution for\neither problem, we provide a summary of monitoring and enforcement strategies,\nparametrised by environment dynamics, prediction horizon, and confidence\nthresholds. For both problems, we present general results under simple or\nminimal assumptions. We survey existing solutions for the monitoring problem\nfor Markovian and additive dynamics, and existing solutions for the enforcement\nproblem in static settings with known dynamics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u516c\u5e73\u6027\u5206\u6790\u6846\u67b6\uff0c\u5c06\u516c\u5e73\u6027\u89c6\u4e3a\u8fd0\u884c\u65f6\u5c5e\u6027\uff0c\u800c\u975e\u9759\u6001\u5c5e\u6027\u3002\u901a\u8fc7\u57fa\u4e8e\u786c\u5e01\u629b\u63b7\u5e8f\u5217\u7684\u6a21\u578b\uff0c\u7814\u7a76\u4e86\u76d1\u63a7\u548c\u6267\u884c\u516c\u5e73\u6027\u7684\u7b56\u7565\uff0c\u5e76\u603b\u7ed3\u4e86\u4e0d\u540c\u73af\u5883\u52a8\u6001\u4e0b\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u4f20\u7edf\u516c\u5e73\u6027\u7814\u7a76\u5c06\u516c\u5e73\u6027\u89c6\u4e3a\u9759\u6001\u5c5e\u6027\uff0c\u800c\u5b9e\u9645AI\u7cfb\u7edf\u662f\u52a8\u6001\u6f14\u5316\u7684\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u51fa\u52a8\u6001\u516c\u5e73\u6027\u5206\u6790\u6846\u67b6\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u786c\u5e01\u629b\u63b7\u5e8f\u5217\u7684\u6a21\u578b\uff0c\u7814\u7a76\u76d1\u63a7\u548c\u6267\u884c\u516c\u5e73\u6027\u7684\u7b56\u7565\uff0c\u53c2\u6570\u5316\u73af\u5883\u52a8\u6001\u3001\u9884\u6d4b\u8303\u56f4\u548c\u7f6e\u4fe1\u9608\u503c\u3002", "result": "\u603b\u7ed3\u4e86\u4e0d\u540c\u52a8\u6001\u4e0b\u7684\u76d1\u63a7\u548c\u6267\u884c\u7b56\u7565\uff0c\u5e76\u5728\u7b80\u5355\u5047\u8bbe\u4e0b\u63d0\u4f9b\u4e86\u901a\u7528\u7ed3\u679c\u3002", "conclusion": "\u52a8\u6001\u516c\u5e73\u6027\u5206\u6790\u9700\u8981\u9488\u5bf9\u4e0d\u540c\u573a\u666f\u5b9a\u5236\u89e3\u51b3\u65b9\u6848\uff0c\u672c\u6587\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\u6846\u67b6\u3002"}}
{"id": "2507.20728", "categories": ["cs.AI", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.20728", "abs": "https://arxiv.org/abs/2507.20728", "authors": ["Andr\u00e9s Holgado-S\u00e1nchez", "Holger Billhardt", "Sascha Ossowski", "Sara Degli-Esposti"], "title": "Learning the Value Systems of Societies from Preferences", "comment": "Full version of publication under the same accepted at ECAI 2025\n  conference (Submission 6755). 8 pages + 2 supplementary material", "summary": "Aligning AI systems with human values and the value-based preferences of\nvarious stakeholders (their value systems) is key in ethical AI. In value-aware\nAI systems, decision-making draws upon explicit computational representations\nof individual values (groundings) and their aggregation into value systems. As\nthese are notoriously difficult to elicit and calibrate manually, value\nlearning approaches aim to automatically derive computational models of an\nagent's values and value system from demonstrations of human behaviour.\nNonetheless, social science and humanities literature suggest that it is more\nadequate to conceive the value system of a society as a set of value systems of\ndifferent groups, rather than as the simple aggregation of individual value\nsystems. Accordingly, here we formalize the problem of learning the value\nsystems of societies and propose a method to address it based on heuristic deep\nclustering. The method learns socially shared value groundings and a set of\ndiverse value systems representing a given society by observing qualitative\nvalue-based preferences from a sample of agents. We evaluate the proposal in a\nuse case with real data about travelling decisions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u542f\u53d1\u5f0f\u6df1\u5ea6\u805a\u7c7b\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5b66\u4e60\u793e\u4f1a\u7684\u5171\u4eab\u4ef7\u503c\u57fa\u7840\u548c\u591a\u7ec4\u4ef7\u503c\u7cfb\u7edf\uff0c\u4ee5\u66f4\u597d\u5730\u53cd\u6620\u793e\u4f1a\u591a\u6837\u6027\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u624b\u52a8\u6821\u51c6\u548c\u63d0\u53d6\u4e2a\u4f53\u4ef7\u503c\u7cfb\u7edf\uff0c\u4e14\u793e\u4f1a\u4ef7\u503c\u7cfb\u7edf\u5e94\u88ab\u89c6\u4e3a\u591a\u7ec4\u4ef7\u503c\u7cfb\u7edf\u7684\u96c6\u5408\uff0c\u800c\u975e\u7b80\u5355\u805a\u5408\u3002", "method": "\u91c7\u7528\u542f\u53d1\u5f0f\u6df1\u5ea6\u805a\u7c7b\u65b9\u6cd5\uff0c\u901a\u8fc7\u89c2\u5bdf\u4ee3\u7406\u6837\u672c\u7684\u5b9a\u6027\u4ef7\u503c\u504f\u597d\uff0c\u5b66\u4e60\u5171\u4eab\u4ef7\u503c\u57fa\u7840\u548c\u591a\u6837\u5316\u7684\u793e\u4f1a\u4ef7\u503c\u7cfb\u7edf\u3002", "result": "\u5728\u65c5\u884c\u51b3\u7b56\u7684\u5b9e\u9645\u6570\u636e\u7528\u4f8b\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5b66\u4e60\u793e\u4f1a\u591a\u6837\u5316\u7684\u4ef7\u503c\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.20951", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.20951", "abs": "https://arxiv.org/abs/2507.20951", "authors": ["Yang You", "Vincent Thomas", "Alex Schutz", "Robert Skilton", "Nick Hawes", "Olivier Buffet"], "title": "Partially Observable Monte-Carlo Graph Search", "comment": "To be published in Proceedings of ICAPS 2025", "summary": "Currently, large partially observable Markov decision processes (POMDPs) are\noften solved by sampling-based online methods which interleave planning and\nexecution phases. However, a pre-computed offline policy is more desirable in\nPOMDP applications with time or energy constraints. But previous offline\nalgorithms are not able to scale up to large POMDPs. In this article, we\npropose a new sampling-based algorithm, the partially observable Monte-Carlo\ngraph search (POMCGS) to solve large POMDPs offline. Different from many online\nPOMDP methods, which progressively develop a tree while performing\n(Monte-Carlo) simulations, POMCGS folds this search tree on the fly to\nconstruct a policy graph, so that computations can be drastically reduced, and\nusers can analyze and validate the policy prior to embedding and executing it.\nMoreover, POMCGS, together with action progressive widening and observation\nclustering methods provided in this article, is able to address certain\ncontinuous POMDPs. Through experiments, we demonstrate that POMCGS can generate\npolicies on the most challenging POMDPs, which cannot be computed by previous\noffline algorithms, and these policies' values are competitive compared with\nthe state-of-the-art online POMDP algorithms.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u79bb\u7ebf\u7b97\u6cd5POMCGS\uff0c\u7528\u4e8e\u89e3\u51b3\u5927\u89c4\u6a21POMDP\u95ee\u9898\uff0c\u901a\u8fc7\u52a8\u6001\u6298\u53e0\u641c\u7d22\u6811\u6784\u5efa\u7b56\u7565\u56fe\uff0c\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u91cf\uff0c\u5e76\u652f\u6301\u5bf9\u7b56\u7565\u7684\u9884\u5148\u5206\u6790\u548c\u9a8c\u8bc1\u3002", "motivation": "\u5728\u65f6\u95f4\u6216\u80fd\u91cf\u53d7\u9650\u7684POMDP\u5e94\u7528\u4e2d\uff0c\u79bb\u7ebf\u7b56\u7565\u66f4\u4e3a\u7406\u60f3\uff0c\u4f46\u73b0\u6709\u79bb\u7ebf\u7b97\u6cd5\u65e0\u6cd5\u6269\u5c55\u5230\u5927\u89c4\u6a21POMDP\u3002", "method": "POMCGS\u7b97\u6cd5\u52a8\u6001\u6298\u53e0\u641c\u7d22\u6811\u6784\u5efa\u7b56\u7565\u56fe\uff0c\u7ed3\u5408\u52a8\u4f5c\u6e10\u8fdb\u6269\u5c55\u548c\u89c2\u6d4b\u805a\u7c7b\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u8fde\u7eedPOMDP\u3002", "result": "POMCGS\u80fd\u751f\u6210\u73b0\u6709\u79bb\u7ebf\u7b97\u6cd5\u65e0\u6cd5\u5904\u7406\u7684\u590d\u6742POMDP\u7b56\u7565\uff0c\u5176\u6027\u80fd\u4e0e\u6700\u5148\u8fdb\u7684\u5728\u7ebf\u7b97\u6cd5\u76f8\u5f53\u3002", "conclusion": "POMCGS\u4e3a\u5927\u89c4\u6a21POMDP\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u79bb\u7ebf\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.20755", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.20755", "abs": "https://arxiv.org/abs/2507.20755", "authors": ["Arpan Dasgupta", "Sarvesh Gharat", "Neha Madhiwalla", "Aparna Hegde", "Milind Tambe", "Aparna Taneja"], "title": "Beyond Listenership: AI-Predicted Interventions Drive Improvements in Maternal Health Behaviours", "comment": null, "summary": "Automated voice calls with health information are a proven method for\ndisseminating maternal and child health information among beneficiaries and are\ndeployed in several programs around the world. However, these programs often\nsuffer from beneficiary dropoffs and poor engagement. In previous work, through\nreal-world trials, we showed that an AI model, specifically a restless bandit\nmodel, could identify beneficiaries who would benefit most from live service\ncall interventions, preventing dropoffs and boosting engagement. However, one\nkey question has remained open so far: does such improved listenership via\nAI-targeted interventions translate into beneficiaries' improved knowledge and\nhealth behaviors? We present a first study that shows not only listenership\nimprovements due to AI interventions, but also simultaneously links these\nimprovements to health behavior changes. Specifically, we demonstrate that\nAI-scheduled interventions, which enhance listenership, lead to statistically\nsignificant improvements in beneficiaries' health behaviors such as taking iron\nor calcium supplements in the postnatal period, as well as understanding of\ncritical health topics during pregnancy and infancy. This underscores the\npotential of AI to drive meaningful improvements in maternal and child health.", "AI": {"tldr": "AI\u5e72\u9884\u901a\u8fc7\u63d0\u9ad8\u542c\u4f17\u53c2\u4e0e\u5ea6\uff0c\u663e\u8457\u6539\u5584\u4e86\u5b55\u4ea7\u5987\u548c\u513f\u7ae5\u7684\u5065\u5eb7\u884c\u4e3a\u548c\u77e5\u8bc6\u3002", "motivation": "\u89e3\u51b3\u81ea\u52a8\u8bed\u97f3\u547c\u53eb\u9879\u76ee\u4e2d\u53d7\u76ca\u8005\u6d41\u5931\u548c\u53c2\u4e0e\u5ea6\u4f4e\u7684\u95ee\u9898\uff0c\u5e76\u9a8c\u8bc1AI\u5e72\u9884\u662f\u5426\u80fd\u8f6c\u5316\u4e3a\u5065\u5eb7\u884c\u4e3a\u7684\u5b9e\u9645\u6539\u5584\u3002", "method": "\u4f7f\u7528AI\u6a21\u578b\uff08\u5982\u4e0d\u5b89\u5b9a\u8001\u864e\u673a\u6a21\u578b\uff09\u8bc6\u522b\u6700\u9700\u8981\u5e72\u9884\u7684\u53d7\u76ca\u8005\uff0c\u5e76\u8bc4\u4f30\u5176\u5bf9\u5065\u5eb7\u884c\u4e3a\u7684\u5f71\u54cd\u3002", "result": "AI\u5e72\u9884\u663e\u8457\u63d0\u9ad8\u4e86\u542c\u4f17\u53c2\u4e0e\u5ea6\uff0c\u5e76\u6539\u5584\u4e86\u5065\u5eb7\u884c\u4e3a\uff08\u5982\u4ea7\u540e\u8865\u5145\u94c1\u548c\u9499\uff09\u53ca\u5173\u952e\u5065\u5eb7\u77e5\u8bc6\u7684\u7406\u89e3\u3002", "conclusion": "AI\u5728\u5b55\u4ea7\u5987\u548c\u513f\u7ae5\u5065\u5eb7\u9886\u57df\u5177\u6709\u63a8\u52a8\u5b9e\u8d28\u6027\u6539\u5584\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.20758", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.20758", "abs": "https://arxiv.org/abs/2507.20758", "authors": ["Hao Yang", "Qinghua Zhao", "Lei Li"], "title": "How Chain-of-Thought Works? Tracing Information Flow from Decoding, Projection, and Activation", "comment": null, "summary": "Chain-of-Thought (CoT) prompting significantly enhances model reasoning, yet\nits internal mechanisms remain poorly understood. We analyze CoT's operational\nprinciples by reversely tracing information flow across decoding, projection,\nand activation phases. Our quantitative analysis suggests that CoT may serve as\na decoding space pruner, leveraging answer templates to guide output\ngeneration, with higher template adherence strongly correlating with improved\nperformance. Furthermore, we surprisingly find that CoT modulates neuron\nengagement in a task-dependent manner: reducing neuron activation in\nopen-domain tasks, yet increasing it in closed-domain scenarios. These findings\noffer a novel mechanistic interpretability framework and critical insights for\nenabling targeted CoT interventions to design more efficient and robust\nprompts. We released our code and data at\nhttps://anonymous.4open.science/r/cot-D247.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u9006\u5411\u8ffd\u8e2a\u4fe1\u606f\u6d41\uff0c\u63ed\u793a\u4e86Chain-of-Thought (CoT)\u63d0\u793a\u7684\u5185\u90e8\u673a\u5236\uff0c\u53d1\u73b0\u5176\u4f5c\u4e3a\u89e3\u7801\u7a7a\u95f4\u4fee\u526a\u5668\uff0c\u901a\u8fc7\u7b54\u6848\u6a21\u677f\u5f15\u5bfc\u8f93\u51fa\u751f\u6210\uff0c\u4e14\u4efb\u52a1\u4f9d\u8d56\u6027\u5730\u8c03\u8282\u795e\u7ecf\u5143\u6fc0\u6d3b\u3002", "motivation": "\u7406\u89e3CoT\u63d0\u793a\u7684\u5185\u90e8\u673a\u5236\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u63a8\u7406\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u9006\u5411\u8ffd\u8e2a\u4fe1\u606f\u6d41\uff0c\u5206\u6790\u89e3\u7801\u3001\u6295\u5f71\u548c\u6fc0\u6d3b\u9636\u6bb5\uff0c\u5b9a\u91cf\u7814\u7a76CoT\u7684\u4f5c\u7528\u3002", "result": "CoT\u4f5c\u4e3a\u89e3\u7801\u7a7a\u95f4\u4fee\u526a\u5668\uff0c\u4efb\u52a1\u4f9d\u8d56\u6027\u5730\u8c03\u8282\u795e\u7ecf\u5143\u6fc0\u6d3b\uff0c\u5f00\u653e\u57df\u4efb\u52a1\u51cf\u5c11\u6fc0\u6d3b\uff0c\u5c01\u95ed\u57df\u4efb\u52a1\u589e\u52a0\u6fc0\u6d3b\u3002", "conclusion": "\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u673a\u5236\u89e3\u91ca\u6846\u67b6\uff0c\u4e3a\u8bbe\u8ba1\u66f4\u9ad8\u6548\u548c\u9c81\u68d2\u7684\u63d0\u793a\u63d0\u4f9b\u4e86\u5173\u952e\u89c1\u89e3\u3002"}}
{"id": "2507.20774", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.20774", "abs": "https://arxiv.org/abs/2507.20774", "authors": ["Fatou Ndiaye Mbodji"], "title": "evalSmarT: An LLM-Based Framework for Evaluating Smart Contract Generated Comments", "comment": "4 pages, 4 tables", "summary": "Smart contract comment generation has gained traction as a means to improve\ncode comprehension and maintainability in blockchain systems. However,\nevaluating the quality of generated comments remains a challenge. Traditional\nmetrics such as BLEU and ROUGE fail to capture domain-specific nuances, while\nhuman evaluation is costly and unscalable. In this paper, we present\n\\texttt{evalSmarT}, a modular and extensible framework that leverages large\nlanguage models (LLMs) as evaluators. The system supports over 400 evaluator\nconfigurations by combining approximately 40 LLMs with 10 prompting strategies.\nWe demonstrate its application in benchmarking comment generation tools and\nselecting the most informative outputs. Our results show that prompt design\nsignificantly impacts alignment with human judgment, and that LLM-based\nevaluation offers a scalable and semantically rich alternative to existing\nmethods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86evalSmarT\u6846\u67b6\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8bc4\u4f30\u667a\u80fd\u5408\u7ea6\u6ce8\u91ca\u751f\u6210\u8d28\u91cf\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6307\u6807\u548c\u4eba\u5de5\u8bc4\u4f30\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u667a\u80fd\u5408\u7ea6\u6ce8\u91ca\u751f\u6210\u7684\u8d28\u91cf\u8bc4\u4f30\u7f3a\u4e4f\u6709\u6548\u65b9\u6cd5\uff0c\u4f20\u7edf\u6307\u6807\u65e0\u6cd5\u6355\u6349\u9886\u57df\u7279\u6027\uff0c\u4eba\u5de5\u8bc4\u4f30\u6210\u672c\u9ad8\u4e14\u4e0d\u53ef\u6269\u5c55\u3002", "method": "\u63d0\u51faevalSmarT\u6846\u67b6\uff0c\u7ed3\u5408\u7ea640\u79cdLLMs\u548c10\u79cd\u63d0\u793a\u7b56\u7565\uff0c\u652f\u6301400\u591a\u79cd\u8bc4\u4f30\u914d\u7f6e\uff0c\u7528\u4e8e\u8bc4\u6d4b\u6ce8\u91ca\u751f\u6210\u5de5\u5177\u3002", "result": "\u7ed3\u679c\u8868\u660e\u63d0\u793a\u8bbe\u8ba1\u663e\u8457\u5f71\u54cd\u4e0e\u4eba\u7c7b\u5224\u65ad\u7684\u4e00\u81f4\u6027\uff0cLLM\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u8bed\u4e49\u4e30\u5bcc\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "LLM\u4e3a\u57fa\u7840\u7684\u8bc4\u4f30\u662f\u667a\u80fd\u5408\u7ea6\u6ce8\u91ca\u8d28\u91cf\u8bc4\u4f30\u7684\u6709\u6548\u4e14\u53ef\u6269\u5c55\u65b9\u6cd5\u3002"}}
{"id": "2507.20804", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.20804", "abs": "https://arxiv.org/abs/2507.20804", "authors": ["Xueyao Wan", "Hang Yu"], "title": "MMGraphRAG: Bridging Vision and Language with Interpretable Multimodal Knowledge Graphs", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) enhances language model generation by\nretrieving relevant information from external knowledge bases. However,\nconventional RAG methods face the issue of missing multimodal information.\nMultimodal RAG methods address this by fusing images and text through mapping\nthem into a shared embedding space, but they fail to capture the structure of\nknowledge and logical chains between modalities. Moreover, they also require\nlarge-scale training for specific tasks, resulting in limited generalizing\nability. To address these limitations, we propose MMGraphRAG, which refines\nvisual content through scene graphs and constructs a multimodal knowledge graph\n(MMKG) in conjunction with text-based KG. It employs spectral clustering to\nachieve cross-modal entity linking and retrieves context along reasoning paths\nto guide the generative process. Experimental results show that MMGraphRAG\nachieves state-of-the-art performance on the DocBench and MMLongBench datasets,\ndemonstrating strong domain adaptability and clear reasoning paths.", "AI": {"tldr": "MMGraphRAG\u901a\u8fc7\u6784\u5efa\u591a\u6a21\u6001\u77e5\u8bc6\u56fe\u8c31\u548c\u573a\u666f\u56fe\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfRAG\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u4fe1\u606f\u878d\u5408\u548c\u77e5\u8bc6\u7ed3\u6784\u6355\u6349\u4e0a\u7684\u4e0d\u8db3\uff0c\u63d0\u5347\u4e86\u751f\u6210\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u548c\u9002\u5e94\u6027\u3002", "motivation": "\u4f20\u7edfRAG\u65b9\u6cd5\u5728\u591a\u6a21\u6001\u4fe1\u606f\u878d\u5408\u548c\u77e5\u8bc6\u7ed3\u6784\u6355\u6349\u4e0a\u5b58\u5728\u4e0d\u8db3\uff0c\u4e14\u9700\u8981\u5927\u89c4\u6a21\u4efb\u52a1\u7279\u5b9a\u8bad\u7ec3\uff0c\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002", "method": "MMGraphRAG\u901a\u8fc7\u573a\u666f\u56fe\u7ec6\u5316\u89c6\u89c9\u5185\u5bb9\uff0c\u6784\u5efa\u591a\u6a21\u6001\u77e5\u8bc6\u56fe\u8c31\uff08MMKG\uff09\uff0c\u7ed3\u5408\u6587\u672c\u77e5\u8bc6\u56fe\u8c31\uff0c\u5229\u7528\u8c31\u805a\u7c7b\u5b9e\u73b0\u8de8\u6a21\u6001\u5b9e\u4f53\u94fe\u63a5\uff0c\u5e76\u6cbf\u63a8\u7406\u8def\u5f84\u68c0\u7d22\u4e0a\u4e0b\u6587\u4ee5\u6307\u5bfc\u751f\u6210\u3002", "result": "\u5728DocBench\u548cMMLongBench\u6570\u636e\u96c6\u4e0a\uff0cMMGraphRAG\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u9886\u57df\u9002\u5e94\u6027\u548c\u6e05\u6670\u7684\u63a8\u7406\u8def\u5f84\u3002", "conclusion": "MMGraphRAG\u901a\u8fc7\u591a\u6a21\u6001\u77e5\u8bc6\u56fe\u8c31\u548c\u63a8\u7406\u8def\u5f84\u68c0\u7d22\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u6a21\u578b\u7684\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2507.20960", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.20960", "abs": "https://arxiv.org/abs/2507.20960", "authors": ["Bill Cochran"], "title": "On the Limits of Hierarchically Embedded Logic in Classical Neural Networks", "comment": "9 pages", "summary": "We propose a formal model of reasoning limitations in large neural net models\nfor language, grounded in the depth of their neural architecture. By treating\nneural networks as linear operators over logic predicate space we show that\neach layer can encode at most one additional level of logical reasoning. We\nprove that a neural network of depth a particular depth cannot faithfully\nrepresent predicates in a one higher order logic, such as simple counting over\ncomplex predicates, implying a strict upper bound on logical expressiveness.\nThis structure induces a nontrivial null space during tokenization and\nembedding, excluding higher-order predicates from representability. Our\nframework offers a natural explanation for phenomena such as hallucination,\nrepetition, and limited planning, while also providing a foundation for\nunderstanding how approximations to higher-order logic may emerge. These\nresults motivate architectural extensions and interpretability strategies in\nfuture development of language models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u6df1\u5ea6\u7684\u5f62\u5f0f\u5316\u6a21\u578b\uff0c\u7528\u4e8e\u5206\u6790\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u9650\u5236\uff0c\u8bc1\u660e\u5176\u903b\u8f91\u8868\u8fbe\u80fd\u529b\u5b58\u5728\u4e25\u683c\u4e0a\u9650\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u89e3\u91ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u903b\u8f91\u63a8\u7406\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u5982\u5e7b\u89c9\u3001\u91cd\u590d\u548c\u6709\u9650\u89c4\u5212\u80fd\u529b\uff0c\u5e76\u4e3a\u672a\u6765\u6a21\u578b\u8bbe\u8ba1\u548c\u53ef\u89e3\u91ca\u6027\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u3002", "method": "\u5c06\u795e\u7ecf\u7f51\u7edc\u89c6\u4e3a\u903b\u8f91\u8c13\u8bcd\u7a7a\u95f4\u4e0a\u7684\u7ebf\u6027\u7b97\u5b50\uff0c\u5206\u6790\u6bcf\u5c42\u7f16\u7801\u7684\u903b\u8f91\u63a8\u7406\u5c42\u6b21\uff0c\u8bc1\u660e\u7f51\u7edc\u6df1\u5ea6\u5bf9\u9ad8\u9636\u903b\u8f91\u8868\u8fbe\u7684\u9650\u5236\u3002", "result": "\u53d1\u73b0\u795e\u7ecf\u7f51\u7edc\u65e0\u6cd5\u5fe0\u5b9e\u8868\u793a\u9ad8\u4e00\u9636\u903b\u8f91\uff08\u5982\u590d\u6742\u8c13\u8bcd\u8ba1\u6570\uff09\uff0c\u5e76\u63ed\u793a\u4e86\u5d4c\u5165\u8fc7\u7a0b\u4e2d\u7684\u975e\u5e73\u51e1\u96f6\u7a7a\u95f4\u3002", "conclusion": "\u7814\u7a76\u4e3a\u7406\u89e3\u8bed\u8a00\u6a21\u578b\u7684\u903b\u8f91\u9650\u5236\u63d0\u4f9b\u4e86\u6846\u67b6\uff0c\u5e76\u5efa\u8bae\u672a\u6765\u901a\u8fc7\u67b6\u6784\u6269\u5c55\u548c\u53ef\u89e3\u91ca\u6027\u7b56\u7565\u63d0\u5347\u6a21\u578b\u80fd\u529b\u3002"}}
{"id": "2507.20964", "categories": ["cs.AI", "cs.CC", "cs.GT", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.20964", "abs": "https://arxiv.org/abs/2507.20964", "authors": ["Aran Nayebi"], "title": "Core Safety Values for Provably Corrigible Agents", "comment": "14 pages", "summary": "We introduce the first implementable framework for corrigibility, with\nprovable guarantees in multi-step, partially observed environments. Our\nframework replaces a single opaque reward with five *structurally separate*\nutility heads -- deference, switch-access preservation, truthfulness,\nlow-impact behavior via a belief-based extension of Attainable Utility\nPreservation, and bounded task reward -- combined lexicographically by strict\nweight gaps. Theorem 1 proves exact single-round corrigibility in the partially\nobservable off-switch game; Theorem 3 extends the guarantee to multi-step,\nself-spawning agents, showing that even if each head is \\emph{learned} to\nmean-squared error $\\varepsilon$ and the planner is $\\varepsilon$-sub-optimal,\nthe probability of violating \\emph{any} safety property is bounded while still\nensuring net human benefit. In contrast to Constitutional AI or RLHF/RLAIF,\nwhich merge all norms into one learned scalar, our separation makes obedience\nand impact-limits dominate even when incentives conflict. For open-ended\nsettings where adversaries can modify the agent, we prove that deciding whether\nan arbitrary post-hack agent will ever violate corrigibility is undecidable by\nreduction to the halting problem, then carve out a finite-horizon ``decidable\nisland'' where safety can be certified in randomized polynomial time and\nverified with privacy-preserving, constant-round zero-knowledge proofs.\nConsequently, the remaining challenge is the ordinary ML task of data coverage\nand generalization: reward-hacking risk is pushed into evaluation quality\nrather than hidden incentive leak-through, giving clearer implementation\nguidance for today's LLM assistants and future autonomous systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u53ef\u5b9e\u73b0\u7684\u4fee\u6b63\u6027\u6846\u67b6\uff0c\u5728\u591a\u6b65\u3001\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e2d\u63d0\u4f9b\u53ef\u8bc1\u660e\u7684\u4fdd\u8bc1\uff0c\u901a\u8fc7\u4e94\u4e2a\u7ed3\u6784\u5206\u79bb\u7684\u6548\u7528\u5934\u5b9e\u73b0\u5b89\u5168\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\uff08\u5982Constitutional AI\u6216RLHF/RLAIF\uff09\u5c06\u6240\u6709\u89c4\u8303\u5408\u5e76\u4e3a\u4e00\u4e2a\u6807\u91cf\u7684\u95ee\u9898\uff0c\u786e\u4fdd\u4fee\u6b63\u6027\u5728\u591a\u6b65\u73af\u5883\u4e2d\u4ecd\u80fd\u4fdd\u6301\u3002", "method": "\u4f7f\u7528\u4e94\u4e2a\u6548\u7528\u5934\uff08\u987a\u4ece\u6027\u3001\u5f00\u5173\u8bbf\u95ee\u4fdd\u62a4\u3001\u771f\u5b9e\u6027\u3001\u4f4e\u5f71\u54cd\u884c\u4e3a\u548c\u6709\u9650\u4efb\u52a1\u5956\u52b1\uff09\u6309\u4e25\u683c\u6743\u91cd\u5dee\u8ddd\u7ec4\u5408\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u8bc1\u660e\u5176\u6709\u6548\u6027\u3002", "result": "\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u7684\u5f00\u5173\u6e38\u620f\u4e2d\u8bc1\u660e\u4e86\u5355\u8f6e\u4fee\u6b63\u6027\uff0c\u5e76\u6269\u5c55\u5230\u591a\u6b65\u81ea\u751f\u6210\u4ee3\u7406\u4e2d\uff0c\u5b89\u5168\u6027\u8fdd\u89c4\u6982\u7387\u6709\u754c\u4e14\u786e\u4fdd\u4eba\u7c7b\u51c0\u6536\u76ca\u3002", "conclusion": "\u6846\u67b6\u5c06\u5956\u52b1\u9ed1\u5ba2\u98ce\u9669\u8f6c\u79fb\u5230\u8bc4\u4f30\u8d28\u91cf\u4e2d\uff0c\u4e3a\u5f53\u524dLLM\u52a9\u624b\u548c\u672a\u6765\u81ea\u4e3b\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u6e05\u6670\u7684\u5b9e\u73b0\u6307\u5bfc\u3002"}}
{"id": "2507.21017", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.21017", "abs": "https://arxiv.org/abs/2507.21017", "authors": ["Weichen Zhang", "Yiyou Sun", "Pohao Huang", "Jiayue Pu", "Heyue Lin", "Dawn Song"], "title": "MIRAGE-Bench: LLM Agent is Hallucinating and Where to Find Them", "comment": "Code and data: https://github.com/sunblaze-ucb/mirage-bench.git", "summary": "Hallucinations pose critical risks for large language model (LLM)-based\nagents, often manifesting as hallucinative actions resulting from fabricated or\nmisinterpreted information within the cognitive context. While recent studies\nhave exposed such failures, existing evaluations remain fragmented and lack a\nprincipled testbed. In this paper, we present MIRAGE-Bench--Measuring Illusions\nin Risky AGEnt settings--the first unified benchmark for eliciting and\nevaluating hallucinations in interactive LLM-agent scenarios. We begin by\nintroducing a three-part taxonomy to address agentic hallucinations: actions\nthat are unfaithful to (i) task instructions, (ii) execution history, or (iii)\nenvironment observations. To analyze, we first elicit such failures by\nperforming a systematic audit of existing agent benchmarks, then synthesize\ntest cases using a snapshot strategy that isolates decision points in\ndeterministic and reproducible manners. To evaluate hallucination behaviors, we\nadopt a fine-grained-level LLM-as-a-Judge paradigm with tailored risk-aware\nprompts, enabling scalable, high-fidelity assessment of agent actions without\nenumerating full action spaces. MIRAGE-Bench provides actionable insights on\nfailure modes of LLM agents and lays the groundwork for principled progress in\nmitigating hallucinations in interactive environments.", "AI": {"tldr": "MIRAGE-Bench\u662f\u9996\u4e2a\u7528\u4e8e\u8bc4\u4f30\u4ea4\u4e92\u5f0fLLM\u4ee3\u7406\u5e7b\u89c9\u7684\u7edf\u4e00\u57fa\u51c6\uff0c\u901a\u8fc7\u4e09\u90e8\u5206\u5206\u7c7b\u6cd5\u548c\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u65b9\u6cd5\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u8bc4\u4f30\u7684\u788e\u7247\u5316\u95ee\u9898\uff0c\u4e3aLLM\u4ee3\u7406\u7684\u5e7b\u89c9\u884c\u4e3a\u63d0\u4f9b\u7cfb\u7edf\u6027\u6d4b\u8bd5\u6846\u67b6\u3002", "method": "\u5f15\u5165\u4e09\u90e8\u5206\u5206\u7c7b\u6cd5\uff0c\u901a\u8fc7\u7cfb\u7edf\u5ba1\u8ba1\u548c\u5feb\u7167\u7b56\u7565\u5408\u6210\u6d4b\u8bd5\u7528\u4f8b\uff0c\u91c7\u7528LLM-as-a-Judge\u8303\u5f0f\u8bc4\u4f30\u3002", "result": "MIRAGE-Bench\u80fd\u591f\u9ad8\u6548\u8bc4\u4f30\u4ee3\u7406\u884c\u4e3a\uff0c\u63ed\u793a\u5931\u8d25\u6a21\u5f0f\u3002", "conclusion": "\u4e3a\u4ea4\u4e92\u73af\u5883\u4e2d\u5e7b\u89c9\u7684\u7f13\u89e3\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u8df5\u6307\u5bfc\u3002"}}
{"id": "2507.21027", "categories": ["cs.AI", "cs.SE", "D.1.6; I.2.1"], "pdf": "https://arxiv.org/pdf/2507.21027", "abs": "https://arxiv.org/abs/2507.21027", "authors": ["Lucia Bal\u00e1\u017eov\u00e1", "Richard Comploi-Taupe", "Susana Hahn", "Nicolas R\u00fchling", "Gottfried Schenner"], "title": "Smart Expansion Techniques for ASP-based Interactive Configuration", "comment": "Under consideration for publication in Theory and Practice of Logic\n  Programming (TPLP)", "summary": "Product configuration is a successful application of Answer Set Programming\n(ASP). However, challenges are still open for interactive systems to\neffectively guide users through the configuration process. The aim of our work\nis to provide an ASP-based solver for interactive configuration that can deal\nwith large-scale industrial configuration problems and that supports intuitive\nuser interfaces via an API. In this paper, we focus on improving the\nperformance of automatically completing a partial configuration. Our main\ncontribution enhances the classical incremental approach for multi-shot solving\nby four different smart expansion functions. The core idea is to determine and\nadd specific objects or associations to the partial configuration by exploiting\ncautious and brave consequences before checking for the existence of a complete\nconfiguration with the current objects in each iteration. This approach limits\nthe number of costly unsatisfiability checks and reduces the search space,\nthereby improving solving performance. In addition, we present a user interface\nthat uses our API and is implemented in ASP.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eASP\u7684\u4ea4\u4e92\u5f0f\u914d\u7f6e\u6c42\u89e3\u5668\uff0c\u901a\u8fc7\u56db\u79cd\u667a\u80fd\u6269\u5c55\u51fd\u6570\u4f18\u5316\u90e8\u5206\u914d\u7f6e\u7684\u81ea\u52a8\u5b8c\u6210\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5927\u89c4\u6a21\u5de5\u4e1a\u914d\u7f6e\u95ee\u9898\u4e2d\u4ea4\u4e92\u5f0f\u7cfb\u7edf\u7684\u6027\u80fd\u6311\u6218\uff0c\u652f\u6301\u76f4\u89c2\u7684\u7528\u6237\u754c\u9762\u3002", "method": "\u91c7\u7528\u589e\u91cf\u5f0f\u591a\u8f6e\u6c42\u89e3\u65b9\u6cd5\uff0c\u7ed3\u5408\u8c28\u614e\u548c\u52c7\u6562\u7684\u63a8\u7406\u7ed3\u679c\uff0c\u51cf\u5c11\u6602\u8d35\u7684\u4e0d\u53ef\u6ee1\u8db3\u6027\u68c0\u67e5\u3002", "result": "\u901a\u8fc7\u9650\u5236\u641c\u7d22\u7a7a\u95f4\u548c\u51cf\u5c11\u4e0d\u53ef\u6ee1\u8db3\u6027\u68c0\u67e5\u6b21\u6570\uff0c\u663e\u8457\u63d0\u5347\u6c42\u89e3\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u63d0\u9ad8\u4e86\u4ea4\u4e92\u5f0f\u914d\u7f6e\u7684\u6548\u7387\uff0c\u5e76\u901a\u8fc7ASP\u5b9e\u73b0\u7684\u7528\u6237\u754c\u9762\u5c55\u793a\u4e86\u5176\u5b9e\u7528\u6027\u3002"}}
{"id": "2507.21035", "categories": ["cs.AI", "cs.LG", "cs.MA", "q-bio.GN"], "pdf": "https://arxiv.org/pdf/2507.21035", "abs": "https://arxiv.org/abs/2507.21035", "authors": ["Haoyang Liu", "Yijiang Li", "Haohan Wang"], "title": "GenoMAS: A Multi-Agent Framework for Scientific Discovery via Code-Driven Gene Expression Analysis", "comment": "51 pages, 5 figures", "summary": "Gene expression analysis holds the key to many biomedical discoveries, yet\nextracting insights from raw transcriptomic data remains formidable due to the\ncomplexity of multiple large, semi-structured files and the need for extensive\ndomain expertise. Current automation approaches are often limited by either\ninflexible workflows that break down in edge cases or by fully autonomous\nagents that lack the necessary precision for rigorous scientific inquiry.\nGenoMAS charts a different course by presenting a team of LLM-based scientists\nthat integrates the reliability of structured workflows with the adaptability\nof autonomous agents. GenoMAS orchestrates six specialized LLM agents through\ntyped message-passing protocols, each contributing complementary strengths to a\nshared analytic canvas. At the heart of GenoMAS lies a guided-planning\nframework: programming agents unfold high-level task guidelines into Action\nUnits and, at each juncture, elect to advance, revise, bypass, or backtrack,\nthereby maintaining logical coherence while bending gracefully to the\nidiosyncrasies of genomic data.\n  On the GenoTEX benchmark, GenoMAS reaches a Composite Similarity Correlation\nof 89.13% for data preprocessing and an F$_1$ of 60.48% for gene\nidentification, surpassing the best prior art by 10.61% and 16.85%\nrespectively. Beyond metrics, GenoMAS surfaces biologically plausible\ngene-phenotype associations corroborated by the literature, all while adjusting\nfor latent confounders. Code is available at https://github.com/Liu-Hy/GenoMAS.", "AI": {"tldr": "GenoMAS\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u56e2\u961f\u534f\u4f5c\u7cfb\u7edf\uff0c\u7ed3\u5408\u7ed3\u6784\u5316\u5de5\u4f5c\u6d41\u548c\u81ea\u4e3b\u4ee3\u7406\u7684\u7075\u6d3b\u6027\uff0c\u7528\u4e8e\u57fa\u56e0\u8868\u8fbe\u5206\u6790\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6570\u636e\u9884\u5904\u7406\u548c\u57fa\u56e0\u8bc6\u522b\u7684\u6027\u80fd\u3002", "motivation": "\u57fa\u56e0\u8868\u8fbe\u5206\u6790\u590d\u6742\u4e14\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\uff0c\u73b0\u6709\u81ea\u52a8\u5316\u65b9\u6cd5\u5728\u7075\u6d3b\u6027\u548c\u7cbe\u786e\u6027\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002GenoMAS\u65e8\u5728\u7ed3\u5408\u53ef\u9760\u6027\u548c\u9002\u5e94\u6027\u3002", "method": "GenoMAS\u901a\u8fc7\u516d\u4e2a\u4e13\u4e1aLLM\u4ee3\u7406\u534f\u4f5c\uff0c\u91c7\u7528\u7c7b\u578b\u5316\u6d88\u606f\u4f20\u9012\u534f\u8bae\u548c\u5f15\u5bfc\u89c4\u5212\u6846\u67b6\uff0c\u52a8\u6001\u8c03\u6574\u4efb\u52a1\u6267\u884c\u3002", "result": "\u5728GenoTEX\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6570\u636e\u9884\u5904\u7406\u548c\u57fa\u56e0\u8bc6\u522b\u7684\u6027\u80fd\u5206\u522b\u63d0\u534710.61%\u548c16.85%\uff0c\u5e76\u53d1\u73b0\u751f\u7269\u5b66\u53ef\u4fe1\u7684\u57fa\u56e0-\u8868\u578b\u5173\u8054\u3002", "conclusion": "GenoMAS\u5728\u57fa\u56e0\u8868\u8fbe\u5206\u6790\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u7ed3\u5408\u4e86\u5de5\u4f5c\u6d41\u7684\u53ef\u9760\u6027\u548c\u4ee3\u7406\u7684\u7075\u6d3b\u6027\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.21046", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.21046", "abs": "https://arxiv.org/abs/2507.21046", "authors": ["Huan-ang Gao", "Jiayi Geng", "Wenyue Hua", "Mengkang Hu", "Xinzhe Juan", "Hongzhang Liu", "Shilong Liu", "Jiahao Qiu", "Xuan Qi", "Yiran Wu", "Hongru Wang", "Han Xiao", "Yuhang Zhou", "Shaokun Zhang", "Jiayi Zhang", "Jinyu Xiang", "Yixiong Fang", "Qiwen Zhao", "Dongrui Liu", "Qihan Ren", "Cheng Qian", "Zhenghailong Wang", "Minda Hu", "Huazheng Wang", "Qingyun Wu", "Heng Ji", "Mengdi Wang"], "title": "A Survey of Self-Evolving Agents: On Path to Artificial Super Intelligence", "comment": "51 pages, 9 figures", "summary": "Large Language Models (LLMs) have demonstrated strong capabilities but remain\nfundamentally static, unable to adapt their internal parameters to novel tasks,\nevolving knowledge domains, or dynamic interaction contexts. As LLMs are\nincreasingly deployed in open-ended, interactive environments, this static\nnature has become a critical bottleneck, necessitating agents that can\nadaptively reason, act, and evolve in real time. This paradigm shift -- from\nscaling static models to developing self-evolving agents -- has sparked growing\ninterest in architectures and methods enabling continual learning and\nadaptation from data, interactions, and experiences. This survey provides the\nfirst systematic and comprehensive review of self-evolving agents, organized\naround three foundational dimensions -- what to evolve, when to evolve, and how\nto evolve. We examine evolutionary mechanisms across agent components (e.g.,\nmodels, memory, tools, architecture), categorize adaptation methods by stages\n(e.g., intra-test-time, inter-test-time), and analyze the algorithmic and\narchitectural designs that guide evolutionary adaptation (e.g., scalar rewards,\ntextual feedback, single-agent and multi-agent systems). Additionally, we\nanalyze evaluation metrics and benchmarks tailored for self-evolving agents,\nhighlight applications in domains such as coding, education, and healthcare,\nand identify critical challenges and research directions in safety,\nscalability, and co-evolutionary dynamics. By providing a structured framework\nfor understanding and designing self-evolving agents, this survey establishes a\nroadmap for advancing adaptive agentic systems in both research and real-world\ndeployments, ultimately shedding lights to pave the way for the realization of\nArtificial Super Intelligence (ASI), where agents evolve autonomously,\nperforming at or beyond human-level intelligence across a wide array of tasks.", "AI": {"tldr": "\u8bba\u6587\u7efc\u8ff0\u4e86\u81ea\u8fdb\u5316\u667a\u80fd\u4f53\u7684\u7814\u7a76\uff0c\u56f4\u7ed5\u201c\u8fdb\u5316\u4ec0\u4e48\u3001\u4f55\u65f6\u8fdb\u5316\u3001\u5982\u4f55\u8fdb\u5316\u201d\u4e09\u4e2a\u7ef4\u5ea6\uff0c\u5206\u6790\u4e86\u5176\u673a\u5236\u3001\u65b9\u6cd5\u548c\u5e94\u7528\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u9759\u6001\u7279\u6027\u9650\u5236\u4e86\u5176\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u80fd\u591f\u5b9e\u65f6\u8fdb\u5316\u7684\u667a\u80fd\u4f53\u3002", "method": "\u901a\u8fc7\u5206\u7c7b\u8fdb\u5316\u673a\u5236\uff08\u5982\u6a21\u578b\u3001\u8bb0\u5fc6\u3001\u5de5\u5177\uff09\u3001\u9002\u5e94\u65b9\u6cd5\uff08\u5982\u6d4b\u8bd5\u65f6\u5185\u3001\u6d4b\u8bd5\u65f6\u95f4\u5916\uff09\u548c\u8bbe\u8ba1\uff08\u5982\u5956\u52b1\u673a\u5236\u3001\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff09\u6765\u7cfb\u7edf\u5206\u6790\u81ea\u8fdb\u5316\u667a\u80fd\u4f53\u3002", "result": "\u63d0\u51fa\u4e86\u81ea\u8fdb\u5316\u667a\u80fd\u4f53\u7684\u7ed3\u6784\u5316\u6846\u67b6\uff0c\u5e76\u603b\u7ed3\u4e86\u5176\u5728\u7f16\u7a0b\u3001\u6559\u80b2\u3001\u533b\u7597\u7b49\u9886\u57df\u7684\u5e94\u7528\u53ca\u6311\u6218\u3002", "conclusion": "\u81ea\u8fdb\u5316\u667a\u80fd\u4f53\u662f\u5b9e\u73b0\u4eba\u5de5\u8d85\u7ea7\u667a\u80fd\uff08ASI\uff09\u7684\u5173\u952e\uff0c\u672a\u6765\u9700\u89e3\u51b3\u5b89\u5168\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u534f\u540c\u8fdb\u5316\u7b49\u95ee\u9898\u3002"}}
