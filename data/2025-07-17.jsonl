{"id": "2507.11595", "categories": ["cs.AI", "cs.CY", "I.4.8; I.2.6"], "pdf": "https://arxiv.org/pdf/2507.11595", "abs": "https://arxiv.org/abs/2507.11595", "authors": ["Hengyue Zhao"], "title": "A Study on the Application of Artificial Intelligence in Ecological Design", "comment": null, "summary": "This paper asks whether our relationship with nature can move from human\ndominance to genuine interdependence, and whether artificial intelligence (AI)\ncan mediate that shift. We examine a new ecological-design paradigm in which AI\ninteracts with non-human life forms. Through case studies we show how artists\nand designers apply AI for data analysis, image recognition, and ecological\nrestoration, producing results that differ from conventional media. We argue\nthat AI not only expands creative methods but also reframes the theory and\npractice of ecological design. Building on the author's prototype for\nAI-assisted water remediation, the study proposes design pathways that couple\nreinforcement learning with plant-based phytoremediation. The findings\nhighlight AI's potential to link scientific insight, artistic practice, and\nenvironmental stewardship, offering a roadmap for future research on\nsustainable, technology-enabled ecosystems."}
{"id": "2507.11633", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11633", "abs": "https://arxiv.org/abs/2507.11633", "authors": ["Yuxuan Zhang", "Haoyang Yu", "Lanxiang Hu", "Haojian Jin", "Hao Zhang"], "title": "General Modular Harness for LLM Agents in Multi-Turn Gaming Environments", "comment": "8 pages, ICML MAS workshop", "summary": "We introduce a modular harness design for LLM agents that composes of\nperception, memory, and reasoning components, enabling a single LLM or VLM\nbackbone to tackle a wide spectrum of multi turn gaming environments without\ndomain-specific engineering. Using classic and modern game suites as\nlow-barrier, high-diversity testbeds, our framework provides a unified workflow\nfor analyzing how each module affects performance across dynamic interactive\nsettings. Extensive experiments demonstrate that the harness lifts gameplay\nperformance consistently over un-harnessed baselines and reveals distinct\ncontribution patterns, for example, memory dominates in long-horizon puzzles\nwhile perception is critical in vision noisy arcades. These findings highlight\nthe effectiveness of our modular harness design in advancing general-purpose\nagent, given the familiarity and ubiquity of games in everyday human\nexperience."}
{"id": "2507.11662", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11662", "abs": "https://arxiv.org/abs/2507.11662", "authors": ["Moises Andrade", "Joonhyuk Cha", "Brandon Ho", "Vriksha Srihari", "Karmesh Yadav", "Zsolt Kira"], "title": "Let's Think in Two Steps: Mitigating Agreement Bias in MLLMs with Self-Grounded Verification", "comment": "Our code and data are publicly available at\n  https://github.com/mshalimay/mllm-verifiers-abias-sgv", "summary": "Verifiers -- functions assigning rewards to agent behavior -- have been key\nfor AI progress in domains like math and board games. However, extending these\ngains to domains without clear-cut success criteria (e.g.,computer use) remains\na challenge: while humans can recognize suitable outcomes, translating this\nintuition into scalable rules is non-trivial. Multimodal Large Language\nModels(MLLMs) emerge as a promising solution, given their world knowledge,\nhuman-preference alignment, and reasoning skills. We evaluate MLLMs as\nverifiers of agent trajectories across web navigation, computer use, and\nrobotic manipulation, and identify a critical limitation: agreement bias, a\nstrong tendency for MLLMs to favor information in their context window, often\ngenerating chains of thought to rationalize flawed behavior. This bias is\npervasive across models, resilient to test-time scaling, and can impact several\nmethods using MLLMs as evaluators (e.g.,data filtering). Notably, it occurs\ndespite MLLMs showing strong, human-aligned priors on desired behavior. To\naddress this, we propose Self-Grounded Verification (SGV), a lightweight method\nthat enables more effective use of MLLMs' knowledge and reasoning by harnessing\ntheir own sampling mechanisms via unconditional and conditional generation. SGV\noperates in two steps: first, the MLLM is elicited to retrieve broad priors\nabout task completion, independent of the data under evaluation. Then,\nconditioned on self-generated priors, it reasons over and evaluates a candidate\ntrajectory. Enhanced with SGV, MLLM verifiers show gains of up to 20 points in\naccuracy and failure detection rates, and can perform real-time supervision of\nheterogeneous agents, boosting task completion of a GUI specialist in OSWorld,\na diffusion policy in robomimic, and a ReAct agent in VisualWebArena -- setting\na new state of the art on the benchmark, surpassing the previous best by 48%."}
{"id": "2507.11621", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.11621", "abs": "https://arxiv.org/abs/2507.11621", "authors": ["Tianyi Wang", "Yangyang Wang", "Jie Pan", "Junfeng Jiao", "Christian Claudel"], "title": "HCOMC: A Hierarchical Cooperative On-Ramp Merging Control Framework in Mixed Traffic Environment on Two-Lane Highways", "comment": "7 pages, 2 figures, 3 tables, accepted for IEEE International\n  Conference on Intelligent Transportation Systems (ITSC) 2025", "summary": "Highway on-ramp merging areas are common bottlenecks to traffic congestion\nand accidents. Currently, a cooperative control strategy based on connected and\nautomated vehicles (CAVs) is a fundamental solution to this problem. While CAVs\nare not fully widespread, it is necessary to propose a hierarchical cooperative\non-ramp merging control (HCOMC) framework for heterogeneous traffic flow on\ntwo-lane highways to address this gap. This paper extends longitudinal\ncar-following models based on the intelligent driver model and lateral\nlane-changing models using the quintic polynomial curve to account for\nhuman-driven vehicles (HDVs) and CAVs, comprehensively considering human\nfactors and cooperative adaptive cruise control. Besides, this paper proposes a\nHCOMC framework, consisting of a hierarchical cooperative planning model based\non the modified virtual vehicle model, a discretionary lane-changing model\nbased on game theory, and a multi-objective optimization model using the\nelitist non-dominated sorting genetic algorithm to ensure the safe, smooth, and\nefficient merging process. Then, the performance of our HCOMC is analyzed under\ndifferent traffic densities and CAV penetration rates through simulation. The\nfindings underscore our HCOMC's pronounced comprehensive advantages in\nenhancing the safety of group vehicles, stabilizing and expediting merging\nprocess, optimizing traffic efficiency, and economizing fuel consumption\ncompared with benchmarks."}
{"id": "2507.11733", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11733", "abs": "https://arxiv.org/abs/2507.11733", "authors": ["Srikanth Vemula"], "title": "ClarifAI: Enhancing AI Interpretability and Transparency through Case-Based Reasoning and Ontology-Driven Approach for Improved Decision-Making", "comment": null, "summary": "This Study introduces Clarity and Reasoning Interface for Artificial\nIntelligence(ClarifAI), a novel approach designed to augment the transparency\nand interpretability of artificial intelligence (AI) in the realm of improved\ndecision making. Leveraging the Case-Based Reasoning (CBR) methodology and\nintegrating an ontology-driven approach, ClarifAI aims to meet the intricate\nexplanatory demands of various stakeholders involved in AI-powered\napplications. The paper elaborates on ClarifAI's theoretical foundations,\ncombining CBR and ontologies to furnish exhaustive explanation mechanisms. It\nfurther elaborates on the design principles and architectural blueprint,\nhighlighting ClarifAI's potential to enhance AI interpretability across\ndifferent sectors and its applicability in high-stake environments. This\nresearch delineates the significant role of ClariAI in advancing the\ninterpretability of AI systems, paving the way for its deployment in critical\ndecision-making processes."}
{"id": "2507.11623", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.11623", "abs": "https://arxiv.org/abs/2507.11623", "authors": ["Alan Papalia", "Charles Dawson", "Laurentiu L. Anton", "Norhan Magdy Bayomi", "Bianca Champenois", "Jung-Hoon Cho", "Levi Cai", "Joseph DelPreto", "Kristen Edwards", "Bilha-Catherine Githinji", "Cameron Hickert", "Vindula Jayawardana", "Matthew Kramer", "Shreyaa Raghavan", "David Russell", "Shide Salimi", "Jingnan Shi", "Soumya Sudhakar", "Yanwei Wang", "Shouyi Wang", "Luca Carlone", "Vijay Kumar", "Daniela Rus", "John E. Fernandez", "Cathy Wu", "George Kantor", "Derek Young", "Hanumant Singh"], "title": "A Roadmap for Climate-Relevant Robotics Research", "comment": null, "summary": "Climate change is one of the defining challenges of the 21st century, and\nmany in the robotics community are looking for ways to contribute. This paper\npresents a roadmap for climate-relevant robotics research, identifying\nhigh-impact opportunities for collaboration between roboticists and experts\nacross climate domains such as energy, the built environment, transportation,\nindustry, land use, and Earth sciences. These applications include problems\nsuch as energy systems optimization, construction, precision agriculture,\nbuilding envelope retrofits, autonomous trucking, and large-scale environmental\nmonitoring. Critically, we include opportunities to apply not only physical\nrobots but also the broader robotics toolkit - including planning, perception,\ncontrol, and estimation algorithms - to climate-relevant problems. A central\ngoal of this roadmap is to inspire new research directions and collaboration by\nhighlighting specific, actionable problems at the intersection of robotics and\nclimate. This work represents a collaboration between robotics researchers and\ndomain experts in various climate disciplines, and it serves as an invitation\nto the robotics community to bring their expertise to bear on urgent climate\npriorities."}
{"id": "2507.11737", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11737", "abs": "https://arxiv.org/abs/2507.11737", "authors": ["Chenyu Zhou", "Jingyuan Yang", "Linwei Xin", "Yitian Chen", "Ziyan He", "Dongdong Ge"], "title": "Auto-Formulating Dynamic Programming Problems with Large Language Models", "comment": null, "summary": "Dynamic programming (DP) is a fundamental method in operations research, but\nformulating DP models has traditionally required expert knowledge of both the\nproblem context and DP techniques. Large Language Models (LLMs) offer the\npotential to automate this process. However, DP problems pose unique challenges\ndue to their inherently stochastic transitions and the limited availability of\ntraining data. These factors make it difficult to directly apply existing\nLLM-based models or frameworks developed for other optimization problems, such\nas linear or integer programming. We introduce DP-Bench, the first benchmark\ncovering a wide range of textbook-level DP problems to enable systematic\nevaluation. We present Dynamic Programming Language Model (DPLM), a\n7B-parameter specialized model that achieves performance comparable to\nstate-of-the-art LLMs like OpenAI's o1 and DeepSeek-R1, and surpasses them on\nhard problems. Central to DPLM's effectiveness is DualReflect, our novel\nsynthetic data generation pipeline, designed to scale up training data from a\nlimited set of initial examples. DualReflect combines forward generation for\ndiversity and backward generation for reliability. Our results reveal a key\ninsight: backward generation is favored in low-data regimes for its strong\ncorrectness guarantees, while forward generation, though lacking such\nguarantees, becomes increasingly valuable at scale for introducing diverse\nformulations. This trade-off highlights the complementary strengths of both\napproaches and the importance of combining them."}
{"id": "2507.11716", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11716", "abs": "https://arxiv.org/abs/2507.11716", "authors": ["Yifan Xu", "Qianwei Wang", "Jordan Lillie", "Vineet Kamat", "Carol Menassa", "Clive D'Souza"], "title": "CoNav Chair: Development and Evaluation of a Shared Control based Wheelchair for the Built Environment", "comment": "13 pages, 10 figures", "summary": "As the global population of people with disabilities (PWD) continues to grow,\nso will the need for mobility solutions that promote independent living and\nsocial integration. Wheelchairs are vital for the mobility of PWD in both\nindoor and outdoor environments. The current SOTA in powered wheelchairs is\nbased on either manually controlled or fully autonomous modes of operation,\noffering limited flexibility and often proving difficult to navigate in\nspatially constrained environments. Moreover, research on robotic wheelchairs\nhas focused predominantly on complete autonomy or improved manual control;\napproaches that can compromise efficiency and user trust. To overcome these\nchallenges, this paper introduces the CoNav Chair, a smart wheelchair based on\nthe Robot Operating System (ROS) and featuring shared control navigation and\nobstacle avoidance capabilities that are intended to enhance navigational\nefficiency, safety, and ease of use for the user. The paper outlines the CoNav\nChair's design and presents a preliminary usability evaluation comparing three\ndistinct navigation modes, namely, manual, shared, and fully autonomous,\nconducted with 21 healthy, unimpaired participants traversing an indoor\nbuilding environment. Study findings indicated that the shared control\nnavigation framework had significantly fewer collisions and performed\ncomparably, if not superior to the autonomous and manual modes, on task\ncompletion time, trajectory length, and smoothness; and was perceived as being\nsafer and more efficient based on user reported subjective assessments of\nusability. Overall, the CoNav system demonstrated acceptable safety and\nperformance, laying the foundation for subsequent usability testing with end\nusers, namely, PWDs who rely on a powered wheelchair for mobility."}
{"id": "2507.11787", "categories": ["cs.AI", "68-68W50"], "pdf": "https://arxiv.org/pdf/2507.11787", "abs": "https://arxiv.org/abs/2507.11787", "authors": ["Chandrashekar Muniyappa", "Eunjin Kim"], "title": "Survey of Swarm Intelligence Approaches to Search Documents Based On Semantic Similarity", "comment": "CSAIDE '25: Proceedings of the 2025 4th International Conference on\n  Cyber Security, Artificial Intelligence and the Digital Economy", "summary": "Swarm Intelligence (SI) is gaining a lot of popularity in artificial\nintelligence, where the natural behavior of animals and insects is observed and\ntranslated into computer algorithms called swarm computing to solve real-world\nproblems. Due to their effectiveness, they are applied in solving various\ncomputer optimization problems. This survey will review all the latest\ndevelopments in Searching for documents based on semantic similarity using\nSwarm Intelligence algorithms and recommend future research directions."}
{"id": "2507.11770", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11770", "abs": "https://arxiv.org/abs/2507.11770", "authors": ["Giang Nguyen", "Mihai Pomarlan", "Sascha Jongebloed", "Nils Leusmann", "Minh Nhat Vu", "Michael Beetz"], "title": "Generating Actionable Robot Knowledge Bases by Combining 3D Scene Graphs with Robot Ontologies", "comment": "8 pages, 7 figures, IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS2025)", "summary": "In robotics, the effective integration of environmental data into actionable\nknowledge remains a significant challenge due to the variety and\nincompatibility of data formats commonly used in scene descriptions, such as\nMJCF, URDF, and SDF. This paper presents a novel approach that addresses these\nchallenges by developing a unified scene graph model that standardizes these\nvaried formats into the Universal Scene Description (USD) format. This\nstandardization facilitates the integration of these scene graphs with robot\nontologies through semantic reporting, enabling the translation of complex\nenvironmental data into actionable knowledge essential for cognitive robotic\ncontrol. We evaluated our approach by converting procedural 3D environments\ninto USD format, which is then annotated semantically and translated into a\nknowledge graph to effectively answer competency questions, demonstrating its\nutility for real-time robotic decision-making. Additionally, we developed a\nweb-based visualization tool to support the semantic mapping process, providing\nusers with an intuitive interface to manage the 3D environment."}
{"id": "2507.11916", "categories": ["cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2507.11916", "abs": "https://arxiv.org/abs/2507.11916", "authors": ["Ehsan Futuhi", "Nathan R. Sturtevant"], "title": "A Parallel CPU-GPU Framework for Cost-Bounded DFS with Applications to IDA* and BTS", "comment": null, "summary": "The rapid advancement of GPU technology has unlocked powerful parallel\nprocessing capabilities, creating new opportunities to enhance classic search\nalgorithms. A recent successful application of GPUs is in compressing large\npattern database (PDB) heuristics using neural networks while preserving\nheuristic admissibility. However, very few algorithms have been designed to\nexploit GPUs during search. Several variants of A* exist that batch GPU\ncomputations. In this paper we introduce a method for batching GPU computations\nin depth first search. In particular, we describe a new cost-bounded\ndepth-first search (CB-DFS) method that leverages the combined parallelism of\nmodern CPUs and GPUs. This is used to create algorithms like \\emph{Batch IDA*},\nan extension of the Iterative Deepening A* (IDA*) algorithm, or Batch BTS, an\nextensions of Budgeted Tree Search. Our approach builds on the general approach\nused by Asynchronous Parallel IDA* (AIDA*), while maintaining optimality\nguarantees. We evaluate the approach on the 3x3 Rubik's Cube and 4x4 sliding\ntile puzzle (STP), showing that GPU operations can be efficiently batched in\nDFS. Additionally, we conduct extensive experiments to analyze the effects of\nhyperparameters, neural network heuristic size, and hardware resources on\nperformance."}
{"id": "2507.11840", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11840", "abs": "https://arxiv.org/abs/2507.11840", "authors": ["Gaofeng Li", "Ruize Wang", "Peisen Xu", "Qi Ye", "Jiming Chen"], "title": "The Developments and Challenges towards Dexterous and Embodied Robotic Manipulation: A Survey", "comment": null, "summary": "Achieving human-like dexterous robotic manipulation remains a central goal\nand a pivotal challenge in robotics. The development of Artificial Intelligence\n(AI) has allowed rapid progress in robotic manipulation. This survey summarizes\nthe evolution of robotic manipulation from mechanical programming to embodied\nintelligence, alongside the transition from simple grippers to multi-fingered\ndexterous hands, outlining key characteristics and main challenges. Focusing on\nthe current stage of embodied dexterous manipulation, we highlight recent\nadvances in two critical areas: dexterous manipulation data collection (via\nsimulation, human demonstrations, and teleoperation) and skill-learning\nframeworks (imitation and reinforcement learning). Then, based on the overview\nof the existing data collection paradigm and learning framework, three key\nchallenges restricting the development of dexterous robotic manipulation are\nsummarized and discussed."}
{"id": "2507.11988", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11988", "abs": "https://arxiv.org/abs/2507.11988", "authors": ["Yexuan Shi", "Mingyu Wang", "Yunxiang Cao", "Hongjie Lai", "Junjian Lan", "Xin Han", "Yu Wang", "Jie Geng", "Zhenan Li", "Zihao Xia", "Xiang Chen", "Chen Li", "Jian Xu", "Wenbo Duan", "Yuanshuo Zhu"], "title": "Aime: Towards Fully-Autonomous Multi-Agent Framework", "comment": "14 pages, 1 figures,", "summary": "Multi-Agent Systems (MAS) powered by Large Language Models (LLMs) are\nemerging as a powerful paradigm for solving complex, multifaceted problems.\nHowever, the potential of these systems is often constrained by the prevalent\nplan-and-execute framework, which suffers from critical limitations: rigid plan\nexecution, static agent capabilities, and inefficient communication. These\nweaknesses hinder their adaptability and robustness in dynamic environments.\nThis paper introduces Aime, a novel multi-agent framework designed to overcome\nthese challenges through dynamic, reactive planning and execution. Aime\nreplaces the conventional static workflow with a fluid and adaptive\narchitecture. Its core innovations include: (1) a Dynamic Planner that\ncontinuously refines the overall strategy based on real-time execution\nfeedback; (2) an Actor Factory that implements Dynamic Actor instantiation,\nassembling specialized agents on-demand with tailored tools and knowledge; and\n(3) a centralized Progress Management Module that serves as a single source of\ntruth for coherent, system-wide state awareness. We empirically evaluated Aime\non a diverse suite of benchmarks spanning general reasoning (GAIA), software\nengineering (SWE-bench Verified), and live web navigation (WebVoyager). The\nresults demonstrate that Aime consistently outperforms even highly specialized\nstate-of-the-art agents in their respective domains. Its superior adaptability\nand task success rate establish Aime as a more resilient and effective\nfoundation for multi-agent collaboration."}
{"id": "2507.11852", "categories": ["cs.RO", "cs.CV", "93C85", "F.2.2; I.2.7"], "pdf": "https://arxiv.org/pdf/2507.11852", "abs": "https://arxiv.org/abs/2507.11852", "authors": ["Mohammed Hassanin", "Mohammad Abu Alsheikh", "Carlos C. N. Kuhn", "Damith Herath", "Dinh Thai Hoang", "Ibrahim Radwan"], "title": "Towards Autonomous Riding: A Review of Perception, Planning, and Control in Intelligent Two-Wheelers", "comment": "17 pages", "summary": "The rapid adoption of micromobility solutions, particularly two-wheeled\nvehicles like e-scooters and e-bikes, has created an urgent need for reliable\nautonomous riding (AR) technologies. While autonomous driving (AD) systems have\nmatured significantly, AR presents unique challenges due to the inherent\ninstability of two-wheeled platforms, limited size, limited power, and\nunpredictable environments, which pose very serious concerns about road users'\nsafety. This review provides a comprehensive analysis of AR systems by\nsystematically examining their core components, perception, planning, and\ncontrol, through the lens of AD technologies. We identify critical gaps in\ncurrent AR research, including a lack of comprehensive perception systems for\nvarious AR tasks, limited industry and government support for such\ndevelopments, and insufficient attention from the research community. The\nreview analyses the gaps of AR from the perspective of AD to highlight\npromising research directions, such as multimodal sensor techniques for\nlightweight platforms and edge deep learning architectures. By synthesising\ninsights from AD research with the specific requirements of AR, this review\naims to accelerate the development of safe, efficient, and scalable autonomous\nriding systems for future urban mobility."}
{"id": "2507.11992", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11992", "abs": "https://arxiv.org/abs/2507.11992", "authors": ["Pranav Rajbhandari", "Abhi Veda", "Matthew Garratt", "Mandayam Srinivasan", "Sridhar Ravi"], "title": "Understanding visual attention beehind bee-inspired UAV navigation", "comment": null, "summary": "Bio-inspired design is often used in autonomous UAV navigation due to the\ncapacity of biological systems for flight and obstacle avoidance despite\nlimited sensory and computational capabilities. In particular, honeybees mainly\nuse the sensory input of optic flow, the apparent motion of objects in their\nvisual field, to navigate cluttered environments. In our work, we train a\nReinforcement Learning agent to navigate a tunnel with obstacles using only\noptic flow as sensory input. We inspect the attention patterns of trained\nagents to determine the regions of optic flow on which they primarily base\ntheir motor decisions. We find that agents trained in this way pay most\nattention to regions of discontinuity in optic flow, as well as regions with\nlarge optic flow magnitude. The trained agents appear to navigate a cluttered\ntunnel by avoiding the obstacles that produce large optic flow, while\nmaintaining a centered position in their environment, which resembles the\nbehavior seen in flying insects. This pattern persists across independently\ntrained agents, which suggests that this could be a good strategy for\ndeveloping a simple explicit control law for physical UAVs."}
{"id": "2507.11880", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11880", "abs": "https://arxiv.org/abs/2507.11880", "authors": ["Jinyuan Liu", "Minglei Fu", "Ling Shi", "Chenguang Yang", "Wenan Zhang"], "title": "A Fast Method for Planning All Optimal Homotopic Configurations for Tethered Robots and Its Extended Applications", "comment": "37 pages, 33 figures", "summary": "Tethered robots play a pivotal role in specialized environments such as\ndisaster response and underground exploration, where their stable power supply\nand reliable communication offer unparalleled advantages. However, their motion\nplanning is severely constrained by tether length limitations and entanglement\nrisks, posing significant challenges to achieving optimal path planning. To\naddress these challenges, this study introduces CDT-TCS (Convex Dissection\nTopology-based Tethered Configuration Search), a novel algorithm that leverages\nCDT Encoding as a homotopy invariant to represent topological states of paths.\nBy integrating algebraic topology with geometric optimization, CDT-TCS\nefficiently computes the complete set of optimal feasible configurations for\ntethered robots at all positions in 2D environments through a single\ncomputation. Building on this foundation, we further propose three\napplication-specific algorithms: i) CDT-TPP for optimal tethered path planning,\nii) CDT-TMV for multi-goal visiting with tether constraints, iii) CDT-UTPP for\ndistance-optimal path planning of untethered robots. All theoretical results\nand propositions underlying these algorithms are rigorously proven and\nthoroughly discussed in this paper. Extensive simulations demonstrate that the\nproposed algorithms significantly outperform state-of-the-art methods in their\nrespective problem domains. Furthermore, real-world experiments on robotic\nplatforms validate the practicality and engineering value of the proposed\nframework."}
{"id": "2507.12110", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12110", "abs": "https://arxiv.org/abs/2507.12110", "authors": ["Ye Han", "Lijun Zhang", "Dejian Meng", "Zhuang Zhang"], "title": "Topology Enhanced MARL for Multi-Vehicle Cooperative Decision-Making of CAVs", "comment": "16 pages, 16 figures", "summary": "The exploration-exploitation trade-off constitutes one of the fundamental\nchallenges in reinforcement learning (RL), which is exacerbated in multi-agent\nreinforcement learning (MARL) due to the exponential growth of joint\nstate-action spaces. This paper proposes a topology-enhanced MARL (TPE-MARL)\nmethod for optimizing cooperative decision-making of connected and autonomous\nvehicles (CAVs) in mixed traffic. This work presents two primary contributions:\nFirst, we construct a game topology tensor for dynamic traffic flow,\neffectively compressing high-dimensional traffic state information and decrease\nthe search space for MARL algorithms. Second, building upon the designed game\ntopology tensor and using QMIX as the backbone RL algorithm, we establish a\ntopology-enhanced MARL framework incorporating visit counts and agent mutual\ninformation. Extensive simulations across varying traffic densities and CAV\npenetration rates demonstrate the effectiveness of TPE-MARL. Evaluations\nencompassing training dynamics, exploration patterns, macroscopic traffic\nperformance metrics, and microscopic vehicle behaviors reveal that TPE-MARL\nsuccessfully balances exploration and exploitation. Consequently, it exhibits\nsuperior performance in terms of traffic efficiency, safety, decision\nsmoothness, and task completion. Furthermore, the algorithm demonstrates\ndecision-making rationality comparable to or exceeding that of human drivers in\nboth mixed-autonomy and fully autonomous traffic scenarios. Code of our work is\navailable at\n\\href{https://github.com/leoPub/tpemarl}{https://github.com/leoPub/tpemarl}."}
{"id": "2507.11889", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11889", "abs": "https://arxiv.org/abs/2507.11889", "authors": ["Adnan Abdullah", "Alankrit Gupta", "Vaishnav Ramesh", "Shivali Patel", "Md Jahidul Islam"], "title": "NemeSys: An Online Underwater Explorer with Goal-Driven Adaptive Autonomy", "comment": "10 pages, V1", "summary": "Adaptive mission control and dynamic parameter reconfiguration are essential\nfor autonomous underwater vehicles (AUVs) operating in GPS-denied,\ncommunication-limited marine environments. However, most current AUV platforms\nexecute static, pre-programmed missions or rely on tethered connections and\nhigh-latency acoustic channels for mid-mission updates, significantly limiting\ntheir adaptability and responsiveness. In this paper, we introduce NemeSys, a\nnovel AUV system designed to support real-time mission reconfiguration through\ncompact optical and magnetoelectric (OME) signaling facilitated by floating\nbuoys. We present the full system design, control architecture, and a semantic\nmission encoding framework that enables interactive exploration and task\nadaptation via low-bandwidth communication. The proposed system is validated\nthrough analytical modeling, controlled experimental evaluations, and\nopen-water trials. Results confirm the feasibility of online mission adaptation\nand semantic task updates, highlighting NemeSys as an online AUV platform for\ngoal-driven adaptive autonomy in dynamic and uncertain underwater environments."}
{"id": "2507.12186", "categories": ["cs.AI", "I.2.8; I.2.9"], "pdf": "https://arxiv.org/pdf/2507.12186", "abs": "https://arxiv.org/abs/2507.12186", "authors": ["Edward Kim", "Hanna Kurniawati"], "title": "Partially Observable Reference Policy Programming: Solving POMDPs Sans Numerical Optimisation", "comment": "8 pages, 2 tables, 3 figures. To be presented at International Joint\n  Conference on Artificial Intelligence 2025", "summary": "This paper proposes Partially Observable Reference Policy Programming, a\nnovel anytime online approximate POMDP solver which samples meaningful future\nhistories very deeply while simultaneously forcing a gradual policy update. We\nprovide theoretical guarantees for the algorithm's underlying scheme which say\nthat the performance loss is bounded by the average of the sampling\napproximation errors rather than the usual maximum, a crucial requirement given\nthe sampling sparsity of online planning. Empirical evaluations on two\nlarge-scale problems with dynamically evolving environments -- including a\nhelicopter emergency scenario in the Corsica region requiring approximately 150\nplanning steps -- corroborate the theoretical results and indicate that our\nsolver considerably outperforms current online benchmarks."}
{"id": "2507.11920", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.11920", "abs": "https://arxiv.org/abs/2507.11920", "authors": ["Jeongyong Yang", "KwangBin Lee", "SooJean Han"], "title": "Hybrid Conformal Prediction-based Risk-Aware Model Predictive Planning in Dense, Uncertain Environments", "comment": null, "summary": "Real-time path planning in dense, uncertain environments remains a\nchallenging problem, as predicting the future motions of numerous dynamic\nobstacles is computationally burdensome and unrealistic. To address this, we\nintroduce Hybrid Prediction-based Risk-Aware Planning (HyPRAP), a\nprediction-based risk-aware path-planning framework which uses a hybrid\ncombination of models to predict local obstacle movement. HyPRAP uses a novel\nPrediction-based Collision Risk Index (P-CRI) to evaluate the risk posed by\neach obstacle, enabling the selective use of predictors based on whether the\nagent prioritizes high predictive accuracy or low computational prediction\noverhead. This selective routing enables the agent to focus on high-risk\nobstacles while ignoring or simplifying low-risk ones, making it suitable for\nenvironments with a large number of obstacles. Moreover, HyPRAP incorporates\nuncertainty quantification through hybrid conformal prediction by deriving\nconfidence bounds simultaneously achieved by multiple predictions across\ndifferent models. Theoretical analysis demonstrates that HyPRAP effectively\nbalances safety and computational efficiency by leveraging the diversity of\nprediction models. Extensive simulations validate these insights for more\ngeneral settings, confirming that HyPRAP performs better compared to single\npredictor methods, and P-CRI performs better over naive proximity-based risk\nassessment."}
{"id": "2507.12207", "categories": ["cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2507.12207", "abs": "https://arxiv.org/abs/2507.12207", "authors": ["Subin Lin", "Chuanbo Hua"], "title": "BuildEvo: Designing Building Energy Consumption Forecasting Heuristics via LLM-driven Evolution", "comment": "ICML 2025 CO-Build Workshop Poster", "summary": "Accurate building energy forecasting is essential, yet traditional heuristics\noften lack precision, while advanced models can be opaque and struggle with\ngeneralization by neglecting physical principles. This paper introduces\nBuildEvo, a novel framework that uses Large Language Models (LLMs) to\nautomatically design effective and interpretable energy prediction heuristics.\nWithin an evolutionary process, BuildEvo guides LLMs to construct and enhance\nheuristics by systematically incorporating physical insights from building\ncharacteristics and operational data (e.g., from the Building Data Genome\nProject 2). Evaluations show BuildEvo achieves state-of-the-art performance on\nbenchmarks, offering improved generalization and transparent prediction logic.\nThis work advances the automated design of robust, physically grounded\nheuristics, promoting trustworthy models for complex energy systems."}
{"id": "2507.11938", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11938", "abs": "https://arxiv.org/abs/2507.11938", "authors": ["Hao Chen", "Takuya Kiyokawa", "Zhengtao Hu", "Weiwei Wan", "Kensuke Harada"], "title": "A Multi-Level Similarity Approach for Single-View Object Grasping: Matching, Planning, and Fine-Tuning", "comment": "Accepted by IEEE T-RO", "summary": "Grasping unknown objects from a single view has remained a challenging topic\nin robotics due to the uncertainty of partial observation. Recent advances in\nlarge-scale models have led to benchmark solutions such as GraspNet-1Billion.\nHowever, such learning-based approaches still face a critical limitation in\nperformance robustness for their sensitivity to sensing noise and environmental\nchanges. To address this bottleneck in achieving highly generalized grasping,\nwe abandon the traditional learning framework and introduce a new perspective:\nsimilarity matching, where similar known objects are utilized to guide the\ngrasping of unknown target objects. We newly propose a method that robustly\nachieves unknown-object grasping from a single viewpoint through three key\nsteps: 1) Leverage the visual features of the observed object to perform\nsimilarity matching with an existing database containing various object models,\nidentifying potential candidates with high similarity; 2) Use the candidate\nmodels with pre-existing grasping knowledge to plan imitative grasps for the\nunknown target object; 3) Optimize the grasp quality through a local\nfine-tuning process. To address the uncertainty caused by partial and noisy\nobservation, we propose a multi-level similarity matching framework that\nintegrates semantic, geometric, and dimensional features for comprehensive\nevaluation. Especially, we introduce a novel point cloud geometric descriptor,\nthe C-FPFH descriptor, which facilitates accurate similarity assessment between\npartial point clouds of observed objects and complete point clouds of database\nmodels. In addition, we incorporate the use of large language models, introduce\nthe semi-oriented bounding box, and develop a novel point cloud registration\napproach based on plane detection to enhance matching accuracy under\nsingle-view conditions. Videos are available at https://youtu.be/qQDIELMhQmk."}
{"id": "2507.12215", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12215", "abs": "https://arxiv.org/abs/2507.12215", "authors": ["Yuhao Chen", "Shuochen Liu", "Yuanjie Lyu", "Chao Zhang", "Jiayao Shi", "Tong Xu"], "title": "Xiangqi-R1: Enhancing Spatial Strategic Reasoning in LLMs for Chinese Chess via Reinforcement Learning", "comment": "10 pages, 7 figures", "summary": "Game playing has long served as a fundamental benchmark for evaluating\nArtificial General Intelligence (AGI). While Large Language Models (LLMs) have\ndemonstrated impressive capabilities in general reasoning, their effectiveness\nin spatial strategic reasoning, which is critical for complex and fully\nobservable board games, remains insufficiently explored. In this work, we adopt\nChinese Chess (Xiangqi) as a challenging and rich testbed due to its intricate\nrules and spatial complexity. To advance LLMs' strategic competence in such\nenvironments, we propose a training framework tailored to Xiangqi, built upon a\nlarge-scale dataset of five million board-move pairs enhanced with expert\nannotations and engine evaluations. Building on this foundation, we introduce\nXiangqi-R1, a 7B-parameter model trained in multi-stage manner: (1) fine-tuning\nfor legal move prediction to capture basic spatial rules, (2) incorporating\nstrategic annotations to improve decision-making, and (3) applying\nreinforcement learning via Group Relative Policy Optimization (GRPO) with\nmulti-dimensional reward signals to enhance reasoning stability. Our\nExperimental results indicate that, despite their size and power,\ngeneral-purpose LLMs struggle to achieve satisfactory performance in these\ntasks. Compared to general-purpose LLMs, Xiangqi-R1 greatly advances with an\n18% rise in move legality and a 22% boost in analysis accuracy. Our results\npoint to a promising path for creating general strategic intelligence in\nspatially complex areas."}
{"id": "2507.11940", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.11940", "abs": "https://arxiv.org/abs/2507.11940", "authors": ["Kanghyun Ryu", "Minjun Sung", "Piyush Gupta", "Jovin D'sa", "Faizan M. Tariq", "David Isele", "Sangjae Bae"], "title": "IANN-MPPI: Interaction-Aware Neural Network-Enhanced Model Predictive Path Integral Approach for Autonomous Driving", "comment": "To be published in The IEEE International Conference on Intelligent\n  Transportation Systems (ITSC) 2025", "summary": "Motion planning for autonomous vehicles (AVs) in dense traffic is\nchallenging, often leading to overly conservative behavior and unmet planning\nobjectives. This challenge stems from the AVs' limited ability to anticipate\nand respond to the interactive behavior of surrounding agents. Traditional\ndecoupled prediction and planning pipelines rely on non-interactive predictions\nthat overlook the fact that agents often adapt their behavior in response to\nthe AV's actions. To address this, we propose Interaction-Aware Neural\nNetwork-Enhanced Model Predictive Path Integral (IANN-MPPI) control, which\nenables interactive trajectory planning by predicting how surrounding agents\nmay react to each control sequence sampled by MPPI. To improve performance in\nstructured lane environments, we introduce a spline-based prior for the MPPI\nsampling distribution, enabling efficient lane-changing behavior. We evaluate\nIANN-MPPI in a dense traffic merging scenario, demonstrating its ability to\nperform efficient merging maneuvers. Our project website is available at\nhttps://sites.google.com/berkeley.edu/iann-mppi"}
{"id": "2507.11621", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.11621", "abs": "https://arxiv.org/abs/2507.11621", "authors": ["Tianyi Wang", "Yangyang Wang", "Jie Pan", "Junfeng Jiao", "Christian Claudel"], "title": "HCOMC: A Hierarchical Cooperative On-Ramp Merging Control Framework in Mixed Traffic Environment on Two-Lane Highways", "comment": "7 pages, 2 figures, 3 tables, accepted for IEEE International\n  Conference on Intelligent Transportation Systems (ITSC) 2025", "summary": "Highway on-ramp merging areas are common bottlenecks to traffic congestion\nand accidents. Currently, a cooperative control strategy based on connected and\nautomated vehicles (CAVs) is a fundamental solution to this problem. While CAVs\nare not fully widespread, it is necessary to propose a hierarchical cooperative\non-ramp merging control (HCOMC) framework for heterogeneous traffic flow on\ntwo-lane highways to address this gap. This paper extends longitudinal\ncar-following models based on the intelligent driver model and lateral\nlane-changing models using the quintic polynomial curve to account for\nhuman-driven vehicles (HDVs) and CAVs, comprehensively considering human\nfactors and cooperative adaptive cruise control. Besides, this paper proposes a\nHCOMC framework, consisting of a hierarchical cooperative planning model based\non the modified virtual vehicle model, a discretionary lane-changing model\nbased on game theory, and a multi-objective optimization model using the\nelitist non-dominated sorting genetic algorithm to ensure the safe, smooth, and\nefficient merging process. Then, the performance of our HCOMC is analyzed under\ndifferent traffic densities and CAV penetration rates through simulation. The\nfindings underscore our HCOMC's pronounced comprehensive advantages in\nenhancing the safety of group vehicles, stabilizing and expediting merging\nprocess, optimizing traffic efficiency, and economizing fuel consumption\ncompared with benchmarks."}
{"id": "2507.11974", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11974", "abs": "https://arxiv.org/abs/2507.11974", "authors": ["Waseem Akram", "Muhayy Ud Din", "Lyes Saad Soud", "Irfan Hussain"], "title": "A Review of Generative AI in Aquaculture: Foundations, Applications, and Future Directions for Smart and Sustainable Farming", "comment": null, "summary": "Generative Artificial Intelligence (GAI) has rapidly emerged as a\ntransformative force in aquaculture, enabling intelligent synthesis of\nmultimodal data, including text, images, audio, and simulation outputs for\nsmarter, more adaptive decision-making. As the aquaculture industry shifts\ntoward data-driven, automation and digital integration operations under the\nAquaculture 4.0 paradigm, GAI models offer novel opportunities across\nenvironmental monitoring, robotics, disease diagnostics, infrastructure\nplanning, reporting, and market analysis. This review presents the first\ncomprehensive synthesis of GAI applications in aquaculture, encompassing\nfoundational architectures (e.g., diffusion models, transformers, and retrieval\naugmented generation), experimental systems, pilot deployments, and real-world\nuse cases. We highlight GAI's growing role in enabling underwater perception,\ndigital twin modeling, and autonomous planning for remotely operated vehicle\n(ROV) missions. We also provide an updated application taxonomy that spans\nsensing, control, optimization, communication, and regulatory compliance.\nBeyond technical capabilities, we analyze key limitations, including limited\ndata availability, real-time performance constraints, trust and explainability,\nenvironmental costs, and regulatory uncertainty. This review positions GAI not\nmerely as a tool but as a critical enabler of smart, resilient, and\nenvironmentally aligned aquaculture systems."}
{"id": "2507.11623", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.11623", "abs": "https://arxiv.org/abs/2507.11623", "authors": ["Alan Papalia", "Charles Dawson", "Laurentiu L. Anton", "Norhan Magdy Bayomi", "Bianca Champenois", "Jung-Hoon Cho", "Levi Cai", "Joseph DelPreto", "Kristen Edwards", "Bilha-Catherine Githinji", "Cameron Hickert", "Vindula Jayawardana", "Matthew Kramer", "Shreyaa Raghavan", "David Russell", "Shide Salimi", "Jingnan Shi", "Soumya Sudhakar", "Yanwei Wang", "Shouyi Wang", "Luca Carlone", "Vijay Kumar", "Daniela Rus", "John E. Fernandez", "Cathy Wu", "George Kantor", "Derek Young", "Hanumant Singh"], "title": "A Roadmap for Climate-Relevant Robotics Research", "comment": null, "summary": "Climate change is one of the defining challenges of the 21st century, and\nmany in the robotics community are looking for ways to contribute. This paper\npresents a roadmap for climate-relevant robotics research, identifying\nhigh-impact opportunities for collaboration between roboticists and experts\nacross climate domains such as energy, the built environment, transportation,\nindustry, land use, and Earth sciences. These applications include problems\nsuch as energy systems optimization, construction, precision agriculture,\nbuilding envelope retrofits, autonomous trucking, and large-scale environmental\nmonitoring. Critically, we include opportunities to apply not only physical\nrobots but also the broader robotics toolkit - including planning, perception,\ncontrol, and estimation algorithms - to climate-relevant problems. A central\ngoal of this roadmap is to inspire new research directions and collaboration by\nhighlighting specific, actionable problems at the intersection of robotics and\nclimate. This work represents a collaboration between robotics researchers and\ndomain experts in various climate disciplines, and it serves as an invitation\nto the robotics community to bring their expertise to bear on urgent climate\npriorities."}
{"id": "2507.11991", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11991", "abs": "https://arxiv.org/abs/2507.11991", "authors": ["Juanran Wang", "Marc R. Schlichting", "Mykel J. Kochenderfer"], "title": "Robust Planning for Autonomous Vehicles with Diffusion-Based Failure Samplers", "comment": null, "summary": "High-risk traffic zones such as intersections are a major cause of\ncollisions. This study leverages deep generative models to enhance the safety\nof autonomous vehicles in an intersection context. We train a 1000-step\ndenoising diffusion probabilistic model to generate collision-causing sensor\nnoise sequences for an autonomous vehicle navigating a four-way intersection\nbased on the current relative position and velocity of an intruder. Using the\ngenerative adversarial architecture, the 1000-step model is distilled into a\nsingle-step denoising diffusion model which demonstrates fast inference speed\nwhile maintaining similar sampling quality. We demonstrate one possible\napplication of the single-step model in building a robust planner for the\nautonomous vehicle. The planner uses the single-step model to efficiently\nsample potential failure cases based on the currently measured traffic state to\ninform its decision-making. Through simulation experiments, the robust planner\ndemonstrates significantly lower failure rate and delay rate compared with the\nbaseline Intelligent Driver Model controller."}
{"id": "2507.11991", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11991", "abs": "https://arxiv.org/abs/2507.11991", "authors": ["Juanran Wang", "Marc R. Schlichting", "Mykel J. Kochenderfer"], "title": "Robust Planning for Autonomous Vehicles with Diffusion-Based Failure Samplers", "comment": null, "summary": "High-risk traffic zones such as intersections are a major cause of\ncollisions. This study leverages deep generative models to enhance the safety\nof autonomous vehicles in an intersection context. We train a 1000-step\ndenoising diffusion probabilistic model to generate collision-causing sensor\nnoise sequences for an autonomous vehicle navigating a four-way intersection\nbased on the current relative position and velocity of an intruder. Using the\ngenerative adversarial architecture, the 1000-step model is distilled into a\nsingle-step denoising diffusion model which demonstrates fast inference speed\nwhile maintaining similar sampling quality. We demonstrate one possible\napplication of the single-step model in building a robust planner for the\nautonomous vehicle. The planner uses the single-step model to efficiently\nsample potential failure cases based on the currently measured traffic state to\ninform its decision-making. Through simulation experiments, the robust planner\ndemonstrates significantly lower failure rate and delay rate compared with the\nbaseline Intelligent Driver Model controller."}
{"id": "2507.12067", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.12067", "abs": "https://arxiv.org/abs/2507.12067", "authors": ["Xing Tong", "Michele D. Simoni"], "title": "Robust Route Planning for Sidewalk Delivery Robots", "comment": null, "summary": "Sidewalk delivery robots are a promising solution for urban freight\ndistribution, reducing congestion compared to trucks and providing a safer,\nhigher-capacity alternative to drones. However, unreliable travel times on\nsidewalks due to pedestrian density, obstacles, and varying infrastructure\nconditions can significantly affect their efficiency. This study addresses the\nrobust route planning problem for sidewalk robots, explicitly accounting for\ntravel time uncertainty due to varying sidewalk conditions. Optimization is\nintegrated with simulation to reproduce the effect of obstacles and pedestrian\nflows and generate realistic travel times. The study investigates three\ndifferent approaches to derive uncertainty sets, including budgeted,\nellipsoidal, and support vector clustering (SVC)-based methods, along with a\ndistributionally robust method to solve the shortest path (SP) problem. A\nrealistic case study reproducing pedestrian patterns in Stockholm's city center\nis used to evaluate the efficiency of robust routing across various robot\ndesigns and environmental conditions. The results show that, when compared to a\nconventional SP, robust routing significantly enhances operational reliability\nunder variable sidewalk conditions. The Ellipsoidal and DRSP approaches\noutperform the other methods, yielding the most efficient paths in terms of\naverage and worst-case delay. Sensitivity analyses reveal that robust\napproaches consistently outperform the conventional SP, particularly for\nsidewalk delivery robots that are wider, slower, and have more conservative\nnavigation behaviors. These benefits are even more pronounced in adverse\nweather conditions and high pedestrian congestion scenarios."}
{"id": "2507.12440", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12440", "abs": "https://arxiv.org/abs/2507.12440", "authors": ["Ruihan Yang", "Qinxi Yu", "Yecheng Wu", "Rui Yan", "Borui Li", "An-Chieh Cheng", "Xueyan Zou", "Yunhao Fang", "Hongxu Yin", "Sifei Liu", "Song Han", "Yao Lu", "Xiaolong Wang"], "title": "EgoVLA: Learning Vision-Language-Action Models from Egocentric Human Videos", "comment": "More videos can be found on our website:\n  https://rchalyang.github.io/EgoVLA", "summary": "Real robot data collection for imitation learning has led to significant\nadvancements in robotic manipulation. However, the requirement for robot\nhardware in the process fundamentally constrains the scale of the data. In this\npaper, we explore training Vision-Language-Action (VLA) models using egocentric\nhuman videos. The benefit of using human videos is not only for their scale but\nmore importantly for the richness of scenes and tasks. With a VLA trained on\nhuman video that predicts human wrist and hand actions, we can perform Inverse\nKinematics and retargeting to convert the human actions to robot actions. We\nfine-tune the model using a few robot manipulation demonstrations to obtain the\nrobot policy, namely EgoVLA. We propose a simulation benchmark called Isaac\nHumanoid Manipulation Benchmark, where we design diverse bimanual manipulation\ntasks with demonstrations. We fine-tune and evaluate EgoVLA with Isaac Humanoid\nManipulation Benchmark and show significant improvements over baselines and\nablate the importance of human data. Videos can be found on our website:\nhttps://rchalyang.github.io/EgoVLA"}
{"id": "2507.12093", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.12093", "abs": "https://arxiv.org/abs/2507.12093", "authors": ["David Rapado-Rincon", "Gert Kootstra"], "title": "Tree-SLAM: semantic object SLAM for efficient mapping of individual trees in orchards", "comment": "Paper submitted to Smart Agricultural Technology", "summary": "Accurate mapping of individual trees is an important component for precision\nagriculture in orchards, as it allows autonomous robots to perform tasks like\ntargeted operations or individual tree monitoring. However, creating these maps\nis challenging because GPS signals are often unreliable under dense tree\ncanopies. Furthermore, standard Simultaneous Localization and Mapping (SLAM)\napproaches struggle in orchards because the repetitive appearance of trees can\nconfuse the system, leading to mapping errors. To address this, we introduce\nTree-SLAM, a semantic SLAM approach tailored for creating maps of individual\ntrees in orchards. Utilizing RGB-D images, our method detects tree trunks with\nan instance segmentation model, estimates their location and re-identifies them\nusing a cascade-graph-based data association algorithm. These re-identified\ntrunks serve as landmarks in a factor graph framework that integrates noisy GPS\nsignals, odometry, and trunk observations. The system produces maps of\nindividual trees with a geo-localization error as low as 18 cm, which is less\nthan 20\\% of the planting distance. The proposed method was validated on\ndiverse datasets from apple and pear orchards across different seasons,\ndemonstrating high mapping accuracy and robustness in scenarios with unreliable\nGPS signals."}
{"id": "2507.12148", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.12148", "abs": "https://arxiv.org/abs/2507.12148", "authors": ["Xing Tong", "Michele D. Simoni", "Kaj Munhoz Arfvidsson", "Jonas Mårtensson"], "title": "Leveraging Sidewalk Robots for Walkability-Related Analyses", "comment": null, "summary": "Walkability is a key component of sustainable urban development, while\ncollecting detailed data on its related features remains challenging due to the\nhigh costs and limited scalability of traditional methods. Sidewalk delivery\nrobots, increasingly deployed in urban environments, offer a promising solution\nto these limitations. This paper explores how these robots can serve as mobile\ndata collection platforms, capturing sidewalk-level features related to\nwalkability in a scalable, automated, and real-time manner. A sensor-equipped\nrobot was deployed on a sidewalk network at KTH in Stockholm, completing 101\ntrips covering 900 segments. From the collected data, different typologies of\nfeatures are derived, including robot trip characteristics (e.g., speed,\nduration), sidewalk conditions (e.g., width, surface unevenness), and sidewalk\nutilization (e.g., pedestrian density). Their walkability-related implications\nwere investigated with a series of analyses. The results demonstrate that\npedestrian movement patterns are strongly influenced by sidewalk\ncharacteristics, with higher density, reduced width, and surface irregularity\nassociated with slower and more variable trajectories. Notably, robot speed\nclosely mirrors pedestrian behavior, highlighting its potential as a proxy for\nassessing pedestrian dynamics. The proposed framework enables continuous\nmonitoring of sidewalk conditions and pedestrian behavior, contributing to the\ndevelopment of more walkable, inclusive, and responsive urban environments."}
{"id": "2507.12158", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.12158", "abs": "https://arxiv.org/abs/2507.12158", "authors": ["Nawshin Mannan Proma", "Gricel Vázquez", "Sepeedeh Shahbeigi", "Arjun Badyal", "Victoria Hodge"], "title": "Probabilistic Safety Verification for an Autonomous Ground Vehicle: A Situation Coverage Grid Approach", "comment": "6 pages, 6 figures", "summary": "As industrial autonomous ground vehicles are increasingly deployed in\nsafety-critical environments, ensuring their safe operation under diverse\nconditions is paramount. This paper presents a novel approach for their safety\nverification based on systematic situation extraction, probabilistic modelling\nand verification. We build upon the concept of a situation coverage grid, which\nexhaustively enumerates environmental configurations relevant to the vehicle's\noperation. This grid is augmented with quantitative probabilistic data\ncollected from situation-based system testing, capturing probabilistic\ntransitions between situations. We then generate a probabilistic model that\nencodes the dynamics of both normal and unsafe system behaviour. Safety\nproperties extracted from hazard analysis and formalised in temporal logic are\nverified through probabilistic model checking against this model. The results\ndemonstrate that our approach effectively identifies high-risk situations,\nprovides quantitative safety guarantees, and supports compliance with\nregulatory standards, thereby contributing to the robust deployment of\nautonomous systems."}
{"id": "2507.12174", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.12174", "abs": "https://arxiv.org/abs/2507.12174", "authors": ["Zhenmin Huang", "Yusen Xie", "Benshan Ma", "Shaojie Shen", "Jun Ma"], "title": "Fast and Scalable Game-Theoretic Trajectory Planning with Intentional Uncertainties", "comment": null, "summary": "Trajectory planning involving multi-agent interactions has been a\nlong-standing challenge in the field of robotics, primarily burdened by the\ninherent yet intricate interactions among agents. While game-theoretic methods\nare widely acknowledged for their effectiveness in managing multi-agent\ninteractions, significant impediments persist when it comes to accommodating\nthe intentional uncertainties of agents. In the context of intentional\nuncertainties, the heavy computational burdens associated with existing\ngame-theoretic methods are induced, leading to inefficiencies and poor\nscalability. In this paper, we propose a novel game-theoretic interactive\ntrajectory planning method to effectively address the intentional uncertainties\nof agents, and it demonstrates both high efficiency and enhanced scalability.\nAs the underpinning basis, we model the interactions between agents under\nintentional uncertainties as a general Bayesian game, and we show that its\nagent-form equivalence can be represented as a potential game under certain\nminor assumptions. The existence and attainability of the optimal interactive\ntrajectories are illustrated, as the corresponding Bayesian Nash equilibrium\ncan be attained by optimizing a unified optimization problem. Additionally, we\npresent a distributed algorithm based on the dual consensus alternating\ndirection method of multipliers (ADMM) tailored to the parallel solving of the\nproblem, thereby significantly improving the scalability. The attendant\noutcomes from simulations and experiments demonstrate that the proposed method\nis effective across a range of scenarios characterized by general forms of\nintentional uncertainties. Its scalability surpasses that of existing\ncentralized and decentralized baselines, allowing for real-time interactive\ntrajectory planning in uncertain game settings."}
{"id": "2507.12194", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.12194", "abs": "https://arxiv.org/abs/2507.12194", "authors": ["Hongming Shen", "Xun Chen", "Yulin Hui", "Zhenyu Wu", "Wei Wang", "Qiyang Lyu", "Tianchen Deng", "Danwei Wang"], "title": "UniLGL: Learning Uniform Place Recognition for FOV-limited/Panoramic LiDAR Global Localization", "comment": null, "summary": "Existing LGL methods typically consider only partial information (e.g.,\ngeometric features) from LiDAR observations or are designed for homogeneous\nLiDAR sensors, overlooking the uniformity in LGL. In this work, a uniform LGL\nmethod is proposed, termed UniLGL, which simultaneously achieves spatial and\nmaterial uniformity, as well as sensor-type uniformity. The key idea of the\nproposed method is to encode the complete point cloud, which contains both\ngeometric and material information, into a pair of BEV images (i.e., a spatial\nBEV image and an intensity BEV image). An end-to-end multi-BEV fusion network\nis designed to extract uniform features, equipping UniLGL with spatial and\nmaterial uniformity. To ensure robust LGL across heterogeneous LiDAR sensors, a\nviewpoint invariance hypothesis is introduced, which replaces the conventional\ntranslation equivariance assumption commonly used in existing LPR networks and\nsupervises UniLGL to achieve sensor-type uniformity in both global descriptors\nand local feature representations. Finally, based on the mapping between local\nfeatures on the 2D BEV image and the point cloud, a robust global pose\nestimator is derived that determines the global minimum of the global pose on\nSE(3) without requiring additional registration. To validate the effectiveness\nof the proposed uniform LGL, extensive benchmarks are conducted in real-world\nenvironments, and the results show that the proposed UniLGL is demonstratively\ncompetitive compared to other State-of-the-Art LGL methods. Furthermore, UniLGL\nhas been deployed on diverse platforms, including full-size trucks and agile\nMicro Aerial Vehicles (MAVs), to enable high-precision localization and mapping\nas well as multi-MAV collaborative exploration in port and forest environments,\ndemonstrating the applicability of UniLGL in industrial and field scenarios."}
{"id": "2507.12273", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.12273", "abs": "https://arxiv.org/abs/2507.12273", "authors": ["Luca Garello", "Francesca Cocchella", "Alessandra Sciutti", "Manuel Catalano", "Francesco Rea"], "title": "Next-Gen Museum Guides: Autonomous Navigation and Visitor Interaction with an Agentic Robot", "comment": null, "summary": "Autonomous robots are increasingly being tested into public spaces to enhance\nuser experiences, particularly in cultural and educational settings. This paper\npresents the design, implementation, and evaluation of the autonomous museum\nguide robot Alter-Ego equipped with advanced navigation and interactive\ncapabilities. The robot leverages state-of-the-art Large Language Models (LLMs)\nto provide real-time, context aware question-and-answer (Q&A) interactions,\nallowing visitors to engage in conversations about exhibits. It also employs\nrobust simultaneous localization and mapping (SLAM) techniques, enabling\nseamless navigation through museum spaces and route adaptation based on user\nrequests. The system was tested in a real museum environment with 34\nparticipants, combining qualitative analysis of visitor-robot conversations and\nquantitative analysis of pre and post interaction surveys. Results showed that\nthe robot was generally well-received and contributed to an engaging museum\nexperience, despite some limitations in comprehension and responsiveness. This\nstudy sheds light on HRI in cultural spaces, highlighting not only the\npotential of AI-driven robotics to support accessibility and knowledge\nacquisition, but also the current limitations and challenges of deploying such\ntechnologies in complex, real-world environments."}
{"id": "2507.12391", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.12391", "abs": "https://arxiv.org/abs/2507.12391", "authors": ["Jacinto Colan", "Ana Davila", "Yasuhisa Hasegawa"], "title": "Assessing the Value of Visual Input: A Benchmark of Multimodal Large Language Models for Robotic Path Planning", "comment": "Accepted at the 2025 SICE Festival with Annual Conference (SICE FES)", "summary": "Large Language Models (LLMs) show potential for enhancing robotic path\nplanning. This paper assesses visual input's utility for multimodal LLMs in\nsuch tasks via a comprehensive benchmark. We evaluated 15 multimodal LLMs on\ngenerating valid and optimal paths in 2D grid environments, simulating\nsimplified robotic planning, comparing text-only versus text-plus-visual inputs\nacross varying model sizes and grid complexities. Our results indicate moderate\nsuccess rates on simpler small grids, where visual input or few-shot text\nprompting offered some benefits. However, performance significantly degraded on\nlarger grids, highlighting a scalability challenge. While larger models\ngenerally achieved higher average success, the visual modality was not\nuniversally dominant over well-structured text for these multimodal systems,\nand successful paths on simpler grids were generally of high quality. These\nresults indicate current limitations in robust spatial reasoning, constraint\nadherence, and scalable multimodal integration, identifying areas for future\nLLM development in robotic path planning."}
{"id": "2507.12407", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.12407", "abs": "https://arxiv.org/abs/2507.12407", "authors": ["Svetlana Levit", "Marc Toussaint"], "title": "Regrasp Maps for Sequential Manipulation Planning", "comment": null, "summary": "We consider manipulation problems in constrained and cluttered settings,\nwhich require several regrasps at unknown locations. We propose to inform an\noptimization-based task and motion planning (TAMP) solver with possible regrasp\nareas and grasp sequences to speed up the search. Our main idea is to use a\nstate space abstraction, a regrasp map, capturing the combinations of available\ngrasps in different parts of the configuration space, and allowing us to\nprovide the solver with guesses for the mode switches and additional\nconstraints for the object placements. By interleaving the creation of regrasp\nmaps, their adaptation based on failed refinements, and solving TAMP\n(sub)problems, we are able to provide a robust search method for challenging\nregrasp manipulation problems."}
{"id": "2507.12431", "categories": ["cs.RO", "physics.ins-det"], "pdf": "https://arxiv.org/pdf/2507.12431", "abs": "https://arxiv.org/abs/2507.12431", "authors": ["Connor Burgess", "Kyle Douin", "Amir Kordijazi"], "title": "Design and Development of an Automated Contact Angle Tester (ACAT) for Surface Wettability Measurement", "comment": "14 pages, 4 figures", "summary": "The Automated Contact Angle Tester (ACAT) is a fully integrated robotic work\ncell developed to automate the measurement of surface wettability on 3D-printed\nmaterials. Designed for precision, repeatability, and safety, ACAT addresses\nthe limitations of manual contact angle testing by combining programmable\nrobotics, precise liquid dispensing, and a modular software-hardware\narchitecture. The system is composed of three core subsystems: (1) an\nelectrical system including power, control, and safety circuits compliant with\nindustrial standards such as NEC 70, NFPA 79, and UL 508A; (2) a software\ncontrol system based on a Raspberry Pi and Python, featuring fault detection,\nGPIO logic, and operator interfaces; and (3) a mechanical system that includes\na 3-axis Cartesian robot, pneumatic actuation, and a precision liquid dispenser\nenclosed within a safety-certified frame. The ACAT enables high-throughput,\nautomated surface characterization and provides a robust platform for future\nintegration into smart manufacturing and materials discovery workflows. This\npaper details the design methodology, implementation strategies, and system\nintegration required to develop the ACAT platform."}
{"id": "2507.12440", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12440", "abs": "https://arxiv.org/abs/2507.12440", "authors": ["Ruihan Yang", "Qinxi Yu", "Yecheng Wu", "Rui Yan", "Borui Li", "An-Chieh Cheng", "Xueyan Zou", "Yunhao Fang", "Hongxu Yin", "Sifei Liu", "Song Han", "Yao Lu", "Xiaolong Wang"], "title": "EgoVLA: Learning Vision-Language-Action Models from Egocentric Human Videos", "comment": "More videos can be found on our website:\n  https://rchalyang.github.io/EgoVLA", "summary": "Real robot data collection for imitation learning has led to significant\nadvancements in robotic manipulation. However, the requirement for robot\nhardware in the process fundamentally constrains the scale of the data. In this\npaper, we explore training Vision-Language-Action (VLA) models using egocentric\nhuman videos. The benefit of using human videos is not only for their scale but\nmore importantly for the richness of scenes and tasks. With a VLA trained on\nhuman video that predicts human wrist and hand actions, we can perform Inverse\nKinematics and retargeting to convert the human actions to robot actions. We\nfine-tune the model using a few robot manipulation demonstrations to obtain the\nrobot policy, namely EgoVLA. We propose a simulation benchmark called Isaac\nHumanoid Manipulation Benchmark, where we design diverse bimanual manipulation\ntasks with demonstrations. We fine-tune and evaluate EgoVLA with Isaac Humanoid\nManipulation Benchmark and show significant improvements over baselines and\nablate the importance of human data. Videos can be found on our website:\nhttps://rchalyang.github.io/EgoVLA"}
{"id": "2507.11662", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11662", "abs": "https://arxiv.org/abs/2507.11662", "authors": ["Moises Andrade", "Joonhyuk Cha", "Brandon Ho", "Vriksha Srihari", "Karmesh Yadav", "Zsolt Kira"], "title": "Let's Think in Two Steps: Mitigating Agreement Bias in MLLMs with Self-Grounded Verification", "comment": "Our code and data are publicly available at\n  https://github.com/mshalimay/mllm-verifiers-abias-sgv", "summary": "Verifiers -- functions assigning rewards to agent behavior -- have been key\nfor AI progress in domains like math and board games. However, extending these\ngains to domains without clear-cut success criteria (e.g.,computer use) remains\na challenge: while humans can recognize suitable outcomes, translating this\nintuition into scalable rules is non-trivial. Multimodal Large Language\nModels(MLLMs) emerge as a promising solution, given their world knowledge,\nhuman-preference alignment, and reasoning skills. We evaluate MLLMs as\nverifiers of agent trajectories across web navigation, computer use, and\nrobotic manipulation, and identify a critical limitation: agreement bias, a\nstrong tendency for MLLMs to favor information in their context window, often\ngenerating chains of thought to rationalize flawed behavior. This bias is\npervasive across models, resilient to test-time scaling, and can impact several\nmethods using MLLMs as evaluators (e.g.,data filtering). Notably, it occurs\ndespite MLLMs showing strong, human-aligned priors on desired behavior. To\naddress this, we propose Self-Grounded Verification (SGV), a lightweight method\nthat enables more effective use of MLLMs' knowledge and reasoning by harnessing\ntheir own sampling mechanisms via unconditional and conditional generation. SGV\noperates in two steps: first, the MLLM is elicited to retrieve broad priors\nabout task completion, independent of the data under evaluation. Then,\nconditioned on self-generated priors, it reasons over and evaluates a candidate\ntrajectory. Enhanced with SGV, MLLM verifiers show gains of up to 20 points in\naccuracy and failure detection rates, and can perform real-time supervision of\nheterogeneous agents, boosting task completion of a GUI specialist in OSWorld,\na diffusion policy in robomimic, and a ReAct agent in VisualWebArena -- setting\na new state of the art on the benchmark, surpassing the previous best by 48%."}
