{"id": "2507.17846", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.17846", "abs": "https://arxiv.org/abs/2507.17846", "authors": ["Alison Bartsch", "Arvind Car", "Amir Barati Farimani"], "title": "PinchBot: Long-Horizon Deformable Manipulation with Guided Diffusion Policy", "comment": null, "summary": "Pottery creation is a complicated art form that requires dexterous, precise\nand delicate actions to slowly morph a block of clay to a meaningful, and often\nuseful 3D goal shape. In this work, we aim to create a robotic system that can\ncreate simple pottery goals with only pinch-based actions. This pinch pottery\ntask allows us to explore the challenges of a highly multi-modal and\nlong-horizon deformable manipulation task. To this end, we present PinchBot, a\ngoal-conditioned diffusion policy model that when combined with pre-trained 3D\npoint cloud embeddings, task progress prediction and collision-constrained\naction projection, is able to successfully create a variety of simple pottery\ngoals. For experimental videos and access to the demonstration dataset, please\nvisit our project website:\nhttps://sites.google.com/andrew.cmu.edu/pinchbot/home."}
{"id": "2507.17856", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.17856", "abs": "https://arxiv.org/abs/2507.17856", "authors": ["Dennis Benders", "Laura Ferranti", "Johannes KÃ¶hler"], "title": "A Step-by-step Guide on Nonlinear Model Predictive Control for Safe Mobile Robot Navigation", "comment": "51 pages, 3 figures", "summary": "Designing a Model Predictive Control (MPC) scheme that enables a mobile robot\nto safely navigate through an obstacle-filled environment is a complicated yet\nessential task in robotics. In this technical report, safety refers to ensuring\nthat the robot respects state and input constraints while avoiding collisions\nwith obstacles despite the presence of disturbances and measurement noise. This\nreport offers a step-by-step approach to implementing Nonlinear Model\nPredictive Control (NMPC) schemes addressing these safety requirements.\nNumerous books and survey papers provide comprehensive overviews of linear MPC\n(LMPC) \\cite{bemporad2007robust,kouvaritakis2016model}, NMPC\n\\cite{rawlings2017model,allgower2004nonlinear,mayne2014model,grune2017nonlinear,saltik2018outlook},\nand their applications in various domains, including robotics\n\\cite{nascimento2018nonholonomic,nguyen2021model,shi2021advanced,wei2022mpc}.\nThis report does not aim to replicate those exhaustive reviews. Instead, it\nfocuses specifically on NMPC as a foundation for safe mobile robot navigation.\nThe goal is to provide a practical and accessible path from theoretical\nconcepts to mathematical proofs and implementation, emphasizing safety and\nperformance guarantees. It is intended for researchers, robotics engineers, and\npractitioners seeking to bridge the gap between theoretical NMPC formulations\nand real-world robotic applications.\n  This report is not necessarily meant to remain fixed over time. If someone\nfinds an error in the presented theory, please reach out via the given email\naddresses. We are happy to update the document if necessary."}
{"id": "2507.18033", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18033", "abs": "https://arxiv.org/abs/2507.18033", "authors": ["Mingfeng Yuan", "Letian Wang", "Steven L. Waslander"], "title": "OpenNav: Open-World Navigation with Multimodal Large Language Models", "comment": null, "summary": "Pre-trained large language models (LLMs) have demonstrated strong\ncommon-sense reasoning abilities, making them promising for robotic navigation\nand planning tasks. However, despite recent progress, bridging the gap between\nlanguage descriptions and actual robot actions in the open-world, beyond merely\ninvoking limited predefined motion primitives, remains an open challenge. In\nthis work, we aim to enable robots to interpret and decompose complex language\ninstructions, ultimately synthesizing a sequence of trajectory points to\ncomplete diverse navigation tasks given open-set instructions and open-set\nobjects. We observe that multi-modal large language models (MLLMs) exhibit\nstrong cross-modal understanding when processing free-form language\ninstructions, demonstrating robust scene comprehension. More importantly,\nleveraging their code-generation capability, MLLMs can interact with\nvision-language perception models to generate compositional 2D bird-eye-view\nvalue maps, effectively integrating semantic knowledge from MLLMs with spatial\ninformation from maps to reinforce the robot's spatial understanding. To\nfurther validate our approach, we effectively leverage large-scale autonomous\nvehicle datasets (AVDs) to validate our proposed zero-shot vision-language\nnavigation framework in outdoor navigation tasks, demonstrating its capability\nto execute a diverse range of free-form natural language navigation\ninstructions while maintaining robustness against object detection errors and\nlinguistic ambiguities. Furthermore, we validate our system on a Husky robot in\nboth indoor and outdoor scenes, demonstrating its real-world robustness and\napplicability. Supplementary videos are available at\nhttps://trailab.github.io/OpenNav-website/"}
{"id": "2507.18070", "categories": ["cs.RO", "cs.SY", "eess.SP", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.18070", "abs": "https://arxiv.org/abs/2507.18070", "authors": ["Behzad Zamani", "Jochen Trumpf", "Chris Manzie"], "title": "Modular Robot and Landmark Localisation Using Relative Bearing Measurements", "comment": "Submitted to RA-L", "summary": "In this paper we propose a modular nonlinear least squares filtering approach\nfor systems composed of independent subsystems. The state and error covariance\nestimate of each subsystem is updated independently, even when a relative\nmeasurement simultaneously depends on the states of multiple subsystems. We\nintegrate the Covariance Intersection (CI) algorithm as part of our solution in\norder to prevent double counting of information when subsystems share estimates\nwith each other. An alternative derivation of the CI algorithm based on least\nsquares estimation makes this integration possible. We particularise the\nproposed approach to the robot-landmark localization problem. In this problem,\nnoisy measurements of the bearing angle to a stationary landmark position\nmeasured relative to the SE(2) pose of a moving robot couple the estimation\nproblems for the robot pose and the landmark position. In a randomized\nsimulation study, we benchmark the proposed modular method against a monolithic\njoint state filter to elucidate their respective trade-offs. In this study we\nalso include variants of the proposed method that achieve a graceful\ndegradation of performance with reduced communication and bandwidth\nrequirements."}
{"id": "2507.17777", "categories": ["cs.AI", "76A02"], "pdf": "https://arxiv.org/pdf/2507.17777", "abs": "https://arxiv.org/abs/2507.17777", "authors": ["Theofanis Aravanis", "Grigorios Chrimatopoulos", "Mohammad Ferdows", "Michalis Xenos", "Efstratios Em Tzirtzilakis"], "title": "ASP-Assisted Symbolic Regression: Uncovering Hidden Physics in Fluid Mechanics", "comment": "This research was implemented in the framework of the Action\n  \"Flagship actions in interdisciplinary scientific fields with a special focus\n  on the productive fabric'', which is implemented through the National\n  Recovery and Resilience Fund Greece 2.0 and funded by the European\n  Union--NextGenerationEU (Project ID: TAEDR-0535983)", "summary": "Unlike conventional Machine-Learning (ML) approaches, often criticized as\n\"black boxes\", Symbolic Regression (SR) stands out as a powerful tool for\nrevealing interpretable mathematical relationships in complex physical systems,\nrequiring no a priori assumptions about models' structures. Motivated by the\nrecognition that, in fluid mechanics, an understanding of the underlying flow\nphysics is as crucial as accurate prediction, this study applies SR to model a\nfundamental three-dimensional (3D) incompressible flow in a rectangular\nchannel, focusing on the (axial) velocity and pressure fields under laminar\nconditions. By employing the PySR library, compact symbolic equations were\nderived directly from numerical simulation data, revealing key characteristics\nof the flow dynamics. These equations not only approximate the parabolic\nvelocity profile and pressure drop observed in the studied fluid flow, but also\nperfectly coincide with analytical solutions from the literature. Furthermore,\nwe propose an innovative approach that integrates SR with the\nknowledge-representation framework of Answer Set Programming (ASP), combining\nthe generative power of SR with the declarative reasoning strengths of ASP. The\nproposed hybrid SR/ASP framework ensures that the SR-generated symbolic\nexpressions are not only statistically accurate, but also physically plausible,\nadhering to domain-specific principles. Overall, the study highlights two key\ncontributions: SR's ability to simplify complex flow behaviours into concise,\ninterpretable equations, and the potential of knowledge-representation\napproaches to improve the reliability and alignment of data-driven SR models\nwith domain principles. Insights from the examined 3D channel flow pave the way\nfor integrating such hybrid approaches into efficient frameworks, [...] where\nexplainable predictions and real-time data analysis are crucial."}
{"id": "2507.18138", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.18138", "abs": "https://arxiv.org/abs/2507.18138", "authors": ["Min-Gyu Kim", "Dongyun Kang", "Hajun Kim", "Hae-Won Park"], "title": "A Modular Residual Learning Framework to Enhance Model-Based Approach for Robust Locomotion", "comment": "8 pages, IEEE RA-L accepted (July 2025)", "summary": "This paper presents a novel approach that combines the advantages of both\nmodel-based and learning-based frameworks to achieve robust locomotion. The\nresidual modules are integrated with each corresponding part of the model-based\nframework, a footstep planner and dynamic model designed using heuristics, to\ncomplement performance degradation caused by a model mismatch. By utilizing a\nmodular structure and selecting the appropriate learning-based method for each\nresidual module, our framework demonstrates improved control performance in\nenvironments with high uncertainty, while also achieving higher learning\nefficiency compared to baseline methods. Moreover, we observed that our\nproposed methodology not only enhances control performance but also provides\nadditional benefits, such as making nominal controllers more robust to\nparameter tuning. To investigate the feasibility of our framework, we\ndemonstrated residual modules combined with model predictive control in a real\nquadrupedal robot. Despite uncertainties beyond the simulation, the robot\nsuccessfully maintains balance and tracks the commanded velocity."}
{"id": "2507.17874", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17874", "abs": "https://arxiv.org/abs/2507.17874", "authors": ["SaiBarath Sundar", "Pranav Satheesan", "Udayaadithya Avadhanam"], "title": "I2I-STRADA -- Information to Insights via Structured Reasoning Agent for Data Analysis", "comment": null, "summary": "Recent advances in agentic systems for data analysis have emphasized\nautomation of insight generation through multi-agent frameworks, and\norchestration layers. While these systems effectively manage tasks like query\ntranslation, data transformation, and visualization, they often overlook the\nstructured reasoning process underlying analytical thinking. Reasoning large\nlanguage models (LLMs) used for multi-step problem solving are trained as\ngeneral-purpose problem solvers. As a result, their reasoning or thinking steps\ndo not adhere to fixed processes for specific tasks. Real-world data analysis\nrequires a consistent cognitive workflow: interpreting vague goals, grounding\nthem in contextual knowledge, constructing abstract plans, and adapting\nexecution based on intermediate outcomes. We introduce I2I-STRADA\n(Information-to-Insight via Structured Reasoning Agent for Data Analysis), an\nagentic architecture designed to formalize this reasoning process. I2I-STRADA\nfocuses on modeling how analysis unfolds via modular sub-tasks that reflect the\ncognitive steps of analytical reasoning. Evaluations on the DABstep and DABench\nbenchmarks show that I2I-STRADA outperforms prior systems in planning coherence\nand insight alignment, highlighting the importance of structured cognitive\nworkflows in agent design for data analysis."}
{"id": "2507.18160", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.18160", "abs": "https://arxiv.org/abs/2507.18160", "authors": ["Luka Å iktar", "Branimir Äaran", "Bojan Å ekoranja", "Marko Å vaco"], "title": "Autonomous UAV Navigation for Search and Rescue Missions Using Computer Vision and Convolutional Neural Networks", "comment": "The paper is accepted and presented on the 34th International\n  Conference on Robotics in Alpe-Adria-Danube Region, RAAD 2025, Belgrade\n  Serbia", "summary": "In this paper, we present a subsystem, using Unmanned Aerial Vehicles (UAV),\nfor search and rescue missions, focusing on people detection, face recognition\nand tracking of identified individuals. The proposed solution integrates a UAV\nwith ROS2 framework, that utilizes multiple convolutional neural networks (CNN)\nfor search missions. System identification and PD controller deployment are\nperformed for autonomous UAV navigation. The ROS2 environment utilizes the\nYOLOv11 and YOLOv11-pose CNNs for tracking purposes, and the dlib library CNN\nfor face recognition. The system detects a specific individual, performs face\nrecognition and starts tracking. If the individual is not yet known, the UAV\noperator can manually locate the person, save their facial image and\nimmediately initiate the tracking process. The tracking process relies on\nspecific keypoints identified on the human body using the YOLOv11-pose CNN\nmodel. These keypoints are used to track a specific individual and maintain a\nsafe distance. To enhance accurate tracking, system identification is\nperformed, based on measurement data from the UAVs IMU. The identified system\nparameters are used to design PD controllers that utilize YOLOv11-pose to\nestimate the distance between the UAVs camera and the identified individual.\nThe initial experiments, conducted on 14 known individuals, demonstrated that\nthe proposed subsystem can be successfully used in real time. The next step\ninvolves implementing the system on a large experimental UAV for field use and\nintegrating autonomous navigation with GPS-guided control for rescue operations\nplanning."}
{"id": "2507.17927", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17927", "abs": "https://arxiv.org/abs/2507.17927", "authors": ["Timothy Tin Long Yu", "Mahdi Mostajabdaveh", "Jabo Serge Byusa", "Rindra Ramamonjison", "Giuseppe Carenini", "Kun Mao", "Zirui Zhou", "Yong Zhang"], "title": "SMARTAPS: Tool-augmented LLMs for Operations Management", "comment": "https://aaai.org/conference/aaai/aaai-25/bridge-ai-orms/", "summary": "Large language models (LLMs) present intriguing opportunities to enhance user\ninteraction with traditional algorithms and tools in real-world applications.\nAn advanced planning system (APS) is a sophisticated software that leverages\noptimization to help operations planners create, interpret, and modify an\noperational plan. While highly beneficial, many customers are priced out of\nusing an APS due to the ongoing costs of consultants responsible for\ncustomization and maintenance. To address the need for a more accessible APS\nexpressed by supply chain planners, we present SmartAPS, a conversational\nsystem built on a tool-augmented LLM. Our system provides operations planners\nwith an intuitive natural language chat interface, allowing them to query\ninformation, perform counterfactual reasoning, receive recommendations, and\nexecute scenario analysis to better manage their operation. A short video\ndemonstrating the system has been released: https://youtu.be/KtIrJjlDbyw"}
{"id": "2507.18206", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18206", "abs": "https://arxiv.org/abs/2507.18206", "authors": ["Arup Kumar Sahoo", "Itzik Klein"], "title": "MoRPI-PINN: A Physics-Informed Framework for Mobile Robot Pure Inertial Navigation", "comment": "9 pages, 5 figures", "summary": "A fundamental requirement for full autonomy in mobile robots is accurate\nnavigation even in situations where satellite navigation or cameras are\nunavailable. In such practical situations, relying only on inertial sensors\nwill result in navigation solution drift due to the sensors' inherent noise and\nerror terms. One of the emerging solutions to mitigate drift is to maneuver the\nrobot in a snake-like slithering motion to increase the inertial\nsignal-to-noise ratio, allowing the regression of the mobile robot position. In\nthis work, we propose MoRPI-PINN as a physics-informed neural network framework\nfor accurate inertial-based mobile robot navigation. By embedding physical laws\nand constraints into the training process, MoRPI-PINN is capable of providing\nan accurate and robust navigation solution. Using real-world experiments, we\nshow accuracy improvements of over 85% compared to other approaches. MoRPI-PINN\nis a lightweight approach that can be implemented even on edge devices and used\nin any typical mobile robot application."}
{"id": "2507.17988", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17988", "abs": "https://arxiv.org/abs/2507.17988", "authors": ["Dario Della Monica", "Angelo Montanari", "Pietro Sala"], "title": "Synthesis of timeline-based planning strategies avoiding determinization", "comment": "arXiv admin note: text overlap with arXiv:2410.22757", "summary": "Qualitative timeline-based planning models domains as sets of independent,\nbut\n  interacting, components whose behaviors over time, the timelines, are\ngoverned\n  by sets of qualitative temporal constraints (ordering relations), called\n  synchronization rules.\n  Its plan-existence problem has been shown to be PSPACE-complete; in\n  particular, PSPACE-membership has been proved via reduction to the\n  nonemptiness problem for nondeterministic finite automata.\n  However, nondeterministic automata cannot be directly used to synthesize\n  planning strategies as a costly determinization step is needed.\n  In this paper, we identify a fragment of qualitative timeline-based planning\n  whose plan-existence problem can be directly mapped into the nonemptiness\n  problem of deterministic finite automata, which can then\n  synthesize strategies.\n  In addition, we identify a maximal subset of Allen's relations that fits into\n  such a deterministic fragment."}
{"id": "2507.18248", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18248", "abs": "https://arxiv.org/abs/2507.18248", "authors": ["Ines Frajtag", "Marko Å vaco", "Filip Å uligoj"], "title": "Evaluation of facial landmark localization performance in a surgical setting", "comment": null, "summary": "The use of robotics, computer vision, and their applications is becoming\nincreasingly widespread in various fields, including medicine. Many face\ndetection algorithms have found applications in neurosurgery, ophthalmology,\nand plastic surgery. A common challenge in using these algorithms is variable\nlighting conditions and the flexibility of detection positions to identify and\nprecisely localize patients. The proposed experiment tests the MediaPipe\nalgorithm for detecting facial landmarks in a controlled setting, using a\nrobotic arm that automatically adjusts positions while the surgical light and\nthe phantom remain in a fixed position. The results of this study demonstrate\nthat the improved accuracy of facial landmark detection under surgical lighting\nsignificantly enhances the detection performance at larger yaw and pitch\nangles. The increase in standard deviation/dispersion occurs due to imprecise\ndetection of selected facial landmarks. This analysis allows for a discussion\non the potential integration of the MediaPipe algorithm into medical\nprocedures."}
{"id": "2507.18004", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18004", "abs": "https://arxiv.org/abs/2507.18004", "authors": ["Yusen Peng", "Shuhua Mao"], "title": "E.A.R.T.H.: Structuring Creative Evolution through Model Error in Generative AI", "comment": "44 pages,11 figures", "summary": "How can AI move beyond imitation toward genuine creativity? This paper\nproposes the E.A.R.T.H. framework, a five-stage generative pipeline that\ntransforms model-generated errors into creative assets through Error\ngeneration, Amplification, Refine selection, Transform, and Harness feedback.\nDrawing on cognitive science and generative modeling, we posit that \"creative\npotential hides in failure\" and operationalize this via structured prompts,\nsemantic scoring, and human-in-the-loop evaluation. Implemented using\nLLaMA-2-7B-Chat, SBERT, BERTScore, CLIP, BLIP-2, and Stable Diffusion, the\npipeline employs a composite reward function based on novelty, surprise, and\nrelevance. At the Refine stage, creativity scores increase by 52.5% (1.179 to\n1.898, t = -5.56, p < 0.001), with final outputs reaching 2.010 - a 70.4%\nimprovement. Refined slogans are 48.4% shorter, 40.7% more novel, with only a\n4.0% drop in relevance. Cross-modal tests show strong slogan-to-image alignment\n(CLIPScore: 0.249; BERTScore F1: 0.816). In human evaluations, 60% of outputs\nscored >= 4.0, with metaphorical slogans (avg. 4.09) outperforming literal ones\n(3.99). Feedback highlights stylistic precision and emotional resonance. These\nresults demonstrate that error-centered, feedback-driven generation enhances\ncreativity, offering a scalable path toward self-evolving, human-aligned\ncreative AI."}
{"id": "2507.18262", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.18262", "abs": "https://arxiv.org/abs/2507.18262", "authors": ["Chenyu Su", "Weiwei Shang", "Chen Qian", "Fei Zhang", "Shuang Cong"], "title": "ReSem3D: Refinable 3D Spatial Constraints via Fine-Grained Semantic Grounding for Generalizable Robotic Manipulation", "comment": "12 pages,9 figures", "summary": "Semantics-driven 3D spatial constraints align highlevel semantic\nrepresentations with low-level action spaces, facilitating the unification of\ntask understanding and execution in robotic manipulation. The synergistic\nreasoning of Multimodal Large Language Models (MLLMs) and Vision Foundation\nModels (VFMs) enables cross-modal 3D spatial constraint construction.\nNevertheless, existing methods have three key limitations: (1) coarse semantic\ngranularity in constraint modeling, (2) lack of real-time closed-loop planning,\n(3) compromised robustness in semantically diverse environments. To address\nthese challenges, we propose ReSem3D, a unified manipulation framework for\nsemantically diverse environments, leveraging the synergy between VFMs and\nMLLMs to achieve fine-grained visual grounding and dynamically constructs\nhierarchical 3D spatial constraints for real-time manipulation. Specifically,\nthe framework is driven by hierarchical recursive reasoning in MLLMs, which\ninteract with VFMs to automatically construct 3D spatial constraints from\nnatural language instructions and RGB-D observations in two stages: part-level\nextraction and region-level refinement. Subsequently, these constraints are\nencoded as real-time optimization objectives in joint space, enabling reactive\nbehavior to dynamic disturbances. Extensive simulation and real-world\nexperiments are conducted in semantically rich household and sparse chemical\nlab environments. The results demonstrate that ReSem3D performs diverse\nmanipulation tasks under zero-shot conditions, exhibiting strong adaptability\nand generalization. Code and videos at https://resem3d.github.io."}
{"id": "2507.18022", "categories": ["cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.18022", "abs": "https://arxiv.org/abs/2507.18022", "authors": ["Victoria R. Li", "Johnathan Sun", "Martin Wattenberg"], "title": "Does visualization help AI understand data?", "comment": "5 pages, 6 figures", "summary": "Charts and graphs help people analyze data, but can they also be useful to AI\nsystems? To investigate this question, we perform a series of experiments with\ntwo commercial vision-language models: GPT 4.1 and Claude 3.5. Across three\nrepresentative analysis tasks, the two systems describe synthetic datasets more\nprecisely and accurately when raw data is accompanied by a scatterplot,\nespecially as datasets grow in complexity. Comparison with two baselines --\nproviding a blank chart and a chart with mismatched data -- shows that the\nimproved performance is due to the content of the charts. Our results are\ninitial evidence that AI systems, like humans, can benefit from visualization."}
{"id": "2507.18276", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18276", "abs": "https://arxiv.org/abs/2507.18276", "authors": ["Xiaojie Zhang", "Yuanfei Wang", "Ruihai Wu", "Kunqi Xu", "Yu Li", "Liuyu Xiang", "Hao Dong", "Zhaofeng He"], "title": "Adaptive Articulated Object Manipulation On The Fly with Foundation Model Reasoning and Part Grounding", "comment": "ICCV 2025", "summary": "Articulated objects pose diverse manipulation challenges for robots. Since\ntheir internal structures are not directly observable, robots must adaptively\nexplore and refine actions to generate successful manipulation trajectories.\nWhile existing works have attempted cross-category generalization in adaptive\narticulated object manipulation, two major challenges persist: (1) the\ngeometric diversity of real-world articulated objects complicates visual\nperception and understanding, and (2) variations in object functions and\nmechanisms hinder the development of a unified adaptive manipulation strategy.\nTo address these challenges, we propose AdaRPG, a novel framework that\nleverages foundation models to extract object parts, which exhibit greater\nlocal geometric similarity than entire objects, thereby enhancing visual\naffordance generalization for functional primitive skills. To support this, we\nconstruct a part-level affordance annotation dataset to train the affordance\nmodel. Additionally, AdaRPG utilizes the common knowledge embedded in\nfoundation models to reason about complex mechanisms and generate high-level\ncontrol codes that invoke primitive skill functions based on part affordance\ninference. Simulation and real-world experiments demonstrate AdaRPG's strong\ngeneralization ability across novel articulated object categories."}
{"id": "2507.18059", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.18059", "abs": "https://arxiv.org/abs/2507.18059", "authors": ["Yueheng Li", "Guangming Xie", "Zongqing Lu"], "title": "Multi-Agent Guided Policy Optimization", "comment": null, "summary": "Due to practical constraints such as partial observability and limited\ncommunication, Centralized Training with Decentralized Execution (CTDE) has\nbecome the dominant paradigm in cooperative Multi-Agent Reinforcement Learning\n(MARL). However, existing CTDE methods often underutilize centralized training\nor lack theoretical guarantees. We propose Multi-Agent Guided Policy\nOptimization (MAGPO), a novel framework that better leverages centralized\ntraining by integrating centralized guidance with decentralized execution.\nMAGPO uses an auto-regressive joint policy for scalable, coordinated\nexploration and explicitly aligns it with decentralized policies to ensure\ndeployability under partial observability. We provide theoretical guarantees of\nmonotonic policy improvement and empirically evaluate MAGPO on 43 tasks across\n6 diverse environments. Results show that MAGPO consistently outperforms strong\nCTDE baselines and matches or surpasses fully centralized approaches, offering\na principled and practical solution for decentralized multi-agent learning. Our\ncode and experimental data can be found in https://github.com/liyheng/MAGPO."}
{"id": "2507.18317", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.18317", "abs": "https://arxiv.org/abs/2507.18317", "authors": ["Chenglong Qian", "Yang Xu", "Xiufang Shi", "Jiming Chen", "Liang Li"], "title": "AF-RLIO: Adaptive Fusion of Radar-LiDAR-Inertial Information for Robust Odometry in Challenging Environments", "comment": null, "summary": "In robotic navigation, maintaining precise pose estimation and navigation in\ncomplex and dynamic environments is crucial. However, environmental challenges\nsuch as smoke, tunnels, and adverse weather can significantly degrade the\nperformance of single-sensor systems like LiDAR or GPS, compromising the\noverall stability and safety of autonomous robots. To address these challenges,\nwe propose AF-RLIO: an adaptive fusion approach that integrates 4D\nmillimeter-wave radar, LiDAR, inertial measurement unit (IMU), and GPS to\nleverage the complementary strengths of these sensors for robust odometry\nestimation in complex environments. Our method consists of three key modules.\nFirstly, the pre-processing module utilizes radar data to assist LiDAR in\nremoving dynamic points and determining when environmental conditions are\ndegraded for LiDAR. Secondly, the dynamic-aware multimodal odometry selects\nappropriate point cloud data for scan-to-map matching and tightly couples it\nwith the IMU using the Iterative Error State Kalman Filter. Lastly, the factor\ngraph optimization module balances weights between odometry and GPS data,\nconstructing a pose graph for optimization. The proposed approach has been\nevaluated on datasets and tested in real-world robotic environments,\ndemonstrating its effectiveness and advantages over existing methods in\nchallenging conditions such as smoke and tunnels."}
{"id": "2507.18074", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18074", "abs": "https://arxiv.org/abs/2507.18074", "authors": ["Yixiu Liu", "Yang Nan", "Weixian Xu", "Xiangkun Hu", "Lyumanshan Ye", "Zhen Qin", "Pengfei Liu"], "title": "AlphaGo Moment for Model Architecture Discovery", "comment": null, "summary": "While AI systems demonstrate exponentially improving capabilities, the pace\nof AI research itself remains linearly bounded by human cognitive capacity,\ncreating an increasingly severe development bottleneck. We present ASI-Arch,\nthe first demonstration of Artificial Superintelligence for AI research\n(ASI4AI) in the critical domain of neural architecture discovery--a fully\nautonomous system that shatters this fundamental constraint by enabling AI to\nconduct its own architectural innovation. Moving beyond traditional Neural\nArchitecture Search (NAS), which is fundamentally limited to exploring\nhuman-defined spaces, we introduce a paradigm shift from automated optimization\nto automated innovation. ASI-Arch can conduct end-to-end scientific research in\nthe domain of architecture discovery, autonomously hypothesizing novel\narchitectural concepts, implementing them as executable code, training and\nempirically validating their performance through rigorous experimentation and\npast experience. ASI-Arch conducted 1,773 autonomous experiments over 20,000\nGPU hours, culminating in the discovery of 106 innovative, state-of-the-art\n(SOTA) linear attention architectures. Like AlphaGo's Move 37 that revealed\nunexpected strategic insights invisible to human players, our AI-discovered\narchitectures demonstrate emergent design principles that systematically\nsurpass human-designed baselines and illuminate previously unknown pathways for\narchitectural innovation. Crucially, we establish the first empirical scaling\nlaw for scientific discovery itself--demonstrating that architectural\nbreakthroughs can be scaled computationally, transforming research progress\nfrom a human-limited to a computation-scalable process. We provide\ncomprehensive analysis of the emergent design patterns and autonomous research\ncapabilities that enabled these breakthroughs, establishing a blueprint for\nself-accelerating AI systems."}
{"id": "2507.18344", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.18344", "abs": "https://arxiv.org/abs/2507.18344", "authors": ["Gyuhyeon Pak", "Hae Min Cho", "Euntai Kim"], "title": "G2S-ICP SLAM: Geometry-aware Gaussian Splatting ICP SLAM", "comment": "8 pages, 6 figures", "summary": "In this paper, we present a novel geometry-aware RGB-D Gaussian Splatting\nSLAM system, named G2S-ICP SLAM. The proposed method performs high-fidelity 3D\nreconstruction and robust camera pose tracking in real-time by representing\neach scene element using a Gaussian distribution constrained to the local\ntangent plane. This effectively models the local surface as a 2D Gaussian disk\naligned with the underlying geometry, leading to more consistent depth\ninterpretation across multiple viewpoints compared to conventional 3D\nellipsoid-based representations with isotropic uncertainty. To integrate this\nrepresentation into the SLAM pipeline, we embed the surface-aligned Gaussian\ndisks into a Generalized ICP framework by introducing anisotropic covariance\nprior without altering the underlying registration formulation. Furthermore we\npropose a geometry-aware loss that supervises photometric, depth, and normal\nconsistency. Our system achieves real-time operation while preserving both\nvisual and geometric fidelity. Extensive experiments on the Replica and\nTUM-RGBD datasets demonstrate that G2S-ICP SLAM outperforms prior SLAM systems\nin terms of localization accuracy, reconstruction completeness, while\nmaintaining the rendering quality."}
{"id": "2507.18115", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.ET", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.18115", "abs": "https://arxiv.org/abs/2507.18115", "authors": ["Soorya Ram Shimgekar", "Shayan Vassef", "Abhay Goyal", "Navin Kumar", "Koustuv Saha"], "title": "Agentic AI framework for End-to-End Medical Data Inference", "comment": "10 pages, 5 figures, 2 tables, BIBM conference", "summary": "Building and deploying machine learning solutions in healthcare remains\nexpensive and labor-intensive due to fragmented preprocessing workflows, model\ncompatibility issues, and stringent data privacy constraints. In this work, we\nintroduce an Agentic AI framework that automates the entire clinical data\npipeline, from ingestion to inference, through a system of modular,\ntask-specific agents. These agents handle both structured and unstructured\ndata, enabling automatic feature selection, model selection, and preprocessing\nrecommendation without manual intervention. We evaluate the system on publicly\navailable datasets from geriatrics, palliative care, and colonoscopy imaging.\nFor example, in the case of structured data (anxiety data) and unstructured\ndata (colonoscopy polyps data), the pipeline begins with file-type detection by\nthe Ingestion Identifier Agent, followed by the Data Anonymizer Agent ensuring\nprivacy compliance, where we first identify the data type and then anonymize\nit. The Feature Extraction Agent identifies features using an embedding-based\napproach for tabular data, extracting all column names, and a multi-stage\nMedGemma-based approach for image data, which infers modality and disease name.\nThese features guide the Model-Data Feature Matcher Agent in selecting the\nbest-fit model from a curated repository. The Preprocessing Recommender Agent\nand Preprocessing Implementor Agent then apply tailored preprocessing based on\ndata type and model requirements. Finally, the ``Model Inference Agent\" runs\nthe selected model on the uploaded data and generates interpretable outputs\nusing tools like SHAP, LIME, and DETR attention maps. By automating these\nhigh-friction stages of the ML lifecycle, the proposed framework reduces the\nneed for repeated expert intervention, offering a scalable, cost-efficient\npathway for operationalizing AI in clinical environments."}
{"id": "2507.18396", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.18396", "abs": "https://arxiv.org/abs/2507.18396", "authors": ["Yonghao Fu", "Cheng Hu", "Haokun Xiong", "Zhangpeng Bao", "Wenyuan Du", "Edoardo Ghignone", "Michele Magno", "Lei Xie", "Hongye Su"], "title": "Residual Koopman Model Predictive Control for Enhanced Vehicle Dynamics with Small On-Track Data Input", "comment": null, "summary": "In vehicle trajectory tracking tasks, the simplest approach is the Pure\nPursuit (PP) Control. However, this single-point preview tracking strategy\nfails to consider vehicle model constraints, compromising driving safety. Model\nPredictive Control (MPC) as a widely adopted control method, optimizes control\nactions by incorporating mechanistic models and physical constraints. While its\ncontrol performance critically depends on the accuracy of vehicle modeling.\nTraditional vehicle modeling approaches face inherent trade-offs between\ncapturing nonlinear dynamics and maintaining computational efficiency, often\nresulting in reduced control performance. To address these challenges, this\npaper proposes Residual Koopman Model Predictive Control (RKMPC) framework.\nThis method uses two linear MPC architecture to calculate control inputs: a\nLinear Model Predictive Control (LMPC) computes the baseline control input\nbased on the vehicle kinematic model, and a neural network-based RKMPC\ncalculates the compensation input. The final control command is obtained by\nadding these two components. This design preserves the reliability and\ninterpretability of traditional mechanistic model while achieving performance\noptimization through residual modeling. This method has been validated on the\nCarsim-Matlab joint simulation platform and a physical 1:10 scale F1TENTH\nracing car. Experimental results show that RKMPC requires only 20% of the\ntraining data needed by traditional Koopman Model Predictive Control (KMPC)\nwhile delivering superior tracking performance. Compared to traditional LMPC,\nRKMPC reduces lateral error by 11.7%-22.1%, decreases heading error by\n8.9%-15.8%, and improves front-wheel steering stability by up to 27.6%. The\nimplementation code is available at: https://github.com/ZJU-DDRX/Residual\nKoopman."}
{"id": "2507.18123", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.18123", "abs": "https://arxiv.org/abs/2507.18123", "authors": ["Sedigh Khademi", "Christopher Palmer", "Muhammad Javed", "Hazel Clothier", "Jim Buttery", "Gerardo Luis Dimaguila", "Jim Black"], "title": "Actively evaluating and learning the distinctions that matter: Vaccine safety signal detection from emergency triage notes", "comment": "14 pages", "summary": "The rapid development of COVID-19 vaccines has showcased the global\ncommunitys ability to combat infectious diseases. However, the need for\npost-licensure surveillance systems has grown due to the limited window for\nsafety data collection in clinical trials and early widespread implementation.\nThis study aims to employ Natural Language Processing techniques and Active\nLearning to rapidly develop a classifier that detects potential vaccine safety\nissues from emergency department notes. ED triage notes, containing expert,\nsuccinct vital patient information at the point of entry to health systems, can\nsignificantly contribute to timely vaccine safety signal surveillance. While\nkeyword-based classification can be effective, it may yield false positives and\ndemand extensive keyword modifications. This is exacerbated by the infrequency\nof vaccination-related ED presentations and their similarity to other reasons\nfor ED visits. NLP offers a more accurate and efficient alternative, albeit\nrequiring annotated data, which is often scarce in the medical field. Active\nlearning optimizes the annotation process and the quality of annotated data,\nwhich can result in faster model implementation and improved model performance.\nThis work combines active learning, data augmentation, and active learning and\nevaluation techniques to create a classifier that is used to enhance vaccine\nsafety surveillance from ED triage notes."}
{"id": "2507.18436", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.18436", "abs": "https://arxiv.org/abs/2507.18436", "authors": ["David Blanco-Mulero", "JÃºlia BorrÃ s", "Carme Torras"], "title": "Evaluating the Pre-Dressing Step: Unfolding Medical Garments Via Imitation Learning", "comment": "6 pages, 4 figures, 2 tables. Accepted to IEEE/RSJ IROS 2025. Project\n  website: https://sites.google.com/view/pre-dressing", "summary": "Robotic-assisted dressing has the potential to significantly aid both\npatients as well as healthcare personnel, reducing the workload and improving\nthe efficiency in clinical settings. While substantial progress has been made\nin robotic dressing assistance, prior works typically assume that garments are\nalready unfolded and ready for use. However, in medical applications gowns and\naprons are often stored in a folded configuration, requiring an additional\nunfolding step. In this paper, we introduce the pre-dressing step, the process\nof unfolding garments prior to assisted dressing. We leverage imitation\nlearning for learning three manipulation primitives, including both high and\nlow acceleration motions. In addition, we employ a visual classifier to\ncategorise the garment state as closed, partly opened, and fully opened. We\nconduct an empirical evaluation of the learned manipulation primitives as well\nas their combinations. Our results show that highly dynamic motions are not\neffective for unfolding freshly unpacked garments, where the combination of\nmotions can efficiently enhance the opening configuration."}
{"id": "2507.18145", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.18145", "abs": "https://arxiv.org/abs/2507.18145", "authors": ["Moritz SchÃ¶nherr", "Carsten Lutz"], "title": "Logical Characterizations of GNNs with Mean Aggregation", "comment": null, "summary": "We study the expressive power of graph neural networks (GNNs) with mean as\nthe aggregation function. In the non-uniform setting, we show that such GNNs\nhave exactly the same expressive power as ratio modal logic, which has modal\noperators expressing that at least a certain ratio of the successors of a\nvertex satisfies a specified property. The non-uniform expressive power of mean\nGNNs is thus higher than that of GNNs with max aggregation, but lower than for\nsum aggregation--the latter are characterized by modal logic and graded modal\nlogic, respectively. In the uniform setting, we show that the expressive power\nrelative to MSO is exactly that of alternation-free modal logic, under the\nnatural assumptions that combination functions are continuous and\nclassification functions are thresholds. This implies that, relative to MSO and\nin the uniform setting, mean GNNs are strictly less expressive than sum GNNs\nand max GNNs. When any of the assumptions is dropped, the expressive power\nincreases."}
{"id": "2507.18462", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.18462", "abs": "https://arxiv.org/abs/2507.18462", "authors": ["Alghalya Al-Hajri", "Ejmen Al-Ubejdij", "Aiman Erbad", "Ali Safa"], "title": "A Novel Monte-Carlo Compressed Sensing and Dictionary Learning Method for the Efficient Path Planning of Remote Sensing Robots", "comment": null, "summary": "In recent years, Compressed Sensing (CS) has gained significant interest as a\ntechnique for acquiring high-resolution sensory data using fewer measurements\nthan traditional Nyquist sampling requires. At the same time, autonomous\nrobotic platforms such as drones and rovers have become increasingly popular\ntools for remote sensing and environmental monitoring tasks, including\nmeasurements of temperature, humidity, and air quality. Within this context,\nthis paper presents, to the best of our knowledge, the first investigation into\nhow the structure of CS measurement matrices can be exploited to design\noptimized sampling trajectories for robotic environmental data collection. We\npropose a novel Monte Carlo optimization framework that generates measurement\nmatrices designed to minimize both the robot's traversal path length and the\nsignal reconstruction error within the CS framework. Central to our approach is\nthe application of Dictionary Learning (DL) to obtain a data-driven sparsifying\ntransform, which enhances reconstruction accuracy while further reducing the\nnumber of samples that the robot needs to collect. We demonstrate the\neffectiveness of our method through experiments reconstructing $NO_2$ pollution\nmaps over the Gulf region. The results indicate that our approach can reduce\nrobot travel distance to less than $10\\%$ of a full-coverage path, while\nimproving reconstruction accuracy by over a factor of five compared to\ntraditional CS methods based on DCT and polynomial dictionaries, as well as by\na factor of two compared to previously-proposed Informative Path Planning (IPP)\nmethods."}
{"id": "2507.18178", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18178", "abs": "https://arxiv.org/abs/2507.18178", "authors": ["Mutian Yang", "Jiandong Gao", "Ji Wu"], "title": "Decoupling Knowledge and Reasoning in LLMs: An Exploration Using Cognitive Dual-System Theory", "comment": null, "summary": "While large language models (LLMs) leverage both knowledge and reasoning\nduring inference, the capacity to distinguish between them plays a pivotal role\nin model analysis, interpretability, and development. Inspired by dual-system\ncognitive theory, we propose a cognition attribution framework to decouple the\ncontribution of knowledge and reasoning. In particular, the cognition of LLMs\nis decomposed into two distinct yet complementary phases: knowledge retrieval\n(Phase 1) and reasoning adjustment (Phase 2). To separate these phases, LLMs\nare prompted to generate answers under two different cognitive modes, fast\nthinking and slow thinking, respectively. The performance under different\ncognitive modes is analyzed to quantify the contribution of knowledge and\nreasoning. This architecture is employed to 15 LLMs across 3 datasets. Results\nreveal: (1) reasoning adjustment is domain-specific, benefiting\nreasoning-intensive domains (e.g., mathematics, physics, and chemistry) and\npotentially imparing knowledge-intensive domains. (2) Parameter scaling\nimproves both knowledge and reasoning, with knowledge improvements being more\npronounced. Additionally, parameter scaling make LLMs reasoning significantly\nmore prudent, while moderately more intelligent. (3) Knowledge primarily\nresides in lower network layers, while reasoning operates in higher layers. Our\nframework not only helps understand LLMs from a \"decoupling\" perspective, but\nalso provides new insights into existing research, including scaling laws,\nhierarchical knowledge editing, and limitations of small-model reasoning."}
{"id": "2507.18502", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.18502", "abs": "https://arxiv.org/abs/2507.18502", "authors": ["Sait Sovukluk", "Grazia Zambella", "Tobias Egle", "Christian Ott"], "title": "Experimental Comparison of Whole-Body Control Formulations for Humanoid Robots in Task Acceleration and Task Force Spaces", "comment": "This paper has been accepted for publication in 2025 IEEE/RSJ\n  International Conference on Intelligent Robots and Systems (IROS 2025). -\n  Link to video: https://youtu.be/Nfm50ycz-FU", "summary": "This paper studies the experimental comparison of two different whole-body\ncontrol formulations for humanoid robots: inverse dynamics whole-body control\n(ID-WBC) and passivity-based whole-body control (PB-WBC). The two controllers\nfundamentally differ from each other as the first is formulated in task\nacceleration space and the latter is in task force space with passivity\nconsiderations. Even though both control methods predict stability under ideal\nconditions in closed-loop dynamics, their robustness against joint friction,\nsensor noise, unmodeled external disturbances, and non-perfect contact\nconditions is not evident. Therefore, we analyze and experimentally compare the\ntwo controllers on a humanoid robot platform through swing foot position and\norientation control, squatting with and without unmodeled additional weights,\nand jumping. We also relate the observed performance and characteristic\ndifferences with the controller formulations and highlight each controller's\nadvantages and disadvantages."}
{"id": "2507.18198", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18198", "abs": "https://arxiv.org/abs/2507.18198", "authors": ["Felicidad Aguado", "Pedro Cabalar", "Brais MuÃ±iz", "Gilberto PÃ©rez", "ConcepciÃ³n Vidal"], "title": "Comparing Non-minimal Semantics for Disjunction in Answer Set Programming", "comment": null, "summary": "In this paper, we compare four different semantics for disjunction in Answer\nSet Programming that, unlike stable models, do not adhere to the principle of\nmodel minimality. Two of these approaches, Cabalar and Mu\\~niz' \\emph{Justified\nModels} and Doherty and Szalas' \\emph{Strongly Supported Models}, directly\nprovide an alternative non-minimal semantics for disjunction. The other two,\nAguado et al's \\emph{Forks} and Shen and Eiter's \\emph{Determining Inference}\n(DI) semantics, actually introduce a new disjunction connective, but are\ncompared here as if they constituted new semantics for the standard disjunction\noperator. We are able to prove that three of these approaches (Forks, Justified\nModels and a reasonable relaxation of the DI semantics) actually coincide,\nconstituting a common single approach under different definitions. Moreover,\nthis common semantics always provides a superset of the stable models of a\nprogram (in fact, modulo any context) and is strictly stronger than the fourth\napproach (Strongly Supported Models), that actually treats disjunctions as in\nclassical logic."}
{"id": "2507.18290", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18290", "abs": "https://arxiv.org/abs/2507.18290", "authors": ["Antonino Rotolo", "Beatrice Ferrigno", "Jose Miguel Angel Garcia Godinez", "Claudio Novelli", "Giovanni Sartor"], "title": "Foundations for Risk Assessment of AI in Protecting Fundamental Rights", "comment": "24 pages, 1 figure. To be published in: The Philosophical Foundations\n  of Information Technology Law. Oxford University Press, Oxford", "summary": "This chapter introduces a conceptual framework for qualitative risk\nassessment of AI, particularly in the context of the EU AI Act. The framework\naddresses the complexities of legal compliance and fundamental rights\nprotection by itegrating definitional balancing and defeasible reasoning.\nDefinitional balancing employs proportionality analysis to resolve conflicts\nbetween competing rights, while defeasible reasoning accommodates the dynamic\nnature of legal decision-making. Our approach stresses the need for an analysis\nof AI deployment scenarios and for identifying potential legal violations and\nmulti-layered impacts on fundamental rights. On the basis of this analysis, we\nprovide philosophical foundations for a logical account of AI risk analysis. In\nparticular, we consider the basic building blocks for conceptually grasping the\ninteraction between AI deployment scenarios and fundamental rights,\nincorporating in defeasible reasoning definitional balancing and arguments\nabout the contextual promotion or demotion of rights. This layered approach\nallows for more operative models of assessment of both high-risk AI systems and\nGeneral Purpose AI (GPAI) systems, emphasizing the broader applicability of the\nlatter. Future work aims to develop a formal model and effective algorithms to\nenhance AI risk assessment, bridging theoretical insights with practical\napplications to support responsible AI governance."}
{"id": "2507.18337", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18337", "abs": "https://arxiv.org/abs/2507.18337", "authors": ["Peter Baumgartner", "Lachlan McGinness"], "title": "The AlphaPhysics Term Rewriting System for Marking Algebraic Expressions in Physics Exams", "comment": null, "summary": "We present our method for automatically marking Physics exams. The marking\nproblem consists in assessing typed student answers for correctness with\nrespect to a ground truth solution. This is a challenging problem that we seek\nto tackle using a combination of a computer algebra system, an SMT solver and a\nterm rewriting system. A Large Language Model is used to interpret and remove\nerrors from student responses and rewrite these in a machine readable format.\nOnce formalized and language-aligned, the next step then consists in applying\nautomated reasoning techniques for assessing student solution correctness. We\nconsider two methods of automated theorem proving: off-the-shelf SMT solving\nand term rewriting systems tailored for physics problems involving\ntrigonometric expressions. The development of the term rewrite system and\nestablishing termination and confluence properties was not trivial, and we\ndescribe it in some detail in the paper. We evaluate our system on a rich pool\nof over 1500 real-world student exam responses from the 2023 Australian Physics\nOlympiad."}
{"id": "2507.18368", "categories": ["cs.AI", "I.2.0; I.2.6; J.4"], "pdf": "https://arxiv.org/pdf/2507.18368", "abs": "https://arxiv.org/abs/2507.18368", "authors": ["Zhuang Qiang Bok", "Watson Wei Khong Chua"], "title": "Reasoning Beyond the Obvious: Evaluating Divergent and Convergent Thinking in LLMs for Financial Scenarios", "comment": "Accepted by Agentic & GenAI Evaluation KDD2025: KDD workshop on\n  Evaluation and Trustworthiness of Agentic and Generative AI Models\n  https://kdd-eval-workshop.github.io/genai-evaluation-kdd2025/", "summary": "Most reasoning benchmarks for LLMs emphasize factual accuracy or step-by-step\nlogic. In finance, however, professionals must not only converge on optimal\ndecisions but also generate creative, plausible futures under uncertainty. We\nintroduce ConDiFi, a benchmark that jointly evaluates divergent and convergent\nthinking in LLMs for financial tasks.\n  ConDiFi features 607 macro-financial prompts for divergent reasoning and 990\nmulti-hop adversarial MCQs for convergent reasoning. Using this benchmark, we\nevaluated 14 leading models and uncovered striking differences. Despite high\nfluency, GPT-4o underperforms on Novelty and Actionability. In contrast, models\nlike DeepSeek-R1 and Cohere Command R+ rank among the top for generating\nactionable, insights suitable for investment decisions. ConDiFi provides a new\nperspective to assess reasoning capabilities essential to safe and strategic\ndeployment of LLMs in finance."}
{"id": "2507.18391", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18391", "abs": "https://arxiv.org/abs/2507.18391", "authors": ["Shiye Lei", "Zhihao Cheng", "Kai Jia", "Dacheng Tao"], "title": "Revisiting LLM Reasoning via Information Bottleneck", "comment": null, "summary": "Large language models (LLMs) have recently demonstrated remarkable progress\nin reasoning capabilities through reinforcement learning with verifiable\nrewards (RLVR). By leveraging simple rule-based rewards, RL effectively\nincentivizes LLMs to produce extended chain-of-thought (CoT) reasoning\ntrajectories, progressively guiding them toward correct answers. However,\nexisting approaches remain largely heuristic and intuition-driven, limiting the\ndevelopment of principled methodologies. In this paper, we present a\ntheoretical characterization of LLM reasoning grounded in information\nbottleneck (IB) principle, introducing IB-aware reasoning optimization (IBRO),\na framework that encourages reasoning trajectories to be both informative about\nthe final correct answer and generalizable across diverse prompts. We derive a\npractical token-level surrogate objective and propose an efficient\napproximation, resulting in the lightweight IB regularization method. This\ntechnique integrates seamlessly into existing RL-based post-training frameworks\nwithout additional computational overhead, requiring only a one-line code\nmodification. Empirically, we validate IB regularization across multiple\nmathematical reasoning benchmarks and RL algorithms, demonstrating consistent\nimprovements in LLM reasoning performance."}
{"id": "2507.18398", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18398", "abs": "https://arxiv.org/abs/2507.18398", "authors": ["Kwong Ho Li", "Wathsala Karunarathne"], "title": "Optimising Call Centre Operations using Reinforcement Learning: Value Iteration versus Proximal Policy Optimisation", "comment": "10 pages", "summary": "This paper investigates the application of Reinforcement Learning (RL) to\noptimise call routing in call centres to minimise client waiting time and staff\nidle time. Two methods are compared: a model-based approach using Value\nIteration (VI) under known system dynamics, and a model-free approach using\nProximal Policy Optimisation (PPO) that learns from experience. For the\nmodel-based approach, a theoretical model is used, while a simulation model\ncombining Discrete Event Simulation (DES) with the OpenAI Gym environment is\ndeveloped for model-free learning. Both models frame the problem as a Markov\nDecision Process (MDP) within a Skills-Based Routing (SBR) framework, with\nPoisson client arrivals and exponentially distributed service and abandonment\ntimes. For policy evaluation, random, VI, and PPO policies are evaluated using\nthe simulation model. After 1,000 test episodes, PPO consistently achives the\nhighest rewards, along with the lowest client waiting time and staff idle time,\ndespite requiring longer training time."}
{"id": "2507.18413", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18413", "abs": "https://arxiv.org/abs/2507.18413", "authors": ["Enrico Santi", "Fabio Tardivo", "Agostino Dovier", "Andrea Formisano"], "title": "GPU Accelerated Compact-Table Propagation", "comment": "Under consideration in Theory and Practice of Logic Programming\n  (TPLP)", "summary": "Constraint Programming developed within Logic Programming in the Eighties;\nnowadays all Prolog systems encompass modules capable of handling constraint\nprogramming on finite domains demanding their solution to a constraint solver.\nThis work focuses on a specific form of constraint, the so-called table\nconstraint, used to specify conditions on the values of variables as an\nenumeration of alternative options. Since every condition on a set of finite\ndomain variables can be ultimately expressed as a finite set of cases, Table\ncan, in principle, simulate any other constraint. These characteristics make\nTable one of the most studied constraints ever, leading to a series of\nincreasingly efficient propagation algorithms. Despite this, it is not uncommon\nto encounter real-world problems with hundreds or thousands of valid cases that\nare simply too many to be handled effectively with standard CPU-based\napproaches. In this paper, we deal with the Compact-Table (CT) algorithm, the\nstate-of-the-art propagation algorithms for Table. We describe how CT can be\nenhanced by exploiting the massive computational power offered by modern GPUs\nto handle large Table constraints. In particular, we report on the design and\nimplementation of GPU-accelerated CT, on its integration into an existing\nconstraint solver, and on an experimental validation performed on a significant\nset of instances."}
{"id": "2507.18550", "categories": ["cs.AI", "cs.CV", "cs.LG", "cs.NE"], "pdf": "https://arxiv.org/pdf/2507.18550", "abs": "https://arxiv.org/abs/2507.18550", "authors": ["Manuel de Sousa Ribeiro", "Afonso Leote", "JoÃ£o Leite"], "title": "On the Performance of Concept Probing: The Influence of the Data (Extended Version)", "comment": "Extended version of the paper published in Proceedings of the\n  European Conference on Artificial Intelligence (ECAI 2025)", "summary": "Concept probing has recently garnered increasing interest as a way to help\ninterpret artificial neural networks, dealing both with their typically large\nsize and their subsymbolic nature, which ultimately renders them unfeasible for\ndirect human interpretation. Concept probing works by training additional\nclassifiers to map the internal representations of a model into human-defined\nconcepts of interest, thus allowing humans to peek inside artificial neural\nnetworks. Research on concept probing has mainly focused on the model being\nprobed or the probing model itself, paying limited attention to the data\nrequired to train such probing models. In this paper, we address this gap.\nFocusing on concept probing in the context of image classification tasks, we\ninvestigate the effect of the data used to train probing models on their\nperformance. We also make available concept labels for two widely used\ndatasets."}
{"id": "2507.18576", "categories": ["cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.18576", "abs": "https://arxiv.org/abs/2507.18576", "authors": ["Shanghai AI Lab", ":", "Yicheng Bao", "Guanxu Chen", "Mingkang Chen", "Yunhao Chen", "Chiyu Chen", "Lingjie Chen", "Sirui Chen", "Xinquan Chen", "Jie Cheng", "Yu Cheng", "Dengke Deng", "Yizhuo Ding", "Dan Ding", "Xiaoshan Ding", "Yi Ding", "Zhichen Dong", "Lingxiao Du", "Yuyu Fan", "Xinshun Feng", "Yanwei Fu", "Yuxuan Gao", "Ruijun Ge", "Tianle Gu", "Lujun Gui", "Jiaxuan Guo", "Qianxi He", "Yuenan Hou", "Xuhao Hu", "Hong Huang", "Kaichen Huang", "Shiyang Huang", "Yuxian Jiang", "Shanzhe Lei", "Jie Li", "Lijun Li", "Hao Li", "Juncheng Li", "Xiangtian Li", "Yafu Li", "Lingyu Li", "Xueyan Li", "Haotian Liang", "Dongrui Liu", "Qihua Liu", "Zhixuan Liu", "Bangwei Liu", "Huacan Liu", "Yuexiao Liu", "Zongkai Liu", "Chaochao Lu", "Yudong Lu", "Xiaoya Lu", "Zhenghao Lu", "Qitan Lv", "Caoyuan Ma", "Jiachen Ma", "Xiaoya Ma", "Zhongtian Ma", "Lingyu Meng", "Ziqi Miao", "Yazhe Niu", "Yuezhang Peng", "Yuan Pu", "Han Qi", "Chen Qian", "Xingge Qiao", "Jingjing Qu", "Jiashu Qu", "Wanying Qu", "Wenwen Qu", "Xiaoye Qu", "Qihan Ren", "Qingnan Ren", "Qingyu Ren", "Jing Shao", "Wenqi Shao", "Shuai Shao", "Dongxing Shi", "Xin Song", "Xinhao Song", "Yan Teng", "Xuan Tong", "Yingchun Wang", "Xuhong Wang", "Shujie Wang", "Xin Wang", "Yige Wang", "Yixu Wang", "Yuanfu Wang", "Futing Wang", "Ruofan Wang", "Wenjie Wang", "Yajie Wang", "Muhao Wei", "Xiaoyu Wen", "Fenghua Weng", "Yuqi Wu", "Yingtong Xiong", "Xingcheng Xu", "Chao Yang", "Yue Yang", "Yang Yao", "Yulei Ye", "Zhenyun Yin", "Yi Yu", "Bo Zhang", "Qiaosheng Zhang", "Jinxuan Zhang", "Yexin Zhang", "Yinqiang Zheng", "Hefeng Zhou", "Zhanhui Zhou", "Pengyu Zhu", "Qingzi Zhu", "Yubo Zhu", "Bowen Zhou"], "title": "SafeWork-R1: Coevolving Safety and Intelligence under the AI-45$^{\\circ}$ Law", "comment": "47 pages, 18 figures, authors are listed in alphabetical order by\n  their last names", "summary": "We introduce SafeWork-R1, a cutting-edge multimodal reasoning model that\ndemonstrates the coevolution of capabilities and safety. It is developed by our\nproposed SafeLadder framework, which incorporates large-scale, progressive,\nsafety-oriented reinforcement learning post-training, supported by a suite of\nmulti-principled verifiers. Unlike previous alignment methods such as RLHF that\nsimply learn human preferences, SafeLadder enables SafeWork-R1 to develop\nintrinsic safety reasoning and self-reflection abilities, giving rise to safety\n`aha' moments. Notably, SafeWork-R1 achieves an average improvement of\n$46.54\\%$ over its base model Qwen2.5-VL-72B on safety-related benchmarks\nwithout compromising general capabilities, and delivers state-of-the-art safety\nperformance compared to leading proprietary models such as GPT-4.1 and Claude\nOpus 4. To further bolster its reliability, we implement two distinct\ninference-time intervention methods and a deliberative search mechanism,\nenforcing step-level verification. Finally, we further develop\nSafeWork-R1-InternVL3-78B, SafeWork-R1-DeepSeek-70B, and\nSafeWork-R1-Qwen2.5VL-7B. All resulting models demonstrate that safety and\ncapability can co-evolve synergistically, highlighting the generalizability of\nour framework in building robust, reliable, and trustworthy general-purpose AI."}
{"id": "2507.18033", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18033", "abs": "https://arxiv.org/abs/2507.18033", "authors": ["Mingfeng Yuan", "Letian Wang", "Steven L. Waslander"], "title": "OpenNav: Open-World Navigation with Multimodal Large Language Models", "comment": null, "summary": "Pre-trained large language models (LLMs) have demonstrated strong\ncommon-sense reasoning abilities, making them promising for robotic navigation\nand planning tasks. However, despite recent progress, bridging the gap between\nlanguage descriptions and actual robot actions in the open-world, beyond merely\ninvoking limited predefined motion primitives, remains an open challenge. In\nthis work, we aim to enable robots to interpret and decompose complex language\ninstructions, ultimately synthesizing a sequence of trajectory points to\ncomplete diverse navigation tasks given open-set instructions and open-set\nobjects. We observe that multi-modal large language models (MLLMs) exhibit\nstrong cross-modal understanding when processing free-form language\ninstructions, demonstrating robust scene comprehension. More importantly,\nleveraging their code-generation capability, MLLMs can interact with\nvision-language perception models to generate compositional 2D bird-eye-view\nvalue maps, effectively integrating semantic knowledge from MLLMs with spatial\ninformation from maps to reinforce the robot's spatial understanding. To\nfurther validate our approach, we effectively leverage large-scale autonomous\nvehicle datasets (AVDs) to validate our proposed zero-shot vision-language\nnavigation framework in outdoor navigation tasks, demonstrating its capability\nto execute a diverse range of free-form natural language navigation\ninstructions while maintaining robustness against object detection errors and\nlinguistic ambiguities. Furthermore, we validate our system on a Husky robot in\nboth indoor and outdoor scenes, demonstrating its real-world robustness and\napplicability. Supplementary videos are available at\nhttps://trailab.github.io/OpenNav-website/"}
{"id": "2507.18206", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.18206", "abs": "https://arxiv.org/abs/2507.18206", "authors": ["Arup Kumar Sahoo", "Itzik Klein"], "title": "MoRPI-PINN: A Physics-Informed Framework for Mobile Robot Pure Inertial Navigation", "comment": "9 pages, 5 figures", "summary": "A fundamental requirement for full autonomy in mobile robots is accurate\nnavigation even in situations where satellite navigation or cameras are\nunavailable. In such practical situations, relying only on inertial sensors\nwill result in navigation solution drift due to the sensors' inherent noise and\nerror terms. One of the emerging solutions to mitigate drift is to maneuver the\nrobot in a snake-like slithering motion to increase the inertial\nsignal-to-noise ratio, allowing the regression of the mobile robot position. In\nthis work, we propose MoRPI-PINN as a physics-informed neural network framework\nfor accurate inertial-based mobile robot navigation. By embedding physical laws\nand constraints into the training process, MoRPI-PINN is capable of providing\nan accurate and robust navigation solution. Using real-world experiments, we\nshow accuracy improvements of over 85% compared to other approaches. MoRPI-PINN\nis a lightweight approach that can be implemented even on edge devices and used\nin any typical mobile robot application."}
{"id": "2507.18262", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.18262", "abs": "https://arxiv.org/abs/2507.18262", "authors": ["Chenyu Su", "Weiwei Shang", "Chen Qian", "Fei Zhang", "Shuang Cong"], "title": "ReSem3D: Refinable 3D Spatial Constraints via Fine-Grained Semantic Grounding for Generalizable Robotic Manipulation", "comment": "12 pages,9 figures", "summary": "Semantics-driven 3D spatial constraints align highlevel semantic\nrepresentations with low-level action spaces, facilitating the unification of\ntask understanding and execution in robotic manipulation. The synergistic\nreasoning of Multimodal Large Language Models (MLLMs) and Vision Foundation\nModels (VFMs) enables cross-modal 3D spatial constraint construction.\nNevertheless, existing methods have three key limitations: (1) coarse semantic\ngranularity in constraint modeling, (2) lack of real-time closed-loop planning,\n(3) compromised robustness in semantically diverse environments. To address\nthese challenges, we propose ReSem3D, a unified manipulation framework for\nsemantically diverse environments, leveraging the synergy between VFMs and\nMLLMs to achieve fine-grained visual grounding and dynamically constructs\nhierarchical 3D spatial constraints for real-time manipulation. Specifically,\nthe framework is driven by hierarchical recursive reasoning in MLLMs, which\ninteract with VFMs to automatically construct 3D spatial constraints from\nnatural language instructions and RGB-D observations in two stages: part-level\nextraction and region-level refinement. Subsequently, these constraints are\nencoded as real-time optimization objectives in joint space, enabling reactive\nbehavior to dynamic disturbances. Extensive simulation and real-world\nexperiments are conducted in semantically rich household and sparse chemical\nlab environments. The results demonstrate that ReSem3D performs diverse\nmanipulation tasks under zero-shot conditions, exhibiting strong adaptability\nand generalization. Code and videos at https://resem3d.github.io."}
