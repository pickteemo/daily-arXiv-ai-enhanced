{"id": "2507.13455", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.13455", "abs": "https://arxiv.org/abs/2507.13455", "authors": ["Dean Chen", "Armin Pomeroy", "Brandon T. Peterson", "Will Flanagan", "He Kai Lim", "Alexandra Stavrakis", "Nelson F. SooHoo", "Jonathan B. Hopkins", "Tyler R. Clites"], "title": "Hard-Stop Synthesis for Multi-DOF Compliant Mechanisms", "comment": "42 pages, 17 figures. Under review at ASME Journal of Mechanical\n  Design", "summary": "Compliant mechanisms have significant potential in precision applications due\nto their ability to guide motion without contact. However, an inherent\nvulnerability to fatigue and mechanical failure has hindered the translation of\ncompliant mechanisms to real-world applications. This is particularly\nchallenging in service environments where loading is complex and uncertain, and\nthe cost of failure is high. In such cases, mechanical hard stops are critical\nto prevent yielding and buckling. Conventional hard-stop designs, which rely on\nstacking single-DOF limits, must be overly restrictive in multi-DOF space to\nguarantee safety in the presence of unknown loads. In this study, we present a\nsystematic design synthesis method to guarantee overload protection in\ncompliant mechanisms by integrating coupled multi-DOF motion limits within a\nsingle pair of compact hard-stop surfaces. Specifically, we introduce a\ntheoretical and practical framework for optimizing the contact surface geometry\nto maximize the mechanisms multi-DOF working space while still ensuring that\nthe mechanism remains within its elastic regime. We apply this synthesis method\nto a case study of a caged-hinge mechanism for orthopaedic implants, and\nprovide numerical and experimental validation that the derived design offers\nreliable protection against fatigue, yielding, and buckling. This work\nestablishes a foundation for precision hard-stop design in compliant systems\noperating under uncertain loads, which is a crucial step toward enabling the\napplication of compliant mechanisms in real-world systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u5316\u7684\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u96c6\u6210\u591a\u81ea\u7531\u5ea6\u8fd0\u52a8\u9650\u5236\u7684\u7d27\u51d1\u786c\u6b62\u6321\u8868\u9762\uff0c\u4e3a\u67d4\u6027\u673a\u6784\u63d0\u4f9b\u8fc7\u8f7d\u4fdd\u62a4\uff0c\u786e\u4fdd\u5176\u5728\u5f39\u6027\u8303\u56f4\u5185\u5de5\u4f5c\u3002", "motivation": "\u67d4\u6027\u673a\u6784\u5728\u7cbe\u5bc6\u5e94\u7528\u4e2d\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u5176\u75b2\u52b3\u548c\u673a\u68b0\u6545\u969c\u7684\u8106\u5f31\u6027\u963b\u788d\u4e86\u5b9e\u9645\u5e94\u7528\u3002\u590d\u6742\u548c\u4e0d\u786e\u5b9a\u7684\u8d1f\u8f7d\u73af\u5883\u5c24\u5176\u9700\u8981\u786c\u6b62\u6321\u6765\u9632\u6b62\u5c48\u670d\u548c\u5c48\u66f2\u3002", "method": "\u5f15\u5165\u7406\u8bba\u548c\u5b9e\u8df5\u6846\u67b6\uff0c\u4f18\u5316\u63a5\u89e6\u8868\u9762\u51e0\u4f55\u5f62\u72b6\uff0c\u4ee5\u6700\u5927\u5316\u673a\u6784\u7684\u591a\u81ea\u7531\u5ea6\u5de5\u4f5c\u7a7a\u95f4\uff0c\u540c\u65f6\u786e\u4fdd\u5176\u4fdd\u6301\u5728\u5f39\u6027\u8303\u56f4\u5185\u3002", "result": "\u901a\u8fc7\u6570\u503c\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u5728\u9aa8\u79d1\u690d\u5165\u7269\u7684\u7b3c\u5f0f\u94f0\u94fe\u673a\u6784\u4e2d\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u75b2\u52b3\u3001\u5c48\u670d\u548c\u5c48\u66f2\u4fdd\u62a4\u3002", "conclusion": "\u4e3a\u5728\u4e0d\u786e\u5b9a\u8d1f\u8f7d\u4e0b\u5de5\u4f5c\u7684\u67d4\u6027\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7cbe\u5bc6\u786c\u6b62\u6321\u8bbe\u8ba1\u7684\u57fa\u7840\uff0c\u63a8\u52a8\u4e86\u67d4\u6027\u673a\u6784\u5728\u5b9e\u9645\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2507.13468", "categories": ["cs.RO", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.13468", "abs": "https://arxiv.org/abs/2507.13468", "authors": ["Shiye Cao", "Maia Stiber", "Amama Mahmood", "Maria Teresa Parreira", "Wendy Ju", "Micol Spitale", "Hatice Gunes", "Chien-Ming Huang"], "title": "ERR@HRI 2.0 Challenge: Multimodal Detection of Errors and Failures in Human-Robot Conversations", "comment": null, "summary": "The integration of large language models (LLMs) into conversational robots\nhas made human-robot conversations more dynamic. Yet, LLM-powered\nconversational robots remain prone to errors, e.g., misunderstanding user\nintent, prematurely interrupting users, or failing to respond altogether.\nDetecting and addressing these failures is critical for preventing\nconversational breakdowns, avoiding task disruptions, and sustaining user\ntrust. To tackle this problem, the ERR@HRI 2.0 Challenge provides a multimodal\ndataset of LLM-powered conversational robot failures during human-robot\nconversations and encourages researchers to benchmark machine learning models\ndesigned to detect robot failures. The dataset includes 16 hours of dyadic\nhuman-robot interactions, incorporating facial, speech, and head movement\nfeatures. Each interaction is annotated with the presence or absence of robot\nerrors from the system perspective, and perceived user intention to correct for\na mismatch between robot behavior and user expectation. Participants are\ninvited to form teams and develop machine learning models that detect these\nfailures using multimodal data. Submissions will be evaluated using various\nperformance metrics, including detection accuracy and false positive rate. This\nchallenge represents another key step toward improving failure detection in\nhuman-robot interaction through social signal analysis.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86LLM\u9a71\u52a8\u7684\u5bf9\u8bdd\u673a\u5668\u4eba\u5728\u4eba\u673a\u4ea4\u4e92\u4e2d\u7684\u9519\u8bef\u68c0\u6d4b\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u6a21\u6001\u6570\u636e\u96c6\uff08ERR@HRI 2.0 Challenge\uff09\u7528\u4e8e\u6a21\u578b\u6d4b\u8bd5\u3002", "motivation": "\u5c3d\u7ba1LLM\u4f7f\u5bf9\u8bdd\u673a\u5668\u4eba\u66f4\u52a8\u6001\uff0c\u4f46\u5176\u4ecd\u6613\u51fa\u9519\uff08\u5982\u8bef\u89e3\u7528\u6237\u610f\u56fe\u6216\u4e2d\u65ad\u5bf9\u8bdd\uff09\uff0c\u9700\u68c0\u6d4b\u548c\u89e3\u51b3\u8fd9\u4e9b\u9519\u8bef\u4ee5\u7ef4\u6301\u7528\u6237\u4fe1\u4efb\u3002", "method": "\u901a\u8fc7\u63d0\u4f9b\u5305\u542b16\u5c0f\u65f6\u4eba\u673a\u4ea4\u4e92\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff08\u542b\u9762\u90e8\u3001\u8bed\u97f3\u548c\u5934\u90e8\u52a8\u4f5c\u7279\u5f81\uff09\uff0c\u9f13\u52b1\u7814\u7a76\u8005\u5f00\u53d1\u673a\u5668\u5b66\u4e60\u6a21\u578b\u68c0\u6d4b\u673a\u5668\u4eba\u9519\u8bef\u3002", "result": "\u6570\u636e\u96c6\u6807\u6ce8\u4e86\u673a\u5668\u4eba\u9519\u8bef\u548c\u7528\u6237\u610f\u56fe\u4fee\u6b63\uff0c\u6a21\u578b\u5c06\u57fa\u4e8e\u68c0\u6d4b\u51c6\u786e\u7387\u548c\u8bef\u62a5\u7387\u7b49\u6307\u6807\u8bc4\u4f30\u3002", "conclusion": "\u8be5\u6311\u6218\u662f\u6539\u8fdb\u4eba\u673a\u4ea4\u4e92\u4e2d\u9519\u8bef\u68c0\u6d4b\u7684\u5173\u952e\u4e00\u6b65\uff0c\u901a\u8fc7\u793e\u4ea4\u4fe1\u53f7\u5206\u6790\u63d0\u5347\u673a\u5668\u4eba\u6027\u80fd\u3002"}}
{"id": "2507.13539", "categories": ["cs.RO", "cs.NE"], "pdf": "https://arxiv.org/pdf/2507.13539", "abs": "https://arxiv.org/abs/2507.13539", "authors": ["Jim O'Connor", "Jay B. Nash", "Derin Gezgin", "Gary B. Parker"], "title": "SCOPE for Hexapod Gait Generation", "comment": "IJCCI Conference on Evolutionary Computation and Theory and\n  Applications, 2025", "summary": "Evolutionary methods have previously been shown to be an effective learning\nmethod for walking gaits on hexapod robots. However, the ability of these\nalgorithms to evolve an effective policy rapidly degrades as the input space\nbecomes more complex. This degradation is due to the exponential growth of the\nsolution space, resulting from an increasing parameter count to handle a more\ncomplex input. In order to address this challenge, we introduce Sparse Cosine\nOptimized Policy Evolution (SCOPE). SCOPE utilizes the Discrete Cosine\nTransform (DCT) to learn directly from the feature coefficients of an input\nmatrix. By truncating the coefficient matrix returned by the DCT, we can reduce\nthe dimensionality of an input while retaining the highest energy features of\nthe original input. We demonstrate the effectiveness of this method by using\nSCOPE to learn the gait of a hexapod robot. The hexapod controller is given a\nmatrix input containing time-series information of previous poses, which are\nthen transformed to gait parameters by an evolved policy. In this task, the\naddition of SCOPE to a reference algorithm achieves a 20% increase in efficacy.\nSCOPE achieves this result by reducing the total input size of the time-series\npose data from 2700 to 54, a 98% decrease. Additionally, SCOPE is capable of\ncompressing an input to any output shape, provided that each output dimension\nis no greater than the corresponding input dimension. This paper demonstrates\nthat SCOPE is capable of significantly compressing the size of an input to an\nevolved controller, resulting in a statistically significant gain in efficacy.", "AI": {"tldr": "SCOPE\u65b9\u6cd5\u901a\u8fc7\u79bb\u6563\u4f59\u5f26\u53d8\u6362\uff08DCT\uff09\u538b\u7f29\u8f93\u5165\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u516d\u8db3\u673a\u5668\u4eba\u6b65\u6001\u5b66\u4e60\u6548\u7387\u3002", "motivation": "\u968f\u7740\u8f93\u5165\u7a7a\u95f4\u590d\u6742\u5ea6\u589e\u52a0\uff0c\u8fdb\u5316\u7b97\u6cd5\u6027\u80fd\u6025\u5267\u4e0b\u964d\uff0c\u9700\u89e3\u51b3\u9ad8\u7ef4\u8f93\u5165\u95ee\u9898\u3002", "method": "\u5229\u7528DCT\u63d0\u53d6\u8f93\u5165\u77e9\u9635\u7279\u5f81\u7cfb\u6570\uff0c\u622a\u65ad\u7cfb\u6570\u77e9\u9635\u4ee5\u964d\u4f4e\u7ef4\u5ea6\uff0c\u4fdd\u7559\u9ad8\u80fd\u91cf\u7279\u5f81\u3002", "result": "SCOPE\u5c06\u8f93\u5165\u6570\u636e\u4ece2700\u538b\u7f29\u81f354\uff0c\u63d0\u5347\u7b97\u6cd5\u6548\u738720%\u3002", "conclusion": "SCOPE\u80fd\u663e\u8457\u538b\u7f29\u8f93\u5165\u6570\u636e\uff0c\u63d0\u5347\u63a7\u5236\u5668\u6027\u80fd\uff0c\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.13602", "categories": ["cs.RO", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13602", "abs": "https://arxiv.org/abs/2507.13602", "authors": ["Shivakanth Sujit", "Luca Nunziante", "Dan Ogawa Lillrank", "Rousslan Fernand Julien Dossa", "Kai Arulkumaran"], "title": "Improving Low-Cost Teleoperation: Augmenting GELLO with Force", "comment": "Accepted at the 2025 IEEE/SICE International Symposium on System\n  Integration", "summary": "In this work we extend the low-cost GELLO teleoperation system, initially\ndesigned for joint position control, with additional force information. Our\nfirst extension is to implement force feedback, allowing users to feel\nresistance when interacting with the environment. Our second extension is to\nadd force information into the data collection process and training of\nimitation learning models. We validate our additions by implementing these on a\nGELLO system with a Franka Panda arm as the follower robot, performing a user\nstudy, and comparing the performance of policies trained with and without force\ninformation on a range of simulated and real dexterous manipulation tasks.\nQualitatively, users with robotics experience preferred our controller, and the\naddition of force inputs improved task success on the majority of tasks.", "AI": {"tldr": "\u6269\u5c55\u4e86\u4f4e\u6210\u672cGELLO\u9065\u64cd\u4f5c\u7cfb\u7edf\uff0c\u589e\u52a0\u4e86\u529b\u53cd\u9988\u548c\u529b\u4fe1\u606f\u6570\u636e\u6536\u96c6\uff0c\u9a8c\u8bc1\u4e86\u529b\u4fe1\u606f\u5bf9\u4efb\u52a1\u6210\u529f\u7387\u7684\u63d0\u5347\u3002", "motivation": "\u63d0\u5347\u9065\u64cd\u4f5c\u7cfb\u7edf\u7684\u7528\u6237\u4f53\u9a8c\u548c\u4efb\u52a1\u6027\u80fd\uff0c\u901a\u8fc7\u529b\u53cd\u9988\u548c\u529b\u4fe1\u606f\u589e\u5f3a\u6a21\u4eff\u5b66\u4e60\u6a21\u578b\u7684\u8bad\u7ec3\u6548\u679c\u3002", "method": "\u5728GELLO\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u529b\u53cd\u9988\uff0c\u5e76\u5c06\u529b\u4fe1\u606f\u52a0\u5165\u6570\u636e\u6536\u96c6\u548c\u6a21\u4eff\u5b66\u4e60\u6a21\u578b\u8bad\u7ec3\uff1b\u901a\u8fc7\u7528\u6237\u7814\u7a76\u548c\u4efb\u52a1\u6027\u80fd\u5bf9\u6bd4\u9a8c\u8bc1\u6539\u8fdb\u3002", "result": "\u6709\u673a\u5668\u4eba\u7ecf\u9a8c\u7684\u7528\u6237\u504f\u597d\u65b0\u63a7\u5236\u5668\uff0c\u529b\u4fe1\u606f\u663e\u8457\u63d0\u9ad8\u4e86\u591a\u6570\u4efb\u52a1\u7684\u6210\u529f\u7387\u3002", "conclusion": "\u529b\u53cd\u9988\u548c\u529b\u4fe1\u606f\u7684\u52a0\u5165\u6709\u6548\u63d0\u5347\u4e86\u9065\u64cd\u4f5c\u7cfb\u7edf\u7684\u6027\u80fd\u548c\u7528\u6237\u4f53\u9a8c\u3002"}}
{"id": "2507.13511", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13511", "abs": "https://arxiv.org/abs/2507.13511", "authors": ["Nabil Abdelaziz Ferhat Taleb", "Abdolazim Rezaei", "Raj Atulkumar Patel", "Mehdi Sookhak"], "title": "GraphTrafficGPT: Enhancing Traffic Management Through Graph-Based AI Agent Coordination", "comment": null, "summary": "Large Language Models (LLMs) offer significant promise for intelligent\ntraffic management; however, current chain-based systems like TrafficGPT are\nhindered by sequential task execution, high token usage, and poor scalability,\nmaking them inefficient for complex, real-world scenarios. To address these\nlimitations, we propose GraphTrafficGPT, a novel graph-based architecture,\nwhich fundamentally redesigns the task coordination process for LLM-driven\ntraffic applications. GraphTrafficGPT represents tasks and their dependencies\nas nodes and edges in a directed graph, enabling efficient parallel execution\nand dynamic resource allocation. The main idea behind the proposed model is a\nBrain Agent that decomposes user queries, constructs optimized dependency\ngraphs, and coordinates a network of specialized agents for data retrieval,\nanalysis, visualization, and simulation. By introducing advanced context-aware\ntoken management and supporting concurrent multi-query processing, the proposed\narchitecture handles interdependent tasks typical of modern urban mobility\nenvironments. Experimental results demonstrate that GraphTrafficGPT reduces\ntoken consumption by 50.2% and average response latency by 19.0% compared to\nTrafficGPT, while supporting simultaneous multi-query execution with up to\n23.0% improvement in efficiency.", "AI": {"tldr": "GraphTrafficGPT\u662f\u4e00\u79cd\u57fa\u4e8e\u56fe\u7684\u67b6\u6784\uff0c\u901a\u8fc7\u5e76\u884c\u6267\u884c\u548c\u52a8\u6001\u8d44\u6e90\u5206\u914d\u4f18\u5316LLM\u9a71\u52a8\u7684\u4ea4\u901a\u7ba1\u7406\u4efb\u52a1\uff0c\u663e\u8457\u964d\u4f4e\u4ee4\u724c\u6d88\u8017\u548c\u54cd\u5e94\u5ef6\u8fdf\u3002", "motivation": "\u73b0\u6709\u94fe\u5f0f\u7cfb\u7edf\uff08\u5982TrafficGPT\uff09\u56e0\u987a\u5e8f\u6267\u884c\u4efb\u52a1\u3001\u9ad8\u4ee4\u724c\u4f7f\u7528\u548c\u6269\u5c55\u6027\u5dee\uff0c\u96be\u4ee5\u5e94\u5bf9\u590d\u6742\u4ea4\u901a\u573a\u666f\u3002", "method": "\u63d0\u51faGraphTrafficGPT\uff0c\u5c06\u4efb\u52a1\u53ca\u5176\u4f9d\u8d56\u5173\u7cfb\u5efa\u6a21\u4e3a\u6709\u5411\u56fe\uff0c\u901a\u8fc7Brain Agent\u5206\u89e3\u67e5\u8be2\u5e76\u534f\u8c03\u591a\u4e2a\u4e13\u4e1a\u4ee3\u7406\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u4ee4\u724c\u6d88\u8017\u51cf\u5c1150.2%\uff0c\u54cd\u5e94\u5ef6\u8fdf\u964d\u4f4e19.0%\uff0c\u591a\u67e5\u8be2\u6267\u884c\u6548\u7387\u63d0\u534723.0%\u3002", "conclusion": "GraphTrafficGPT\u663e\u8457\u63d0\u5347\u4e86LLM\u5728\u4ea4\u901a\u7ba1\u7406\u4e2d\u7684\u6548\u7387\u548c\u6269\u5c55\u6027\u3002"}}
{"id": "2507.13647", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13647", "abs": "https://arxiv.org/abs/2507.13647", "authors": ["Minze Li", "Wei Zhao", "Ran Chen", "Mingqiang Wei"], "title": "Improved particle swarm optimization algorithm: multi-target trajectory optimization for swarm drones", "comment": "8 papers,7 figures", "summary": "Real-time trajectory planning for unmanned aerial vehicles (UAVs) in dynamic\nenvironments remains a key challenge due to high computational demands and the\nneed for fast, adaptive responses. Traditional Particle Swarm Optimization\n(PSO) methods, while effective for offline planning, often struggle with\npremature convergence and latency in real-time scenarios. To overcome these\nlimitations, we propose PE-PSO, an enhanced PSO-based online trajectory\nplanner. The method introduces a persistent exploration mechanism to preserve\nswarm diversity and an entropy-based parameter adjustment strategy to\ndynamically adapt optimization behavior. UAV trajectories are modeled using\nB-spline curves, which ensure path smoothness while reducing optimization\ncomplexity. To extend this capability to UAV swarms, we develop a multi-agent\nframework that combines genetic algorithm (GA)-based task allocation with\ndistributed PE-PSO, supporting scalable and coordinated trajectory generation.\nThe distributed architecture allows for parallel computation and decentralized\ncontrol, enabling effective cooperation among agents while maintaining\nreal-time performance. Comprehensive simulations demonstrate that the proposed\nframework outperforms conventional PSO and other swarm-based planners across\nseveral metrics, including trajectory quality, energy efficiency, obstacle\navoidance, and computation time. These results confirm the effectiveness and\napplicability of PE-PSO in real-time multi-UAV operations under complex\nenvironmental conditions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684PSO\u65b9\u6cd5\uff08PE-PSO\uff09\uff0c\u7528\u4e8e\u65e0\u4eba\u673a\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5b9e\u65f6\u8f68\u8ff9\u89c4\u5212\uff0c\u901a\u8fc7\u6301\u4e45\u63a2\u7d22\u673a\u5236\u548c\u71b5\u53c2\u6570\u8c03\u6574\u63d0\u5347\u6027\u80fd\uff0c\u5e76\u7ed3\u5408B\u6837\u6761\u66f2\u7ebf\u548c\u591a\u667a\u80fd\u4f53\u6846\u67b6\u5b9e\u73b0\u9ad8\u6548\u534f\u540c\u3002", "motivation": "\u4f20\u7edfPSO\u65b9\u6cd5\u5728\u5b9e\u65f6\u573a\u666f\u4e2d\u5b58\u5728\u8fc7\u65e9\u6536\u655b\u548c\u5ef6\u8fdf\u95ee\u9898\uff0c\u65e0\u6cd5\u6ee1\u8db3\u65e0\u4eba\u673a\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5feb\u901f\u54cd\u5e94\u9700\u6c42\u3002", "method": "\u5f15\u5165\u6301\u4e45\u63a2\u7d22\u673a\u5236\u548c\u71b5\u53c2\u6570\u8c03\u6574\u7b56\u7565\uff0c\u7ed3\u5408B\u6837\u6761\u66f2\u7ebf\u5efa\u6a21\u8f68\u8ff9\uff0c\u5e76\u5f00\u53d1\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff08\u7ed3\u5408GA\u4efb\u52a1\u5206\u914d\u548c\u5206\u5e03\u5f0fPE-PSO\uff09\u3002", "result": "\u4eff\u771f\u8868\u660e\uff0cPE-PSO\u5728\u8f68\u8ff9\u8d28\u91cf\u3001\u80fd\u6548\u3001\u907f\u969c\u548c\u8ba1\u7b97\u65f6\u95f4\u4e0a\u4f18\u4e8e\u4f20\u7edfPSO\u548c\u5176\u4ed6\u7fa4\u667a\u80fd\u65b9\u6cd5\u3002", "conclusion": "PE-PSO\u9002\u7528\u4e8e\u590d\u6742\u73af\u5883\u4e0b\u7684\u5b9e\u65f6\u591a\u65e0\u4eba\u673a\u534f\u540c\u64cd\u4f5c\uff0c\u5177\u6709\u9ad8\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2507.13541", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13541", "abs": "https://arxiv.org/abs/2507.13541", "authors": ["Shuyue Stella Li", "Melanie Sclar", "Hunter Lang", "Ansong Ni", "Jacqueline He", "Puxin Xu", "Andrew Cohen", "Chan Young Park", "Yulia Tsvetkov", "Asli Celikyilmaz"], "title": "PrefPalette: Personalized Preference Modeling with Latent Attributes", "comment": "17 pages, 6 tables, 5 figures", "summary": "Personalizing AI systems requires understanding not just what users prefer,\nbut the reasons that underlie those preferences - yet current preference models\ntypically treat human judgment as a black box. We introduce PrefPalette, a\nframework that decomposes preferences into attribute dimensions and tailors its\npreference prediction to distinct social community values in a\nhuman-interpretable manner. PrefPalette operationalizes a cognitive science\nprinciple known as multi-attribute decision making in two ways: (1) a scalable\ncounterfactual attribute synthesis step that involves generating synthetic\ntraining data to isolate for individual attribute effects (e.g., formality,\nhumor, cultural values), and (2) attention-based preference modeling that\nlearns how different social communities dynamically weight these attributes.\nThis approach moves beyond aggregate preference modeling to capture the diverse\nevaluation frameworks that drive human judgment. When evaluated on 45 social\ncommunities from the online platform Reddit, PrefPalette outperforms GPT-4o by\n46.6% in average prediction accuracy. Beyond raw predictive improvements,\nPrefPalette also shed light on intuitive, community-specific profiles:\nscholarly communities prioritize verbosity and stimulation, conflict-oriented\ncommunities value sarcasm and directness, and support-based communities\nemphasize empathy. By modeling the attribute-mediated structure of human\njudgment, PrefPalette delivers both superior preference modeling and\ntransparent, interpretable insights, and serves as a first step toward more\ntrustworthy, value-aware personalized applications.", "AI": {"tldr": "PrefPalette\u901a\u8fc7\u5206\u89e3\u504f\u597d\u5c5e\u6027\u5e76\u57fa\u4e8e\u793e\u533a\u4ef7\u503c\u89c2\u9884\u6d4b\u504f\u597d\uff0c\u4f18\u4e8eGPT-4o 46.6%\uff0c\u540c\u65f6\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u793e\u533a\u7279\u5f81\u5206\u6790\u3002", "motivation": "\u5f53\u524d\u504f\u597d\u6a21\u578b\u5c06\u4eba\u7c7b\u5224\u65ad\u89c6\u4e3a\u9ed1\u7bb1\uff0c\u7f3a\u4e4f\u5bf9\u504f\u597d\u80cc\u540e\u539f\u56e0\u7684\u7406\u89e3\uff0cPrefPalette\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u591a\u5c5e\u6027\u51b3\u7b56\u7406\u8bba\uff0c\u901a\u8fc7\u5408\u6210\u53cd\u4e8b\u5b9e\u5c5e\u6027\u548c\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u504f\u597d\u5efa\u6a21\uff0c\u6355\u6349\u793e\u533a\u52a8\u6001\u6743\u91cd\u3002", "result": "\u5728Reddit\u768445\u4e2a\u793e\u533a\u4e2d\uff0cPrefPalette\u9884\u6d4b\u51c6\u786e\u7387\u6bd4GPT-4o\u9ad846.6%\uff0c\u5e76\u63ed\u793a\u793e\u533a\u7279\u5b9a\u504f\u597d\u7279\u5f81\u3002", "conclusion": "PrefPalette\u4e0d\u4ec5\u63d0\u5347\u9884\u6d4b\u6027\u80fd\uff0c\u8fd8\u63d0\u4f9b\u900f\u660e\u3001\u53ef\u89e3\u91ca\u7684\u504f\u597d\u5206\u6790\uff0c\u4e3a\u4e2a\u6027\u5316AI\u7cfb\u7edf\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2507.13650", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.13650", "abs": "https://arxiv.org/abs/2507.13650", "authors": ["Yu-Ting Lai", "Yasamin Foroutani", "Aya Barzelay", "Tsu-Chin Tsao"], "title": "Safe Robotic Capsule Cleaning with Integrated Transpupillary and Intraocular Optical Coherence Tomography", "comment": "12 pages, 27 figures", "summary": "Secondary cataract is one of the most common complications of vision loss due\nto the proliferation of residual lens materials that naturally grow on the lens\ncapsule after cataract surgery. A potential treatment is capsule cleaning, a\nsurgical procedure that requires enhanced visualization of the entire capsule\nand tool manipulation on the thin membrane. This article presents a robotic\nsystem capable of performing the capsule cleaning procedure by integrating a\nstandard transpupillary and an intraocular optical coherence tomography probe\non a surgical instrument for equatorial capsule visualization and real-time\ntool-to-tissue distance feedback. Using robot precision, the developed system\nenables complete capsule mapping in the pupillary and equatorial regions with\nin-situ calibration of refractive index and fiber offset, which are still\ncurrent challenges in obtaining an accurate capsule model. To demonstrate\neffectiveness, the capsule mapping strategy was validated through five\nexperimental trials on an eye phantom that showed reduced root-mean-square\nerrors in the constructed capsule model, while the cleaning strategy was\nperformed in three ex-vivo pig eyes without tissue damage.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u7528\u4e8e\u767d\u5185\u969c\u624b\u672f\u540e\u56ca\u819c\u6e05\u6d01\uff0c\u901a\u8fc7\u96c6\u6210\u5149\u5b66\u76f8\u5e72\u65ad\u5c42\u626b\u63cf\u63a2\u5934\u5b9e\u73b0\u5b9e\u65f6\u53ef\u89c6\u5316\u4e0e\u7cbe\u786e\u64cd\u4f5c\u3002", "motivation": "\u767d\u5185\u969c\u624b\u672f\u540e\u6b8b\u7559\u6676\u72b6\u4f53\u6750\u6599\u7684\u589e\u6b96\u662f\u89c6\u529b\u4e27\u5931\u7684\u5e38\u89c1\u5e76\u53d1\u75c7\uff0c\u4f20\u7edf\u624b\u672f\u9700\u8981\u9ad8\u7cbe\u5ea6\u7684\u53ef\u89c6\u5316\u4e0e\u64cd\u4f5c\u3002", "method": "\u5f00\u53d1\u4e86\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u96c6\u6210\u5149\u5b66\u76f8\u5e72\u65ad\u5c42\u626b\u63cf\u63a2\u5934\uff0c\u5b9e\u73b0\u56ca\u819c\u5b9e\u65f6\u6210\u50cf\u4e0e\u5de5\u5177-\u7ec4\u7ec7\u8ddd\u79bb\u53cd\u9988\uff0c\u5e76\u901a\u8fc7\u673a\u5668\u4eba\u7cbe\u5ea6\u5b8c\u6210\u56ca\u819c\u6620\u5c04\u3002", "result": "\u5728\u773c\u6a21\u578b\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u56ca\u819c\u6620\u5c04\u7b56\u7565\u7684\u51c6\u786e\u6027\uff0c\u5e76\u5728\u79bb\u4f53\u732a\u773c\u4e2d\u6210\u529f\u5b9e\u65bd\u6e05\u6d01\u64cd\u4f5c\uff0c\u672a\u9020\u6210\u7ec4\u7ec7\u635f\u4f24\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u767d\u5185\u969c\u672f\u540e\u56ca\u819c\u6e05\u6d01\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u7cbe\u5ea6\u3001\u5b89\u5168\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.13550", "categories": ["cs.AI", "cs.CL", "cs.SC"], "pdf": "https://arxiv.org/pdf/2507.13550", "abs": "https://arxiv.org/abs/2507.13550", "authors": ["Eduardo C. Garrido-Merch\u00e1n", "Cristina Puente"], "title": "GOFAI meets Generative AI: Development of Expert Systems by means of Large Language Models", "comment": null, "summary": "The development of large language models (LLMs) has successfully transformed\nknowledge-based systems such as open domain question nswering, which can\nautomatically produce vast amounts of seemingly coherent information. Yet,\nthose models have several disadvantages like hallucinations or confident\ngeneration of incorrect or unverifiable facts. In this paper, we introduce a\nnew approach to the development of expert systems using LLMs in a controlled\nand transparent way. By limiting the domain and employing a well-structured\nprompt-based extraction approach, we produce a symbolic representation of\nknowledge in Prolog, which can be validated and corrected by human experts.\nThis approach also guarantees interpretability, scalability and reliability of\nthe developed expert systems. Via quantitative and qualitative experiments with\nClaude Sonnet 3.7 and GPT-4.1, we show strong adherence to facts and semantic\ncoherence on our generated knowledge bases. We present a transparent hybrid\nsolution that combines the recall capacity of LLMs with the precision of\nsymbolic systems, thereby laying the foundation for dependable AI applications\nin sensitive domains.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e0e\u7b26\u53f7\u7cfb\u7edf\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u9650\u5236\u9886\u57df\u548c\u7ed3\u6784\u5316\u63d0\u793a\u751f\u6210\u53ef\u9a8c\u8bc1\u7684Prolog\u77e5\u8bc6\u8868\u793a\uff0c\u63d0\u5347\u4e13\u5bb6\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u89e3\u51b3LLMs\u5728\u751f\u6210\u4fe1\u606f\u65f6\u53ef\u80fd\u51fa\u73b0\u5e7b\u89c9\u6216\u9519\u8bef\u7684\u95ee\u9898\uff0c\u5f00\u53d1\u53ef\u63a7\u3001\u900f\u660e\u7684\u4e13\u5bb6\u7cfb\u7edf\u3002", "method": "\u91c7\u7528\u7ed3\u6784\u5316\u63d0\u793a\u63d0\u53d6\u65b9\u6cd5\uff0c\u751f\u6210Prolog\u7b26\u53f7\u77e5\u8bc6\u8868\u793a\uff0c\u5e76\u7531\u4eba\u7c7b\u4e13\u5bb6\u9a8c\u8bc1\u548c\u4fee\u6b63\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e8b\u5b9e\u51c6\u786e\u6027\u548c\u8bed\u4e49\u8fde\u8d2f\u6027\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u7ed3\u5408\u4e86LLMs\u7684\u53ec\u56de\u80fd\u529b\u548c\u7b26\u53f7\u7cfb\u7edf\u7684\u7cbe\u786e\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u654f\u611f\u9886\u57df\u63d0\u4f9b\u4e86\u53ef\u9760\u3001\u53ef\u89e3\u91ca\u7684AI\u5e94\u7528\u57fa\u7840\uff0c\u517c\u5177\u53ef\u6269\u5c55\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2507.13654", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.13654", "abs": "https://arxiv.org/abs/2507.13654", "authors": ["Haoran Wang", "Yasamin Foroutani", "Matthew Nepo", "Mercedes Rodriguez", "Ji Ma", "Jean-Pierre Hubschman", "Tsu-Chin Tsao", "Jacob Rosen"], "title": "A Study of Teleoperation Methods in a Simulated Virtual Eye Surgery Environment", "comment": "9 pages, 11 figures", "summary": "This paper examines the performance of Inside and Outside Control modes at\nvarious scaling factors in a simulated vitreoretinal surgical setting. The\nIRISS teleoperated surgical system's console (cockpit) was adapted to project a\nsimulated microscope view of an intraocular setup to a virtual reality (VR)\nheadset. Five experienced vitreoretinal surgeons and five engineers with no\nsurgical experience used the system to perform tasks common to vitreoretinal\nsurgery. Experimental results indicate that Inside Control methods at higher\nscaling factors (20 or 30) achieved the best performance overall, though the\noptimal scaling factor may vary by task and complexity. Optimizing control\nmethods and scaling factors could lead to improvements in surgical efficiency\nand accuracy, as well as minimize risks in future robotic-assisted intraocular\nprocedures.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e86\u4e0d\u540c\u7f29\u653e\u56e0\u5b50\u4e0bInside\u548cOutside Control\u6a21\u5f0f\u5728\u6a21\u62df\u73bb\u7483\u4f53\u89c6\u7f51\u819c\u624b\u672f\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u9ad8\u7f29\u653e\u56e0\u5b50\uff0820\u621630\uff09\u7684Inside Control\u6a21\u5f0f\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u4f18\u5316\u63a7\u5236\u6a21\u5f0f\u548c\u7f29\u653e\u56e0\u5b50\u4ee5\u63d0\u9ad8\u673a\u5668\u4eba\u8f85\u52a9\u773c\u5185\u624b\u672f\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u964d\u4f4e\u98ce\u9669\u3002", "method": "\u4f7f\u7528IRISS\u8fdc\u7a0b\u624b\u672f\u7cfb\u7edf\uff0c\u5c06\u6a21\u62df\u663e\u5fae\u955c\u89c6\u56fe\u6295\u5f71\u5230VR\u5934\u663e\uff0c\u75315\u540d\u7ecf\u9a8c\u4e30\u5bcc\u7684\u73bb\u7483\u4f53\u89c6\u7f51\u819c\u5916\u79d1\u533b\u751f\u548c5\u540d\u65e0\u624b\u672f\u7ecf\u9a8c\u7684\u5de5\u7a0b\u5e08\u6267\u884c\u4efb\u52a1\u3002", "result": "\u9ad8\u7f29\u653e\u56e0\u5b50\u7684Inside Control\u6a21\u5f0f\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u6700\u4f18\u7f29\u653e\u56e0\u5b50\u56e0\u4efb\u52a1\u548c\u590d\u6742\u5ea6\u800c\u5f02\u3002", "conclusion": "\u4f18\u5316\u63a7\u5236\u65b9\u6cd5\u548c\u7f29\u653e\u56e0\u5b50\u53ef\u63d0\u5347\u624b\u672f\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u51cf\u5c11\u672a\u6765\u673a\u5668\u4eba\u8f85\u52a9\u773c\u5185\u624b\u672f\u7684\u98ce\u9669\u3002"}}
{"id": "2507.13558", "categories": ["cs.AI", "cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13558", "abs": "https://arxiv.org/abs/2507.13558", "authors": ["David Poole"], "title": "Why Isn't Relational Learning Taking Over the World?", "comment": "10 pages (6 pages + references + appendices)", "summary": "AI seems to be taking over the world with systems that model pixels, words,\nand phonemes. The world is arguably made up, not of pixels, words, and phonemes\nbut of entities (objects, things, including events) with properties and\nrelations among them. Surely we should model these, not the perception or\ndescription of them. You might suspect that concentrating on modeling words and\npixels is because all of the (valuable) data in the world is in terms of text\nand images. If you look into almost any company you will find their most\nvaluable data is in spreadsheets, databases and other relational formats. These\nare not the form that are studied in introductory machine learning, but are\nfull of product numbers, student numbers, transaction numbers and other\nidentifiers that can't be interpreted naively as numbers. The field that\nstudies this sort of data has various names including relational learning,\nstatistical relational AI, and many others. This paper explains why relational\nlearning is not taking over the world -- except in a few cases with restricted\nrelations -- and what needs to be done to bring it to it's rightful prominence.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86AI\u5e94\u5173\u6ce8\u5b9e\u4f53\u53ca\u5176\u5173\u7cfb\u5efa\u6a21\uff0c\u800c\u975e\u4ec5\u5173\u6ce8\u50cf\u7d20\u548c\u6587\u5b57\uff0c\u5e76\u5206\u6790\u4e86\u5173\u7cfb\u5b66\u4e60\u672a\u666e\u53ca\u7684\u539f\u56e0\u53ca\u6539\u8fdb\u65b9\u5411\u3002", "motivation": "\u5f53\u524dAI\u4e3b\u8981\u5efa\u6a21\u50cf\u7d20\u548c\u6587\u5b57\uff0c\u4f46\u4e16\u754c\u7531\u5b9e\u4f53\u53ca\u5176\u5173\u7cfb\u6784\u6210\uff0c\u5173\u7cfb\u5b66\u4e60\u5e94\u6210\u4e3a\u6838\u5fc3\u3002", "method": "\u5206\u6790\u5173\u7cfb\u5b66\u4e60\u5728\u6570\u636e\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\uff0c\u63a2\u8ba8\u5176\u53d7\u9650\u539f\u56e0\u3002", "result": "\u5173\u7cfb\u5b66\u4e60\u4ec5\u5728\u5c11\u6570\u53d7\u9650\u573a\u666f\u4e2d\u6210\u529f\uff0c\u9700\u8fdb\u4e00\u6b65\u6539\u8fdb\u4ee5\u666e\u53ca\u3002", "conclusion": "\u5173\u7cfb\u5b66\u4e60\u9700\u7a81\u7834\u9650\u5236\uff0c\u624d\u80fd\u5b9e\u73b0\u5176\u6f5c\u529b\u5e76\u6210\u4e3aAI\u4e3b\u6d41\u3002"}}
{"id": "2507.13662", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.13662", "abs": "https://arxiv.org/abs/2507.13662", "authors": ["Jing Cheng", "Yasser G. Alqaham", "Zhenyu Gan", "Amit K. Sanyal"], "title": "Iteratively Learning Muscle Memory for Legged Robots to Master Adaptive and High Precision Locomotion", "comment": null, "summary": "This paper presents a scalable and adaptive control framework for legged\nrobots that integrates Iterative Learning Control (ILC) with a biologically\ninspired torque library (TL), analogous to muscle memory. The proposed method\naddresses key challenges in robotic locomotion, including accurate trajectory\ntracking under unmodeled dynamics and external disturbances. By leveraging the\nrepetitive nature of periodic gaits and extending ILC to nonperiodic tasks, the\nframework enhances accuracy and generalization across diverse locomotion\nscenarios. The control architecture is data-enabled, combining a physics-based\nmodel derived from hybrid-system trajectory optimization with real-time\nlearning to compensate for model uncertainties and external disturbances. A\ncentral contribution is the development of a generalized TL that stores learned\ncontrol profiles and enables rapid adaptation to changes in speed, terrain, and\ngravitational conditions-eliminating the need for repeated learning and\nsignificantly reducing online computation. The approach is validated on the\nbipedal robot Cassie and the quadrupedal robot A1 through extensive simulations\nand hardware experiments. Results demonstrate that the proposed framework\nreduces joint tracking errors by up to 85% within a few seconds and enables\nreliable execution of both periodic and nonperiodic gaits, including slope\ntraversal and terrain adaptation. Compared to state-of-the-art whole-body\ncontrollers, the learned skills eliminate the need for online computation\nduring execution and achieve control update rates exceeding 30x those of\nexisting methods. These findings highlight the effectiveness of integrating ILC\nwith torque memory as a highly data-efficient and practical solution for legged\nlocomotion in unstructured and dynamic environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8fed\u4ee3\u5b66\u4e60\u63a7\u5236\uff08ILC\uff09\u548c\u751f\u7269\u542f\u53d1\u7684\u626d\u77e9\u5e93\uff08TL\uff09\u7684\u53ef\u6269\u5c55\u81ea\u9002\u5e94\u63a7\u5236\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u9ad8\u817f\u5f0f\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u8fd0\u52a8\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u817f\u5f0f\u673a\u5668\u4eba\u5728\u672a\u5efa\u6a21\u52a8\u529b\u5b66\u548c\u5916\u90e8\u5e72\u6270\u4e0b\u7684\u7cbe\u786e\u8f68\u8ff9\u8ddf\u8e2a\u95ee\u9898\uff0c\u540c\u65f6\u63d0\u5347\u5176\u5728\u975e\u5468\u671f\u6027\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u6574\u5408\u7269\u7406\u6a21\u578b\u548c\u5b9e\u65f6\u5b66\u4e60\uff0c\u901a\u8fc7\u626d\u77e9\u5e93\u5b58\u50a8\u63a7\u5236\u914d\u7f6e\u6587\u4ef6\uff0c\u5b9e\u73b0\u5feb\u901f\u9002\u5e94\u901f\u5ea6\u548c\u5730\u5f62\u53d8\u5316\u3002", "result": "\u5728\u53cc\u8db3\u673a\u5668\u4ebaCassie\u548c\u56db\u8db3\u673a\u5668\u4ebaA1\u4e0a\u9a8c\u8bc1\uff0c\u5173\u8282\u8ddf\u8e2a\u8bef\u5dee\u51cf\u5c1185%\uff0c\u63a7\u5236\u66f4\u65b0\u901f\u7387\u63d0\u534730\u500d\u3002", "conclusion": "ILC\u4e0e\u626d\u77e9\u5e93\u7ed3\u5408\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u5b9e\u7528\u7684\u817f\u5f0f\u8fd0\u52a8\u63a7\u5236\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u975e\u7ed3\u6784\u5316\u52a8\u6001\u73af\u5883\u3002"}}
{"id": "2507.13625", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13625", "abs": "https://arxiv.org/abs/2507.13625", "authors": ["Yuxin Zhang", "Xi Wang", "Mo Hu", "Zhenyu Zhang"], "title": "BifrostRAG: Bridging Dual Knowledge Graphs for Multi-Hop Question Answering in Construction Safety", "comment": "19 pages, 13 figures", "summary": "Information retrieval and question answering from safety regulations are\nessential for automated construction compliance checking but are hindered by\nthe linguistic and structural complexity of regulatory text. Many\ncompliance-related queries are multi-hop, requiring synthesis of information\nacross interlinked clauses. This poses a challenge for traditional\nretrieval-augmented generation (RAG) systems. To overcome this, we introduce\nBifrostRAG: a dual-graph RAG-integrated system that explicitly models both\nlinguistic relationships (via an Entity Network Graph) and document structure\n(via a Document Navigator Graph). This architecture powers a hybrid retrieval\nmechanism that combines graph traversal with vector-based semantic search,\nenabling large language models to reason over both the meaning and the\nstructure of the text. Evaluation on a multi-hop question dataset shows that\nBifrostRAG achieves 92.8 percent precision, 85.5 percent recall, and an F1\nscore of 87.3 percent. These results significantly outperform vector-only and\ngraph-only RAG baselines that represent current leading approaches. Error\nanalysis further highlights the comparative advantages of our hybrid method\nover single-modality RAGs. These findings establish BifrostRAG as a robust\nknowledge engine for LLM-driven compliance checking. Its dual-graph, hybrid\nretrieval mechanism offers a transferable blueprint for navigating complex\ntechnical documents across knowledge-intensive engineering domains.", "AI": {"tldr": "BifrostRAG\u662f\u4e00\u79cd\u53cc\u56feRAG\u7cfb\u7edf\uff0c\u901a\u8fc7\u5b9e\u4f53\u7f51\u7edc\u56fe\u548c\u6587\u6863\u5bfc\u822a\u56fe\u7ed3\u5408\u8bed\u8a00\u5173\u7cfb\u548c\u6587\u6863\u7ed3\u6784\uff0c\u663e\u8457\u63d0\u5347\u591a\u8df3\u95ee\u7b54\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edfRAG\u7cfb\u7edf\u5728\u5904\u7406\u590d\u6742\u6cd5\u89c4\u6587\u672c\u548c\u591a\u8df3\u67e5\u8be2\u65f6\u7684\u5c40\u9650\u6027\u3002", "method": "\u91c7\u7528\u53cc\u56fe\u67b6\u6784\uff08\u5b9e\u4f53\u7f51\u7edc\u56fe\u548c\u6587\u6863\u5bfc\u822a\u56fe\uff09\uff0c\u7ed3\u5408\u56fe\u904d\u5386\u548c\u5411\u91cf\u8bed\u4e49\u641c\u7d22\u7684\u6df7\u5408\u68c0\u7d22\u673a\u5236\u3002", "result": "\u5728\u591a\u8df3\u95ee\u7b54\u6570\u636e\u96c6\u4e0a\u8fbe\u523092.8%\u7cbe\u786e\u7387\u300185.5%\u53ec\u56de\u7387\u548c87.3% F1\u5206\u6570\uff0c\u663e\u8457\u4f18\u4e8e\u5355\u6a21\u6001RAG\u57fa\u7ebf\u3002", "conclusion": "BifrostRAG\u4e3a\u590d\u6742\u6280\u672f\u6587\u6863\u7684\u68c0\u7d22\u63d0\u4f9b\u4e86\u53ef\u8fc1\u79fb\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u77e5\u8bc6\u5bc6\u96c6\u578b\u5de5\u7a0b\u9886\u57df\u3002"}}
{"id": "2507.13702", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.13702", "abs": "https://arxiv.org/abs/2507.13702", "authors": ["Junho Choi", "Kihwan Ryoo", "Jeewon Kim", "Taeyun Kim", "Eungchang Lee", "Myeongwoo Jeong", "Kevin Christiansen Marsim", "Hyungtae Lim", "Hyun Myung"], "title": "SaWa-ML: Structure-Aware Pose Correction and Weight Adaptation-Based Robust Multi-Robot Localization", "comment": "This paper has been accepted to the 2025 IEEE/RSJ International\n  Conference on Intelligent Robots and Systems (IROS)", "summary": "Multi-robot localization is a crucial task for implementing multi-robot\nsystems. Numerous researchers have proposed optimization-based multi-robot\nlocalization methods that use camera, IMU, and UWB sensors. Nevertheless,\ncharacteristics of individual robot odometry estimates and distance\nmeasurements between robots used in the optimization are not sufficiently\nconsidered. In addition, previous researches were heavily influenced by the\nodometry accuracy that is estimated from individual robots. Consequently,\nlong-term drift error caused by error accumulation is potentially inevitable.\nIn this paper, we propose a novel visual-inertial-range-based multi-robot\nlocalization method, named SaWa-ML, which enables geometric structure-aware\npose correction and weight adaptation-based robust multi-robot localization.\nOur contributions are twofold: (i) we leverage UWB sensor data, whose range\nerror does not accumulate over time, to first estimate the relative positions\nbetween robots and then correct the positions of each robot, thus reducing\nlong-term drift errors, (ii) we design adaptive weights for robot pose\ncorrection by considering the characteristics of the sensor data and\nvisual-inertial odometry estimates. The proposed method has been validated in\nreal-world experiments, showing a substantial performance increase compared\nwith state-of-the-art algorithms.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSaWa-ML\u7684\u65b0\u578b\u89c6\u89c9-\u60ef\u6027-\u8ddd\u79bb\u591a\u673a\u5668\u4eba\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u901a\u8fc7\u51e0\u4f55\u7ed3\u6784\u611f\u77e5\u7684\u4f4d\u59ff\u6821\u6b63\u548c\u6743\u91cd\u81ea\u9002\u5e94\uff0c\u663e\u8457\u51cf\u5c11\u957f\u671f\u6f02\u79fb\u8bef\u5dee\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u672a\u5145\u5206\u8003\u8651\u673a\u5668\u4eba\u4e2a\u4f53\u91cc\u7a0b\u8ba1\u4f30\u8ba1\u548c\u8ddd\u79bb\u6d4b\u91cf\u7684\u7279\u6027\uff0c\u4e14\u6613\u53d7\u91cc\u7a0b\u8ba1\u7cbe\u5ea6\u5f71\u54cd\uff0c\u5bfc\u81f4\u957f\u671f\u6f02\u79fb\u8bef\u5dee\u4e0d\u53ef\u907f\u514d\u3002", "method": "\u5229\u7528UWB\u4f20\u611f\u5668\u6570\u636e\u4f30\u8ba1\u673a\u5668\u4eba\u95f4\u76f8\u5bf9\u4f4d\u7f6e\u5e76\u6821\u6b63\u4f4d\u59ff\uff0c\u540c\u65f6\u8bbe\u8ba1\u81ea\u9002\u5e94\u6743\u91cd\u4ee5\u4f18\u5316\u4f20\u611f\u5668\u6570\u636e\u548c\u89c6\u89c9-\u60ef\u6027\u91cc\u7a0b\u8ba1\u4f30\u8ba1\u3002", "result": "\u5728\u771f\u5b9e\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u6027\u80fd\u63d0\u5347\uff0c\u4f18\u4e8e\u73b0\u6709\u5148\u8fdb\u7b97\u6cd5\u3002", "conclusion": "SaWa-ML\u901a\u8fc7\u7ed3\u6784\u611f\u77e5\u548c\u6743\u91cd\u81ea\u9002\u5e94\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u673a\u5668\u4eba\u5b9a\u4f4d\u4e2d\u7684\u957f\u671f\u6f02\u79fb\u95ee\u9898\u3002"}}
{"id": "2507.13651", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13651", "abs": "https://arxiv.org/abs/2507.13651", "authors": ["Gerben van der Hoek", "Johan Jeuring", "Rogier Bos"], "title": "Buggy rule diagnosis for combined steps through final answer evaluation in stepwise tasks", "comment": null, "summary": "Many intelligent tutoring systems can support a student in solving a stepwise\ntask. When a student combines several steps in one step, the number of possible\npaths connecting consecutive inputs may be very large. This combinatorial\nexplosion makes error diagnosis hard. Using a final answer to diagnose a\ncombination of steps can mitigate the combinatorial explosion, because there\nare generally fewer possible (erroneous) final answers than (erroneous)\nsolution paths. An intermediate input for a task can be diagnosed by\nautomatically completing it according to the task solution strategy and\ndiagnosing this solution. This study explores the potential of automated error\ndiagnosis based on a final answer. We investigate the design of a service that\nprovides a buggy rule diagnosis when a student combines several steps. To\nvalidate the approach, we apply the service to an existing dataset (n=1939) of\nunique student steps when solving quadratic equations, which could not be\ndiagnosed by a buggy rule service that tries to connect consecutive inputs with\na single rule. Results show that final answer evaluation can diagnose 29,4% of\nthese steps. Moreover, a comparison of the generated diagnoses with teacher\ndiagnoses on a subset (n=115) shows that the diagnoses align in 97% of the\ncases. These results can be considered a basis for further exploration of the\napproach.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u57fa\u4e8e\u6700\u7ec8\u7b54\u6848\u7684\u81ea\u52a8\u9519\u8bef\u8bca\u65ad\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u5b66\u751f\u5728\u5206\u6b65\u4efb\u52a1\u4e2d\u5408\u5e76\u6b65\u9aa4\u5bfc\u81f4\u7684\u7ec4\u5408\u7206\u70b8\u95ee\u9898\u3002", "motivation": "\u5b66\u751f\u5728\u5206\u6b65\u4efb\u52a1\u4e2d\u5408\u5e76\u6b65\u9aa4\u4f1a\u5bfc\u81f4\u53ef\u80fd\u7684\u8def\u5f84\u7ec4\u5408\u7206\u70b8\uff0c\u589e\u52a0\u9519\u8bef\u8bca\u65ad\u96be\u5ea6\u3002\u901a\u8fc7\u6700\u7ec8\u7b54\u6848\u8bca\u65ad\u53ef\u4ee5\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8e\u6700\u7ec8\u7b54\u6848\u7684\u9519\u8bef\u8bca\u65ad\u670d\u52a1\uff0c\u5e76\u5e94\u7528\u4e8e\u4e8c\u6b21\u65b9\u7a0b\u6c42\u89e3\u7684\u5b66\u751f\u6570\u636e\u96c6\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u80fd\u8bca\u65ad29.4%\u7684\u672a\u8bca\u65ad\u6b65\u9aa4\uff0c\u4e14\u4e0e\u6559\u5e08\u8bca\u65ad\u7ed3\u679c\u7684\u4e00\u81f4\u6027\u8fbe97%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u9519\u8bef\u8bca\u65ad\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u503c\u5f97\u8fdb\u4e00\u6b65\u63a2\u7d22\u3002"}}
{"id": "2507.13729", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13729", "abs": "https://arxiv.org/abs/2507.13729", "authors": ["Yu Yao", "Salil Bhatnagar", "Markus Mazzola", "Vasileios Belagiannis", "Igor Gilitschenski", "Luigi Palmieri", "Simon Razniewski", "Marcel Hallgarten"], "title": "AGENTS-LLM: Augmentative GENeration of Challenging Traffic Scenarios with an Agentic LLM Framework", "comment": null, "summary": "Rare, yet critical, scenarios pose a significant challenge in testing and\nevaluating autonomous driving planners. Relying solely on real-world driving\nscenes requires collecting massive datasets to capture these scenarios. While\nautomatic generation of traffic scenarios appears promising, data-driven models\nrequire extensive training data and often lack fine-grained control over the\noutput. Moreover, generating novel scenarios from scratch can introduce a\ndistributional shift from the original training scenes which undermines the\nvalidity of evaluations especially for learning-based planners. To sidestep\nthis, recent work proposes to generate challenging scenarios by augmenting\noriginal scenarios from the test set. However, this involves the manual\naugmentation of scenarios by domain experts. An approach that is unable to meet\nthe demands for scale in the evaluation of self-driving systems. Therefore,\nthis paper introduces a novel LLM-agent based framework for augmenting\nreal-world traffic scenarios using natural language descriptions, addressing\nthe limitations of existing methods. A key innovation is the use of an agentic\ndesign, enabling fine-grained control over the output and maintaining high\nperformance even with smaller, cost-effective LLMs. Extensive human expert\nevaluation demonstrates our framework's ability to accurately adhere to user\nintent, generating high quality augmented scenarios comparable to those created\nmanually.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM-agent\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u589e\u5f3a\u771f\u5b9e\u4ea4\u901a\u573a\u666f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u6d4b\u8bd5\u573a\u666f\u65f6\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u6d4b\u8bd5\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u5668\u65f6\uff0c\u7f55\u89c1\u4f46\u5173\u952e\u7684\u573a\u666f\u96be\u4ee5\u6355\u6349\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u6570\u636e\u6216\u4e13\u5bb6\u624b\u52a8\u64cd\u4f5c\uff0c\u65e0\u6cd5\u6ee1\u8db3\u89c4\u6a21\u5316\u9700\u6c42\u3002", "method": "\u91c7\u7528\u57fa\u4e8eLLM-agent\u7684\u8bbe\u8ba1\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u751f\u6210\u589e\u5f3a\u573a\u666f\uff0c\u5b9e\u73b0\u5bf9\u8f93\u51fa\u7684\u7ec6\u7c92\u5ea6\u63a7\u5236\u3002", "result": "\u4eba\u7c7b\u4e13\u5bb6\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u51c6\u786e\u9075\u5faa\u7528\u6237\u610f\u56fe\uff0c\u751f\u6210\u4e0e\u624b\u52a8\u521b\u5efa\u573a\u666f\u8d28\u91cf\u76f8\u5f53\u7684\u589e\u5f3a\u573a\u666f\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u8bc4\u4f30\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u63a7\u7684\u573a\u666f\u751f\u6210\u65b9\u6cd5\u3002"}}
{"id": "2507.13652", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13652", "abs": "https://arxiv.org/abs/2507.13652", "authors": ["Gerben van der Hoek", "Johan Jeuring", "Rogier Bos"], "title": "Combining model tracing and constraint-based modeling for multistep strategy diagnoses", "comment": null, "summary": "Model tracing and constraint-based modeling are two approaches to diagnose\nstudent input in stepwise tasks. Model tracing supports identifying consecutive\nproblem-solving steps taken by a student, whereas constraint-based modeling\nsupports student input diagnosis even when several steps are combined into one\nstep. We propose an approach that merges both paradigms. By defining\nconstraints as properties that a student input has in common with a step of a\nstrategy, it is possible to provide a diagnosis when a student deviates from a\nstrategy even when the student combines several steps. In this study we explore\nthe design of a system for multistep strategy diagnoses, and evaluate these\ndiagnoses. As a proof of concept, we generate diagnoses for an existing dataset\ncontaining steps students take when solving quadratic equations (n=2136). To\ncompare with human diagnoses, two teachers coded a random sample of deviations\n(n=70) and applications of the strategy (n=70). Results show that that the\nsystem diagnosis aligned with the teacher coding in all of the 140 student\nsteps.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6a21\u578b\u8ffd\u8e2a\u548c\u7ea6\u675f\u5efa\u6a21\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bca\u65ad\u5b66\u751f\u5728\u591a\u6b65\u4efb\u52a1\u4e2d\u7684\u8f93\u5165\uff0c\u5e76\u5728\u5b9e\u9645\u6570\u636e\u4e2d\u9a8c\u8bc1\u5176\u4e0e\u6559\u5e08\u8bca\u65ad\u7684\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u6a21\u578b\u8ffd\u8e2a\u548c\u7ea6\u675f\u5efa\u6a21\uff09\u5404\u6709\u5c40\u9650\u6027\uff0c\u65e0\u6cd5\u5168\u9762\u8bca\u65ad\u5b66\u751f\u8f93\u5165\uff0c\u5c24\u5176\u662f\u5728\u5b66\u751f\u5c06\u591a\u6b65\u5408\u5e76\u4e3a\u4e00\u6b65\u65f6\u3002", "method": "\u901a\u8fc7\u5c06\u7ea6\u675f\u5b9a\u4e49\u4e3a\u5b66\u751f\u8f93\u5165\u4e0e\u7b56\u7565\u6b65\u9aa4\u7684\u5171\u540c\u5c5e\u6027\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u652f\u6301\u591a\u6b65\u7b56\u7565\u8bca\u65ad\u7684\u7cfb\u7edf\u3002", "result": "\u57282136\u4e2a\u5b66\u751f\u89e3\u4e8c\u6b21\u65b9\u7a0b\u7684\u6b65\u9aa4\u4e2d\uff0c\u7cfb\u7edf\u8bca\u65ad\u4e0e\u6559\u5e08\u7f16\u7801\u7684140\u4e2a\u5b66\u751f\u6b65\u9aa4\u5b8c\u5168\u4e00\u81f4\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u7ed3\u5408\u4e86\u4e24\u79cd\u8303\u5f0f\uff0c\u80fd\u591f\u51c6\u786e\u8bca\u65ad\u5b66\u751f\u7684\u591a\u6b65\u8f93\u5165\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.13787", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.13787", "abs": "https://arxiv.org/abs/2507.13787", "authors": ["Doina Pisla", "Alexandru Pusca", "Andrei Caprariu", "Adrian Pisla", "Bogdan Gherman", "Calin Vaida", "Damien Chablat"], "title": "Design Analysis of an Innovative Parallel Robot for Minimally Invasive Pancreatic Surgery", "comment": null, "summary": "This paper focuses on the design of a parallel robot designed for robotic\nassisted minimally invasive pancreatic surgery. Two alternative architectures,\ncalled ATHENA-1 and ATHENA-2, each with 4 degrees of freedom (DOF) are\nproposed. Their kinematic schemes are presented, and the conceptual 3D CAD\nmodels are illustrated. Based on these, two Finite Element Method (FEM)\nsimulations were performed to determine which architecture has the higher\nstiffness. A workspace quantitative analysis is performed to further assess the\nusability of the two proposed parallel architectures related to the medical\ntasks. The obtained results are used to select the architecture which fit the\nrequired design criteria and will be used to develop the experimental model of\nthe surgical robot.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e24\u79cd4\u81ea\u7531\u5ea6\u5e76\u884c\u673a\u5668\u4eba\u67b6\u6784\uff08ATHENA-1\u548cATHENA-2\uff09\uff0c\u7528\u4e8e\u80f0\u817a\u5fae\u521b\u624b\u672f\u8f85\u52a9\uff0c\u901a\u8fc7FEM\u6a21\u62df\u548c\u5b9a\u91cf\u5de5\u4f5c\u7a7a\u95f4\u5206\u6790\u8bc4\u4f30\u5176\u6027\u80fd\uff0c\u6700\u7ec8\u9009\u62e9\u7b26\u5408\u8bbe\u8ba1\u6807\u51c6\u7684\u67b6\u6784\u7528\u4e8e\u5b9e\u9a8c\u6a21\u578b\u5f00\u53d1\u3002", "motivation": "\u8bbe\u8ba1\u9002\u7528\u4e8e\u80f0\u817a\u5fae\u521b\u624b\u672f\u8f85\u52a9\u7684\u9ad8\u6027\u80fd\u5e76\u884c\u673a\u5668\u4eba\u67b6\u6784\uff0c\u4ee5\u6ee1\u8db3\u624b\u672f\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e24\u79cd4\u81ea\u7531\u5ea6\u5e76\u884c\u67b6\u6784\uff0c\u8fdb\u884cFEM\u6a21\u62df\u8bc4\u4f30\u521a\u5ea6\uff0c\u5e76\u901a\u8fc7\u5b9a\u91cf\u5de5\u4f5c\u7a7a\u95f4\u5206\u6790\u9a8c\u8bc1\u5b9e\u7528\u6027\u3002", "result": "\u901a\u8fc7\u6a21\u62df\u548c\u5206\u6790\u786e\u5b9a\u4e86\u4e00\u79cd\u67b6\u6784\u5177\u6709\u66f4\u9ad8\u521a\u5ea6\u548c\u66f4\u597d\u7684\u5de5\u4f5c\u7a7a\u95f4\u9002\u5e94\u6027\u3002", "conclusion": "\u9009\u62e9\u7b26\u5408\u8bbe\u8ba1\u6807\u51c6\u7684\u67b6\u6784\u7528\u4e8e\u540e\u7eed\u5b9e\u9a8c\u6a21\u578b\u5f00\u53d1\uff0c\u4ee5\u652f\u6301\u80f0\u817a\u5fae\u521b\u624b\u672f\u673a\u5668\u4eba\u6280\u672f\u3002"}}
{"id": "2507.13737", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.13737", "abs": "https://arxiv.org/abs/2507.13737", "authors": ["Ye Tian", "Xiaoyuan Ren", "Zihao Wang", "Onat Gungor", "Xiaofan Yu", "Tajana Rosing"], "title": "DailyLLM: Context-Aware Activity Log Generation Using Multi-Modal Sensors and LLMs", "comment": null, "summary": "Rich and context-aware activity logs facilitate user behavior analysis and\nhealth monitoring, making them a key research focus in ubiquitous computing.\nThe remarkable semantic understanding and generation capabilities of Large\nLanguage Models (LLMs) have recently created new opportunities for activity log\ngeneration. However, existing methods continue to exhibit notable limitations\nin terms of accuracy, efficiency, and semantic richness. To address these\nchallenges, we propose DailyLLM. To the best of our knowledge, this is the\nfirst log generation and summarization system that comprehensively integrates\ncontextual activity information across four dimensions: location, motion,\nenvironment, and physiology, using only sensors commonly available on\nsmartphones and smartwatches. To achieve this, DailyLLM introduces a\nlightweight LLM-based framework that integrates structured prompting with\nefficient feature extraction to enable high-level activity understanding.\nExtensive experiments demonstrate that DailyLLM outperforms state-of-the-art\n(SOTA) log generation methods and can be efficiently deployed on personal\ncomputers and Raspberry Pi. Utilizing only a 1.5B-parameter LLM model, DailyLLM\nachieves a 17% improvement in log generation BERTScore precision compared to\nthe 70B-parameter SOTA baseline, while delivering nearly 10x faster inference\nspeed.", "AI": {"tldr": "DailyLLM\u662f\u4e00\u79cd\u57fa\u4e8e\u8f7b\u91cf\u7ea7LLM\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u4f4d\u7f6e\u3001\u8fd0\u52a8\u3001\u73af\u5883\u548c\u751f\u7406\u56db\u4e2a\u7ef4\u5ea6\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6d3b\u52a8\u65e5\u5fd7\u751f\u6210\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u6d3b\u52a8\u65e5\u5fd7\u751f\u6210\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u3001\u6548\u7387\u548c\u8bed\u4e49\u4e30\u5bcc\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u4e14\u5168\u9762\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "DailyLLM\u91c7\u7528\u8f7b\u91cf\u7ea7LLM\u6846\u67b6\uff0c\u7ed3\u5408\u7ed3\u6784\u5316\u63d0\u793a\u548c\u9ad8\u6548\u7279\u5f81\u63d0\u53d6\uff0c\u5b9e\u73b0\u9ad8\u7ea7\u6d3b\u52a8\u7406\u89e3\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDailyLLM\u5728BERTScore\u7cbe\u5ea6\u4e0a\u6bd470B\u53c2\u6570\u7684SOTA\u57fa\u7ebf\u63d0\u534717%\uff0c\u63a8\u7406\u901f\u5ea6\u5feb10\u500d\u3002", "conclusion": "DailyLLM\u4e3a\u6d3b\u52a8\u65e5\u5fd7\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u8f7b\u91cf\u4e14\u5168\u9762\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u667a\u80fd\u624b\u673a\u548c\u667a\u80fd\u624b\u8868\u7b49\u8bbe\u5907\u3002"}}
{"id": "2507.13871", "categories": ["cs.RO", "cs.CV", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.13871", "abs": "https://arxiv.org/abs/2507.13871", "authors": ["Mehul Anand", "Shishir Kolathaya"], "title": "Safety Certification in the Latent space using Control Barrier Functions and World Models", "comment": "6 pages, 6 figures. arXiv admin note: text overlap with\n  arXiv:2409.12616", "summary": "Synthesising safe controllers from visual data typically requires extensive\nsupervised labelling of safety-critical data, which is often impractical in\nreal-world settings. Recent advances in world models enable reliable prediction\nin latent spaces, opening new avenues for scalable and data-efficient safe\ncontrol. In this work, we introduce a semi-supervised framework that leverages\ncontrol barrier certificates (CBCs) learned in the latent space of a world\nmodel to synthesise safe visuomotor policies. Our approach jointly learns a\nneural barrier function and a safe controller using limited labelled data,\nwhile exploiting the predictive power of modern vision transformers for latent\ndynamics modelling.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u534a\u76d1\u7763\u6846\u67b6\uff0c\u5229\u7528\u4e16\u754c\u6a21\u578b\u7684\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u63a7\u5236\u5c4f\u969c\u8bc1\u4e66\uff08CBCs\uff09\u6765\u5408\u6210\u5b89\u5168\u7684\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u3002", "motivation": "\u4ece\u89c6\u89c9\u6570\u636e\u5408\u6210\u5b89\u5168\u63a7\u5236\u5668\u901a\u5e38\u9700\u8981\u5927\u91cf\u6807\u8bb0\u5b89\u5168\u5173\u952e\u6570\u636e\uff0c\u8fd9\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u4e0d\u5207\u5b9e\u9645\u3002", "method": "\u7ed3\u5408\u795e\u7ecf\u5c4f\u969c\u51fd\u6570\u548c\u5b89\u5168\u63a7\u5236\u5668\u7684\u8054\u5408\u5b66\u4e60\uff0c\u5229\u7528\u73b0\u4ee3\u89c6\u89c9\u53d8\u6362\u5668\u7684\u9884\u6d4b\u80fd\u529b\u8fdb\u884c\u6f5c\u5728\u52a8\u529b\u5b66\u5efa\u6a21\u3002", "result": "\u901a\u8fc7\u6709\u9650\u6807\u8bb0\u6570\u636e\u5b9e\u73b0\u4e86\u5b89\u5168\u7684\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u5408\u6210\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6570\u636e\u9ad8\u6548\u7684\u5b89\u5168\u63a7\u5236\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2507.13759", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13759", "abs": "https://arxiv.org/abs/2507.13759", "authors": ["Carlos Bobed", "Carlota Quintana", "Eduardo Mena", "Jorge Bobed", "Fernando Bobillo"], "title": "OntView: What you See is What you Meant", "comment": null, "summary": "In the field of knowledge management and computer science, ontologies provide\na structured framework for modeling domain-specific knowledge by defining\nconcepts and their relationships. However, the lack of tools that provide\neffective visualization is still a significant challenge. While numerous\nontology editors and viewers exist, most of them fail to graphically represent\nontology structures in a meaningful and non-overwhelming way, limiting users'\nability to comprehend dependencies and properties within large ontological\nframeworks.\n  In this paper, we present OntView, an ontology viewer that is designed to\nprovide users with an intuitive visual representation of ontology concepts and\ntheir formal definitions through a user-friendly interface. Building on the use\nof a DL reasoner, OntView follows a \"What you see is what you meant\" paradigm,\nshowing the actual inferred knowledge. One key aspect for this is its ability\nto visualize General Concept Inclusions (GCI), a feature absent in existing\nvisualization tools. Moreover, to avoid a possible information overload,\nOntView also offers different ways to show a simplified view of the ontology\nby: 1) creating ontology summaries by assessing the importance of the concepts\n(according to different available algorithms), 2) focusing the visualization on\nthe existing TBox elements between two given classes and 3) allowing to\nhide/show different branches in a dynamic way without losing the semantics.\nOntView has been released with an open-source license for the whole community.", "AI": {"tldr": "OntView\u662f\u4e00\u4e2a\u65b0\u578b\u672c\u4f53\u53ef\u89c6\u5316\u5de5\u5177\uff0c\u901a\u8fc7\u76f4\u89c2\u754c\u9762\u5c55\u793a\u672c\u4f53\u6982\u5ff5\u53ca\u5176\u5173\u7cfb\uff0c\u652f\u6301GCI\u53ef\u89c6\u5316\uff0c\u5e76\u63d0\u4f9b\u7b80\u5316\u89c6\u56fe\u529f\u80fd\u4ee5\u907f\u514d\u4fe1\u606f\u8fc7\u8f7d\u3002", "motivation": "\u73b0\u6709\u672c\u4f53\u53ef\u89c6\u5316\u5de5\u5177\u65e0\u6cd5\u6709\u6548\u5c55\u793a\u590d\u6742\u672c\u4f53\u7ed3\u6784\uff0c\u9650\u5236\u4e86\u7528\u6237\u5bf9\u4f9d\u8d56\u5173\u7cfb\u548c\u5c5e\u6027\u7684\u7406\u89e3\u3002", "method": "OntView\u57fa\u4e8eDL\u63a8\u7406\u5668\uff0c\u91c7\u7528\u201c\u6240\u89c1\u5373\u6240\u610f\u201d\u8303\u5f0f\uff0c\u652f\u6301GCI\u53ef\u89c6\u5316\uff0c\u5e76\u63d0\u4f9b\u4e09\u79cd\u7b80\u5316\u89c6\u56fe\u65b9\u5f0f\uff1a\u6982\u5ff5\u91cd\u8981\u6027\u8bc4\u4f30\u3001TBox\u5143\u7d20\u805a\u7126\u548c\u52a8\u6001\u5206\u652f\u9690\u85cf\u3002", "result": "OntView\u6210\u529f\u5b9e\u73b0\u4e86\u76f4\u89c2\u4e14\u975e\u5197\u4f59\u7684\u672c\u4f53\u53ef\u89c6\u5316\uff0c\u5e76\u901a\u8fc7\u5f00\u6e90\u65b9\u5f0f\u53d1\u5e03\u3002", "conclusion": "OntView\u89e3\u51b3\u4e86\u73b0\u6709\u5de5\u5177\u5728\u590d\u6742\u672c\u4f53\u53ef\u89c6\u5316\u4e0a\u7684\u4e0d\u8db3\uff0c\u63d0\u5347\u4e86\u7528\u6237\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2507.13903", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.13903", "abs": "https://arxiv.org/abs/2507.13903", "authors": ["Ziliang Li", "Hongming Chen", "Yiyang Lin", "Biyu Ye", "Ximin Lyu"], "title": "AeroThrow: An Autonomous Aerial Throwing System for Precise Payload Delivery", "comment": null, "summary": "Autonomous aerial systems play an increasingly vital role in a wide range of\napplications, particularly for transport and delivery tasks in complex\nenvironments. In airdrop missions, these platforms face the dual challenges of\nabrupt control mode switching and inherent system delays along with control\nerrors. To address these issues, this paper presents an autonomous airdrop\nsystem based on an aerial manipulator (AM). The introduction of additional\nactuated degrees of freedom enables active compensation for UAV tracking\nerrors. By imposing smooth and continuous constraints on the parabolic landing\npoint, the proposed approach generates aerial throwing trajectories that are\nless sensitive to the timing of payload release. A hierarchical disturbance\ncompensation strategy is incorporated into the Nonlinear Model Predictive\nControl (NMPC) framework to mitigate the effects of sudden changes in system\nparameters, while the predictive capabilities of NMPC are further exploited to\nimprove the precision of aerial throwing. Both simulation and real-world\nexperimental results demonstrate that the proposed system achieves greater\nagility and precision in airdrop missions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7a7a\u4e2d\u673a\u68b0\u81c2\uff08AM\uff09\u7684\u81ea\u4e3b\u7a7a\u6295\u7cfb\u7edf\uff0c\u901a\u8fc7\u5f15\u5165\u989d\u5916\u7684\u81ea\u7531\u5ea6\u4e3b\u52a8\u8865\u507f\u65e0\u4eba\u673a\u8ddf\u8e2a\u8bef\u5dee\uff0c\u5e76\u7ed3\u5408\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08NMPC\uff09\u63d0\u9ad8\u7a7a\u6295\u7cbe\u5ea6\u3002", "motivation": "\u81ea\u4e3b\u7a7a\u4e2d\u7cfb\u7edf\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u8fd0\u8f93\u548c\u6295\u9012\u4efb\u52a1\u9762\u4e34\u63a7\u5236\u6a21\u5f0f\u5207\u6362\u548c\u7cfb\u7edf\u5ef6\u8fdf\u7b49\u6311\u6218\uff0c\u9700\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u4ee5\u63d0\u9ad8\u4efb\u52a1\u6027\u80fd\u3002", "method": "\u91c7\u7528\u7a7a\u4e2d\u673a\u68b0\u81c2\u589e\u52a0\u81ea\u7531\u5ea6\uff0c\u7ed3\u5408\u5e73\u6ed1\u7ea6\u675f\u548cNMPC\u6846\u67b6\uff0c\u8bbe\u8ba1\u5206\u5c42\u6270\u52a8\u8865\u507f\u7b56\u7565\u4ee5\u4f18\u5316\u8f68\u8ff9\u751f\u6210\u548c\u63a7\u5236\u3002", "result": "\u4eff\u771f\u548c\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u5728\u7a7a\u6295\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u654f\u6377\u6027\u548c\u7cbe\u5ea6\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u7a7a\u6295\u4efb\u52a1\u4e2d\u7684\u63a7\u5236\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2507.13768", "categories": ["cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2507.13768", "abs": "https://arxiv.org/abs/2507.13768", "authors": ["Renato Ghisellini", "Remo Pareschi", "Marco Pedroni", "Giovanni Battista Raggi"], "title": "From Extraction to Synthesis: Entangled Heuristics for Agent-Augmented Strategic Reasoning", "comment": "Peer-reviewed full paper accepted through a double-blind review\n  process at the HAR 2025 conference (https://har-conf.eu/). The official\n  version will appear in a volume of the Lecture Notes in Computer Science\n  (LNCS) series", "summary": "We present a hybrid architecture for agent-augmented strategic reasoning,\ncombining heuristic extraction, semantic activation, and compositional\nsynthesis. Drawing on sources ranging from classical military theory to\ncontemporary corporate strategy, our model activates and composes multiple\nheuristics through a process of semantic interdependence inspired by research\nin quantum cognition. Unlike traditional decision engines that select the best\nrule, our system fuses conflicting heuristics into coherent and\ncontext-sensitive narratives, guided by semantic interaction modeling and\nrhetorical framing. We demonstrate the framework via a Meta vs. FTC case study,\nwith preliminary validation through semantic metrics. Limitations and\nextensions (e.g., dynamic interference tuning) are discussed.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u542f\u53d1\u5f0f\u63d0\u53d6\u3001\u8bed\u4e49\u6fc0\u6d3b\u548c\u7ec4\u5408\u5408\u6210\u7684\u6df7\u5408\u67b6\u6784\uff0c\u7528\u4e8e\u589e\u5f3a\u4ee3\u7406\u7684\u6218\u7565\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u51b3\u7b56\u5f15\u64ce\u901a\u5e38\u9009\u62e9\u6700\u4f73\u89c4\u5219\uff0c\u800c\u8be5\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u8bed\u4e49\u4ea4\u4e92\u5efa\u6a21\u548c\u4fee\u8f9e\u6846\u67b6\uff0c\u5c06\u51b2\u7a81\u7684\u542f\u53d1\u5f0f\u878d\u5408\u4e3a\u8fde\u8d2f\u4e14\u4e0a\u4e0b\u6587\u654f\u611f\u7684\u53d9\u8ff0\u3002", "method": "\u7ed3\u5408\u7ecf\u5178\u519b\u4e8b\u7406\u8bba\u548c\u73b0\u4ee3\u4f01\u4e1a\u6218\u7565\uff0c\u901a\u8fc7\u8bed\u4e49\u76f8\u4e92\u4f9d\u8d56\u7684\u8fc7\u7a0b\u6fc0\u6d3b\u548c\u7ec4\u5408\u591a\u4e2a\u542f\u53d1\u5f0f\uff0c\u7075\u611f\u6765\u6e90\u4e8e\u91cf\u5b50\u8ba4\u77e5\u7814\u7a76\u3002", "result": "\u901a\u8fc7Meta\u4e0eFTC\u7684\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86\u8be5\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u8bed\u4e49\u6307\u6807\u8fdb\u884c\u4e86\u521d\u6b65\u9a8c\u8bc1\u3002", "conclusion": "\u8ba8\u8bba\u4e86\u52a8\u6001\u5e72\u6270\u8c03\u8c10\u7b49\u5c40\u9650\u6027\u548c\u6269\u5c55\u65b9\u5411\u3002"}}
{"id": "2507.13940", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.13940", "abs": "https://arxiv.org/abs/2507.13940", "authors": ["Qingyi Chen", "Ahmed H. Qureshi"], "title": "NeHMO: Neural Hamilton-Jacobi Reachability Learning for Decentralized Safe Multi-Agent Motion Planning", "comment": null, "summary": "Safe Multi-Agent Motion Planning (MAMP) is a significant challenge in\nrobotics. Despite substantial advancements, existing methods often face a\ndilemma. Decentralized algorithms typically rely on predicting the behavior of\nother agents, sharing contracts, or maintaining communication for safety, while\ncentralized approaches struggle with scalability and real-time decision-making.\nTo address these challenges, we introduce Neural Hamilton-Jacobi Reachability\nLearning (HJR) for Decentralized Multi-Agent Motion Planning. Our method\nprovides scalable neural HJR modeling to tackle high-dimensional configuration\nspaces and capture worst-case collision and safety constraints between agents.\nWe further propose a decentralized trajectory optimization framework that\nincorporates the learned HJR solutions to solve MAMP tasks in real-time. We\ndemonstrate that our method is both scalable and data-efficient, enabling the\nsolution of MAMP problems in higher-dimensional scenarios with complex\ncollision constraints. Our approach generalizes across various dynamical\nsystems, including a 12-dimensional dual-arm setup, and outperforms a range of\nstate-of-the-art techniques in successfully addressing challenging MAMP tasks.\nVideo demonstrations are available at https://youtu.be/IZiePX0p1Mc.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u6c49\u5bc6\u5c14\u987f-\u96c5\u53ef\u6bd4\u53ef\u8fbe\u6027\u5b66\u4e60\uff08HJR\uff09\u7684\u53bb\u4e2d\u5fc3\u5316\u591a\u667a\u80fd\u4f53\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u53ef\u6269\u5c55\u6027\u548c\u5b9e\u65f6\u6027\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u53bb\u4e2d\u5fc3\u5316\u7b97\u6cd5\u4f9d\u8d56\u884c\u4e3a\u9884\u6d4b\u6216\u901a\u4fe1\uff0c\u800c\u96c6\u4e2d\u5f0f\u65b9\u6cd5\u96be\u4ee5\u6269\u5c55\u548c\u5b9e\u65f6\u51b3\u7b56\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u795e\u7ecfHJR\u5efa\u6a21\u5904\u7406\u9ad8\u7ef4\u914d\u7f6e\u7a7a\u95f4\uff0c\u5e76\u7ed3\u5408\u53bb\u4e2d\u5fc3\u5316\u8f68\u8ff9\u4f18\u5316\u6846\u67b6\u5b9e\u73b0\u5b9e\u65f6\u89c4\u5212\u3002", "result": "\u65b9\u6cd5\u572812\u7ef4\u53cc\u81c2\u8bbe\u7f6e\u7b49\u590d\u6742\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5177\u6709\u53ef\u6269\u5c55\u6027\u548c\u6570\u636e\u9ad8\u6548\u6027\uff0c\u9002\u7528\u4e8e\u9ad8\u7ef4\u590d\u6742\u4efb\u52a1\u3002"}}
{"id": "2507.13825", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13825", "abs": "https://arxiv.org/abs/2507.13825", "authors": ["Haoyang Li", "Yuming Xu", "Yiming Li", "Hanmo Liu", "Darian Li", "Chen Jason Zhang", "Lei Chen", "Qing Li"], "title": "When Speed meets Accuracy: an Efficient and Effective Graph Model for Temporal Link Prediction", "comment": "Submitted in 2024. Accepted in 2025", "summary": "Temporal link prediction in dynamic graphs is a critical task with\napplications in diverse domains such as social networks, recommendation\nsystems, and e-commerce platforms. While existing Temporal Graph Neural\nNetworks (T-GNNs) have achieved notable success by leveraging complex\narchitectures to model temporal and structural dependencies, they often suffer\nfrom scalability and efficiency challenges due to high computational overhead.\nIn this paper, we propose EAGLE, a lightweight framework that integrates\nshort-term temporal recency and long-term global structural patterns. EAGLE\nconsists of a time-aware module that aggregates information from a node's most\nrecent neighbors to reflect its immediate preferences, and a structure-aware\nmodule that leverages temporal personalized PageRank to capture the influence\nof globally important nodes. To balance these attributes, EAGLE employs an\nadaptive weighting mechanism to dynamically adjust their contributions based on\ndata characteristics. Also, EAGLE eliminates the need for complex multi-hop\nmessage passing or memory-intensive mechanisms, enabling significant\nimprovements in efficiency. Extensive experiments on seven real-world temporal\ngraphs demonstrate that EAGLE consistently achieves superior performance\nagainst state-of-the-art T-GNNs in both effectiveness and efficiency,\ndelivering more than a 50x speedup over effective transformer-based T-GNNs.", "AI": {"tldr": "EAGLE\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u6574\u5408\u77ed\u671f\u65f6\u95f4\u65b0\u8fd1\u6027\u548c\u957f\u671f\u5168\u5c40\u7ed3\u6784\u6a21\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u52a8\u6001\u56fe\u65f6\u95f4\u94fe\u8def\u9884\u6d4b\u7684\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u73b0\u6709T-GNNs\u56e0\u8ba1\u7b97\u5f00\u9500\u5927\u800c\u9762\u4e34\u53ef\u6269\u5c55\u6027\u548c\u6548\u7387\u95ee\u9898\uff0c\u9700\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "EAGLE\u7ed3\u5408\u65f6\u95f4\u611f\u77e5\u6a21\u5757\uff08\u805a\u5408\u6700\u8fd1\u90bb\u4fe1\u606f\uff09\u548c\u7ed3\u6784\u611f\u77e5\u6a21\u5757\uff08\u5229\u7528PageRank\u6355\u83b7\u5168\u5c40\u8282\u70b9\u5f71\u54cd\uff09\uff0c\u5e76\u901a\u8fc7\u81ea\u9002\u5e94\u6743\u91cd\u673a\u5236\u5e73\u8861\u4e24\u8005\u3002", "result": "\u5728\u4e03\u4e2a\u771f\u5b9e\u4e16\u754c\u52a8\u6001\u56fe\u4e0a\uff0cEAGLE\u5728\u6027\u80fd\u548c\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709T-GNNs\uff0c\u901f\u5ea6\u63d0\u5347\u8d8550\u500d\u3002", "conclusion": "EAGLE\u901a\u8fc7\u7b80\u5316\u67b6\u6784\u548c\u52a8\u6001\u8c03\u6574\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u6709\u6548\u7684\u65f6\u95f4\u94fe\u8def\u9884\u6d4b\u3002"}}
{"id": "2507.13969", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.13969", "abs": "https://arxiv.org/abs/2507.13969", "authors": ["Maria Eduarda Silva de Macedo", "Ana Paula Chiarelli de Souza", "Roberto Silvio Ubertino Rosso Jr.", "Yuri Kaszubowski Lopes"], "title": "A Minimalist Controller for Autonomously Self-Aggregating Robotic Swarms: Enabling Compact Formations in Multitasking Scenarios", "comment": "7 pages total (6 pages of content + 1 page of references). Short\n  paper manuscript submitted to TAROS 2025", "summary": "The deployment of simple emergent behaviors in swarm robotics has been\nwell-rehearsed in the literature. A recent study has shown how self-aggregation\nis possible in a multitask approach -- where multiple self-aggregation task\ninstances occur concurrently in the same environment. The multitask approach\nposes new challenges, in special, how the dynamic of each group impacts the\nperformance of others. So far, the multitask self-aggregation of groups of\nrobots suffers from generating a circular formation -- that is not fully\ncompact -- or is not fully autonomous. In this paper, we present a multitask\nself-aggregation where groups of homogeneous robots sort themselves into\ndifferent compact clusters, relying solely on a line-of-sight sensor. Our\nmultitask self-aggregation behavior was able to scale well and achieve a\ncompact formation. We report scalability results from a series of simulation\ntrials with different configurations in the number of groups and the number of\nrobots per group. We were able to improve the multitask self-aggregation\nbehavior performance in terms of the compactness of the clusters, keeping the\nproportion of clustered robots found in other studies.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u4efb\u52a1\u81ea\u805a\u96c6\u65b9\u6cd5\uff0c\u4f7f\u540c\u8d28\u673a\u5668\u4eba\u4ec5\u4f9d\u8d56\u89c6\u7ebf\u4f20\u611f\u5668\u5f62\u6210\u7d27\u51d1\u96c6\u7fa4\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4e2d\u96c6\u7fa4\u52a8\u6001\u76f8\u4e92\u5f71\u54cd\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u591a\u4efb\u52a1\u81ea\u805a\u96c6\u65b9\u6cd5\u5b58\u5728\u96c6\u7fa4\u52a8\u6001\u76f8\u4e92\u5e72\u6270\u3001\u5f62\u6210\u4e0d\u7d27\u51d1\u7684\u5706\u5f62\u7ed3\u6784\u6216\u7f3a\u4e4f\u5b8c\u5168\u81ea\u4e3b\u6027\u7684\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u540c\u8d28\u673a\u5668\u4eba\uff0c\u4ec5\u4f9d\u8d56\u89c6\u7ebf\u4f20\u611f\u5668\u5b9e\u73b0\u591a\u4efb\u52a1\u81ea\u805a\u96c6\uff0c\u5f62\u6210\u7d27\u51d1\u96c6\u7fa4\u3002", "result": "\u901a\u8fc7\u6a21\u62df\u8bd5\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u53ef\u6269\u5c55\u6027\uff0c\u96c6\u7fa4\u7d27\u51d1\u6027\u5f97\u5230\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5176\u4ed6\u7814\u7a76\u4e2d\u96c6\u7fa4\u673a\u5668\u4eba\u7684\u6bd4\u4f8b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u591a\u4efb\u52a1\u81ea\u805a\u96c6\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u89e3\u51b3\u4e86\u7d27\u51d1\u6027\u548c\u81ea\u4e3b\u6027\u95ee\u9898\u3002"}}
{"id": "2507.13846", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13846", "abs": "https://arxiv.org/abs/2507.13846", "authors": ["Kathrin Korte", "Christian Medeiros Adriano", "Sona Ghahremani", "Holger Giese"], "title": "Causal Knowledge Transfer for Multi-Agent Reinforcement Learning in Dynamic Environments", "comment": null, "summary": "[Context] Multi-agent reinforcement learning (MARL) has achieved notable\nsuccess in environments where agents must learn coordinated behaviors. However,\ntransferring knowledge across agents remains challenging in non-stationary\nenvironments with changing goals. [Problem] Traditional knowledge transfer\nmethods in MARL struggle to generalize, and agents often require costly\nretraining to adapt. [Approach] This paper introduces a causal knowledge\ntransfer framework that enables RL agents to learn and share compact causal\nrepresentations of paths within a non-stationary environment. As the\nenvironment changes (new obstacles), agents' collisions require adaptive\nrecovery strategies. We model each collision as a causal intervention\ninstantiated as a sequence of recovery actions (a macro) whose effect\ncorresponds to a causal knowledge of how to circumvent the obstacle while\nincreasing the chances of achieving the agent's goal (maximizing cumulative\nreward). This recovery action macro is transferred online from a second agent\nand is applied in a zero-shot fashion, i.e., without retraining, just by\nquerying a lookup model with local context information (collisions). [Results]\nOur findings reveal two key insights: (1) agents with heterogeneous goals were\nable to bridge about half of the gap between random exploration and a fully\nretrained policy when adapting to new environments, and (2) the impact of\ncausal knowledge transfer depends on the interplay between environment\ncomplexity and agents' heterogeneous goals.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u56e0\u679c\u77e5\u8bc6\u8f6c\u79fb\u6846\u67b6\uff0c\u5e2e\u52a9\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u5728\u975e\u9759\u6001\u73af\u5883\u4e2d\u5171\u4eab\u7d27\u51d1\u7684\u56e0\u679c\u8868\u793a\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u9002\u5e94\u3002", "motivation": "\u4f20\u7edfMARL\u77e5\u8bc6\u8f6c\u79fb\u65b9\u6cd5\u5728\u975e\u9759\u6001\u73af\u5883\u4e2d\u96be\u4ee5\u6cdb\u5316\uff0c\u667a\u80fd\u4f53\u9700\u9ad8\u6210\u672c\u91cd\u65b0\u8bad\u7ec3\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u56e0\u679c\u77e5\u8bc6\u8f6c\u79fb\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5efa\u6a21\u78b0\u649e\u4e3a\u56e0\u679c\u5e72\u9884\uff0c\u751f\u6210\u6062\u590d\u52a8\u4f5c\u5b8f\uff08macro\uff09\uff0c\u5e76\u5728\u667a\u80fd\u4f53\u95f4\u5728\u7ebf\u8f6c\u79fb\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u9002\u5e94\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u586b\u8865\u968f\u673a\u63a2\u7d22\u4e0e\u5b8c\u5168\u91cd\u65b0\u8bad\u7ec3\u7b56\u7565\u95f4\u7ea6\u4e00\u534a\u7684\u5dee\u8ddd\uff0c\u4e14\u6548\u679c\u53d7\u73af\u5883\u590d\u6742\u5ea6\u548c\u667a\u80fd\u4f53\u76ee\u6807\u5f02\u8d28\u6027\u5f71\u54cd\u3002", "conclusion": "\u56e0\u679c\u77e5\u8bc6\u8f6c\u79fb\u6846\u67b6\u4e3aMARL\u5728\u975e\u9759\u6001\u73af\u5883\u4e2d\u7684\u9002\u5e94\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.13970", "categories": ["cs.RO", "cs.AI", "I.2; I.2.9; I.2.10"], "pdf": "https://arxiv.org/pdf/2507.13970", "abs": "https://arxiv.org/abs/2507.13970", "authors": ["Casper Br\u00f6cheler", "Thomas Vroom", "Derrick Timmermans", "Alan van den Akker", "Guangzhi Tang", "Charalampos S. Kouzinopoulos", "Rico M\u00f6ckel"], "title": "A segmented robot grasping perception neural network for edge AI", "comment": "Accepted by SMC 2025", "summary": "Robotic grasping, the ability of robots to reliably secure and manipulate\nobjects of varying shapes, sizes and orientations, is a complex task that\nrequires precise perception and control. Deep neural networks have shown\nremarkable success in grasp synthesis by learning rich and abstract\nrepresentations of objects. When deployed at the edge, these models can enable\nlow-latency, low-power inference, making real-time grasping feasible in\nresource-constrained environments. This work implements Heatmap-Guided Grasp\nDetection, an end-to-end framework for the detection of 6-Dof grasp poses, on\nthe GAP9 RISC-V System-on-Chip. The model is optimised using hardware-aware\ntechniques, including input dimensionality reduction, model partitioning, and\nquantisation. Experimental evaluation on the GraspNet-1Billion benchmark\nvalidates the feasibility of fully on-chip inference, highlighting the\npotential of low-power MCUs for real-time, autonomous manipulation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u70ed\u56fe\u5f15\u5bfc\u76846\u81ea\u7531\u5ea6\u6293\u53d6\u68c0\u6d4b\u6846\u67b6\uff0c\u5e76\u5728GAP9 RISC-V\u82af\u7247\u4e0a\u5b9e\u73b0\uff0c\u901a\u8fc7\u786c\u4ef6\u4f18\u5316\u6280\u672f\u5b9e\u73b0\u4f4e\u529f\u8017\u5b9e\u65f6\u6293\u53d6\u3002", "motivation": "\u673a\u5668\u4eba\u6293\u53d6\u9700\u8981\u7cbe\u786e\u7684\u611f\u77e5\u4e0e\u63a7\u5236\uff0c\u800c\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u6293\u53d6\u5408\u6210\u4e2d\u8868\u73b0\u4f18\u5f02\u3002\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u8fd9\u4e9b\u6a21\u578b\u53ef\u4ee5\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u3001\u4f4e\u529f\u8017\u7684\u5b9e\u65f6\u6293\u53d6\u3002", "method": "\u91c7\u7528\u70ed\u56fe\u5f15\u5bfc\u7684\u6293\u53d6\u68c0\u6d4b\u6846\u67b6\uff0c\u7ed3\u5408\u8f93\u5165\u964d\u7ef4\u3001\u6a21\u578b\u5206\u533a\u548c\u91cf\u5316\u7b49\u786c\u4ef6\u4f18\u5316\u6280\u672f\uff0c\u5728GAP9 RISC-V\u82af\u7247\u4e0a\u5b9e\u73b0\u3002", "result": "\u5728GraspNet-1Billion\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9a8c\u8bc1\u4e86\u5168\u82af\u7247\u63a8\u7406\u7684\u53ef\u884c\u6027\uff0c\u5c55\u793a\u4e86\u4f4e\u529f\u8017\u5fae\u63a7\u5236\u5668\u5728\u5b9e\u65f6\u81ea\u4e3b\u64cd\u4f5c\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u5b9e\u65f6\u673a\u5668\u4eba\u6293\u53d6\u63d0\u4f9b\u4e86\u53ef\u884c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.13874", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13874", "abs": "https://arxiv.org/abs/2507.13874", "authors": ["Mateusz Bystro\u0144ski", "Miko\u0142aj Ho\u0142ysz", "Grzegorz Piotrowski", "Nitesh V. Chawla", "Tomasz Kajdanowicz"], "title": "Large Language Models as Innovators: A Framework to Leverage Latent Space Exploration for Novelty Discovery", "comment": null, "summary": "Innovative idea generation remains a core challenge in AI, as large language\nmodels (LLMs) often struggle to produce outputs that are both novel and\nrelevant. Despite their fluency, LLMs tend to replicate patterns seen during\ntraining, limiting their ability to diverge creatively without extensive prompt\nengineering. Prior work has addressed this through domain-specific heuristics\nand structured prompting pipelines, but such solutions are brittle and\ndifficult to generalize. In this paper, we propose a model-agnostic\nlatent-space ideation framework that enables controlled, scalable creativity by\nnavigating the continuous embedding space of ideas. Unlike prior methods, our\nframework requires no handcrafted rules and adapts easily to different domains,\ninput formats, and creative tasks. This paper introduces an early-stage\nprototype of our method, outlining the conceptual framework and preliminary\nresults highlighting its potential as a general-purpose co-ideator for human-AI\ncollaboration.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u578b\u65e0\u5173\u7684\u6f5c\u5728\u7a7a\u95f4\u521b\u610f\u6846\u67b6\uff0c\u901a\u8fc7\u5bfc\u822a\u8fde\u7eed\u5d4c\u5165\u7a7a\u95f4\u5b9e\u73b0\u53ef\u63a7\u3001\u53ef\u6269\u5c55\u7684\u521b\u9020\u529b\uff0c\u65e0\u9700\u624b\u5de5\u89c4\u5219\u4e14\u6613\u4e8e\u9002\u5e94\u4e0d\u540c\u9886\u57df\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u751f\u6210\u65b0\u9896\u4e14\u76f8\u5173\u7684\u5185\u5bb9\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u901a\u5e38\u4f9d\u8d56\u8bad\u7ec3\u6570\u636e\u7684\u6a21\u5f0f\u590d\u5236\uff0c\u7f3a\u4e4f\u521b\u9020\u6027\u53d1\u6563\u80fd\u529b\u3002\u73b0\u6709\u89e3\u51b3\u65b9\u6848\uff08\u5982\u9886\u57df\u7279\u5b9a\u542f\u53d1\u5f0f\u65b9\u6cd5\uff09\u8106\u5f31\u4e14\u96be\u4ee5\u6cdb\u5316\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u578b\u65e0\u5173\u7684\u6f5c\u5728\u7a7a\u95f4\u521b\u610f\u6846\u67b6\uff0c\u901a\u8fc7\u5bfc\u822a\u8fde\u7eed\u5d4c\u5165\u7a7a\u95f4\u5b9e\u73b0\u53ef\u63a7\u521b\u9020\u529b\uff0c\u65e0\u9700\u624b\u5de5\u89c4\u5219\uff0c\u9002\u5e94\u6027\u5f3a\u3002", "result": "\u521d\u6b65\u7ed3\u679c\u663e\u793a\u8be5\u6846\u67b6\u5177\u6709\u6f5c\u529b\uff0c\u53ef\u4f5c\u4e3a\u901a\u7528\u7684\u4eba\u7c7b-AI\u534f\u4f5c\u5171\u521b\u5de5\u5177\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aAI\u521b\u610f\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u79cd\u7075\u6d3b\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u671b\u63a8\u52a8\u4eba\u7c7b-AI\u534f\u4f5c\u7684\u53d1\u5c55\u3002"}}
{"id": "2507.14043", "categories": ["cs.RO", "cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2507.14043", "abs": "https://arxiv.org/abs/2507.14043", "authors": ["Genliang Li", "Yaxin Cui", "Jinyu Su"], "title": "A multi-strategy improved snake optimizer for three-dimensional UAV path planning and engineering problems", "comment": "59 pages, 22 figures", "summary": "Metaheuristic algorithms have gained widespread application across various\nfields owing to their ability to generate diverse solutions. One such algorithm\nis the Snake Optimizer (SO), a progressive optimization approach. However, SO\nsuffers from the issues of slow convergence speed and susceptibility to local\noptima. In light of these shortcomings, we propose a novel Multi-strategy\nImproved Snake Optimizer (MISO). Firstly, we propose a new adaptive random\ndisturbance strategy based on sine function to alleviate the risk of getting\ntrapped in a local optimum. Secondly, we introduce adaptive Levy flight\nstrategy based on scale factor and leader and endow the male snake leader with\nflight capability, which makes it easier for the algorithm to leap out of the\nlocal optimum and find the global optimum. More importantly, we put forward a\nposition update strategy combining elite leadership and Brownian motion,\neffectively accelerating the convergence speed while ensuring precision.\nFinally, to demonstrate the performance of MISO, we utilize 30 CEC2017 test\nfunctions and the CEC2022 test suite, comparing it with 11 popular algorithms\nacross different dimensions to validate its effectiveness. Moreover, Unmanned\nAerial Vehicle (UAV) has been widely used in various fields due to its\nadvantages of low cost, high mobility and easy operation. However, the UAV path\nplanning problem is crucial for flight safety and efficiency, and there are\nstill challenges in establishing and optimizing the path model. Therefore, we\napply MISO to the UAV 3D path planning problem as well as 6 engineering design\nproblems to assess its feasibility in practical applications. The experimental\nresults demonstrate that MISO exceeds other competitive algorithms in terms of\nsolution quality and stability, establishing its strong potential for\napplication.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u7b56\u7565\u6539\u8fdb\u7684\u86c7\u4f18\u5316\u7b97\u6cd5\uff08MISO\uff09\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u968f\u673a\u6270\u52a8\u3001Levy\u98de\u884c\u7b56\u7565\u548c\u7cbe\u82f1\u9886\u5bfc\u7ed3\u5408\u5e03\u6717\u8fd0\u52a8\u7684\u4f4d\u7f6e\u66f4\u65b0\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u539f\u86c7\u4f18\u5316\u7b97\u6cd5\uff08SO\uff09\u6536\u655b\u6162\u548c\u6613\u9677\u5165\u5c40\u90e8\u6700\u4f18\u7684\u95ee\u9898\uff0c\u5e76\u5728\u6d4b\u8bd5\u51fd\u6570\u548c\u65e0\u4eba\u673a\u8def\u5f84\u89c4\u5212\u4e2d\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u86c7\u4f18\u5316\u7b97\u6cd5\uff08SO\uff09\u5b58\u5728\u6536\u655b\u901f\u5ea6\u6162\u548c\u6613\u9677\u5165\u5c40\u90e8\u6700\u4f18\u7684\u7f3a\u70b9\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u6548\u679c\u3002\u56e0\u6b64\uff0c\u9700\u8981\u6539\u8fdb\u7b97\u6cd5\u4ee5\u63d0\u5347\u5176\u6027\u80fd\u3002", "method": "1. \u57fa\u4e8e\u6b63\u5f26\u51fd\u6570\u7684\u81ea\u9002\u5e94\u968f\u673a\u6270\u52a8\u7b56\u7565\uff1b2. \u57fa\u4e8e\u5c3a\u5ea6\u56e0\u5b50\u548c\u9886\u5bfc\u8005\u7684\u81ea\u9002\u5e94Levy\u98de\u884c\u7b56\u7565\uff1b3. \u7ed3\u5408\u7cbe\u82f1\u9886\u5bfc\u548c\u5e03\u6717\u8fd0\u52a8\u7684\u4f4d\u7f6e\u66f4\u65b0\u7b56\u7565\u3002", "result": "\u5728CEC2017\u548cCEC2022\u6d4b\u8bd5\u51fd\u6570\u53ca\u65e0\u4eba\u673a3D\u8def\u5f84\u89c4\u5212\u4e2d\uff0cMISO\u8868\u73b0\u4f18\u4e8e11\u79cd\u6d41\u884c\u7b97\u6cd5\uff0c\u5177\u6709\u66f4\u9ad8\u7684\u89e3\u8d28\u91cf\u548c\u7a33\u5b9a\u6027\u3002", "conclusion": "MISO\u5728\u7406\u8bba\u548c\u5b9e\u9645\u5e94\u7528\u4e2d\u5747\u8868\u73b0\u51fa\u8272\uff0c\u5c55\u793a\u4e86\u5176\u5728\u590d\u6742\u4f18\u5316\u95ee\u9898\u4e2d\u7684\u5f3a\u5927\u6f5c\u529b\u3002"}}
{"id": "2507.13956", "categories": ["cs.AI", "cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.13956", "abs": "https://arxiv.org/abs/2507.13956", "authors": ["Yutao Jin", "Haowen Xiao", "Jielei Chu", "Fengmao Lv", "Yuxiao Li", "Tianrui Li"], "title": "Cross-modal Causal Intervention for Alzheimer's Disease Prediction", "comment": null, "summary": "Mild Cognitive Impairment (MCI) serves as a prodromal stage of Alzheimer's\nDisease (AD), where early identification and intervention can effectively slow\nthe progression to dementia. However, diagnosing AD remains a significant\nchallenge in neurology due to the confounders caused mainly by the selection\nbias of multimodal data and the complex relationships between variables. To\naddress these issues, we propose a novel visual-language causal intervention\nframework named Alzheimer's Disease Prediction with Cross-modal Causal\nIntervention (ADPC) for diagnostic assistance. Our ADPC employs large language\nmodel (LLM) to summarize clinical data under strict templates, maintaining\nstructured text outputs even with incomplete or unevenly distributed datasets.\nThe ADPC model utilizes Magnetic Resonance Imaging (MRI), functional MRI (fMRI)\nimages and textual data generated by LLM to classify participants into\nCognitively Normal (CN), MCI, and AD categories. Because of the presence of\nconfounders, such as neuroimaging artifacts and age-related biomarkers,\nnon-causal models are likely to capture spurious input-output correlations,\ngenerating less reliable results. Our framework implicitly eliminates\nconfounders through causal intervention. Experimental results demonstrate the\noutstanding performance of our method in distinguishing CN/MCI/AD cases,\nachieving state-of-the-art (SOTA) metrics across most evaluation metrics. The\nstudy showcases the potential of integrating causal reasoning with multi-modal\nlearning for neurological disease diagnosis.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aADPC\u7684\u65b0\u578b\u89c6\u89c9\u8bed\u8a00\u56e0\u679c\u5e72\u9884\u6846\u67b6\uff0c\u7528\u4e8e\u8f85\u52a9\u8bca\u65ad\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\uff08AD\uff09\uff0c\u901a\u8fc7\u6d88\u9664\u6df7\u6742\u56e0\u7d20\u63d0\u5347\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u65e9\u671f\u8bc6\u522b\u8f7b\u5ea6\u8ba4\u77e5\u969c\u788d\uff08MCI\uff09\u548cAD\u5bf9\u5ef6\u7f13\u75f4\u5446\u8fdb\u5c55\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u56e0\u6570\u636e\u9009\u62e9\u504f\u5dee\u548c\u53d8\u91cf\u95f4\u590d\u6742\u5173\u7cfb\u800c\u53d7\u9650\u3002", "method": "\u7ed3\u5408MRI\u3001fMRI\u56fe\u50cf\u548cLLM\u751f\u6210\u7684\u6587\u672c\u6570\u636e\uff0c\u901a\u8fc7\u56e0\u679c\u5e72\u9884\u6d88\u9664\u6df7\u6742\u56e0\u7d20\uff0c\u5206\u7c7bCN/MCI/AD\u3002", "result": "\u5b9e\u9a8c\u8868\u660eADPC\u5728\u5206\u7c7b\u6027\u80fd\u4e0a\u8fbe\u5230SOTA\u6c34\u5e73\u3002", "conclusion": "\u7814\u7a76\u5c55\u793a\u4e86\u56e0\u679c\u63a8\u7406\u4e0e\u591a\u6a21\u6001\u5b66\u4e60\u7ed3\u5408\u5728\u795e\u7ecf\u75be\u75c5\u8bca\u65ad\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.14049", "categories": ["cs.RO", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.14049", "abs": "https://arxiv.org/abs/2507.14049", "authors": ["Pawe\u0142 Budzianowski", "Wesley Maa", "Matthew Freed", "Jingxiang Mo", "Winston Hsiao", "Aaron Xie", "Tomasz M\u0142oduchowski", "Viraj Tipnis", "Benjamin Bolte"], "title": "EdgeVLA: Efficient Vision-Language-Action Models", "comment": null, "summary": "Vision-Language Models (VLMs) have emerged as a promising approach to address\nthe data scarcity challenge in robotics, enabling the development of\ngeneralizable visuomotor control policies. While models like OpenVLA showcase\nthe potential of this paradigm, deploying large-scale VLMs on\nresource-constrained mobile manipulation systems remains a significant hurdle.\nThis paper introduces Edge VLA (EVLA), a novel approach designed to\nsignificantly enhance the inference speed of Vision-Language-Action (VLA)\nmodels. EVLA maintains the representational power of these models while\nenabling real-time performance on edge devices. We achieve this through two key\ninnovations: 1) Eliminating the autoregressive requirement for end-effector\nposition prediction, leading to a 7x speedup in inference, and 2) Leveraging\nthe efficiency of Small Language Models (SLMs), demonstrating comparable\ntraining performance to larger models with significantly reduced computational\ndemands. Our early results demonstrate that EVLA achieves comparable training\ncharacteristics to OpenVLA while offering substantial gains in inference speed\nand memory efficiency. We release our model checkpoints and training\n\\href{https://github.com/kscalelabs/evla }{codebase} to foster further\nresearch.", "AI": {"tldr": "EVLA\u662f\u4e00\u79cd\u65b0\u578b\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u7684\u63a8\u7406\u901f\u5ea6\uff0c\u9002\u7528\u4e8e\u8fb9\u7f18\u8bbe\u5907\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u7684\u8868\u73b0\u529b\u3002", "motivation": "\u89e3\u51b3\u5728\u8d44\u6e90\u53d7\u9650\u7684\u79fb\u52a8\u64cd\u4f5c\u7cfb\u7edf\u4e2d\u90e8\u7f72\u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u7684\u6311\u6218\u3002", "method": "1\uff09\u6d88\u9664\u81ea\u56de\u5f52\u9700\u6c42\uff0c\u5b9e\u73b07\u500d\u63a8\u7406\u52a0\u901f\uff1b2\uff09\u5229\u7528\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLMs\uff09\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u3002", "result": "EVLA\u5728\u63a8\u7406\u901f\u5ea6\u548c\u5185\u5b58\u6548\u7387\u4e0a\u663e\u8457\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u4e0eOpenVLA\u76f8\u5f53\u7684\u8bad\u7ec3\u6027\u80fd\u3002", "conclusion": "EVLA\u4e3a\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u5b9e\u65f6VLA\u6a21\u578b\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u5e76\u5f00\u6e90\u4ee3\u7801\u4ee5\u4fc3\u8fdb\u7814\u7a76\u3002"}}
{"id": "2507.13958", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.13958", "abs": "https://arxiv.org/abs/2507.13958", "authors": ["Pedro Cabalar", "Mart\u00edn Di\u00e9guez", "Fran\u00e7ois Olivier", "Torsten Schaub", "Igor St\u00e9phan"], "title": "Towards Constraint Temporal Answer Set Programming", "comment": null, "summary": "Reasoning about dynamic systems with a fine-grained temporal and numeric\nresolution presents significant challenges for logic-based approaches like\nAnswer Set Programming (ASP). To address this, we introduce and elaborate upon\na novel temporal and constraint-based extension of the logic of Here-and-There\nand its nonmonotonic equilibrium extension, representing, to the best of our\nknowledge, the first approach to nonmonotonic temporal reasoning with\nconstraints specifically tailored for ASP. This expressive system is achieved\nby a synergistic combination of two foundational ASP extensions: the\nlinear-time logic of Here-and-There, providing robust nonmonotonic temporal\nreasoning capabilities, and the logic of Here-and-There with constraints,\nenabling the direct integration and manipulation of numeric constraints, among\nothers. This work establishes the foundational logical framework for tackling\ncomplex dynamic systems with high resolution within the ASP paradigm.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65f6\u6001\u548c\u7ea6\u675f\u6269\u5c55\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u52a8\u6001\u7cfb\u7edf\u4e2d\u9ad8\u7cbe\u5ea6\u65f6\u6001\u548c\u6570\u503c\u63a8\u7406\u7684\u6311\u6218\uff0c\u7ed3\u5408\u4e86Here-and-There\u903b\u8f91\u7684\u7ebf\u6027\u65f6\u95f4\u7248\u672c\u548c\u7ea6\u675f\u903b\u8f91\u3002", "motivation": "\u52a8\u6001\u7cfb\u7edf\u4e2d\u9ad8\u7cbe\u5ea6\u65f6\u6001\u548c\u6570\u503c\u63a8\u7406\u5bf9\u903b\u8f91\u65b9\u6cd5\uff08\u5982ASP\uff09\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u8981\u65b0\u7684\u6269\u5c55\u6765\u652f\u6301\u975e\u5355\u8c03\u65f6\u6001\u63a8\u7406\u548c\u7ea6\u675f\u5904\u7406\u3002", "method": "\u7ed3\u5408\u7ebf\u6027\u65f6\u95f4\u7684Here-and-There\u903b\u8f91\uff08\u652f\u6301\u975e\u5355\u8c03\u65f6\u6001\u63a8\u7406\uff09\u548c\u5e26\u7ea6\u675f\u7684Here-and-There\u903b\u8f91\uff08\u652f\u6301\u6570\u503c\u7ea6\u675f\uff09\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u903b\u8f91\u6846\u67b6\u3002", "result": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u4e13\u4e3aASP\u8bbe\u8ba1\u7684\u975e\u5355\u8c03\u65f6\u6001\u63a8\u7406\u4e0e\u7ea6\u675f\u5904\u7406\u7684\u903b\u8f91\u6846\u67b6\uff0c\u9002\u7528\u4e8e\u9ad8\u7cbe\u5ea6\u52a8\u6001\u7cfb\u7edf\u5efa\u6a21\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3aASP\u8303\u5f0f\u4e0b\u5904\u7406\u590d\u6742\u52a8\u6001\u7cfb\u7edf\u63d0\u4f9b\u4e86\u57fa\u7840\u903b\u8f91\u6846\u67b6\uff0c\u586b\u8865\u4e86\u975e\u5355\u8c03\u65f6\u6001\u63a8\u7406\u4e0e\u7ea6\u675f\u7ed3\u5408\u7684\u7a7a\u767d\u3002"}}
{"id": "2507.14059", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14059", "abs": "https://arxiv.org/abs/2507.14059", "authors": ["Tianyuan Wang", "Mark A Post", "Mathieu Deremetz"], "title": "Design of a Modular Mobile Inspection and Maintenance Robot for an Orbital Servicing Hub", "comment": "In proceedings of the Towards Autonomous Robotic Systems 2025\n  conference (TAROS 2025), York, UK 6 pages, one page of references, 6 figures", "summary": "The use of autonomous robots in space is an essential part of the \"New Space\"\ncommercial ecosystem of assembly and re-use of space hardware components in\nEarth orbit and beyond. The STARFAB project aims to create a ground\ndemonstration of an orbital automated warehouse as a hub for sustainable\ncommercial operations and servicing. A critical part of this fully-autonomous\nrobotic facility will be the capability to monitor, inspect, and assess the\ncondition of both the components stored in the warehouse, and the STARFAB\nfacility itself. This paper introduces ongoing work on the STARFAB Mobile\nInspection Module (MIM). The MIM uses Standard Interconnects (SI) so that it\ncan be carried by Walking Manipulators (WM) as an independently-mobile robot,\nand multiple MIMs can be stored and retrieved as needed for operations on\nSTARFAB. The MIM carries high-resolution cameras, a 3D profilometer, and a\nthermal imaging sensor, with the capability to add other modular sensors. A\ngrasping tool and torque wrench are stored within the modular body for use by\nan attached WM for maintenance operations. Implementation and testing is still\nongoing at the time of writing. This paper details the concept of operations\nfor the MIM as an on-orbit autonomous inspection and maintenance system, the\nmechanical and electronic design of the MIM, and the sensors package used for\nnon-destructive testing.", "AI": {"tldr": "STARFAB\u9879\u76ee\u5f00\u53d1\u4e86\u4e00\u79cd\u79fb\u52a8\u68c0\u67e5\u6a21\u5757\uff08MIM\uff09\uff0c\u7528\u4e8e\u8f68\u9053\u81ea\u52a8\u5316\u4ed3\u5e93\u7684\u81ea\u4e3b\u68c0\u67e5\u548c\u7ef4\u62a4\u3002", "motivation": "\u4e3a\u652f\u6301\u2018\u65b0\u592a\u7a7a\u2019\u5546\u4e1a\u751f\u6001\u4e2d\u7684\u7a7a\u95f4\u786c\u4ef6\u7ec4\u4ef6\u7ec4\u88c5\u548c\u518d\u5229\u7528\uff0c\u9700\u8981\u81ea\u4e3b\u673a\u5668\u4eba\u8fdb\u884c\u76d1\u63a7\u548c\u7ef4\u62a4\u3002", "method": "MIM\u91c7\u7528\u6807\u51c6\u63a5\u53e3\u8bbe\u8ba1\uff0c\u53ef\u88ab\u884c\u8d70\u673a\u68b0\u81c2\u643a\u5e26\uff0c\u914d\u5907\u591a\u79cd\u4f20\u611f\u5668\u548c\u5de5\u5177\uff0c\u652f\u6301\u6a21\u5757\u5316\u6269\u5c55\u3002", "result": "MIM\u5177\u5907\u9ad8\u5206\u8fa8\u7387\u76f8\u673a\u30013D\u8f6e\u5ed3\u4eea\u3001\u70ed\u6210\u50cf\u4f20\u611f\u5668\u7b49\u529f\u80fd\uff0c\u5e76\u652f\u6301\u7ef4\u62a4\u5de5\u5177\u7684\u4f7f\u7528\u3002", "conclusion": "MIM\u4e3a\u8f68\u9053\u81ea\u4e3b\u68c0\u67e5\u548c\u7ef4\u62a4\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u76ee\u524d\u4ecd\u5728\u6d4b\u8bd5\u4e2d\u3002"}}
{"id": "2507.14032", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.14032", "abs": "https://arxiv.org/abs/2507.14032", "authors": ["Lam Nguyen", "Erika Barcelos", "Roger French", "Yinghui Wu"], "title": "KROMA: Ontology Matching with Knowledge Retrieval and Large Language Models", "comment": "Accepted to the 24th International Semantic Web Conference Research\n  Track (ISWC 2025)", "summary": "Ontology Matching (OM) is a cornerstone task of semantic interoperability,\nyet existing systems often rely on handcrafted rules or specialized models with\nlimited adaptability. We present KROMA, a novel OM framework that harnesses\nLarge Language Models (LLMs) within a Retrieval-Augmented Generation (RAG)\npipeline to dynamically enrich the semantic context of OM tasks with\nstructural, lexical, and definitional knowledge. To optimize both performance\nand efficiency, KROMA integrates a bisimilarity-based concept matching and a\nlightweight ontology refinement step, which prune candidate concepts and\nsubstantially reduce the communication overhead from invoking LLMs. Through\nexperiments on multiple benchmark datasets, we show that integrating knowledge\nretrieval with context-augmented LLMs significantly enhances ontology matching,\noutperforming both classic OM systems and cutting-edge LLM-based approaches\nwhile keeping communication overhead comparable. Our study highlights the\nfeasibility and benefit of the proposed optimization techniques (targeted\nknowledge retrieval, prompt enrichment, and ontology refinement) for ontology\nmatching at scale.", "AI": {"tldr": "KROMA\u662f\u4e00\u4e2a\u65b0\u9896\u7684OM\u6846\u67b6\uff0c\u5229\u7528LLMs\u548cRAG\u52a8\u6001\u589e\u5f3a\u8bed\u4e49\u4e0a\u4e0b\u6587\uff0c\u901a\u8fc7\u53cc\u76f8\u4f3c\u5ea6\u5339\u914d\u548c\u8f7b\u91cf\u7ea7\u672c\u4f53\u4f18\u5316\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709OM\u7cfb\u7edf\u4f9d\u8d56\u624b\u5de5\u89c4\u5219\u6216\u4e13\u7528\u6a21\u578b\uff0c\u9002\u5e94\u6027\u6709\u9650\uff0cKROMA\u65e8\u5728\u901a\u8fc7LLMs\u548c\u77e5\u8bc6\u68c0\u7d22\u63d0\u5347\u8bed\u4e49\u4e92\u64cd\u4f5c\u6027\u3002", "method": "\u7ed3\u5408\u53cc\u76f8\u4f3c\u5ea6\u6982\u5ff5\u5339\u914d\u548c\u8f7b\u91cf\u7ea7\u672c\u4f53\u4f18\u5316\uff0c\u51cf\u5c11LLMs\u8c03\u7528\u5f00\u9500\uff0c\u52a8\u6001\u4e30\u5bcc\u8bed\u4e49\u4e0a\u4e0b\u6587\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u4f20\u7edfOM\u7cfb\u7edf\u548c\u524d\u6cbfLLM\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u4f4e\u901a\u4fe1\u5f00\u9500\u3002", "conclusion": "KROMA\u8bc1\u660e\u4e86\u77e5\u8bc6\u68c0\u7d22\u3001\u63d0\u793a\u589e\u5f3a\u548c\u672c\u4f53\u4f18\u5316\u5728\u5927\u89c4\u6a21OM\u4e2d\u7684\u53ef\u884c\u6027\u548c\u4f18\u52bf\u3002"}}
{"id": "2507.14061", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14061", "abs": "https://arxiv.org/abs/2507.14061", "authors": ["Nataliya Nechyporenko", "Yutong Zhang", "Sean Campbell", "Alessandro Roncone"], "title": "MorphIt: Flexible Spherical Approximation of Robot Morphology for Representation-driven Adaptation", "comment": null, "summary": "What if a robot could rethink its own morphological representation to better\nmeet the demands of diverse tasks? Most robotic systems today treat their\nphysical form as a fixed constraint rather than an adaptive resource, forcing\nthe same rigid geometric representation to serve applications with vastly\ndifferent computational and precision requirements. We introduce MorphIt, a\nnovel algorithm for approximating robot morphology using spherical primitives\nthat balances geometric accuracy with computational efficiency. Unlike existing\napproaches that rely on either labor-intensive manual specification or\ninflexible computational methods, MorphIt implements an automatic\ngradient-based optimization framework with tunable parameters that provides\nexplicit control over the physical fidelity versus computational cost tradeoff.\nQuantitative evaluations demonstrate that MorphIt outperforms baseline\napproaches (Variational Sphere Set Approximation and Adaptive Medial-Axis\nApproximation) across multiple metrics, achieving better mesh approximation\nwith fewer spheres and reduced computational overhead. Our experiments show\nenhanced robot capabilities in collision detection accuracy, contact-rich\ninteraction simulation, and navigation through confined spaces. By dynamically\nadapting geometric representations to task requirements, robots can now exploit\ntheir physical embodiment as an active resource rather than an inflexible\nparameter, opening new frontiers for manipulation in environments where\nphysical form must continuously balance precision with computational\ntractability.", "AI": {"tldr": "MorphIt\u662f\u4e00\u79cd\u65b0\u578b\u7b97\u6cd5\uff0c\u901a\u8fc7\u7403\u5f62\u57fa\u5143\u8fd1\u4f3c\u673a\u5668\u4eba\u5f62\u6001\uff0c\u5e73\u8861\u51e0\u4f55\u7cbe\u5ea6\u4e0e\u8ba1\u7b97\u6548\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u673a\u5668\u4eba\u7cfb\u7edf\u5c06\u7269\u7406\u5f62\u6001\u89c6\u4e3a\u56fa\u5b9a\u7ea6\u675f\uff0c\u65e0\u6cd5\u9002\u5e94\u591a\u6837\u5316\u4efb\u52a1\u9700\u6c42\uff0c\u9650\u5236\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u7cbe\u5ea6\u3002", "method": "MorphIt\u91c7\u7528\u57fa\u4e8e\u68af\u5ea6\u7684\u81ea\u52a8\u4f18\u5316\u6846\u67b6\uff0c\u53ef\u8c03\u53c2\u6570\u63a7\u5236\u7269\u7406\u4fdd\u771f\u5ea6\u4e0e\u8ba1\u7b97\u6210\u672c\u7684\u6743\u8861\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMorphIt\u5728\u7f51\u683c\u8fd1\u4f3c\u3001\u78b0\u649e\u68c0\u6d4b\u3001\u63a5\u89e6\u6a21\u62df\u548c\u72ed\u7a84\u7a7a\u95f4\u5bfc\u822a\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u52a8\u6001\u9002\u5e94\u51e0\u4f55\u8868\u793a\u4f7f\u673a\u5668\u4eba\u80fd\u5c06\u7269\u7406\u5f62\u6001\u4f5c\u4e3a\u4e3b\u52a8\u8d44\u6e90\uff0c\u63d0\u5347\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u64cd\u4f5c\u80fd\u529b\u3002"}}
{"id": "2507.14077", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14077", "abs": "https://arxiv.org/abs/2507.14077", "authors": ["Temiloluwa Prioleau", "Baiying Lu", "Yanjun Cui"], "title": "Glucose-ML: A collection of longitudinal diabetes datasets for development of robust AI solutions", "comment": "19 pages, 3 figures, 6 tables", "summary": "Artificial intelligence (AI) algorithms are a critical part of\nstate-of-the-art digital health technology for diabetes management. Yet, access\nto large high-quality datasets is creating barriers that impede development of\nrobust AI solutions. To accelerate development of transparent, reproducible,\nand robust AI solutions, we present Glucose-ML, a collection of 10 publicly\navailable diabetes datasets, released within the last 7 years (i.e., 2018 -\n2025). The Glucose-ML collection comprises over 300,000 days of continuous\nglucose monitor (CGM) data with a total of 38 million glucose samples collected\nfrom 2500+ people across 4 countries. Participants include persons living with\ntype 1 diabetes, type 2 diabetes, prediabetes, and no diabetes. To support\nresearchers and innovators with using this rich collection of diabetes\ndatasets, we present a comparative analysis to guide algorithm developers with\ndata selection. Additionally, we conduct a case study for the task of blood\nglucose prediction - one of the most common AI tasks within the field. Through\nthis case study, we provide a benchmark for short-term blood glucose prediction\nacross all 10 publicly available diabetes datasets within the Glucose-ML\ncollection. We show that the same algorithm can have significantly different\nprediction results when developed/evaluated with different datasets. Findings\nfrom this study are then used to inform recommendations for developing robust\nAI solutions within the diabetes or broader health domain. We provide direct\nlinks to each longitudinal diabetes dataset in the Glucose-ML collection and\nopenly provide our code.", "AI": {"tldr": "Glucose-ML\u662f\u4e00\u4e2a\u5305\u542b10\u4e2a\u516c\u5f00\u7cd6\u5c3f\u75c5\u6570\u636e\u96c6\u7684\u96c6\u5408\uff0c\u65e8\u5728\u52a0\u901f\u900f\u660e\u3001\u53ef\u91cd\u590d\u548c\u7a33\u5065\u7684AI\u89e3\u51b3\u65b9\u6848\u5f00\u53d1\u3002", "motivation": "\u89e3\u51b3\u9ad8\u8d28\u91cf\u7cd6\u5c3f\u75c5\u6570\u636e\u96c6\u83b7\u53d6\u56f0\u96be\u7684\u95ee\u9898\uff0c\u4fc3\u8fdbAI\u5728\u7cd6\u5c3f\u75c5\u7ba1\u7406\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u6536\u96c6\u5e76\u5206\u679010\u4e2a\u516c\u5f00\u6570\u636e\u96c6\uff0c\u63d0\u4f9b\u6bd4\u8f83\u5206\u6790\u548c\u8840\u7cd6\u9884\u6d4b\u6848\u4f8b\u7814\u7a76\u3002", "result": "\u540c\u4e00\u7b97\u6cd5\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u7684\u9884\u6d4b\u7ed3\u679c\u5dee\u5f02\u663e\u8457\uff0c\u7814\u7a76\u7ed3\u679c\u4e3a\u5f00\u53d1\u7a33\u5065AI\u89e3\u51b3\u65b9\u6848\u63d0\u4f9b\u4e86\u5efa\u8bae\u3002", "conclusion": "Glucose-ML\u6570\u636e\u96c6\u548c\u4ee3\u7801\u7684\u516c\u5f00\u5c06\u652f\u6301\u7cd6\u5c3f\u75c5\u9886\u57df\u7684AI\u7814\u7a76\u548c\u521b\u65b0\u3002"}}
{"id": "2507.14099", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.14099", "abs": "https://arxiv.org/abs/2507.14099", "authors": ["Markus Buchholz", "Ignacio Carlucho", "Michele Grimaldi", "Maria Koskinopoulou", "Yvan R. Petillot"], "title": "Context-Aware Behavior Learning with Heuristic Motion Memory for Underwater Manipulation", "comment": "Accepted at 2025 IEEE International Conference on Intelligent Robots\n  and Systems (IROS)", "summary": "Autonomous motion planning is critical for efficient and safe underwater\nmanipulation in dynamic marine environments. Current motion planning methods\noften fail to effectively utilize prior motion experiences and adapt to\nreal-time uncertainties inherent in underwater settings. In this paper, we\nintroduce an Adaptive Heuristic Motion Planner framework that integrates a\nHeuristic Motion Space (HMS) with Bayesian Networks to enhance motion planning\nfor autonomous underwater manipulation. Our approach employs the Probabilistic\nRoadmap (PRM) algorithm within HMS to optimize paths by minimizing a composite\ncost function that accounts for distance, uncertainty, energy consumption, and\nexecution time. By leveraging HMS, our framework significantly reduces the\nsearch space, thereby boosting computational performance and enabling real-time\nplanning capabilities. Bayesian Networks are utilized to dynamically update\nuncertainty estimates based on real-time sensor data and environmental\nconditions, thereby refining the joint probability of path success. Through\nextensive simulations and real-world test scenarios, we showcase the advantages\nof our method in terms of enhanced performance and robustness. This\nprobabilistic approach significantly advances the capability of autonomous\nunderwater robots, ensuring optimized motion planning in the face of dynamic\nmarine challenges.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u542f\u53d1\u5f0f\u8fd0\u52a8\u89c4\u5212\u6846\u67b6\uff0c\u7ed3\u5408\u542f\u53d1\u5f0f\u8fd0\u52a8\u7a7a\u95f4\u548c\u8d1d\u53f6\u65af\u7f51\u7edc\uff0c\u4f18\u5316\u6c34\u4e0b\u81ea\u4e3b\u64cd\u4f5c\u7684\u8fd0\u52a8\u89c4\u5212\u3002", "motivation": "\u73b0\u6709\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5229\u7528\u5148\u9a8c\u8fd0\u52a8\u7ecf\u9a8c\u5e76\u9002\u5e94\u6c34\u4e0b\u73af\u5883\u4e2d\u7684\u5b9e\u65f6\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u91c7\u7528\u542f\u53d1\u5f0f\u8fd0\u52a8\u7a7a\u95f4\uff08HMS\uff09\u548c\u8d1d\u53f6\u65af\u7f51\u7edc\uff0c\u7ed3\u5408\u6982\u7387\u8def\u7ebf\u56fe\uff08PRM\uff09\u7b97\u6cd5\uff0c\u6700\u5c0f\u5316\u590d\u5408\u6210\u672c\u51fd\u6570\u3002", "result": "\u6846\u67b6\u663e\u8457\u51cf\u5c11\u641c\u7d22\u7a7a\u95f4\uff0c\u63d0\u5347\u8ba1\u7b97\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u8d1d\u53f6\u65af\u7f51\u7edc\u52a8\u6001\u66f4\u65b0\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u4f18\u5316\u8def\u5f84\u6210\u529f\u7387\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u6c34\u4e0b\u81ea\u4e3b\u673a\u5668\u4eba\u7684\u8fd0\u52a8\u89c4\u5212\u80fd\u529b\uff0c\u9002\u5e94\u52a8\u6001\u6d77\u6d0b\u73af\u5883\u6311\u6218\u3002"}}
{"id": "2507.14097", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.14097", "abs": "https://arxiv.org/abs/2507.14097", "authors": ["Hari Iyer", "Neel Macwan", "Atharva Jitendra Hude", "Heejin Jeong", "Shenghan Guo"], "title": "Generative AI-Driven High-Fidelity Human Motion Simulation", "comment": null, "summary": "Human motion simulation (HMS) supports cost-effective evaluation of worker\nbehavior, safety, and productivity in industrial tasks. However, existing\nmethods often suffer from low motion fidelity. This study introduces\nGenerative-AI-Enabled HMS (G-AI-HMS), which integrates text-to-text and\ntext-to-motion models to enhance simulation quality for physical tasks.\nG-AI-HMS tackles two key challenges: (1) translating task descriptions into\nmotion-aware language using Large Language Models aligned with MotionGPT's\ntraining vocabulary, and (2) validating AI-enhanced motions against real human\nmovements using computer vision. Posture estimation algorithms are applied to\nreal-time videos to extract joint landmarks, and motion similarity metrics are\nused to compare them with AI-enhanced sequences. In a case study involving\neight tasks, the AI-enhanced motions showed lower error than human created\ndescriptions in most scenarios, performing better in six tasks based on spatial\naccuracy, four tasks based on alignment after pose normalization, and seven\ntasks based on overall temporal similarity. Statistical analysis showed that\nAI-enhanced prompts significantly (p $<$ 0.0001) reduced joint error and\ntemporal misalignment while retaining comparable posture accuracy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u751f\u6210\u5f0fAI\u7684\u4eba\u4f53\u8fd0\u52a8\u6a21\u62df\u65b9\u6cd5\uff08G-AI-HMS\uff09\uff0c\u901a\u8fc7\u7ed3\u5408\u6587\u672c\u5230\u6587\u672c\u548c\u6587\u672c\u5230\u8fd0\u52a8\u6a21\u578b\uff0c\u63d0\u9ad8\u4e86\u5de5\u4e1a\u4efb\u52a1\u4e2d\u8fd0\u52a8\u6a21\u62df\u7684\u903c\u771f\u5ea6\u3002", "motivation": "\u73b0\u6709\u7684\u4eba\u4f53\u8fd0\u52a8\u6a21\u62df\u65b9\u6cd5\u5728\u8fd0\u52a8\u903c\u771f\u5ea6\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u5f71\u54cd\u4e86\u5bf9\u5de5\u4eba\u884c\u4e3a\u3001\u5b89\u5168\u548c\u751f\u4ea7\u6548\u7387\u7684\u8bc4\u4f30\u3002", "method": "G-AI-HMS\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5c06\u4efb\u52a1\u63cf\u8ff0\u8f6c\u5316\u4e3a\u8fd0\u52a8\u611f\u77e5\u8bed\u8a00\uff0c\u5e76\u901a\u8fc7\u8ba1\u7b97\u673a\u89c6\u89c9\u9a8c\u8bc1AI\u751f\u6210\u7684\u8fd0\u52a8\u4e0e\u771f\u5b9e\u4eba\u7c7b\u52a8\u4f5c\u7684\u76f8\u4f3c\u6027\u3002", "result": "\u5728\u516b\u9879\u4efb\u52a1\u7684\u6848\u4f8b\u7814\u7a76\u4e2d\uff0cAI\u751f\u6210\u7684\u8fd0\u52a8\u5728\u591a\u6570\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u66f4\u4f4e\u7684\u8bef\u5dee\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u5173\u8282\u8bef\u5dee\u548c\u65f6\u95f4\u9519\u4f4d\u3002", "conclusion": "G-AI-HMS\u663e\u8457\u63d0\u5347\u4e86\u8fd0\u52a8\u6a21\u62df\u7684\u8d28\u91cf\uff0c\u4e3a\u5de5\u4e1a\u4efb\u52a1\u4e2d\u7684\u884c\u4e3a\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u5de5\u5177\u3002"}}
{"id": "2507.14107", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.14107", "abs": "https://arxiv.org/abs/2507.14107", "authors": ["Viraj Nishesh Darji", "Callie C. Liao", "Duoduo Liao"], "title": "Automated Interpretation of Non-Destructive Evaluation Contour Maps Using Large Language Models for Bridge Condition Assessment", "comment": null, "summary": "Bridge maintenance and safety are essential for transportation authorities,\nand Non-Destructive Evaluation (NDE) techniques are critical to assessing\nstructural integrity. However, interpreting NDE data can be time-consuming and\nrequires expertise, potentially delaying decision-making. Recent advancements\nin Large Language Models (LLMs) offer new ways to automate and improve this\nanalysis. This pilot study introduces a holistic assessment of LLM capabilities\nfor interpreting NDE contour maps and demonstrates the effectiveness of LLMs in\nproviding detailed bridge condition analyses. It establishes a framework for\nintegrating LLMs into bridge inspection workflows, indicating that LLM-assisted\nanalysis can enhance efficiency without compromising accuracy. In this study,\nseveral LLMs are explored with prompts specifically designed to enhance the\nquality of image descriptions, which are applied to interpret five different\nNDE contour maps obtained through technologies for assessing bridge conditions.\nEach LLM model is evaluated based on its ability to produce detailed\ndescriptions, identify defects, provide actionable recommendations, and\ndemonstrate overall accuracy. The research indicates that four of the nine\nmodels provide better image descriptions, effectively covering a wide range of\ntopics related to the bridge's condition. The outputs from these four models\nare summarized using five different LLMs to form a comprehensive overview of\nthe bridge. Notably, LLMs ChatGPT-4 and Claude 3.5 Sonnet generate more\neffective summaries. The findings suggest that LLMs have the potential to\nsignificantly improve efficiency and accuracy. This pilot study presents an\ninnovative approach that leverages LLMs for image captioning in parallel and\nsummarization, enabling faster decision-making in bridge maintenance and\nenhancing infrastructure management and safety assessments.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u81ea\u52a8\u89e3\u91ca\u65e0\u635f\u8bc4\u4f30\uff08NDE\uff09\u8f6e\u5ed3\u56fe\u7684\u80fd\u529b\uff0c\u4ee5\u63d0\u9ad8\u6865\u6881\u7ef4\u62a4\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u6865\u6881\u7ef4\u62a4\u548c\u5b89\u5168\u81f3\u5173\u91cd\u8981\uff0c\u4f46NDE\u6570\u636e\u89e3\u91ca\u8017\u65f6\u4e14\u9700\u4e13\u4e1a\u77e5\u8bc6\uff0cLLMs\u4e3a\u81ea\u52a8\u5316\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002", "method": "\u7814\u7a76\u8bbe\u8ba1\u4e86\u7279\u5b9a\u63d0\u793a\u8bcd\uff0c\u6d4b\u8bd5\u591a\u79cdLLMs\u5bf9\u4e94\u79cdNDE\u8f6e\u5ed3\u56fe\u7684\u89e3\u91ca\u80fd\u529b\uff0c\u8bc4\u4f30\u5176\u63cf\u8ff0\u7ec6\u8282\u3001\u7f3a\u9677\u8bc6\u522b\u548c\u63a8\u8350\u80fd\u529b\u3002", "result": "\u4e5d\u79cd\u6a21\u578b\u4e2d\u56db\u79cd\u8868\u73b0\u66f4\u4f18\uff0c\u5c24\u5176\u662fChatGPT-4\u548cClaude 3.5 Sonnet\uff0c\u80fd\u751f\u6210\u66f4\u6709\u6548\u7684\u6865\u6881\u72b6\u51b5\u603b\u7ed3\u3002", "conclusion": "LLMs\u53ef\u663e\u8457\u63d0\u5347\u6865\u6881\u7ef4\u62a4\u6548\u7387\u4e0e\u51c6\u786e\u6027\uff0c\u4e3a\u57fa\u7840\u8bbe\u65bd\u7ba1\u7406\u63d0\u4f9b\u521b\u65b0\u652f\u6301\u3002"}}
{"id": "2507.14111", "categories": ["cs.AI", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.14111", "abs": "https://arxiv.org/abs/2507.14111", "authors": ["Xiaoya Li", "Xiaofei Sun", "Albert Wang", "Jiwei Li", "Chris Shum"], "title": "CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement Learning", "comment": "Preprint Version", "summary": "The exponential growth in demand for GPU computing resources, driven by the\nrapid advancement of Large Language Models, has created an urgent need for\nautomated CUDA optimization strategies. While recent advances in LLMs show\npromise for code generation, current SOTA models (e.g. R1, o1) achieve low\nsuccess rates in improving CUDA speed. In this paper, we introduce CUDA-L1, an\nautomated reinforcement learning framework for CUDA optimization.\n  CUDA-L1 achieves performance improvements on the CUDA optimization task:\ntrained on NVIDIA A100, it delivers an average speedup of x17.7 across all 250\nCUDA kernels of KernelBench, with peak speedups reaching x449. Furthermore, the\nmodel also demonstrates excellent portability across GPU architectures,\nachieving average speedups of x17.8 on H100, x19.0 on RTX 3090, x16.5 on L40,\nx14.7 on H800, and x13.9 on H20 despite being optimized specifically for A100.\nBeyond these benchmark results, CUDA-L1 demonstrates several remarkable\nproperties: 1) Discovers a variety of CUDA optimization techniques and learns\nto combine them strategically to achieve optimal performance; 2) Uncovers\nfundamental principles of CUDA optimization; 3) Identifies non-obvious\nperformance bottlenecks and rejects seemingly beneficial optimizations that\nharm performance.\n  The capabilities of CUDA-L1 demonstrate that reinforcement learning can\ntransform an initially poor-performing LLM into an effective CUDA optimizer\nthrough speedup-based reward signals alone, without human expertise or domain\nknowledge. More importantly, the trained RL model extend the acquired reasoning\nabilities to new kernels. This paradigm opens possibilities for automated\noptimization of CUDA operations, and holds promise to substantially promote GPU\nefficiency and alleviate the rising pressure on GPU computing resources.", "AI": {"tldr": "CUDA-L1\u662f\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u81ea\u52a8\u5316CUDA\u4f18\u5316\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347CUDA\u5185\u6838\u6027\u80fd\uff0c\u5e76\u5c55\u793a\u51fa\u8272\u7684\u8de8GPU\u67b6\u6784\u79fb\u690d\u6027\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\uff0cGPU\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u6fc0\u589e\uff0c\u4e9f\u9700\u81ea\u52a8\u5316CUDA\u4f18\u5316\u7b56\u7565\u3002\u73b0\u6709\u6a21\u578b\u5728CUDA\u4f18\u5316\u4e0a\u6210\u529f\u7387\u4f4e\uff0c\u56e0\u6b64\u63d0\u51faCUDA-L1\u3002", "method": "\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u901f\u5ea6\u63d0\u5347\u5956\u52b1\u4fe1\u53f7\u8bad\u7ec3\u6a21\u578b\uff0c\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u6216\u9886\u57df\u77e5\u8bc6\u3002", "result": "\u5728NVIDIA A100\u4e0a\u5e73\u5747\u52a0\u901f17.7\u500d\uff0c\u5cf0\u503c\u8fbe449\u500d\uff1b\u5728\u5176\u4ed6GPU\u67b6\u6784\u4e0a\u4e5f\u8868\u73b0\u4f18\u5f02\u3002\u6a21\u578b\u8fd8\u80fd\u53d1\u73b0\u4f18\u5316\u6280\u5de7\u3001\u8bc6\u522b\u6027\u80fd\u74f6\u9888\u3002", "conclusion": "CUDA-L1\u8bc1\u660e\u5f3a\u5316\u5b66\u4e60\u53ef\u6709\u6548\u4f18\u5316CUDA\u6027\u80fd\uff0c\u4e3a\u81ea\u52a8\u5316GPU\u4f18\u5316\u5f00\u8f9f\u65b0\u9014\u5f84\uff0c\u6709\u671b\u7f13\u89e3GPU\u8d44\u6e90\u538b\u529b\u3002"}}
