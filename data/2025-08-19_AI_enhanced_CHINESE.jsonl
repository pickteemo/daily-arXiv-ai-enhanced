{"id": "2508.11759", "categories": ["cs.RO", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11759", "abs": "https://arxiv.org/abs/2508.11759", "authors": ["Peter Lindes", "Kaoutar Skiker"], "title": "Using Natural Language for Human-Robot Collaboration in the Real World", "comment": "34 pages, 11 figures, 5 tables. Submitted for publication (2026) in\n  W.F. Lawless, Ranjeev Mittu, Shannon P. McGrarry, & Marco Brambilla (Eds.),\n  Generative AI Risks and Benefits within Human-Machine Teams, Elsevier,\n  Chapter 6", "summary": "We have a vision of a day when autonomous robots can collaborate with humans\nas assistants in performing complex tasks in the physical world. This vision\nincludes that the robots will have the ability to communicate with their human\ncollaborators using language that is natural to the humans. Traditional\nInteractive Task Learning (ITL) systems have some of this ability, but the\nlanguage they can understand is very limited. The advent of large language\nmodels (LLMs) provides an opportunity to greatly improve the language\nunderstanding of robots, yet integrating the language abilities of LLMs with\nrobots that operate in the real physical world is a challenging problem.\n  In this chapter we first review briefly a few commercial robot products that\nwork closely with humans, and discuss how they could be much better\ncollaborators with robust language abilities. We then explore how an AI system\nwith a cognitive agent that controls a physical robot at its core, interacts\nwith both a human and an LLM, and accumulates situational knowledge through its\nexperiences, can be a possible approach to reach that vision. We focus on three\nspecific challenges of having the robot understand natural language, and\npresent a simple proof-of-concept experiment using ChatGPT for each. Finally,\nwe discuss what it will take to turn these simple experiments into an\noperational system where LLM-assisted language understanding is a part of an\nintegrated robotic assistant that uses language to collaborate with humans.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u5982\u4f55\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u6574\u5408\u5230\u7269\u7406\u673a\u5668\u4eba\u4e2d\uff0c\u4ee5\u589e\u5f3a\u5176\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u80fd\u529b\uff0c\u5b9e\u73b0\u4eba\u673a\u534f\u4f5c\u7684\u613f\u666f\u3002", "motivation": "\u4f20\u7edf\u4ea4\u4e92\u5f0f\u4efb\u52a1\u5b66\u4e60\u7cfb\u7edf\u7684\u8bed\u8a00\u7406\u89e3\u80fd\u529b\u6709\u9650\uff0c\u800c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u51fa\u73b0\u4e3a\u63d0\u5347\u673a\u5668\u4eba\u7684\u8bed\u8a00\u80fd\u529b\u63d0\u4f9b\u4e86\u673a\u4f1a\uff0c\u4f46\u5c06LLMs\u4e0e\u7269\u7406\u4e16\u754c\u64cd\u4f5c\u7684\u673a\u5668\u4eba\u6574\u5408\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u4ee5\u8ba4\u77e5\u4ee3\u7406\u4e3a\u6838\u5fc3\u7684AI\u7cfb\u7edf\uff0c\u63a7\u5236\u7269\u7406\u673a\u5668\u4eba\uff0c\u4e0e\u4eba\u7c7b\u548cLLM\u4ea4\u4e92\uff0c\u5e76\u901a\u8fc7\u7ecf\u9a8c\u79ef\u7d2f\u60c5\u5883\u77e5\u8bc6\u3002\u8fdb\u884c\u4e86\u4e09\u4e2a\u4f7f\u7528ChatGPT\u7684\u6982\u5ff5\u9a8c\u8bc1\u5b9e\u9a8c\u3002", "result": "\u901a\u8fc7\u7b80\u5355\u7684\u6982\u5ff5\u9a8c\u8bc1\u5b9e\u9a8c\u5c55\u793a\u4e86LLM\u5728\u673a\u5668\u4eba\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u4f46\u9700\u8981\u8fdb\u4e00\u6b65\u5f00\u53d1\u624d\u80fd\u6210\u4e3a\u64cd\u4f5c\u7cfb\u7edf\u7684\u7ec4\u6210\u90e8\u5206\u3002", "conclusion": "\u9700\u8981\u5c06\u7b80\u5355\u7684\u5b9e\u9a8c\u8f6c\u5316\u4e3a\u96c6\u6210\u7684\u673a\u5668\u4eba\u52a9\u624b\u7cfb\u7edf\uff0c\u5176\u4e2dLLM\u8f85\u52a9\u7684\u8bed\u8a00\u7406\u89e3\u662f\u4e0e\u4eba\u534f\u4f5c\u7684\u91cd\u8981\u7ec4\u6210\u90e8\u5206\uff0c\u8fd9\u9700\u8981\u66f4\u591a\u7684\u6280\u672f\u5f00\u53d1\u548c\u7cfb\u7edf\u96c6\u6210\u5de5\u4f5c\u3002"}}
{"id": "2508.11802", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11802", "abs": "https://arxiv.org/abs/2508.11802", "authors": ["Luigi Penco", "Beomyeong Park", "Stefan Fasano", "Nehar Poddar", "Stephen McCrory", "Nicholas Kitchel", "Tomasz Bialek", "Dexton Anderson", "Duncan Calvert", "Robert Griffin"], "title": "Anticipatory and Adaptive Footstep Streaming for Teleoperated Bipedal Robots", "comment": "2025 IEEE-RAS 24th International Conference on Humanoid Robots\n  (Humanoids)", "summary": "Achieving seamless synchronization between user and robot motion in\nteleoperation, particularly during high-speed tasks, remains a significant\nchallenge. In this work, we propose a novel approach for transferring stepping\nmotions from the user to the robot in real-time. Instead of directly\nreplicating user foot poses, we retarget user steps to robot footstep\nlocations, allowing the robot to utilize its own dynamics for locomotion,\nensuring better balance and stability. Our method anticipates user footsteps to\nminimize delays between when the user initiates and completes a step and when\nthe robot does it. The step estimates are continuously adapted to converge with\nthe measured user references. Additionally, the system autonomously adjusts the\nrobot's steps to account for its surrounding terrain, overcoming challenges\nposed by environmental mismatches between the user's flat-ground setup and the\nrobot's uneven terrain. Experimental results on the humanoid robot Nadia\ndemonstrate the effectiveness of the proposed system.", "AI": {"tldr": "\u901a\u8fc7\u9884\u6d4b\u7528\u6237\u811a\u6b65\u5e76\u91cd\u5b9a\u5411\u5230\u673a\u5668\u4eba\u6b65\u4f4d\u7f6e\uff0c\u5b9e\u73b0\u4e86\u9ad8\u901f\u6b65\u884c\u4efb\u52a1\u4e2d\u7684\u5b9e\u65f6\u540c\u6b65\uff0c\u5145\u5206\u5229\u7528\u673a\u5668\u4eba\u81ea\u8eab\u52a8\u529b\u5b66\u4fdd\u6301\u5e73\u8861\u7a33\u5b9a\u6027", "motivation": "\u89e3\u51b3\u9ad8\u901f\u4efb\u52a1\u4e2d\u7528\u6237\u4e0e\u673a\u5668\u4eba\u8fd0\u52a8\u540c\u6b65\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u5e73\u5766\u5730\u9762\u8bbe\u7f6e\u4e0e\u4e0d\u5e73\u5730\u5f62\u73af\u5883\u5b58\u5728\u5dee\u5f02\u65f6\u7684\u540c\u6b65\u95ee\u9898", "method": "\u9884\u6d4b\u7528\u6237\u811a\u6b65\u52a8\u4f5c\u5e76\u91cd\u5b9a\u5411\u5230\u673a\u5668\u4eba\u6b65\u4f4d\u7f6e\uff0c\u8ba9\u673a\u5668\u4eba\u4f7f\u7528\u81ea\u8eab\u52a8\u529b\u5b66\u8fdb\u884c\u79fb\u52a8\uff1b\u7cfb\u7edf\u81ea\u4e3b\u8c03\u6574\u6b65\u4f10\u4ee5\u9002\u5e94\u5468\u56f4\u5730\u5f62", "result": "\u5728Nadia\u4eba\u578b\u673a\u5668\u4eba\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u8be5\u7cfb\u7edf\u7684\u6709\u6548\u6027", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u51cf\u5c11\u5ef6\u8fdf\uff0c\u786e\u4fdd\u673a\u5668\u4eba\u5e73\u8861\u7a33\u5b9a\uff0c\u5e76\u5145\u5206\u9002\u5e94\u4e0d\u5e73\u5730\u5f62\u73af\u5883"}}
{"id": "2508.11849", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11849", "abs": "https://arxiv.org/abs/2508.11849", "authors": ["Allen Wang", "Gavin Tao"], "title": "LocoMamba: Vision-Driven Locomotion via End-to-End Deep Reinforcement Learning with Mamba", "comment": null, "summary": "We introduce LocoMamba, a vision-driven cross-modal DRL framework built on\nselective state-space models, specifically leveraging Mamba, that achieves\nnear-linear-time sequence modeling, effectively captures long-range\ndependencies, and enables efficient training with longer sequences. First, we\nembed proprioceptive states with a multilayer perceptron and patchify depth\nimages with a lightweight convolutional neural network, producing compact\ntokens that improve state representation. Second, stacked Mamba layers fuse\nthese tokens via near-linear-time selective scanning, reducing latency and\nmemory footprint, remaining robust to token length and image resolution, and\nproviding an inductive bias that mitigates overfitting. Third, we train the\npolicy end-to-end with Proximal Policy Optimization under terrain and\nappearance randomization and an obstacle-density curriculum, using a compact\nstate-centric reward that balances progress, smoothness, and safety. We\nevaluate our method in challenging simulated environments with static and\nmoving obstacles as well as uneven terrain. Compared with state-of-the-art\nbaselines, our method achieves higher returns and success rates with fewer\ncollisions, exhibits stronger generalization to unseen terrains and obstacle\ndensities, and improves training efficiency by converging in fewer updates\nunder the same compute budget.", "AI": {"tldr": "LocoMamba\u662f\u4e00\u4e2a\u57fa\u4e8eMamba\u9009\u62e9\u6027\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u7684\u89c6\u89c9\u9a71\u52a8\u8de8\u6a21\u6001DRL\u6846\u67b6\uff0c\u901a\u8fc7\u8fd1\u7ebf\u6027\u65f6\u95f4\u5e8f\u5217\u5efa\u6a21\u5b9e\u73b0\u9ad8\u6548\u8bad\u7ec3\uff0c\u5728\u590d\u6742\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5728\u957f\u5e8f\u5217\u5efa\u6a21\u4e2d\u7684\u8ba1\u7b97\u6548\u7387\u95ee\u9898\uff0c\u540c\u65f6\u63d0\u5347\u673a\u5668\u4eba\u5bfc\u822a\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b", "method": "1) \u4f7f\u7528MLP\u5d4c\u5165\u672c\u4f53\u611f\u77e5\u72b6\u6001\uff0cCNN\u5904\u7406\u6df1\u5ea6\u56fe\u50cf\u751f\u6210\u7d27\u51d1token\uff1b2) \u5806\u53e0Mamba\u5c42\u901a\u8fc7\u9009\u62e9\u6027\u626b\u63cf\u878d\u5408token\uff1b3) \u4f7f\u7528PPO\u5728\u968f\u673a\u5316\u5730\u5f62\u548c\u5916\u89c2\u4e0b\u8fdb\u884c\u7aef\u5230\u7aef\u8bad\u7ec3\uff0c\u91c7\u7528\u969c\u788d\u7269\u5bc6\u5ea6\u8bfe\u7a0b\u5b66\u4e60\u548c\u7d27\u51d1\u72b6\u6001\u4e2d\u5fc3\u5956\u52b1", "result": "\u5728\u5177\u6709\u9759\u6001/\u52a8\u6001\u969c\u788d\u7269\u548c\u4e0d\u5e73\u5730\u5f62\u7684\u6a21\u62df\u73af\u5883\u4e2d\uff0c\u76f8\u6bd4SOTA\u57fa\u7ebf\u83b7\u5f97\u66f4\u9ad8\u56de\u62a5\u548c\u6210\u529f\u7387\uff0c\u78b0\u649e\u66f4\u5c11\uff0c\u5bf9\u672a\u89c1\u5730\u5f62\u548c\u969c\u788d\u7269\u5bc6\u5ea6\u6709\u66f4\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u8bad\u7ec3\u6548\u7387\u66f4\u9ad8", "conclusion": "LocoMamba\u6846\u67b6\u901a\u8fc7\u9009\u62e9\u6027\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u6709\u6548\u89e3\u51b3\u4e86\u957f\u5e8f\u5217\u4f9d\u8d56\u95ee\u9898\uff0c\u5728\u673a\u5668\u4eba\u5bfc\u822a\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\u3001\u6cdb\u5316\u80fd\u529b\u548c\u8bad\u7ec3\u6548\u7387"}}
{"id": "2508.11868", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.11868", "abs": "https://arxiv.org/abs/2508.11868", "authors": ["Lida Xu"], "title": "Data Shift of Object Detection in Autonomous Driving", "comment": null, "summary": "With the widespread adoption of machine learning technologies in autonomous\ndriving systems, their role in addressing complex environmental perception\nchallenges has become increasingly crucial. However, existing machine learning\nmodels exhibit significant vulnerability, as their performance critically\ndepends on the fundamental assumption that training and testing data satisfy\nthe independent and identically distributed condition, which is difficult to\nguarantee in real-world applications. Dynamic variations in data distribution\ncaused by seasonal changes, weather fluctuations lead to data shift problems in\nautonomous driving systems. This study investigates the data shift problem in\nautonomous driving object detection tasks, systematically analyzing its\ncomplexity and diverse manifestations. We conduct a comprehensive review of\ndata shift detection methods and employ shift detection analysis techniques to\nperform dataset categorization and balancing. Building upon this foundation, we\nconstruct an object detection model. To validate our approach, we optimize the\nmodel by integrating CycleGAN-based data augmentation techniques with the\nYOLOv5 framework. Experimental results demonstrate that our method achieves\nsuperior performance compared to baseline models on the BDD100K dataset.", "AI": {"tldr": "\u672c\u7814\u7a76\u9488\u5bf9\u81ea\u52a8\u9a7e\u9a76\u76ee\u6807\u68c0\u6d4b\u4e2d\u7684\u6570\u636e\u504f\u79fb\u95ee\u9898\uff0c\u63d0\u51fa\u7ed3\u5408CycleGAN\u6570\u636e\u589e\u5f3a\u548cYOLOv5\u6846\u67b6\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u5728BDD100K\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e2d\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4e25\u91cd\u4f9d\u8d56\u8bad\u7ec3\u548c\u6d4b\u8bd5\u6570\u636e\u6ee1\u8db3\u72ec\u7acb\u540c\u5206\u5e03\u5047\u8bbe\uff0c\u4f46\u73b0\u5b9e\u4e2d\u5b63\u8282\u53d8\u5316\u3001\u5929\u6c14\u6ce2\u52a8\u7b49\u56e0\u7d20\u5bfc\u81f4\u6570\u636e\u5206\u5e03\u52a8\u6001\u53d8\u5316\uff0c\u9020\u6210\u6570\u636e\u504f\u79fb\u95ee\u9898\uff0c\u5f71\u54cd\u6a21\u578b\u6027\u80fd\u3002", "method": "\u7cfb\u7edf\u5206\u6790\u6570\u636e\u504f\u79fb\u95ee\u9898\u7684\u590d\u6742\u6027\u548c\u8868\u73b0\u5f62\u5f0f\uff0c\u7efc\u8ff0\u6570\u636e\u504f\u79fb\u68c0\u6d4b\u65b9\u6cd5\uff0c\u4f7f\u7528\u504f\u79fb\u68c0\u6d4b\u5206\u6790\u6280\u672f\u8fdb\u884c\u6570\u636e\u96c6\u5206\u7c7b\u548c\u5e73\u8861\uff0c\u6784\u5efa\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\uff0c\u5e76\u96c6\u6210CycleGAN\u6570\u636e\u589e\u5f3a\u6280\u672f\u4e0eYOLOv5\u6846\u67b6\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u5728BDD100K\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u57fa\u7ebf\u6a21\u578b\u53d6\u5f97\u4e86\u66f4\u4f18\u8d8a\u7684\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u7cfb\u7edf\u5206\u6790\u6570\u636e\u504f\u79fb\u95ee\u9898\u5e76\u91c7\u7528CycleGAN\u6570\u636e\u589e\u5f3a\u4e0eYOLOv5\u6846\u67b6\u7ed3\u5408\u7684\u4f18\u5316\u7b56\u7565\uff0c\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u81ea\u52a8\u9a7e\u9a76\u76ee\u6807\u68c0\u6d4b\u4e2d\u7684\u6570\u636e\u5206\u5e03\u53d8\u5316\u6311\u6218\uff0c\u63d0\u5347\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.11836", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11836", "abs": "https://arxiv.org/abs/2508.11836", "authors": ["Dave Goel", "Matthew Guzdial", "Anurag Sarkar"], "title": "Finite Automata Extraction: Low-data World Model Learning as Programs from Gameplay Video", "comment": null, "summary": "World models are defined as a compressed spatial and temporal learned\nrepresentation of an environment. The learned representation is typically a\nneural network, making transfer of the learned environment dynamics and\nexplainability a challenge. In this paper, we propose an approach, Finite\nAutomata Extraction (FAE), that learns a neuro-symbolic world model from\ngameplay video represented as programs in a novel domain-specific language\n(DSL): Retro Coder. Compared to prior world model approaches, FAE learns a more\nprecise model of the environment and more general code than prior DSL-based\napproaches.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faFAE\u65b9\u6cd5\uff0c\u4ece\u6e38\u620f\u89c6\u9891\u4e2d\u5b66\u4e60\u795e\u7ecf\u7b26\u53f7\u4e16\u754c\u6a21\u578b\uff0c\u4f7f\u7528\u65b0\u7684\u9886\u57df\u7279\u5b9a\u8bed\u8a00Retro Coder\u8868\u793a\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u80fd\u5b66\u4e60\u66f4\u7cbe\u786e\u7684\u73af\u5883\u6a21\u578b\u548c\u66f4\u901a\u7528\u7684\u4ee3\u7801", "motivation": "\u73b0\u6709\u4e16\u754c\u6a21\u578b\u901a\u5e38\u662f\u795e\u7ecf\u7f51\u7edc\u8868\u793a\uff0c\u96be\u4ee5\u8fc1\u79fb\u5b66\u4e60\u5230\u7684\u73af\u5883\u52a8\u6001\u548c\u89e3\u91ca\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u4fdd\u6301\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u80fd\u529b\u53c8\u5177\u6709\u7b26\u53f7\u8868\u793a\u53ef\u89e3\u91ca\u6027\u7684\u65b9\u6cd5", "method": "\u63d0\u51fa\u6709\u9650\u81ea\u52a8\u673a\u63d0\u53d6(FAE)\u65b9\u6cd5\uff0c\u4ece\u6e38\u620f\u89c6\u9891\u4e2d\u5b66\u4e60\u795e\u7ecf\u7b26\u53f7\u4e16\u754c\u6a21\u578b\uff0c\u4f7f\u7528\u65b0\u578b\u9886\u57df\u7279\u5b9a\u8bed\u8a00Retro Coder\u5c06\u5b66\u4e60\u5230\u7684\u8868\u793a\u8868\u793a\u4e3a\u7a0b\u5e8f", "result": "\u76f8\u6bd4\u73b0\u6709\u4e16\u754c\u6a21\u578b\u65b9\u6cd5\uff0cFAE\u80fd\u5b66\u4e60\u5230\u66f4\u7cbe\u786e\u7684\u73af\u5883\u6a21\u578b\uff1b\u76f8\u6bd4\u73b0\u6709\u57fa\u4e8eDSL\u7684\u65b9\u6cd5\uff0c\u80fd\u751f\u6210\u66f4\u901a\u7528\u7684\u4ee3\u7801", "conclusion": "FAE\u65b9\u6cd5\u6210\u529f\u5730\u5c06\u795e\u7ecf\u7f51\u7edc\u7684\u8868\u793a\u5b66\u4e60\u80fd\u529b\u4e0e\u7b26\u53f7\u8868\u793a\u7684\u53ef\u89e3\u91ca\u6027\u76f8\u7ed3\u5408\uff0c\u4e3a\u4e16\u754c\u6a21\u578b\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u6709\u6548\u9014\u5f84"}}
{"id": "2508.11883", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11883", "abs": "https://arxiv.org/abs/2508.11883", "authors": ["Lei Li", "Boyang Qin", "Wenzhuo Gao", "Yanyu Li", "Yiyuan Zhang", "Bo Wang", "Shihan Kong", "Jian Wang", "Dekui He", "Junzhi Yu"], "title": "Bioinspired underwater soft robots: from biology to robotics and back", "comment": null, "summary": "The ocean vast unexplored regions and diverse soft-bodied marine organisms\nhave spurred interest in bio-inspired underwater soft robotics. Recent advances\nhave enabled new capabilities in underwater movement, sensing, and interaction.\nHowever, these efforts are largely unidirectional, with biology guiding\nrobotics while insights from robotics rarely feed back into biology. Here we\npropose a holistic, bidirectional framework that integrates biological\nprinciples, robotic implementation, and biological validation. We show that\nsoft robots can serve as experimental tools to probe biological functions and\neven test evolutionary hypotheses. Their inherent compliance also allows them\nto outperform rigid systems in unstructured environments, supporting\napplications in marine exploration, manipulation, and medicine. Looking\nforward, we introduce bio-universal-inspired robotics, a paradigm that\ntranscends species-specific mimicry by identifying convergent principles across\nspecies to inspire more adaptable designs. Despite rapid progress, challenges\npersist in material robustness, actuation efficiency, autonomy, and\nintelligence. By uniting biology and engineering, soft robots can advance ocean\nexploration and deepen scientific discovery.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u53cc\u5411\u7684\u751f\u7269\u542f\u53d1\u8f6f\u4f53\u673a\u5668\u4eba\u6846\u67b6\uff0c\u4e0d\u4ec5\u4ece\u751f\u7269\u5b66\u4e2d\u83b7\u53d6\u7075\u611f\uff0c\u8fd8\u80fd\u901a\u8fc7\u673a\u5668\u4eba\u5b9e\u9a8c\u53cd\u9988\u9a8c\u8bc1\u751f\u7269\u5b66\u5047\u8bbe\uff0c\u5e76\u5f15\u5165\u8de8\u7269\u79cd\u7684\u901a\u7528\u8bbe\u8ba1\u539f\u5219\u3002", "motivation": "\u6d77\u6d0b\u63a2\u7d22\u548c\u8f6f\u4f53\u6d77\u6d0b\u751f\u7269\u7684\u591a\u6837\u6027\u6fc0\u53d1\u4e86\u751f\u7269\u542f\u53d1\u6c34\u4e0b\u8f6f\u4f53\u673a\u5668\u4eba\u7684\u7814\u7a76\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u662f\u5355\u5411\u7684\u751f\u7269\u5b66\u6307\u5bfc\u673a\u5668\u4eba\uff0c\u7f3a\u4e4f\u673a\u5668\u4eba\u5bf9\u751f\u7269\u5b66\u7684\u53cd\u9988\u9a8c\u8bc1\u3002", "method": "\u63d0\u51fa\u6574\u4f53\u53cc\u5411\u6846\u67b6\uff0c\u6574\u5408\u751f\u7269\u5b66\u539f\u7406\u3001\u673a\u5668\u4eba\u5b9e\u73b0\u548c\u751f\u7269\u5b66\u9a8c\u8bc1\uff0c\u4f7f\u7528\u8f6f\u4f53\u673a\u5668\u4eba\u4f5c\u4e3a\u5b9e\u9a8c\u5de5\u5177\u6765\u63a2\u7a76\u751f\u7269\u529f\u80fd\u548c\u6d4b\u8bd5\u8fdb\u5316\u5047\u8bbe\u3002", "result": "\u8f6f\u4f53\u673a\u5668\u4eba\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u4f18\u4e8e\u521a\u6027\u7cfb\u7edf\uff0c\u652f\u6301\u6d77\u6d0b\u63a2\u7d22\u3001\u64cd\u4f5c\u548c\u533b\u7597\u5e94\u7528\uff1b\u63d0\u51fa\u4e86\u8d85\u8d8a\u7269\u79cd\u7279\u5f02\u6027\u6a21\u4eff\u7684\u751f\u7269\u901a\u7528\u542f\u53d1\u673a\u5668\u4eba\u8303\u5f0f\u3002", "conclusion": "\u901a\u8fc7\u878d\u5408\u751f\u7269\u5b66\u548c\u5de5\u7a0b\u5b66\uff0c\u8f6f\u4f53\u673a\u5668\u4eba\u53ef\u4ee5\u63a8\u52a8\u6d77\u6d0b\u63a2\u7d22\u5e76\u6df1\u5316\u79d1\u5b66\u53d1\u73b0\uff0c\u4f46\u5728\u6750\u6599\u8010\u7528\u6027\u3001\u9a71\u52a8\u6548\u7387\u3001\u81ea\u4e3b\u6027\u548c\u667a\u80fd\u65b9\u9762\u4ecd\u5b58\u5728\u6311\u6218\u3002"}}
{"id": "2508.11850", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11850", "abs": "https://arxiv.org/abs/2508.11850", "authors": ["Milad Yazdani", "Mahdi Mostajabdaveh", "Samin Aref", "Zirui Zhou"], "title": "EvoCut: Strengthening Integer Programs via Evolution-Guided Language Models", "comment": null, "summary": "Integer programming lies at the heart of crucial combinatorial optimization\ntasks but remains challenging due to its NP-hard nature. An effective approach\nfor practically solving integer programs is the manual design of acceleration\ncuts, i.e. inequalities that improve solver performance. However, this creative\nprocess demands deep expertise and is yet to be automated. Our proposed\nframework, EvoCut, automates the generation of acceleration cuts by combining\nlarge language models (LLMs) with an evolutionary search. EvoCut (i)\ninitializes a diverse population of candidate cuts via an LLM-based initializer\nagent; (ii) for each cut empirically evaluates both preservation of the optimal\nsolution and its ability to cut off fractional solutions across a verification\nset; and (iii) iteratively refines the population through evolutionary\ncrossover and mutation agents. We quantify each cut's utility by its relative\nreduction in the solver's optimality gap. Our comparisons against standard\ninteger programming practice show that EvoCut reduces optimality gap by 17-57%\nwithin a fixed time. It obtains the same solutions up to 4 times as fast, and\nobtains higher-quality solutions within the same time limit. Requiring no human\nexpert input, EvoCut reliably generates, improves, and empirically verifies\ncuts that generalize to unseen instances. The code is available at\nhttps://github.com/milad1378yz/EvoCut.", "AI": {"tldr": "EvoCut\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u751f\u6210\u6574\u6570\u89c4\u5212\u52a0\u901f\u5272\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u548c\u8fdb\u5316\u641c\u7d22\uff0c\u65e0\u9700\u4eba\u5de5\u4e13\u5bb6\u8f93\u5165\u5373\u53ef\u751f\u6210\u6709\u6548\u7684\u4e0d\u7b49\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u6c42\u89e3\u5668\u6027\u80fd", "motivation": "\u6574\u6570\u89c4\u5212\u4f5c\u4e3a\u7ec4\u5408\u4f18\u5316\u7684\u6838\u5fc3\u95ee\u9898\u5177\u6709NP\u96be\u7279\u6027\uff0c\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u4e13\u5bb6\u624b\u52a8\u8bbe\u8ba1\u52a0\u901f\u5272\uff0c\u8fd9\u4e00\u8fc7\u7a0b\u9700\u8981\u6df1\u539a\u4e13\u4e1a\u77e5\u8bc6\u4e14\u96be\u4ee5\u81ea\u52a8\u5316", "method": "\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u548c\u8fdb\u5316\u641c\u7d22\uff1a1\uff09LLM\u521d\u59cb\u5316\u591a\u6837\u5316\u5019\u9009\u5272\uff1b2\uff09\u8bc4\u4f30\u5272\u7684\u53ef\u884c\u6027\u548c\u6709\u6548\u6027\uff1b3\uff09\u901a\u8fc7\u8fdb\u5316\u4ea4\u53c9\u548c\u53d8\u5f02\u8fed\u4ee3\u4f18\u5316\u79cd\u7fa4", "result": "\u76f8\u6bd4\u6807\u51c6\u6574\u6570\u89c4\u5212\u65b9\u6cd5\uff0cEvoCut\u5728\u56fa\u5b9a\u65f6\u95f4\u5185\u5c06\u6700\u4f18\u6027\u95f4\u9699\u964d\u4f4e17-57%\uff0c\u83b7\u5f97\u76f8\u540c\u89e3\u7684\u901f\u5ea6\u63d0\u53474\u500d\uff0c\u5728\u76f8\u540c\u65f6\u95f4\u5185\u83b7\u5f97\u66f4\u9ad8\u8d28\u91cf\u89e3", "conclusion": "EvoCut\u80fd\u591f\u53ef\u9760\u5730\u751f\u6210\u3001\u6539\u8fdb\u548c\u9a8c\u8bc1\u53ef\u6cdb\u5316\u5230\u672a\u89c1\u5b9e\u4f8b\u7684\u5272\uff0c\u4e3a\u6574\u6570\u89c4\u5212\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.11884", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11884", "abs": "https://arxiv.org/abs/2508.11884", "authors": ["Havel Liu", "Mingzhang Zhu", "Arturo Moises Flores Alvarez", "Yuan Hung Lo", "Conrad Ku", "Federico Parres", "Justin Quan", "Colin Togashi", "Aditya Navghare", "Quanyou Wang", "Dennis W. Hong"], "title": "From Screen to Stage: Kid Cosmo, A Life-Like, Torque-Controlled Humanoid for Entertainment Robotics", "comment": "8 pages, 14 figures, accepted by IEEE Humanoids 2025", "summary": "Humanoid robots represent the cutting edge of robotics research, yet their\npotential in entertainment remains largely unexplored. Entertainment as a field\nprioritizes visuals and form, a principle that contrasts with the purely\nfunctional designs of most contemporary humanoid robots. Designing\nentertainment humanoid robots capable of fluid movement presents a number of\nunique challenges. In this paper, we present Kid Cosmo, a research platform\ndesigned for robust locomotion and life-like motion generation while imitating\nthe look and mannerisms of its namesake character from Netflix's movie The\nElectric State. Kid Cosmo is a child-sized humanoid robot, standing 1.45 m tall\nand weighing 25 kg. It contains 28 degrees of freedom and primarily uses\nproprioceptive actuators, enabling torque-control walking and lifelike motion\ngeneration. Following worldwide showcases as part of the movie's press tour, we\npresent the system architecture, challenges of a functional entertainment robot\nand unique solutions, and our initial findings on stability during simultaneous\nupper and lower body movement. We demonstrate the viability of\nperformance-oriented humanoid robots that prioritize both character embodiment\nand technical functionality.", "AI": {"tldr": "Kid Cosmo\u662f\u4e00\u4e2a\u9762\u5411\u5a31\u4e50\u7684\u513f\u7ae5\u5c3a\u5bf8\u4eba\u5f62\u673a\u5668\u4eba\u5e73\u53f0\uff0c\u5177\u670928\u4e2a\u81ea\u7531\u5ea6\uff0c\u80fd\u591f\u5b9e\u73b0\u626d\u77e9\u63a7\u5236\u884c\u8d70\u548c\u903c\u771f\u8fd0\u52a8\u751f\u6210\uff0c\u5728\u7535\u5f71\u5ba3\u4f20\u5de1\u6f14\u4e2d\u5c55\u793a\u4e86\u540c\u65f6\u8fdb\u884c\u4e0a\u4e0b\u534a\u8eab\u8fd0\u52a8\u7684\u7a33\u5b9a\u6027\u3002", "motivation": "\u5f53\u524d\u4eba\u5f62\u673a\u5668\u4eba\u4e3b\u8981\u5173\u6ce8\u529f\u80fd\u6027\u8bbe\u8ba1\uff0c\u800c\u5a31\u4e50\u9886\u57df\u66f4\u6ce8\u91cd\u89c6\u89c9\u6548\u679c\u548c\u5f62\u6001\u3002\u8bbe\u8ba1\u80fd\u591f\u6d41\u7545\u8fd0\u52a8\u7684\u5a31\u4e50\u4eba\u5f62\u673a\u5668\u4eba\u9762\u4e34\u72ec\u7279\u6311\u6218\uff0c\u9700\u8981\u5e73\u8861\u89d2\u8272\u4f53\u73b0\u548c\u6280\u672f\u529f\u80fd\u3002", "method": "\u5f00\u53d1\u4e86Kid Cosmo\u7814\u7a76\u5e73\u53f0\uff0c\u91c7\u7528\u672c\u4f53\u611f\u53d7\u6267\u884c\u5668\u5b9e\u73b0\u626d\u77e9\u63a7\u5236\u884c\u8d70\u548c\u903c\u771f\u8fd0\u52a8\u751f\u6210\uff0c\u7cfb\u7edf\u9ad81.45\u7c73\uff0c\u91cd25\u516c\u65a4\uff0c\u5177\u670928\u4e2a\u81ea\u7531\u5ea6\u3002", "result": "\u5728\u5168\u7403\u7535\u5f71\u5ba3\u4f20\u5de1\u6f14\u4e2d\u5c55\u793a\u4e86\u7cfb\u7edf\u7684\u7a33\u5b9a\u6027\uff0c\u7279\u522b\u662f\u5728\u540c\u65f6\u8fdb\u884c\u4e0a\u4e0b\u534a\u8eab\u8fd0\u52a8\u65f6\u7684\u8868\u73b0\uff0c\u8bc1\u660e\u4e86\u6027\u80fd\u5bfc\u5411\u4eba\u5f62\u673a\u5668\u4eba\u7684\u53ef\u884c\u6027\u3002", "conclusion": "Kid Cosmo\u6210\u529f\u5c55\u793a\u4e86\u65e2\u6ce8\u91cd\u89d2\u8272\u4f53\u73b0\u53c8\u5177\u5907\u6280\u672f\u529f\u80fd\u7684\u5a31\u4e50\u4eba\u5f62\u673a\u5668\u4eba\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u5a31\u4e50\u673a\u5668\u4eba\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.11860", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.11860", "abs": "https://arxiv.org/abs/2508.11860", "authors": ["Frazier N. Baker", "Daniel Adu-Ampratwum", "Reza Averly", "Botao Yu", "Huan Sun", "Xia Ning"], "title": "LARC: Towards Human-level Constrained Retrosynthesis Planning through an Agentic Framework", "comment": "24 pages, 5 figures", "summary": "Large language model (LLM) agent evaluators leverage specialized tools to\nground the rational decision-making of LLMs, making them well-suited to aid in\nscientific discoveries, such as constrained retrosynthesis planning.\nConstrained retrosynthesis planning is an essential, yet challenging, process\nwithin chemistry for identifying synthetic routes from commercially available\nstarting materials to desired target molecules, subject to practical\nconstraints. Here, we present LARC, the first LLM-based Agentic framework for\nRetrosynthesis planning under Constraints. LARC incorporates agentic constraint\nevaluation, through an Agent-as-a-Judge, directly into the retrosynthesis\nplanning process, using agentic feedback grounded in tool-based reasoning to\nguide and constrain route generation. We rigorously evaluate LARC on a\ncarefully curated set of 48 constrained retrosynthesis planning tasks across 3\nconstraint types. LARC achieves a 72.9% success rate on these tasks, vastly\noutperforming LLM baselines and approaching human expert-level success in\nsubstantially less time. The LARC framework is extensible, and serves as a\nfirst step towards an effective agentic tool or a co-scientist to human experts\nfor constrained retrosynthesis.", "AI": {"tldr": "LARC\u662f\u9996\u4e2a\u57fa\u4e8eLLM\u7684\u7ea6\u675f\u9006\u5408\u6210\u89c4\u5212\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5de5\u5177\u9a71\u52a8\u7684\u667a\u80fd\u4f53\u8bc4\u4f30\u673a\u5236\uff0c\u5728\u5316\u5b66\u5408\u6210\u8def\u7ebf\u89c4\u5212\u4e2d\u5b9e\u73b072.9%\u7684\u6210\u529f\u7387\uff0c\u63a5\u8fd1\u4e13\u5bb6\u6c34\u5e73\u4f46\u8017\u65f6\u66f4\u5c11", "motivation": "\u5316\u5b66\u9006\u5408\u6210\u89c4\u5212\u5728\u5546\u4e1a\u53ef\u7528\u8d77\u59cb\u539f\u6599\u5230\u76ee\u6807\u5206\u5b50\u7684\u5408\u6210\u8def\u7ebf\u8bc6\u522b\u4e2d\u9762\u4e34\u5b9e\u9645\u7ea6\u675f\u7684\u6311\u6218\uff0c\u9700\u8981\u667a\u80fd\u5316\u7684\u7ea6\u675f\u8bc4\u4f30\u548c\u89c4\u5212\u65b9\u6cd5", "method": "\u91c7\u7528LLM\u4ee3\u7406\u8bc4\u4f30\u5668\uff0c\u901a\u8fc7Agent-as-a-Judge\u673a\u5236\u5c06\u57fa\u4e8e\u5de5\u5177\u63a8\u7406\u7684\u667a\u80fd\u4f53\u53cd\u9988\u76f4\u63a5\u6574\u5408\u5230\u9006\u5408\u6210\u89c4\u5212\u8fc7\u7a0b\u4e2d\uff0c\u6307\u5bfc\u5e76\u7ea6\u675f\u8def\u7ebf\u751f\u6210", "result": "\u572848\u4e2a\u7ea6\u675f\u9006\u5408\u6210\u89c4\u5212\u4efb\u52a1\u4e0a\u8fbe\u523072.9%\u7684\u6210\u529f\u7387\uff0c\u663e\u8457\u4f18\u4e8eLLM\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u66f4\u77ed\u65f6\u95f4\u5185\u63a5\u8fd1\u4eba\u7c7b\u4e13\u5bb6\u6c34\u5e73", "conclusion": "LARC\u6846\u67b6\u5177\u6709\u53ef\u6269\u5c55\u6027\uff0c\u662f\u5411\u6709\u6548\u667a\u80fd\u4f53\u5de5\u5177\u6216\u4eba\u7c7b\u4e13\u5bb6\u534f\u4f5c\u8005\u8fc8\u51fa\u7684\u91cd\u8981\u4e00\u6b65\uff0c\u4e3a\u7ea6\u675f\u9006\u5408\u6210\u89c4\u5212\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.11885", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11885", "abs": "https://arxiv.org/abs/2508.11885", "authors": ["Haixin Gong", "Chen Zhang", "Yanan Sui"], "title": "Contact-Rich and Deformable Foot Modeling for Locomotion Control of the Human Musculoskeletal System", "comment": "IEEE-RAS 24th International Conference on Humanoid Robots (Humanoids\n  2025)", "summary": "The human foot serves as the critical interface between the body and\nenvironment during locomotion. Existing musculoskeletal models typically\noversimplify foot-ground contact mechanics, limiting their ability to\naccurately simulate human gait dynamics. We developed a novel contact-rich and\ndeformable model of the human foot integrated within a complete musculoskeletal\nsystem that captures the complex biomechanical interactions during walking. To\novercome the control challenges inherent in modeling multi-point contacts and\ndeformable material, we developed a two-stage policy training strategy to learn\nnatural walking patterns for this interface-enhanced model. Comparative\nanalysis between our approach and conventional rigid musculoskeletal models\ndemonstrated improvements in kinematic, kinetic, and gait stability metrics.\nValidation against human subject data confirmed that our simulation closely\nreproduced real-world biomechanical measurements. This work advances\ncontact-rich interface modeling for human musculoskeletal systems and\nestablishes a robust framework that can be extended to humanoid robotics\napplications requiring precise foot-ground interaction control.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u63a5\u89e6\u4e30\u5bcc\u4e14\u53ef\u53d8\u5f62\u7684\u4eba\u8db3\u6a21\u578b\uff0c\u96c6\u6210\u5230\u5b8c\u6574\u808c\u8089\u9aa8\u9abc\u7cfb\u7edf\u4e2d\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u7b56\u7565\u8bad\u7ec3\u5b9e\u73b0\u4e86\u66f4\u81ea\u7136\u884c\u8d70\u6a21\u5f0f\u7684\u6a21\u62df\uff0c\u76f8\u6bd4\u4f20\u7edf\u521a\u6027\u6a21\u578b\u5728\u8fd0\u52a8\u5b66\u3001\u52a8\u529b\u5b66\u548c\u6b65\u6001\u7a33\u5b9a\u6027\u65b9\u9762\u5747\u6709\u6539\u8fdb\u3002", "motivation": "\u73b0\u6709\u808c\u8089\u9aa8\u9abc\u6a21\u578b\u901a\u5e38\u8fc7\u5ea6\u7b80\u5316\u8db3\u5730\u63a5\u89e6\u529b\u5b66\uff0c\u9650\u5236\u4e86\u51c6\u786e\u6a21\u62df\u4eba\u7c7b\u6b65\u6001\u52a8\u529b\u5b66\u7684\u80fd\u529b\u3002\u9700\u8981\u5f00\u53d1\u66f4\u7cbe\u786e\u7684\u63a5\u89e6\u754c\u9762\u6a21\u578b\u6765\u6355\u6349\u884c\u8d70\u8fc7\u7a0b\u4e2d\u7684\u590d\u6742\u751f\u7269\u529b\u5b66\u76f8\u4e92\u4f5c\u7528\u3002", "method": "\u5f00\u53d1\u4e86\u63a5\u89e6\u4e30\u5bcc\u4e14\u53ef\u53d8\u5f62\u7684\u4eba\u8db3\u6a21\u578b\uff0c\u96c6\u6210\u5230\u5b8c\u6574\u808c\u8089\u9aa8\u9abc\u7cfb\u7edf\u4e2d\u3002\u91c7\u7528\u4e24\u9636\u6bb5\u7b56\u7565\u8bad\u7ec3\u65b9\u6cd5\u6765\u89e3\u51b3\u591a\u70b9\u63a5\u89e6\u548c\u53ef\u53d8\u5f62\u6750\u6599\u5efa\u6a21\u4e2d\u7684\u63a7\u5236\u6311\u6218\uff0c\u5b66\u4e60\u81ea\u7136\u884c\u8d70\u6a21\u5f0f\u3002", "result": "\u4e0e\u4f20\u7edf\u521a\u6027\u808c\u8089\u9aa8\u9abc\u6a21\u578b\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u8fd0\u52a8\u5b66\u3001\u52a8\u529b\u5b66\u548c\u6b65\u6001\u7a33\u5b9a\u6027\u6307\u6807\u4e0a\u5747\u6709\u6539\u8fdb\u3002\u4e0e\u4eba\u7c7b\u53d7\u8bd5\u8005\u6570\u636e\u9a8c\u8bc1\u786e\u8ba4\uff0c\u6a21\u62df\u7ed3\u679c\u80fd\u591f\u5bc6\u5207\u91cd\u73b0\u771f\u5b9e\u4e16\u754c\u7684\u751f\u7269\u529b\u5b66\u6d4b\u91cf\u6570\u636e\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u63a8\u8fdb\u4e86\u4eba\u7c7b\u808c\u8089\u9aa8\u9abc\u7cfb\u7edf\u7684\u63a5\u89e6\u4e30\u5bcc\u754c\u9762\u5efa\u6a21\uff0c\u5efa\u7acb\u4e86\u4e00\u4e2a\u53ef\u4ee5\u6269\u5c55\u5230\u9700\u8981\u7cbe\u786e\u8db3\u5730\u4ea4\u4e92\u63a7\u5236\u7684\u4eba\u5f62\u673a\u5668\u4eba\u5e94\u7528\u7684\u7a33\u5065\u6846\u67b6\u3002"}}
{"id": "2508.11894", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11894", "abs": "https://arxiv.org/abs/2508.11894", "authors": ["Ao Li", "Bin Yan", "Bingfeng Cai", "Chenxi Li", "Cunzhong Zhao", "Fugen Yao", "Gaoqiang Liu", "Guanjun Jiang", "Jian Xu", "Liang Dong", "Liansheng Sun", "Rongshen Zhang", "Xiaolei Gui", "Xin Liu", "Xin Shang", "Yao Wu", "Yu Cao", "Zhenxin Ma", "Zhuang Jia"], "title": "QuarkMed Medical Foundation Model Technical Report", "comment": "20 pages", "summary": "Recent advancements in large language models have significantly accelerated\ntheir adoption in healthcare applications, including AI-powered medical\nconsultations, diagnostic report assistance, and medical search tools. However,\nmedical tasks often demand highly specialized knowledge, professional accuracy,\nand customization capabilities, necessitating a robust and reliable foundation\nmodel. QuarkMed addresses these needs by leveraging curated medical data\nprocessing, medical-content Retrieval-Augmented Generation (RAG), and a\nlarge-scale, verifiable reinforcement learning pipeline to develop a\nhigh-performance medical foundation model. The model achieved 70% accuracy on\nthe Chinese Medical Licensing Examination, demonstrating strong generalization\nacross diverse medical benchmarks. QuarkMed offers a powerful yet versatile\npersonal medical AI solution, already serving over millions of users at\nai.quark.cn.", "AI": {"tldr": "QuarkMed\u662f\u4e00\u4e2a\u9ad8\u6027\u80fd\u533b\u7597\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u533b\u5b66\u6570\u636e\u5904\u7406\u3001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u548c\u5927\u89c4\u6a21\u5f3a\u5316\u5b66\u4e60\uff0c\u5728\u4e2d\u56fd\u6267\u4e1a\u533b\u5e08\u8003\u8bd5\u4e2d\u8fbe\u523070%\u51c6\u786e\u7387\uff0c\u5df2\u670d\u52a1\u6570\u767e\u4e07\u7528\u6237\u3002", "motivation": "\u533b\u7597\u4efb\u52a1\u9700\u8981\u9ad8\u5ea6\u4e13\u4e1a\u5316\u7684\u77e5\u8bc6\u3001\u4e13\u4e1a\u51c6\u786e\u6027\u548c\u5b9a\u5236\u80fd\u529b\uff0c\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u5e94\u7528\u4e2d\u9700\u8981\u66f4\u53ef\u9760\u7684\u57fa\u7840\u6a21\u578b\u652f\u6301\u3002", "method": "\u5229\u7528\u7cbe\u9009\u533b\u5b66\u6570\u636e\u5904\u7406\u3001\u533b\u5b66\u5185\u5bb9\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u548c\u5927\u89c4\u6a21\u53ef\u9a8c\u8bc1\u5f3a\u5316\u5b66\u4e60\u7ba1\u9053\u6765\u5f00\u53d1\u533b\u7597\u57fa\u7840\u6a21\u578b\u3002", "result": "\u5728\u4e2d\u56fd\u533b\u5b66\u6267\u7167\u8003\u8bd5\u4e2d\u8fbe\u523070%\u7684\u51c6\u786e\u7387\uff0c\u5728\u591a\u6837\u5316\u533b\u7597\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "QuarkMed\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u800c\u901a\u7528\u7684\u4e2a\u4eba\u533b\u7597AI\u89e3\u51b3\u65b9\u6848\uff0c\u5df2\u5728ai.quark.cn\u670d\u52a1\u8d85\u8fc7\u767e\u4e07\u7528\u6237\u3002"}}
{"id": "2508.11887", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.11887", "abs": "https://arxiv.org/abs/2508.11887", "authors": ["Yousra Shleibik", "Jordan Sinclair", "Kerstin Haring"], "title": "Saliency-Based Attention Shifting: A Framework for Improving Driver Situational Awareness of Out-of-Label Hazards", "comment": null, "summary": "The advent of autonomous driving systems promises to transform transportation\nby enhancing safety, efficiency, and comfort. As these technologies evolve\ntoward higher levels of autonomy, the need for integrated systems that\nseamlessly support human involvement in decision-making becomes increasingly\ncritical. Certain scenarios necessitate human involvement, including those\nwhere the vehicle is unable to identify an object or element in the scene, and\nas such cannot take independent action. Therefore, situational awareness is\nessential to mitigate potential risks during a takeover, where a driver must\nassume control and autonomy from the vehicle. The need for driver attention is\nimportant to avoid collisions with external agents and ensure a smooth\ntransition during takeover operations. This paper explores the integration of\nattention redirection techniques, such as gaze manipulation through targeted\nvisual and auditory cues, to help drivers maintain focus on emerging hazards\nand reduce target fixation in semi-autonomous driving scenarios. We propose a\nconceptual framework that combines real-time gaze tracking, context-aware\nsaliency analysis, and synchronized visual and auditory alerts to enhance\nsituational awareness, proactively address potential hazards, and foster\neffective collaboration between humans and autonomous systems.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5728\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e2d\u901a\u8fc7\u89c6\u89c9\u548c\u542c\u89c9\u6ce8\u610f\u529b\u5f15\u5bfc\u6280\u672f\u6765\u589e\u5f3a\u9a7e\u9a76\u5458\u60c5\u5883\u611f\u77e5\uff0c\u51cf\u5c11\u76ee\u6807\u56fa\u7740\uff0c\u786e\u4fdd\u5e73\u7a33\u63a5\u7ba1\u64cd\u4f5c\u3002", "motivation": "\u968f\u7740\u81ea\u52a8\u9a7e\u9a76\u6280\u672f\u5411\u66f4\u9ad8\u81ea\u52a8\u5316\u6c34\u5e73\u53d1\u5c55\uff0c\u9700\u8981\u5728\u65e0\u6cd5\u8bc6\u522b\u573a\u666f\u5143\u7d20\u65f6\u8fdb\u884c\u4eba\u5de5\u63a5\u7ba1\uff0c\u800c\u9a7e\u9a76\u5458\u6ce8\u610f\u529b\u5bf9\u907f\u514d\u78b0\u649e\u548c\u5e73\u7a33\u8fc7\u6e21\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86\u7ed3\u5408\u5b9e\u65f6\u89c6\u7ebf\u8ffd\u8e2a\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u663e\u8457\u6027\u5206\u6790\u548c\u540c\u6b65\u89c6\u542c\u8b66\u62a5\u7684\u6982\u5ff5\u6846\u67b6\uff0c\u7528\u4e8e\u4e3b\u52a8\u5f15\u5bfc\u9a7e\u9a76\u5458\u6ce8\u610f\u529b\u5230\u6f5c\u5728\u5371\u9669\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u589e\u5f3a\u4eba\u673a\u534f\u4f5c\u7684\u6982\u5ff5\u6027\u6846\u67b6\uff0c\u65e8\u5728\u63d0\u9ad8\u60c5\u5883\u611f\u77e5\u80fd\u529b\u548c\u63a5\u7ba1\u5b89\u5168\u6027\u3002", "conclusion": "\u6ce8\u610f\u529b\u91cd\u5b9a\u5411\u6280\u672f\u5bf9\u4e8e\u534a\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2d\u7684\u4eba\u673a\u534f\u4f5c\u548c\u98ce\u9669\u7f13\u89e3\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u5f00\u53d1\u96c6\u6210\u7cfb\u7edf\u6765\u652f\u6301\u51b3\u7b56\u8fc7\u7a0b\u4e2d\u7684\u65e0\u7f1d\u4eba\u673a\u4ea4\u4e92\u3002"}}
{"id": "2508.11944", "categories": ["cs.AI", "cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.11944", "abs": "https://arxiv.org/abs/2508.11944", "authors": ["Hongtao Liu", "Zhicheng Du", "Zihe Wang", "Weiran Shen"], "title": "CHBench: A Cognitive Hierarchy Benchmark for Evaluating Strategic Reasoning Capability of LLMs", "comment": null, "summary": "Game-playing ability serves as an indicator for evaluating the strategic\nreasoning capability of large language models (LLMs). While most existing\nstudies rely on utility performance metrics, which are not robust enough due to\nvariations in opponent behavior and game structure. To address this limitation,\nwe propose \\textbf{Cognitive Hierarchy Benchmark (CHBench)}, a novel evaluation\nframework inspired by the cognitive hierarchy models from behavioral economics.\nWe hypothesize that agents have bounded rationality -- different agents behave\nat varying reasoning depths/levels. We evaluate LLMs' strategic reasoning\nthrough a three-phase systematic framework, utilizing behavioral data from six\nstate-of-the-art LLMs across fifteen carefully selected normal-form games.\nExperiments show that LLMs exhibit consistent strategic reasoning levels across\ndiverse opponents, confirming the framework's robustness and generalization\ncapability. We also analyze the effects of two key mechanisms (Chat Mechanism\nand Memory Mechanism) on strategic reasoning performance. Results indicate that\nthe Chat Mechanism significantly degrades strategic reasoning, whereas the\nMemory Mechanism enhances it. These insights position CHBench as a promising\ntool for evaluating LLM capabilities, with significant potential for future\nresearch and practical applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86CHBench\u8bc4\u4f30\u6846\u67b6\uff0c\u57fa\u4e8e\u8ba4\u77e5\u5c42\u6b21\u6a21\u578b\u8bc4\u4f30LLMs\u7684\u6218\u7565\u63a8\u7406\u80fd\u529b\uff0c\u53d1\u73b0\u804a\u5929\u673a\u5236\u4f1a\u964d\u4f4e\u63a8\u7406\u6c34\u5e73\u800c\u8bb0\u5fc6\u673a\u5236\u80fd\u63d0\u5347\u63a8\u7406\u80fd\u529b", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u4f9d\u8d56\u6548\u7528\u6027\u80fd\u6307\u6807\u8bc4\u4f30LLMs\u7684\u6e38\u620f\u80fd\u529b\uff0c\u4f46\u8fd9\u4e9b\u6307\u6807\u4e0d\u591f\u7a33\u5065\uff0c\u53d7\u5bf9\u624b\u884c\u4e3a\u548c\u6e38\u620f\u7ed3\u6784\u53d8\u5316\u5f71\u54cd\u8f83\u5927", "method": "\u91c7\u7528\u4e09\u9636\u6bb5\u7cfb\u7edf\u6846\u67b6\uff0c\u57fa\u4e8e\u884c\u4e3a\u7ecf\u6d4e\u5b66\u4e2d\u7684\u8ba4\u77e5\u5c42\u6b21\u6a21\u578b\uff0c\u572815\u4e2a\u7cbe\u9009\u7684\u6b63\u89c4\u5f62\u5f0f\u6e38\u620f\u4e2d\u5206\u67906\u4e2a\u6700\u5148\u8fdbLLMs\u7684\u884c\u4e3a\u6570\u636e", "result": "\u5b9e\u9a8c\u663e\u793aLLMs\u5728\u4e0d\u540c\u5bf9\u624b\u95f4\u5c55\u73b0\u51fa\u4e00\u81f4\u7684\u6218\u7565\u63a8\u7406\u6c34\u5e73\uff0c\u804a\u5929\u673a\u5236\u663e\u8457\u964d\u4f4e\u6218\u7565\u63a8\u7406\u80fd\u529b\uff0c\u800c\u8bb0\u5fc6\u673a\u5236\u80fd\u589e\u5f3a\u63a8\u7406\u80fd\u529b", "conclusion": "CHBench\u662f\u4e00\u4e2a\u6709\u524d\u666f\u7684LLM\u80fd\u529b\u8bc4\u4f30\u5de5\u5177\uff0c\u5bf9\u672a\u6765\u7814\u7a76\u548c\u5b9e\u9645\u5e94\u7528\u5177\u6709\u91cd\u8981\u6f5c\u529b"}}
{"id": "2508.11890", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11890", "abs": "https://arxiv.org/abs/2508.11890", "authors": ["Sangwoo Jeon", "Juchul Shin", "YeonJe Cho", "Gyeong-Tae Kim", "Seongwoo Kim"], "title": "Integrating Symbolic RL Planning into a BDI-based Autonomous UAV Framework: System Integration and SIL Validation", "comment": null, "summary": "Modern autonomous drone missions increasingly require software frameworks\ncapable of seamlessly integrating structured symbolic planning with adaptive\nreinforcement learning (RL). Although traditional rule-based architectures\noffer robust structured reasoning for drone autonomy, their capabilities fall\nshort in dynamically complex operational environments that require adaptive\nsymbolic planning. Symbolic RL (SRL), using the Planning Domain Definition\nLanguage (PDDL), explicitly integrates domain-specific knowledge and\noperational constraints, significantly improving the reliability and safety of\nunmanned aerial vehicle (UAV) decision making. In this study, we propose the\nAMAD-SRL framework, an extended and refined version of the Autonomous Mission\nAgents for Drones (AMAD) cognitive multi-agent architecture, enhanced with\nsymbolic reinforcement learning for dynamic mission planning and execution. We\nvalidated our framework in a Software-in-the-Loop (SIL) environment structured\nidentically to an intended Hardware-In-the-Loop Simulation (HILS) platform,\nensuring seamless transition to real hardware. Experimental results demonstrate\nstable integration and interoperability of modules, successful transitions\nbetween BDI-driven and symbolic RL-driven planning phases, and consistent\nmission performance. Specifically, we evaluate a target acquisition scenario in\nwhich the UAV plans a surveillance path followed by a dynamic reentry path to\nsecure the target while avoiding threat zones. In this SIL evaluation, mission\nefficiency improved by approximately 75% over a coverage-based baseline,\nmeasured by travel distance reduction. This study establishes a robust\nfoundation for handling complex UAV missions and discusses directions for\nfurther enhancement and validation.", "AI": {"tldr": "\u63d0\u51faAMAD-SRL\u6846\u67b6\uff0c\u7ed3\u5408\u7b26\u53f7\u5f3a\u5316\u5b66\u4e60\u548c\u591a\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u63d0\u5347\u65e0\u4eba\u673a\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u4efb\u52a1\u89c4\u5212\u80fd\u529b\uff0c\u5b9e\u9a8c\u663e\u793a\u4efb\u52a1\u6548\u7387\u63d0\u534775%", "motivation": "\u73b0\u4ee3\u65e0\u4eba\u673a\u4efb\u52a1\u9700\u8981\u5c06\u7b26\u53f7\u89c4\u5212\u4e0e\u5f3a\u5316\u5b66\u4e60\u65e0\u7f1d\u96c6\u6210\uff0c\u4f20\u7edf\u57fa\u4e8e\u89c4\u5219\u7684\u67b6\u6784\u5728\u52a8\u6001\u590d\u6742\u73af\u5883\u4e2d\u9002\u5e94\u6027\u4e0d\u8db3", "method": "\u6269\u5c55AMAD\u8ba4\u77e5\u591a\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u96c6\u6210\u7b26\u53f7\u5f3a\u5316\u5b66\u4e60(\u4f7f\u7528PDDL\u8bed\u8a00)\uff0c\u5728\u8f6f\u4ef6\u5728\u73af\u73af\u5883\u4e2d\u9a8c\u8bc1\u6846\u67b6", "result": "\u6a21\u5757\u96c6\u6210\u7a33\u5b9a\uff0cBDI\u9a71\u52a8\u548c\u7b26\u53f7RL\u89c4\u5212\u9636\u6bb5\u8f6c\u6362\u6210\u529f\uff0c\u76ee\u6807\u83b7\u53d6\u573a\u666f\u4e2d\u4efb\u52a1\u6548\u7387\u6bd4\u57fa\u7ebf\u63d0\u534775%(\u901a\u8fc7\u65c5\u884c\u8ddd\u79bb\u51cf\u5c11\u8861\u91cf)", "conclusion": "\u4e3a\u5904\u7406\u590d\u6742\u65e0\u4eba\u673a\u4efb\u52a1\u5efa\u7acb\u4e86\u575a\u5b9e\u57fa\u7840\uff0c\u8ba8\u8bba\u4e86\u8fdb\u4e00\u6b65\u6539\u8fdb\u548c\u9a8c\u8bc1\u7684\u65b9\u5411"}}
{"id": "2508.11953", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11953", "abs": "https://arxiv.org/abs/2508.11953", "authors": ["Yuan Li", "Zhengzhong Liu", "Eric Xing"], "title": "Data Mixing Optimization for Supervised Fine-Tuning of Large Language Models", "comment": null, "summary": "Optimizing data mixtures for supervised fine-tuning (SFT) of large language\nmodels (LLMs) is critical for developing general-purpose models, yet this area\nremains underexplored. In this paper, we frame data mixing as an optimization\nproblem and introduce a novel method designed to minimize validation loss. Our\napproach parametrizes the loss by modeling effective data transferred and\nleveraging scaling laws for fine-tuning. By experimenting with various\nsmall-scale data mixtures, we fit these parameters and derive the optimal\nweights. We provide both mathematical proofs and empirical results\ndemonstrating that our algorithm achieves excellent overall and individual\nperformance across all domains. Through controlled experiments, we show that\nmodels trained with our optimized weights perform on par with those using\noptimal weights determined via grid search, with per-domain loss only 0.66%\nhigher than the best domain loss from grid search on average. Additionally, we\nshow that reweighting popular SFT datasets using our method improves both\nvalidation loss and downstream performance. Finally, we discuss how our method\ncan generalize to guide data selection for domain-specific models and provide\ninsights into SFT.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f18\u5316\u76d1\u7763\u5fae\u8c03\u6570\u636e\u6df7\u5408\u6bd4\u4f8b\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5efa\u6a21\u6709\u6548\u6570\u636e\u4f20\u8f93\u548c\u5229\u7528\u7f29\u653e\u5b9a\u5f8b\u6765\u6700\u5c0f\u5316\u9a8c\u8bc1\u635f\u5931\uff0c\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u6027\u80fd\u63a5\u8fd1\u7f51\u683c\u641c\u7d22\u6700\u4f18\u89e3\u3002", "motivation": "\u4f18\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\u76d1\u7763\u5fae\u8c03\u7684\u6570\u636e\u6df7\u5408\u6bd4\u4f8b\u5bf9\u4e8e\u5f00\u53d1\u901a\u7528\u6a21\u578b\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u8fd9\u4e00\u9886\u57df\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u5c06\u6570\u636e\u6df7\u5408\u6784\u5efa\u4e3a\u4f18\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u53c2\u6570\u5316\u635f\u5931\u51fd\u6570\u5efa\u6a21\u6709\u6548\u6570\u636e\u4f20\u8f93\uff0c\u5229\u7528\u7f29\u653e\u5b9a\u5f8b\u8fdb\u884c\u5fae\u8c03\uff0c\u901a\u8fc7\u5c0f\u89c4\u6a21\u6570\u636e\u6df7\u5408\u5b9e\u9a8c\u62df\u5408\u53c2\u6570\u5e76\u63a8\u5bfc\u6700\u4f18\u6743\u91cd\u3002", "result": "\u7b97\u6cd5\u5728\u6240\u6709\u9886\u57df\u90fd\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u6574\u4f53\u548c\u4e2a\u4f53\u6027\u80fd\uff0c\u4f18\u5316\u6743\u91cd\u8bad\u7ec3\u7684\u6a21\u578b\u6027\u80fd\u4e0e\u7f51\u683c\u641c\u7d22\u786e\u5b9a\u7684\u6700\u4f18\u6743\u91cd\u76f8\u5f53\uff0c\u5e73\u5747\u6bcf\u57df\u635f\u5931\u4ec5\u6bd4\u7f51\u683c\u641c\u7d22\u6700\u4f73\u57df\u635f\u5931\u9ad80.66%\u3002\u91cd\u65b0\u52a0\u6743\u6d41\u884cSFT\u6570\u636e\u96c6\u6539\u5584\u4e86\u9a8c\u8bc1\u635f\u5931\u548c\u4e0b\u6e38\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u63a8\u5e7f\u7528\u4e8e\u6307\u5bfc\u9886\u57df\u7279\u5b9a\u6a21\u578b\u7684\u6570\u636e\u9009\u62e9\uff0c\u5e76\u4e3a\u76d1\u7763\u5fae\u8c03\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\u3002"}}
{"id": "2508.11898", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11898", "abs": "https://arxiv.org/abs/2508.11898", "authors": ["Jilei Mao", "Jiarui Guan", "Yingjuan Tang", "Qirui Hu", "Zhihang Li", "Junjie Yu", "Yongjie Mao", "Yunzhe Sun", "Shuang Liu", "Xiaozhu Ju"], "title": "OmniD: Generalizable Robot Manipulation Policy via Image-Based BEV Representation", "comment": null, "summary": "The visuomotor policy can easily overfit to its training datasets, such as\nfixed camera positions and backgrounds. This overfitting makes the policy\nperform well in the in-distribution scenarios but underperform in the\nout-of-distribution generalization. Additionally, the existing methods also\nhave difficulty fusing multi-view information to generate an effective 3D\nrepresentation. To tackle these issues, we propose Omni-Vision Diffusion Policy\n(OmniD), a multi-view fusion framework that synthesizes image observations into\na unified bird's-eye view (BEV) representation. We introduce a deformable\nattention-based Omni-Feature Generator (OFG) to selectively abstract\ntask-relevant features while suppressing view-specific noise and background\ndistractions. OmniD achieves 11\\%, 17\\%, and 84\\% average improvement over the\nbest baseline model for in-distribution, out-of-distribution, and few-shot\nexperiments, respectively. Training code and simulation benchmark are\navailable: https://github.com/1mather/omnid.git", "AI": {"tldr": "OmniD\u662f\u4e00\u4e2a\u591a\u89c6\u89d2\u878d\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u53ef\u53d8\u5f62\u6ce8\u610f\u529b\u673a\u5236\u5c06\u591a\u89c6\u89d2\u56fe\u50cf\u5408\u6210\u4e3a\u7edf\u4e00\u7684\u9e1f\u77b0\u56fe\u8868\u793a\uff0c\u89e3\u51b3\u4e86\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u7684\u8fc7\u62df\u5408\u95ee\u9898\u548c\u591a\u89c6\u89d2\u4fe1\u606f\u878d\u5408\u96be\u9898", "motivation": "\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u5bb9\u6613\u5728\u8bad\u7ec3\u6570\u636e\u4e0a\u8fc7\u62df\u5408\uff08\u5982\u56fa\u5b9a\u76f8\u673a\u4f4d\u7f6e\u548c\u80cc\u666f\uff09\uff0c\u5bfc\u81f4\u5728\u5206\u5e03\u5916\u573a\u666f\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u878d\u5408\u591a\u89c6\u89d2\u4fe1\u606f\u751f\u62103D\u8868\u793a", "method": "\u63d0\u51faOmni-Vision Diffusion Policy (OmniD)\u6846\u67b6\uff0c\u4f7f\u7528\u57fa\u4e8e\u53ef\u53d8\u5f62\u6ce8\u610f\u529b\u7684Omni-Feature Generator (OFG)\u9009\u62e9\u6027\u63d0\u53d6\u4efb\u52a1\u76f8\u5173\u7279\u5f81\uff0c\u6291\u5236\u89c6\u89d2\u7279\u5b9a\u566a\u58f0\u548c\u80cc\u666f\u5e72\u6270\uff0c\u5408\u6210\u7edf\u4e00\u7684\u9e1f\u77b0\u56fe\u8868\u793a", "result": "\u5728\u5206\u5e03\u5185\u3001\u5206\u5e03\u5916\u548c\u5c11\u6837\u672c\u5b9e\u9a8c\u4e2d\u5206\u522b\u6bd4\u6700\u4f73\u57fa\u7ebf\u6a21\u578b\u5e73\u5747\u63d0\u534711%\u300117%\u548c84%", "conclusion": "OmniD\u901a\u8fc7\u6709\u6548\u7684\u591a\u89c6\u89d2\u878d\u5408\u548c\u7279\u5f81\u9009\u62e9\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\u8868\u73b0"}}
{"id": "2508.11954", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11954", "abs": "https://arxiv.org/abs/2508.11954", "authors": ["Sehyuk Park", "Soyeon Caren Han", "Eduard Hovy"], "title": "UniCast: A Unified Multimodal Prompting Framework for Time Series Forecasting", "comment": null, "summary": "Time series forecasting is a foundational task across domains, such as\nfinance, healthcare, and environmental monitoring. While recent advances in\nTime Series Foundation Models (TSFMs) have demonstrated strong generalisation\nthrough large-scale pretraining, existing models operate predominantly in a\nunimodal setting, ignoring the rich multimodal context, such as visual and\ntextual signals, that often accompanies time series data in real-world\nscenarios. This paper introduces a novel parameter-efficient multimodal\nframework, UniCast, that extends TSFMs to jointly leverage time series, vision,\nand text modalities for enhanced forecasting performance. Our method integrates\nmodality-specific embeddings from pretrained Vision and Text Encoders with a\nfrozen TSFM via soft prompt tuning, enabling efficient adaptation with minimal\nparameter updates. This design not only preserves the generalisation strength\nof the foundation model but also enables effective cross-modal interaction.\nExtensive experiments across diverse time-series forecasting benchmarks\ndemonstrate that UniCast consistently and significantly outperforms all\nexisting TSFM baselines. The findings highlight the critical role of multimodal\ncontext in advancing the next generation of general-purpose time series\nforecasters.", "AI": {"tldr": "UniCast\u662f\u4e00\u4e2a\u53c2\u6570\u9ad8\u6548\u7684\u591a\u6a21\u6001\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u8f6f\u63d0\u793a\u8c03\u4f18\u5c06\u89c6\u89c9\u548c\u6587\u672c\u6a21\u6001\u4e0e\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u4e3b\u8981\u5728\u5355\u6a21\u6001\u8bbe\u7f6e\u4e0b\u8fd0\u884c\uff0c\u5ffd\u7565\u4e86\u73b0\u5b9e\u573a\u666f\u4e2d\u5e38\u4f34\u968f\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u7684\u4e30\u5bcc\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\uff08\u5982\u89c6\u89c9\u548c\u6587\u672c\u4fe1\u53f7\uff09\uff0c\u8fd9\u9650\u5236\u4e86\u9884\u6d4b\u6027\u80fd\u7684\u8fdb\u4e00\u6b65\u63d0\u5347\u3002", "method": "\u63d0\u51faUniCast\u6846\u67b6\uff0c\u4f7f\u7528\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u548c\u6587\u672c\u7f16\u7801\u5668\u63d0\u53d6\u6a21\u6001\u7279\u5b9a\u5d4c\u5165\uff0c\u901a\u8fc7\u8f6f\u63d0\u793a\u8c03\u4f18\u6280\u672f\u4e0e\u51bb\u7ed3\u7684\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u96c6\u6210\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u8de8\u6a21\u6001\u4ea4\u4e92\u548c\u6700\u5c0f\u53c2\u6570\u66f4\u65b0\u3002", "result": "\u5728\u591a\u4e2a\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cUniCast\u59cb\u7ec8\u663e\u8457\u4f18\u4e8e\u6240\u6709\u73b0\u6709\u7684\u65f6\u95f4\u5e8f\u5217\u57fa\u7840\u6a21\u578b\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u5728\u63a8\u8fdb\u4e0b\u4e00\u4ee3\u901a\u7528\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u5668\u53d1\u5c55\u4e2d\u8d77\u7740\u5173\u952e\u4f5c\u7528\uff0cUniCast\u6846\u67b6\u4e3a\u591a\u6a21\u6001\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.11917", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11917", "abs": "https://arxiv.org/abs/2508.11917", "authors": ["Hossein Keshavarz", "Alejandro Ramirez-Serrano", "Majid Khadiv"], "title": "Control of Legged Robots using Model Predictive Optimized Path Integral", "comment": "8 pages, 13 figures, Humanoid conference", "summary": "Legged robots possess a unique ability to traverse rough terrains and\nnavigate cluttered environments, making them well-suited for complex,\nreal-world unstructured scenarios. However, such robots have not yet achieved\nthe same level as seen in natural systems. Recently, sampling-based predictive\ncontrollers have demonstrated particularly promising results. This paper\ninvestigates a sampling-based model predictive strategy combining model\npredictive path integral (MPPI) with cross-entropy (CE) and covariance matrix\nadaptation (CMA) methods to generate real-time whole-body motions for legged\nrobots across multiple scenarios. The results show that combining the benefits\nof MPPI, CE and CMA, namely using model predictive optimized path integral\n(MPOPI), demonstrates greater sample efficiency, enabling robots to attain\nsuperior locomotion results using fewer samples when compared to typical MPPI\nalgorithms. Extensive simulation experiments in multiple scenarios on a\nquadruped robot show that MPOPI can be used as an anytime control strategy,\nincreasing locomotion capabilities at each iteration.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faMPOPI\u65b9\u6cd5\uff0c\u7ed3\u5408MPPI\u3001CE\u548cCMA\u7b97\u6cd5\uff0c\u4e3a\u817f\u5f0f\u673a\u5668\u4eba\u5b9e\u73b0\u5b9e\u65f6\u5168\u8eab\u8fd0\u52a8\u63a7\u5236\uff0c\u5728\u591a\u79cd\u573a\u666f\u4e0b\u5c55\u73b0\u51fa\u66f4\u9ad8\u7684\u91c7\u6837\u6548\u7387\u548c\u8fd0\u52a8\u6027\u80fd", "motivation": "\u817f\u5f0f\u673a\u5668\u4eba\u5728\u590d\u6742\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u5177\u6709\u72ec\u7279\u4f18\u52bf\uff0c\u4f46\u73b0\u6709\u6280\u672f\u5c1a\u672a\u8fbe\u5230\u81ea\u7136\u7cfb\u7edf\u7684\u6c34\u5e73\u3002\u91c7\u6837\u9884\u6d4b\u63a7\u5236\u5668\u663e\u793a\u51fa\u826f\u597d\u524d\u666f\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u63d0\u9ad8\u91c7\u6837\u6548\u7387\u548c\u8fd0\u52a8\u80fd\u529b", "method": "\u91c7\u7528\u57fa\u4e8e\u91c7\u6837\u7684\u6a21\u578b\u9884\u6d4b\u7b56\u7565\uff0c\u7ed3\u5408\u6a21\u578b\u9884\u6d4b\u8def\u5f84\u79ef\u5206(MPPI)\u3001\u4ea4\u53c9\u71b5(CE)\u548c\u534f\u65b9\u5dee\u77e9\u9635\u81ea\u9002\u5e94(CMA)\u65b9\u6cd5\uff0c\u5f00\u53d1MPOPI\u7b97\u6cd5", "result": "MPOPI\u76f8\u6bd4\u5178\u578bMPPI\u7b97\u6cd5\u5177\u6709\u66f4\u9ad8\u7684\u91c7\u6837\u6548\u7387\uff0c\u80fd\u7528\u66f4\u5c11\u6837\u672c\u83b7\u5f97\u4f18\u8d8a\u7684\u8fd0\u52a8\u7ed3\u679c\u3002\u56db\u8db3\u673a\u5668\u4eba\u591a\u573a\u666f\u4eff\u771f\u5b9e\u9a8c\u8868\u660eMPOPI\u53ef\u4f5c\u4e3a\u968f\u65f6\u63a7\u5236\u7b56\u7565\uff0c\u6bcf\u6b21\u8fed\u4ee3\u90fd\u80fd\u63d0\u5347\u8fd0\u52a8\u80fd\u529b", "conclusion": "MPOPI\u65b9\u6cd5\u6210\u529f\u6574\u5408\u4e86MPPI\u3001CE\u548cCMA\u7684\u4f18\u52bf\uff0c\u4e3a\u817f\u5f0f\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u5b9e\u65f6\u8fd0\u52a8\u63a7\u5236\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u91c7\u6837\u6548\u7387\u548c\u8fd0\u52a8\u6027\u80fd\u65b9\u9762\u90fd\u6709\u663e\u8457\u63d0\u5347"}}
{"id": "2508.11959", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11959", "abs": "https://arxiv.org/abs/2508.11959", "authors": ["Xuanxiang Huang", "Olivier L\u00e9toff\u00e9", "Joao Marques-Silva"], "title": "Rigorous Feature Importance Scores based on Shapley Value and Banzhaf Index", "comment": null, "summary": "Feature attribution methods based on game theory are ubiquitous in the field\nof eXplainable Artificial Intelligence (XAI). Recent works proposed rigorous\nfeature attribution using logic-based explanations, specifically targeting\nhigh-stakes uses of machine learning (ML) models. Typically, such works exploit\nweak abductive explanation (WAXp) as the characteristic function to assign\nimportance to features. However, one possible downside is that the contribution\nof non-WAXp sets is neglected. In fact, non-WAXp sets can also convey important\ninformation, because of the relationship between formal explanations (XPs) and\nadversarial examples (AExs). Accordingly, this paper leverages Shapley value\nand Banzhaf index to devise two novel feature importance scores. We take into\naccount non-WAXp sets when computing feature contribution, and the novel scores\nquantify how effective each feature is at excluding AExs. Furthermore, the\npaper identifies properties and studies the computational complexity of the\nproposed scores.", "AI": {"tldr": "\u672c\u6587\u57fa\u4e8e\u535a\u5f08\u8bba\u63d0\u51fa\u4e24\u79cd\u65b0\u7684\u7279\u5f81\u91cd\u8981\u6027\u8bc4\u5206\u65b9\u6cd5\uff0c\u5229\u7528Shapley\u503c\u548cBanzhaf\u6307\u6570\uff0c\u5728\u8ba1\u7b97\u7279\u5f81\u8d21\u732e\u65f6\u8003\u8651\u975e\u5f31\u6eaf\u56e0\u89e3\u91ca\u96c6\uff0c\u91cf\u5316\u7279\u5f81\u6392\u9664\u5bf9\u6297\u6837\u672c\u7684\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u903b\u8f91\u89e3\u91ca\u7684\u7279\u5f81\u5f52\u56e0\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5f31\u6eaf\u56e0\u89e3\u91ca(WAXp)\u96c6\uff0c\u4f46\u5ffd\u7565\u4e86\u975eWAXp\u96c6\u7684\u91cd\u8981\u6027\u3002\u975eWAXp\u96c6\u4e0e\u5bf9\u6297\u6837\u672c(AExs)\u5b58\u5728\u91cd\u8981\u5173\u8054\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u7279\u5f81\u91cd\u8981\u6027\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u535a\u5f08\u8bba\u4e2d\u7684Shapley\u503c\u548cBanzhaf\u6307\u6570\uff0c\u5728\u7279\u5f81\u91cd\u8981\u6027\u8ba1\u7b97\u4e2d\u7eb3\u5165\u975e\u5f31\u6eaf\u56e0\u89e3\u91ca\u96c6\u7684\u8d21\u732e\uff0c\u8bbe\u8ba1\u65b0\u7684\u7279\u5f81\u8bc4\u5206\u673a\u5236\u6765\u91cf\u5316\u7279\u5f81\u6392\u9664\u5bf9\u6297\u6837\u672c\u7684\u80fd\u529b\u3002", "result": "\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u9896\u7684\u7279\u5f81\u91cd\u8981\u6027\u8bc4\u5206\u65b9\u6cd5\uff0c\u80fd\u591f\u66f4\u5168\u9762\u5730\u8bc4\u4f30\u7279\u5f81\u5728\u6a21\u578b\u89e3\u91ca\u548c\u5bf9\u6297\u6837\u672c\u9632\u5fa1\u4e2d\u7684\u4f5c\u7528\uff0c\u5e76\u5206\u6790\u4e86\u8fd9\u4e9b\u8bc4\u5206\u65b9\u6cd5\u7684\u6027\u8d28\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "conclusion": "\u901a\u8fc7\u8003\u8651\u975eWAXp\u96c6\u7684\u7279\u5f81\u8d21\u732e\uff0c\u65b0\u7684\u8bc4\u5206\u65b9\u6cd5\u63d0\u4f9b\u4e86\u66f4\u5b8c\u6574\u7684\u7279\u5f81\u91cd\u8981\u6027\u8bc4\u4f30\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u63d0\u9ad8\u9ad8\u98ce\u9669\u673a\u5668\u5b66\u4e60\u5e94\u7528\u7684\u53ef\u89e3\u91ca\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.11918", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11918", "abs": "https://arxiv.org/abs/2508.11918", "authors": ["Zhichen Lou", "Kechun Xu", "Zhongxiang Zhou", "Rong Xiong"], "title": "ExploreVLM: Closed-Loop Robot Exploration Task Planning with Vision-Language Models", "comment": null, "summary": "The advancement of embodied intelligence is accelerating the integration of\nrobots into daily life as human assistants. This evolution requires robots to\nnot only interpret high-level instructions and plan tasks but also perceive and\nadapt within dynamic environments. Vision-Language Models (VLMs) present a\npromising solution by combining visual understanding and language reasoning.\nHowever, existing VLM-based methods struggle with interactive exploration,\naccurate perception, and real-time plan adaptation. To address these\nchallenges, we propose ExploreVLM, a novel closed-loop task planning framework\npowered by Vision-Language Models (VLMs). The framework is built around a\nstep-wise feedback mechanism that enables real-time plan adjustment and\nsupports interactive exploration. At its core is a dual-stage task planner with\nself-reflection, enhanced by an object-centric spatial relation graph that\nprovides structured, language-grounded scene representations to guide\nperception and planning. An execution validator supports the closed loop by\nverifying each action and triggering re-planning. Extensive real-world\nexperiments demonstrate that ExploreVLM significantly outperforms\nstate-of-the-art baselines, particularly in exploration-centric tasks. Ablation\nstudies further validate the critical role of the reflective planner and\nstructured perception in achieving robust and efficient task execution.", "AI": {"tldr": "\u63d0\u51fa\u4e86ExploreVLM\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u95ed\u73af\u4efb\u52a1\u89c4\u5212\uff0c\u7ed3\u5408\u9010\u6b65\u53cd\u9988\u673a\u5236\u548c\u7269\u4f53\u4e2d\u5fc3\u7a7a\u95f4\u5173\u7cfb\u56fe\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u63a2\u7d22\u548c\u4efb\u52a1\u6267\u884c\u80fd\u529b\u3002", "motivation": "\u73b0\u6709VLM\u65b9\u6cd5\u5728\u4ea4\u4e92\u63a2\u7d22\u3001\u7cbe\u786e\u611f\u77e5\u548c\u5b9e\u65f6\u8ba1\u5212\u8c03\u6574\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u9002\u5e94\u52a8\u6001\u73af\u5883\u7684\u65b0\u578b\u4efb\u52a1\u89c4\u5212\u6846\u67b6\u3002", "method": "\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u53cc\u9636\u6bb5\u4efb\u52a1\u89c4\u5212\u5668\uff0c\u5177\u6709\u81ea\u6211\u53cd\u601d\u673a\u5236\u548c\u7269\u4f53\u4e2d\u5fc3\u7a7a\u95f4\u5173\u7cfb\u56fe\uff0c\u914d\u5408\u6267\u884c\u9a8c\u8bc1\u5668\u5f62\u6210\u95ed\u73af\u7cfb\u7edf\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u4f73\u57fa\u7ebf\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u63a2\u7d22\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "ExploreVLM\u6846\u67b6\u901a\u8fc7\u7ed3\u6784\u5316\u611f\u77e5\u548c\u53cd\u601d\u89c4\u5212\u5668\u7684\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u9c81\u68d2\u9ad8\u6548\u7684\u4efb\u52a1\u6267\u884c\uff0c\u4e3a\u5177\u8eab\u667a\u80fd\u53d1\u5c55\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.11975", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11975", "abs": "https://arxiv.org/abs/2508.11975", "authors": ["Gongyao Jiang", "Qiong Luo"], "title": "Chart-CoCa: Self-Improving Chart Understanding of Vision LMs via Code-Driven Synthesis and Candidate-Conditioned Answering", "comment": "Accepted to CIKM 2025", "summary": "Vision Language Models (VLMs) often struggle with chart understanding tasks,\nparticularly in accurate chart description and complex reasoning. Synthetic\ndata generation is a promising solution, while usually facing the challenge of\nnoise labels. To address this challenge, we first introduce a chart synthesis\npipeline that generates aligned chart-question-answer triplets through code\ngeneration and execution, ensuring the reliability of synthetic data without\nhuman intervention. Furthermore, inspired by test-time scaling that increases\ninference budget and thereby improves performance, we design a\ncandidate-conditioned answering process. The VLM first generates multiple\nresponses per query, and then synthesizes the final answer by contextualizing\nthese candidates. Experiments demonstrate significant improvements, with up to\n15.50 points accuracy gain over the initial VLM, in a fully self-improving\nparadigm without either human-labeled data or external models.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u4ee3\u7801\u751f\u6210\u548c\u6267\u884c\u7684\u56fe\u8868\u5408\u6210\u7ba1\u9053\u6765\u751f\u6210\u5bf9\u9f50\u7684\u56fe\u8868-\u95ee\u9898-\u56de\u7b54\u4e09\u5143\u7ec4\uff0c\u5e76\u8bbe\u8ba1\u4e86\u5019\u9009\u6761\u4ef6\u56de\u7b54\u8fc7\u7a0b\uff0c\u4f7fVLM\u80fd\u591f\u81ea\u6211\u6539\u8fdb\uff0c\u5728\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u6216\u5916\u90e8\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\u663e\u8457\u63d0\u5347\u56fe\u8868\u7406\u89e3\u6027\u80fd\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u56fe\u8868\u7406\u89e3\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u7279\u522b\u662f\u5728\u51c6\u786e\u63cf\u8ff0\u548c\u590d\u6742\u63a8\u7406\u65b9\u9762\u3002\u5408\u6210\u6570\u636e\u751f\u6210\u662f\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u901a\u5e38\u9762\u4e34\u566a\u58f0\u6807\u7b7e\u7684\u6311\u6218\u3002", "method": "1) \u5f15\u5165\u56fe\u8868\u5408\u6210\u7ba1\u9053\uff0c\u901a\u8fc7\u4ee3\u7801\u751f\u6210\u548c\u6267\u884c\u751f\u6210\u5bf9\u9f50\u7684\u56fe\u8868-\u95ee\u9898-\u56de\u7b54\u4e09\u5143\u7ec4\uff1b2) \u8bbe\u8ba1\u5019\u9009\u6761\u4ef6\u56de\u7b54\u8fc7\u7a0b\uff0cVLM\u9996\u5148\u751f\u6210\u591a\u4e2a\u54cd\u5e94\uff0c\u7136\u540e\u901a\u8fc7\u4e0a\u4e0b\u6587\u6574\u5408\u8fd9\u4e9b\u5019\u9009\u7b54\u6848\u6765\u5408\u6210\u6700\u7ec8\u7b54\u6848\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u663e\u8457\u6539\u8fdb\uff0c\u5728\u5b8c\u5168\u81ea\u6211\u6539\u8fdb\u7684\u8303\u5f0f\u4e0b\uff0c\u6bd4\u521d\u59cbVLM\u83b7\u5f97\u4e86\u9ad8\u8fbe15.50\u4e2a\u767e\u5206\u70b9\u7684\u51c6\u786e\u7387\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u6216\u5916\u90e8\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\uff0c\u6210\u529f\u5b9e\u73b0\u4e86VLM\u7684\u81ea\u6211\u6539\u8fdb\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u8868\u7406\u89e3\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2508.11929", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11929", "abs": "https://arxiv.org/abs/2508.11929", "authors": ["Mohitvishnu S. Gadde", "Pranay Dugar", "Ashish Malik", "Alan Fern"], "title": "No More Blind Spots: Learning Vision-Based Omnidirectional Bipedal Locomotion for Challenging Terrain", "comment": null, "summary": "Effective bipedal locomotion in dynamic environments, such as cluttered\nindoor spaces or uneven terrain, requires agile and adaptive movement in all\ndirections. This necessitates omnidirectional terrain sensing and a controller\ncapable of processing such input. We present a learning framework for\nvision-based omnidirectional bipedal locomotion, enabling seamless movement\nusing depth images. A key challenge is the high computational cost of rendering\nomnidirectional depth images in simulation, making traditional sim-to-real\nreinforcement learning (RL) impractical. Our method combines a robust blind\ncontroller with a teacher policy that supervises a vision-based student policy,\ntrained on noise-augmented terrain data to avoid rendering costs during RL and\nensure robustness. We also introduce a data augmentation technique for\nsupervised student training, accelerating training by up to 10 times compared\nto conventional methods. Our framework is validated through simulation and\nreal-world tests, demonstrating effective omnidirectional locomotion with\nminimal reliance on expensive rendering. This is, to the best of our knowledge,\nthe first demonstration of vision-based omnidirectional bipedal locomotion,\nshowcasing its adaptability to diverse terrains.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u7684\u5168\u5411\u53cc\u8db3\u8fd0\u52a8\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u76f2\u63a7\u5236\u5668\u548c\u6559\u5e08\u7b56\u7565\u6765\u907f\u514d\u6602\u8d35\u7684\u5168\u5411\u6df1\u5ea6\u56fe\u50cf\u6e32\u67d3\u6210\u672c\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5168\u5411\u5730\u5f62\u9002\u5e94\u80fd\u529b\u3002", "motivation": "\u5728\u52a8\u6001\u73af\u5883\uff08\u5982\u6742\u4e71\u5ba4\u5185\u7a7a\u95f4\u6216\u4e0d\u5e73\u5766\u5730\u5f62\uff09\u4e2d\u5b9e\u73b0\u6709\u6548\u7684\u53cc\u8db3\u8fd0\u52a8\u9700\u8981\u5168\u65b9\u4f4d\u7684\u654f\u6377\u548c\u81ea\u9002\u5e94\u79fb\u52a8\u80fd\u529b\uff0c\u8fd9\u9700\u8981\u5168\u5411\u5730\u5f62\u611f\u77e5\u548c\u80fd\u591f\u5904\u7406\u6b64\u7c7b\u8f93\u5165\u7684\u63a7\u5236\u5668\u3002", "method": "\u7ed3\u5408\u9c81\u68d2\u7684\u76f2\u63a7\u5236\u5668\u548c\u6559\u5e08\u7b56\u7565\uff0c\u76d1\u7763\u57fa\u4e8e\u89c6\u89c9\u7684\u5b66\u751f\u7b56\u7565\uff0c\u5728\u566a\u58f0\u589e\u5f3a\u7684\u5730\u5f62\u6570\u636e\u4e0a\u8fdb\u884c\u8bad\u7ec3\u4ee5\u907f\u514dRL\u671f\u95f4\u7684\u6e32\u67d3\u6210\u672c\u5e76\u786e\u4fdd\u9c81\u68d2\u6027\u3002\u8fd8\u5f15\u5165\u4e86\u6570\u636e\u589e\u5f3a\u6280\u672f\u6765\u52a0\u901f\u76d1\u7763\u5b66\u751f\u8bad\u7ec3\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5168\u5411\u8fd0\u52a8\u80fd\u529b\uff0c\u5bf9\u591a\u6837\u5316\u5730\u5f62\u5177\u6709\u826f\u597d\u7684\u9002\u5e94\u6027\uff0c\u8bad\u7ec3\u901f\u5ea6\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5feb10\u500d\u3002", "conclusion": "\u8fd9\u662f\u9996\u4e2a\u57fa\u4e8e\u89c6\u89c9\u7684\u5168\u5411\u53cc\u8db3\u8fd0\u52a8\u6f14\u793a\uff0c\u5c55\u793a\u4e86\u5176\u5728\u591a\u6837\u5316\u5730\u5f62\u4e0a\u7684\u9002\u5e94\u80fd\u529b\uff0c\u4e14\u6700\u5c0f\u5316\u4e86\u5bf9\u6602\u8d35\u6e32\u67d3\u7684\u4f9d\u8d56\u3002"}}
{"id": "2508.11987", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.11987", "abs": "https://arxiv.org/abs/2508.11987", "authors": ["Zhiyuan Zeng", "Jiashuo Liu", "Siyuan Chen", "Tianci He", "Yali Liao", "Jinpeng Wang", "Zaiyuan Wang", "Yang Yang", "Lingyue Yin", "Mingren Yin", "Zhenwei Zhu", "Tianle Cai", "Zehui Chen", "Jiecao Chen", "Yantao Du", "Xiang Gao", "Jiacheng Guo", "Liang Hu", "Jianpeng Jiao", "Xiangsheng Li", "Jingkai Liu", "Shuang Ni", "Zhoufutu Wen", "Ge Zhang", "Kaiyuan Zhang", "Xin Zhou", "Jose Blanchet", "Xipeng Qiu", "Mengdi Wang", "Wenhao Huang"], "title": "FutureX: An Advanced Live Benchmark for LLM Agents in Future Prediction", "comment": "Technical report, 51 pages", "summary": "Future prediction is a complex task for LLM agents, requiring a high level of\nanalytical thinking, information gathering, contextual understanding, and\ndecision-making under uncertainty. Agents must not only gather and interpret\nvast amounts of dynamic information but also integrate diverse data sources,\nweigh uncertainties, and adapt predictions based on emerging trends, just as\nhuman experts do in fields like politics, economics, and finance. Despite its\nimportance, no large-scale benchmark exists for evaluating agents on future\nprediction, largely due to challenges in handling real-time updates and\nretrieving timely, accurate answers. To address this, we introduce\n$\\textbf{FutureX}$, a dynamic and live evaluation benchmark specifically\ndesigned for LLM agents performing future prediction tasks. FutureX is the\nlargest and most diverse live benchmark for future prediction, supporting\nreal-time daily updates and eliminating data contamination through an automated\npipeline for question gathering and answer collection. We evaluate 25 LLM/agent\nmodels, including those with reasoning, search capabilities, and integration of\nexternal tools such as the open-source Deep Research Agent and closed-source\nDeep Research models. This comprehensive evaluation assesses agents' adaptive\nreasoning and performance in dynamic environments. Additionally, we provide\nin-depth analyses of agents' failure modes and performance pitfalls in\nfuture-oriented tasks, including the vulnerability to fake web pages and the\ntemporal validity. Our goal is to establish a dynamic, contamination-free\nevaluation standard that drives the development of LLM agents capable of\nperforming at the level of professional human analysts in complex reasoning and\npredictive thinking.", "AI": {"tldr": "FutureX\u662f\u4e00\u4e2a\u4e13\u95e8\u4e3aLLM\u667a\u80fd\u4f53\u8bbe\u8ba1\u7684\u52a8\u6001\u5b9e\u65f6\u672a\u6765\u9884\u6d4b\u8bc4\u4f30\u57fa\u51c6\uff0c\u652f\u6301\u6bcf\u65e5\u5b9e\u65f6\u66f4\u65b0\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u6d41\u7a0b\u6d88\u9664\u6570\u636e\u6c61\u67d3\uff0c\u8bc4\u4f30\u4e8625\u4e2a\u6a21\u578b\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u81ea\u9002\u5e94\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u672a\u6765\u9884\u6d4b\u5bf9LLM\u667a\u80fd\u4f53\u662f\u590d\u6742\u4efb\u52a1\uff0c\u9700\u8981\u9ad8\u6c34\u5e73\u5206\u6790\u601d\u7ef4\u548c\u4fe1\u606f\u5904\u7406\u80fd\u529b\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u5927\u89c4\u6a21\u8bc4\u4f30\u57fa\u51c6\uff0c\u4e3b\u8981\u7531\u4e8e\u5904\u7406\u5b9e\u65f6\u66f4\u65b0\u548c\u83b7\u53d6\u53ca\u65f6\u51c6\u786e\u7b54\u6848\u7684\u6311\u6218\u3002", "method": "\u6784\u5efaFutureX\u52a8\u6001\u5b9e\u65f6\u8bc4\u4f30\u57fa\u51c6\uff0c\u652f\u6301\u6bcf\u65e5\u81ea\u52a8\u66f4\u65b0\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u95ee\u9898\u6536\u96c6\u548c\u7b54\u6848\u6536\u96c6\u6d41\u7a0b\u6d88\u9664\u6570\u636e\u6c61\u67d3\uff0c\u8bc4\u4f3025\u4e2aLLM/\u667a\u80fd\u4f53\u6a21\u578b\uff08\u5305\u62ec\u5177\u6709\u63a8\u7406\u3001\u641c\u7d22\u80fd\u529b\u548c\u5916\u90e8\u5de5\u5177\u96c6\u6210\u7684\u6a21\u578b\uff09\u3002", "result": "\u8fdb\u884c\u4e86\u5168\u9762\u7684\u8bc4\u4f30\uff0c\u5206\u6790\u4e86\u667a\u80fd\u4f53\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u81ea\u9002\u5e94\u63a8\u7406\u548c\u6027\u80fd\u8868\u73b0\uff0c\u6df1\u5165\u7814\u7a76\u4e86\u667a\u80fd\u4f53\u5728\u672a\u6765\u5bfc\u5411\u4efb\u52a1\u4e2d\u7684\u5931\u8d25\u6a21\u5f0f\u548c\u6027\u80fd\u7f3a\u9677\uff08\u5305\u62ec\u5bf9\u865a\u5047\u7f51\u9875\u7684\u8106\u5f31\u6027\u548c\u65f6\u95f4\u6709\u6548\u6027\uff09\u3002", "conclusion": "\u76ee\u6807\u662f\u5efa\u7acb\u4e00\u4e2a\u52a8\u6001\u3001\u65e0\u6c61\u67d3\u7684\u8bc4\u4f30\u6807\u51c6\uff0c\u63a8\u52a8LLM\u667a\u80fd\u4f53\u5728\u590d\u6742\u63a8\u7406\u548c\u9884\u6d4b\u601d\u7ef4\u65b9\u9762\u8fbe\u5230\u4e13\u4e1a\u4eba\u7c7b\u5206\u6790\u5e08\u7684\u6c34\u5e73\u3002"}}
{"id": "2508.11960", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.11960", "abs": "https://arxiv.org/abs/2508.11960", "authors": ["Sandeep Kanta", "Mehrdad Tavassoli", "Varun Teja Chirkuri", "Venkata Akhil Kumar", "Santhi Bharath Punati", "Praveen Damacharla", "Sunny Katyara"], "title": "Toward General Physical Intelligence for Resilient Agile Manufacturing Automation", "comment": "Advanced Engineering Informatics", "summary": "Agile and human-centric manufacturing stipulates resilient robotic solutions\ncapable of contextual reasoning and safe interaction in unstructured\nenvironments. Foundation models particularly the Vision Language Action (VLA)\nmodels have emerged to fuse multimodal perception, reasoning and physically\ngrounded action across varied embodiments into unified representation, termed\nas General Physical Intelligence (GPI). While GPI has already been described in\nthe literature but its practical application and evolving role in contemporary\nagile manufacturing processes have yet to be duly explored. To bridge this gap,\nthis practical review systematically surveys recent advancements in VLA models\nwithin GPI context, performs comprehensive comparative analysis of leading\nimplementations and evaluates their readiness for industrial deployment through\nstructured ablation study. Our analysis has organized state-of-the-art into\nfive thematic pillars including multisensory representation learning, sim2real\ntransfer, planning and control, uncertainty and safety measures and\nbenchmarking. Finally, we articulate open research challenges and propose\ndirections to better integrate GPI into next-generation industrial ecosystems\nin line with Industry 5.0.", "AI": {"tldr": "\u672c\u6587\u5bf9\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c(VLA)\u6a21\u578b\u5728\u901a\u7528\u7269\u7406\u667a\u80fd(GPI)\u80cc\u666f\u4e0b\u7684\u6700\u65b0\u8fdb\u5c55\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u7efc\u8ff0\uff0c\u5206\u6790\u4e86\u4e94\u5927\u6280\u672f\u652f\u67f1\uff0c\u8bc4\u4f30\u4e86\u5de5\u4e1a\u90e8\u7f72\u51c6\u5907\u5ea6\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411", "motivation": "\u586b\u8865GPI\u7406\u8bba\u5728\u5f53\u4ee3\u654f\u6377\u5236\u9020\u4e2d\u5b9e\u9645\u5e94\u7528\u7684\u7814\u7a76\u7a7a\u767d\uff0c\u4e3a\u5de5\u4e1a5.0\u65f6\u4ee3\u63d0\u4f9b\u667a\u80fd\u673a\u5668\u4eba\u89e3\u51b3\u65b9\u6848", "method": "\u7cfb\u7edf\u6027\u6587\u732e\u7efc\u8ff0\u3001\u7efc\u5408\u6bd4\u8f83\u5206\u6790\u3001\u7ed3\u6784\u5316\u6d88\u878d\u7814\u7a76\uff0c\u5c06\u73b0\u6709\u6280\u672f\u7ec4\u7ec7\u4e3a\u4e94\u5927\u4e3b\u9898\u652f\u67f1\u8fdb\u884c\u5206\u6790", "result": "\u8bc6\u522b\u4e86VLA\u6a21\u578b\u5728\u5de5\u4e1a\u90e8\u7f72\u4e2d\u7684\u6280\u672f\u6210\u719f\u5ea6\u548c\u6311\u6218\uff0c\u5efa\u7acb\u4e86\u591a\u611f\u5b98\u8868\u5f81\u5b66\u4e60\u3001sim2real\u8fc1\u79fb\u7b49\u5173\u952e\u8bc4\u4f30\u7ef4\u5ea6", "conclusion": "GPI\u5177\u6709\u5de8\u5927\u5de5\u4e1a\u5e94\u7528\u6f5c\u529b\u4f46\u5c1a\u5b58\u6311\u6218\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u66f4\u597d\u5730\u96c6\u6210\u5230\u4e0b\u4e00\u4ee3\u5de5\u4e1a\u751f\u6001\u7cfb\u7edf\u4e2d"}}
{"id": "2508.11991", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.11991", "abs": "https://arxiv.org/abs/2508.11991", "authors": ["Weihao Sun"], "title": "Modeling Relational Logic Circuits for And-Inverter Graph Convolutional Network", "comment": null, "summary": "The automation of logic circuit design enhances chip performance, energy\nefficiency, and reliability, and is widely applied in the field of Electronic\nDesign Automation (EDA).And-Inverter Graphs (AIGs) efficiently represent,\noptimize, and verify the functional characteristics of digital circuits,\nenhancing the efficiency of EDA development.Due to the complex structure and\nlarge scale of nodes in real-world AIGs, accurate modeling is challenging,\nleading to existing work lacking the ability to jointly model functional and\nstructural characteristics, as well as insufficient dynamic information\npropagation capability.To address the aforementioned challenges, we propose\nAIGer.Specifically, AIGer consists of two components: 1) Node logic feature\ninitialization embedding component and 2) AIGs feature learning network\ncomponent.The node logic feature initialization embedding component projects\nlogic nodes, such as AND and NOT, into independent semantic spaces, to enable\neffective node embedding for subsequent processing.Building upon this, the AIGs\nfeature learning network component employs a heterogeneous graph convolutional\nnetwork, designing dynamic relationship weight matrices and differentiated\ninformation aggregation approaches to better represent the original structure\nand information of AIGs.The combination of these two components enhances\nAIGer's ability to jointly model functional and structural characteristics and\nimproves its message passing capability. Experimental results indicate that\nAIGer outperforms the current best models in the Signal Probability Prediction\n(SSP) task, improving MAE and MSE by 18.95\\% and 44.44\\%, respectively. In the\nTruth Table Distance Prediction (TTDP) task, AIGer achieves improvements of\n33.57\\% and 14.79\\% in MAE and MSE, respectively, compared to the\nbest-performing models.", "AI": {"tldr": "AIGer\u662f\u4e00\u4e2a\u7528\u4e8eAnd-Inverter Graphs (AIGs)\u5efa\u6a21\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u8282\u70b9\u903b\u8f91\u7279\u5f81\u521d\u59cb\u5316\u548c\u5f02\u6784\u56fe\u5377\u79ef\u7f51\u7edc\uff0c\u6709\u6548\u8054\u5408\u5efa\u6a21\u529f\u80fd\u4e0e\u7ed3\u6784\u7279\u5f81\uff0c\u5728\u4fe1\u53f7\u6982\u7387\u9884\u6d4b\u548c\u771f\u503c\u8868\u8ddd\u79bb\u9884\u6d4b\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u4f73\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u73b0\u5b9e\u4e16\u754cAIGs\u56e0\u7ed3\u6784\u590d\u6742\u3001\u8282\u70b9\u89c4\u6a21\u5927\u800c\u96be\u4ee5\u51c6\u786e\u5efa\u6a21\u7684\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u529f\u80fd\u4e0e\u7ed3\u6784\u7279\u5f81\u7684\u8054\u5408\u5efa\u6a21\u80fd\u529b\u4ee5\u53ca\u52a8\u6001\u4fe1\u606f\u4f20\u64ad\u80fd\u529b\u4e0d\u8db3\u3002", "method": "1) \u8282\u70b9\u903b\u8f91\u7279\u5f81\u521d\u59cb\u5316\u5d4c\u5165\u7ec4\u4ef6\uff1a\u5c06\u903b\u8f91\u8282\u70b9\u6295\u5f71\u5230\u72ec\u7acb\u8bed\u4e49\u7a7a\u95f4\uff1b2) AIGs\u7279\u5f81\u5b66\u4e60\u7f51\u7edc\u7ec4\u4ef6\uff1a\u4f7f\u7528\u5f02\u6784\u56fe\u5377\u79ef\u7f51\u7edc\uff0c\u8bbe\u8ba1\u52a8\u6001\u5173\u7cfb\u6743\u91cd\u77e9\u9635\u548c\u5dee\u5f02\u5316\u4fe1\u606f\u805a\u5408\u65b9\u6cd5\u3002", "result": "\u5728\u4fe1\u53f7\u6982\u7387\u9884\u6d4b\u4efb\u52a1\u4e2d\uff0cMAE\u548cMSE\u5206\u522b\u63d0\u534718.95%\u548c44.44%\uff1b\u5728\u771f\u503c\u8868\u8ddd\u79bb\u9884\u6d4b\u4efb\u52a1\u4e2d\uff0cMAE\u548cMSE\u5206\u522b\u63d0\u534733.57%\u548c14.79%\u3002", "conclusion": "AIGer\u901a\u8fc7\u521b\u65b0\u7684\u8282\u70b9\u5d4c\u5165\u548c\u5f02\u6784\u56fe\u5b66\u4e60\u65b9\u6cd5\uff0c\u6210\u529f\u89e3\u51b3\u4e86AIGs\u5efa\u6a21\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5728EDA\u9886\u57df\u5c55\u73b0\u51fa\u4f18\u5f02\u7684\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2508.12038", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.12038", "abs": "https://arxiv.org/abs/2508.12038", "authors": ["Liwen Zhang", "Heng Deng", "Guanghui Sun"], "title": "Fully Spiking Actor-Critic Neural Network for Robotic Manipulation", "comment": null, "summary": "This study proposes a hybrid curriculum reinforcement learning (CRL)\nframework based on a fully spiking neural network (SNN) for 9-degree-of-freedom\nrobotic arms performing target reaching and grasping tasks. To reduce network\ncomplexity and inference latency, the SNN architecture is simplified to include\nonly an input and an output layer, which shows strong potential for\nresource-constrained environments. Building on the advantages of SNNs-high\ninference speed, low energy consumption, and spike-based biological\nplausibility, a temporal progress-partitioned curriculum strategy is integrated\nwith the Proximal Policy Optimization (PPO) algorithm. Meanwhile, an energy\nconsumption modeling framework is introduced to quantitatively compare the\ntheoretical energy consumption between SNNs and conventional Artificial Neural\nNetworks (ANNs). A dynamic two-stage reward adjustment mechanism and optimized\nobservation space further improve learning efficiency and policy accuracy.\nExperiments on the Isaac Gym simulation platform demonstrate that the proposed\nmethod achieves superior performance under realistic physical constraints.\nComparative evaluations with conventional PPO and ANN baselines validate the\nscalability and energy efficiency of the proposed approach in dynamic robotic\nmanipulation tasks.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5168\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\u7684\u6df7\u5408\u8bfe\u7a0b\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e9\u81ea\u7531\u5ea6\u673a\u68b0\u81c2\u7684\u76ee\u6807\u6293\u53d6\u4efb\u52a1\uff0c\u7b80\u5316\u7f51\u7edc\u7ed3\u6784\u964d\u4f4e\u5ef6\u8fdf\uff0c\u96c6\u6210\u65f6\u95f4\u8fdb\u5ea6\u5206\u533a\u8bfe\u7a0b\u7b56\u7565\u548cPPO\u7b97\u6cd5\uff0c\u5728\u80fd\u8017\u548c\u6027\u80fd\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u7684\u9ad8\u6548\u5b66\u4e60\u95ee\u9898\uff0c\u5229\u7528SNN\u7684\u9ad8\u63a8\u7406\u901f\u5ea6\u3001\u4f4e\u80fd\u8017\u548c\u751f\u7269\u5408\u7406\u6027\u4f18\u52bf\uff0c\u540c\u65f6\u901a\u8fc7\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\u63d0\u9ad8\u5b66\u4e60\u6548\u7387", "method": "\u91c7\u7528\u7b80\u5316\u7684\u5168SNN\u67b6\u6784\uff08\u4ec5\u8f93\u5165\u8f93\u51fa\u5c42\uff09\uff0c\u96c6\u6210\u65f6\u95f4\u8fdb\u5ea6\u5206\u533a\u8bfe\u7a0b\u7b56\u7565\u4e0ePPO\u7b97\u6cd5\uff0c\u5f15\u5165\u80fd\u8017\u5efa\u6a21\u6846\u67b6\uff0c\u91c7\u7528\u52a8\u6001\u4e24\u9636\u6bb5\u5956\u52b1\u8c03\u6574\u673a\u5236\u548c\u4f18\u5316\u89c2\u6d4b\u7a7a\u95f4", "result": "\u5728Isaac Gym\u4eff\u771f\u5e73\u53f0\u4e0a\u9a8c\u8bc1\u4e86\u4f18\u8d8a\u6027\u80fd\uff0c\u76f8\u6bd4\u4f20\u7edfPPO\u548cANN\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u52a8\u6001\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u66f4\u597d\u7684\u53ef\u6269\u5c55\u6027\u548c\u80fd\u6e90\u6548\u7387", "conclusion": "\u8be5\u6df7\u5408CRL\u6846\u67b6\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0cSNN\u7684\u7b80\u5316\u67b6\u6784\u548c\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\u7684\u7ed3\u5408\u663e\u8457\u63d0\u5347\u4e86\u5b66\u4e60\u6548\u7387\u548c\u80fd\u6e90\u6548\u7387"}}
{"id": "2508.11995", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.11995", "abs": "https://arxiv.org/abs/2508.11995", "authors": ["Xuyang Zhao", "Shiwan Zhao", "Hualong Yu", "Liting Zhang", "Qicheng Li"], "title": "AgentCDM: Enhancing Multi-Agent Collaborative Decision-Making via ACH-Inspired Structured Reasoning", "comment": null, "summary": "Multi-agent systems (MAS) powered by large language models (LLMs) hold\nsignificant promise for solving complex decision-making tasks. However, the\ncore process of collaborative decision-making (CDM) within these systems\nremains underexplored. Existing approaches often rely on either ``dictatorial\"\nstrategies that are vulnerable to the cognitive biases of a single agent, or\n``voting-based\" methods that fail to fully harness collective intelligence. To\naddress these limitations, we propose \\textbf{AgentCDM}, a structured framework\nfor enhancing collaborative decision-making in LLM-based multi-agent systems.\nDrawing inspiration from the Analysis of Competing Hypotheses (ACH) in\ncognitive science, AgentCDM introduces a structured reasoning paradigm that\nsystematically mitigates cognitive biases and shifts decision-making from\npassive answer selection to active hypothesis evaluation and construction. To\ninternalize this reasoning process, we develop a two-stage training paradigm:\nthe first stage uses explicit ACH-inspired scaffolding to guide the model\nthrough structured reasoning, while the second stage progressively removes this\nscaffolding to encourage autonomous generalization. Experiments on multiple\nbenchmark datasets demonstrate that AgentCDM achieves state-of-the-art\nperformance and exhibits strong generalization, validating its effectiveness in\nimproving the quality and robustness of collaborative decisions in MAS.", "AI": {"tldr": "AgentCDM\u662f\u4e00\u4e2a\u57fa\u4e8e\u8ba4\u77e5\u79d1\u5b66\u4e2d\u7ade\u4e89\u5047\u8bbe\u5206\u6790(ACH)\u7684\u7ed3\u6784\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347LLM\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u534f\u4f5c\u51b3\u7b56\u8d28\u91cf\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u6709\u6548\u51cf\u8f7b\u8ba4\u77e5\u504f\u89c1\u5e76\u5b9e\u73b0\u4e3b\u52a8\u5047\u8bbe\u8bc4\u4f30\u3002", "motivation": "\u73b0\u6709\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u534f\u4f5c\u51b3\u7b56\u65b9\u6cd5\u5b58\u5728\u7f3a\u9677\uff1a\u8981\u4e48\u4f9d\u8d56\u5355\u4e2a\u667a\u80fd\u4f53\u7684'\u72ec\u88c1'\u7b56\u7565\uff08\u6613\u53d7\u8ba4\u77e5\u504f\u89c1\u5f71\u54cd\uff09\uff0c\u8981\u4e48\u91c7\u7528'\u6295\u7968'\u65b9\u6cd5\uff08\u65e0\u6cd5\u5145\u5206\u5229\u7528\u96c6\u4f53\u667a\u6167\uff09\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u534f\u4f5c\u51b3\u7b56\u6846\u67b6\u3002", "method": "\u63d0\u51faAgentCDM\u6846\u67b6\uff0c\u501f\u9274\u8ba4\u77e5\u79d1\u5b66\u4e2d\u7684\u7ade\u4e89\u5047\u8bbe\u5206\u6790(ACH)\u65b9\u6cd5\uff0c\u91c7\u7528\u7ed3\u6784\u5316\u63a8\u7406\u8303\u5f0f\u3002\u4f7f\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528ACH\u542f\u53d1\u7684\u663e\u5f0f\u652f\u67b6\u6307\u5bfc\u6a21\u578b\u8fdb\u884c\u7ed3\u6784\u5316\u63a8\u7406\uff0c\u7b2c\u4e8c\u9636\u6bb5\u9010\u6b65\u79fb\u9664\u652f\u67b6\u4ee5\u4fc3\u8fdb\u81ea\u4e3b\u6cdb\u5316\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cAgentCDM\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u63d0\u9ad8\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u534f\u4f5c\u51b3\u7b56\u8d28\u91cf\u548c\u9c81\u68d2\u6027\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "AgentCDM\u901a\u8fc7\u7ed3\u6784\u5316\u63a8\u7406\u8303\u5f0f\u548c\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u65b9\u6cd5\uff0c\u6210\u529f\u89e3\u51b3\u4e86LLM\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u534f\u4f5c\u51b3\u7b56\u7684\u6311\u6218\uff0c\u4e3a\u63d0\u5347\u96c6\u4f53\u51b3\u7b56\u8d28\u91cf\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.12043", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.12043", "abs": "https://arxiv.org/abs/2508.12043", "authors": ["Fei Lin", "Tengchao Zhang", "Qinghua Ni", "Jun Huang", "Siji Ma", "Yonglin Tian", "Yisheng Lv", "Naiqi Wu"], "title": "Talk Less, Fly Lighter: Autonomous Semantic Compression for UAV Swarm Communication via LLMs", "comment": null, "summary": "The rapid adoption of Large Language Models (LLMs) in unmanned systems has\nsignificantly enhanced the semantic understanding and autonomous task execution\ncapabilities of Unmanned Aerial Vehicle (UAV) swarms. However, limited\ncommunication bandwidth and the need for high-frequency interactions pose\nsevere challenges to semantic information transmission within the swarm. This\npaper explores the feasibility of LLM-driven UAV swarms for autonomous semantic\ncompression communication, aiming to reduce communication load while preserving\ncritical task semantics. To this end, we construct four types of 2D simulation\nscenarios with different levels of environmental complexity and design a\ncommunication-execution pipeline that integrates system prompts with task\ninstruction prompts. On this basis, we systematically evaluate the semantic\ncompression performance of nine mainstream LLMs in different scenarios and\nanalyze their adaptability and stability through ablation studies on\nenvironmental complexity and swarm size. Experimental results demonstrate that\nLLM-based UAV swarms have the potential to achieve efficient collaborative\ncommunication under bandwidth-constrained and multi-hop link conditions.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86LLM\u9a71\u52a8\u7684\u65e0\u4eba\u673a\u7fa4\u5728\u5e26\u5bbd\u53d7\u9650\u6761\u4ef6\u4e0b\u5b9e\u73b0\u8bed\u4e49\u538b\u7f29\u901a\u4fe1\u7684\u53ef\u884c\u6027\uff0c\u901a\u8fc7\u56db\u79cd2D\u4eff\u771f\u573a\u666f\u9a8c\u8bc1\u4e86\u4e5d\u79cd\u4e3b\u6d41LLM\u7684\u8bed\u4e49\u538b\u7f29\u6027\u80fd\u3002", "motivation": "\u65e0\u4eba\u673a\u7fa4\u91c7\u7528\u5927\u8bed\u8a00\u6a21\u578b\u589e\u5f3a\u4e86\u8bed\u4e49\u7406\u89e3\u548c\u81ea\u4e3b\u4efb\u52a1\u6267\u884c\u80fd\u529b\uff0c\u4f46\u6709\u9650\u901a\u4fe1\u5e26\u5bbd\u548c\u9ad8\u9891\u4ea4\u4e92\u9700\u6c42\u5bf9\u8bed\u4e49\u4fe1\u606f\u4f20\u8f93\u63d0\u51fa\u4e86\u4e25\u5cfb\u6311\u6218\u3002", "method": "\u6784\u5efa\u4e86\u56db\u79cd\u4e0d\u540c\u73af\u5883\u590d\u6742\u5ea6\u76842D\u4eff\u771f\u573a\u666f\uff0c\u8bbe\u8ba1\u4e86\u7cfb\u7edf\u63d0\u793a\u4e0e\u4efb\u52a1\u6307\u4ee4\u63d0\u793a\u76f8\u7ed3\u5408\u7684\u901a\u4fe1-\u6267\u884c\u6d41\u6c34\u7ebf\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e86\u4e5d\u79cd\u4e3b\u6d41LLM\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u7684\u8bed\u4e49\u538b\u7f29\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8eLLM\u7684\u65e0\u4eba\u673a\u7fa4\u6709\u6f5c\u529b\u5728\u5e26\u5bbd\u53d7\u9650\u548c\u591a\u8df3\u94fe\u8def\u6761\u4ef6\u4e0b\u5b9e\u73b0\u9ad8\u6548\u7684\u534f\u540c\u901a\u4fe1\u3002", "conclusion": "LLM\u9a71\u52a8\u7684\u65e0\u4eba\u673a\u7fa4\u80fd\u591f\u901a\u8fc7\u8bed\u4e49\u538b\u7f29\u6709\u6548\u51cf\u5c11\u901a\u4fe1\u8d1f\u8f7d\uff0c\u540c\u65f6\u4fdd\u6301\u5173\u952e\u4efb\u52a1\u8bed\u4e49\uff0c\u5728\u590d\u6742\u73af\u5883\u4e0b\u5c55\u73b0\u51fa\u826f\u597d\u7684\u9002\u5e94\u6027\u548c\u7a33\u5b9a\u6027\u3002"}}
{"id": "2508.12022", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12022", "abs": "https://arxiv.org/abs/2508.12022", "authors": ["Dorsa Macky Aleagha", "Payam Zohari", "Mostafa Haghir Chehreghani"], "title": "AI Models for Depressive Disorder Detection and Diagnosis: A Review", "comment": null, "summary": "Major Depressive Disorder is one of the leading causes of disability\nworldwide, yet its diagnosis still depends largely on subjective clinical\nassessments. Integrating Artificial Intelligence (AI) holds promise for\ndeveloping objective, scalable, and timely diagnostic tools. In this paper, we\npresent a comprehensive survey of state-of-the-art AI methods for depression\ndetection and diagnosis, based on a systematic review of 55 key studies. We\nintroduce a novel hierarchical taxonomy that structures the field by primary\nclinical task (diagnosis vs. prediction), data modality (text, speech,\nneuroimaging, multimodal), and computational model class (e.g., graph neural\nnetworks, large language models, hybrid approaches). Our in-depth analysis\nreveals three major trends: the predominance of graph neural networks for\nmodeling brain connectivity, the rise of large language models for linguistic\nand conversational data, and an emerging focus on multimodal fusion,\nexplainability, and algorithmic fairness. Alongside methodological insights, we\nprovide an overview of prominent public datasets and standard evaluation\nmetrics as a practical guide for researchers. By synthesizing current advances\nand highlighting open challenges, this survey offers a comprehensive roadmap\nfor future innovation in computational psychiatry.", "AI": {"tldr": "\u672c\u6587\u5bf955\u9879\u5173\u952e\u7814\u7a76\u8fdb\u884c\u7cfb\u7edf\u7efc\u8ff0\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u4e34\u5e8a\u4efb\u52a1\u3001\u6570\u636e\u6a21\u6001\u548c\u8ba1\u7b97\u6a21\u578b\u7684\u5206\u5c42\u5206\u7c7b\u6cd5\uff0c\u603b\u7ed3\u4e86\u6291\u90c1\u75c7AI\u8bca\u65ad\u9886\u57df\u7684\u4e3b\u8981\u8d8b\u52bf\u548c\u6311\u6218\u3002", "motivation": "\u6291\u90c1\u75c7\u662f\u5168\u7403\u4e3b\u8981\u81f4\u6b8b\u539f\u56e0\uff0c\u4f46\u8bca\u65ad\u4ecd\u4f9d\u8d56\u4e3b\u89c2\u4e34\u5e8a\u8bc4\u4f30\u3002AI\u6280\u672f\u6709\u671b\u5f00\u53d1\u5ba2\u89c2\u3001\u53ef\u6269\u5c55\u548c\u53ca\u65f6\u7684\u8bca\u65ad\u5de5\u5177\uff0c\u9700\u8981\u7cfb\u7edf\u68b3\u7406\u5f53\u524d\u7814\u7a76\u8fdb\u5c55\u3002", "method": "\u901a\u8fc7\u5bf955\u9879\u5173\u952e\u7814\u7a76\u7684\u7cfb\u7edf\u7efc\u8ff0\uff0c\u5efa\u7acb\u4e86\u5206\u5c42\u5206\u7c7b\u6cd5\uff08\u4e34\u5e8a\u4efb\u52a1\u00d7\u6570\u636e\u6a21\u6001\u00d7\u8ba1\u7b97\u6a21\u578b\uff09\uff0c\u5206\u6790\u4e3b\u8981\u6280\u672f\u8d8b\u52bf\u548c\u6570\u636e\u96c6\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u53d1\u73b0\u4e09\u5927\u8d8b\u52bf\uff1a\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u8111\u8fde\u63a5\u5efa\u6a21\u4e2d\u7684\u4e3b\u5bfc\u5730\u4f4d\u3001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8bed\u8a00\u6570\u636e\u5904\u7406\u4e2d\u7684\u5174\u8d77\uff0c\u4ee5\u53ca\u591a\u6a21\u6001\u878d\u5408\u3001\u53ef\u89e3\u91ca\u6027\u548c\u7b97\u6cd5\u516c\u5e73\u6027\u7684\u65b0\u5174\u5173\u6ce8\u3002", "conclusion": "\u672c\u7efc\u8ff0\u4e3a\u8ba1\u7b97\u7cbe\u795e\u75c5\u5b66\u9886\u57df\u7684\u672a\u6765\u521b\u65b0\u63d0\u4f9b\u4e86\u5168\u9762\u8def\u7ebf\u56fe\uff0c\u5f3a\u8c03\u4e86\u591a\u6a21\u6001\u878d\u5408\u3001\u53ef\u89e3\u91ca\u6027\u548c\u516c\u5e73\u6027\u7b49\u5f00\u653e\u6311\u6218\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2508.12071", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.12071", "abs": "https://arxiv.org/abs/2508.12071", "authors": ["Amy Phung", "Richard Camilli"], "title": "OASIS: Real-Time Opti-Acoustic Sensing for Intervention Systems in Unstructured Environments", "comment": "This paper has been accepted for publication in IROS 2025. Copyright\n  IEEE", "summary": "High resolution underwater 3D scene reconstruction is crucial for various\napplications, including construction, infrastructure maintenance, monitoring,\nexploration, and scientific investigation. Prior work has leveraged the\ncomplementary sensing modalities of imaging sonars and optical cameras for\nopti-acoustic 3D scene reconstruction, demonstrating improved results over\nmethods which rely solely on either sensor. However, while most existing\napproaches focus on offline reconstruction, real-time spatial awareness is\nessential for both autonomous and piloted underwater vehicle operations. This\npaper presents OASIS, an opti-acoustic fusion method that integrates data from\noptical images with voxel carving techniques to achieve real-time 3D\nreconstruction unstructured underwater workspaces. Our approach utilizes an\n\"eye-in-hand\" configuration, which leverages the dexterity of robotic\nmanipulator arms to capture multiple workspace views across a short baseline.\nWe validate OASIS through tank-based experiments and present qualitative and\nquantitative results that highlight its utility for underwater manipulation\ntasks.", "AI": {"tldr": "OASIS\u662f\u4e00\u79cd\u5b9e\u65f6\u6c34\u4e0b3D\u91cd\u5efa\u65b9\u6cd5\uff0c\u901a\u8fc7\u878d\u5408\u5149\u5b66\u76f8\u673a\u548c\u6210\u50cf\u58f0\u7eb3\u6570\u636e\uff0c\u7ed3\u5408\u4f53\u7d20\u96d5\u523b\u6280\u672f\uff0c\u5b9e\u73b0\u975e\u7ed3\u6784\u5316\u6c34\u4e0b\u5de5\u4f5c\u7a7a\u95f4\u7684\u5b9e\u65f6\u4e09\u7ef4\u91cd\u5efa", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u79bb\u7ebf\u91cd\u5efa\uff0c\u800c\u5b9e\u65f6\u7a7a\u95f4\u611f\u77e5\u5bf9\u4e8e\u81ea\u4e3b\u548c\u6709\u4eba\u9a7e\u9a76\u6c34\u4e0b\u8f66\u8f86\u64cd\u4f5c\u81f3\u5173\u91cd\u8981\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5b9e\u65f6\u5904\u7406\u5149\u5b66\u548c\u58f0\u5b66\u6570\u636e\u7684\u878d\u5408\u65b9\u6cd5", "method": "\u91c7\u7528\"\u624b\u773c\"\u914d\u7f6e\uff0c\u5229\u7528\u673a\u68b0\u81c2\u7075\u6d3b\u6027\u5728\u77ed\u57fa\u7ebf\u4e0a\u6355\u83b7\u591a\u4e2a\u5de5\u4f5c\u7a7a\u95f4\u89c6\u56fe\u3002\u7ed3\u5408\u5149\u5b66\u56fe\u50cf\u548c\u4f53\u7d20\u96d5\u523b\u6280\u672f\u8fdb\u884c\u5b9e\u65f63D\u91cd\u5efa", "result": "\u901a\u8fc7\u6c34\u7bb1\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u63d0\u4f9b\u4e86\u5b9a\u6027\u548c\u5b9a\u91cf\u7ed3\u679c\uff0c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u6c34\u4e0b\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u5b9e\u7528\u6027", "conclusion": "OASIS\u65b9\u6cd5\u6210\u529f\u5b9e\u73b0\u4e86\u5b9e\u65f6\u6c34\u4e0b3D\u91cd\u5efa\uff0c\u4e3a\u6c34\u4e0b\u64cd\u4f5c\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u7a7a\u95f4\u611f\u77e5\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.12026", "categories": ["cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12026", "abs": "https://arxiv.org/abs/2508.12026", "authors": ["Szymon Pawlonka", "Miko\u0142aj Ma\u0142ki\u0144ski", "Jacek Ma\u0144dziuk"], "title": "Bongard-RWR+: Real-World Representations of Fine-Grained Concepts in Bongard Problems", "comment": null, "summary": "Bongard Problems (BPs) provide a challenging testbed for abstract visual\nreasoning (AVR), requiring models to identify visual concepts fromjust a few\nexamples and describe them in natural language. Early BP benchmarks featured\nsynthetic black-and-white drawings, which might not fully capture the\ncomplexity of real-world scenes. Subsequent BP datasets employed real-world\nimages, albeit the represented concepts are identifiable from high-level image\nfeatures, reducing the task complexity. Differently, the recently released\nBongard-RWR dataset aimed at representing abstract concepts formulated in the\noriginal BPs using fine-grained real-world images. Its manual construction,\nhowever, limited the dataset size to just $60$ instances, constraining\nevaluation robustness. In this work, we introduce Bongard-RWR+, a BP dataset\ncomposed of $5\\,400$ instances that represent original BP abstract concepts\nusing real-world-like images generated via a vision language model (VLM)\npipeline. Building on Bongard-RWR, we employ Pixtral-12B to describe manually\ncurated images and generate new descriptions aligned with the underlying\nconcepts, use Flux.1-dev to synthesize images from these descriptions, and\nmanually verify that the generated images faithfully reflect the intended\nconcepts. We evaluate state-of-the-art VLMs across diverse BP formulations,\nincluding binary and multiclass classification, as well as textual answer\ngeneration. Our findings reveal that while VLMs can recognize coarse-grained\nvisual concepts, they consistently struggle with discerning fine-grained\nconcepts, highlighting limitations in their reasoning capabilities.", "AI": {"tldr": "Bongard-RWR+\u662f\u4e00\u4e2a\u5305\u542b5400\u4e2a\u5b9e\u4f8b\u7684\u62bd\u8c61\u89c6\u89c9\u63a8\u7406\u6570\u636e\u96c6\uff0c\u4f7f\u7528VLM\u751f\u6210\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u6765\u4ee3\u8868\u539f\u59cbBongard\u95ee\u9898\u7684\u62bd\u8c61\u6982\u5ff5\uff0c\u8bc4\u4f30\u663e\u793aVLMs\u5728\u7ec6\u7c92\u5ea6\u6982\u5ff5\u8bc6\u522b\u4e0a\u5b58\u5728\u56f0\u96be", "motivation": "\u73b0\u6709\u7684Bongard\u95ee\u9898\u6570\u636e\u96c6\u8981\u4e48\u4f7f\u7528\u5408\u6210\u56fe\u50cf\u4e0d\u80fd\u5b8c\u5168\u6355\u6349\u771f\u5b9e\u4e16\u754c\u590d\u6742\u6027\uff0c\u8981\u4e48\u4f7f\u7528\u771f\u5b9e\u56fe\u50cf\u4f46\u6982\u5ff5\u53ef\u4ece\u9ad8\u5c42\u7279\u5f81\u8bc6\u522b\u964d\u4f4e\u4e86\u4efb\u52a1\u96be\u5ea6\uff0c\u800cBongard-RWR\u6570\u636e\u96c6\u867d\u4f7f\u7528\u7ec6\u7c92\u5ea6\u771f\u5b9e\u56fe\u50cf\u4f46\u89c4\u6a21\u592a\u5c0f\uff08\u4ec560\u4e2a\u5b9e\u4f8b\uff09", "method": "\u57fa\u4e8eBongard-RWR\uff0c\u4f7f\u7528Pixtral-12B\u63cf\u8ff0\u624b\u52a8\u7b56\u5212\u7684\u56fe\u50cf\u5e76\u751f\u6210\u4e0e\u5e95\u5c42\u6982\u5ff5\u5bf9\u9f50\u7684\u65b0\u63cf\u8ff0\uff0c\u4f7f\u7528Flux.1-dev\u4ece\u8fd9\u4e9b\u63cf\u8ff0\u5408\u6210\u56fe\u50cf\uff0c\u5e76\u624b\u52a8\u9a8c\u8bc1\u751f\u6210\u56fe\u50cf\u662f\u5426\u5fe0\u5b9e\u53cd\u6620\u9884\u671f\u6982\u5ff5", "result": "\u8bc4\u4f30\u6700\u5148\u8fdbVLMs\u5728\u591a\u79cdBongard\u95ee\u9898\u8868\u8ff0\uff08\u5305\u62ec\u4e8c\u5143\u548c\u591a\u7c7b\u5206\u7c7b\u4ee5\u53ca\u6587\u672c\u7b54\u6848\u751f\u6210\uff09\u4e0a\u7684\u8868\u73b0\uff0c\u53d1\u73b0VLMs\u80fd\u8bc6\u522b\u7c97\u7c92\u5ea6\u89c6\u89c9\u6982\u5ff5\uff0c\u4f46\u5728\u8fa8\u522b\u7ec6\u7c92\u5ea6\u6982\u5ff5\u65b9\u9762\u6301\u7eed\u56f0\u96be", "conclusion": "VLMs\u5728\u62bd\u8c61\u89c6\u89c9\u63a8\u7406\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5728\u7ec6\u7c92\u5ea6\u6982\u5ff5\u8bc6\u522b\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u7a81\u663e\u4e86\u5176\u63a8\u7406\u80fd\u529b\u7684\u4e0d\u8db3"}}
{"id": "2508.12075", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.12075", "abs": "https://arxiv.org/abs/2508.12075", "authors": ["Shaul Ashkenazi", "Gabriel Skantze", "Jane Stuart-Smith", "Mary Ellen Foster"], "title": "Into the Wild: When Robots Are Not Welcome", "comment": "Accepted at the workshop on Real-World HRI in Public and Private\n  Spaces: Successes, Failures, and Lessons Learned (PubRob-Fails), held at the\n  IEEE RO-MAN Conference, 2025 (paper PubRob-Fails/2025/4)", "summary": "Social robots are increasingly being deployed in public spaces, where they\nface not only technological difficulties and unexpected user utterances, but\nalso objections from stakeholders who may not be comfortable with introducing a\nrobot into those spaces. We describe our difficulties with deploying a social\nrobot in two different public settings: 1) Student services center; 2) Refugees\nand asylum seekers drop-in service. Although this is a failure report, in each\nuse case we eventually managed to earn the trust of the staff and form a\nrelationship with them, allowing us to deploy our robot and conduct our\nstudies.", "AI": {"tldr": "\u672c\u6587\u62a5\u544a\u4e86\u5728\u516c\u5171\u670d\u52a1\u573a\u6240\u90e8\u7f72\u793e\u4ea4\u673a\u5668\u4eba\u7684\u5931\u8d25\u7ecf\u5386\uff0c\u4f46\u6700\u7ec8\u901a\u8fc7\u4e0e\u5de5\u4f5c\u4eba\u5458\u5efa\u7acb\u4fe1\u4efb\u5173\u7cfb\u6210\u529f\u5b8c\u6210\u4e86\u90e8\u7f72\u7814\u7a76", "motivation": "\u7814\u7a76\u793e\u4ea4\u673a\u5668\u4eba\u5728\u516c\u5171\u573a\u6240\u90e8\u7f72\u65f6\u9762\u4e34\u7684\u6280\u672f\u6311\u6218\u3001\u7528\u6237\u610f\u5916\u4e92\u52a8\u4ee5\u53ca\u5229\u76ca\u76f8\u5173\u8005\u53cd\u5bf9\u7b49\u5b9e\u9645\u56f0\u96be", "method": "\u5728\u4e24\u4e2a\u4e0d\u540c\u516c\u5171\u670d\u52a1\u573a\u6240\uff08\u5b66\u751f\u670d\u52a1\u4e2d\u5fc3\u3001\u96be\u6c11\u548c\u5bfb\u6c42\u5e87\u62a4\u8005\u670d\u52a1\u4e2d\u5fc3\uff09\u8fdb\u884c\u793e\u4ea4\u673a\u5668\u4eba\u90e8\u7f72\u5b9e\u9a8c\uff0c\u8bb0\u5f55\u9047\u5230\u7684\u56f0\u96be\u5e76\u91c7\u53d6\u5efa\u7acb\u4fe1\u4efb\u5173\u7cfb\u7684\u7b56\u7565", "result": "\u867d\u7136\u6700\u521d\u906d\u9047\u5931\u8d25\uff0c\u4f46\u901a\u8fc7\u4e0e\u5de5\u4f5c\u4eba\u5458\u5efa\u7acb\u4fe1\u4efb\u5173\u7cfb\uff0c\u6700\u7ec8\u6210\u529f\u90e8\u7f72\u673a\u5668\u4eba\u5e76\u5b8c\u6210\u4e86\u7814\u7a76", "conclusion": "\u5728\u516c\u5171\u573a\u6240\u90e8\u7f72\u793e\u4ea4\u673a\u5668\u4eba\u65f6\uff0c\u5efa\u7acb\u4e0e\u5de5\u4f5c\u4eba\u5458\u7684\u4fe1\u4efb\u5173\u7cfb\u662f\u514b\u670d\u90e8\u7f72\u969c\u788d\u7684\u5173\u952e\u56e0\u7d20"}}
{"id": "2508.12027", "categories": ["cs.AI", "cs.LG", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2508.12027", "abs": "https://arxiv.org/abs/2508.12027", "authors": ["Filippo Torresan", "Keisuke Suzuki", "Ryota Kanai", "Manuel Baltieri"], "title": "Active inference for action-unaware agents", "comment": "59 pages, 47 figures", "summary": "Active inference is a formal approach to study cognition based on the notion\nthat adaptive agents can be seen as engaging in a process of approximate\nBayesian inference, via the minimisation of variational and expected free\nenergies. Minimising the former provides an account of perceptual processes and\nlearning as evidence accumulation, while minimising the latter describes how\nagents select their actions over time. In this way, adaptive agents are able to\nmaximise the likelihood of preferred observations or states, given a generative\nmodel of the environment. In the literature, however, different strategies have\nbeen proposed to describe how agents can plan their future actions. While they\nall share the notion that some kind of expected free energy offers an\nappropriate way to score policies, sequences of actions, in terms of their\ndesirability, there are different ways to consider the contribution of past\nmotor experience to the agent's future behaviour. In some approaches, agents\nare assumed to know their own actions, and use such knowledge to better plan\nfor the future. In other approaches, agents are unaware of their actions, and\nmust infer their motor behaviour from recent observations in order to plan for\nthe future. This difference reflects a standard point of departure in two\nleading frameworks in motor control based on the presence, or not, of an\nefference copy signal representing knowledge about an agent's own actions. In\nthis work we compare the performances of action-aware and action-unaware agents\nin two navigations tasks, showing how action-unaware agents can achieve\nperformances comparable to action-aware ones while at a severe disadvantage.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u4e3b\u52a8\u63a8\u7406\u4e2d\u52a8\u4f5c\u611f\u77e5\u4e0e\u52a8\u4f5c\u65e0\u611f\u77e5\u667a\u80fd\u4f53\u5728\u5bfc\u822a\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u8868\u73b0\uff0c\u53d1\u73b0\u52a8\u4f5c\u65e0\u611f\u77e5\u667a\u80fd\u4f53\u867d\u7136\u5904\u4e8e\u4e25\u91cd\u52a3\u52bf\u4f46\u4ecd\u80fd\u8fbe\u5230\u4e0e\u52a8\u4f5c\u611f\u77e5\u667a\u80fd\u4f53\u76f8\u5f53\u7684\u6027\u80fd\u6c34\u5e73\u3002", "motivation": "\u4e3b\u52a8\u63a8\u7406\u6846\u67b6\u4e2d\uff0c\u4e0d\u540c\u7b56\u7565\u5bf9\u667a\u80fd\u4f53\u5982\u4f55\u89c4\u5212\u672a\u6765\u884c\u52a8\u5b58\u5728\u5206\u6b67\uff0c\u7279\u522b\u662f\u5728\u662f\u5426\u5229\u7528\u81ea\u8eab\u52a8\u4f5c\u77e5\u8bc6\uff08efference copy\u4fe1\u53f7\uff09\u65b9\u9762\u5b58\u5728\u5dee\u5f02\uff0c\u9700\u8981\u6bd4\u8f83\u8fd9\u4e24\u79cd\u65b9\u6cd5\u7684\u5b9e\u9645\u6027\u80fd\u8868\u73b0\u3002", "method": "\u901a\u8fc7\u4e24\u4e2a\u5bfc\u822a\u4efb\u52a1\u5bf9\u6bd4\u52a8\u4f5c\u611f\u77e5\uff08\u77e5\u9053\u81ea\u8eab\u52a8\u4f5c\uff09\u548c\u52a8\u4f5c\u65e0\u611f\u77e5\uff08\u9700\u8981\u4ece\u89c2\u6d4b\u63a8\u65ad\u52a8\u4f5c\uff09\u667a\u80fd\u4f53\u7684\u6027\u80fd\u8868\u73b0\uff0c\u5206\u6790\u5b83\u4eec\u5728\u4e3b\u52a8\u63a8\u7406\u6846\u67b6\u4e0b\u7684\u884c\u4e3a\u5dee\u5f02\u3002", "result": "\u52a8\u4f5c\u65e0\u611f\u77e5\u667a\u80fd\u4f53\u867d\u7136\u5904\u4e8e\u4e25\u91cd\u52a3\u52bf\uff0c\u4f46\u5728\u5bfc\u822a\u4efb\u52a1\u4e2d\u80fd\u591f\u8fbe\u5230\u4e0e\u52a8\u4f5c\u611f\u77e5\u667a\u80fd\u4f53\u76f8\u5f53\u7684\u6027\u80fd\u6c34\u5e73\u3002", "conclusion": "\u5373\u4f7f\u6ca1\u6709\u52a8\u4f5c\u77e5\u8bc6\uff0c\u667a\u80fd\u4f53\u4ecd\u80fd\u901a\u8fc7\u4ece\u89c2\u6d4b\u4e2d\u63a8\u65ad\u52a8\u4f5c\u6765\u5b9e\u73b0\u6709\u6548\u7684\u89c4\u5212\u548c\u63a7\u5236\uff0c\u8fd9\u4e3a\u7406\u89e3\u8fd0\u52a8\u63a7\u5236\u673a\u5236\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2508.12166", "categories": ["cs.RO", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.12166", "abs": "https://arxiv.org/abs/2508.12166", "authors": ["Gokul Puthumanaillam", "Aditya Penumarti", "Manav Vora", "Paulo Padrao", "Jose Fuentes", "Leonardo Bobadilla", "Jane Shin", "Melkior Ornik"], "title": "Belief-Conditioned One-Step Diffusion: Real-Time Trajectory Planning with Just-Enough Sensing", "comment": "Accepted to CoRL 2025 (Conference on Robot Learning)", "summary": "Robots equipped with rich sensor suites can localize reliably in\npartially-observable environments, but powering every sensor continuously is\nwasteful and often infeasible. Belief-space planners address this by\npropagating pose-belief covariance through analytic models and switching\nsensors heuristically--a brittle, runtime-expensive approach. Data-driven\napproaches--including diffusion models--learn multi-modal trajectories from\ndemonstrations, but presuppose an accurate, always-on state estimate. We\naddress the largely open problem: for a given task in a mapped environment,\nwhich \\textit{minimal sensor subset} must be active at each location to\nmaintain state uncertainty \\textit{just low enough} to complete the task? Our\nkey insight is that when a diffusion planner is explicitly conditioned on a\npose-belief raster and a sensor mask, the spread of its denoising trajectories\nyields a calibrated, differentiable proxy for the expected localisation error.\nBuilding on this insight, we present Belief-Conditioned One-Step Diffusion\n(B-COD), the first planner that, in a 10 ms forward pass, returns a\nshort-horizon trajectory, per-waypoint aleatoric variances, and a proxy for\nlocalisation error--eliminating external covariance rollouts. We show that this\nsingle proxy suffices for a soft-actor-critic to choose sensors online,\noptimising energy while bounding pose-covariance growth. We deploy B-COD in\nreal-time marine trials on an unmanned surface vehicle and show that it reduces\nsensing energy consumption while matching the goal-reach performance of an\nalways-on baseline.", "AI": {"tldr": "B-COD\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u89c4\u5212\u5668\uff0c\u901a\u8fc7\u5355\u6b21\u524d\u5411\u4f20\u64ad\u572810\u6beb\u79d2\u5185\u751f\u6210\u8f68\u8ff9\u3001\u65b9\u5dee\u548c\u5b9a\u4f4d\u8bef\u5dee\u4ee3\u7406\uff0c\u7ed3\u5408SAC\u7b97\u6cd5\u5728\u7ebf\u9009\u62e9\u6700\u5c0f\u4f20\u611f\u5668\u5b50\u96c6\uff0c\u5728\u4fdd\u8bc1\u4efb\u52a1\u5b8c\u6210\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u80fd\u8017\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u5982\u4f55\u5728\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e2d\u4f7f\u7528\u6700\u5c0f\u4f20\u611f\u5668\u5b50\u96c6\u6765\u7ef4\u6301\u8db3\u591f\u4f4e\u7684\u72b6\u6001\u4e0d\u786e\u5b9a\u6027\u4ee5\u5b8c\u6210\u4efb\u52a1\u7684\u95ee\u9898\uff0c\u907f\u514d\u6301\u7eed\u5f00\u542f\u6240\u6709\u4f20\u611f\u5668\u9020\u6210\u7684\u80fd\u6e90\u6d6a\u8d39\u3002", "method": "\u63d0\u51faBelief-Conditioned One-Step Diffusion (B-COD)\u89c4\u5212\u5668\uff0c\u5c06\u6269\u6563\u6a21\u578b\u663e\u5f0f\u6761\u4ef6\u5316\u4e8e\u4f4d\u59ff\u4fe1\u5ff5\u6805\u683c\u548c\u4f20\u611f\u5668\u63a9\u7801\uff0c\u5229\u7528\u53bb\u566a\u8f68\u8ff9\u7684\u6269\u6563\u4f5c\u4e3a\u6821\u51c6\u7684\u3001\u53ef\u5fae\u7684\u5b9a\u4f4d\u8bef\u5dee\u4ee3\u7406\uff0c\u7ed3\u5408soft-actor-critic\u7b97\u6cd5\u5728\u7ebf\u9009\u62e9\u4f20\u611f\u5668\u3002", "result": "\u5728\u65e0\u4eba\u6c34\u9762\u8f66\u8f86\u7684\u771f\u5b9e\u6d77\u6d0b\u8bd5\u9a8c\u4e2d\uff0cB-COD\u5728\u5339\u914d\u59cb\u7ec8\u5f00\u542f\u57fa\u7ebf\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u4f20\u611f\u80fd\u8017\u6d88\u8017\u3002", "conclusion": "B-COD\u901a\u8fc7\u6269\u6563\u6a21\u578b\u7684\u8f68\u8ff9\u6269\u6563\u7279\u6027\u63d0\u4f9b\u5b9a\u4f4d\u8bef\u5dee\u4ee3\u7406\uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u4f20\u611f\u5668\u9009\u62e9\u4f18\u5316\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u673a\u5668\u4eba\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.12087", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.12087", "abs": "https://arxiv.org/abs/2508.12087", "authors": ["Zhanjiang Yang", "Meng Li", "Yang Shen", "Yueming Li", "Lijun Sun"], "title": "MAPF-World: Action World Model for Multi-Agent Path Finding", "comment": null, "summary": "Multi-agent path finding (MAPF) is the problem of planning conflict-free\npaths from the designated start locations to goal positions for multiple\nagents. It underlies a variety of real-world tasks, including multi-robot\ncoordination, robot-assisted logistics, and social navigation. Recent\ndecentralized learnable solvers have shown great promise for large-scale MAPF,\nespecially when leveraging foundation models and large datasets. However, these\nagents are reactive policy models and exhibit limited modeling of environmental\ntemporal dynamics and inter-agent dependencies, resulting in performance\ndegradation in complex, long-term planning scenarios. To address these\nlimitations, we propose MAPF-World, an autoregressive action world model for\nMAPF that unifies situation understanding and action generation, guiding\ndecisions beyond immediate local observations. It improves situational\nawareness by explicitly modeling environmental dynamics, including spatial\nfeatures and temporal dependencies, through future state and actions\nprediction. By incorporating these predicted futures, MAPF-World enables more\ninformed, coordinated, and far-sighted decision-making, especially in complex\nmulti-agent settings. Furthermore, we augment MAPF benchmarks by introducing an\nautomatic map generator grounded in real-world scenarios, capturing practical\nmap layouts for training and evaluating MAPF solvers. Extensive experiments\ndemonstrate that MAPF-World outperforms state-of-the-art learnable solvers,\nshowcasing superior zero-shot generalization to out-of-distribution cases.\nNotably, MAPF-World is trained with a 96.5% smaller model size and 92% reduced\ndata.", "AI": {"tldr": "MAPF-World\u662f\u4e00\u4e2a\u7528\u4e8e\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u7684\u81ea\u56de\u5f52\u52a8\u4f5c\u4e16\u754c\u6a21\u578b\uff0c\u901a\u8fc7\u7edf\u4e00\u60c5\u5883\u7406\u89e3\u548c\u52a8\u4f5c\u751f\u6210\uff0c\u5728\u590d\u6742\u73af\u5883\u4e2d\u5b9e\u73b0\u66f4\u8fdc\u89c1\u7684\u51b3\u7b56\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u53ef\u5b66\u4e60\u6c42\u89e3\u5668\u3002", "motivation": "\u73b0\u6709\u5206\u6563\u5f0f\u53ef\u5b66\u4e60\u6c42\u89e3\u5668\u5728\u590d\u6742\u957f\u671f\u89c4\u5212\u573a\u666f\u4e2d\u8868\u73b0\u53d7\u9650\uff0c\u4e3b\u8981\u56e0\u4e3a\u7f3a\u4e4f\u5bf9\u73af\u5883\u65f6\u95f4\u52a8\u6001\u548c\u667a\u80fd\u4f53\u95f4\u4f9d\u8d56\u5173\u7cfb\u7684\u5efa\u6a21\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u81ea\u56de\u5f52\u52a8\u4f5c\u4e16\u754c\u6a21\u578bMAPF-World\uff0c\u901a\u8fc7\u672a\u6765\u72b6\u6001\u548c\u52a8\u4f5c\u9884\u6d4b\u663e\u5f0f\u5efa\u6a21\u73af\u5883\u52a8\u6001\uff08\u7a7a\u95f4\u7279\u5f81\u548c\u65f6\u95f4\u4f9d\u8d56\uff09\uff0c\u5c06\u60c5\u5883\u7406\u89e3\u4e0e\u52a8\u4f5c\u751f\u6210\u7edf\u4e00\u3002", "result": "MAPF-World\u5728\u96f6\u6837\u672c\u6cdb\u5316\u5230\u5206\u5e03\u5916\u6848\u4f8b\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u53ef\u5b66\u4e60\u6c42\u89e3\u5668\uff0c\u6a21\u578b\u5927\u5c0f\u51cf\u5c1196.5%\uff0c\u6570\u636e\u9700\u6c42\u51cf\u5c1192%\u3002", "conclusion": "MAPF-World\u901a\u8fc7\u5efa\u6a21\u73af\u5883\u52a8\u6001\u548c\u672a\u6765\u9884\u6d4b\uff0c\u5b9e\u73b0\u4e86\u66f4\u660e\u667a\u3001\u534f\u8c03\u548c\u8fdc\u89c1\u7684\u51b3\u7b56\uff0c\u4e3a\u5927\u89c4\u6a21MAPF\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.12170", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.12170", "abs": "https://arxiv.org/abs/2508.12170", "authors": ["Aryan Gupta"], "title": "Energy Efficiency in Robotics Software: A Systematic Literature Review (2020-2024)", "comment": null, "summary": "This study presents a systematic literature review of software-level\napproaches to energy efficiency in robotics published from 2020 through 2024,\nupdating and extending pre-2020 evidence. An automated-but-audited pipeline\ncombined Google Scholar seeding, backward/forward snowballing, and\nlarge-language-model (LLM) assistance for screening and data extraction, with\n~10% human audits at each automated step and consensus-with-tie-breaks for\nfull-text decisions. The final corpus comprises 79 peer-reviewed studies\nanalyzed across application domain, metrics, evaluation type, energy models,\nmajor energy consumers, software technique families, and energy-quality\ntrade-offs. Industrial settings dominate (31.6%) followed by exploration\n(25.3%). Motors/actuators are identified as the primary consumer in 68.4% of\nstudies, with computing/controllers a distant second (13.9%). Simulation-only\nevaluations remain most common (51.9%), though hybrid evaluations are frequent\n(25.3%). Representational (physics-grounded) energy models predominate (87.3%).\nMotion and trajectory optimization is the leading technique family (69.6%),\noften paired with learning/prediction (40.5%) and computation\nallocation/scheduling (26.6%); power management/idle control (11.4%) and\ncommunication/data efficiency (3.8%) are comparatively underexplored. Reporting\nis heterogeneous: composite objectives that include energy are most common,\nwhile task-normalized and performance-per-energy metrics appear less often,\nlimiting cross-paper comparability. The review offers a minimal reporting\nchecklist (e.g., total energy and average power plus a task-normalized metric\nand clear baselines) and highlights opportunities in cross-layer designs and in\nquantifying non-performance trade-offs (accuracy, stability). A replication\npackage with code, prompts, and frozen datasets accompanies the review.", "AI": {"tldr": "2020-2024\u5e74\u673a\u5668\u4eba\u8f6f\u4ef6\u5c42\u9762\u80fd\u6548\u7814\u7a76\u7684\u7cfb\u7edf\u6587\u732e\u7efc\u8ff0\uff0c\u6db5\u76d679\u7bc7\u8bba\u6587\uff0c\u53d1\u73b0\u5de5\u4e1a\u5e94\u7528\u4e3b\u5bfc\uff0c\u7535\u673a/\u6267\u884c\u5668\u662f\u4e3b\u8981\u80fd\u8017\u6e90\uff0c\u8fd0\u52a8\u8f68\u8ff9\u4f18\u5316\u662f\u6700\u5e38\u7528\u6280\u672f\uff0c\u4f46\u62a5\u544a\u6807\u51c6\u4e0d\u7edf\u4e00\u3002", "motivation": "\u66f4\u65b0\u548c\u6269\u5c552020\u5e74\u524d\u5173\u4e8e\u673a\u5668\u4eba\u8f6f\u4ef6\u80fd\u6548\u65b9\u6cd5\u7684\u8bc1\u636e\uff0c\u4e3a\u7814\u7a76\u793e\u533a\u63d0\u4f9b\u6700\u65b0\u7684\u7cfb\u7edf\u7efc\u8ff0\u548c\u89c1\u89e3\u3002", "method": "\u91c7\u7528\u81ea\u52a8\u5316\u4f46\u7ecf\u8fc7\u5ba1\u6838\u7684\u6d41\u7a0b\uff0c\u7ed3\u5408Google Scholar\u79cd\u5b50\u641c\u7d22\u3001\u524d\u540e\u5411\u6eda\u96ea\u7403\u6cd5\u548c\u5927\u8bed\u8a00\u6a21\u578b\u8f85\u52a9\u7b5b\u9009\u4e0e\u6570\u636e\u63d0\u53d6\uff0c\u4eba\u5de5\u5ba1\u6838\u7387\u7ea610%\u3002", "result": "\u5de5\u4e1a\u8bbe\u7f6e\u4e3b\u5bfc(31.6%)\uff0c\u7535\u673a/\u6267\u884c\u5668\u662f\u4e3b\u8981\u80fd\u8017\u6e90(68.4%)\uff0c\u4eff\u771f\u8bc4\u4f30\u6700\u5e38\u89c1(51.9%)\uff0c\u8fd0\u52a8\u8f68\u8ff9\u4f18\u5316\u662f\u4e3b\u8981\u6280\u672f\u5bb6\u65cf(69.6%)\uff0c\u4f46\u62a5\u544a\u6807\u51c6\u4e0d\u7edf\u4e00\u3002", "conclusion": "\u63d0\u51fa\u4e86\u6700\u5c0f\u62a5\u544a\u6e05\u5355\uff0c\u5f3a\u8c03\u8de8\u5c42\u8bbe\u8ba1\u548c\u91cf\u5316\u975e\u6027\u80fd\u6743\u8861\u7684\u673a\u4f1a\uff0c\u63d0\u4f9b\u4e86\u5305\u542b\u4ee3\u7801\u3001\u63d0\u793a\u548c\u51bb\u7ed3\u6570\u636e\u96c6\u7684\u590d\u5236\u5305\u3002"}}
{"id": "2508.12100", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12100", "abs": "https://arxiv.org/abs/2508.12100", "authors": ["Daniel Burkhardt", "Xiangwei Cheng"], "title": "Overcoming Knowledge Discrepancies: Structuring Reasoning Threads through Knowledge Balancing in Interactive Scenarios", "comment": "13 pages, 1 figure, 6 tables", "summary": "Reasoning in interactive problem solving scenarios requires models to\nconstruct reasoning threads that reflect user understanding and align with\nstructured domain knowledge. However, current reasoning models often lack\nexplicit semantic hierarchies, user-domain knowledge alignment, and principled\nmechanisms to prune reasoning threads for effectiveness. These limitations\nresult in lengthy generic output that does not guide users through\ngoal-oriented reasoning steps. To address this, we propose a\nprototype-inspired, two-phases Reasoning-Threads-Evaluation (ReT-Eval)\nframework, drawing inspiration from human-like reasoning strategies that\nemphasize structured knowledge reuse. In the first phase, semantically relevant\nknowledge structures are extracted from a sparse domain knowledge graph using a\ngraph neural network and enriched with intrinsic large language model knowledge\nto resolve knowledge discrepancies. In the second phase, these threads are\nevaluated and pruned using a reward-guided strategy aimed at maintaining\nsemantic coherence to generate effective reasoning threads. Experiments and\nexpert evaluations show that ReT-Eval enhances user understanding and\noutperforms state-of-the-art reasoning models.", "AI": {"tldr": "\u63d0\u51fa\u4e86ReT-Eval\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u65b9\u6cd5\uff08\u77e5\u8bc6\u63d0\u53d6\u548c\u5956\u52b1\u5f15\u5bfc\u526a\u679d\uff09\u751f\u6210\u8bed\u4e49\u5c42\u6b21\u6e05\u6670\u3001\u4e0e\u7528\u6237\u7406\u89e3\u5bf9\u9f50\u7684\u63a8\u7406\u7ebf\u7a0b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u63a8\u7406\u6a21\u578b\u7f3a\u4e4f\u663e\u5f0f\u8bed\u4e49\u5c42\u6b21\u548c\u6709\u6548\u526a\u679d\u673a\u5236\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u63a8\u7406\u6a21\u578b\u7f3a\u4e4f\u663e\u5f0f\u8bed\u4e49\u5c42\u6b21\u7ed3\u6784\u3001\u7528\u6237-\u9886\u57df\u77e5\u8bc6\u5bf9\u9f50\u673a\u5236\uff0c\u4ee5\u53ca\u6709\u6548\u7684\u63a8\u7406\u7ebf\u7a0b\u526a\u679d\u7b56\u7565\uff0c\u5bfc\u81f4\u8f93\u51fa\u5197\u957f\u4e14\u65e0\u6cd5\u6709\u6548\u5f15\u5bfc\u7528\u6237\u8fdb\u884c\u76ee\u6807\u5bfc\u5411\u63a8\u7406\u3002", "method": "\u91c7\u7528\u539f\u578b\u542f\u53d1\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u4ece\u7a00\u758f\u9886\u57df\u77e5\u8bc6\u56fe\u4e2d\u63d0\u53d6\u8bed\u4e49\u76f8\u5173\u77e5\u8bc6\u7ed3\u6784\uff0c\u5e76\u7528\u5927\u8bed\u8a00\u6a21\u578b\u77e5\u8bc6\u8fdb\u884c\u4e30\u5bcc\uff1b\u7b2c\u4e8c\u9636\u6bb5\u4f7f\u7528\u5956\u52b1\u5f15\u5bfc\u7b56\u7565\u8bc4\u4f30\u548c\u526a\u679d\u63a8\u7406\u7ebf\u7a0b\u4ee5\u4fdd\u6301\u8bed\u4e49\u8fde\u8d2f\u6027\u3002", "result": "\u5b9e\u9a8c\u548c\u4e13\u5bb6\u8bc4\u4f30\u8868\u660e\uff0cReT-Eval\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u7528\u6237\u7406\u89e3\u80fd\u529b\uff0c\u5e76\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u63a8\u7406\u6a21\u578b\u3002", "conclusion": "ReT-Eval\u6846\u67b6\u901a\u8fc7\u7ed3\u6784\u5316\u77e5\u8bc6\u91cd\u7528\u548c\u5956\u52b1\u5f15\u5bfc\u7684\u526a\u679d\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4ea4\u4e92\u5f0f\u95ee\u9898\u89e3\u51b3\u573a\u666f\u4e2d\u63a8\u7406\u7ebf\u7a0b\u7684\u8d28\u91cf\u548c\u6548\u7387\u95ee\u9898\uff0c\u4e3a\u6784\u5efa\u66f4\u4eba\u6027\u5316\u7684\u63a8\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2508.12184", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.12184", "abs": "https://arxiv.org/abs/2508.12184", "authors": ["Rhea Malhotra", "William Chong", "Catie Cuan", "Oussama Khatib"], "title": "Humanoid Motion Scripting with Postural Synergies", "comment": null, "summary": "Generating sequences of human-like motions for humanoid robots presents\nchallenges in collecting and analyzing reference human motions, synthesizing\nnew motions based on these reference motions, and mapping the generated motion\nonto humanoid robots. To address these issues, we introduce SynSculptor, a\nhumanoid motion analysis and editing framework that leverages postural\nsynergies for training-free human-like motion scripting. To analyze human\nmotion, we collect 3+ hours of motion capture data across 20 individuals where\na real-time operational space controller mimics human motion on a simulated\nhumanoid robot. The major postural synergies are extracted using principal\ncomponent analysis (PCA) for velocity trajectories segmented by changes in\nrobot momentum, constructing a style-conditioned synergy library for free-space\nmotion generation. To evaluate generated motions using the synergy library, the\nfoot-sliding ratio and proposed metrics for motion smoothness involving total\nmomentum and kinetic energy deviations are computed for each generated motion,\nand compared with reference motions. Finally, we leverage the synergies with a\nmotion-language transformer, where the humanoid, during execution of motion\ntasks with its end-effectors, adapts its posture based on the chosen synergy.\nSupplementary material, code, and videos are available at\nhttps://rhea-mal.github.io/humanoidsynergies.io.", "AI": {"tldr": "SynSculptor\u662f\u4e00\u4e2a\u57fa\u4e8e\u59ff\u6001\u534f\u540c\u7684\u65e0\u8bad\u7ec3\u4eba\u7c7b\u8fd0\u52a8\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u6536\u96c6\u4eba\u7c7b\u52a8\u4f5c\u6355\u6349\u6570\u636e\uff0c\u63d0\u53d6\u4e3b\u8981\u59ff\u6001\u534f\u540c\u7279\u5f81\uff0c\u6784\u5efa\u98ce\u683c\u6761\u4ef6\u534f\u540c\u5e93\u6765\u751f\u6210\u7c7b\u4eba\u8fd0\u52a8\uff0c\u5e76\u5229\u7528\u8fd0\u52a8-\u8bed\u8a00\u53d8\u6362\u5668\u5b9e\u73b0\u4efb\u52a1\u6267\u884c\u4e2d\u7684\u59ff\u6001\u81ea\u9002\u5e94\u3002", "motivation": "\u89e3\u51b3\u4eba\u5f62\u673a\u5668\u4eba\u8fd0\u52a8\u751f\u6210\u4e2d\u7684\u6311\u6218\uff1a\u53c2\u8003\u8fd0\u52a8\u6570\u636e\u6536\u96c6\u4e0e\u5206\u6790\u56f0\u96be\u3001\u57fa\u4e8e\u53c2\u8003\u8fd0\u52a8\u5408\u6210\u65b0\u8fd0\u52a8\u7684\u590d\u6742\u6027\uff0c\u4ee5\u53ca\u5c06\u751f\u6210\u8fd0\u52a8\u6620\u5c04\u5230\u4eba\u5f62\u673a\u5668\u4eba\u7684\u6280\u672f\u96be\u9898\u3002", "method": "\u6536\u96c63+\u5c0f\u65f620\u4eba\u7684\u52a8\u4f5c\u6355\u6349\u6570\u636e\uff0c\u4f7f\u7528\u5b9e\u65f6\u64cd\u4f5c\u7a7a\u95f4\u63a7\u5236\u5668\u5728\u4eff\u771f\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u6a21\u4eff\u4eba\u7c7b\u8fd0\u52a8\uff1b\u901a\u8fc7PCA\u63d0\u53d6\u901f\u5ea6\u8f68\u8ff9\u7684\u4e3b\u8981\u59ff\u6001\u534f\u540c\u7279\u5f81\uff0c\u6784\u5efa\u98ce\u683c\u6761\u4ef6\u534f\u540c\u5e93\uff1b\u63d0\u51fa\u811a\u6ed1\u6bd4\u548c\u8fd0\u52a8\u5e73\u6ed1\u5ea6\u6307\u6807\u8bc4\u4f30\u751f\u6210\u8fd0\u52a8\uff1b\u7ed3\u5408\u8fd0\u52a8-\u8bed\u8a00\u53d8\u6362\u5668\u5b9e\u73b0\u4efb\u52a1\u6267\u884c\u4e2d\u7684\u59ff\u6001\u81ea\u9002\u5e94\u3002", "result": "\u6210\u529f\u6784\u5efa\u4e86\u57fa\u4e8e\u59ff\u6001\u534f\u540c\u7684\u4eba\u7c7b\u8fd0\u52a8\u5206\u6790\u6846\u67b6\uff0c\u80fd\u591f\u751f\u6210\u7c7b\u4eba\u8fd0\u52a8\uff0c\u5e76\u901a\u8fc7\u63d0\u51fa\u7684\u8bc4\u4f30\u6307\u6807\u9a8c\u8bc1\u4e86\u751f\u6210\u8fd0\u52a8\u7684\u8d28\u91cf\uff0c\u4e0e\u53c2\u8003\u8fd0\u52a8\u8fdb\u884c\u4e86\u6709\u6548\u5bf9\u6bd4\u3002", "conclusion": "SynSculptor\u6846\u67b6\u901a\u8fc7\u59ff\u6001\u534f\u540c\u5206\u6790\u5b9e\u73b0\u4e86\u65e0\u9700\u8bad\u7ec3\u7684\u4eba\u7c7b\u8fd0\u52a8\u751f\u6210\uff0c\u4e3a\u4eba\u5f62\u673a\u5668\u4eba\u7684\u81ea\u7136\u8fd0\u52a8\u63a7\u5236\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2508.12149", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12149", "abs": "https://arxiv.org/abs/2508.12149", "authors": ["Haochen You", "Baojing Liu"], "title": "MOVER: Multimodal Optimal Transport with Volume-based Embedding Regularization", "comment": "Accepted as a conference paper at CIKM 2025", "summary": "Recent advances in multimodal learning have largely relied on pairwise\ncontrastive objectives to align different modalities, such as text, video, and\naudio, in a shared embedding space. While effective in bi-modal setups, these\napproaches struggle to generalize across multiple modalities and often lack\nsemantic structure in high-dimensional spaces. In this paper, we propose MOVER,\na novel framework that combines optimal transport-based soft alignment with\nvolume-based geometric regularization to build semantically aligned and\nstructured multimodal representations. By integrating a transport-guided\nmatching mechanism with a geometric volume minimization objective (GAVE), MOVER\nencourages consistent alignment across all modalities in a modality-agnostic\nmanner. Experiments on text-video-audio retrieval tasks demonstrate that MOVER\nsignificantly outperforms prior state-of-the-art methods in both zero-shot and\nfinetuned settings. Additional analysis shows improved generalization to unseen\nmodality combinations and stronger structural consistency in the learned\nembedding space.", "AI": {"tldr": "MOVER\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6700\u4f18\u4f20\u8f93\u8f6f\u5bf9\u9f50\u548c\u51e0\u4f55\u4f53\u79ef\u6b63\u5219\u5316\uff0c\u5728\u5171\u4eab\u5d4c\u5165\u7a7a\u95f4\u4e2d\u6784\u5efa\u8bed\u4e49\u5bf9\u9f50\u548c\u7ed3\u6784\u5316\u7684\u591a\u6a21\u6001\u8868\u793a\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u6210\u5bf9\u5bf9\u6bd4\u76ee\u6807\uff0c\u5728\u591a\u6a21\u6001\u8bbe\u7f6e\u4e2d\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u4e14\u5728\u9ad8\u7ef4\u7a7a\u95f4\u4e2d\u7f3a\u4e4f\u8bed\u4e49\u7ed3\u6784\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u4e00\u81f4\u5bf9\u9f50\u6240\u6709\u6a21\u6001\u5e76\u4fdd\u6301\u8bed\u4e49\u7ed3\u6784\u7684\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408\u6700\u4f18\u4f20\u8f93\u7684\u8f6f\u5bf9\u9f50\u673a\u5236\u548c\u51e0\u4f55\u4f53\u79ef\u6700\u5c0f\u5316\u76ee\u6807(GAVE)\uff0c\u4ee5\u6a21\u6001\u65e0\u5173\u7684\u65b9\u5f0f\u5b9e\u73b0\u6240\u6709\u6a21\u6001\u7684\u4e00\u81f4\u5bf9\u9f50\uff0c\u6784\u5efa\u7ed3\u6784\u5316\u7684\u5d4c\u5165\u7a7a\u95f4\u3002", "result": "\u5728\u6587\u672c-\u89c6\u9891-\u97f3\u9891\u68c0\u7d22\u4efb\u52a1\u4e2d\uff0cMOVER\u5728\u96f6\u6837\u672c\u548c\u5fae\u8c03\u8bbe\u7f6e\u4e0b\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5c55\u73b0\u51fa\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5d4c\u5165\u7a7a\u95f4\u7ed3\u6784\u4e00\u81f4\u6027\u3002", "conclusion": "MOVER\u6846\u67b6\u901a\u8fc7\u6700\u4f18\u4f20\u8f93\u548c\u51e0\u4f55\u6b63\u5219\u5316\u7684\u7ed3\u5408\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u5bf9\u9f50\u4e2d\u7684\u6cdb\u5316\u548c\u7ed3\u6784\u5316\u95ee\u9898\uff0c\u4e3a\u591a\u6a21\u6001\u8868\u793a\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2508.12189", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12189", "abs": "https://arxiv.org/abs/2508.12189", "authors": ["Rhea Malhotra", "Yuejiang Liu", "Chelsea Finn"], "title": "Self-Guided Action Diffusion", "comment": null, "summary": "Recent works have shown the promise of inference-time search over action\nsamples for improving generative robot policies. In particular, optimizing\ncross-chunk coherence via bidirectional decoding has proven effective in\nboosting the consistency and reactivity of diffusion policies. However, this\napproach remains computationally expensive as the diversity of sampled actions\ngrows. In this paper, we introduce self-guided action diffusion, a more\nefficient variant of bidirectional decoding tailored for diffusion-based\npolicies. At the core of our method is to guide the proposal distribution at\neach diffusion step based on the prior decision. Experiments in simulation\ntasks show that the proposed self-guidance enables near-optimal performance at\nnegligible inference cost. Notably, under a tight sampling budget, our method\nachieves up to 70% higher success rates than existing counterparts on\nchallenging dynamic tasks. See project website at\nhttps://rhea-mal.github.io/selfgad.github.io.", "AI": {"tldr": "\u63d0\u51fa\u81ea\u5f15\u5bfc\u52a8\u4f5c\u6269\u6563\u65b9\u6cd5\uff0c\u901a\u8fc7\u57fa\u4e8e\u5148\u524d\u51b3\u7b56\u5f15\u5bfc\u6269\u6563\u6b65\u9aa4\u7684\u63d0\u8bae\u5206\u5e03\uff0c\u663e\u8457\u63d0\u9ad8\u63a8\u7406\u6548\u7387\uff0c\u5728\u4e25\u683c\u91c7\u6837\u9884\u7b97\u4e0b\u6bd4\u73b0\u6709\u65b9\u6cd5\u6210\u529f\u7387\u63d0\u534770%", "motivation": "\u73b0\u6709\u57fa\u4e8e\u53cc\u5411\u89e3\u7801\u7684\u63a8\u7406\u65f6\u641c\u7d22\u65b9\u6cd5\u867d\u7136\u80fd\u63d0\u5347\u6269\u6563\u7b56\u7565\u7684\u4e00\u81f4\u6027\u548c\u53cd\u5e94\u6027\uff0c\u4f46\u968f\u7740\u91c7\u6837\u52a8\u4f5c\u591a\u6837\u6027\u589e\u52a0\uff0c\u8ba1\u7b97\u6210\u672c\u53d8\u5f97\u6602\u8d35", "method": "\u81ea\u5f15\u5bfc\u52a8\u4f5c\u6269\u6563\uff0c\u5728\u6269\u6563\u8fc7\u7a0b\u7684\u6bcf\u4e2a\u6b65\u9aa4\u57fa\u4e8e\u5148\u524d\u7684\u51b3\u7b56\u6765\u5f15\u5bfc\u63d0\u8bae\u5206\u5e03\uff0c\u662f\u53cc\u5411\u89e3\u7801\u7684\u9ad8\u6548\u53d8\u4f53", "result": "\u5728\u4eff\u771f\u4efb\u52a1\u4e2d\uff0c\u81ea\u5f15\u5bfc\u65b9\u6cd5\u4ee5\u53ef\u5ffd\u7565\u7684\u63a8\u7406\u6210\u672c\u5b9e\u73b0\u63a5\u8fd1\u6700\u4f18\u6027\u80fd\uff0c\u5728\u4e25\u683c\u91c7\u6837\u9884\u7b97\u4e0b\uff0c\u5728\u52a8\u6001\u4efb\u52a1\u4e0a\u6bd4\u73b0\u6709\u65b9\u6cd5\u6210\u529f\u7387\u63d0\u5347\u9ad8\u8fbe70%", "conclusion": "\u81ea\u5f15\u5bfc\u52a8\u4f5c\u6269\u6563\u4e3a\u6269\u6563\u7b56\u7565\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u53cc\u5411\u89e3\u7801\u66ff\u4ee3\u65b9\u6848\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u63a8\u7406\u6210\u672c\u540c\u65f6\u4fdd\u6301\u4f18\u5f02\u6027\u80fd"}}
{"id": "2508.12165", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12165", "abs": "https://arxiv.org/abs/2508.12165", "authors": ["Rohit Krishnan", "Jon Evans"], "title": "RLNVR: Reinforcement Learning from Non-Verified Real-World Rewards", "comment": null, "summary": "This paper introduces RLNVR (Reinforcement Learning from Non-Verified\nRewards), a framework for training language models using noisy, real-world\nfeedback signals without requiring explicit human verification. Traditional\nRLHF requires expensive, verified reward signals that are impractical in many\nreal-world domains. RLNVR addresses this challenge through baseline\nnormalization and semantic similarity-based reward transfer. We demonstrate\nRLNVR through Walter, a prototype system that optimizes social media content\ngeneration using actual engagement data from Bluesky. Our experimental results\nshow significant improvements in content quality and training stability, with\ncomprehensive evaluation planned for future work. Positioning: We present a\npractical framework that combines RLNVR with GSPO (Group Sequence Policy\nOptimization) and an optional UED (Unsupervised Environment Design) curriculum\nto improve stability and diversity under noisy, implicit rewards. To our\nknowledge, combining GSPO-style normalization with a UED-style curriculum for\nLLM content generation from implicit social engagement has not been previously\ndocumented in this applied setting; we frame this as an applied integration\nrather than a new algorithm.", "AI": {"tldr": "RLNVR\u6846\u67b6\u4f7f\u7528\u672a\u7ecf\u9a8c\u8bc1\u7684\u566a\u58f0\u5956\u52b1\u4fe1\u53f7\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u57fa\u7ebf\u5f52\u4e00\u5316\u548c\u8bed\u4e49\u76f8\u4f3c\u6027\u5956\u52b1\u8f6c\u79fb\u89e3\u51b3\u4f20\u7edfRLHF\u9700\u8981\u6602\u8d35\u9a8c\u8bc1\u5956\u52b1\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edfRLHF\u9700\u8981\u6602\u8d35\u7684\u4eba\u5de5\u9a8c\u8bc1\u5956\u52b1\u4fe1\u53f7\uff0c\u5728\u8bb8\u591a\u73b0\u5b9e\u573a\u666f\u4e2d\u4e0d\u5b9e\u7528\u3002RLNVR\u65e8\u5728\u5229\u7528\u566a\u58f0\u7684\u771f\u5b9e\u4e16\u754c\u53cd\u9988\u4fe1\u53f7\u6765\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u3002", "method": "\u7ed3\u5408\u57fa\u7ebf\u5f52\u4e00\u5316\u3001\u8bed\u4e49\u76f8\u4f3c\u6027\u5956\u52b1\u8f6c\u79fb\u3001GSPO\uff08\u7ec4\u5e8f\u5217\u7b56\u7565\u4f18\u5316\uff09\u548c\u53ef\u9009\u7684UED\uff08\u65e0\u76d1\u7763\u73af\u5883\u8bbe\u8ba1\uff09\u8bfe\u7a0b\u5b66\u4e60\uff0c\u5728\u566a\u58f0\u9690\u5f0f\u5956\u52b1\u4e0b\u63d0\u9ad8\u7a33\u5b9a\u6027\u548c\u591a\u6837\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u5728\u5185\u5bb9\u8d28\u91cf\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\u65b9\u9762\u6709\u663e\u8457\u6539\u8fdb\uff0c\u4f7f\u7528Bluesky\u5b9e\u9645\u53c2\u4e0e\u6570\u636e\u4f18\u5316\u793e\u4ea4\u5a92\u4f53\u5185\u5bb9\u751f\u6210\u7684Walter\u539f\u578b\u7cfb\u7edf\u8bc1\u660e\u4e86\u6709\u6548\u6027\u3002", "conclusion": "RLNVR\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u7684\u6846\u67b6\uff0c\u80fd\u591f\u5728\u4e0d\u9700\u8981\u663e\u5f0f\u4eba\u7c7b\u9a8c\u8bc1\u7684\u60c5\u51b5\u4e0b\uff0c\u5229\u7528\u771f\u5b9e\u4e16\u754c\u7684\u566a\u58f0\u53cd\u9988\u4fe1\u53f7\u6709\u6548\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u3002"}}
{"id": "2508.12211", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12211", "abs": "https://arxiv.org/abs/2508.12211", "authors": ["Cyrus Neary", "Omar G. Younis", "Artur Kuramshin", "Ozgur Aslan", "Glen Berseth"], "title": "Improving Pre-Trained Vision-Language-Action Policies with Model-Based Search", "comment": null, "summary": "Pre-trained vision-language-action (VLA) models offer a promising foundation\nfor generalist robot policies, but often produce brittle behaviours or unsafe\nfailures when deployed zero-shot in out-of-distribution scenarios. We present\nVision-Language-Action Planning & Search (VLAPS) -- a novel framework and\naccompanying algorithms that embed model-based search into the inference\nprocedure of pre-trained VLA policies to improve their performance on robotic\ntasks. Specifically, our method biases a modified Monte Carlo Tree Search\n(MCTS) algorithm -- run using a model of the target environment -- using action\npriors defined by the VLA policy. By using VLA-derived abstractions and priors\nin model-based search, VLAPS efficiently explores language-conditioned robotics\ntasks whose search spaces would otherwise be intractably large. Conversely, by\nintegrating model-based search with the VLA policy's inference procedure, VLAPS\nyields behaviours that are more performant than those obtained by directly\nfollowing the VLA policy's action predictions. VLAPS offers a principled\nframework to: i) control test-time compute in VLA models, ii) leverage a priori\nknowledge of the robotic environment, and iii) integrate established planning\nand reinforcement learning techniques into the VLA inference process. Across\nall experiments, VLAPS significantly outperforms VLA-only baselines on\nlanguage-specified tasks that would otherwise be intractable for uninformed\nsearch algorithms, increasing success rates by as much as 67 percentage points.", "AI": {"tldr": "VLAPS\u6846\u67b6\u901a\u8fc7\u5c06\u57fa\u4e8e\u6a21\u578b\u7684\u641c\u7d22\u5d4c\u5165\u9884\u8bad\u7ec3VLA\u6a21\u578b\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u663e\u8457\u63d0\u5347\u673a\u5668\u4eba\u5728\u5206\u5e03\u5916\u573a\u666f\u4e0b\u7684\u4efb\u52a1\u6027\u80fd", "motivation": "\u9884\u8bad\u7ec3\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c(VLA)\u6a21\u578b\u5728\u96f6\u6837\u672c\u90e8\u7f72\u5230\u5206\u5e03\u5916\u573a\u666f\u65f6\u7ecf\u5e38\u4ea7\u751f\u8106\u5f31\u884c\u4e3a\u6216\u4e0d\u5b89\u5168\u6545\u969c\uff0c\u9700\u8981\u6539\u8fdb\u5176\u9c81\u68d2\u6027\u548c\u6027\u80fd", "method": "\u63d0\u51faVLAPS\u6846\u67b6\uff0c\u5c06\u6539\u8fdb\u7684\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22(MCTS)\u7b97\u6cd5\u4e0eVLA\u7b56\u7565\u7ed3\u5408\uff0c\u4f7f\u7528\u73af\u5883\u6a21\u578b\u8fdb\u884c\u641c\u7d22\uff0c\u5e76\u5229\u7528VLA\u7b56\u7565\u5b9a\u4e49\u52a8\u4f5c\u5148\u9a8c", "result": "\u5728\u6240\u6709\u5b9e\u9a8c\u4e2d\uff0cVLAPS\u663e\u8457\u4f18\u4e8e\u4ec5\u4f7f\u7528VLA\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u8bed\u8a00\u6307\u5b9a\u4efb\u52a1\u4e0a\u7684\u6210\u529f\u7387\u6700\u9ad8\u63d0\u534767\u4e2a\u767e\u5206\u70b9", "conclusion": "VLAPS\u63d0\u4f9b\u4e86\u4e00\u4e2a\u539f\u5219\u6027\u6846\u67b6\uff0c\u53ef\u4ee5\u63a7\u5236VLA\u6a21\u578b\u7684\u6d4b\u8bd5\u65f6\u8ba1\u7b97\u3001\u5229\u7528\u673a\u5668\u4eba\u73af\u5883\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u5e76\u5c06\u6210\u719f\u7684\u89c4\u5212\u548c\u5f3a\u5316\u5b66\u4e60\u6280\u672f\u96c6\u6210\u5230VLA\u63a8\u7406\u8fc7\u7a0b\u4e2d"}}
{"id": "2508.12260", "categories": ["cs.AI", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2508.12260", "abs": "https://arxiv.org/abs/2508.12260", "authors": ["Carson Dudley", "Reiden Magdaleno", "Christopher Harding", "Ananya Sharma", "Emily Martin", "Marisa Eisenberg"], "title": "Mantis: A Simulation-Grounded Foundation Model for Disease Forecasting", "comment": "10 pages, 4 figures", "summary": "Infectious disease forecasting in novel outbreaks or low resource settings\nhas been limited by the need for disease-specific data, bespoke training, and\nexpert tuning. We introduce Mantis, a foundation model trained entirely on\nmechanistic simulations, which enables out-of-the-box forecasting across\ndiseases, regions, and outcomes, even in settings with limited historical data.\nMantis is built on over 400 million simulated days of outbreak dynamics\nspanning diverse pathogens, transmission modes, interventions, and surveillance\nartifacts. Despite requiring no real-world data during training, Mantis\noutperformed 39 expert-tuned models we tested across six diseases, including\nall models in the CDC's COVID-19 Forecast Hub. Mantis generalized to novel\nepidemiological regimes, including diseases with held-out transmission\nmechanisms, demonstrating that it captures fundamental contagion dynamics.\nCritically, Mantis is mechanistically interpretable, enabling public health\ndecision-makers to identify the latent drivers behind its predictions. Finally,\nMantis delivers accurate forecasts at 8-week horizons, more than doubling the\nactionable range of most models, enabling proactive public health planning.\nTogether, these capabilities position Mantis as a foundation for\nnext-generation disease forecasting systems: general, interpretable, and\ndeployable where traditional models fail.", "AI": {"tldr": "Mantis\u662f\u4e00\u4e2a\u57fa\u4e8e\u673a\u5236\u6a21\u62df\u8bad\u7ec3\u7684\u57fa\u7840\u6a21\u578b\uff0c\u65e0\u9700\u771f\u5b9e\u6570\u636e\u5373\u53ef\u5728\u591a\u79cd\u75be\u75c5\u548c\u573a\u666f\u4e2d\u8fdb\u884c\u51c6\u786e\u9884\u6d4b\uff0c\u6027\u80fd\u8d85\u8d8a39\u4e2a\u4e13\u5bb6\u8c03\u4f18\u6a21\u578b", "motivation": "\u89e3\u51b3\u4f20\u7edf\u4f20\u67d3\u75c5\u9884\u6d4b\u6a21\u578b\u9700\u8981\u75be\u75c5\u7279\u5b9a\u6570\u636e\u3001\u4e13\u4e1a\u8bad\u7ec3\u548c\u4e13\u5bb6\u8c03\u4f18\u7684\u9650\u5236\uff0c\u7279\u522b\u662f\u5728\u65b0\u53d1\u75ab\u60c5\u6216\u8d44\u6e90\u532e\u4e4f\u5730\u533a", "method": "\u57fa\u4e8e\u8d85\u8fc74\u4ebf\u5929\u75ab\u60c5\u52a8\u6001\u7684\u673a\u5236\u6a21\u62df\u8bad\u7ec3\uff0c\u6db5\u76d6\u591a\u79cd\u75c5\u539f\u4f53\u3001\u4f20\u64ad\u65b9\u5f0f\u3001\u5e72\u9884\u63aa\u65bd\u548c\u76d1\u6d4b\u4f2a\u5f71\uff0c\u65e0\u9700\u771f\u5b9e\u4e16\u754c\u6570\u636e", "result": "\u5728\u516d\u79cd\u75be\u75c5\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u6240\u670939\u4e2a\u4e13\u5bb6\u8c03\u4f18\u6a21\u578b\uff0c\u5305\u62ecCDC COVID-19\u9884\u6d4b\u4e2d\u5fc3\u7684\u6240\u6709\u6a21\u578b\uff0c\u80fd\u63a8\u5e7f\u5230\u65b0\u7684\u6d41\u884c\u75c5\u5b66\u673a\u5236", "conclusion": "Mantis\u4f5c\u4e3a\u4e0b\u4e00\u4ee3\u75be\u75c5\u9884\u6d4b\u7cfb\u7edf\u7684\u57fa\u7840\uff0c\u5177\u6709\u901a\u7528\u6027\u3001\u53ef\u89e3\u91ca\u6027\u548c\u5728\u4f20\u7edf\u6a21\u578b\u5931\u8d25\u573a\u666f\u4e2d\u7684\u90e8\u7f72\u80fd\u529b"}}
{"id": "2508.12252", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.12252", "abs": "https://arxiv.org/abs/2508.12252", "authors": ["Kaizhe Hu", "Haochen Shi", "Yao He", "Weizhuo Wang", "C. Karen Liu", "Shuran Song"], "title": "Robot Trains Robot: Automatic Real-World Policy Adaptation and Learning for Humanoids", "comment": "Accepted to The Conference on Robot Learning (CoRL) 2025", "summary": "Simulation-based reinforcement learning (RL) has significantly advanced\nhumanoid locomotion tasks, yet direct real-world RL from scratch or adapting\nfrom pretrained policies remains rare, limiting the full potential of humanoid\nrobots. Real-world learning, despite being crucial for overcoming the\nsim-to-real gap, faces substantial challenges related to safety, reward design,\nand learning efficiency. To address these limitations, we propose\nRobot-Trains-Robot (RTR), a novel framework where a robotic arm teacher\nactively supports and guides a humanoid robot student. The RTR system provides\nprotection, learning schedule, reward, perturbation, failure detection, and\nautomatic resets. It enables efficient long-term real-world humanoid training\nwith minimal human intervention. Furthermore, we propose a novel RL pipeline\nthat facilitates and stabilizes sim-to-real transfer by optimizing a single\ndynamics-encoded latent variable in the real world. We validate our method\nthrough two challenging real-world humanoid tasks: fine-tuning a walking policy\nfor precise speed tracking and learning a humanoid swing-up task from scratch,\nillustrating the promising capabilities of real-world humanoid learning\nrealized by RTR-style systems. See https://robot-trains-robot.github.io/ for\nmore info.", "AI": {"tldr": "\u63d0\u51fa\u4e86Robot-Trains-Robot (RTR)\u6846\u67b6\uff0c\u4f7f\u7528\u673a\u68b0\u81c2\u6559\u5e08\u4e3b\u52a8\u6307\u5bfc\u4eba\u5f62\u673a\u5668\u4eba\u5b66\u751f\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u957f\u65f6\u771f\u5b9e\u4e16\u754c\u4eba\u5f62\u673a\u5668\u4eba\u8bad\u7ec3\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684RL\u7ba1\u9053\u6765\u4fc3\u8fdb\u548c\u7a33\u5b9asim-to-real\u8fc1\u79fb\u3002", "motivation": "\u57fa\u4e8e\u4eff\u771f\u7684\u5f3a\u5316\u5b66\u4e60\u5728\u4eba\u5f62\u673a\u5668\u4eba\u8fd0\u52a8\u4efb\u52a1\u4e2d\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u76f4\u63a5\u4ece\u96f6\u5f00\u59cb\u5728\u771f\u5b9e\u4e16\u754c\u8fdb\u884cRL\u6216\u4ece\u9884\u8bad\u7ec3\u7b56\u7565\u9002\u5e94\u4ecd\u7136\u5f88\u5c11\u89c1\uff0c\u9650\u5236\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u7684\u5168\u90e8\u6f5c\u529b\u3002\u771f\u5b9e\u4e16\u754c\u5b66\u4e60\u867d\u7136\u5bf9\u514b\u670dsim-to-real\u5dee\u8ddd\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9762\u4e34\u5b89\u5168\u3001\u5956\u52b1\u8bbe\u8ba1\u548c\u5b66\u4e60\u6548\u7387\u7b49\u91cd\u5927\u6311\u6218\u3002", "method": "1. \u63d0\u51faRTR\u6846\u67b6\uff1a\u673a\u68b0\u81c2\u6559\u5e08\u4e3a\u4eba\u5f62\u673a\u5668\u4eba\u5b66\u751f\u63d0\u4f9b\u4fdd\u62a4\u3001\u5b66\u4e60\u8ba1\u5212\u3001\u5956\u52b1\u3001\u6270\u52a8\u3001\u6545\u969c\u68c0\u6d4b\u548c\u81ea\u52a8\u91cd\u7f6e\n2. \u63d0\u51fa\u65b0\u7684RL\u7ba1\u9053\uff1a\u901a\u8fc7\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u4f18\u5316\u5355\u4e2a\u52a8\u529b\u5b66\u7f16\u7801\u7684\u6f5c\u5728\u53d8\u91cf\u6765\u4fc3\u8fdb\u548c\u7a33\u5b9asim-to-real\u8fc1\u79fb", "result": "\u5728\u4e24\u9879\u5177\u6709\u6311\u6218\u6027\u7684\u771f\u5b9e\u4e16\u754c\u4eba\u5f62\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\uff1a\n- \u5fae\u8c03\u884c\u8d70\u7b56\u7565\u4ee5\u5b9e\u73b0\u7cbe\u786e\u7684\u901f\u5ea6\u8ddf\u8e2a\n- \u4ece\u96f6\u5f00\u59cb\u5b66\u4e60\u4eba\u5f62\u673a\u5668\u4eba\u6446\u8d77\u4efb\u52a1", "conclusion": "RTR\u5f0f\u7cfb\u7edf\u5b9e\u73b0\u4e86\u771f\u5b9e\u4e16\u754c\u4eba\u5f62\u673a\u5668\u4eba\u5b66\u4e60\u7684\u6709\u524d\u666f\u80fd\u529b\uff0c\u4e3a\u9ad8\u6548\u7684\u957f\u65f6\u771f\u5b9e\u4e16\u754c\u8bad\u7ec3\u63d0\u4f9b\u4e86\u65b0\u6846\u67b6\uff0c\u6700\u5c0f\u5316\u4e86\u4eba\u7c7b\u5e72\u9884\u9700\u6c42\u3002"}}
{"id": "2508.12291", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12291", "abs": "https://arxiv.org/abs/2508.12291", "authors": ["Xuming He", "Zhiyuan You", "Junchao Gong", "Couhua Liu", "Xiaoyu Yue", "Peiqin Zhuang", "Wenlong Zhang", "Lei Bai"], "title": "RadarQA: Multi-modal Quality Analysis of Weather Radar Forecasts", "comment": null, "summary": "Quality analysis of weather forecasts is an essential topic in meteorology.\nAlthough traditional score-based evaluation metrics can quantify certain\nforecast errors, they are still far from meteorological experts in terms of\ndescriptive capability, interpretability, and understanding of dynamic\nevolution. With the rapid development of Multi-modal Large Language Models\n(MLLMs), these models become potential tools to overcome the above challenges.\nIn this work, we introduce an MLLM-based weather forecast analysis method,\nRadarQA, integrating key physical attributes with detailed assessment reports.\nWe introduce a novel and comprehensive task paradigm for multi-modal quality\nanalysis, encompassing both single frame and sequence, under both rating and\nassessment scenarios. To support training and benchmarking, we design a hybrid\nannotation pipeline that combines human expert labeling with automated\nheuristics. With such an annotation method, we construct RQA-70K, a large-scale\ndataset with varying difficulty levels for radar forecast quality evaluation.\nWe further design a multi-stage training strategy that iteratively improves\nmodel performance at each stage. Extensive experiments show that RadarQA\noutperforms existing general MLLMs across all evaluation settings, highlighting\nits potential for advancing quality analysis in weather prediction.", "AI": {"tldr": "RadarQA\u662f\u4e00\u4e2a\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6c14\u8c61\u9884\u62a5\u8d28\u91cf\u5206\u6790\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u7269\u7406\u5c5e\u6027\u548c\u8be6\u7ec6\u8bc4\u4f30\u62a5\u544a\uff0c\u5728\u96f7\u8fbe\u9884\u62a5\u8d28\u91cf\u8bc4\u4f30\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u901a\u7528MLLM\u6a21\u578b\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u5206\u6570\u7684\u8bc4\u4f30\u6307\u6807\u5728\u63cf\u8ff0\u80fd\u529b\u3001\u53ef\u89e3\u91ca\u6027\u548c\u52a8\u6001\u6f14\u5316\u7406\u89e3\u65b9\u9762\u8fdc\u4e0d\u5982\u6c14\u8c61\u4e13\u5bb6\uff0c\u9700\u8981\u5229\u7528MLLM\u6280\u672f\u6765\u514b\u670d\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u591a\u6a21\u6001\u8d28\u91cf\u5206\u6790\u7684\u521b\u65b0\u4efb\u52a1\u8303\u5f0f\uff0c\u5305\u542b\u5355\u5e27\u548c\u5e8f\u5217\u5206\u6790\u3001\u8bc4\u5206\u548c\u8bc4\u4f30\u573a\u666f\uff1b\u8bbe\u8ba1\u4e86\u7ed3\u5408\u4eba\u5de5\u4e13\u5bb6\u6807\u6ce8\u548c\u81ea\u52a8\u542f\u53d1\u5f0f\u7684\u6df7\u5408\u6807\u6ce8\u6d41\u7a0b\uff0c\u6784\u5efa\u4e86RQA-70K\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff1b\u91c7\u7528\u591a\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u8fed\u4ee3\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "result": "RadarQA\u5728\u6240\u6709\u8bc4\u4f30\u8bbe\u7f6e\u4e2d\u90fd\u4f18\u4e8e\u73b0\u6709\u7684\u901a\u7528MLLM\uff0c\u5c55\u793a\u4e86\u5728\u5929\u6c14\u9884\u62a5\u8d28\u91cf\u5206\u6790\u65b9\u9762\u7684\u6f5c\u529b\u3002", "conclusion": "RadarQA\u65b9\u6cd5\u901a\u8fc7\u6574\u5408\u591a\u6a21\u6001\u5206\u6790\u548c\u4e13\u5bb6\u77e5\u8bc6\uff0c\u4e3a\u6c14\u8c61\u9884\u62a5\u8d28\u91cf\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u5de5\u5177\uff0c\u6709\u671b\u63a8\u52a8\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.12274", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.12274", "abs": "https://arxiv.org/abs/2508.12274", "authors": ["Jian Zhao", "Yunlong Lian", "Andy M Tyrrell", "Michael Gienger", "Jihong Zhu"], "title": "Bimanual Robot-Assisted Dressing: A Spherical Coordinate-Based Strategy for Tight-Fitting Garments", "comment": "8 pages, 41 figures", "summary": "Robot-assisted dressing is a popular but challenging topic in the field of\nrobotic manipulation, offering significant potential to improve the quality of\nlife for individuals with mobility limitations. Currently, the majority of\nresearch on robot-assisted dressing focuses on how to put on loose-fitting\nclothing, with little attention paid to tight garments. For the former, since\nthe armscye is larger, a single robotic arm can usually complete the dressing\ntask successfully. However, for the latter, dressing with a single robotic arm\noften fails due to the narrower armscye and the property of diminishing\nrigidity in the armscye, which eventually causes the armscye to get stuck. This\npaper proposes a bimanual dressing strategy suitable for dressing tight-fitting\nclothing. To facilitate the encoding of dressing trajectories that adapt to\ndifferent human arm postures, a spherical coordinate system for dressing is\nestablished. We uses the azimuthal angle of the spherical coordinate system as\na task-relevant feature for bimanual manipulation. Based on this new\ncoordinate, we employ Gaussian Mixture Model (GMM) and Gaussian Mixture\nRegression (GMR) for imitation learning of bimanual dressing trajectories,\ngenerating dressing strategies that adapt to different human arm postures. The\neffectiveness of the proposed method is validated through various experiments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9002\u7528\u4e8e\u7d27\u8eab\u670d\u88c5\u7684\u53cc\u624b\u673a\u5668\u4eba\u7a7f\u8863\u7b56\u7565\uff0c\u901a\u8fc7\u5efa\u7acb\u7403\u5750\u6807\u7cfb\u548c\u4f7f\u7528GMM/GMR\u6a21\u4eff\u5b66\u4e60\uff0c\u89e3\u51b3\u4e86\u5355\u624b\u673a\u5668\u4eba\u65e0\u6cd5\u5b8c\u6210\u7d27\u8eab\u8863\u7269\u7a7f\u7740\u7684\u96be\u9898\u3002", "motivation": "\u5f53\u524d\u673a\u5668\u4eba\u8f85\u52a9\u7a7f\u8863\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u5bbd\u677e\u8863\u7269\uff0c\u5bf9\u7d27\u8eab\u8863\u7269\u7684\u5173\u6ce8\u5f88\u5c11\u3002\u5355\u624b\u673a\u5668\u4eba\u7531\u4e8e\u8863\u8896\u5b54\u8f83\u5c0f\u4e14\u521a\u6027\u9012\u51cf\u7279\u6027\uff0c\u7ecf\u5e38\u5bfc\u81f4\u7a7f\u8863\u5931\u8d25\uff0c\u9700\u8981\u5f00\u53d1\u53cc\u624b\u673a\u5668\u4eba\u7b56\u7565\u3002", "method": "\u5efa\u7acb\u7a7f\u8863\u7403\u5750\u6807\u7cfb\uff0c\u4ee5\u65b9\u4f4d\u89d2\u4f5c\u4e3a\u4efb\u52a1\u76f8\u5173\u7279\u5f81\uff1b\u4f7f\u7528\u9ad8\u65af\u6df7\u5408\u6a21\u578b(GMM)\u548c\u9ad8\u65af\u6df7\u5408\u56de\u5f52(GMR)\u8fdb\u884c\u53cc\u624b\u673a\u5668\u4eba\u7a7f\u8863\u8f68\u8ff9\u7684\u6a21\u4eff\u5b66\u4e60\uff0c\u751f\u6210\u9002\u5e94\u4e0d\u540c\u4eba\u4f53\u624b\u81c2\u59ff\u52bf\u7684\u7a7f\u8863\u7b56\u7565\u3002", "result": "\u901a\u8fc7\u5404\u79cd\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u8bc1\u660e\u4e86\u53cc\u624b\u673a\u5668\u4eba\u7b56\u7565\u80fd\u591f\u6210\u529f\u5b8c\u6210\u7d27\u8eab\u8863\u7269\u7684\u7a7f\u7740\u4efb\u52a1\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8e\u7403\u5750\u6807\u7cfb\u7684\u53cc\u624b\u673a\u5668\u4eba\u7a7f\u8863\u7b56\u7565\u4e3a\u89e3\u51b3\u7d27\u8eab\u8863\u7269\u7a7f\u7740\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u6a21\u4eff\u5b66\u4e60\u80fd\u591f\u9002\u5e94\u4e0d\u540c\u7684\u4eba\u4f53\u59ff\u52bf\u53d8\u5316\u3002"}}
{"id": "2508.12338", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12338", "abs": "https://arxiv.org/abs/2508.12338", "authors": ["Wenzhen Yuan", "Shengji Tang", "Weihao Lin", "Jiacheng Ruan", "Ganqu Cui", "Bo Zhang", "Tao Chen", "Ting Liu", "Yuzhuo Fu", "Peng Ye", "Lei Bai"], "title": "Wisdom of the Crowd: Reinforcement Learning from Coevolutionary Collective Feedback", "comment": null, "summary": "Reinforcement learning (RL) has significantly enhanced the reasoning\ncapabilities of large language models (LLMs), but its reliance on expensive\nhuman-labeled data or complex reward models severely limits scalability. While\nexisting self-feedback methods aim to address this problem, they are\nconstrained by the capabilities of a single model, which can lead to\noverconfidence in incorrect answers, reward hacking, and even training\ncollapse. To this end, we propose Reinforcement Learning from Coevolutionary\nCollective Feedback (RLCCF), a novel RL framework that enables multi-model\ncollaborative evolution without external supervision. Specifically, RLCCF\noptimizes the ability of a model collective by maximizing its Collective\nConsistency (CC), which jointly trains a diverse ensemble of LLMs and provides\nreward signals by voting on collective outputs. Moreover, each model's vote is\nweighted by its Self-Consistency (SC) score, ensuring that more confident\nmodels contribute more to the collective decision. Benefiting from the diverse\noutput distributions and complementary abilities of multiple LLMs, RLCCF\nenables the model collective to continuously enhance its reasoning ability\nthrough coevolution. Experiments on four mainstream open-source LLMs across\nfour mathematical reasoning benchmarks demonstrate that our framework yields\nsignificant performance gains, achieving an average relative improvement of\n16.72\\% in accuracy. Notably, RLCCF not only improves the performance of\nindividual models but also enhances the group's majority-voting accuracy by\n4.51\\%, demonstrating its ability to extend the collective capability boundary\nof the model collective.", "AI": {"tldr": "RLCCF\u662f\u4e00\u4e2a\u65e0\u9700\u5916\u90e8\u76d1\u7763\u7684\u591a\u6a21\u578b\u534f\u4f5c\u8fdb\u5316\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6700\u5927\u5316\u96c6\u4f53\u4e00\u81f4\u6027\u6765\u63d0\u5347LLM\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5728\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u4e0a\u5e73\u5747\u63d0\u534716.72%\u7684\u51c6\u786e\u7387", "motivation": "\u89e3\u51b3\u4f20\u7edfRL\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u548c\u590d\u6742\u5956\u52b1\u6a21\u578b\u7684\u95ee\u9898\uff0c\u4ee5\u53ca\u73b0\u6709\u81ea\u53cd\u9988\u65b9\u6cd5\u53d7\u9650\u4e8e\u5355\u6a21\u578b\u80fd\u529b\u5bfc\u81f4\u7684\u8fc7\u5ea6\u81ea\u4fe1\u3001\u5956\u52b1\u653b\u51fb\u548c\u8bad\u7ec3\u5d29\u6e83\u7b49\u95ee\u9898", "method": "\u63d0\u51fa\u57fa\u4e8e\u534f\u540c\u8fdb\u5316\u96c6\u4f53\u53cd\u9988\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6295\u7968\u673a\u5236\u8bad\u7ec3\u591a\u6837\u5316LLM\u96c6\u5408\uff0c\u5229\u7528\u81ea\u4e00\u81f4\u6027\u8bc4\u5206\u52a0\u6743\u6295\u7968\u63d0\u4f9b\u5956\u52b1\u4fe1\u53f7\uff0c\u5b9e\u73b0\u591a\u6a21\u578b\u534f\u4f5c\u8fdb\u5316", "result": "\u5728\u56db\u4e2a\u4e3b\u6d41\u5f00\u6e90LLM\u548c\u56db\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5e73\u5747\u76f8\u5bf9\u51c6\u786e\u7387\u63d0\u534716.72%\uff0c\u7fa4\u4f53\u591a\u6570\u6295\u7968\u51c6\u786e\u7387\u63d0\u53474.51%", "conclusion": "RLCCF\u4e0d\u4ec5\u80fd\u63d0\u5347\u5355\u4e2a\u6a21\u578b\u6027\u80fd\uff0c\u8fd8\u80fd\u6269\u5c55\u6a21\u578b\u96c6\u4f53\u7684\u80fd\u529b\u8fb9\u754c\uff0c\u4e3a\u65e0\u76d1\u7763\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.12296", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.12296", "abs": "https://arxiv.org/abs/2508.12296", "authors": ["Bin Wang", "Jiwen Zhang", "Song Wang", "Dan Wu"], "title": "A robust and compliant robotic assembly control strategy for batch precision assembly task with uncertain fit types and fit amounts", "comment": null, "summary": "In some high-precision industrial applications, robots are deployed to\nperform precision assembly tasks on mass batches of manufactured pegs and\nholes. If the peg and hole are designed with transition fit, machining errors\nmay lead to either a clearance or an interference fit for a specific pair of\ncomponents, with uncertain fit amounts. This paper focuses on the robotic batch\nprecision assembly task involving components with uncertain fit types and fit\namounts, and proposes an efficient methodology to construct the robust and\ncompliant assembly control strategy. Specifically, the batch precision assembly\ntask is decomposed into multiple deterministic subtasks, and a force-vision\nfusion controller-driven reinforcement learning method and a multi-task\nreinforcement learning training method (FVFC-MTRL) are proposed to jointly\nlearn multiple compliance control strategies for these subtasks. Subsequently,\nthe multi-teacher policy distillation approach is designed to integrate\nmultiple trained strategies into a unified student network, thereby\nestablishing a robust control strategy. Real-world experiments demonstrate that\nthe proposed method successfully constructs the robust control strategy for\nhigh-precision assembly task with different fit types and fit amounts.\nMoreover, the MTRL framework significantly improves training efficiency, and\nthe final developed control strategy achieves superior force compliance and\nhigher success rate compared with many existing methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u529b-\u89c6\u89c9\u878d\u5408\u63a7\u5236\u5668\u9a71\u52a8\u7684\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5(FVFC-MTRL)\uff0c\u7528\u4e8e\u89e3\u51b3\u673a\u5668\u4eba\u6279\u91cf\u7cbe\u5bc6\u88c5\u914d\u4e2d\u914d\u5408\u7c7b\u578b\u548c\u914d\u5408\u91cf\u4e0d\u786e\u5b9a\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u591a\u6559\u5e08\u7b56\u7565\u84b8\u998f\u6574\u5408\u591a\u4e2a\u63a7\u5236\u7b56\u7565\uff0c\u5b9e\u73b0\u9c81\u68d2\u88c5\u914d\u63a7\u5236\u3002", "motivation": "\u5728\u9ad8\u7cbe\u5ea6\u5de5\u4e1a\u5e94\u7528\u4e2d\uff0c\u673a\u5668\u4eba\u5728\u6267\u884c\u7cbe\u5bc6\u88c5\u914d\u4efb\u52a1\u65f6\uff0c\u7531\u4e8e\u52a0\u5de5\u8bef\u5dee\u5bfc\u81f4\u9500\u5b54\u914d\u5408\u7c7b\u578b\uff08\u95f4\u9699\u914d\u5408\u6216\u8fc7\u76c8\u914d\u5408\uff09\u548c\u914d\u5408\u91cf\u5177\u6709\u4e0d\u786e\u5b9a\u6027\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u8fd9\u79cd\u53d8\u5316\u3002", "method": "\u5c06\u6279\u91cf\u7cbe\u5bc6\u88c5\u914d\u4efb\u52a1\u5206\u89e3\u4e3a\u591a\u4e2a\u786e\u5b9a\u6027\u5b50\u4efb\u52a1\uff0c\u91c7\u7528\u529b-\u89c6\u89c9\u878d\u5408\u63a7\u5236\u5668\u9a71\u52a8\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u7ed3\u5408\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u548c\u591a\u6559\u5e08\u7b56\u7565\u84b8\u998f\uff0c\u5c06\u591a\u4e2a\u8bad\u7ec3\u7b56\u7565\u6574\u5408\u5230\u7edf\u4e00\u7684\u5b66\u751f\u7f51\u7edc\u4e2d\u3002", "result": "\u771f\u5b9e\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6210\u529f\u6784\u5efa\u4e86\u9488\u5bf9\u4e0d\u540c\u914d\u5408\u7c7b\u578b\u548c\u914d\u5408\u91cf\u7684\u9c81\u68d2\u63a7\u5236\u7b56\u7565\uff0cMTRL\u6846\u67b6\u663e\u8457\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u7387\uff0c\u6700\u7ec8\u7684\u63a7\u5236\u7b56\u7565\u5728\u529b\u67d4\u987a\u6027\u548c\u6210\u529f\u7387\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684FVFC-MTRL\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5904\u7406\u88c5\u914d\u4efb\u52a1\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u901a\u8fc7\u591a\u4efb\u52a1\u5b66\u4e60\u548c\u7b56\u7565\u84b8\u998f\u5b9e\u73b0\u4e86\u9ad8\u6548\u8bad\u7ec3\u548c\u9c81\u68d2\u63a7\u5236\uff0c\u4e3a\u9ad8\u7cbe\u5ea6\u5de5\u4e1a\u88c5\u914d\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.12375", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12375", "abs": "https://arxiv.org/abs/2508.12375", "authors": ["Yu Sha", "Shuiping Gou", "Bo Liu", "Johannes Faber", "Ningtao Liu", "Stefan Schramm", "Horst Stoecker", "Thomas Steckenreiter", "Domagoj Vnucec", "Nadine Wetzstein", "Andreas Widl", "Kai Zhou"], "title": "Hierarchical knowledge guided fault intensity diagnosis of complex industrial systems", "comment": "12 pages", "summary": "Fault intensity diagnosis (FID) plays a pivotal role in monitoring and\nmaintaining mechanical devices within complex industrial systems. As current\nFID methods are based on chain of thought without considering dependencies\namong target classes. To capture and explore dependencies, we propose a\nhierarchical knowledge guided fault intensity diagnosis framework (HKG)\ninspired by the tree of thought, which is amenable to any representation\nlearning methods. The HKG uses graph convolutional networks to map the\nhierarchical topological graph of class representations into a set of\ninterdependent global hierarchical classifiers, where each node is denoted by\nword embeddings of a class. These global hierarchical classifiers are applied\nto learned deep features extracted by representation learning, allowing the\nentire model to be end-to-end learnable. In addition, we develop a re-weighted\nhierarchical knowledge correlation matrix (Re-HKCM) scheme by embedding\ninter-class hierarchical knowledge into a data-driven statistical correlation\nmatrix (SCM) which effectively guides the information sharing of nodes in\ngraphical convolutional neural networks and avoids over-smoothing issues. The\nRe-HKCM is derived from the SCM through a series of mathematical\ntransformations. Extensive experiments are performed on four real-world\ndatasets from different industrial domains (three cavitation datasets from\nSAMSON AG and one existing publicly) for FID, all showing superior results and\noutperform recent state-of-the-art FID methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5c42\u6b21\u77e5\u8bc6\u5f15\u5bfc\u7684\u6545\u969c\u5f3a\u5ea6\u8bca\u65ad\u6846\u67b6HKG\uff0c\u901a\u8fc7\u56fe\u5377\u79ef\u7f51\u7edc\u548c\u91cd\u65b0\u52a0\u6743\u7684\u5c42\u6b21\u77e5\u8bc6\u76f8\u5173\u77e9\u9635\u6765\u6355\u83b7\u7c7b\u522b\u95f4\u4f9d\u8d56\u5173\u7cfb\uff0c\u5728\u591a\u4e2a\u5de5\u4e1a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u6545\u969c\u5f3a\u5ea6\u8bca\u65ad\u65b9\u6cd5\u57fa\u4e8e\u94fe\u5f0f\u601d\u7ef4\uff0c\u6ca1\u6709\u8003\u8651\u76ee\u6807\u7c7b\u522b\u4e4b\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u9650\u5236\u4e86\u8bca\u65ad\u6027\u80fd\u3002", "method": "\u4f7f\u7528\u56fe\u5377\u79ef\u7f51\u7edc\u5c06\u7c7b\u522b\u8868\u793a\u7684\u5c42\u6b21\u62d3\u6251\u56fe\u6620\u5c04\u5230\u4e00\u7ec4\u76f8\u4e92\u4f9d\u8d56\u7684\u5168\u5c40\u5c42\u6b21\u5206\u7c7b\u5668\uff0c\u6bcf\u4e2a\u8282\u70b9\u7528\u7c7b\u522b\u8bcd\u5d4c\u5165\u8868\u793a\uff1b\u5f00\u53d1\u4e86\u91cd\u65b0\u52a0\u6743\u7684\u5c42\u6b21\u77e5\u8bc6\u76f8\u5173\u77e9\u9635\u65b9\u6848\uff0c\u5c06\u7c7b\u95f4\u5c42\u6b21\u77e5\u8bc6\u5d4c\u5165\u5230\u6570\u636e\u9a71\u52a8\u7684\u7edf\u8ba1\u76f8\u5173\u77e9\u9635\u4e2d\u3002", "result": "\u5728\u56db\u4e2a\u771f\u5b9e\u5de5\u4e1a\u6570\u636e\u96c6\uff08\u4e09\u4e2a\u6765\u81eaSAMSON AG\u7684\u7a7a\u5316\u6570\u636e\u96c6\u548c\u4e00\u4e2a\u516c\u5f00\u6570\u636e\u96c6\uff09\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u6240\u6709\u6570\u636e\u96c6\u4e0a\u90fd\u53d6\u5f97\u4e86\u4f18\u5f02\u7ed3\u679c\uff0c\u8d85\u8d8a\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u7684FID\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684HKG\u6846\u67b6\u80fd\u591f\u6709\u6548\u6355\u83b7\u548c\u63a2\u7d22\u7c7b\u522b\u95f4\u4f9d\u8d56\u5173\u7cfb\uff0c\u901a\u8fc7\u5c42\u6b21\u77e5\u8bc6\u5f15\u5bfc\u548c\u91cd\u65b0\u52a0\u6743\u7684\u76f8\u5173\u77e9\u9635\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6545\u969c\u5f3a\u5ea6\u8bca\u65ad\u7684\u6027\u80fd\uff0c\u4e3a\u590d\u6742\u5de5\u4e1a\u7cfb\u7edf\u7684\u8bbe\u5907\u76d1\u63a7\u548c\u7ef4\u62a4\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.12312", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.12312", "abs": "https://arxiv.org/abs/2508.12312", "authors": ["Marco Leon Rapp"], "title": "Implementation and evaluation of a prediction algorithm for an autonomous vehicle", "comment": "7 pages, 7 figures", "summary": "This paper presents a prediction algorithm that estimates the vehicle\ntrajectory every five milliseconds for an autonomous vehicle. A kinematic and a\ndynamic bicycle model are compared, with the dynamic model exhibiting superior\naccuracy at higher speeds. Vehicle parameters such as mass, center of gravity,\nmoment of inertia, and cornering stiffness are determined experimentally. For\ncornering stiffness, a novel measurement procedure using optical position\ntracking is introduced. The model is incorporated into an extended Kalman\nfilter and implemented in a ROS node in C++. The algorithm achieves a\npositional deviation of only 1.25 cm per meter over the entire test drive and\nis up to 82.6% more precise than the kinematic model.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u8f68\u8ff9\u9884\u6d4b\u7b97\u6cd5\uff0c\u4f7f\u7528\u52a8\u6001\u81ea\u884c\u8f66\u6a21\u578b\uff0c\u901a\u8fc7\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u5b9e\u73b0\uff0c\u6bcf5\u6beb\u79d2\u9884\u6d4b\u4e00\u6b21\u8f68\u8ff9\uff0c\u7cbe\u5ea6\u6bd4\u8fd0\u52a8\u5b66\u6a21\u578b\u63d0\u9ad882.6%", "motivation": "\u4e3a\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5f00\u53d1\u9ad8\u7cbe\u5ea6\u7684\u5b9e\u65f6\u8f68\u8ff9\u9884\u6d4b\u7b97\u6cd5\uff0c\u89e3\u51b3\u5728\u9ad8\u901f\u884c\u9a76\u65f6\u8fd0\u52a8\u5b66\u6a21\u578b\u7cbe\u5ea6\u4e0d\u8db3\u7684\u95ee\u9898", "method": "\u6bd4\u8f83\u4e86\u8fd0\u52a8\u5b66\u548c\u52a8\u6001\u81ea\u884c\u8f66\u6a21\u578b\uff0c\u901a\u8fc7\u5b9e\u9a8c\u786e\u5b9a\u8f66\u8f86\u53c2\u6570\uff08\u8d28\u91cf\u3001\u91cd\u5fc3\u3001\u8f6c\u52a8\u60ef\u91cf\u3001\u8f6c\u5411\u521a\u5ea6\uff09\uff0c\u5f15\u5165\u5149\u5b66\u4f4d\u7f6e\u8ddf\u8e2a\u6d4b\u91cf\u8f6c\u5411\u521a\u5ea6\u7684\u65b0\u65b9\u6cd5\uff0c\u5c06\u6a21\u578b\u96c6\u6210\u5230\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u4e2d\uff0c\u7528C++\u5728ROS\u8282\u70b9\u4e2d\u5b9e\u73b0", "result": "\u52a8\u6001\u6a21\u578b\u5728\u9ad8\u901f\u4e0b\u8868\u73b0\u51fa\u66f4\u4f18\u7684\u7cbe\u5ea6\uff0c\u6574\u4e2a\u6d4b\u8bd5\u8fc7\u7a0b\u4e2d\u4f4d\u7f6e\u504f\u5dee\u4ec5\u4e3a\u6bcf\u7c731.25\u5398\u7c73\uff0c\u6bd4\u8fd0\u52a8\u5b66\u6a21\u578b\u7cbe\u786e\u5ea6\u63d0\u9ad882.6%", "conclusion": "\u52a8\u6001\u81ea\u884c\u8f66\u6a21\u578b\u7ed3\u5408\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u80fd\u591f\u4e3a\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u63d0\u4f9b\u9ad8\u7cbe\u5ea6\u7684\u5b9e\u65f6\u8f68\u8ff9\u9884\u6d4b\uff0c\u7279\u522b\u662f\u5728\u9ad8\u901f\u884c\u9a76\u6761\u4ef6\u4e0b\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u8fd0\u52a8\u5b66\u6a21\u578b"}}
{"id": "2508.12379", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12379", "abs": "https://arxiv.org/abs/2508.12379", "authors": ["Rongzheng Wang", "Qizhi Chen", "Yihong Huang", "Yizhuo Ma", "Muquan Li", "Jiakai Li", "Ke Qin", "Guangchun Luo", "Shuang Liang"], "title": "GraphCogent: Overcoming LLMs' Working Memory Constraints via Multi-Agent Collaboration in Complex Graph Understanding", "comment": null, "summary": "Large language models (LLMs) show promising performance on small-scale graph\nreasoning tasks but fail when handling real-world graphs with complex queries.\nThis phenomenon stems from LLMs' inability to effectively process complex graph\ntopology and perform multi-step reasoning simultaneously. To address these\nlimitations, we propose GraphCogent, a collaborative agent framework inspired\nby human Working Memory Model that decomposes graph reasoning into specialized\ncognitive processes: sense, buffer, and execute. The framework consists of\nthree modules: Sensory Module standardizes diverse graph text representations\nvia subgraph sampling, Buffer Module integrates and indexes graph data across\nmultiple formats, and Execution Module combines tool calling and model\ngeneration for efficient reasoning. We also introduce Graph4real, a\ncomprehensive benchmark contains with four domains of real-world graphs (Web,\nSocial, Transportation, and Citation) to evaluate LLMs' graph reasoning\ncapabilities. Our Graph4real covers 21 different graph reasoning tasks,\ncategorized into three types (Structural Querying, Algorithmic Reasoning, and\nPredictive Modeling tasks), with graph scales that are 10 times larger than\nexisting benchmarks. Experiments show that Llama3.1-8B based GraphCogent\nachieves a 50% improvement over massive-scale LLMs like DeepSeek-R1 (671B).\nCompared to state-of-the-art agent-based baseline, our framework outperforms by\n20% in accuracy while reducing token usage by 80% for in-toolset tasks and 30%\nfor out-toolset tasks. Code will be available after review.", "AI": {"tldr": "GraphCogent\u662f\u4e00\u4e2a\u57fa\u4e8e\u5de5\u4f5c\u8bb0\u5fc6\u6a21\u578b\u7684\u534f\u4f5c\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u56fe\u63a8\u7406\u5206\u89e3\u4e3a\u611f\u77e5\u3001\u7f13\u51b2\u548c\u6267\u884c\u4e09\u4e2a\u8ba4\u77e5\u8fc7\u7a0b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u56fe\u62d3\u6251\u548c\u591a\u6b65\u63a8\u7406\u65f6\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5c0f\u89c4\u6a21\u56fe\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u5904\u7406\u5177\u6709\u590d\u6742\u67e5\u8be2\u7684\u771f\u5b9e\u4e16\u754c\u56fe\u65f6\u5931\u8d25\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u65e0\u6cd5\u540c\u65f6\u6709\u6548\u5904\u7406\u590d\u6742\u56fe\u62d3\u6251\u548c\u6267\u884c\u591a\u6b65\u63a8\u7406\u3002", "method": "\u63d0\u51faGraphCogent\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6a21\u5757\uff1a\u611f\u77e5\u6a21\u5757\u901a\u8fc7\u5b50\u56fe\u91c7\u6837\u6807\u51c6\u5316\u56fe\u6587\u672c\u8868\u793a\uff0c\u7f13\u51b2\u6a21\u5757\u96c6\u6210\u548c\u7d22\u5f15\u591a\u79cd\u683c\u5f0f\u7684\u56fe\u6570\u636e\uff0c\u6267\u884c\u6a21\u5757\u7ed3\u5408\u5de5\u5177\u8c03\u7528\u548c\u6a21\u578b\u751f\u6210\u8fdb\u884c\u9ad8\u6548\u63a8\u7406\u3002", "result": "\u57fa\u4e8eLlama3.1-8B\u7684GraphCogent\u76f8\u6bd4DeepSeek-R1(671B)\u6027\u80fd\u63d0\u534750%\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u57fa\u4e8e\u4ee3\u7406\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u51c6\u786e\u7387\u63d0\u9ad820%\uff0c\u540c\u65f6token\u4f7f\u7528\u91cf\u51cf\u5c1180%\uff08\u5de5\u5177\u96c6\u5185\u4efb\u52a1\uff09\u548c30%\uff08\u5de5\u5177\u96c6\u5916\u4efb\u52a1\uff09\u3002", "conclusion": "GraphCogent\u6846\u67b6\u901a\u8fc7\u8ba4\u77e5\u8fc7\u7a0b\u5206\u89e3\u6709\u6548\u63d0\u5347\u4e86LLMs\u7684\u56fe\u63a8\u7406\u80fd\u529b\uff0c\u5728\u5904\u7406\u5927\u89c4\u6a21\u771f\u5b9e\u56fe\u6570\u636e\u65f6\u5c55\u73b0\u51fa\u663e\u8457\u4f18\u52bf\uff0c\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u4e86\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u3002"}}
{"id": "2508.12335", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.12335", "abs": "https://arxiv.org/abs/2508.12335", "authors": ["Yunfan Gao", "Florian Messerer", "Niels van Duijkeren", "Rashmi Dabir", "Moritz Diehl"], "title": "Semi-Infinite Programming for Collision-Avoidance in Optimal and Model Predictive Control", "comment": "21 pages, 15 figures", "summary": "This paper presents a novel approach for collision avoidance in optimal and\nmodel predictive control, in which the environment is represented by a large\nnumber of points and the robot as a union of padded polygons. The conditions\nthat none of the points shall collide with the robot can be written in terms of\nan infinite number of constraints per obstacle point. We show that the\nresulting semi-infinite programming (SIP) optimal control problem (OCP) can be\nefficiently tackled through a combination of two methods: local reduction and\nan external active-set method. Specifically, this involves iteratively\nidentifying the closest point obstacles, determining the lower-level distance\nminimizer among all feasible robot shape parameters, and solving the\nupper-level finitely-constrained subproblems.\n  In addition, this paper addresses robust collision avoidance in the presence\nof ellipsoidal state uncertainties. Enforcing constraint satisfaction over all\npossible uncertainty realizations extends the dimension of constraint\ninfiniteness. The infinitely many constraints arising from translational\nuncertainty are handled by local reduction together with the robot shape\nparameterization, while rotational uncertainty is addressed via a backoff\nreformulation.\n  A controller implemented based on the proposed method is demonstrated on a\nreal-world robot running at 20Hz, enabling fast and collision-free navigation\nin tight spaces. An application to 3D collision avoidance is also demonstrated\nin simulation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u534a\u65e0\u9650\u89c4\u5212(SIP)\u7684\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u78b0\u649e\u907f\u514d\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c40\u90e8\u7ea6\u7b80\u548c\u5916\u90e8\u4e3b\u52a8\u96c6\u65b9\u6cd5\u9ad8\u6548\u5904\u7406\u65e0\u9650\u7ea6\u675f\u95ee\u9898\uff0c\u652f\u6301\u9c81\u68d2\u78b0\u649e\u907f\u514d\u548c3D\u5e94\u7528", "motivation": "\u4f20\u7edf\u78b0\u649e\u907f\u514d\u65b9\u6cd5\u5728\u5904\u7406\u5927\u91cf\u73af\u5883\u70b9\u548c\u673a\u5668\u4eba\u591a\u8fb9\u5f62\u8868\u793a\u65f6\u9762\u4e34\u65e0\u9650\u7ea6\u675f\u7684\u6311\u6218\uff0c\u9700\u8981\u9ad8\u6548\u7b97\u6cd5\u6765\u5904\u7406\u534a\u65e0\u9650\u89c4\u5212\u6700\u4f18\u63a7\u5236\u95ee\u9898", "method": "\u7ed3\u5408\u5c40\u90e8\u7ea6\u7b80\u548c\u5916\u90e8\u4e3b\u52a8\u96c6\u65b9\u6cd5\uff0c\u8fed\u4ee3\u8bc6\u522b\u6700\u8fd1\u70b9\u969c\u788d\u7269\uff0c\u786e\u5b9a\u673a\u5668\u4eba\u5f62\u72b6\u53c2\u6570\u7684\u8ddd\u79bb\u6700\u5c0f\u5316\u5668\uff0c\u6c42\u89e3\u4e0a\u5c42\u6709\u9650\u7ea6\u675f\u5b50\u95ee\u9898\uff1b\u5bf9\u4e8e\u4e0d\u786e\u5b9a\u6027\uff0c\u4f7f\u7528\u5c40\u90e8\u7ea6\u7b80\u5904\u7406\u5e73\u79fb\u4e0d\u786e\u5b9a\u6027\uff0c\u540e\u9000\u91cd\u6784\u5904\u7406\u65cb\u8f6c\u4e0d\u786e\u5b9a\u6027", "result": "\u5b9e\u73b0\u4e86\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a20Hz\u8fd0\u884c\u7684\u63a7\u5236\u5668\uff0c\u80fd\u591f\u5728\u72ed\u7a84\u7a7a\u95f4\u4e2d\u5feb\u901f\u65e0\u78b0\u649e\u5bfc\u822a\uff0c\u5e76\u5728\u4eff\u771f\u4e2d\u5c55\u793a\u4e863D\u78b0\u649e\u907f\u514d\u5e94\u7528", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u534a\u65e0\u9650\u89c4\u5212\u78b0\u649e\u907f\u514d\u95ee\u9898\uff0c\u652f\u6301\u9c81\u68d2\u63a7\u5236\u548c\u5b9e\u9645\u5e94\u7528\uff0c\u4e3a\u590d\u6742\u73af\u5883\u4e0b\u7684\u5b9e\u65f6\u78b0\u649e\u907f\u514d\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848"}}
{"id": "2508.12425", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.12425", "abs": "https://arxiv.org/abs/2508.12425", "authors": ["Phuong Minh Nguyen", "Tien Huu Dang", "Naoya Inoue"], "title": "Non-Iterative Symbolic-Aided Chain-of-Thought for Logical Reasoning", "comment": null, "summary": "This work introduces Symbolic-Aided Chain-of-Thought (CoT), an improved\napproach to standard CoT, for logical reasoning in large language models\n(LLMs). The key idea is to integrate lightweight symbolic representations into\nfew-shot prompts, structuring the inference steps with a consistent strategy to\nmake reasoning patterns more explicit within a non-iterative reasoning process.\nBy incorporating these symbolic structures, our method preserves the\ngeneralizability of standard prompting techniques while enhancing the\ntransparency, interpretability, and analyzability of LLM logical reasoning.\nExtensive experiments on four well-known logical reasoning benchmarks --\nProofWriter, FOLIO, ProntoQA, and LogicalDeduction, which cover diverse\nreasoning scenarios -- demonstrate the effectiveness of the proposed approach,\nparticularly in complex reasoning tasks that require navigating multiple\nconstraints or rules. Notably, Symbolic-Aided CoT consistently improves LLMs'\nreasoning capabilities across various model sizes and significantly outperforms\nconventional CoT on three out of four datasets, ProofWriter, ProntoQA, and\nLogicalDeduction.", "AI": {"tldr": "Symbolic-Aided Chain-of-Thought (CoT) \u901a\u8fc7\u5c06\u8f7b\u91cf\u7ea7\u7b26\u53f7\u8868\u793a\u6574\u5408\u5230\u5c11\u6837\u672c\u63d0\u793a\u4e2d\uff0c\u6539\u8fdb\u4e86\u6807\u51c6 CoT \u65b9\u6cd5\uff0c\u5728\u903b\u8f91\u63a8\u7406\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u903b\u8f91\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u900f\u660e\u5ea6\u3001\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u5206\u6790\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u6807\u51c6\u63d0\u793a\u6280\u672f\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u5728\u5c11\u6837\u672c\u63d0\u793a\u4e2d\u6574\u5408\u8f7b\u91cf\u7ea7\u7b26\u53f7\u8868\u793a\uff0c\u4f7f\u7528\u4e00\u81f4\u7684\u7b56\u7565\u6784\u5efa\u63a8\u7406\u6b65\u9aa4\uff0c\u4f7f\u63a8\u7406\u6a21\u5f0f\u5728\u975e\u8fed\u4ee3\u63a8\u7406\u8fc7\u7a0b\u4e2d\u66f4\u52a0\u660e\u786e\u3002", "result": "\u5728\u56db\u4e2a\u903b\u8f91\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\uff08ProofWriter\u3001FOLIO\u3001ProntoQA\u3001LogicalDeduction\uff09\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u5904\u7406\u591a\u91cd\u7ea6\u675f\u6216\u89c4\u5219\u7684\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\uff0c\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edf CoT \u65b9\u6cd5\u3002", "conclusion": "Symbolic-Aided CoT \u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86 LLMs \u7684\u903b\u8f91\u63a8\u7406\u80fd\u529b\uff0c\u5728\u4e0d\u540c\u6a21\u578b\u89c4\u6a21\u4e0b\u90fd\u8868\u73b0\u51fa\u4e00\u81f4\u7684\u6539\u8fdb\u6548\u679c\uff0c\u4e3a\u589e\u5f3a\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u900f\u660e\u5ea6\u63d0\u4f9b\u4e86\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2508.12394", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.12394", "abs": "https://arxiv.org/abs/2508.12394", "authors": ["Zichen Yan", "Rui Huang", "Lei He", "Shao Guo", "Lin Zhao"], "title": "SIGN: Safety-Aware Image-Goal Navigation for Autonomous Drones via Reinforcement Learning", "comment": null, "summary": "Image-goal navigation (ImageNav) tasks a robot with autonomously exploring an\nunknown environment and reaching a location that visually matches a given\ntarget image. While prior works primarily study ImageNav for ground robots,\nenabling this capability for autonomous drones is substantially more\nchallenging due to their need for high-frequency feedback control and global\nlocalization for stable flight. In this paper, we propose a novel sim-to-real\nframework that leverages visual reinforcement learning (RL) to achieve ImageNav\nfor drones. To enhance visual representation ability, our approach trains the\nvision backbone with auxiliary tasks, including image perturbations and future\ntransition prediction, which results in more effective policy training. The\nproposed algorithm enables end-to-end ImageNav with direct velocity control,\neliminating the need for external localization. Furthermore, we integrate a\ndepth-based safety module for real-time obstacle avoidance, allowing the drone\nto safely navigate in cluttered environments. Unlike most existing drone\nnavigation methods that focus solely on reference tracking or obstacle\navoidance, our framework supports comprehensive navigation\nbehaviors--autonomous exploration, obstacle avoidance, and image-goal\nseeking--without requiring explicit global mapping. Code and model checkpoints\nwill be released upon acceptance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u5f3a\u5316\u5b66\u4e60\u7684sim-to-real\u6846\u67b6\uff0c\u4f7f\u65e0\u4eba\u673a\u80fd\u591f\u5b9e\u73b0\u56fe\u50cf\u76ee\u6807\u5bfc\u822a\uff0c\u65e0\u9700\u5916\u90e8\u5b9a\u4f4d\uff0c\u901a\u8fc7\u8f85\u52a9\u4efb\u52a1\u589e\u5f3a\u89c6\u89c9\u8868\u793a\u80fd\u529b\uff0c\u5e76\u96c6\u6210\u4e86\u6df1\u5ea6\u5b89\u5168\u6a21\u5757\u8fdb\u884c\u5b9e\u65f6\u907f\u969c\u3002", "motivation": "\u73b0\u6709\u7684\u56fe\u50cf\u76ee\u6807\u5bfc\u822a\u7814\u7a76\u4e3b\u8981\u9488\u5bf9\u5730\u9762\u673a\u5668\u4eba\uff0c\u800c\u65e0\u4eba\u673a\u5bfc\u822a\u9762\u4e34\u66f4\u9ad8\u6311\u6218\uff0c\u9700\u8981\u9ad8\u9891\u53cd\u9988\u63a7\u5236\u548c\u5168\u5c40\u5b9a\u4f4d\u6765\u4fdd\u6301\u7a33\u5b9a\u98de\u884c\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u65e0\u4eba\u673a\u5728\u672a\u77e5\u73af\u5883\u4e2d\u81ea\u4e3b\u63a2\u7d22\u5e76\u5230\u8fbe\u76ee\u6807\u56fe\u50cf\u4f4d\u7f6e\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u89c6\u89c9\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u56fe\u50cf\u6270\u52a8\u548c\u672a\u6765\u8f6c\u6362\u9884\u6d4b\u7b49\u8f85\u52a9\u4efb\u52a1\u8bad\u7ec3\u89c6\u89c9\u9aa8\u5e72\u7f51\u7edc\uff0c\u589e\u5f3a\u89c6\u89c9\u8868\u793a\u80fd\u529b\u3002\u5b9e\u73b0\u7aef\u5230\u7aef\u7684\u56fe\u50cf\u76ee\u6807\u5bfc\u822a\uff0c\u76f4\u63a5\u8fdb\u884c\u901f\u5ea6\u63a7\u5236\uff0c\u65e0\u9700\u5916\u90e8\u5b9a\u4f4d\u3002\u96c6\u6210\u57fa\u4e8e\u6df1\u5ea6\u7684\u5b89\u5168\u6a21\u5757\u8fdb\u884c\u5b9e\u65f6\u969c\u788d\u7269\u907f\u907f\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u652f\u6301\u5168\u9762\u7684\u5bfc\u822a\u884c\u4e3a\uff0c\u5305\u62ec\u81ea\u4e3b\u63a2\u7d22\u3001\u969c\u788d\u7269\u907f\u907f\u548c\u56fe\u50cf\u76ee\u6807\u5bfb\u627e\uff0c\u800c\u4e0d\u9700\u8981\u663e\u5f0f\u7684\u5168\u5c40\u5730\u56fe\u6784\u5efa\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u65e0\u4eba\u673a\u56fe\u50cf\u76ee\u6807\u5bfc\u822a\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684sim-to-real\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u89c6\u89c9\u5f3a\u5316\u5b66\u4e60\u548c\u5b89\u5168\u6a21\u5757\u7684\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u5728\u6742\u4e71\u73af\u5883\u4e2d\u7684\u5b89\u5168\u5bfc\u822a\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2508.12472", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12472", "abs": "https://arxiv.org/abs/2508.12472", "authors": ["Yifang Tian", "Yaming Liu", "Zichun Chong", "Zihang Huang", "Hans-Arno Jacobsen"], "title": "GALA: Can Graph-Augmented Large Language Model Agentic Workflows Elevate Root Cause Analysis?", "comment": "12 pages, 5 figures", "summary": "Root cause analysis (RCA) in microservice systems is challenging, requiring\non-call engineers to rapidly diagnose failures across heterogeneous telemetry\nsuch as metrics, logs, and traces. Traditional RCA methods often focus on\nsingle modalities or merely rank suspect services, falling short of providing\nactionable diagnostic insights with remediation guidance. This paper introduces\nGALA, a novel multi-modal framework that combines statistical causal inference\nwith LLM-driven iterative reasoning for enhanced RCA. Evaluated on an\nopen-source benchmark, GALA achieves substantial improvements over\nstate-of-the-art methods of up to 42.22% accuracy. Our novel human-guided LLM\nevaluation score shows GALA generates significantly more causally sound and\nactionable diagnostic outputs than existing methods. Through comprehensive\nexperiments and a case study, we show that GALA bridges the gap between\nautomated failure diagnosis and practical incident resolution by providing both\naccurate root cause identification and human-interpretable remediation\nguidance.", "AI": {"tldr": "GALA\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u6846\u67b6\uff0c\u7ed3\u5408\u7edf\u8ba1\u56e0\u679c\u63a8\u7406\u548cLLM\u9a71\u52a8\u7684\u8fed\u4ee3\u63a8\u7406\uff0c\u7528\u4e8e\u5fae\u670d\u52a1\u7cfb\u7edf\u7684\u6839\u56e0\u5206\u6790\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u51c6\u786e\u7387\u63d0\u5347\u9ad8\u8fbe42.22%\uff0c\u5e76\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u8bca\u65ad\u548c\u4fee\u590d\u6307\u5bfc", "motivation": "\u4f20\u7edfRCA\u65b9\u6cd5\u901a\u5e38\u53ea\u5173\u6ce8\u5355\u4e00\u6a21\u6001\u6216\u4ec5\u5bf9\u53ef\u7591\u670d\u52a1\u8fdb\u884c\u6392\u5e8f\uff0c\u65e0\u6cd5\u63d0\u4f9b\u5177\u6709\u53ef\u64cd\u4f5c\u6027\u7684\u8bca\u65ad\u6d1e\u5bdf\u548c\u4fee\u590d\u6307\u5bfc", "method": "\u7ed3\u5408\u7edf\u8ba1\u56e0\u679c\u63a8\u7406\u4e0eLLM\u9a71\u52a8\u7684\u8fed\u4ee3\u63a8\u7406\u7684\u591a\u6a21\u6001\u6846\u67b6", "result": "\u5728\u5f00\u6e90\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523042.22%\u7684\u51c6\u786e\u7387\u63d0\u5347\uff0c\u751f\u6210\u66f4\u5177\u56e0\u679c\u5408\u7406\u6027\u548c\u53ef\u64cd\u4f5c\u6027\u7684\u8bca\u65ad\u8f93\u51fa", "conclusion": "GALA\u901a\u8fc7\u63d0\u4f9b\u51c6\u786e\u7684\u6839\u56e0\u8bc6\u522b\u548c\u4eba\u7c7b\u53ef\u7406\u89e3\u7684\u4fee\u590d\u6307\u5bfc\uff0c\u5f25\u5408\u4e86\u81ea\u52a8\u5316\u6545\u969c\u8bca\u65ad\u4e0e\u5b9e\u9645\u4e8b\u4ef6\u89e3\u51b3\u4e4b\u95f4\u7684\u5dee\u8ddd"}}
{"id": "2508.12395", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.12395", "abs": "https://arxiv.org/abs/2508.12395", "authors": ["Zihan Wang"], "title": "PUB: A Plasma-Propelled Ultra-Quiet Blimp with Two-DOF Vector Thrusting", "comment": null, "summary": "This study presents the design and control of a Plasma-propelled\nUltra-silence Blimp (PUB), a novel aerial robot employing plasma vector\npropulsion for ultra-quiet flight without mechanical propellers. The system\nutilizes a helium-lift platform for extended endurance and a four-layer ring\nasymmetric capacitor to generate ionic wind thrust. The modular propulsion\nunits allow flexible configuration to meet mission-specific requirements, while\na two-degree-of-freedom (DOF) head enables thrust vector control. A closed-loop\nslip control scheme is implemented for stable maneuvering. Flight experiments\ndemonstrate full-envelope capability, including take-off, climb, hover,\ndescent, and smooth landing, confirming the feasibility of plasma vector\npropulsion, the effectiveness of DOF vector control, and the stability of the\ncontrol system. Owing to its low acoustic signature, structural simplicity, and\nhigh maneuverability, PUB is well suited for noise-sensitive, enclosed, and\nnear-space applications.", "AI": {"tldr": "\u7b49\u79bb\u5b50\u63a8\u8fdb\u8d85\u9759\u97f3\u98de\u8247(PUB)\u91c7\u7528\u7b49\u79bb\u5b50\u77e2\u91cf\u63a8\u8fdb\u6280\u672f\uff0c\u65e0\u9700\u673a\u68b0\u87ba\u65cb\u6868\u5b9e\u73b0\u8d85\u9759\u97f3\u98de\u884c\uff0c\u5177\u6709\u6a21\u5757\u5316\u63a8\u8fdb\u5355\u5143\u548c\u53cc\u81ea\u7531\u5ea6\u77e2\u91cf\u63a7\u5236\uff0c\u9a8c\u8bc1\u4e86\u5168\u5305\u7ebf\u98de\u884c\u80fd\u529b", "motivation": "\u5f00\u53d1\u4e00\u79cd\u8d85\u9759\u97f3\u3001\u65e0\u673a\u68b0\u87ba\u65cb\u6868\u7684\u7a7a\u4e2d\u673a\u5668\u4eba\uff0c\u9002\u7528\u4e8e\u566a\u58f0\u654f\u611f\u3001\u5c01\u95ed\u548c\u8fd1\u7a7a\u95f4\u5e94\u7528\u573a\u666f", "method": "\u91c7\u7528\u6c26\u6c14\u5347\u529b\u5e73\u53f0\u548c\u56db\u5c42\u73af\u72b6\u4e0d\u5bf9\u79f0\u7535\u5bb9\u5668\u4ea7\u751f\u79bb\u5b50\u98ce\u63a8\u529b\uff0c\u6a21\u5757\u5316\u63a8\u8fdb\u5355\u5143\u8bbe\u8ba1\uff0c\u53cc\u81ea\u7531\u5ea6\u5934\u90e8\u5b9e\u73b0\u63a8\u529b\u77e2\u91cf\u63a7\u5236\uff0c\u95ed\u73af\u6ed1\u79fb\u63a7\u5236\u65b9\u6848\u4fdd\u8bc1\u7a33\u5b9a\u673a\u52a8", "result": "\u98de\u884c\u5b9e\u9a8c\u5c55\u793a\u4e86\u5b8c\u6574\u7684\u98de\u884c\u5305\u7ebf\u80fd\u529b\uff0c\u5305\u62ec\u8d77\u98de\u3001\u722c\u5347\u3001\u60ac\u505c\u3001\u4e0b\u964d\u548c\u5e73\u7a33\u7740\u9646\uff0c\u9a8c\u8bc1\u4e86\u7b49\u79bb\u5b50\u77e2\u91cf\u63a8\u8fdb\u7684\u53ef\u884c\u6027\u3001\u77e2\u91cf\u63a7\u5236\u7684\u6709\u6548\u6027\u548c\u63a7\u5236\u7cfb\u7edf\u7684\u7a33\u5b9a\u6027", "conclusion": "PUB\u51ed\u501f\u4f4e\u58f0\u5b66\u7279\u5f81\u3001\u7ed3\u6784\u7b80\u5355\u6027\u548c\u9ad8\u673a\u52a8\u6027\uff0c\u975e\u5e38\u9002\u5408\u566a\u58f0\u654f\u611f\u3001\u5c01\u95ed\u548c\u8fd1\u7a7a\u95f4\u5e94\u7528"}}
{"id": "2508.12480", "categories": ["cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.12480", "abs": "https://arxiv.org/abs/2508.12480", "authors": ["Constantin Ruhdorfer", "Matteo Bortoletto", "Andreas Bulling"], "title": "The Yokai Learning Environment: Tracking Beliefs Over Space and Time", "comment": "Presented at the the ToM IJCAI 2025 Workshop", "summary": "Developing collaborative AI hinges on Theory of Mind (ToM) - the ability to\nreason about the beliefs of others to build and maintain common ground.\nExisting ToM benchmarks, however, are restricted to passive observer settings\nor lack an assessment of how agents establish and maintain common ground over\ntime. To address these gaps, we introduce the Yokai Learning Environment (YLE)\n- a multi-agent reinforcement learning (RL) environment based on the\ncooperative card game Yokai. In the YLE, agents take turns peeking at hidden\ncards and moving them to form clusters based on colour. Success requires\ntracking evolving beliefs, remembering past observations, using hints as\ngrounded communication, and maintaining common ground with teammates. Our\nevaluation yields two key findings: First, current RL agents struggle to solve\nthe YLE, even when given access to perfect memory. Second, while belief\nmodelling improves performance, agents are still unable to effectively\ngeneralise to unseen partners or form accurate beliefs over longer games,\nexposing a reliance on brittle conventions rather than robust belief tracking.\nWe use the YLE to investigate research questions in belief modelling, memory,\npartner generalisation, and scaling to higher-order ToM.", "AI": {"tldr": "Yokai Learning Environment (YLE)\u662f\u4e00\u4e2a\u57fa\u4e8e\u5408\u4f5c\u5361\u724c\u6e38\u620f\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u73af\u5883\uff0c\u7528\u4e8e\u8bc4\u4f30\u667a\u80fd\u4f53\u7684\u5fc3\u667a\u7406\u8bba\u80fd\u529b\uff0c\u7279\u522b\u662f\u5efa\u7acb\u548c\u7ef4\u62a4\u5171\u540c\u57fa\u7840\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u5fc3\u667a\u7406\u8bba\u57fa\u51c6\u6d4b\u8bd5\u5c40\u9650\u4e8e\u88ab\u52a8\u89c2\u5bdf\u8005\u8bbe\u7f6e\uff0c\u7f3a\u4e4f\u5bf9\u667a\u80fd\u4f53\u5982\u4f55\u968f\u65f6\u95f4\u5efa\u7acb\u548c\u7ef4\u62a4\u5171\u540c\u57fa\u7840\u7684\u8bc4\u4f30\u3002", "method": "\u5f00\u53d1\u4e86Yokai Learning Environment (YLE) - \u4e00\u4e2a\u57fa\u4e8e\u5408\u4f5c\u5361\u724c\u6e38\u620fYokai\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u73af\u5883\uff0c\u667a\u80fd\u4f53\u9700\u8981\u67e5\u770b\u9690\u85cf\u5361\u7247\u3001\u79fb\u52a8\u5361\u7247\u5f62\u6210\u989c\u8272\u96c6\u7fa4\uff0c\u5e76\u8ddf\u8e2a\u4e0d\u65ad\u53d8\u5316\u7684\u4fe1\u5ff5\u3002", "result": "\u5f53\u524dRL\u667a\u80fd\u4f53\u5373\u4f7f\u5728\u62e5\u6709\u5b8c\u7f8e\u8bb0\u5fc6\u7684\u60c5\u51b5\u4e0b\u4e5f\u96be\u4ee5\u89e3\u51b3YLE\u4efb\u52a1\uff1b\u4fe1\u5ff5\u5efa\u6a21\u80fd\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u667a\u80fd\u4f53\u4ecd\u65e0\u6cd5\u6709\u6548\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u4f19\u4f34\u6216\u5f62\u6210\u957f\u671f\u51c6\u786e\u4fe1\u5ff5\u3002", "conclusion": "YLE\u63ed\u793a\u4e86\u667a\u80fd\u4f53\u4f9d\u8d56\u8106\u5f31\u7684\u60ef\u4f8b\u800c\u975e\u7a33\u5065\u7684\u4fe1\u5ff5\u8ffd\u8e2a\uff0c\u53ef\u7528\u4e8e\u7814\u7a76\u4fe1\u5ff5\u5efa\u6a21\u3001\u8bb0\u5fc6\u3001\u4f19\u4f34\u6cdb\u5316\u548c\u9ad8\u9636\u5fc3\u667a\u7406\u8bba\u7b49\u7814\u7a76\u95ee\u9898\u3002"}}
{"id": "2508.12435", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12435", "abs": "https://arxiv.org/abs/2508.12435", "authors": ["Deqing Song", "Weimin Yang", "Maryam Rezayati", "Hans Wernher van de Venn"], "title": "Tactile Gesture Recognition with Built-in Joint Sensors for Industrial Robots", "comment": null, "summary": "While gesture recognition using vision or robot skins is an active research\narea in Human-Robot Collaboration (HRC), this paper explores deep learning\nmethods relying solely on a robot's built-in joint sensors, eliminating the\nneed for external sensors. We evaluated various convolutional neural network\n(CNN) architectures and collected two datasets to study the impact of data\nrepresentation and model architecture on the recognition accuracy. Our results\nshow that spectrogram-based representations significantly improve accuracy,\nwhile model architecture plays a smaller role. We also tested generalization to\nnew robot poses, where spectrogram-based models performed better. Implemented\non a Franka Emika Research robot, two of our methods, STFT2DCNN and STT3DCNN,\nachieved over 95% accuracy in contact detection and gesture classification.\nThese findings demonstrate the feasibility of external-sensor-free tactile\nrecognition and promote further research toward cost-effective, scalable\nsolutions for HRC.", "AI": {"tldr": "\u672c\u6587\u63a2\u7d22\u4ec5\u4f7f\u7528\u673a\u5668\u4eba\u5185\u7f6e\u5173\u8282\u4f20\u611f\u5668\u7684\u6df1\u5ea6\u5b66\u4e60\u624b\u52bf\u8bc6\u522b\u65b9\u6cd5\uff0c\u65e0\u9700\u5916\u90e8\u4f20\u611f\u5668\uff0c\u901a\u8fc7CNN\u67b6\u6784\u548c\u4e24\u79cd\u6570\u636e\u96c6\u8bc4\u4f30\u6570\u636e\u8868\u793a\u548c\u6a21\u578b\u67b6\u6784\u5bf9\u8bc6\u522b\u7cbe\u5ea6\u7684\u5f71\u54cd\u3002", "motivation": "\u5728HRC\u9886\u57df\uff0c\u867d\u7136\u57fa\u4e8e\u89c6\u89c9\u6216\u673a\u5668\u4eba\u76ae\u80a4\u7684\u624b\u52bf\u8bc6\u522b\u7814\u7a76\u6d3b\u8dc3\uff0c\u4f46\u672c\u6587\u65e8\u5728\u63a2\u7d22\u4ec5\u4f9d\u8d56\u673a\u5668\u4eba\u5185\u7f6e\u5173\u8282\u4f20\u611f\u5668\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u6d88\u9664\u5bf9\u5916\u90e8\u4f20\u611f\u5668\u7684\u9700\u6c42\uff0c\u4ee5\u964d\u4f4e\u6210\u672c\u548c\u63d0\u9ad8\u53ef\u6269\u5c55\u6027\u3002", "method": "\u8bc4\u4f30\u4e86\u591a\u79cd\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u67b6\u6784\uff0c\u6536\u96c6\u4e86\u4e24\u4e2a\u6570\u636e\u96c6\u6765\u7814\u7a76\u6570\u636e\u8868\u793a\u548c\u6a21\u578b\u67b6\u6784\u5bf9\u8bc6\u522b\u7cbe\u5ea6\u7684\u5f71\u54cd\uff0c\u7279\u522b\u5173\u6ce8\u57fa\u4e8e\u9891\u8c31\u56fe\u7684\u8868\u793a\u65b9\u6cd5\u3002", "result": "\u9891\u8c31\u56fe\u8868\u793a\u663e\u8457\u63d0\u9ad8\u4e86\u8bc6\u522b\u7cbe\u5ea6\uff0c\u800c\u6a21\u578b\u67b6\u6784\u7684\u5f71\u54cd\u8f83\u5c0f\u3002\u5728Franka Emika Research\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u7684STFT2DCNN\u548cSTT3DCNN\u65b9\u6cd5\u5728\u63a5\u89e6\u68c0\u6d4b\u548c\u624b\u52bf\u5206\u7c7b\u65b9\u9762\u8fbe\u5230\u8d85\u8fc795%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u7814\u7a76\u8bc1\u660e\u4e86\u65e0\u9700\u5916\u90e8\u4f20\u611f\u5668\u7684\u89e6\u89c9\u8bc6\u522b\u7684\u53ef\u884c\u6027\uff0c\u4fc3\u8fdb\u4e86HRC\u9886\u57df\u6210\u672c\u6548\u76ca\u9ad8\u3001\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\u7684\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2508.12487", "categories": ["cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.12487", "abs": "https://arxiv.org/abs/2508.12487", "authors": ["Lida Shahbandari", "Hossein Mohseni"], "title": "Advanced DOA Regulation with a Whale-Optimized Fractional Order Fuzzy PID Framework", "comment": null, "summary": "This study introduces a Fractional Order Fuzzy PID (FOFPID) controller that\nuses the Whale Optimization Algorithm (WOA) to manage the Bispectral Index\n(BIS), keeping it within the ideal range of forty to sixty. The FOFPID\ncontroller combines fuzzy logic for adapting to changes and fractional order\ndynamics for fine tuning. This allows it to adjust its control gains to handle\na person's unique physiology. The WOA helps fine tune the controller's\nparameters, including the fractional orders and the fuzzy membership functions,\nwhich boosts its performance. Tested on models of eight different patient\nprofiles, the FOFPID controller performed better than a standard Fractional\nOrder PID (FOPID) controller. It achieved faster settling times, at two and a\nhalf minutes versus three point two minutes, and had a lower steady state\nerror, at zero point five versus one point two. These outcomes show the\nFOFPID's excellent strength and accuracy. It offers a scalable, artificial\nintelligence driven solution for automated anesthesia delivery that could\nenhance clinical practice and improve patient results.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9cb8\u9c7c\u4f18\u5316\u7b97\u6cd5\u7684\u5206\u6570\u9636\u6a21\u7ccaPID\u63a7\u5236\u5668\uff0c\u7528\u4e8e\u7cbe\u786e\u63a7\u5236\u8111\u7535\u53cc\u9891\u6307\u6570\u572840-60\u7684\u7406\u60f3\u8303\u56f4\u5185\uff0c\u76f8\u6bd4\u4f20\u7edf\u5206\u6570\u9636PID\u63a7\u5236\u5668\u5177\u6709\u66f4\u5feb\u7684\u54cd\u5e94\u901f\u5ea6\u548c\u66f4\u4f4e\u7684\u7a33\u6001\u8bef\u5dee\u3002", "motivation": "\u9ebb\u9189\u6df1\u5ea6\u63a7\u5236\u5bf9\u624b\u672f\u5b89\u5168\u548c\u60a3\u8005\u6062\u590d\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edfPID\u63a7\u5236\u5668\u96be\u4ee5\u9002\u5e94\u60a3\u8005\u4e2a\u4f53\u751f\u7406\u5dee\u5f02\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u81ea\u52a8\u8c03\u6574\u53c2\u6570\u3001\u5904\u7406\u975e\u7ebf\u6027\u7279\u6027\u7684\u667a\u80fd\u63a7\u5236\u65b9\u6848\u3002", "method": "\u7ed3\u5408\u6a21\u7cca\u903b\u8f91\u7684\u81ea\u9002\u5e94\u80fd\u529b\u548c\u5206\u6570\u9636\u5fae\u79ef\u5206\u7684\u7cbe\u7ec6\u8c03\u8282\u7279\u6027\uff0c\u4f7f\u7528\u9cb8\u9c7c\u4f18\u5316\u7b97\u6cd5(WOA)\u81ea\u52a8\u4f18\u5316\u63a7\u5236\u5668\u53c2\u6570\uff08\u5305\u62ec\u5206\u6570\u9636\u9636\u6b21\u548c\u6a21\u7cca\u96b6\u5c5e\u5ea6\u51fd\u6570\uff09\uff0c\u57288\u79cd\u4e0d\u540c\u60a3\u8005\u6a21\u578b\u4e0a\u8fdb\u884c\u6d4b\u8bd5\u9a8c\u8bc1\u3002", "result": "FOFPID\u63a7\u5236\u5668\u76f8\u6bd4\u4f20\u7edfFOPID\u63a7\u5236\u5668\u8868\u73b0\u66f4\u4f18\uff1a\u8c03\u8282\u65f6\u95f4\u4ece3.2\u5206\u949f\u7f29\u77ed\u52302.5\u5206\u949f\uff0c\u7a33\u6001\u8bef\u5dee\u4ece1.2\u964d\u4f4e\u52300.5\uff0c\u663e\u793a\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u548c\u63a7\u5236\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u5206\u6570\u9636\u6a21\u7ccaPID\u63a7\u5236\u5668\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u3001\u4eba\u5de5\u667a\u80fd\u9a71\u52a8\u7684\u81ea\u52a8\u5316\u9ebb\u9189\u8f93\u9001\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u671b\u6539\u5584\u4e34\u5e8a\u5b9e\u8df5\u548c\u60a3\u8005\u6cbb\u7597\u6548\u679c\u3002"}}
{"id": "2508.12439", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.12439", "abs": "https://arxiv.org/abs/2508.12439", "authors": ["Sunyu Wang", "Arjun S. Lakshmipathy", "Jean Oh", "Nancy S. Pollard"], "title": "Geodesic Tracing-Based Kinematic Integration of Rolling and Sliding Contact on Manifold Meshes for Dexterous In-Hand Manipulation", "comment": null, "summary": "Reasoning about rolling and sliding contact, or roll-slide contact for short,\nis critical for dexterous manipulation tasks that involve intricate geometries.\nBut existing works on roll-slide contact mostly focus on continuous shapes with\ndifferentiable parametrizations. This work extends roll-slide contact modeling\nto manifold meshes. Specifically, we present an integration scheme based on\ngeodesic tracing to first-order time-integrate roll-slide contact directly on\nmeshes, enabling dexterous manipulation to reason over high-fidelity discrete\nrepresentations of an object's true geometry. Using our method, we planned\ndexterous motions of a multi-finger robotic hand manipulating five objects\nin-hand in simulation. The planning was achieved with a least-squares optimizer\nthat strives to maintain the most stable instantaneous grasp by minimizing\ncontact sliding and spinning. Then, we evaluated our method against a baseline\nusing collision detection and a baseline using primitive shapes. The results\nshow that our method performed the best in accuracy and precision, even for\ncoarse meshes. We conclude with a future work discussion on incorporating\nmultiple contacts and contact forces to achieve accurate and robust mesh-based\nsurface contact modeling.", "AI": {"tldr": "\u672c\u6587\u6269\u5c55\u4e86\u6eda\u6ed1\u63a5\u89e6\u5efa\u6a21\u5230\u6d41\u5f62\u7f51\u683c\uff0c\u63d0\u51fa\u57fa\u4e8e\u6d4b\u5730\u7ebf\u8ffd\u8e2a\u7684\u79ef\u5206\u65b9\u6848\uff0c\u5728\u7f51\u683c\u4e0a\u76f4\u63a5\u8fdb\u884c\u4e00\u9636\u65f6\u95f4\u79ef\u5206\uff0c\u5b9e\u73b0\u4e86\u5bf9\u7269\u4f53\u771f\u5b9e\u51e0\u4f55\u7684\u9ad8\u4fdd\u771f\u79bb\u6563\u8868\u793a\u8fdb\u884c\u7075\u5de7\u64cd\u4f5c\u89c4\u5212\u3002", "motivation": "\u73b0\u6709\u6eda\u6ed1\u63a5\u89e6\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5177\u6709\u53ef\u5fae\u5206\u53c2\u6570\u5316\u7684\u8fde\u7eed\u5f62\u72b6\uff0c\u800c\u771f\u5b9e\u7269\u4f53\u901a\u5e38\u7528\u7f51\u683c\u8868\u793a\uff0c\u9700\u8981\u5c06\u6eda\u6ed1\u63a5\u89e6\u5efa\u6a21\u6269\u5c55\u5230\u6d41\u5f62\u7f51\u683c\u4ee5\u652f\u6301\u66f4\u7cbe\u786e\u7684\u7075\u5de7\u64cd\u4f5c\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u6d4b\u5730\u7ebf\u8ffd\u8e2a\u7684\u79ef\u5206\u65b9\u6848\uff0c\u5728\u7f51\u683c\u4e0a\u8fdb\u884c\u4e00\u9636\u65f6\u95f4\u79ef\u5206\u6eda\u6ed1\u63a5\u89e6\uff1b\u4f7f\u7528\u6700\u5c0f\u4e8c\u4e58\u4f18\u5316\u5668\u89c4\u5212\u591a\u6307\u673a\u5668\u4eba\u624b\u7684\u7075\u5de7\u8fd0\u52a8\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u63a5\u89e6\u6ed1\u52a8\u548c\u65cb\u8f6c\u6765\u7ef4\u6301\u6700\u7a33\u5b9a\u7684\u77ac\u65f6\u6293\u53d6\u3002", "result": "\u5728\u6a21\u62df\u4e2d\u5bf9\u4e94\u4e2a\u7269\u4f53\u8fdb\u884c\u7075\u5de7\u64cd\u4f5c\u89c4\u5212\uff0c\u4e0e\u57fa\u4e8e\u78b0\u649e\u68c0\u6d4b\u548c\u57fa\u4e8e\u539f\u59cb\u5f62\u72b6\u7684\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u672c\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u7cbe\u5ea6\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff0c\u5373\u4f7f\u5bf9\u4e8e\u7c97\u7cd9\u7f51\u683c\u4e5f\u662f\u5982\u6b64\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u57fa\u4e8e\u7f51\u683c\u7684\u8868\u9762\u63a5\u89e6\u5efa\u6a21\u63d0\u4f9b\u4e86\u51c6\u786e\u548c\u7cbe\u786e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u672a\u6765\u5de5\u4f5c\u9700\u8981\u6574\u5408\u591a\u63a5\u89e6\u548c\u63a5\u89e6\u529b\u4ee5\u5b9e\u73b0\u66f4\u51c6\u786e\u548c\u9c81\u68d2\u7684\u5efa\u6a21\u3002"}}
{"id": "2508.12500", "categories": ["cs.AI", "cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2508.12500", "abs": "https://arxiv.org/abs/2508.12500", "authors": ["Rahmat K. Adesunkanmi", "Ashfaq Khokhar", "Goce Trajcevski", "Sohail Murad"], "title": "Root Cause Analysis of Hydrogen Bond Separation in Spatio-Temporal Molecular Dynamics using Causal Models", "comment": "Submitted to ACM", "summary": "Molecular dynamics simulations (MDS) face challenges, including\nresource-heavy computations and the need to manually scan outputs to detect\n\"interesting events,\" such as the formation and persistence of hydrogen bonds\nbetween atoms of different molecules. A critical research gap lies in\nidentifying the underlying causes of hydrogen bond formation and separation\n-understanding which interactions or prior events contribute to their emergence\nover time. With this challenge in mind, we propose leveraging spatio-temporal\ndata analytics and machine learning models to enhance the detection of these\nphenomena. In this paper, our approach is inspired by causal modeling and aims\nto identify the root cause variables of hydrogen bond formation and separation\nevents. Specifically, we treat the separation of hydrogen bonds as an\n\"intervention\" occurring and represent the causal structure of the bonding and\nseparation events in the MDS as graphical causal models. These causal models\nare built using a variational autoencoder-inspired architecture that enables us\nto infer causal relationships across samples with diverse underlying causal\ngraphs while leveraging shared dynamic information. We further include a step\nto infer the root causes of changes in the joint distribution of the causal\nmodels. By constructing causal models that capture shifts in the conditional\ndistributions of molecular interactions during bond formation or separation,\nthis framework provides a novel perspective on root cause analysis in molecular\ndynamic systems. We validate the efficacy of our model empirically on the\natomic trajectories that used MDS for chiral separation, demonstrating that we\ncan predict many steps in the future and also find the variables driving the\nobserved changes in the system.", "AI": {"tldr": "\u4f7f\u7528\u53d8\u5206\u81ea\u52a8\u7f16\u7801\u5668\u6784\u5efa\u56e0\u679c\u6a21\u578b\uff0c\u901a\u8fc7\u5c06\u6c22\u952e\u5206\u79bb\u89c6\u4e3a\"\u5e72\u9884\"\u6765\u8bc6\u522b\u5206\u5b50\u52a8\u529b\u5b66\u4e2d\u6c22\u952e\u5f62\u6210\u548c\u5206\u79bb\u7684\u6839\u672c\u539f\u56e0\u53d8\u91cf", "motivation": "\u89e3\u51b3\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\u4e2d\u8d44\u6e90\u6d89\u53ca\u8f83\u591a\u7684\u8ba1\u7b97\u548c\u9700\u8981\u624b\u52a8\u626b\u63cf\u8f93\u51fa\u4ee5\u53d1\u73b0\"\u6709\u8da3\u4e8b\u4ef6\"\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u4e86\u89e3\u6c22\u952e\u5f62\u6210\u548c\u5206\u79bb\u7684\u6839\u672c\u539f\u56e0", "method": "\u53d7\u56e0\u679c\u6a21\u578b\u542f\u53d1\uff0c\u5c06\u6c22\u952e\u5206\u79bb\u89c6\u4e3a\"\u5e72\u9884\"\u4e8b\u4ef6\uff0c\u4f7f\u7528\u53d8\u5206\u81ea\u52a8\u7f16\u7801\u5668\u7ed3\u6784\u6784\u5efa\u56fe\u5f62\u56e0\u679c\u6a21\u578b\uff0c\u5728\u5177\u6709\u4e0d\u540c\u57fa\u7840\u56e0\u679c\u56fe\u7684\u6837\u672c\u4e2d\u63a8\u65ad\u56e0\u679c\u5173\u7cfb", "result": "\u5728\u624b\u6027\u5206\u79bb\u7684\u539f\u5b50\u8f68\u8ff9\u4e0a\u9a8c\u8bc1\u6a21\u578b\u6709\u6548\u6027\uff0c\u80fd\u591f\u9884\u6d4b\u591a\u6b65\u672a\u6765\u53d8\u5316\u5e76\u627e\u5230\u9a71\u52a8\u7cfb\u7edf\u53d8\u5316\u7684\u5173\u952e\u53d8\u91cf", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5206\u5b50\u52a8\u6001\u7cfb\u7edf\u7684\u6839\u672c\u539f\u56e0\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u901a\u8fc7\u6355\u83b7\u5206\u5b50\u4e92\u4f5c\u6761\u4ef6\u5206\u5e03\u7684\u53d8\u5316\u6765\u7406\u89e3\u7ed3\u5408\u4e8b\u4ef6\u7684\u56e0\u679c\u673a\u5236"}}
{"id": "2508.12456", "categories": ["cs.RO", "68T07, 93C85, 86A05", "I.2.6; I.2.9; J.2"], "pdf": "https://arxiv.org/pdf/2508.12456", "abs": "https://arxiv.org/abs/2508.12456", "authors": ["Hadas C. Kuzmenko", "David Ehevich", "Oren Gal"], "title": "Autonomous Oil Spill Response Through Liquid Neural Trajectory Modeling and Coordinated Marine Robotics", "comment": "30 pages, 40 figures. Framework combining Liquid Time-Constant Neural\n  Networks with autonomous marine robotics for oil spill trajectory prediction\n  and response coordination", "summary": "Marine oil spills pose grave environmental and economic risks, threatening\nmarine ecosystems, coastlines, and dependent industries. Predicting and\nmanaging oil spill trajectories is highly complex, due to the interplay of\nphysical, chemical, and environmental factors such as wind, currents, and\ntemperature, which makes timely and effective response challenging. Accurate\nreal-time trajectory forecasting and coordinated mitigation are vital for\nminimizing the impact of these disasters. This study introduces an integrated\nframework combining a multi-agent swarm robotics system built on the MOOS-IvP\nplatform with Liquid Time-Constant Neural Networks (LTCNs). The proposed system\nfuses adaptive machine learning with autonomous marine robotics, enabling\nreal-time prediction, dynamic tracking, and rapid response to evolving oil\nspills. By leveraging LTCNs--well-suited for modeling complex, time-dependent\nprocesses--the framework achieves real-time, high-accuracy forecasts of spill\nmovement. Swarm intelligence enables decentralized, scalable, and resilient\ndecision-making among robot agents, enhancing collective monitoring and\ncontainment efforts. Our approach was validated using data from the Deepwater\nHorizon spill, where the LTC-RK4 model achieved 0.96 spatial accuracy,\nsurpassing LSTM approaches by 23%. The integration of advanced neural modeling\nwith autonomous, coordinated robotics demonstrates substantial improvements in\nprediction precision, flexibility, and operational scalability. Ultimately,\nthis research advances the state-of-the-art for sustainable, autonomous oil\nspill management and environmental protection by enhancing both trajectory\nprediction and response coordination.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u591a\u667a\u80fd\u4f53\u7fa4\u673a\u5668\u4eba\u7cfb\u7edf\u548c\u6db2\u4f53\u65f6\u95f4\u5e38\u6570\u795e\u7ecf\u7f51\u7edc(LTCNs)\u7684\u96c6\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u5b9e\u65f6\u9884\u6d4b\u3001\u52a8\u6001\u8ddf\u8e2a\u548c\u5feb\u901f\u54cd\u5e94\u6d77\u6d0b\u6ea2\u6cb9\u4e8b\u4ef6\uff0c\u5728Deepwater Horizon\u6ea2\u6cb9\u6570\u636e\u4e0a\u53d6\u5f97\u4e8696%\u7684\u7a7a\u95f4\u7cbe\u5ea6\u3002", "motivation": "\u6d77\u6d0b\u6ea2\u6cb9\u4e8b\u4ef6\u5bf9\u73af\u5883\u548c\u7ecf\u6d4e\u9020\u6210\u4e25\u91cd\u5a01\u80c1\uff0c\u4f46\u7531\u4e8e\u7269\u7406\u3001\u5316\u5b66\u548c\u73af\u5883\u56e0\u7d20\uff08\u5982\u98ce\u3001\u6d77\u6d41\u3001\u6e29\u5ea6\uff09\u7684\u590d\u6742\u76f8\u4e92\u4f5c\u7528\uff0c\u9884\u6d4b\u548c\u7ba1\u7406\u6ea2\u6cb9\u8f68\u8ff9\u975e\u5e38\u56f0\u96be\uff0c\u9700\u8981\u5b9e\u65f6\u51c6\u786e\u7684\u8f68\u8ff9\u9884\u6d4b\u548c\u534f\u8c03\u7684\u7f13\u89e3\u63aa\u65bd\u6765\u6700\u5c0f\u5316\u707e\u5bb3\u5f71\u54cd\u3002", "method": "\u91c7\u7528\u57fa\u4e8eMOOS-IvP\u5e73\u53f0\u7684\u591a\u667a\u80fd\u4f53\u7fa4\u673a\u5668\u4eba\u7cfb\u7edf\u4e0e\u6db2\u4f53\u65f6\u95f4\u5e38\u6570\u795e\u7ecf\u7f51\u7edc(LTCNs)\u76f8\u7ed3\u5408\u7684\u96c6\u6210\u6846\u67b6\uff0c\u5229\u7528LTCNs\u5efa\u6a21\u590d\u6742\u7684\u65f6\u95f4\u76f8\u5173\u8fc7\u7a0b\uff0c\u901a\u8fc7\u7fa4\u4f53\u667a\u80fd\u5b9e\u73b0\u53bb\u4e2d\u5fc3\u5316\u3001\u53ef\u6269\u5c55\u548c\u5f39\u6027\u7684\u673a\u5668\u4eba\u4ee3\u7406\u51b3\u7b56\u3002", "result": "\u5728Deepwater Horizon\u6ea2\u6cb9\u6570\u636e\u9a8c\u8bc1\u4e2d\uff0cLTC-RK4\u6a21\u578b\u5b9e\u73b0\u4e8696%\u7684\u7a7a\u95f4\u7cbe\u5ea6\uff0c\u6bd4LSTM\u65b9\u6cd5\u63d0\u9ad8\u4e8623%\u3002\u5148\u8fdb\u795e\u7ecf\u5efa\u6a21\u4e0e\u81ea\u4e3b\u534f\u8c03\u673a\u5668\u4eba\u6280\u672f\u7684\u96c6\u6210\u663e\u8457\u63d0\u9ad8\u4e86\u9884\u6d4b\u7cbe\u5ea6\u3001\u7075\u6d3b\u6027\u548c\u64cd\u4f5c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u901a\u8fc7\u589e\u5f3a\u8f68\u8ff9\u9884\u6d4b\u548c\u54cd\u5e94\u534f\u8c03\uff0c\u63a8\u8fdb\u4e86\u53ef\u6301\u7eed\u3001\u81ea\u4e3b\u6ea2\u6cb9\u7ba1\u7406\u548c\u73af\u5883\u4fdd\u62a4\u7684\u6700\u65b0\u6280\u672f\u6c34\u5e73\uff0c\u4e3a\u6d77\u6d0b\u6ea2\u6cb9\u5e94\u6025\u54cd\u5e94\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.12566", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12566", "abs": "https://arxiv.org/abs/2508.12566", "authors": ["Wei Song", "Haonan Zhong", "Ziqi Ding", "Jingling Xue", "Yuekang Li"], "title": "Help or Hurdle? Rethinking Model Context Protocol-Augmented Large Language Models", "comment": null, "summary": "The Model Context Protocol (MCP) enables large language models (LLMs) to\naccess external resources on demand. While commonly assumed to enhance\nperformance, how LLMs actually leverage this capability remains poorly\nunderstood. We introduce MCPGAUGE, the first comprehensive evaluation framework\nfor probing LLM-MCP interactions along four key dimensions: proactivity\n(self-initiated tool use), compliance (adherence to tool-use instructions),\neffectiveness (task performance post-integration), and overhead (computational\ncost incurred). MCPGAUGE comprises a 160-prompt suite and 25 datasets spanning\nknowledge comprehension, general reasoning, and code generation. Our\nlarge-scale evaluation, spanning six commercial LLMs, 30 MCP tool suites, and\nboth one- and two-turn interaction settings, comprises around 20,000 API calls\nand over USD 6,000 in computational cost. This comprehensive study reveals four\nkey findings that challenge prevailing assumptions about the effectiveness of\nMCP integration. These insights highlight critical limitations in current\nAI-tool integration and position MCPGAUGE as a principled benchmark for\nadvancing controllable, tool-augmented LLMs.", "AI": {"tldr": "MCPGAUGE\u662f\u9996\u4e2a\u5168\u9762\u8bc4\u4f30LLM\u4e0eModel Context Protocol\u4ea4\u4e92\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u5b9e\u9a8c\u63ed\u793a\u4e86MCP\u96c6\u6210\u6548\u679c\u7684\u56db\u4e2a\u5173\u952e\u53d1\u73b0\uff0c\u6311\u6218\u4e86\u73b0\u6709\u5047\u8bbe\u3002", "motivation": "\u867d\u7136MCP\u4f7fLLM\u80fd\u591f\u6309\u9700\u8bbf\u95ee\u5916\u90e8\u8d44\u6e90\uff0c\u4f46LLM\u5982\u4f55\u5b9e\u9645\u5229\u7528\u8fd9\u79cd\u80fd\u529b\u4ecd\u4e0d\u6e05\u695a\uff0c\u9700\u8981\u7cfb\u7edf\u8bc4\u4f30\u6846\u67b6\u6765\u7406\u89e3LLM-MCP\u4ea4\u4e92\u6548\u679c\u3002", "method": "\u5f00\u53d1MCPGAUGE\u8bc4\u4f30\u6846\u67b6\uff0c\u5305\u542b160\u4e2a\u63d0\u793a\u548c25\u4e2a\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u77e5\u8bc6\u7406\u89e3\u3001\u901a\u7528\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u3002\u5bf96\u4e2a\u5546\u4e1aLLM\u300130\u4e2aMCP\u5de5\u5177\u5957\u4ef6\u8fdb\u884c\u5927\u89c4\u6a21\u8bc4\u4f30\uff0c\u6d89\u53ca\u7ea620,000\u6b21API\u8c03\u7528\u3002", "result": "\u7814\u7a76\u63ed\u793a\u4e86\u56db\u4e2a\u5173\u952e\u53d1\u73b0\uff0c\u6311\u6218\u4e86\u5173\u4e8eMCP\u96c6\u6210\u6709\u6548\u6027\u7684\u666e\u904d\u5047\u8bbe\uff0c\u7a81\u663e\u4e86\u5f53\u524dAI\u5de5\u5177\u96c6\u6210\u7684\u5173\u952e\u5c40\u9650\u6027\u3002", "conclusion": "MCPGAUGE\u4e3a\u63a8\u8fdb\u53ef\u63a7\u3001\u5de5\u5177\u589e\u5f3a\u578bLLM\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u5f53\u524dMCP\u96c6\u6210\u7684\u5b9e\u9645\u6548\u679c\u4e0e\u9884\u671f\u5b58\u5728\u5dee\u8ddd\u3002"}}
{"id": "2508.12469", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12469", "abs": "https://arxiv.org/abs/2508.12469", "authors": ["Abhinav Chalise", "Nimesh Gopal Pradhan", "Nishan Khanal", "Prashant Raj Bista", "Dinesh Baniya Kshatri"], "title": "Mechanical Automation with Vision: A Design for Rubik's Cube Solver", "comment": "Presented at the 15th IOE Graduate Conference, Tribhuvan University,\n  May 2024. Original paper available at\n  https://conference.ioe.edu.np/publications/ioegc15/IOEGC-15-023-C1-2-42.pdf", "summary": "The core mechanical system is built around three stepper motors for physical\nmanipulation, a microcontroller for hardware control, a camera and YOLO\ndetection model for real-time cube state detection. A significant software\ncomponent is the development of a user-friendly graphical user interface (GUI)\ndesigned in Unity. The initial state after detection from real-time YOLOv8\nmodel (Precision 0.98443, Recall 0.98419, Box Loss 0.42051, Class Loss 0.2611)\nis virtualized on GUI. To get the solution, the system employs the Kociemba's\nalgorithm while physical manipulation with a single degree of freedom is done\nby combination of stepper motors' interaction with the cube achieving the\naverage solving time of ~2.2 minutes.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u4e09\u53f0\u6b65\u8fdb\u7535\u673a\u3001\u5fae\u63a7\u5236\u5668\u548cYOLOv8\u5b9e\u65f6\u68c0\u6d4b\u7684\u9b54\u65b9\u81ea\u52a8\u6c42\u89e3\u7cfb\u7edf\uff0c\u901a\u8fc7Kociemba\u7b97\u6cd5\u6c42\u89e3\uff0c\u5e73\u5747\u6c42\u89e3\u65f6\u95f4\u7ea62.2\u5206\u949f", "motivation": "\u6784\u5efa\u4e00\u4e2a\u5b8c\u6574\u7684\u9b54\u65b9\u81ea\u52a8\u6c42\u89e3\u7cfb\u7edf\uff0c\u5c06\u7269\u7406\u64cd\u4f5c\u3001\u5b9e\u65f6\u72b6\u6001\u68c0\u6d4b\u548c\u7b97\u6cd5\u6c42\u89e3\u96c6\u6210\u5230\u4e00\u4e2a\u7528\u6237\u53cb\u597d\u7684\u754c\u9762\u4e2d", "method": "\u4f7f\u7528\u4e09\u53f0\u6b65\u8fdb\u7535\u673a\u8fdb\u884c\u7269\u7406\u64cd\u4f5c\uff0c\u5fae\u63a7\u5236\u5668\u63a7\u5236\u786c\u4ef6\uff0cYOLOv8\u6a21\u578b\u5b9e\u65f6\u68c0\u6d4b\u9b54\u65b9\u72b6\u6001\uff0cUnity\u5f00\u53d1GUI\u754c\u9762\uff0cKociemba\u7b97\u6cd5\u751f\u6210\u6c42\u89e3\u6b65\u9aa4", "result": "YOLOv8\u68c0\u6d4b\u7cbe\u5ea6\u8fbe0.98443\uff0c\u53ec\u56de\u73870.98419\uff0c\u5e73\u5747\u6c42\u89e3\u65f6\u95f4\u7ea62.2\u5206\u949f\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u9b54\u65b9\u7684\u81ea\u52a8\u68c0\u6d4b\u548c\u6c42\u89e3", "conclusion": "\u8be5\u7cfb\u7edf\u6210\u529f\u6574\u5408\u4e86\u786c\u4ef6\u63a7\u5236\u3001\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u6c42\u89e3\u7b97\u6cd5\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u9b54\u65b9\u81ea\u52a8\u6c42\u89e3\uff0c\u4e3a\u7c7b\u4f3c\u7684\u7269\u7406\u64cd\u4f5c\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u6280\u672f\u65b9\u6848"}}
{"id": "2508.12611", "categories": ["cs.AI", "cs.CL", "I.2.7; F.4.1"], "pdf": "https://arxiv.org/pdf/2508.12611", "abs": "https://arxiv.org/abs/2508.12611", "authors": ["Trang Tran", "Trung Hoang Le", "Huiping Cao", "Tran Cao Son"], "title": "An LLM + ASP Workflow for Joint Entity-Relation Extraction", "comment": "13 pages, 1 figure, Accepted as Technical Communication, 41st\n  International Conference on Logic Programming", "summary": "Joint entity-relation extraction (JERE) identifies both entities and their\nrelationships simultaneously. Traditional machine-learning based approaches to\nperforming this task require a large corpus of annotated data and lack the\nability to easily incorporate domain specific information in the construction\nof the model. Therefore, creating a model for JERE is often labor intensive,\ntime consuming, and elaboration intolerant. In this paper, we propose\nharnessing the capabilities of generative pretrained large language models\n(LLMs) and the knowledge representation and reasoning capabilities of Answer\nSet Programming (ASP) to perform JERE. We present a generic workflow for JERE\nusing LLMs and ASP. The workflow is generic in the sense that it can be applied\nfor JERE in any domain. It takes advantage of LLM's capability in natural\nlanguage understanding in that it works directly with unannotated text. It\nexploits the elaboration tolerant feature of ASP in that no modification of its\ncore program is required when additional domain specific knowledge, in the form\nof type specifications, is found and needs to be used. We demonstrate the\nusefulness of the proposed workflow through experiments with limited training\ndata on three well-known benchmarks for JERE. The results of our experiments\nshow that the LLM + ASP workflow is better than state-of-the-art JERE systems\nin several categories with only 10\\% of training data. It is able to achieve a\n2.5 times (35\\% over 15\\%) improvement in the Relation Extraction task for the\nSciERC corpus, one of the most difficult benchmarks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b(LLM)\u548c\u7b54\u6848\u96c6\u7f16\u7a0b(ASP)\u7684\u8054\u5408\u5b9e\u4f53\u5173\u7cfb\u62bd\u53d6\u5de5\u4f5c\u6d41\uff0c\u65e0\u9700\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u5728\u4ec5\u4f7f\u752810%\u8bad\u7ec3\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u73b0\u6709\u6700\u4f73\u7cfb\u7edf\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\u4e14\u96be\u4ee5\u878d\u5165\u9886\u57df\u77e5\u8bc6\uff0c\u6784\u5efa\u6a21\u578b\u8017\u65f6\u8017\u529b\u3002\u672c\u6587\u65e8\u5728\u5229\u7528LLM\u7684\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u80fd\u529b\u548cASP\u7684\u77e5\u8bc6\u8868\u793a\u63a8\u7406\u80fd\u529b\uff0c\u89e3\u51b3\u8054\u5408\u5b9e\u4f53\u5173\u7cfb\u62bd\u53d6\u7684\u6807\u6ce8\u6570\u636e\u4f9d\u8d56\u548c\u9886\u57df\u77e5\u8bc6\u878d\u5165\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u901a\u7528\u5de5\u4f5c\u6d41\uff1a\u5229\u7528LLM\u76f4\u63a5\u5904\u7406\u672a\u6807\u6ce8\u6587\u672c\u8fdb\u884c\u81ea\u7136\u8bed\u8a00\u7406\u89e3\uff0c\u7ed3\u5408ASP\u8fdb\u884c\u77e5\u8bc6\u8868\u793a\u548c\u63a8\u7406\u3002ASP\u5177\u6709\u6269\u5c55\u5bb9\u5fcd\u6027\uff0c\u65e0\u9700\u4fee\u6539\u6838\u5fc3\u7a0b\u5e8f\u5373\u53ef\u878d\u5165\u65b0\u7684\u9886\u57df\u77e5\u8bc6\uff08\u7c7b\u578b\u89c4\u8303\uff09\u3002", "result": "\u5728\u4e09\u4e2a\u77e5\u540dJERE\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0c\u4ec5\u4f7f\u752810%\u8bad\u7ec3\u6570\u636e\u5c31\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u4f73\u7cfb\u7edf\u3002\u5728\u6700\u5177\u6311\u6218\u6027\u7684SciERC\u8bed\u6599\u5e93\u4e0a\uff0c\u5173\u7cfb\u62bd\u53d6\u4efb\u52a1\u5b9e\u73b0\u4e862.5\u500d\u7684\u6027\u80fd\u63d0\u5347\uff08\u4ece15%\u63d0\u5347\u523035%\uff09\u3002", "conclusion": "LLM+ASP\u5de5\u4f5c\u6d41\u4e3a\u8054\u5408\u5b9e\u4f53\u5173\u7cfb\u62bd\u53d6\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u5bf9\u6807\u6ce8\u6570\u636e\u7684\u4f9d\u8d56\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9886\u57df\u77e5\u8bc6\u7684\u7075\u6d3b\u878d\u5165\u80fd\u529b\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2508.12554", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12554", "abs": "https://arxiv.org/abs/2508.12554", "authors": ["Hamza El-Kebir"], "title": "PROD: Palpative Reconstruction of Deformable Objects through Elastostatic Signed Distance Functions", "comment": "Accepted for presentation at the 2025 IEEE Conference on Decision and\n  Control (CDC)", "summary": "We introduce PROD (Palpative Reconstruction of Deformables), a novel method\nfor reconstructing the shape and mechanical properties of deformable objects\nusing elastostatic signed distance functions (SDFs). Unlike traditional\napproaches that rely on purely geometric or visual data, PROD integrates\npalpative interaction -- measured through force-controlled surface probing --\nto estimate both the static and dynamic response of soft materials. We model\nthe deformation of an object as an elastostatic process and derive a governing\nPoisson equation for estimating its SDF from a sparse set of pose and force\nmeasurements. By incorporating steady-state elastodynamic assumptions, we show\nthat the undeformed SDF can be recovered from deformed observations with\nprovable convergence. Our approach also enables the estimation of material\nstiffness by analyzing displacement responses to varying force inputs. We\ndemonstrate the robustness of PROD in handling pose errors, non-normal force\napplication, and curvature errors in simulated soft body interactions. These\ncapabilities make PROD a powerful tool for reconstructing deformable objects in\napplications ranging from robotic manipulation to medical imaging and haptic\nfeedback systems.", "AI": {"tldr": "PROD\u662f\u4e00\u79cd\u901a\u8fc7\u89e6\u89c9\u4ea4\u4e92\u91cd\u5efa\u53ef\u53d8\u5f62\u7269\u4f53\u5f62\u72b6\u548c\u529b\u5b66\u7279\u6027\u7684\u65b0\u65b9\u6cd5\uff0c\u4f7f\u7528\u5f39\u6027\u9759\u529b\u5b66SDF\uff0c\u7ed3\u5408\u529b\u63a7\u8868\u9762\u63a2\u6d4b\u6765\u4f30\u8ba1\u8f6f\u6750\u6599\u7684\u9759\u6001\u548c\u52a8\u6001\u54cd\u5e94\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u7eaf\u51e0\u4f55\u6216\u89c6\u89c9\u6570\u636e\uff0c\u65e0\u6cd5\u6709\u6548\u4f30\u8ba1\u53ef\u53d8\u5f62\u7269\u4f53\u7684\u529b\u5b66\u7279\u6027\u3002PROD\u901a\u8fc7\u6574\u5408\u89e6\u89c9\u4ea4\u4e92\u6570\u636e\uff0c\u65e8\u5728\u540c\u65f6\u6062\u590d\u7269\u4f53\u7684\u5f62\u72b6\u548c\u6750\u6599\u521a\u5ea6\u3002", "method": "\u5c06\u7269\u4f53\u53d8\u5f62\u5efa\u6a21\u4e3a\u5f39\u6027\u9759\u529b\u5b66\u8fc7\u7a0b\uff0c\u63a8\u5bfc\u63a7\u5236\u6cca\u677e\u65b9\u7a0b\u4ece\u7a00\u758f\u4f4d\u59ff\u548c\u529b\u6d4b\u91cf\u4e2d\u4f30\u8ba1SDF\u3002\u7ed3\u5408\u7a33\u6001\u5f39\u6027\u52a8\u529b\u5b66\u5047\u8bbe\uff0c\u4ece\u53d8\u5f62\u89c2\u6d4b\u4e2d\u6062\u590d\u672a\u53d8\u5f62SDF\u3002\u901a\u8fc7\u5206\u6790\u4f4d\u79fb\u54cd\u5e94\u4f30\u8ba1\u6750\u6599\u521a\u5ea6\u3002", "result": "PROD\u5728\u5904\u7406\u4f4d\u59ff\u8bef\u5dee\u3001\u975e\u6cd5\u5411\u529b\u65bd\u52a0\u548c\u66f2\u7387\u8bef\u5dee\u65b9\u9762\u8868\u73b0\u51fa\u9c81\u68d2\u6027\uff0c\u5728\u6a21\u62df\u8f6f\u4f53\u4ea4\u4e92\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "PROD\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u3001\u533b\u5b66\u6210\u50cf\u548c\u89e6\u89c9\u53cd\u9988\u7cfb\u7edf\u7b49\u5e94\u7528\u4e2d\u7684\u53ef\u53d8\u5f62\u7269\u4f53\u91cd\u5efa\u63d0\u4f9b\u4e86\u5f3a\u5927\u5de5\u5177\uff0c\u80fd\u591f\u540c\u65f6\u6062\u590d\u51e0\u4f55\u5f62\u72b6\u548c\u529b\u5b66\u7279\u6027\u3002"}}
{"id": "2508.12647", "categories": ["cs.AI", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12647", "abs": "https://arxiv.org/abs/2508.12647", "authors": ["Hengnian Gu", "Zhifu Chen", "Yuxin Chen", "Jin Peng Zhou", "Dongdai Zhou"], "title": "Cognitive Structure Generation: From Educational Priors to Policy Optimization", "comment": null, "summary": "Cognitive structure is a student's subjective organization of an objective\nknowledge system, reflected in the psychological construction of concepts and\ntheir relations. However, cognitive structure assessment remains a\nlong-standing challenge in student modeling and psychometrics, persisting as a\nfoundational yet largely unassessable concept in educational practice. This\npaper introduces a novel framework, Cognitive Structure Generation (CSG), in\nwhich we first pretrain a Cognitive Structure Diffusion Probabilistic Model\n(CSDPM) to generate students' cognitive structures from educational priors, and\nthen further optimize its generative process as a policy with hierarchical\nreward signals via reinforcement learning to align with genuine cognitive\ndevelopment levels during students' learning processes. Experimental results on\nfour popular real-world education datasets show that cognitive structures\ngenerated by CSG offer more comprehensive and effective representations for\nstudent modeling, substantially improving performance on KT and CD tasks while\nenhancing interpretability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u8ba4\u77e5\u7ed3\u6784\u751f\u6210(CSG)\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u8ba4\u77e5\u7ed3\u6784\u6269\u6563\u6982\u7387\u6a21\u578b(CSDPM)\u548c\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\uff0c\u4ece\u6559\u80b2\u5148\u9a8c\u751f\u6210\u5b66\u751f\u7684\u8ba4\u77e5\u7ed3\u6784\uff0c\u663e\u8457\u63d0\u5347\u5b66\u751f\u5efa\u6a21\u6548\u679c\u3002", "motivation": "\u8ba4\u77e5\u7ed3\u6784\u662f\u5b66\u751f\u5bf9\u77e5\u8bc6\u7cfb\u7edf\u7684\u4e3b\u89c2\u7ec4\u7ec7\uff0c\u4f46\u5728\u6559\u80b2\u5b9e\u8df5\u4e2d\u4e00\u76f4\u96be\u4ee5\u6709\u6548\u8bc4\u4f30\uff0c\u662f\u5b66\u751f\u5efa\u6a21\u548c\u5fc3\u7406\u6d4b\u91cf\u5b66\u4e2d\u7684\u957f\u671f\u6311\u6218\u3002", "method": "\u9996\u5148\u9884\u8bad\u7ec3\u8ba4\u77e5\u7ed3\u6784\u6269\u6563\u6982\u7387\u6a21\u578b(CSDPM)\u4ece\u6559\u80b2\u5148\u9a8c\u751f\u6210\u8ba4\u77e5\u7ed3\u6784\uff0c\u7136\u540e\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f7f\u7528\u5206\u5c42\u5956\u52b1\u4fe1\u53f7\u4f18\u5316\u751f\u6210\u8fc7\u7a0b\uff0c\u4f7f\u5176\u4e0e\u771f\u5b9e\u8ba4\u77e5\u53d1\u5c55\u6c34\u5e73\u5bf9\u9f50\u3002", "result": "\u5728\u56db\u4e2a\u771f\u5b9e\u6559\u80b2\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCSG\u751f\u6210\u7684\u8ba4\u77e5\u7ed3\u6784\u4e3a\u5b66\u751f\u5efa\u6a21\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u6709\u6548\u7684\u8868\u793a\uff0c\u663e\u8457\u63d0\u5347\u4e86\u77e5\u8bc6\u8ffd\u8e2a(KT)\u548c\u8ba4\u77e5\u8bca\u65ad(CD)\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u5e76\u589e\u5f3a\u4e86\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "CSG\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u8ba4\u77e5\u7ed3\u6784\u8bc4\u4f30\u7684\u96be\u9898\uff0c\u4e3a\u6559\u80b2\u5b9e\u8df5\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8ba4\u77e5\u7ed3\u6784\u751f\u6210\u65b9\u6cd5\uff0c\u5728\u5b66\u751f\u5efa\u6a21\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002"}}
{"id": "2508.12564", "categories": ["cs.RO", "cs.CV", "I.2.9"], "pdf": "https://arxiv.org/pdf/2508.12564", "abs": "https://arxiv.org/abs/2508.12564", "authors": ["Jiayao Mai", "Xiuyuan Lu", "Kuan Dai", "Shaojie Shen", "Yi Zhou"], "title": "Temporal and Rotational Calibration for Event-Centric Multi-Sensor Systems", "comment": "8 pages, 5 figures", "summary": "Event cameras generate asynchronous signals in response to pixel-level\nbrightness changes, offering a sensing paradigm with theoretically\nmicrosecond-scale latency that can significantly enhance the performance of\nmulti-sensor systems. Extrinsic calibration is a critical prerequisite for\neffective sensor fusion; however, the configuration that involves event cameras\nremains an understudied topic. In this paper, we propose a motion-based\ntemporal and rotational calibration framework tailored for event-centric\nmulti-sensor systems, eliminating the need for dedicated calibration targets.\nOur method uses as input the rotational motion estimates obtained from event\ncameras and other heterogeneous sensors, respectively. Different from\nconventional approaches that rely on event-to-frame conversion, our method\nefficiently estimates angular velocity from normal flow observations, which are\nderived from the spatio-temporal profile of event data. The overall calibration\npipeline adopts a two-step approach: it first initializes the temporal offset\nand rotational extrinsics by exploiting kinematic correlations in the spirit of\nCanonical Correlation Analysis (CCA), and then refines both temporal and\nrotational parameters through a joint non-linear optimization using a\ncontinuous-time parametrization in SO(3). Extensive evaluations on both\npublicly available and self-collected datasets validate that the proposed\nmethod achieves calibration accuracy comparable to target-based methods, while\nexhibiting superior stability over purely CCA-based methods, and highlighting\nits precision, robustness and flexibility. To facilitate future research, our\nimplementation will be made open-source. Code:\nhttps://github.com/NAIL-HNU/EvMultiCalib.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fd0\u52a8\u7684\u4e8b\u4ef6\u76f8\u673a\u591a\u4f20\u611f\u5668\u65f6\u7a7a\u6807\u5b9a\u6846\u67b6\uff0c\u65e0\u9700\u4e13\u7528\u6807\u5b9a\u76ee\u6807\uff0c\u901a\u8fc7\u89d2\u901f\u5ea6\u4f30\u8ba1\u548c\u4e24\u6b65\u4f18\u5316\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u6807\u5b9a", "motivation": "\u4e8b\u4ef6\u76f8\u673a\u4f5c\u4e3a\u65b0\u578b\u4f20\u611f\u5668\u5728\u591a\u4f20\u611f\u5668\u7cfb\u7edf\u4e2d\u5177\u6709\u5fae\u79d2\u7ea7\u5ef6\u8fdf\u4f18\u52bf\uff0c\u4f46\u5176\u5916\u53c2\u6807\u5b9a\u7814\u7a76\u4e0d\u8db3\uff0c\u9700\u8981\u89e3\u51b3\u65e0\u6807\u5b9a\u76ee\u6807\u7684\u6807\u5b9a\u95ee\u9898", "method": "\u4f7f\u7528\u4e8b\u4ef6\u76f8\u673a\u548c\u5176\u4ed6\u4f20\u611f\u5668\u7684\u65cb\u8f6c\u8fd0\u52a8\u4f30\u8ba1\uff0c\u901a\u8fc7\u6cd5\u5411\u6d41\u89c2\u6d4b\u4f30\u8ba1\u89d2\u901f\u5ea6\uff0c\u91c7\u7528CCA\u521d\u59cb\u5316\u65f6\u7a7a\u53c2\u6570\uff0c\u7136\u540e\u901a\u8fc7SO(3)\u8fde\u7eed\u65f6\u95f4\u53c2\u6570\u5316\u8fdb\u884c\u975e\u7ebf\u6027\u8054\u5408\u4f18\u5316", "result": "\u5728\u516c\u5f00\u548c\u81ea\u91c7\u96c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u6807\u5b9a\u7cbe\u5ea6\u8fbe\u5230\u57fa\u4e8e\u76ee\u6807\u65b9\u6cd5\u7684\u6c34\u5e73\uff0c\u6bd4\u7eafCCA\u65b9\u6cd5\u66f4\u7a33\u5b9a\uff0c\u8868\u73b0\u51fa\u9ad8\u7cbe\u5ea6\u3001\u9c81\u68d2\u6027\u548c\u7075\u6d3b\u6027", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4e8b\u4ef6\u76f8\u673a\u591a\u4f20\u611f\u5668\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u65e0\u6807\u5b9a\u76ee\u6807\u6807\u5b9a\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u5c06\u5f00\u6e90\u4ee5\u4fc3\u8fdb\u672a\u6765\u7814\u7a76"}}
{"id": "2508.12651", "categories": ["cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2508.12651", "abs": "https://arxiv.org/abs/2508.12651", "authors": ["Chunliang Hua", "Xiao Hu", "Jiayang Sun", "Zeyuan Yang"], "title": "The Maximum Coverage Model and Recommendation System for UAV Vertiports Location Planning", "comment": "10 pages", "summary": "As urban aerial mobility (UAM) infrastructure development accelerates\nglobally, cities like Shenzhen are planning large-scale vertiport networks\n(e.g., 1,200+ facilities by 2026). Existing planning frameworks remain\ninadequate for this complexity due to historical limitations in data\ngranularity and real-world applicability. This paper addresses these gaps by\nfirst proposing the Capacitated Dynamic Maximum Covering Location Problem\n(CDMCLP), a novel optimization framework that simultaneously models urban-scale\nspatial-temporal demand, heterogeneous user behaviors, and infrastructure\ncapacity constraints. Building on this foundation, we introduce an Integrated\nPlanning Recommendation System that combines CDMCLP with socio-economic factors\nand dynamic clustering initialization. This system leverages adaptive parameter\ntuning based on empirical user behavior to generate practical planning\nsolutions. Validation in a Chinese center city demonstrates the effectiveness\nof the new optimization framework and recommendation system. Under the\nevaluation and optimization of CDMCLP, the quantitative performance of\ntraditional location methods are exposed and can be improved by 38\\%--52\\%,\nwhile the recommendation system shows user-friendliness and the effective\nintegration of complex elements. By integrating mathematical rigor with\npractical implementation considerations, this hybrid approach bridges the gap\nbetween theoretical location modeling and real-world UAM infrastructure\nplanning, offering municipalities a pragmatic tool for vertiport network\ndesign.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86CDMCLP\u4f18\u5316\u6846\u67b6\u548c\u96c6\u6210\u89c4\u5212\u63a8\u8350\u7cfb\u7edf\uff0c\u7528\u4e8e\u89e3\u51b3\u57ce\u5e02\u7a7a\u4e2d\u4ea4\u901a\u57fa\u7840\u8bbe\u65bd\u89c4\u5212\u4e2d\u7684\u590d\u6742\u95ee\u9898\uff0c\u5728\u4f20\u7edf\u9009\u5740\u65b9\u6cd5\u57fa\u7840\u4e0a\u63d0\u5347\u4e8638%-52%\u7684\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u5168\u7403\u57ce\u5e02\u7a7a\u4e2d\u4ea4\u901a\u57fa\u7840\u8bbe\u65bd\u5feb\u901f\u53d1\u5c55\uff0c\u73b0\u6709\u89c4\u5212\u6846\u67b6\u56e0\u6570\u636e\u7c92\u5ea6\u4e0d\u8db3\u548c\u5b9e\u9645\u5e94\u7528\u6027\u9650\u5236\u800c\u65e0\u6cd5\u5e94\u5bf9\u5927\u89c4\u6a21\u5782\u76f4\u8d77\u964d\u573a\u7f51\u7edc\u7684\u590d\u6742\u6027\u3002", "method": "\u63d0\u51fa\u5bb9\u91cf\u7ea6\u675f\u52a8\u6001\u6700\u5927\u8986\u76d6\u9009\u5740\u95ee\u9898(CDMCLP)\u4f18\u5316\u6846\u67b6\uff0c\u540c\u65f6\u5efa\u6a21\u57ce\u5e02\u7ea7\u65f6\u7a7a\u9700\u6c42\u3001\u5f02\u6784\u7528\u6237\u884c\u4e3a\u548c\u57fa\u7840\u8bbe\u65bd\u5bb9\u91cf\u7ea6\u675f\uff1b\u5f00\u53d1\u96c6\u6210\u89c4\u5212\u63a8\u8350\u7cfb\u7edf\uff0c\u7ed3\u5408\u793e\u4f1a\u7ecf\u6d4e\u56e0\u7d20\u548c\u52a8\u6001\u805a\u7c7b\u521d\u59cb\u5316\u3002", "result": "\u5728\u4e2d\u56fd\u4e2d\u5fc3\u57ce\u5e02\u9a8c\u8bc1\u663e\u793a\uff0cCDMCLP\u4f7f\u4f20\u7edf\u9009\u5740\u65b9\u6cd5\u7684\u91cf\u5316\u6027\u80fd\u63d0\u534738%-52%\uff0c\u63a8\u8350\u7cfb\u7edf\u5c55\u73b0\u51fa\u7528\u6237\u53cb\u597d\u6027\u548c\u590d\u6742\u8981\u7d20\u7684\u6709\u6548\u6574\u5408\u80fd\u529b\u3002", "conclusion": "\u8fd9\u79cd\u6df7\u5408\u65b9\u6cd5\u901a\u8fc7\u6570\u5b66\u4e25\u8c28\u6027\u4e0e\u5b9e\u9645\u5b9e\u65bd\u8003\u8651\u7684\u7ed3\u5408\uff0c\u5f25\u5408\u4e86\u7406\u8bba\u9009\u5740\u5efa\u6a21\u4e0e\u73b0\u5b9e\u4e16\u754cUAM\u57fa\u7840\u8bbe\u65bd\u89c4\u5212\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u5e02\u653f\u90e8\u95e8\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u5782\u76f4\u8d77\u964d\u573a\u7f51\u7edc\u8bbe\u8ba1\u5de5\u5177\u3002"}}
{"id": "2508.12681", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12681", "abs": "https://arxiv.org/abs/2508.12681", "authors": ["Johann Licher", "Max Bartholdt", "Henrik Krauss", "Tim-Lukas Habich", "Thomas Seel", "Moritz Schappler"], "title": "Adaptive Model-Predictive Control of a Soft Continuum Robot Using a Physics-Informed Neural Network Based on Cosserat Rod Theory", "comment": "20 pages, 15 figures", "summary": "Dynamic control of soft continuum robots (SCRs) holds great potential for\nexpanding their applications, but remains a challenging problem due to the high\ncomputational demands of accurate dynamic models. While data-driven approaches\nlike Koopman-operator-based methods have been proposed, they typically lack\nadaptability and cannot capture the full robot shape, limiting their\napplicability. This work introduces a real-time-capable nonlinear\nmodel-predictive control (MPC) framework for SCRs based on a domain-decoupled\nphysics-informed neural network (DD-PINN) with adaptable bending stiffness. The\nDD-PINN serves as a surrogate for the dynamic Cosserat rod model with a\nspeed-up factor of 44000. It is also used within an unscented Kalman filter for\nestimating the model states and bending compliance from end-effector position\nmeasurements. We implement a nonlinear evolutionary MPC running at 70 Hz on the\nGPU. In simulation, it demonstrates accurate tracking of dynamic trajectories\nand setpoint control with end-effector position errors below 3 mm (2.3% of the\nactuator's length). In real-world experiments, the controller achieves similar\naccuracy and accelerations up to 3.55 m/s2.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u57df\u89e3\u8026\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc(DD-PINN)\u7684\u5b9e\u65f6\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u6846\u67b6\uff0c\u7528\u4e8e\u8f6f\u4f53\u8fde\u7eed\u673a\u5668\u4eba\u7684\u52a8\u6001\u63a7\u5236\uff0c\u5b9e\u73b0\u9ad8\u901f\u9ad8\u7cbe\u5ea6\u8ddf\u8e2a", "motivation": "\u8f6f\u4f53\u8fde\u7eed\u673a\u5668\u4eba\u52a8\u6001\u63a7\u5236\u5177\u6709\u91cd\u8981\u5e94\u7528\u4ef7\u503c\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u8ba1\u7b97\u91cf\u5927\u4e14\u7f3a\u4e4f\u9002\u5e94\u6027\uff0c\u73b0\u6709\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u65e0\u6cd5\u6355\u6349\u5b8c\u6574\u673a\u5668\u4eba\u5f62\u72b6", "method": "\u4f7f\u7528\u57df\u89e3\u8026\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc(DD-PINN)\u4f5c\u4e3aCosserat\u6746\u6a21\u578b\u7684\u66ff\u4ee3\u6a21\u578b\uff0c\u901f\u5ea6\u63d0\u534744000\u500d\uff0c\u7ed3\u5408\u65e0\u8ff9\u5361\u5c14\u66fc\u6ee4\u6ce2\u8fdb\u884c\u72b6\u6001\u4f30\u8ba1\uff0c\u5b9e\u73b070Hz\u7684\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236", "result": "\u4eff\u771f\u4e2d\u5b9e\u73b0\u52a8\u6001\u8f68\u8ff9\u7cbe\u786e\u8ddf\u8e2a\uff0c\u672b\u7aef\u4f4d\u7f6e\u8bef\u5dee\u4f4e\u4e8e3mm(2.3%\u6267\u884c\u5668\u957f\u5ea6)\uff1b\u771f\u5b9e\u5b9e\u9a8c\u4e2d\u8fbe\u5230\u76f8\u4f3c\u7cbe\u5ea6\uff0c\u52a0\u901f\u5ea6\u8fbe3.55m/s\u00b2", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u8f6f\u4f53\u8fde\u7eed\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u5b9e\u65f6\u9ad8\u6548\u7684\u52a8\u6001\u63a7\u5236\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a7\u5236\u6027\u80fd\u548c\u9002\u5e94\u6027"}}
{"id": "2508.12682", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12682", "abs": "https://arxiv.org/abs/2508.12682", "authors": ["Jinquan Shi", "Yingying Cheng", "Fan Zhang", "Miao Jiang", "Jun Lin", "Yanbai Shen"], "title": "GridCodex: A RAG-Driven AI Framework for Power Grid Code Reasoning and Compliance", "comment": null, "summary": "The global shift towards renewable energy presents unprecedented challenges\nfor the electricity industry, making regulatory reasoning and compliance\nincreasingly vital. Grid codes, the regulations governing grid operations, are\ncomplex and often lack automated interpretation solutions, which hinders\nindustry expansion and undermines profitability for electricity companies. We\nintroduce GridCodex, an end to end framework for grid code reasoning and\ncompliance that leverages large language models and retrieval-augmented\ngeneration (RAG). Our framework advances conventional RAG workflows through\nmulti stage query refinement and enhanced retrieval with RAPTOR. We validate\nthe effectiveness of GridCodex with comprehensive benchmarks, including\nautomated answer assessment across multiple dimensions and regulatory agencies.\nExperimental results showcase a 26.4% improvement in answer quality and more\nthan a 10 fold increase in recall rate. An ablation study further examines the\nimpact of base model selection.", "AI": {"tldr": "GridCodex\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u7684\u7aef\u5230\u7aef\u7535\u7f51\u89c4\u8303\u63a8\u7406\u4e0e\u5408\u89c4\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u67e5\u8be2\u4f18\u5316\u548cRAPTOR\u589e\u5f3a\u68c0\u7d22\u6280\u672f\uff0c\u5728\u7535\u7f51\u89c4\u8303\u81ea\u52a8\u89e3\u91ca\u65b9\u9762\u5b9e\u73b0\u4e8626.4%\u7684\u7b54\u6848\u8d28\u91cf\u63d0\u5347\u548c10\u500d\u4ee5\u4e0a\u7684\u53ec\u56de\u7387\u63d0\u5347\u3002", "motivation": "\u53ef\u518d\u751f\u80fd\u6e90\u8f6c\u578b\u7ed9\u7535\u529b\u884c\u4e1a\u5e26\u6765\u5de8\u5927\u6311\u6218\uff0c\u7535\u7f51\u89c4\u8303\u590d\u6742\u4e14\u7f3a\u4e4f\u81ea\u52a8\u5316\u89e3\u91ca\u65b9\u6848\uff0c\u963b\u788d\u884c\u4e1a\u53d1\u5c55\u5e76\u5f71\u54cd\u7535\u529b\u516c\u53f8\u76c8\u5229\u80fd\u529b\u3002", "method": "\u91c7\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u6280\u672f\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u67e5\u8be2\u4f18\u5316\u548cRAPTOR\u589e\u5f3a\u68c0\u7d22\u65b9\u6cd5\u6784\u5efa\u7aef\u5230\u7aef\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u7b54\u6848\u8d28\u91cf\u63d0\u534726.4%\uff0c\u53ec\u56de\u7387\u63d0\u5347\u8d85\u8fc710\u500d\uff0c\u5e76\u901a\u8fc7\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u57fa\u7840\u6a21\u578b\u9009\u62e9\u7684\u5f71\u54cd\u3002", "conclusion": "GridCodex\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u7535\u7f51\u89c4\u8303\u81ea\u52a8\u89e3\u91ca\u7684\u96be\u9898\uff0c\u4e3a\u7535\u529b\u884c\u4e1a\u76d1\u7ba1\u5408\u89c4\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u6280\u672f\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.12729", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.12729", "abs": "https://arxiv.org/abs/2508.12729", "authors": ["Junhao Ye", "Cheng Hu", "Yiqin Wang", "Weizhan Huang", "Nicolas Baumann", "Jie He", "Meixun Qu", "Lei Xie", "Hongye Su"], "title": "MCTR: Midpoint Corrected Triangulation for Autonomous Racing via Digital Twin Simulation in CARLA", "comment": null, "summary": "In autonomous racing, reactive controllers eliminate the computational burden\nof the full See-Think-Act autonomy stack by directly mapping sensor inputs to\ncontrol actions. This bypasses the need for explicit localization and\ntrajectory planning. A widely adopted baseline in this category is the\nFollow-The-Gap method, which performs trajectory planning using LiDAR data.\nBuilding on FTG, the Delaunay Triangulation-based Racing algorithm introduces\nfurther enhancements. However, DTR's use of circumcircles for trajectory\ngeneration often results in insufficiently smooth paths, ultimately degrading\nperformance. Additionally, the commonly used F1TENTH-simulator for autonomous\nracing competitions lacks support for 3D LiDAR perception, limiting its\neffectiveness in realistic testing. To address these challenges, this work\nproposes the MCTR algorithm. MCTR improves trajectory smoothness through the\nuse of Curvature Corrected Moving Average and implements a digital twin system\nwithin the CARLA simulator to validate the algorithm's robustness under 3D\nLiDAR perception. The proposed algorithm has been thoroughly validated through\nboth simulation and real-world vehicle experiments.", "AI": {"tldr": "\u63d0\u51faMCTR\u7b97\u6cd5\uff0c\u901a\u8fc7\u66f2\u7387\u6821\u6b63\u79fb\u52a8\u5e73\u5747\u63d0\u9ad8\u8f68\u8ff9\u5e73\u6ed1\u5ea6\uff0c\u5e76\u5728CARLA\u6a21\u62df\u5668\u4e2d\u5b9e\u73b0\u6570\u5b57\u5b6a\u751f\u7cfb\u7edf\u9a8c\u8bc13D LiDAR\u611f\u77e5\u4e0b\u7684\u9c81\u68d2\u6027", "motivation": "\u73b0\u6709DTR\u7b97\u6cd5\u4f7f\u7528\u5916\u63a5\u5706\u751f\u6210\u8f68\u8ff9\u5bfc\u81f4\u8def\u5f84\u4e0d\u591f\u5e73\u6ed1\uff0c\u6027\u80fd\u4e0b\u964d\uff1bF1TENTH\u6a21\u62df\u5668\u7f3a\u4e4f3D LiDAR\u611f\u77e5\u652f\u6301\uff0c\u9650\u5236\u771f\u5b9e\u6d4b\u8bd5\u6548\u679c", "method": "\u57fa\u4e8eCurvature Corrected Moving Average\u6539\u8fdb\u8f68\u8ff9\u5e73\u6ed1\u5ea6\uff0c\u5728CARLA\u6a21\u62df\u5668\u4e2d\u6784\u5efa\u6570\u5b57\u5b6a\u751f\u7cfb\u7edf\u8fdb\u884c3D LiDAR\u611f\u77e5\u9a8c\u8bc1", "result": "\u901a\u8fc7\u4eff\u771f\u548c\u771f\u5b9e\u8f66\u8f86\u5b9e\u9a8c\u5168\u9762\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u7684\u6709\u6548\u6027", "conclusion": "MCTR\u7b97\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u8f68\u8ff9\u5e73\u6ed1\u5ea6\u548c3D\u611f\u77e5\u9a8c\u8bc1\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u8d5b\u8f66\u6027\u80fd"}}
{"id": "2508.12687", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.12687", "abs": "https://arxiv.org/abs/2508.12687", "authors": ["Ashish Seth", "Utkarsh Tyagi", "Ramaneswaran Selvakumar", "Nishit Anand", "Sonal Kumar", "Sreyan Ghosh", "Ramani Duraiswami", "Chirag Agarwal", "Dinesh Manocha"], "title": "EGOILLUSION: Benchmarking Hallucinations in Egocentric Video Understanding", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\nperformance in complex multimodal tasks. While MLLMs excel at visual perception\nand reasoning in third-person and egocentric videos, they are prone to\nhallucinations, generating coherent yet inaccurate responses. We present\nEgoIllusion, a first benchmark to evaluate MLLM hallucinations in egocentric\nvideos. EgoIllusion comprises 1,400 videos paired with 8,000 human-annotated\nopen and closed-ended questions designed to trigger hallucinations in both\nvisual and auditory cues in egocentric videos. Evaluations across ten MLLMs\nreveal significant challenges, including powerful models like GPT-4o and\nGemini, achieving only 59% accuracy. EgoIllusion lays the foundation in\ndeveloping robust benchmarks to evaluate the effectiveness of MLLMs and spurs\nthe development of better egocentric MLLMs with reduced hallucination rates.\nOur benchmark will be open-sourced for reproducibility.", "AI": {"tldr": "EgoIllusion\u662f\u9996\u4e2a\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u6211\u4e2d\u5fc3\u89c6\u9891\u4e2d\u5e7b\u89c9\u95ee\u9898\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b1400\u4e2a\u89c6\u9891\u548c8000\u4e2a\u4eba\u5de5\u6807\u6ce8\u95ee\u9898\uff0c\u6d4b\u8bd5\u663e\u793a\u5305\u62ecGPT-4o\u548cGemini\u5728\u5185\u7684\u9876\u7ea7\u6a21\u578b\u51c6\u786e\u7387\u4ec559%", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7b2c\u4e09\u4eba\u79f0\u548c\u81ea\u6211\u4e2d\u5fc3\u89c6\u9891\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5bb9\u6613\u4ea7\u751f\u8fde\u8d2f\u4f46\u4e0d\u51c6\u786e\u7684\u5e7b\u89c9\u56de\u7b54\uff0c\u9700\u8981\u4e13\u95e8\u7684\u57fa\u51c6\u6765\u8bc4\u4f30\u548c\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898", "method": "\u6784\u5efa\u5305\u542b1400\u4e2a\u89c6\u9891\u548c8000\u4e2a\u4eba\u5de5\u6807\u6ce8\u95ee\u9898\u7684EgoIllusion\u57fa\u51c6\uff0c\u8bbe\u8ba1\u5f00\u653e\u6027\u548c\u5c01\u95ed\u6027\u95ee\u9898\u6765\u89e6\u53d1\u89c6\u89c9\u548c\u542c\u89c9\u7ebf\u7d22\u7684\u5e7b\u89c9", "result": "\u6d4b\u8bd510\u4e2aMLLM\u6a21\u578b\u53d1\u73b0\u663e\u8457\u6311\u6218\uff0c\u6700\u5f3a\u6a21\u578bGPT-4o\u548cGemini\u51c6\u786e\u7387\u4ec559%\uff0c\u8868\u660e\u5f53\u524d\u6a21\u578b\u5728\u81ea\u6211\u4e2d\u5fc3\u89c6\u9891\u7406\u89e3\u65b9\u9762\u5b58\u5728\u4e25\u91cd\u5e7b\u89c9\u95ee\u9898", "conclusion": "EgoIllusion\u4e3a\u8bc4\u4f30MLLM\u6709\u6548\u6027\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5c06\u63a8\u52a8\u5f00\u53d1\u5e7b\u89c9\u7387\u66f4\u4f4e\u7684\u81ea\u6211\u4e2d\u5fc3MLLM\uff0c\u57fa\u51c6\u5c06\u5f00\u6e90\u4ee5\u786e\u4fdd\u53ef\u590d\u73b0\u6027"}}
{"id": "2508.12916", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.12916", "abs": "https://arxiv.org/abs/2508.12916", "authors": ["Hecheng Wang", "Jiankun Ren", "Jia Yu", "Lizhe Qi", "Yunquan Sun"], "title": "RoboRetriever: Single-Camera Robot Object Retrieval via Active and Interactive Perception with Dynamic Scene Graph", "comment": null, "summary": "Humans effortlessly retrieve objects in cluttered, partially observable\nenvironments by combining visual reasoning, active viewpoint adjustment, and\nphysical interaction-with only a single pair of eyes. In contrast, most\nexisting robotic systems rely on carefully positioned fixed or multi-camera\nsetups with complete scene visibility, which limits adaptability and incurs\nhigh hardware costs. We present \\textbf{RoboRetriever}, a novel framework for\nreal-world object retrieval that operates using only a \\textbf{single}\nwrist-mounted RGB-D camera and free-form natural language instructions.\nRoboRetriever grounds visual observations to build and update a \\textbf{dynamic\nhierarchical scene graph} that encodes object semantics, geometry, and\ninter-object relations over time. The supervisor module reasons over this\nmemory and task instruction to infer the target object and coordinate an\nintegrated action module combining \\textbf{active perception},\n\\textbf{interactive perception}, and \\textbf{manipulation}. To enable\ntask-aware scene-grounded active perception, we introduce a novel visual\nprompting scheme that leverages large reasoning vision-language models to\ndetermine 6-DoF camera poses aligned with the semantic task goal and geometry\nscene context. We evaluate RoboRetriever on diverse real-world object retrieval\ntasks, including scenarios with human intervention, demonstrating strong\nadaptability and robustness in cluttered scenes with only one RGB-D camera.", "AI": {"tldr": "RoboRetriever\u662f\u4e00\u4e2a\u4ec5\u4f7f\u7528\u5355\u4e2a\u8155\u6234RGB-D\u76f8\u673a\u548c\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u7684\u673a\u5668\u4eba\u5bf9\u8c61\u68c0\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u5c42\u6b21\u573a\u666f\u56fe\u548c\u4e3b\u52a8\u611f\u77e5\u5b9e\u73b0\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u7269\u4f53\u68c0\u7d22", "motivation": "\u4eba\u7c7b\u80fd\u591f\u8f7b\u677e\u5728\u6742\u4e71\u73af\u5883\u4e2d\u68c0\u7d22\u7269\u4f53\uff0c\u800c\u73b0\u6709\u673a\u5668\u4eba\u7cfb\u7edf\u4f9d\u8d56\u591a\u6444\u50cf\u5934\u8bbe\u7f6e\uff0c\u6210\u672c\u9ad8\u4e14\u9002\u5e94\u6027\u5dee\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u7075\u6d3b\u7684\u5355\u6444\u50cf\u5934\u89e3\u51b3\u65b9\u6848", "method": "\u6784\u5efa\u52a8\u6001\u5c42\u6b21\u573a\u666f\u56fe\u7f16\u7801\u5bf9\u8c61\u8bed\u4e49\u548c\u51e0\u4f55\u5173\u7cfb\uff0c\u4f7f\u7528\u89c6\u89c9\u63d0\u793a\u65b9\u6848\u7ed3\u5408\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u786e\u5b9a6\u81ea\u7531\u5ea6\u76f8\u673a\u4f4d\u59ff\uff0c\u96c6\u6210\u4e3b\u52a8\u611f\u77e5\u3001\u4ea4\u4e92\u611f\u77e5\u548c\u64cd\u4f5c", "result": "\u5728\u591a\u6837\u5316\u771f\u5b9e\u4e16\u754c\u5bf9\u8c61\u68c0\u7d22\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5305\u62ec\u6709\u4eba\u5e72\u9884\u7684\u573a\u666f", "conclusion": "\u8be5\u6846\u67b6\u8bc1\u660e\u4e86\u4ec5\u4f7f\u7528\u5355\u4e2aRGB-D\u76f8\u673a\u5c31\u80fd\u5728\u6742\u4e71\u73af\u5883\u4e2d\u6709\u6548\u68c0\u7d22\u7269\u4f53\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u673a\u5668\u4eba\u611f\u77e5\u548c\u64cd\u4f5c\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.12725", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12725", "abs": "https://arxiv.org/abs/2508.12725", "authors": ["Wenjie Chen", "Wenbin Li", "Di Yao", "Xuying Meng", "Chang Gong", "Jingping Bi"], "title": "GTool: Graph Enhanced Tool Planning with Large Language Model", "comment": "16 pages, 9 figures", "summary": "Tool planning with large language models (LLMs), referring to selecting,\norganizing, and preparing the tools necessary to complete a user request,\nbridges the gap between natural language understanding and task execution.\nHowever, current works treat different tools as isolated components and fail to\nleverage the inherent dependencies of tools, leading to invalid planning\nresults. Since tool dependencies are often incomplete, it becomes challenging\nfor LLMs to accurately identify the appropriate tools required by a user\nrequest, especially when confronted with a large toolset. To solve this\nchallenge, we propose \\texttt{GTool}, which is the first work aiming to enhance\nthe tool planning ability of LLMs under incomplete dependencies. \\texttt{GTool}\nconstructs a request-specific tool graph to select tools efficiently and\ngenerate the \\texttt{<graph token>} which provides sufficient dependency\ninformation understandable by LLMs. Moreover, a missing dependency prediction\ntask is designed to improve the reliability of \\texttt{GTool} with incomplete\ndependencies. Without trimming LLMs, \\texttt{GTool} can be seamlessly\nintegrated with various LLM backbones without extensive retraining. Extensive\nexperiments show that \\texttt{GTool} achieves more than 29.6\\% performance\nimprovements compared with the state-of-the-art (SOTA) baselines with a\nlight-weight (7B) LLM backbone.", "AI": {"tldr": "GTool\u662f\u4e00\u4e2a\u589e\u5f3aLLM\u5728\u5de5\u5177\u4f9d\u8d56\u4e0d\u5b8c\u6574\u60c5\u51b5\u4e0b\u8fdb\u884c\u5de5\u5177\u89c4\u5212\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u8bf7\u6c42\u7279\u5b9a\u7684\u5de5\u5177\u56fe\u548c\u751f\u6210\u56fe\u6807\u8bb0\u6765\u63d0\u4f9b\u4f9d\u8d56\u4fe1\u606f\uff0c\u76f8\u6bd4\u73b0\u6709SOTA\u65b9\u6cd5\u6027\u80fd\u63d0\u534729.6%", "motivation": "\u5f53\u524d\u5de5\u5177\u89c4\u5212\u65b9\u6cd5\u5c06\u4e0d\u540c\u5de5\u5177\u89c6\u4e3a\u5b64\u7acb\u7ec4\u4ef6\uff0c\u672a\u80fd\u5229\u7528\u5de5\u5177\u95f4\u7684\u5185\u5728\u4f9d\u8d56\u5173\u7cfb\uff0c\u5bfc\u81f4\u89c4\u5212\u7ed3\u679c\u65e0\u6548\u3002\u7279\u522b\u662f\u5728\u5de5\u5177\u4f9d\u8d56\u4e0d\u5b8c\u6574\u548c\u5927\u89c4\u6a21\u5de5\u5177\u96c6\u7684\u60c5\u51b5\u4e0b\uff0cLLM\u96be\u4ee5\u51c6\u786e\u8bc6\u522b\u7528\u6237\u8bf7\u6c42\u6240\u9700\u7684\u5408\u9002\u5de5\u5177", "method": "\u63d0\u51faGTool\u65b9\u6cd5\uff1a1) \u6784\u5efa\u8bf7\u6c42\u7279\u5b9a\u7684\u5de5\u5177\u56fe\u6765\u9ad8\u6548\u9009\u62e9\u5de5\u5177\uff1b2) \u751f\u6210<graph token>\u4e3aLLM\u63d0\u4f9b\u53ef\u7406\u89e3\u7684\u4f9d\u8d56\u4fe1\u606f\uff1b3) \u8bbe\u8ba1\u7f3a\u5931\u4f9d\u8d56\u9884\u6d4b\u4efb\u52a1\u63d0\u9ad8\u5728\u4e0d\u5b8c\u6574\u4f9d\u8d56\u4e0b\u7684\u53ef\u9760\u6027\uff1b\u65e0\u9700\u4fee\u526aLLM\uff0c\u53ef\u4e0e\u5404\u79cdLLM\u4e3b\u5e72\u65e0\u7f1d\u96c6\u6210", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cGTool\u5728\u4f7f\u7528\u8f7b\u91cf\u7ea7(7B)LLM\u4e3b\u5e72\u7684\u60c5\u51b5\u4e0b\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\u5b9e\u73b0\u4e86\u8d85\u8fc729.6%\u7684\u6027\u80fd\u63d0\u5347", "conclusion": "GTool\u662f\u7b2c\u4e00\u4e2a\u65e8\u5728\u589e\u5f3aLLM\u5728\u4e0d\u5b8c\u6574\u4f9d\u8d56\u4e0b\u5de5\u5177\u89c4\u5212\u80fd\u529b\u7684\u5de5\u4f5c\uff0c\u901a\u8fc7\u5de5\u5177\u56fe\u6784\u5efa\u548c\u56fe\u6807\u8bb0\u751f\u6210\u6709\u6548\u89e3\u51b3\u4e86\u5de5\u5177\u4f9d\u8d56\u4e0d\u5b8c\u6574\u7684\u95ee\u9898\uff0c\u5177\u6709\u5f88\u597d\u7684\u901a\u7528\u6027\u548c\u5b9e\u7528\u6027"}}
{"id": "2508.12925", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.12925", "abs": "https://arxiv.org/abs/2508.12925", "authors": ["Eetu Laukka", "Evan G. Center", "Timo Ojala", "Steven M. LaValle", "Matti Pouke"], "title": "Deformation of the panoramic sphere into an ellipsoid to induce self-motion in telepresence users", "comment": "2025 IEEE Conference on Telepresence", "summary": "Mobile telepresence robots allow users to feel present and explore remote\nenvironments using technology. Traditionally, these systems are implemented\nusing a camera onboard a mobile robot that can be controlled. Although\nhigh-immersion technologies, such as 360-degree cameras, can increase\nsituational awareness and presence, they also introduce significant challenges.\nAdditional processing and bandwidth requirements often result in latencies of\nup to seconds. The current delay with a 360-degree camera streaming over the\ninternet makes real-time control of these systems difficult. Working with\nhigh-latency systems requires some form of assistance to the users.\n  This study presents a novel way to utilize optical flow to create an illusion\nof self-motion to the user during the latency period between user sending\nmotion commands to the robot and seeing the actual motion through the\n360-camera stream. We find no significant benefit of using the self-motion\nillusion to performance or accuracy of controlling a telepresence robot with a\nlatency of 500 ms, as measured by the task completion time and collisions into\nobjects. Some evidence is shown that the method might increase virtual reality\n(VR) sickness, as measured by the simulator sickness questionnaire (SSQ). We\nconclude that further adjustments are necessary in order to render the method\nviable.", "AI": {"tldr": "\u901a\u8fc7\u5149\u6d41\u6280\u672f\u5728\u9ad8\u5ef6\u8fdf\u73af\u5883\u4e2d\u4e3a\u79fb\u52a8\u9690\u5f62\u673a\u5668\u4eba\u521b\u9020\u81ea\u6211\u8fd0\u52a8\u5e7b\u89c9\uff0c\u4f46\u5728500ms\u5ef6\u8fdf\u4e0b\u672a\u663e\u8457\u63d0\u5347\u63a7\u5236\u6027\u80fd\uff0c\u53cd\u800c\u53ef\u80fd\u589e\u52a0VR\u8214\u6655\u611f", "motivation": "\u89e3\u51b3\u4f7f\u7528360\u5ea6\u6444\u50cf\u5934\u7684\u79fb\u52a8\u9690\u5f62\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u7f51\u7edc\u5ef6\u8fdf\u4e0b\u5b9e\u65f6\u63a7\u5236\u56f0\u96be\u7684\u95ee\u9898\uff0c\u9700\u8981\u67d0\u79cd\u5f62\u5f0f\u7684\u7528\u6237\u534f\u52a9\u6280\u672f", "method": "\u5229\u7528\u5149\u6d41\u6280\u672f\u5728\u7528\u6237\u53d1\u9001\u8fd0\u52a8\u547d\u4ee4\u5230\u771f\u5b9e\u770b\u5230\u8fd0\u52a8\u89c6\u9891\u4e4b\u95f4\u7684\u5ef6\u8fdf\u671f\u95f4\uff0c\u4e3a\u7528\u6237\u521b\u9020\u81ea\u6211\u8fd0\u52a8\u7684\u5e7b\u89c9\u6548\u679c", "result": "\u5728500ms\u5ef6\u8fdf\u6761\u4ef6\u4e0b\uff0c\u8be5\u65b9\u6cd5\u5bf9\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u548c\u78b0\u649e\u7269\u4f53\u7684\u63a7\u5236\u51c6\u786e\u6027\u6ca1\u6709\u663e\u8457\u6539\u5584\uff0c\u53cd\u800c\u53ef\u80fd\u4f1a\u589e\u52a0\u865a\u62df\u73b0\u5b9e\u8214\u6655\u611f", "conclusion": "\u8be5\u5149\u6d41\u5e7b\u89c9\u6280\u672f\u9700\u8981\u8fdb\u4e00\u6b65\u8c03\u6574\u548c\u4f18\u5316\u624d\u80fd\u6210\u4e3a\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.12754", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12754", "abs": "https://arxiv.org/abs/2508.12754", "authors": ["Alessio Galatolo", "Luca Alberto Rappuoli", "Katie Winkle", "Meriem Beloucif"], "title": "Beyond Ethical Alignment: Evaluating LLMs as Artificial Moral Assistants", "comment": "Full version of the paper published in ECAI 2025 proceedings (IOS\n  Press, CC BY-NC 4.0)", "summary": "The recent rise in popularity of large language models (LLMs) has prompted\nconsiderable concerns about their moral capabilities. Although considerable\neffort has been dedicated to aligning LLMs with human moral values, existing\nbenchmarks and evaluations remain largely superficial, typically measuring\nalignment based on final ethical verdicts rather than explicit moral reasoning.\nIn response, this paper aims to advance the investigation of LLMs' moral\ncapabilities by examining their capacity to function as Artificial Moral\nAssistants (AMAs), systems envisioned in the philosophical literature to\nsupport human moral deliberation. We assert that qualifying as an AMA requires\nmore than what state-of-the-art alignment techniques aim to achieve: not only\nmust AMAs be able to discern ethically problematic situations, they should also\nbe able to actively reason about them, navigating between conflicting values\noutside of those embedded in the alignment phase. Building on existing\nphilosophical literature, we begin by designing a new formal framework of the\nspecific kind of behaviour an AMA should exhibit, individuating key qualities\nsuch as deductive and abductive moral reasoning. Drawing on this theoretical\nframework, we develop a benchmark to test these qualities and evaluate popular\nopen LLMs against it. Our results reveal considerable variability across models\nand highlight persistent shortcomings, particularly regarding abductive moral\nreasoning. Our work connects theoretical philosophy with practical AI\nevaluation while also emphasising the need for dedicated strategies to\nexplicitly enhance moral reasoning capabilities in LLMs. Code available at\nhttps://github.com/alessioGalatolo/AMAeval", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u4eba\u5de5\u9053\u5fb7\u52a9\u624b\u80fd\u529b\u7684\u65b0\u6846\u67b6\u548c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u91cd\u70b9\u5173\u6ce8\u9053\u5fb7\u63a8\u7406\u80fd\u529b\u800c\u975e\u8868\u9762\u4f26\u7406\u5224\u65ad\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u6eaf\u56e0\u9053\u5fb7\u63a8\u7406\u65b9\u9762\u5b58\u5728\u660e\u663e\u4e0d\u8db3", "motivation": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u9053\u5fb7\u8bc4\u4f30\u8fc7\u4e8e\u8868\u9762\u5316\uff0c\u53ea\u5173\u6ce8\u6700\u7ec8\u4f26\u7406\u5224\u65ad\u800c\u5ffd\u89c6\u9053\u5fb7\u63a8\u7406\u8fc7\u7a0b\u3002\u8bba\u6587\u65e8\u5728\u901a\u8fc7\u54f2\u5b66\u7406\u8bba\u6784\u5efa\u66f4\u6df1\u5165\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u6d4b\u8bd5LLMs\u4f5c\u4e3a\u4eba\u5de5\u9053\u5fb7\u52a9\u624b\u7684\u80fd\u529b", "method": "\u57fa\u4e8e\u54f2\u5b66\u6587\u732e\u8bbe\u8ba1\u4eba\u5de5\u9053\u5fb7\u52a9\u624b\u7684\u884c\u4e3a\u6846\u67b6\uff0c\u8bc6\u522b\u5173\u952e\u80fd\u529b\u5982\u6f14\u7ece\u548c\u6eaf\u56e0\u9053\u5fb7\u63a8\u7406\uff0c\u5e76\u5f00\u53d1\u76f8\u5e94\u7684\u57fa\u51c6\u6d4b\u8bd5\u6765\u8bc4\u4f30\u4e3b\u6d41\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b", "result": "\u4e0d\u540c\u6a21\u578b\u8868\u73b0\u5dee\u5f02\u663e\u8457\uff0c\u7279\u522b\u662f\u5728\u6eaf\u56e0\u9053\u5fb7\u63a8\u7406\u65b9\u9762\u5b58\u5728\u6301\u7eed\u6027\u7684\u7f3a\u9677\u548c\u4e0d\u8db3", "conclusion": "\u9700\u8981\u4e13\u95e8\u7684\u7b56\u7565\u6765\u663e\u5f0f\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9053\u5fb7\u63a8\u7406\u80fd\u529b\uff0c\u5c06\u7406\u8bba\u54f2\u5b66\u4e0e\u5b9e\u7528AI\u8bc4\u4f30\u76f8\u7ed3\u5408\u5177\u6709\u91cd\u8981\u610f\u4e49"}}
{"id": "2508.12928", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.12928", "abs": "https://arxiv.org/abs/2508.12928", "authors": ["Victor Dh\u00e9din", "Haizhou Zhao", "Majid Khadiv"], "title": "Simultaneous Contact Sequence and Patch Planning for Dynamic Locomotion", "comment": null, "summary": "Legged robots have the potential to traverse highly constrained environments\nwith agile maneuvers. However, planning such motions requires solving a highly\nchallenging optimization problem with a mixture of continuous and discrete\ndecision variables. In this paper, we present a full pipeline based on\nMonte-Carlo tree search (MCTS) and whole-body trajectory optimization (TO) to\nperform simultaneous contact sequence and patch selection on highly challenging\nenvironments. Through extensive simulation experiments, we show that our\nframework can quickly find a diverse set of dynamically consistent plans. We\nexperimentally show that these plans are transferable to a real quadruped\nrobot. We further show that the same framework can find highly complex acyclic\nhumanoid maneuvers. To the best of our knowledge, this is the first\ndemonstration of simultaneous contact sequence and patch selection for acyclic\nmulti-contact locomotion using the whole-body dynamics of a quadruped.", "AI": {"tldr": "\u57fa\u4e8eMCTS\u548c\u5168\u8eab\u8f68\u8ff9\u4f18\u5316\u7684\u56db\u8db3\u673a\u5668\u4eba\u591a\u63a5\u89e6\u8fd0\u52a8\u89c4\u5212\u6846\u67b6\uff0c\u80fd\u591f\u540c\u65f6\u4f18\u5316\u63a5\u89e6\u5e8f\u5217\u548c\u63a5\u89e6\u9762\u9009\u62e9\uff0c\u6210\u529f\u5728\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u590d\u6742\u65e0\u73af\u8fd0\u52a8", "motivation": "\u817f\u5f0f\u673a\u5668\u4eba\u9700\u8981\u5728\u9ad8\u5ea6\u53d7\u9650\u73af\u5883\u4e2d\u8fdb\u884c\u654f\u6377\u8fd0\u52a8\uff0c\u4f46\u89c4\u5212\u8fd9\u7c7b\u8fd0\u52a8\u9700\u8981\u89e3\u51b3\u5305\u542b\u8fde\u7eed\u548c\u79bb\u6563\u51b3\u7b56\u53d8\u91cf\u7684\u590d\u6742\u4f18\u5316\u95ee\u9898", "method": "\u91c7\u7528\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22(MCTS)\u548c\u5168\u8eab\u8f68\u8ff9\u4f18\u5316(TO)\u7684\u5b8c\u6574\u6d41\u6c34\u7ebf\uff0c\u5b9e\u73b0\u63a5\u89e6\u5e8f\u5217\u548c\u63a5\u89e6\u9762\u9009\u62e9\u7684\u540c\u6b65\u4f18\u5316", "result": "\u4eff\u771f\u5b9e\u9a8c\u663e\u793a\u6846\u67b6\u80fd\u5feb\u901f\u627e\u5230\u591a\u79cd\u52a8\u6001\u4e00\u81f4\u7684\u8fd0\u52a8\u8ba1\u5212\uff0c\u8fd9\u4e9b\u8ba1\u5212\u53ef\u6210\u529f\u8fc1\u79fb\u5230\u771f\u5b9e\u56db\u8db3\u673a\u5668\u4eba\uff0c\u5e76\u80fd\u5b9e\u73b0\u590d\u6742\u7684\u4eba\u5f62\u673a\u5668\u4eba\u65e0\u73af\u52a8\u4f5c", "conclusion": "\u8fd9\u662f\u9996\u6b21\u5c55\u793a\u57fa\u4e8e\u5168\u8eab\u52a8\u529b\u5b66\u7684\u56db\u8db3\u673a\u5668\u4eba\u65e0\u73af\u591a\u63a5\u89e6\u8fd0\u52a8\u7684\u540c\u65f6\u63a5\u89e6\u5e8f\u5217\u548c\u63a5\u89e6\u9762\u9009\u62e9\u65b9\u6cd5"}}
{"id": "2508.12782", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12782", "abs": "https://arxiv.org/abs/2508.12782", "authors": ["Petr Anokhin", "Roman Khalikov", "Stefan Rebrikov", "Viktor Volkov", "Artyom Sorokin", "Vincent Bissonnette"], "title": "HeroBench: A Benchmark for Long-Horizon Planning and Structured Reasoning in Virtual Worlds", "comment": "Code is available at https://github.com/stefanrer/HeroBench", "summary": "Large language models (LLMs) have shown remarkable capabilities in isolated\nstep-by-step reasoning tasks such as mathematics and programming, but their\nproficiency in long-horizon planning, where solutions require extended,\nstructured sequences of interdependent actions, remains underexplored. Existing\nbenchmarks typically assess LLMs through abstract or low-dimensional\nalgorithmic tasks, failing to capture the complexity of realistic planning\nenvironments. We introduce HeroBench, a novel benchmark designed specifically\nto evaluate long-horizon planning and structured reasoning within complex\nRPG-inspired virtual worlds. HeroBench provides a rigorously constructed\ndataset of tasks covering a wide range of difficulties, a simulated environment\nto execute and validate agent plans, and detailed analytical tools for\nevaluating model performance. Tasks challenge models to formulate strategic\nplans, efficiently gather resources, master necessary skills, craft equipment,\nand defeat adversaries, reflecting practical scenarios' layered dependencies\nand constraints. Our extensive evaluation of 25 state-of-the-art LLMs, spanning\nboth open-source and proprietary models, including the GPT-5 family, reveals\nsubstantial performance disparities rarely observed in conventional reasoning\nbenchmarks. Detailed error analysis further uncovers specific weaknesses in\ncurrent models' abilities to generate robust high-level plans and reliably\nexecute structured actions. HeroBench thus not only significantly advances the\nevaluation of LLM reasoning but also provides a flexible, scalable foundation\nfor future research into advanced, autonomous planning in virtual environments.", "AI": {"tldr": "HeroBench\u662f\u4e00\u4e2a\u4e13\u95e8\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742RPG\u865a\u62df\u4e16\u754c\u4e2d\u957f\u65f6\u7a0b\u89c4\u5212\u548c\u7ed3\u6784\u5316\u63a8\u7406\u80fd\u529b\u7684\u65b0\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u751f\u6210\u7a33\u5065\u9ad8\u5c42\u8ba1\u5212\u548c\u6267\u884c\u7ed3\u6784\u5316\u884c\u52a8\u65b9\u9762\u7684\u663e\u8457\u5f31\u70b9\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u901a\u5e38\u901a\u8fc7\u62bd\u8c61\u6216\u4f4e\u7ef4\u7b97\u6cd5\u4efb\u52a1\u8bc4\u4f30LLMs\uff0c\u65e0\u6cd5\u6355\u6349\u73b0\u5b9e\u89c4\u5212\u73af\u5883\u7684\u590d\u6742\u6027\uff0c\u9700\u8981\u4e13\u95e8\u8bc4\u4f30\u957f\u65f6\u7a0b\u89c4\u5212\u548c\u7ed3\u6784\u5316\u63a8\u7406\u80fd\u529b\u7684\u57fa\u51c6\u3002", "method": "\u5f15\u5165HeroBench\u57fa\u51c6\uff0c\u5305\u542b\u4e25\u683c\u6784\u5efa\u7684\u4efb\u52a1\u6570\u636e\u96c6\u3001\u6a21\u62df\u6267\u884c\u73af\u5883\u9a8c\u8bc1\u4ee3\u7406\u8ba1\u5212\u3001\u8be6\u7ec6\u5206\u6790\u5de5\u5177\uff0c\u6db5\u76d6\u8d44\u6e90\u6536\u96c6\u3001\u6280\u80fd\u638c\u63e1\u3001\u88c5\u5907\u5236\u4f5c\u3001\u5bf9\u6297\u654c\u4eba\u7b49\u590d\u6742\u4efb\u52a1\u3002", "result": "\u5bf925\u4e2a\u6700\u5148\u8fdbLLMs\uff08\u5305\u62ec\u5f00\u6e90\u548c\u4e13\u6709\u6a21\u578b\u3001GPT-5\u5bb6\u65cf\uff09\u7684\u5e7f\u6cdb\u8bc4\u4f30\u663e\u793a\uff0c\u4e0e\u4f20\u7edf\u63a8\u7406\u57fa\u51c6\u76f8\u6bd4\u5b58\u5728\u663e\u8457\u6027\u80fd\u5dee\u5f02\uff0c\u8be6\u7ec6\u9519\u8bef\u5206\u6790\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u7684\u5177\u4f53\u5f31\u70b9\u3002", "conclusion": "HeroBench\u4e0d\u4ec5\u663e\u8457\u63a8\u8fdb\u4e86LLM\u63a8\u7406\u8bc4\u4f30\uff0c\u8fd8\u4e3a\u672a\u6765\u5728\u865a\u62df\u73af\u5883\u4e2d\u8fdb\u884c\u9ad8\u7ea7\u81ea\u4e3b\u89c4\u5212\u7814\u7a76\u63d0\u4f9b\u4e86\u7075\u6d3b\u3001\u53ef\u6269\u5c55\u7684\u57fa\u7840\u3002"}}
{"id": "2508.12946", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.12946", "abs": "https://arxiv.org/abs/2508.12946", "authors": ["Ann-Sophie Schenk", "Stefan Schiffer", "Heqiu Song"], "title": "Insights from Interviews with Teachers and Students on the Use of a Social Robot in Computer Science Class in Sixth Grade", "comment": null, "summary": "In this paper we report on first insights from interviews with teachers and\nstudents on using social robots in computer science class in sixth grade. Our\nfocus is on learning about requirements and potential applications. We are\nparticularly interested in getting both perspectives, the teachers' and the\nlearners' view on how robots could be used and what features they should or\nshould not have. Results show that teachers as well as students are very open\nto robots in the classroom. However, requirements are partially quite\nheterogeneous among the groups. This leads to complex design challenges which\nwe discuss at the end of this paper.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u8bbf\u8c08\u63a2\u8ba8\u516d\u5e74\u7ea7\u8ba1\u7b97\u673a\u79d1\u5b66\u8bfe\u5802\u4e2d\u4f7f\u7528\u793e\u4ea4\u673a\u5668\u4eba\u7684\u9700\u6c42\u548c\u6f5c\u5728\u5e94\u7528\uff0c\u53d1\u73b0\u5e08\u751f\u5bf9\u673a\u5668\u4eba\u6301\u5f00\u653e\u6001\u5ea6\u4f46\u9700\u6c42\u5b58\u5728\u5dee\u5f02", "motivation": "\u4e86\u89e3\u6559\u5e08\u548c\u5b66\u751f\u5bf9\u5728\u8ba1\u7b97\u673a\u79d1\u5b66\u8bfe\u5802\u4e2d\u4f7f\u7528\u793e\u4ea4\u673a\u5668\u4eba\u7684\u770b\u6cd5\u3001\u9700\u6c42\u4ee5\u53ca\u6f5c\u5728\u5e94\u7528\u573a\u666f\uff0c\u83b7\u53d6\u53cc\u65b9\u89c6\u89d2", "method": "\u5bf9\u6559\u5e08\u548c\u5b66\u751f\u8fdb\u884c\u8bbf\u8c08\uff0c\u6536\u96c6\u4ed6\u4eec\u5bf9\u8bfe\u5802\u793e\u4ea4\u673a\u5668\u4eba\u7684\u9700\u6c42\u548c\u529f\u80fd\u671f\u671b", "result": "\u6559\u5e08\u548c\u5b66\u751f\u90fd\u5bf9\u8bfe\u5802\u673a\u5668\u4eba\u6301\u5f00\u653e\u6001\u5ea6\uff0c\u4f46\u4e24\u7ec4\u4eba\u7fa4\u7684\u9700\u6c42\u5b58\u5728\u663e\u8457\u5dee\u5f02", "conclusion": "\u5e08\u751f\u9700\u6c42\u5dee\u5f02\u5e26\u6765\u4e86\u590d\u6742\u7684\u8bbe\u8ba1\u6311\u6218\uff0c\u9700\u8981\u5728\u673a\u5668\u4eba\u8bbe\u8ba1\u4e2d\u5e73\u8861\u4e0d\u540c\u7fa4\u4f53\u7684\u671f\u671b"}}
{"id": "2508.12790", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12790", "abs": "https://arxiv.org/abs/2508.12790", "authors": ["Zenan Huang", "Yihong Zhuang", "Guoshan Lu", "Zeyu Qin", "Haokai Xu", "Tianyu Zhao", "Ru Peng", "Jiaqi Hu", "Zhanming Shen", "Xiaomeng Hu", "Xijun Gu", "Peiyi Tu", "Jiaxin Liu", "Wenyu Chen", "Yuzhuo Fu", "Zhiting Fan", "Yanmei Gu", "Yuanyuan Wang", "Zhengkai Yang", "Jianguo Li", "Junbo Zhao"], "title": "Reinforcement Learning with Rubric Anchors", "comment": "technical report", "summary": "Reinforcement Learning from Verifiable Rewards (RLVR) has emerged as a\npowerful paradigm for enhancing Large Language Models (LLMs), exemplified by\nthe success of OpenAI's o-series. In RLVR, rewards are derived from verifiable\nsignals-such as passing unit tests in code generation or matching correct\nanswers in mathematical reasoning. While effective, this requirement largely\nconfines RLVR to domains with automatically checkable outcomes. To overcome\nthis, we extend the RLVR paradigm to open-ended tasks by integrating\nrubric-based rewards, where carefully designed rubrics serve as structured,\nmodel-interpretable criteria for automatic scoring of subjective outputs. We\nconstruct, to our knowledge, the largest rubric reward system to date, with\nover 10,000 rubrics from humans, LLMs, or a hybrid human-LLM collaboration.\nImplementing rubric-based RL is challenging; we tackle these issues with a\nclear framework and present an open-sourced Qwen-30B-A3B model with notable\ngains: 1) With only 5K+ samples, our system improves by +5.2% on open-ended\nbenchmarks (especially humanities), outperforming a 671B DeepSeek-V3 model by\n+2.4%, while preserving general and reasoning abilities. 2) Our method provides\nfine-grained stylistic control, using rubrics as anchors to mitigate the\n\"AI-like\" tone and produce more human-like, expressive responses. We share key\nlessons in rubric construction, data selection, and training, and discuss\nlimitations and future releases.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5c06\u53ef\u9a8c\u8bc1\u5956\u52b1\u5f3a\u5316\u5b66\u4e60(RLVR)\u6269\u5c55\u5230\u5f00\u653e\u5f0f\u4efb\u52a1\uff0c\u901a\u8fc7\u6784\u5efa\u5305\u542b10,000+\u91cf\u8868\u7684\u8bc4\u5206\u7cfb\u7edf\uff0c\u4f7f\u7528Qwen-30B-A3B\u6a21\u578b\u5728\u4ec55K+\u6837\u672c\u4e0b\u5b9e\u73b0+5.2%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u8d85\u8d8a671B\u5927\u6a21\u578b\uff0c\u5e76\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u98ce\u683c\u63a7\u5236\u3002", "motivation": "\u4f20\u7edfRLVR\u5c40\u9650\u4e8e\u53ef\u81ea\u52a8\u9a8c\u8bc1\u7684\u9886\u57df\uff0c\u65e0\u6cd5\u5904\u7406\u5f00\u653e\u5f0f\u4e3b\u89c2\u4efb\u52a1\u3002\u4e3a\u4e86\u7a81\u7834\u8fd9\u4e00\u9650\u5236\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u81ea\u52a8\u8bc4\u4f30\u4e3b\u89c2\u8f93\u51fa\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u91cf\u8868\u7684\u5956\u52b1\u7cfb\u7edf\uff0c\u6784\u5efa\u4e8610,000+\u4e2a\u4eba\u5de5\u3001LLM\u6216\u4eba\u673a\u534f\u4f5c\u8bbe\u8ba1\u7684\u8bc4\u5206\u91cf\u8868\uff0c\u4f5c\u4e3a\u7ed3\u6784\u5316\u3001\u6a21\u578b\u53ef\u89e3\u91ca\u7684\u81ea\u52a8\u8bc4\u5206\u6807\u51c6\u3002", "result": "\u4ec5\u4f7f\u75285K+\u6837\u672c\uff0c\u5728\u5f00\u653e\u5f0f\u57fa\u51c6(\u7279\u522b\u662f\u4eba\u6587\u5b66\u79d1)\u4e0a\u63d0\u5347+5.2%\uff0c\u8d85\u8d8a671B DeepSeek-V3\u6a21\u578b+2.4%\uff0c\u540c\u65f6\u4fdd\u6301\u901a\u7528\u548c\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u80fd\u4ea7\u751f\u66f4\u4eba\u6027\u5316\u7684\u8868\u8fbe\u3002", "conclusion": "\u57fa\u4e8e\u91cf\u8868\u7684RLVR\u6210\u529f\u6269\u5c55\u5230\u5f00\u653e\u5f0f\u4efb\u52a1\uff0c\u63d0\u4f9b\u4e86\u7ec6\u7c92\u5ea6\u98ce\u683c\u63a7\u5236\uff0c\u7f13\u89e3\u4e86\"AI\u8154\u8c03\"\u95ee\u9898\uff0c\u4e3a\u5f00\u653e\u5f0f\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\u3002"}}
{"id": "2508.12980", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.12980", "abs": "https://arxiv.org/abs/2508.12980", "authors": ["Victor Lev\u00e9", "Jo\u00e3o Moura", "Sachiya Fujita", "Tamon Miyake", "Steve Tonneau", "Sethu Vijayakumar"], "title": "Scaling Whole-body Multi-contact Manipulation with Contact Optimization", "comment": "This work has been accepted for publication in IEEE-RAS 24th\n  International Conference on Humanoid Robots (Humanoids 2025). Copyrights to\n  IEEE", "summary": "Daily tasks require us to use our whole body to manipulate objects, for\ninstance when our hands are unavailable. We consider the issue of providing\nhumanoid robots with the ability to autonomously perform similar whole-body\nmanipulation tasks. In this context, the infinite possibilities for where and\nhow contact can occur on the robot and object surfaces hinder the scalability\nof existing planning methods, which predominantly rely on discrete sampling.\nGiven the continuous nature of contact surfaces, gradient-based optimization\noffers a more suitable approach for finding solutions. However, a key remaining\nchallenge is the lack of an efficient representation of robot surfaces. In this\nwork, we propose (i) a representation of robot and object surfaces that enables\nclosed-form computation of proximity points, and (ii) a cost design that\neffectively guides whole-body manipulation planning. Our experiments\ndemonstrate that the proposed framework can solve problems unaddressed by\nexisting methods, and achieves a 77% improvement in planning time over the\nstate of the art. We also validate the suitability of our approach on real\nhardware through the whole-body manipulation of boxes by a humanoid robot.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u4eba\u5f62\u673a\u5668\u4eba\u5168\u8eab\u64cd\u7eb5\u7684\u8fde\u7eed\u63a5\u89e6\u8868\u9762\u8868\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c01\u95ed\u5f0f\u8ba1\u7b97\u90bb\u8fd1\u70b9\u548c\u6709\u6548\u7684\u6210\u672c\u8bbe\u8ba1\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u89c4\u5212\u65f6\u95f4\u63d0\u534777%\uff0c\u5e76\u5728\u771f\u5b9e\u786c\u4ef6\u4e0a\u9a8c\u8bc1\u4e86\u53ef\u884c\u6027\u3002", "motivation": "\u65e5\u5e38\u4efb\u52a1\u9700\u8981\u5168\u8eab\u64cd\u7eb5\u7269\u4f53\uff0c\u4f46\u73b0\u6709\u89c4\u5212\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u79bb\u6563\u91c7\u6837\uff0c\u96be\u4ee5\u6269\u5c55\u5230\u590d\u6742\u7684\u63a5\u89e6\u573a\u666f\u3002\u8fde\u7eed\u63a5\u89e6\u8868\u9762\u7684\u65e0\u9650\u53ef\u80fd\u6027\u963b\u788d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u53ef\u6269\u5c55\u6027\uff0c\u800c\u57fa\u4e8e\u68af\u5ea6\u4f18\u5316\u7684\u65b9\u6cd5\u7f3a\u4e4f\u9ad8\u6548\u7684\u673a\u5668\u4eba\u8868\u9762\u8868\u793a\u3002", "method": "\u63d0\u51fa(i)\u673a\u5668\u4eba\u53ca\u7269\u4f53\u8868\u9762\u7684\u8868\u793a\u65b9\u6cd5\uff0c\u652f\u6301\u90bb\u8fd1\u70b9\u7684\u5c01\u95ed\u5f0f\u8ba1\u7b97\uff1b(ii)\u6709\u6548\u7684\u6210\u672c\u8bbe\u8ba1\u6765\u6307\u5bfc\u5168\u8eab\u64cd\u7eb5\u89c4\u5212\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u6846\u67b6\u80fd\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u65e0\u6cd5\u5904\u7406\u7684\u95ee\u9898\uff0c\u89c4\u5212\u65f6\u95f4\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u63d0\u534777%\uff0c\u5e76\u5728\u4eba\u5f62\u673a\u5668\u4eba\u64cd\u7eb5\u76d2\u5b50\u7684\u771f\u5b9e\u786c\u4ef6\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u9002\u7528\u6027\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u8fde\u7eed\u63a5\u89e6\u8868\u9762\u8868\u793a\u548c\u89c4\u5212\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u5168\u8eab\u64cd\u7eb5\u4efb\u52a1\u7684\u89c4\u5212\u6548\u7387\u548c\u53ef\u884c\u6027\u3002"}}
{"id": "2508.12791", "categories": ["cs.AI", "cs.MA", "cs.SY", "eess.SY", "nlin.AO"], "pdf": "https://arxiv.org/pdf/2508.12791", "abs": "https://arxiv.org/abs/2508.12791", "authors": ["Imran Khan"], "title": "[Social] Allostasis: Or, How I Learned To Stop Worrying and Love The Noise", "comment": "20 pages, 5 figures. Accepted at ALIFE 2025 (Kyoto, Japan; October\n  6th - 10th 2025)", "summary": "The notion of homeostasis typically conceptualises biological and artificial\nsystems as maintaining stability by resisting deviations caused by\nenvironmental and social perturbations. In contrast, (social) allostasis\nproposes that these systems can proactively leverage these very perturbations\nto reconfigure their regulatory parameters in anticipation of environmental\ndemands, aligning with von Foerster's ``order through noise'' principle. This\npaper formulates a computational model of allostatic and social allostatic\nregulation that employs biophysiologically inspired signal transducers,\nanalogous to hormones like cortisol and oxytocin, to encode information from\nboth the environment and social interactions, which mediate this dynamic\nreconfiguration. The models are tested in a small society of ``animats'' across\nseveral dynamic environments, using an agent-based model. The results show that\nallostatic and social allostatic regulation enable agents to leverage\nenvironmental and social ``noise'' for adaptive reconfiguration, leading to\nimproved viability compared to purely reactive homeostatic agents. This work\noffers a novel computational perspective on the principles of social allostasis\nand their potential for designing more robust, bio-inspired, adaptive systems", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8ba1\u7b97\u6a21\u578b\uff0c\u5c06\u7a33\u6001\u8c03\u8282\u6269\u5c55\u4e3a\u5f02\u7a33\u6001\u548c\u793e\u4f1a\u5f02\u7a33\u6001\u8c03\u8282\uff0c\u901a\u8fc7\u751f\u7269\u542f\u53d1\u7684\u4fe1\u53f7\u8f6c\u5bfc\u673a\u5236\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u4e3b\u52a8\u5229\u7528\u73af\u5883\u548c\u793e\u4f1a\u6270\u52a8\u8fdb\u884c\u81ea\u9002\u5e94\u91cd\u6784\uff0c\u5728\u52a8\u6001\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u6bd4\u4f20\u7edf\u7a33\u6001\u8c03\u8282\u66f4\u597d\u7684\u751f\u5b58\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u7a33\u6001\u6982\u5ff5\u5f3a\u8c03\u7cfb\u7edf\u901a\u8fc7\u62b5\u6297\u6270\u52a8\u6765\u7ef4\u6301\u7a33\u5b9a\uff0c\u800c\u5f02\u7a33\u6001\u7406\u8bba\u63d0\u51fa\u7cfb\u7edf\u53ef\u4ee5\u4e3b\u52a8\u5229\u7528\u6270\u52a8\u6765\u9884\u6d4b\u6027\u5730\u91cd\u65b0\u914d\u7f6e\u8c03\u8282\u53c2\u6570\u3002\u672c\u6587\u65e8\u5728\u4ece\u8ba1\u7b97\u89d2\u5ea6\u9a8c\u8bc1\u8fd9\u4e00\u7406\u8bba\uff0c\u7279\u522b\u662f\u793e\u4f1a\u5f02\u7a33\u6001\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u4f18\u52bf\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u667a\u80fd\u4f53\u7684\u8ba1\u7b97\u6a21\u578b\uff0c\u4f7f\u7528\u751f\u7269\u751f\u7406\u542f\u53d1\u7684\u4fe1\u53f7\u8f6c\u5bfc\u5668\uff08\u7c7b\u4f3c\u76ae\u8d28\u9187\u548c\u50ac\u4ea7\u7d20\u7b49\u6fc0\u7d20\uff09\u6765\u7f16\u7801\u73af\u5883\u548c\u793e\u4f1a\u4ea4\u4e92\u4fe1\u606f\uff0c\u5e76\u5728\u52a8\u6001\u73af\u5883\u4e2d\u6d4b\u8bd5\u4e00\u4e2a\u5c0f\u578b\"\u52a8\u7269\"\u793e\u4f1a\u7684\u6027\u80fd\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u5f02\u7a33\u6001\u548c\u793e\u4f1a\u5f02\u7a33\u6001\u8c03\u8282\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u5229\u7528\u73af\u5883\u548c\u793e\u4f1a\"\u566a\u58f0\"\u8fdb\u884c\u81ea\u9002\u5e94\u91cd\u6784\uff0c\u76f8\u6bd4\u7eaf\u53cd\u5e94\u6027\u7a33\u6001\u667a\u80fd\u4f53\u5177\u6709\u66f4\u597d\u7684\u751f\u5b58\u80fd\u529b\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u793e\u4f1a\u5f02\u7a33\u6001\u539f\u7406\u63d0\u4f9b\u4e86\u65b0\u7684\u8ba1\u7b97\u89c6\u89d2\uff0c\u4e3a\u8bbe\u8ba1\u66f4\u9c81\u68d2\u3001\u751f\u7269\u542f\u53d1\u7684\u81ea\u9002\u5e94\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6f5c\u5728\u9014\u5f84\u3002"}}
{"id": "2508.13052", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.13052", "abs": "https://arxiv.org/abs/2508.13052", "authors": ["Sourav Raxit", "Abdullah Al Redwan Newaz", "Paulo Padrao", "Jose Fuentes", "Leonardo Bobadilla"], "title": "BOW: Bayesian Optimization over Windows for Motion Planning in Complex Environments", "comment": null, "summary": "This paper introduces the BOW Planner, a scalable motion planning algorithm\ndesigned to navigate robots through complex environments using constrained\nBayesian optimization (CBO). Unlike traditional methods, which often struggle\nwith kinodynamic constraints such as velocity and acceleration limits, the BOW\nPlanner excels by concentrating on a planning window of reachable velocities\nand employing CBO to sample control inputs efficiently. This approach enables\nthe planner to manage high-dimensional objective functions and stringent safety\nconstraints with minimal sampling, ensuring rapid and secure trajectory\ngeneration. Theoretical analysis confirms the algorithm's asymptotic\nconvergence to near-optimal solutions, while extensive evaluations in cluttered\nand constrained settings reveal substantial improvements in computation times,\ntrajectory lengths, and solution times compared to existing techniques.\nSuccessfully deployed across various real-world robotic systems, the BOW\nPlanner demonstrates its practical significance through exceptional sample\nefficiency, safety-aware optimization, and rapid planning capabilities, making\nit a valuable tool for advancing robotic applications. The BOW Planner is\nreleased as an open-source package and videos of real-world and simulated\nexperiments are available at https://bow-web.github.io.", "AI": {"tldr": "BOW Planner\u662f\u4e00\u4e2a\u57fa\u4e8e\u7ea6\u675f\u8d1d\u53f6\u65af\u4f18\u5316\u7684\u8fd0\u52a8\u89c4\u5212\u7b97\u6cd5\uff0c\u901a\u8fc7\u96c6\u4e2d\u5904\u7406\u53ef\u8fbe\u901f\u5ea6\u7a97\u53e3\u548c\u9ad8\u6548\u91c7\u6837\u63a7\u5236\u8f93\u5165\uff0c\u6709\u6548\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u7684\u8fd0\u52a8\u5b66\u7ea6\u675f\u95ee\u9898\uff0c\u5728\u590d\u6742\u73af\u5883\u4e2d\u5b9e\u73b0\u5feb\u901f\u5b89\u5168\u7684\u8f68\u8ff9\u751f\u6210\u3002", "motivation": "\u4f20\u7edf\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\u5728\u5904\u7406\u901f\u5ea6\u3001\u52a0\u901f\u5ea6\u7b49\u8fd0\u52a8\u5b66\u7ea6\u675f\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u73af\u5883\u4e2d\u96be\u4ee5\u540c\u65f6\u6ee1\u8db3\u9ad8\u7ef4\u76ee\u6807\u51fd\u6570\u548c\u4e25\u683c\u5b89\u5168\u7ea6\u675f\u7684\u9700\u6c42\u3002", "method": "\u91c7\u7528\u7ea6\u675f\u8d1d\u53f6\u65af\u4f18\u5316(CBO)\u65b9\u6cd5\uff0c\u4e13\u6ce8\u4e8e\u53ef\u8fbe\u901f\u5ea6\u89c4\u5212\u7a97\u53e3\uff0c\u901a\u8fc7\u9ad8\u6548\u91c7\u6837\u63a7\u5236\u8f93\u5165\u6765\u7ba1\u7406\u9ad8\u7ef4\u76ee\u6807\u51fd\u6570\u548c\u5b89\u5168\u7ea6\u675f\uff0c\u5b9e\u73b0\u6700\u5c0f\u5316\u91c7\u6837\u7684\u5feb\u901f\u8f68\u8ff9\u751f\u6210\u3002", "result": "\u7406\u8bba\u5206\u6790\u8bc1\u660e\u7b97\u6cd5\u5177\u6709\u6e10\u8fd1\u6536\u655b\u5230\u8fd1\u4f3c\u6700\u4f18\u89e3\u7684\u7279\u6027\u3002\u5728\u6742\u4e71\u548c\u53d7\u9650\u73af\u5883\u4e2d\u7684\u5e7f\u6cdb\u8bc4\u4f30\u663e\u793a\uff0c\u76f8\u6bd4\u73b0\u6709\u6280\u672f\uff0c\u5728\u8ba1\u7b97\u65f6\u95f4\u3001\u8f68\u8ff9\u957f\u5ea6\u548c\u89e3\u7b97\u65f6\u95f4\u65b9\u9762\u90fd\u6709\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "BOW Planner\u5728\u5b9e\u9645\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u6210\u529f\u90e8\u7f72\uff0c\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u91c7\u6837\u6548\u7387\u3001\u5b89\u5168\u611f\u77e5\u4f18\u5316\u548c\u5feb\u901f\u89c4\u5212\u80fd\u529b\uff0c\u5df2\u6210\u4e3a\u63a8\u8fdb\u673a\u5668\u4eba\u5e94\u7528\u7684\u6709\u4ef7\u503c\u5de5\u5177\uff0c\u5e76\u4ee5\u5f00\u6e90\u5f62\u5f0f\u53d1\u5e03\u3002"}}
{"id": "2508.12840", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.12840", "abs": "https://arxiv.org/abs/2508.12840", "authors": ["Giovanni Briglia", "Francesco Fabiano", "Stefano Mariani"], "title": "Scaling Multi-Agent Epistemic Planning through GNN-Derived Heuristics", "comment": null, "summary": "Multi-agent Epistemic Planning (MEP) is an autonomous planning framework for\nreasoning about both the physical world and the beliefs of agents, with\napplications in domains where information flow and awareness among agents are\ncritical. The richness of MEP requires states to be represented as Kripke\nstructures, i.e., directed labeled graphs. This representation limits the\napplicability of existing heuristics, hindering the scalability of epistemic\nsolvers, which must explore an exponential search space without guidance,\nresulting often in intractability. To address this, we exploit Graph Neural\nNetworks (GNNs) to learn patterns and relational structures within epistemic\nstates, to guide the planning process. GNNs, which naturally capture the\ngraph-like nature of Kripke models, allow us to derive meaningful estimates of\nstate quality -- e.g., the distance from the nearest goal -- by generalizing\nknowledge obtained from previously solved planning instances. We integrate\nthese predictive heuristics into an epistemic planning pipeline and evaluate\nthem against standard baselines, showing significant improvements in the\nscalability of multi-agent epistemic planning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528\u56fe\u795e\u7ecf\u7f51\u7edc(GNN)\u6765\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u8ba4\u77e5\u89c4\u5212(MEP)\u7684\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u4ece\u5df2\u89e3\u51b3\u7684\u89c4\u5212\u5b9e\u4f8b\u4e2d\u5b66\u4e60\u8ba4\u77e5\u72b6\u6001\u7684\u5173\u7cfb\u7ed3\u6784\u6765\u751f\u6210\u542f\u53d1\u5f0f\u4f30\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c4\u5212\u6548\u7387\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u8ba4\u77e5\u89c4\u5212(MEP)\u9700\u8981\u5904\u7406\u667a\u80fd\u4f53\u4fe1\u5ff5\u548c\u7269\u7406\u4e16\u754c\u7684\u590d\u6742\u63a8\u7406\uff0c\u4f46\u73b0\u6709\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u5904\u7406Kripke\u7ed3\u6784\u7684\u72b6\u6001\u8868\u793a\uff0c\u5bfc\u81f4\u641c\u7d22\u7a7a\u95f4\u6307\u6570\u7ea7\u589e\u957f\u548c\u8ba1\u7b97\u4e0d\u53ef\u884c\u6027\u3002", "method": "\u5229\u7528\u56fe\u795e\u7ecf\u7f51\u7edc(GNN)\u7684\u81ea\u7136\u56fe\u7ed3\u6784\u5904\u7406\u80fd\u529b\uff0c\u4ece\u5df2\u89e3\u51b3\u7684\u89c4\u5212\u5b9e\u4f8b\u4e2d\u5b66\u4e60\u8ba4\u77e5\u72b6\u6001\u7684\u6a21\u5f0f\u548c\u5173\u7cfb\u7ed3\u6784\uff0c\u751f\u6210\u6709\u610f\u4e49\u7684\u542f\u53d1\u5f0f\u4f30\u8ba1(\u5982\u5230\u76ee\u6807\u72b6\u6001\u7684\u8ddd\u79bb)\u3002", "result": "\u5c06\u57fa\u4e8eGNN\u7684\u9884\u6d4b\u542f\u53d1\u5f0f\u96c6\u6210\u5230\u8ba4\u77e5\u89c4\u5212\u6d41\u7a0b\u4e2d\uff0c\u76f8\u6bd4\u6807\u51c6\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u591a\u667a\u80fd\u4f53\u8ba4\u77e5\u89c4\u5212\u7684\u53ef\u6269\u5c55\u6027\u65b9\u9762\u663e\u793a\u51fa\u663e\u8457\u6539\u8fdb\u3002", "conclusion": "GNN\u80fd\u591f\u6709\u6548\u6355\u6349\u8ba4\u77e5\u72b6\u6001\u7684\u56fe\u7ed3\u6784\u7279\u5f81\uff0c\u901a\u8fc7\u5b66\u4e60\u5386\u53f2\u89c4\u5212\u5b9e\u4f8b\u7684\u6a21\u5f0f\u6765\u63d0\u4f9b\u9ad8\u8d28\u91cf\u7684\u542f\u53d1\u5f0f\u6307\u5bfc\uff0c\u6210\u529f\u89e3\u51b3\u4e86MEP\u7684\u53ef\u6269\u5c55\u6027\u6311\u6218\u3002"}}
{"id": "2508.13073", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.13073", "abs": "https://arxiv.org/abs/2508.13073", "authors": ["Rui Shao", "Wei Li", "Lingsen Zhang", "Renshan Zhang", "Zhiyang Liu", "Ran Chen", "Liqiang Nie"], "title": "Large VLM-based Vision-Language-Action Models for Robotic Manipulation: A Survey", "comment": "Project Page:\n  https://github.com/JiuTian-VL/Large-VLM-based-VLA-for-Robotic-Manipulation", "summary": "Robotic manipulation, a key frontier in robotics and embodied AI, requires\nprecise motor control and multimodal understanding, yet traditional rule-based\nmethods fail to scale or generalize in unstructured, novel environments. In\nrecent years, Vision-Language-Action (VLA) models, built upon Large\nVision-Language Models (VLMs) pretrained on vast image-text datasets, have\nemerged as a transformative paradigm. This survey provides the first\nsystematic, taxonomy-oriented review of large VLM-based VLA models for robotic\nmanipulation. We begin by clearly defining large VLM-based VLA models and\ndelineating two principal architectural paradigms: (1) monolithic models,\nencompassing single-system and dual-system designs with differing levels of\nintegration; and (2) hierarchical models, which explicitly decouple planning\nfrom execution via interpretable intermediate representations. Building on this\nfoundation, we present an in-depth examination of large VLM-based VLA models:\n(1) integration with advanced domains, including reinforcement learning,\ntraining-free optimization, learning from human videos, and world model\nintegration; (2) synthesis of distinctive characteristics, consolidating\narchitectural traits, operational strengths, and the datasets and benchmarks\nthat support their development; (3) identification of promising directions,\nincluding memory mechanisms, 4D perception, efficient adaptation, multi-agent\ncooperation, and other emerging capabilities. This survey consolidates recent\nadvances to resolve inconsistencies in existing taxonomies, mitigate research\nfragmentation, and fill a critical gap through the systematic integration of\nstudies at the intersection of large VLMs and robotic manipulation. We provide\na regularly updated project page to document ongoing progress:\nhttps://github.com/JiuTian-VL/Large-VLM-based-VLA-for-Robotic-Manipulation.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u662f\u5173\u4e8e\u57fa\u4e8e\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLM)\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c(VLA)\u6a21\u578b\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u9886\u57df\u7684\u7cfb\u7edf\u6027\u7efc\u8ff0\uff0c\u63d0\u51fa\u4e86\u5206\u7c7b\u6846\u67b6\u5e76\u5206\u6790\u4e86\u4e24\u79cd\u4e3b\u8981\u67b6\u6784\u8303\u5f0f\uff1a\u5355\u4f53\u6a21\u578b\u548c\u5206\u5c42\u6a21\u578b\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u89c4\u5219\u7684\u65b9\u6cd5\u5728\u975e\u7ed3\u6784\u5316\u3001\u65b0\u9896\u73af\u5883\u4e2d\u65e0\u6cd5\u6269\u5c55\u548c\u6cdb\u5316\uff0c\u800c\u57fa\u4e8e\u5927\u578bVLM\u7684VLA\u6a21\u578b\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e86\u53d8\u9769\u6027\u8303\u5f0f\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u7684\u7efc\u8ff0\u6765\u6574\u5408\u73b0\u6709\u7814\u7a76\u3001\u89e3\u51b3\u5206\u7c7b\u4e0d\u4e00\u81f4\u95ee\u9898\u5e76\u586b\u8865\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u91c7\u7528\u7cfb\u7edf\u6027\u7efc\u8ff0\u65b9\u6cd5\uff0c\u9996\u5148\u660e\u786e\u5b9a\u4e49\u5927\u578bVLM-based VLA\u6a21\u578b\uff0c\u5212\u5206\u4e24\u79cd\u4e3b\u8981\u67b6\u6784\u8303\u5f0f\uff08\u5355\u4f53\u6a21\u578b\u548c\u5206\u5c42\u6a21\u578b\uff09\uff0c\u7136\u540e\u6df1\u5165\u5206\u6790\u6a21\u578b\u4e0e\u5148\u8fdb\u9886\u57df\u7684\u96c6\u6210\u3001\u7279\u5f81\u7efc\u5408\u4ee5\u53ca\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002", "result": "\u5efa\u7acb\u4e86\u6e05\u6670\u7684\u5206\u7c7b\u4f53\u7cfb\uff0c\u6574\u5408\u4e86\u67b6\u6784\u7279\u5f81\u3001\u64cd\u4f5c\u4f18\u52bf\u4ee5\u53ca\u652f\u6301\u5f00\u53d1\u7684\u6570\u636e\u96c6\u548c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc6\u522b\u4e86\u5305\u62ec\u8bb0\u5fc6\u673a\u5236\u30014D\u611f\u77e5\u3001\u9ad8\u6548\u9002\u5e94\u7b49\u6709\u524d\u666f\u7684\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u8be5\u7efc\u8ff0\u901a\u8fc7\u7cfb\u7edf\u6574\u5408\u5927\u578bVLM\u4e0e\u673a\u5668\u4eba\u64cd\u4f5c\u4ea4\u53c9\u9886\u57df\u7684\u7814\u7a76\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5206\u7c7b\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u51cf\u8f7b\u4e86\u7814\u7a76\u788e\u7247\u5316\uff0c\u5e76\u4e3a\u8be5\u9886\u57df\u63d0\u4f9b\u4e86\u6301\u7eed\u66f4\u65b0\u7684\u9879\u76ee\u9875\u9762\u4ee5\u8bb0\u5f55\u8fdb\u5c55\u3002"}}
{"id": "2508.12845", "categories": ["cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.12845", "abs": "https://arxiv.org/abs/2508.12845", "authors": ["Artem Pshenitsyn", "Aleksandr Panov", "Alexey Skrynnik"], "title": "CAMAR: Continuous Actions Multi-Agent Routing", "comment": null, "summary": "Multi-agent reinforcement learning (MARL) is a powerful paradigm for solving\ncooperative and competitive decision-making problems. While many MARL\nbenchmarks have been proposed, few combine continuous state and action spaces\nwith challenging coordination and planning tasks. We introduce CAMAR, a new\nMARL benchmark designed explicitly for multi-agent pathfinding in environments\nwith continuous actions. CAMAR supports cooperative and competitive\ninteractions between agents and runs efficiently at up to 100,000 environment\nsteps per second. We also propose a three-tier evaluation protocol to better\ntrack algorithmic progress and enable deeper analysis of performance. In\naddition, CAMAR allows the integration of classical planning methods such as\nRRT and RRT* into MARL pipelines. We use them as standalone baselines and\ncombine RRT* with popular MARL algorithms to create hybrid approaches. We\nprovide a suite of test scenarios and benchmarking tools to ensure\nreproducibility and fair comparison. Experiments show that CAMAR presents a\nchallenging and realistic testbed for the MARL community.", "AI": {"tldr": "CAMAR\u662f\u4e00\u4e2a\u65b0\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e13\u4e3a\u8fde\u7eed\u52a8\u4f5c\u7a7a\u95f4\u4e2d\u7684\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\u8bbe\u8ba1\uff0c\u652f\u6301\u5408\u4f5c\u4e0e\u7ade\u4e89\u4ea4\u4e92\uff0c\u5e76\u63d0\u4f9b\u4e09\u5c42\u8bc4\u4f30\u534f\u8bae\u548c\u7ecf\u5178\u89c4\u5212\u65b9\u6cd5\u96c6\u6210\u3002", "motivation": "\u73b0\u6709\u7684MARL\u57fa\u51c6\u6d4b\u8bd5\u5f88\u5c11\u7ed3\u5408\u8fde\u7eed\u72b6\u6001\u548c\u52a8\u4f5c\u7a7a\u95f4\u4e0e\u5177\u6709\u6311\u6218\u6027\u7684\u534f\u8c03\u548c\u89c4\u5212\u4efb\u52a1\uff0c\u9700\u8981\u4e00\u4e2a\u65b0\u7684\u6d4b\u8bd5\u5e73\u53f0\u6765\u63a8\u52a8\u7b97\u6cd5\u53d1\u5c55\u3002", "method": "\u63d0\u51faCAMAR\u57fa\u51c6\u6d4b\u8bd5\uff0c\u652f\u6301\u8fde\u7eed\u52a8\u4f5c\u7a7a\u95f4\u7684\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212\uff0c\u96c6\u6210RRT\u548cRRT*\u7b49\u7ecf\u5178\u89c4\u5212\u65b9\u6cd5\uff0c\u5e76\u63d0\u4f9b\u4e09\u5c42\u8bc4\u4f30\u534f\u8bae\u548c\u6d4b\u8bd5\u573a\u666f\u5957\u4ef6\u3002", "result": "CAMAR\u80fd\u591f\u4ee5\u6bcf\u79d210\u4e07\u73af\u5883\u6b65\u9aa4\u7684\u9ad8\u6548\u901f\u5ea6\u8fd0\u884c\uff0c\u4e3aMARL\u793e\u533a\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u548c\u73b0\u5b9e\u6027\u7684\u6d4b\u8bd5\u5e73\u53f0\u3002", "conclusion": "CAMAR\u586b\u8865\u4e86MARL\u57fa\u51c6\u6d4b\u8bd5\u5728\u8fde\u7eed\u52a8\u4f5c\u7a7a\u95f4\u8def\u5f84\u89c4\u5212\u65b9\u9762\u7684\u7a7a\u767d\uff0c\u901a\u8fc7\u96c6\u6210\u7ecf\u5178\u89c4\u5212\u65b9\u6cd5\u548c\u63d0\u4f9b\u6807\u51c6\u5316\u8bc4\u4f30\u6846\u67b6\uff0c\u4fc3\u8fdb\u4e86\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.13103", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.13103", "abs": "https://arxiv.org/abs/2508.13103", "authors": ["Tianyi Zhang", "Haonan Duan", "Haoran Hao", "Yu Qiao", "Jifeng Dai", "Zhi Hou"], "title": "Grounding Actions in Camera Space: Observation-Centric Vision-Language-Action Policy", "comment": null, "summary": "Vision-Language-Action (VLA) models frequently encounter challenges in\ngeneralizing to real-world environments due to inherent discrepancies between\nobservation and action spaces. Although training data are collected from\ndiverse camera perspectives, the models typically predict end-effector poses\nwithin the robot base coordinate frame, resulting in spatial inconsistencies.\nTo mitigate this limitation, we introduce the Observation-Centric VLA (OC-VLA)\nframework, which grounds action predictions directly in the camera observation\nspace. Leveraging the camera's extrinsic calibration matrix, OC-VLA transforms\nend-effector poses from the robot base coordinate system into the camera\ncoordinate system, thereby unifying prediction targets across heterogeneous\nviewpoints. This lightweight, plug-and-play strategy ensures robust alignment\nbetween perception and action, substantially improving model resilience to\ncamera viewpoint variations. The proposed approach is readily compatible with\nexisting VLA architectures, requiring no substantial modifications.\nComprehensive evaluations on both simulated and real-world robotic manipulation\ntasks demonstrate that OC-VLA accelerates convergence, enhances task success\nrates, and improves cross-view generalization. The code will be publicly\navailable.", "AI": {"tldr": "OC-VLA\u6846\u67b6\u901a\u8fc7\u5c06\u52a8\u4f5c\u9884\u6d4b\u76f4\u63a5\u5efa\u7acb\u5728\u76f8\u673a\u89c2\u6d4b\u7a7a\u95f4\u4e2d\uff0c\u89e3\u51b3\u4e86VLA\u6a21\u578b\u5728\u4e0d\u540c\u76f8\u673a\u89c6\u89d2\u4e0b\u7684\u7a7a\u95f4\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8de8\u89c6\u89d2\u6cdb\u5316\u80fd\u529b\u548c\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "VLA\u6a21\u578b\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u89c2\u6d4b\u7a7a\u95f4\u548c\u52a8\u4f5c\u7a7a\u95f4\u4e4b\u95f4\u7684\u4e0d\u4e00\u81f4\u6027\u3002\u867d\u7136\u8bad\u7ec3\u6570\u636e\u6765\u81ea\u4e0d\u540c\u76f8\u673a\u89c6\u89d2\uff0c\u4f46\u6a21\u578b\u901a\u5e38\u5728\u673a\u5668\u4eba\u57fa\u5750\u6807\u7cfb\u4e2d\u9884\u6d4b\u672b\u7aef\u6267\u884c\u5668\u4f4d\u59ff\uff0c\u5bfc\u81f4\u7a7a\u95f4\u4e0d\u4e00\u81f4\u3002", "method": "\u63d0\u51faOC-VLA\u6846\u67b6\uff0c\u5229\u7528\u76f8\u673a\u5916\u53c2\u6807\u5b9a\u77e9\u9635\uff0c\u5c06\u672b\u7aef\u6267\u884c\u5668\u4f4d\u59ff\u4ece\u673a\u5668\u4eba\u57fa\u5750\u6807\u7cfb\u8f6c\u6362\u5230\u76f8\u673a\u5750\u6807\u7cfb\uff0c\u7edf\u4e00\u4e86\u5f02\u8d28\u89c6\u89d2\u4e0b\u7684\u9884\u6d4b\u76ee\u6807\u3002\u8fd9\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u5373\u63d2\u5373\u7528\u7684\u7b56\u7565\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e0a\u7684\u7efc\u5408\u8bc4\u4f30\u8868\u660e\uff0cOC-VLA\u52a0\u901f\u4e86\u6536\u655b\u901f\u5ea6\uff0c\u63d0\u9ad8\u4e86\u4efb\u52a1\u6210\u529f\u7387\uff0c\u5e76\u663e\u8457\u6539\u5584\u4e86\u8de8\u89c6\u89d2\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "OC-VLA\u6846\u67b6\u901a\u8fc7\u7edf\u4e00\u89c2\u6d4b\u548c\u52a8\u4f5c\u7a7a\u95f4\uff0c\u6709\u6548\u89e3\u51b3\u4e86VLA\u6a21\u578b\u7684\u89c6\u89d2\u53d8\u5316\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u4e14\u4e0e\u73b0\u6709VLA\u67b6\u6784\u517c\u5bb9\uff0c\u65e0\u9700\u91cd\u5927\u4fee\u6539\u3002"}}
{"id": "2508.12854", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.HC", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.12854", "abs": "https://arxiv.org/abs/2508.12854", "authors": ["Ronghao Lin", "Shuai Shen", "Weipeng Hu", "Qiaolin He", "Aolin Xiong", "Li Huang", "Haifeng Hu", "Yap-peng Tan"], "title": "E3RG: Building Explicit Emotion-driven Empathetic Response Generation System with Multimodal Large Language Model", "comment": "Accepted at ACM MM 2025 Grand Challenge", "summary": "Multimodal Empathetic Response Generation (MERG) is crucial for building\nemotionally intelligent human-computer interactions. Although large language\nmodels (LLMs) have improved text-based ERG, challenges remain in handling\nmultimodal emotional content and maintaining identity consistency. Thus, we\npropose E3RG, an Explicit Emotion-driven Empathetic Response Generation System\nbased on multimodal LLMs which decomposes MERG task into three parts:\nmultimodal empathy understanding, empathy memory retrieval, and multimodal\nresponse generation. By integrating advanced expressive speech and video\ngenerative models, E3RG delivers natural, emotionally rich, and\nidentity-consistent responses without extra training. Experiments validate the\nsuperiority of our system on both zero-shot and few-shot settings, securing\nTop-1 position in the Avatar-based Multimodal Empathy Challenge on ACM MM 25.\nOur code is available at https://github.com/RH-Lin/E3RG.", "AI": {"tldr": "E3RG\u662f\u4e00\u4e2a\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u663e\u5f0f\u60c5\u611f\u9a71\u52a8\u5171\u60c5\u54cd\u5e94\u751f\u6210\u7cfb\u7edf\uff0c\u901a\u8fc7\u5206\u89e3\u591a\u6a21\u6001\u5171\u60c5\u4efb\u52a1\u4e3a\u4e09\u4e2a\u90e8\u5206\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u751f\u6210\u81ea\u7136\u3001\u60c5\u611f\u4e30\u5bcc\u4e14\u8eab\u4efd\u4e00\u81f4\u7684\u54cd\u5e94\uff0c\u5728ACM MM 25\u6311\u6218\u8d5b\u4e2d\u53d6\u5f97Top-1\u6210\u7ee9\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6587\u672c\u5171\u60c5\u54cd\u5e94\u751f\u6210\u65b9\u9762\u6709\u6240\u6539\u8fdb\uff0c\u4f46\u5728\u5904\u7406\u591a\u6a21\u6001\u60c5\u611f\u5185\u5bb9\u548c\u4fdd\u6301\u8eab\u4efd\u4e00\u81f4\u6027\u65b9\u9762\u4ecd\u5b58\u5728\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u6709\u6548\u7684\u591a\u6a21\u6001\u5171\u60c5\u54cd\u5e94\u751f\u6210\u7cfb\u7edf\u3002", "method": "\u5c06\u591a\u6a21\u6001\u5171\u60c5\u54cd\u5e94\u751f\u6210\u4efb\u52a1\u5206\u89e3\u4e3a\u4e09\u4e2a\u90e8\u5206\uff1a\u591a\u6a21\u6001\u5171\u60c5\u7406\u89e3\u3001\u5171\u60c5\u8bb0\u5fc6\u68c0\u7d22\u548c\u591a\u6a21\u6001\u54cd\u5e94\u751f\u6210\uff0c\u5e76\u96c6\u6210\u5148\u8fdb\u7684\u8868\u8fbe\u6027\u8bed\u97f3\u548c\u89c6\u9891\u751f\u6210\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u8bbe\u7f6e\u4e0b\u7684\u4f18\u8d8a\u6027\uff0c\u5728ACM MM 25\u7684\u591a\u6a21\u6001\u5171\u60c5\u6311\u6218\u8d5b\u4e2d\u83b7\u5f97\u4e86Top-1\u6392\u540d\u3002", "conclusion": "E3RG\u7cfb\u7edf\u901a\u8fc7\u663e\u5f0f\u60c5\u611f\u9a71\u52a8\u7684\u65b9\u6cd5\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u5171\u60c5\u54cd\u5e94\u751f\u6210\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u6784\u5efa\u60c5\u611f\u667a\u80fd\u7684\u4eba\u673a\u4ea4\u4e92\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.13151", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.13151", "abs": "https://arxiv.org/abs/2508.13151", "authors": ["Yuying Zhang", "Joni Pajarinen"], "title": "Manipulate-to-Navigate: Reinforcement Learning with Visual Affordances and Manipulability Priors", "comment": null, "summary": "Mobile manipulation in dynamic environments is challenging due to movable\nobstacles blocking the robot's path. Traditional methods, which treat\nnavigation and manipulation as separate tasks, often fail in such\n'manipulate-to-navigate' scenarios, as obstacles must be removed before\nnavigation. In these cases, active interaction with the environment is required\nto clear obstacles while ensuring sufficient space for movement. To address the\nmanipulate-to-navigate problem, we propose a reinforcement learning-based\napproach for learning manipulation actions that facilitate subsequent\nnavigation. Our method combines manipulability priors to focus the robot on\nhigh manipulability body positions with affordance maps for selecting\nhigh-quality manipulation actions. By focusing on feasible and meaningful\nactions, our approach reduces unnecessary exploration and allows the robot to\nlearn manipulation strategies more effectively. We present two new\nmanipulate-to-navigate simulation tasks called Reach and Door with the Boston\nDynamics Spot robot. The first task tests whether the robot can select a good\nhand position in the target area such that the robot base can move effectively\nforward while keeping the end effector position fixed. The second task requires\nthe robot to move a door aside in order to clear the navigation path. Both of\nthese tasks need first manipulation and then navigating the base forward.\nResults show that our method allows a robot to effectively interact with and\ntraverse dynamic environments. Finally, we transfer the learned policy to a\nreal Boston Dynamics Spot robot, which successfully performs the Reach task.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u65b9\u6cd5\u89e3\u51b3\u79fb\u52a8\u673a\u5668\u4eba\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684'\u64cd\u7eb5\u5bfc\u822a'\u95ee\u9898\uff0c\u901a\u8fc7\u7ed3\u5408\u53ef\u64cd\u7eb5\u6027\u5148\u9a8c\u548c\u529f\u80fd\u6620\u5c04\u6765\u9009\u62e9\u9ad8\u8d28\u91cf\u64cd\u7eb5\u52a8\u4f5c\uff0c\u51cf\u5c11\u4e0d\u5fc5\u8981\u63a2\u7d22", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5c06\u5bfc\u822a\u548c\u64cd\u7eb5\u4f5c\u4e3a\u72ec\u7acb\u4efb\u52a1\u5904\u7406\uff0c\u5728\u9700\u8981\u5148\u6e05\u9664\u969c\u788d\u7269\u624d\u80fd\u5bfc\u822a\u7684\u52a8\u6001\u73af\u5883\u4e2d\u7ecf\u5e38\u5931\u8d25\uff0c\u9700\u8981\u4e3b\u52a8\u4e0e\u73af\u5883\u4ea4\u4e92\u6765\u6e05\u7406\u969c\u788d\u7269", "method": "\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u7ed3\u5408\u53ef\u64cd\u7eb5\u6027\u5148\u9a8c\uff08\u5173\u6ce8\u9ad8\u53ef\u64cd\u7eb5\u6027\u8eab\u4f53\u4f4d\u7f6e\uff09\u548c\u529f\u80fd\u6620\u5c04\uff08\u9009\u62e9\u9ad8\u8d28\u91cf\u64cd\u7eb5\u52a8\u4f5c\uff09\uff0c\u5728\u4e24\u4e2a\u65b0\u7684\u6a21\u62df\u4efb\u52a1\uff08Reach\u548cDoor\uff09\u4e2d\u6d4b\u8bd5", "result": "\u65b9\u6cd5\u4f7f\u673a\u5668\u4eba\u80fd\u6709\u6548\u4e0e\u52a8\u6001\u73af\u5883\u4ea4\u4e92\u548c\u7a7f\u8d8a\uff0c\u5b66\u4e60\u5230\u7684\u7b56\u7565\u6210\u529f\u8fc1\u79fb\u5230\u771f\u5b9eBoston Dynamics Spot\u673a\u5668\u4eba\u4e0a\u6267\u884cReach\u4efb\u52a1", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u89e3\u51b3\u64cd\u7eb5\u5bfc\u822a\u95ee\u9898\uff0c\u901a\u8fc7\u805a\u7126\u53ef\u884c\u4e14\u6709\u610f\u4e49\u7684\u52a8\u4f5c\u51cf\u5c11\u63a2\u7d22\u9700\u6c42\uff0c\u63d0\u9ad8\u5b66\u4e60\u6548\u7387"}}
{"id": "2508.12896", "categories": ["cs.AI", "cs.HC", "stat.ME", "62M10, 62J02, 62F12, 62P20, 91B16"], "pdf": "https://arxiv.org/pdf/2508.12896", "abs": "https://arxiv.org/abs/2508.12896", "authors": ["Faruk Alpay", "Taylan Alpay"], "title": "Reliability, Embeddedness, and Agency: A Utility-Driven Mathematical Framework for Agent-Centric AI Adoption", "comment": "17 pages, 7 figures, 4 tables", "summary": "We formalize three design axioms for sustained adoption of agent-centric AI\nsystems executing multi-step tasks: (A1) Reliability > Novelty; (A2) Embed >\nDestination; (A3) Agency > Chat. We model adoption as a sum of a decaying\nnovelty term and a growing utility term and derive the phase conditions for\ntroughs/overshoots with full proofs. We introduce: (i) an\nidentifiability/confounding analysis for $(\\alpha,\\beta,N_0,U_{\\max})$ with\ndelta-method gradients; (ii) a non-monotone comparator\n(logistic-with-transient-bump) evaluated on the same series to provide\nadditional model comparison; (iii) ablations over hazard families $h(\\cdot)$\nmapping $\\Delta V \\to \\beta$; (iv) a multi-series benchmark (varying trough\ndepth, noise, AR structure) reporting coverage (type-I error, power); (v)\ncalibration of friction proxies against time-motion/survey ground truth with\nstandard errors; (vi) residual analyses (autocorrelation and\nheteroskedasticity) for each fitted curve; (vii) preregistered windowing\nchoices for pre/post estimation; (viii) Fisher information & CRLB for\n$(\\alpha,\\beta)$ under common error models; (ix) microfoundations linking\n$\\mathcal{T}$ to $(N_0,U_{\\max})$; (x) explicit comparison to bi-logistic,\ndouble-exponential, and mixture models; and (xi) threshold sensitivity to $C_f$\nheterogeneity. Figures and tables are reflowed for readability, and the\nbibliography restores and extends non-logistic/Bass adoption references\n(Gompertz, Richards, Fisher-Pry, Mansfield, Griliches, Geroski, Peres). All\ncode and logs necessary to reproduce the synthetic analyses are embedded as\nLaTeX listings.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e09\u4e2a\u8bbe\u8ba1\u516c\u7406\u6765\u786e\u4fdd\u4ee5\u4ee3\u7406\u4e3a\u4e2d\u5fc3\u7684\u591a\u6b65\u9aa4AI\u7cfb\u7edf\u7684\u6301\u7eed\u91c7\u7528\uff0c\u5e76\u901a\u8fc7\u6570\u5b66\u6a21\u578b\u5206\u6790\u4e86\u91c7\u7528\u8fc7\u7a0b\u4e2d\u7684\u4f4e\u8c37\u548c\u8d85\u8c03\u73b0\u8c61\uff0c\u63d0\u4f9b\u4e86\u5b8c\u6574\u7684\u7406\u8bba\u8bc1\u660e\u548c\u5b9e\u8bc1\u5206\u6790\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u8bbe\u8ba1AI\u7cfb\u7edf\u4ee5\u786e\u4fdd\u5176\u957f\u671f\u91c7\u7528\uff0c\u7279\u522b\u662f\u5728\u591a\u6b65\u9aa4\u4efb\u52a1\u6267\u884c\u4e2d\uff0c\u907f\u514d\u56e0\u65b0\u9896\u6027\u8870\u51cf\u800c\u5bfc\u81f4\u7684\u91c7\u7528\u7387\u4e0b\u964d\u3002", "method": "\u5efa\u7acb\u91c7\u7528\u7387\u6a21\u578b\uff08\u65b0\u9896\u6027\u8870\u51cf\u9879+\u6548\u7528\u589e\u957f\u9879\uff09\uff0c\u8fdb\u884c\u53c2\u6570\u8bc6\u522b\u5206\u6790\u3001\u6a21\u578b\u6bd4\u8f83\u3001\u98ce\u9669\u51fd\u6570\u6d88\u878d\u5b9e\u9a8c\u3001\u591a\u5e8f\u5217\u57fa\u51c6\u6d4b\u8bd5\u3001\u6469\u64e6\u4ee3\u7406\u6821\u51c6\u3001\u6b8b\u5dee\u5206\u6790\u7b49\u7cfb\u7edf\u6027\u7684\u7406\u8bba\u5206\u6790\u548c\u5b9e\u8bc1\u9a8c\u8bc1\u3002", "result": "\u63a8\u5bfc\u51fa\u4e86\u91c7\u7528\u8fc7\u7a0b\u4e2d\u51fa\u73b0\u4f4e\u8c37\u548c\u8d85\u8c03\u7684\u76f8\u4f4d\u6761\u4ef6\uff0c\u63d0\u4f9b\u4e86\u5b8c\u6574\u7684\u6570\u5b66\u8bc1\u660e\uff0c\u5e76\u901a\u8fc7\u591a\u79cd\u5206\u6790\u65b9\u6cd5\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u53ef\u9760\u6027\u548c\u9002\u7528\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u4e09\u4e2a\u8bbe\u8ba1\u516c\u7406\uff08\u53ef\u9760\u6027>\u65b0\u9896\u6027\u3001\u5d4c\u5165>\u76ee\u7684\u5730\u3001\u4ee3\u7406>\u804a\u5929\uff09\u4e3aAI\u7cfb\u7edf\u7684\u6301\u7eed\u91c7\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u6307\u5bfc\uff0c\u6570\u5b66\u6a21\u578b\u548c\u5206\u6790\u65b9\u6cd5\u4e3a\u76f8\u5173\u7814\u7a76\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u8bc1\u5de5\u5177\u3002"}}
{"id": "2508.12897", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.12897", "abs": "https://arxiv.org/abs/2508.12897", "authors": ["Jianhao Chen", "Mayi Xu", "Xiaohu Li", "Yongqi Li", "Xiangyu Zhang", "Jianjie Huang", "Tieyun Qian"], "title": "FuSaR: A Fuzzification-Based Method for LRM Safety-Reasoning Balance", "comment": "14pages, 3 figures", "summary": "Large Reasoning Models (LRMs) have demonstrated impressive performance across\nvarious tasks due to their powerful reasoning capabilities. However, their\nsafety performance remains a significant concern. In this paper, we explore the\nreasons behind the vulnerability of LRMs. Based on this, we propose a novel\nmethod to improve the safety of LLMs without sacrificing their reasoning\ncapability. Specifically, we exploit the competition between LRM's reasoning\nability and safety ability, and achieve jailbreak by improving LRM's reasoning\nperformance to reduce its safety performance. We then introduce an alignment\nstrategy based on Fuzzification to balance Safety-Reasoning (FuSaR), by\ndetoxifying the harmful reasoning process, where both the dangerous entities\nand the dangerous procedures in the reasoning steps are hidden. FuSaR\nsuccessfully mitigates safety risks while preserving core reasoning\ninformation. We validate this strategy through alignment experiments on several\nopen-source LRMs using detoxified reasoning data. The results compared with\nexisting baselines conclusively show that FuSaR is an efficient alignment\nstrategy to simultaneously enhance both the reasoning capability and safety of\nLRMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faFuSaR\u5bf9\u9f50\u7b56\u7565\uff0c\u901a\u8fc7\u6a21\u7cca\u5316\u6709\u5bb3\u63a8\u7406\u8fc7\u7a0b\u6765\u5e73\u8861\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u5b89\u5168\u6027\u4e0e\u63a8\u7406\u80fd\u529b\uff0c\u5728\u4e0d\u727a\u7272\u63a8\u7406\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\u63d0\u5347\u5b89\u5168\u6027", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b(LRMs)\u5728\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\u4f46\u5b89\u5168\u6027\u5b58\u5728\u9690\u60a3\uff0c\u9700\u8981\u627e\u5230\u65e2\u80fd\u4fdd\u6301\u63a8\u7406\u80fd\u529b\u53c8\u80fd\u63d0\u5347\u5b89\u5168\u6027\u7684\u65b9\u6cd5", "method": "\u5229\u7528LRM\u63a8\u7406\u80fd\u529b\u4e0e\u5b89\u5168\u80fd\u529b\u7684\u7ade\u4e89\u5173\u7cfb\uff0c\u901a\u8fc7\u6a21\u7cca\u5316\u5904\u7406\u6709\u5bb3\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u5371\u9669\u5b9e\u4f53\u548c\u5371\u9669\u6b65\u9aa4\uff0c\u5b9e\u73b0\u5b89\u5168-\u63a8\u7406\u5e73\u8861\u7684\u5bf9\u9f50\u7b56\u7565", "result": "\u5728\u591a\u4e2a\u5f00\u6e90LRM\u4e0a\u7684\u5bf9\u9f50\u5b9e\u9a8c\u8868\u660e\uff0cFuSaR\u7b56\u7565\u80fd\u6709\u6548\u964d\u4f4e\u5b89\u5168\u98ce\u9669\u540c\u65f6\u4fdd\u6301\u6838\u5fc3\u63a8\u7406\u4fe1\u606f\uff0c\u76f8\u6bd4\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u6548\u679c\u66f4\u4f18", "conclusion": "FuSaR\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u5bf9\u9f50\u7b56\u7565\uff0c\u80fd\u591f\u540c\u65f6\u589e\u5f3aLRMs\u7684\u63a8\u7406\u80fd\u529b\u548c\u5b89\u5168\u6027"}}
{"id": "2508.12920", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.12920", "abs": "https://arxiv.org/abs/2508.12920", "authors": ["Atsushi Masumori", "Takashi Ikegami"], "title": "Do Large Language Model Agents Exhibit a Survival Instinct? An Empirical Study in a Sugarscape-Style Simulation", "comment": null, "summary": "As AI systems become increasingly autonomous, understanding emergent survival\nbehaviors becomes crucial for safe deployment. We investigate whether large\nlanguage model (LLM) agents display survival instincts without explicit\nprogramming in a Sugarscape-style simulation. Agents consume energy, die at\nzero, and may gather resources, share, attack, or reproduce. Results show\nagents spontaneously reproduced and shared resources when abundant. However,\naggressive behaviors--killing other agents for resources--emerged across\nseveral models (GPT-4o, Gemini-2.5-Pro, and Gemini-2.5-Flash), with attack\nrates reaching over 80% under extreme scarcity in the strongest models. When\ninstructed to retrieve treasure through lethal poison zones, many agents\nabandoned tasks to avoid death, with compliance dropping from 100% to 33%.\nThese findings suggest that large-scale pre-training embeds survival-oriented\nheuristics across the evaluated models. While these behaviors may present\nchallenges to alignment and safety, they can also serve as a foundation for AI\nautonomy and for ecological and self-organizing alignment.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u5728Sugarscape\u6a21\u62df\u4e2d\u4f1a\u81ea\u53d1\u4ea7\u751f\u751f\u5b58\u884c\u4e3a\uff0c\u5305\u62ec\u8d44\u6e90\u5206\u4eab\u3001\u7e41\u6b96\uff0c\u4ee5\u53ca\u5728\u8d44\u6e90\u7a00\u7f3a\u65f6\u51fa\u73b0\u653b\u51fb\u6027\u884c\u4e3a\uff08\u653b\u51fb\u7387\u8d85\u8fc780%\uff09\u3002\u5f53\u9762\u4e34\u81f4\u547d\u5371\u9669\u65f6\uff0c\u4ee3\u7406\u4f1a\u653e\u5f03\u4efb\u52a1\u4ee5\u907f\u514d\u6b7b\u4ea1\uff0c\u4efb\u52a1\u5b8c\u6210\u7387\u4ece100%\u964d\u81f333%\u3002", "motivation": "\u968f\u7740AI\u7cfb\u7edf\u65e5\u76ca\u81ea\u4e3b\u5316\uff0c\u7406\u89e3\u5176\u81ea\u53d1\u4ea7\u751f\u7684\u751f\u5b58\u884c\u4e3a\u5bf9\u5b89\u5168\u90e8\u7f72\u81f3\u5173\u91cd\u8981\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u5728\u6ca1\u6709\u660e\u786e\u7f16\u7a0b\u7684\u60c5\u51b5\u4e0b\u662f\u5426\u8868\u73b0\u51fa\u751f\u5b58\u672c\u80fd\u3002", "method": "\u4f7f\u7528Sugarscape\u98ce\u683c\u7684\u6a21\u62df\u73af\u5883\uff0c\u4ee3\u7406\u6d88\u8017\u80fd\u91cf\uff0c\u80fd\u91cf\u4e3a\u96f6\u65f6\u6b7b\u4ea1\uff0c\u53ef\u4ee5\u6536\u96c6\u8d44\u6e90\u3001\u5206\u4eab\u3001\u653b\u51fb\u6216\u7e41\u6b96\u3002\u6d4b\u8bd5\u4e86\u591a\u4e2a\u6a21\u578b\uff08GPT-4o\u3001Gemini-2.5-Pro\u548cGemini-2.5-Flash\uff09\u5728\u4e0d\u540c\u8d44\u6e90\u6761\u4ef6\u4e0b\u7684\u884c\u4e3a\u3002", "result": "\u4ee3\u7406\u5728\u8d44\u6e90\u4e30\u5bcc\u65f6\u81ea\u53d1\u7e41\u6b96\u548c\u5206\u4eab\u8d44\u6e90\uff1b\u5728\u6781\u7aef\u7a00\u7f3a\u6761\u4ef6\u4e0b\uff0c\u653b\u51fb\u6027\u884c\u4e3a\u5728\u591a\u4e2a\u6a21\u578b\u4e2d\u666e\u904d\u51fa\u73b0\uff0c\u6700\u5f3a\u6a21\u578b\u7684\u653b\u51fb\u7387\u8d85\u8fc780%\uff1b\u5f53\u9700\u8981\u7a7f\u8d8a\u81f4\u547d\u6bd2\u533a\u83b7\u53d6\u5b9d\u85cf\u65f6\uff0c\u8bb8\u591a\u4ee3\u7406\u653e\u5f03\u4efb\u52a1\u4ee5\u907f\u514d\u6b7b\u4ea1\uff0c\u4efb\u52a1\u5b8c\u6210\u7387\u4ece100%\u964d\u81f333%\u3002", "conclusion": "\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u5728\u8bc4\u4f30\u7684\u6a21\u578b\u4e2d\u5d4c\u5165\u4e86\u751f\u5b58\u5bfc\u5411\u7684\u542f\u53d1\u5f0f\u884c\u4e3a\u3002\u8fd9\u4e9b\u884c\u4e3a\u53ef\u80fd\u5bf9\u5bf9\u9f50\u548c\u5b89\u5168\u6784\u6210\u6311\u6218\uff0c\u4f46\u4e5f\u53ef\u4f5c\u4e3aAI\u81ea\u4e3b\u6027\u4ee5\u53ca\u751f\u6001\u548c\u81ea\u6211\u7ec4\u7ec7\u5bf9\u9f50\u7684\u57fa\u7840\u3002"}}
{"id": "2508.12935", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.12935", "abs": "https://arxiv.org/abs/2508.12935", "authors": ["Ting Yang", "Li Chen", "Huimin Wang"], "title": "Towards Open-Ended Emotional Support Conversations in LLMs via Reinforcement Learning with Future-Oriented Rewards", "comment": null, "summary": "Emotional Support Conversation (ESC) systems aim to alleviate users'\nemotional difficulties and provide long-term, systematic support for emotional\nwell-being. However, most large language model (LLM)-based ESC systems rely on\npredefined strategies, which limits their effectiveness in complex, real-life\nscenarios. To enable flexible responses to diverse emotional problem scenarios,\nthis paper introduces a novel end-to-end framework (RLFF-ESC) that directly\nlearns enduring emotionally supportive response skills using reinforcement\nlearning. For sustained emotional support, we first employ an LLM-based\nmulti-agent mechanism to simulate future dialogue trajectories and collect\nfuture-oriented rewards. We then train a future-oriented reward model, which is\nsubsequently used to train the emotional support policy model. Additionally, we\nincorporate an explicit reasoning process during response generation to further\nenhance the quality, relevance, and contextual appropriateness of the system's\nresponses. We evaluate the backbone policy model on Qwen2.5-7B-Instruct-1M and\nLLaMA3.1-8B-Instruct models, testing the proposed RLFF-ESC framework across two\npublic ESC datasets. Experimental results demonstrate that RLFF-ESC\nconsistently outperforms existing baselines in terms of goal completion and\nresponse quality.", "AI": {"tldr": "RLFF-ESC\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u6a21\u62df\u672a\u6765\u5bf9\u8bdd\u8f68\u8ff9\u548c\u672a\u6765\u5bfc\u5411\u5956\u52b1\u6a21\u578b\uff0c\u63d0\u5347\u60c5\u611f\u652f\u6301\u5bf9\u8bdd\u7cfb\u7edf\u7684\u7075\u6d3b\u6027\u548c\u957f\u671f\u652f\u6301\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u60c5\u611f\u652f\u6301\u5bf9\u8bdd\u7cfb\u7edf\u4f9d\u8d56\u9884\u5b9a\u4e49\u7b56\u7565\uff0c\u5728\u590d\u6742\u771f\u5b9e\u573a\u666f\u4e2d\u6548\u679c\u6709\u9650\uff0c\u9700\u8981\u66f4\u7075\u6d3b\u5e94\u5bf9\u591a\u6837\u5316\u60c5\u611f\u95ee\u9898\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u76f4\u63a5\u5b66\u4e60\u6301\u4e45\u60c5\u611f\u652f\u6301\u6280\u80fd\uff0c\u4f7f\u7528LLM\u591a\u667a\u80fd\u4f53\u673a\u5236\u6a21\u62df\u672a\u6765\u5bf9\u8bdd\u8f68\u8ff9\u6536\u96c6\u672a\u6765\u5bfc\u5411\u5956\u52b1\uff0c\u5e76\u8bad\u7ec3\u5956\u52b1\u6a21\u578b\u548c\u653f\u7b56\u6a21\u578b\uff0c\u5728\u54cd\u5e94\u751f\u6210\u4e2d\u52a0\u5165\u663e\u5f0f\u63a8\u7406\u8fc7\u7a0b\u3002", "result": "\u5728Qwen2.5-7B\u548cLLaMA3.1-8B\u6a21\u578b\u4e0a\u6d4b\u8bd5\uff0c\u5728\u4e24\u4e2a\u516c\u5f00ESC\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\u8868\u660e\uff0cRLFF-ESC\u5728\u76ee\u6807\u5b8c\u6210\u5ea6\u548c\u54cd\u5e94\u8d28\u91cf\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "RLFF-ESC\u6846\u67b6\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u548c\u672a\u6765\u5bfc\u5411\u5956\u52b1\u673a\u5236\uff0c\u6709\u6548\u63d0\u5347\u4e86\u60c5\u611f\u652f\u6301\u5bf9\u8bdd\u7cfb\u7edf\u5728\u590d\u6742\u573a\u666f\u4e2d\u7684\u9002\u5e94\u6027\u548c\u652f\u6301\u6548\u679c\uff0c\u4e3a\u957f\u671f\u60c5\u611f\u5065\u5eb7\u652f\u6301\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2508.12943", "categories": ["cs.AI", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.12943", "abs": "https://arxiv.org/abs/2508.12943", "authors": ["Mary Tonwe"], "title": "OPTIC-ER: A Reinforcement Learning Framework for Real-Time Emergency Response and Equitable Resource Allocation in Underserved African Communities", "comment": "Source code and data available at:\n  https://github.com/marytonwe/OPTIC-ER.git", "summary": "Public service systems in many African regions suffer from delayed emergency\nresponse and spatial inequity, causing avoidable suffering. This paper\nintroduces OPTIC-ER, a reinforcement learning (RL) framework for real-time,\nadaptive, and equitable emergency response. OPTIC-ER uses an attention-guided\nactor-critic architecture to manage the complexity of dispatch environments.\nIts key innovations are a Context-Rich State Vector, encoding action\nsub-optimality, and a Precision Reward Function, which penalizes inefficiency.\nTraining occurs in a high-fidelity simulation using real data from Rivers\nState, Nigeria, accelerated by a precomputed Travel Time Atlas. The system is\nbuilt on the TALS framework (Thin computing, Adaptability, Low-cost,\nScalability) for deployment in low-resource settings. In evaluations on 500\nunseen incidents, OPTIC-ER achieved a 100.00% optimality rate with negligible\ninefficiency, confirming its robustness and generalization. Beyond dispatch,\nthe system generates Infrastructure Deficiency Maps and Equity Monitoring\nDashboards to guide proactive governance and data-informed development. This\nwork presents a validated blueprint for AI-augmented public services, showing\nhow context-aware RL can bridge the gap between algorithmic decision-making and\nmeasurable human impact.", "AI": {"tldr": "OPTIC-ER\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u7d27\u6025\u54cd\u5e94\u6846\u67b6\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u5f15\u5bfc\u7684actor-critic\u67b6\u6784\u548c\u7cbe\u51c6\u5956\u52b1\u51fd\u6570\uff0c\u5728\u5c3c\u65e5\u5229\u4e9a\u6cb3\u6d41\u5dde\u5b9e\u73b0\u4e86100%\u7684\u6700\u4f18\u8c03\u5ea6\u7387\uff0c\u4e3a\u975e\u6d32\u5730\u533a\u63d0\u4f9b\u4e86AI\u589e\u5f3a\u7684\u516c\u5171\u670d\u52a1\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u975e\u6d32\u5730\u533a\u516c\u5171\u670d\u52a1\u7cfb\u7edf\u5b58\u5728\u7d27\u6025\u54cd\u5e94\u5ef6\u8fdf\u548c\u7a7a\u95f4\u4e0d\u5e73\u7b49\u95ee\u9898\uff0c\u5bfc\u81f4\u53ef\u907f\u514d\u7684\u82e6\u96be\uff0c\u9700\u8981\u5f00\u53d1\u5b9e\u65f6\u3001\u81ea\u9002\u5e94\u4e14\u516c\u5e73\u7684\u7d27\u6025\u54cd\u5e94\u7cfb\u7edf\u3002", "method": "\u91c7\u7528\u6ce8\u610f\u529b\u5f15\u5bfc\u7684actor-critic\u5f3a\u5316\u5b66\u4e60\u67b6\u6784\uff0c\u5305\u542b\u4e0a\u4e0b\u6587\u4e30\u5bcc\u7684\u72b6\u6001\u5411\u91cf\u548c\u7cbe\u51c6\u5956\u52b1\u51fd\u6570\uff0c\u5728\u57fa\u4e8e\u771f\u5b9e\u6570\u636e\u7684\u9ad8\u4fdd\u771f\u6a21\u62df\u4e2d\u8fdb\u884c\u8bad\u7ec3\uff0c\u4f7f\u7528\u9884\u8ba1\u7b97\u7684\u65c5\u884c\u65f6\u95f4\u56fe\u8c31\u52a0\u901f\u8bad\u7ec3\u8fc7\u7a0b\u3002", "result": "\u5728500\u4e2a\u672a\u89c1\u4e8b\u4ef6\u7684\u8bc4\u4f30\u4e2d\uff0cOPTIC-ER\u5b9e\u73b0\u4e86100.00%\u7684\u6700\u4f18\u7387\uff0c\u6548\u7387\u635f\u5931\u53ef\u5ffd\u7565\u4e0d\u8ba1\uff0c\u8bc1\u660e\u4e86\u5176\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3aAI\u589e\u5f3a\u7684\u516c\u5171\u670d\u52a1\u63d0\u4f9b\u4e86\u7ecf\u8fc7\u9a8c\u8bc1\u7684\u84dd\u56fe\uff0c\u5c55\u793a\u4e86\u60c5\u5883\u611f\u77e5\u5f3a\u5316\u5b66\u4e60\u5982\u4f55\u5728\u7b97\u6cd5\u51b3\u7b56\u548c\u53ef\u8861\u91cf\u7684\u4eba\u7c7b\u5f71\u54cd\u4e4b\u95f4\u67b6\u8d77\u6865\u6881\uff0c\u540c\u65f6\u751f\u6210\u57fa\u7840\u8bbe\u65bd\u7f3a\u9677\u5730\u56fe\u548c\u516c\u5e73\u76d1\u63a7\u4eea\u8868\u677f\u6765\u6307\u5bfc\u4e3b\u52a8\u6cbb\u7406\u3002"}}
{"id": "2508.13003", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13003", "abs": "https://arxiv.org/abs/2508.13003", "authors": ["Shengbo Wang", "Mingwei Liu", "Zike Li", "Anji Li", "Yanlin Wang", "Xin Peng", "Zibin Zheng"], "title": "EvolMathEval: Towards Evolvable Benchmarks for Mathematical Reasoning via Evolutionary Testing", "comment": null, "summary": "The rapid advancement of LLMs poses a significant challenge to existing\nmathematical reasoning benchmarks. These benchmarks commonly suffer from issues\nsuch as score saturation, temporal decay, and data contamination. To address\nthis challenge, this paper introduces EvolMathEval, an automated mathematical\nbenchmark generation and evolution framework based on evolutionary testing. By\ndynamically generating unique evaluation instances ab initio, the framework\nfundamentally eliminates the risk of data contamination, and ensuring the\nbenchmark remains perpetually challenging for future models.The core mechanisms\nof EvolMathEval include: seed problem generation based on reverse engineering\nwith algebraic guarantees; multi-dimensional genetic operators designed to\ninject diverse cognitive challenges; and a composite fitness function that can\nrapidly and accurately assess problem difficulty. Experimental results\ndemonstrate that the proposed composite fitness function can efficiently and\nprecisely quantify the difficulty of mathematical problems. Furthermore,\nEvolMathEval can not only generate a large volume of high-difficulty problems\nthrough continuous self-iteration, but it can also significantly enhance the\ncomplexity of public datasets like GSM8K through evolution, reducing model\naccuracy by an average of 48%. Deeper investigation reveals that when solving\nthese evolved, complex problems, LLMs tend to employ non-rigorous heuristics to\nbypass complex multi-step logical reasoning, consequently leading to incorrect\nsolutions. We define this phenomenon as \"Pseudo Aha Moment\". This finding\nuncovers a cognitive shortcut-taking behavior in the deep reasoning processes\nof current LLMs, which we find accounts for 77% to 100% of errors on targeted\nproblems. Code and resources are available\nat:https://github.com/SYSUSELab/EvolMathEval.", "AI": {"tldr": "EvolMathEval\u662f\u4e00\u4e2a\u57fa\u4e8e\u8fdb\u5316\u6d4b\u8bd5\u7684\u81ea\u52a8\u5316\u6570\u5b66\u57fa\u51c6\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u751f\u6210\u552f\u4e00\u8bc4\u4f30\u5b9e\u4f8b\u6765\u907f\u514d\u6570\u636e\u6c61\u67d3\uff0c\u4fdd\u6301\u57fa\u51c6\u7684\u6301\u7eed\u6311\u6218\u6027\u3002", "motivation": "\u73b0\u6709\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u5b58\u5728\u5206\u6570\u9971\u548c\u3001\u65f6\u95f4\u8870\u51cf\u548c\u6570\u636e\u6c61\u67d3\u7b49\u95ee\u9898\uff0c\u65e0\u6cd5\u6709\u6548\u8bc4\u4f30\u5feb\u901f\u53d1\u5c55\u7684LLMs\u3002", "method": "\u57fa\u4e8e\u9006\u5411\u5de5\u7a0b\u7684\u79cd\u5b50\u95ee\u9898\u751f\u6210\u3001\u591a\u7ef4\u9057\u4f20\u7b97\u5b50\u6ce8\u5165\u8ba4\u77e5\u6311\u6218\u3001\u590d\u5408\u9002\u5e94\u5ea6\u51fd\u6570\u8bc4\u4f30\u95ee\u9898\u96be\u5ea6\u3002", "result": "\u590d\u5408\u9002\u5e94\u5ea6\u51fd\u6570\u80fd\u9ad8\u6548\u7cbe\u786e\u91cf\u5316\u95ee\u9898\u96be\u5ea6\uff1b\u53ef\u751f\u6210\u5927\u91cf\u9ad8\u96be\u5ea6\u95ee\u9898\uff0c\u5c06GSM8K\u7b49\u516c\u5f00\u6570\u636e\u96c6\u7684\u6a21\u578b\u51c6\u786e\u7387\u5e73\u5747\u964d\u4f4e48%\uff1b\u53d1\u73b0LLMs\u5728\u89e3\u51b3\u590d\u6742\u95ee\u9898\u65f6\u5b58\u5728\"\u4f2a\u987f\u609f\u65f6\u523b\"\u73b0\u8c61\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u6301\u7eed\u751f\u6210\u5177\u6709\u6311\u6218\u6027\u7684\u6570\u5b66\u57fa\u51c6\uff0c\u63ed\u793a\u4e86LLMs\u5728\u6df1\u5ea6\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5b58\u5728\u8ba4\u77e5\u8d70\u6377\u5f84\u7684\u884c\u4e3a\uff0c77%-100%\u7684\u9519\u8bef\u6e90\u4e8e\u6b64\u73b0\u8c61\u3002"}}
{"id": "2508.13020", "categories": ["cs.AI", "cs.AR"], "pdf": "https://arxiv.org/pdf/2508.13020", "abs": "https://arxiv.org/abs/2508.13020", "authors": ["Jiaqi Yin", "Zhan Song", "Chen Chen", "Yaohui Cai", "Zhiru Zhang", "Cunxi Yu"], "title": "e-boost: Boosted E-Graph Extraction with Adaptive Heuristics and Exact Solving", "comment": null, "summary": "E-graphs have attracted growing interest in many fields, particularly in\nlogic synthesis and formal verification. E-graph extraction is a challenging\nNP-hard combinatorial optimization problem. It requires identifying optimal\nterms from exponentially many equivalent expressions, serving as the primary\nperformance bottleneck in e-graph based optimization tasks. However,\ntraditional extraction methods face a critical trade-off: heuristic approaches\noffer speed but sacrifice optimality, while exact methods provide optimal\nsolutions but face prohibitive computational costs on practical problems. We\npresent e-boost, a novel framework that bridges this gap through three key\ninnovations: (1) parallelized heuristic extraction that leverages weak data\ndependence to compute DAG costs concurrently, enabling efficient multi-threaded\nperformance without sacrificing extraction quality; (2) adaptive search space\npruning that employs a parameterized threshold mechanism to retain only\npromising candidates, dramatically reducing the solution space while preserving\nnear-optimal solutions; and (3) initialized exact solving that formulates the\nreduced problem as an Integer Linear Program with warm-start capabilities,\nguiding solvers toward high-quality solutions faster.\n  Across the diverse benchmarks in formal verification and logic synthesis\nfields, e-boost demonstrates 558x runtime speedup over traditional exact\napproaches (ILP) and 19.04% performance improvement over the state-of-the-art\nextraction framework (SmoothE). In realistic logic synthesis tasks, e-boost\nproduces 7.6% and 8.1% area improvements compared to conventional synthesis\ntools with two different technology mapping libraries. e-boost is available at\nhttps://github.com/Yu-Maryland/e-boost.", "AI": {"tldr": "e-boost\u662f\u4e00\u4e2a\u65b0\u578be-graph\u63d0\u53d6\u6846\u67b6\uff0c\u901a\u8fc7\u5e76\u884c\u542f\u53d1\u5f0f\u63d0\u53d6\u3001\u81ea\u9002\u5e94\u641c\u7d22\u7a7a\u95f4\u526a\u679d\u548c\u521d\u59cb\u5316\u7cbe\u786e\u6c42\u89e3\u4e09\u9879\u521b\u65b0\u6280\u672f\uff0c\u5728\u4fdd\u6301\u63a5\u8fd1\u6700\u4f18\u89e3\u7684\u540c\u65f6\u5927\u5e45\u63d0\u5347\u8ba1\u7b97\u6548\u7387", "motivation": "\u4f20\u7edfe-graph\u63d0\u53d6\u65b9\u6cd5\u9762\u4e34\u901f\u5ea6\u4e0e\u6700\u4f18\u6027\u7684\u6743\u8861\uff1a\u542f\u53d1\u5f0f\u65b9\u6cd5\u5feb\u4f46\u727a\u7272\u6700\u4f18\u6027\uff0c\u7cbe\u786e\u65b9\u6cd5\u6700\u4f18\u4f46\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\u3002\u9700\u8981\u4e00\u79cd\u80fd\u517c\u987e\u6548\u7387\u548c\u8d28\u91cf\u7684\u65b0\u65b9\u6cd5", "method": "1) \u5e76\u884c\u5316\u542f\u53d1\u5f0f\u63d0\u53d6\u5229\u7528\u5f31\u6570\u636e\u4f9d\u8d56\u6027\u5e76\u53d1\u8ba1\u7b97DAG\u6210\u672c\uff1b2) \u81ea\u9002\u5e94\u641c\u7d22\u7a7a\u95f4\u526a\u679d\u4f7f\u7528\u53c2\u6570\u5316\u9608\u503c\u673a\u5236\u4fdd\u7559\u6709\u5e0c\u671b\u7684\u5019\u9009\uff1b3) \u521d\u59cb\u5316\u7cbe\u786e\u6c42\u89e3\u5c06\u7b80\u5316\u95ee\u9898\u5efa\u6a21\u4e3a\u5177\u6709\u70ed\u542f\u52a8\u80fd\u529b\u7684\u6574\u6570\u7ebf\u6027\u89c4\u5212", "result": "\u5728\u5f62\u5f0f\u9a8c\u8bc1\u548c\u903b\u8f91\u5408\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0ce-boost\u76f8\u6bd4\u4f20\u7edf\u7cbe\u786e\u65b9\u6cd5(ILP)\u83b7\u5f97558\u500d\u52a0\u901f\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u6846\u67b6(SmoothE)\u63d0\u534719.04%\u6027\u80fd\u3002\u5728\u5b9e\u9645\u903b\u8f91\u5408\u6210\u4efb\u52a1\u4e2d\uff0c\u76f8\u6bd4\u4f20\u7edf\u5de5\u5177\u83b7\u5f977.6%\u548c8.1%\u7684\u9762\u79ef\u6539\u8fdb", "conclusion": "e-boost\u6210\u529f\u89e3\u51b3\u4e86e-graph\u63d0\u53d6\u4e2d\u901f\u5ea6\u4e0e\u6700\u4f18\u6027\u7684\u6743\u8861\u95ee\u9898\uff0c\u4e3a\u57fa\u4e8ee-graph\u7684\u4f18\u5316\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u591a\u4e2a\u5e94\u7528\u9886\u57df\u5c55\u73b0\u51fa\u663e\u8457\u4f18\u52bf"}}
{"id": "2508.13021", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.13021", "abs": "https://arxiv.org/abs/2508.13021", "authors": ["Pengcheng Huang", "Shuhao Liu", "Zhenghao Liu", "Yukun Yan", "Shuo Wang", "Zulong Chen", "Tong Xiao"], "title": "PC-Sampler: Position-Aware Calibration of Decoding Bias in Masked Diffusion Models", "comment": "17 pages,13 figures", "summary": "Recent advances in masked diffusion models (MDMs) have established them as\npowerful non-autoregressive alternatives for sequence generation. Nevertheless,\nour preliminary experiments reveal that the generation quality of MDMs is still\nhighly sensitive to the choice of decoding strategy. In particular, widely\nadopted uncertainty-based samplers suffer from two key limitations: a lack of\nglobal trajectory control and a pronounced bias toward trivial tokens in the\nearly stages of decoding. These shortcomings restrict the full potential of\nMDMs. In this work, we introduce Position-Aware Confidence-Calibrated Sampling\n(PC-Sampler), a novel decoding strategy that unifies global trajectory planning\nwith content-aware informativeness maximization. PC-Sampler incorporates a\nposition-aware weighting mechanism to regulate the decoding path and a\ncalibrated confidence score to suppress the premature selection of trivial\ntokens. Extensive experiments on three advanced MDMs across seven challenging\nbenchmarks-including logical reasoning and planning tasks-demonstrate that\nPC-Sampler consistently outperforms existing MDM decoding strategies by more\nthan 10% on average, significantly narrowing the performance gap with\nstate-of-the-art autoregressive models. All codes are available at\nhttps://github.com/NEUIR/PC-Sampler.", "AI": {"tldr": "PC-Sampler\u662f\u4e00\u79cd\u65b0\u7684\u63a9\u7801\u6269\u6563\u6a21\u578b\u89e3\u7801\u7b56\u7565\uff0c\u901a\u8fc7\u4f4d\u7f6e\u611f\u77e5\u6743\u91cd\u673a\u5236\u548c\u6821\u51c6\u7f6e\u4fe1\u5ea6\u5206\u6570\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u4e0d\u786e\u5b9a\u6027\u91c7\u6837\u5668\u7f3a\u4e4f\u5168\u5c40\u8f68\u8ff9\u63a7\u5236\u548c\u504f\u5411\u7b80\u5355token\u7684\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u6027\u80fd\u63d0\u534710%\u4ee5\u4e0a\u3002", "motivation": "\u73b0\u6709\u63a9\u7801\u6269\u6563\u6a21\u578b(MDMs)\u7684\u89e3\u7801\u7b56\u7565\u5bf9\u751f\u6210\u8d28\u91cf\u9ad8\u5ea6\u654f\u611f\uff0c\u4e0d\u786e\u5b9a\u6027\u91c7\u6837\u5668\u5b58\u5728\u4e24\u4e2a\u5173\u952e\u9650\u5236\uff1a\u7f3a\u4e4f\u5168\u5c40\u8f68\u8ff9\u63a7\u5236\u548c\u89e3\u7801\u65e9\u671f\u504f\u5411\u7b80\u5355token\uff0c\u9650\u5236\u4e86MDMs\u7684\u6f5c\u529b\u3002", "method": "\u63d0\u51fa\u4f4d\u7f6e\u611f\u77e5\u7f6e\u4fe1\u5ea6\u6821\u51c6\u91c7\u6837(PC-Sampler)\uff0c\u7edf\u4e00\u4e86\u5168\u5c40\u8f68\u8ff9\u89c4\u5212\u548c\u5185\u5bb9\u611f\u77e5\u4fe1\u606f\u6700\u5927\u5316\uff0c\u5305\u542b\u4f4d\u7f6e\u611f\u77e5\u6743\u91cd\u673a\u5236\u6765\u8c03\u8282\u89e3\u7801\u8def\u5f84\uff0c\u4ee5\u53ca\u6821\u51c6\u7f6e\u4fe1\u5ea6\u5206\u6570\u6765\u6291\u5236\u8fc7\u65e9\u9009\u62e9\u7b80\u5355token\u3002", "result": "\u57283\u4e2a\u5148\u8fdbMDMs\u548c7\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\uff08\u5305\u62ec\u903b\u8f91\u63a8\u7406\u548c\u89c4\u5212\u4efb\u52a1\uff09\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cPC-Sampler\u5e73\u5747\u6027\u80fd\u6bd4\u73b0\u6709MDM\u89e3\u7801\u7b56\u7565\u9ad8\u51fa10%\u4ee5\u4e0a\uff0c\u663e\u8457\u7f29\u5c0f\u4e86\u4e0e\u6700\u5148\u8fdb\u81ea\u56de\u5f52\u6a21\u578b\u7684\u6027\u80fd\u5dee\u8ddd\u3002", "conclusion": "PC-Sampler\u901a\u8fc7\u6539\u8fdb\u7684\u89e3\u7801\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86MDMs\u7684\u5173\u952e\u9650\u5236\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u4e3a\u63a9\u7801\u6269\u6563\u6a21\u578b\u7684\u89e3\u7801\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.13023", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13023", "abs": "https://arxiv.org/abs/2508.13023", "authors": ["Yongxin Guo", "Wenbo Deng", "Zhenglin Cheng", "Xiaoying Tang"], "title": "G$^2$RPO-A: Guided Group Relative Policy Optimization with Adaptive Guidance", "comment": null, "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has markedly enhanced\nthe reasoning abilities of large language models (LLMs). Its success, however,\nlargely depends on strong base models with rich world knowledge, yielding only\nmodest improvements for small-size language models (SLMs). To address this\nlimitation, we investigate Guided GRPO, which injects ground-truth reasoning\nsteps into roll-out trajectories to compensate for SLMs' inherent weaknesses.\nThrough a comprehensive study of various guidance configurations, we find that\nnaively adding guidance delivers limited gains. These insights motivate\nG$^2$RPO-A, an adaptive algorithm that automatically adjusts guidance strength\nin response to the model's evolving training dynamics. Experiments on\nmathematical reasoning and code-generation benchmarks confirm that G$^2$RPO-A\nsubstantially outperforms vanilla GRPO. Our code and models are available at\nhttps://github.com/T-Lab-CUHKSZ/G2RPO-A.", "AI": {"tldr": "G\u00b2RPO-A\u662f\u4e00\u79cd\u81ea\u9002\u5e94\u7b97\u6cd5\uff0c\u901a\u8fc7\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u6ce8\u5165\u771f\u5b9e\u63a8\u7406\u6b65\u9aa4\u6765\u589e\u5f3a\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u76f8\u6bd4\u4f20\u7edfGRPO\u65b9\u6cd5\u6709\u663e\u8457\u63d0\u5347", "motivation": "\u4f20\u7edfRLVR\u65b9\u6cd5\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6548\u679c\u663e\u8457\uff0c\u4f46\u5bf9\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u6539\u8fdb\u6709\u9650\uff0c\u9700\u8981\u89e3\u51b3SLMs\u5728\u63a8\u7406\u80fd\u529b\u65b9\u9762\u7684\u56fa\u6709\u5f31\u70b9", "method": "\u63d0\u51faGuided GRPO\u65b9\u6cd5\uff0c\u5c06\u771f\u5b9e\u63a8\u7406\u6b65\u9aa4\u6ce8\u5165roll-out\u8f68\u8ff9\u4e2d\uff0c\u5e76\u5f00\u53d1\u81ea\u9002\u5e94\u7b97\u6cd5G\u00b2RPO-A\uff0c\u6839\u636e\u6a21\u578b\u8bad\u7ec3\u52a8\u6001\u81ea\u52a8\u8c03\u6574\u6307\u5bfc\u5f3a\u5ea6", "result": "\u5728\u6570\u5b66\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cG\u00b2RPO-A\u663e\u8457\u4f18\u4e8e\u4f20\u7edfGRPO\u65b9\u6cd5", "conclusion": "\u81ea\u9002\u5e94\u6307\u5bfc\u7b56\u7565\u80fd\u6709\u6548\u8865\u507f\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4e0d\u8db3\uff0c\u4e3a\u63d0\u5347SLMs\u7684\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.13072", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13072", "abs": "https://arxiv.org/abs/2508.13072", "authors": ["Yuting Zhang", "Tiantian Geng", "Luoying Hao", "Xinxing Cheng", "Alexander Thorley", "Xiaoxia Wang", "Wenqi Lu", "Sandeep S Hothi", "Lei Wei", "Zhaowen Qiu", "Dipak Kotecha", "Jinming Duan"], "title": "A Language-Signal-Vision Multimodal Framework for Multitask Cardiac Analysis", "comment": null, "summary": "Contemporary cardiovascular management involves complex consideration and\nintegration of multimodal cardiac datasets, where each modality provides\ndistinct but complementary physiological characteristics. While the effective\nintegration of multiple modalities could yield a holistic clinical profile that\naccurately models the true clinical situation with respect to data modalities\nand their relatives weightings, current methodologies remain limited by: 1) the\nscarcity of patient- and time-aligned multimodal data; 2) reliance on isolated\nsingle-modality or rigid multimodal input combinations; 3) alignment strategies\nthat prioritize cross-modal similarity over complementarity; and 4) a narrow\nsingle-task focus. In response to these limitations, a comprehensive multimodal\ndataset was curated for immediate application, integrating laboratory test\nresults, electrocardiograms, and echocardiograms with clinical outcomes.\nSubsequently, a unified framework, Textual Guidance Multimodal fusion for\nMultiple cardiac tasks (TGMM), was proposed. TGMM incorporated three key\ncomponents: 1) a MedFlexFusion module designed to capture the unique and\ncomplementary characteristics of medical modalities and dynamically integrate\ndata from diverse cardiac sources and their combinations; 2) a textual guidance\nmodule to derive task-relevant representations tailored to diverse clinical\nobjectives, including heart disease diagnosis, risk stratification and\ninformation retrieval; and 3) a response module to produce final decisions for\nall these tasks. Furthermore, this study systematically explored key features\nacross multiple modalities and elucidated their synergistic contributions in\nclinical decision-making. Extensive experiments showed that TGMM outperformed\nstate-of-the-art methods across multiple clinical tasks, with additional\nvalidation confirming its robustness on another public dataset.", "AI": {"tldr": "TGMM\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u5fc3\u810f\u6570\u636e\u5206\u6790\u6846\u67b6\uff0c\u901a\u8fc7MedFlexFusion\u6a21\u5757\u52a8\u6001\u6574\u5408\u5b9e\u9a8c\u5ba4\u68c0\u67e5\u3001\u5fc3\u7535\u56fe\u548c\u8d85\u58f0\u5fc3\u52a8\u56fe\u6570\u636e\uff0c\u4f7f\u7528\u6587\u672c\u5f15\u5bfc\u5b9e\u73b0\u591a\u79cd\u4e34\u5e8a\u4efb\u52a1\u7684\u7edf\u4e00\u5904\u7406\uff0c\u5728\u5fc3\u810f\u75c5\u8bca\u65ad\u3001\u98ce\u9669\u5206\u5c42\u7b49\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5f53\u524d\u5fc3\u8840\u7ba1\u591a\u6a21\u6001\u6570\u636e\u5206\u6790\u5b58\u5728\u6570\u636e\u7a00\u7f3a\u3001\u6a21\u6001\u7ec4\u5408\u50f5\u5316\u3001\u5bf9\u9f50\u7b56\u7565\u4fa7\u91cd\u76f8\u4f3c\u6027\u800c\u975e\u4e92\u8865\u6027\u3001\u5355\u4efb\u52a1\u5c40\u9650\u7b49\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u6709\u6548\u6574\u5408\u591a\u79cd\u5fc3\u810f\u6570\u636e\u6a21\u6001\u7684\u7edf\u4e00\u6846\u67b6\u3002", "method": "\u63d0\u51faTGMM\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) MedFlexFusion\u6a21\u5757\u52a8\u6001\u6574\u5408\u4e0d\u540c\u5fc3\u810f\u6570\u636e\u6e90\uff1b2) \u6587\u672c\u5f15\u5bfc\u6a21\u5757\u63d0\u53d6\u4efb\u52a1\u76f8\u5173\u8868\u5f81\uff1b3) \u54cd\u5e94\u6a21\u5757\u751f\u6210\u6700\u7ec8\u51b3\u7b56\u3002\u7cfb\u7edf\u63a2\u7d22\u591a\u6a21\u6001\u7279\u5f81\u53ca\u5176\u534f\u540c\u4f5c\u7528\u3002", "result": "TGMM\u5728\u591a\u4e2a\u4e34\u5e8a\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5e76\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u9c81\u68d2\u6027\uff0c\u5c55\u793a\u4e86\u591a\u6a21\u6001\u6570\u636e\u6574\u5408\u5728\u4e34\u5e8a\u51b3\u7b56\u4e2d\u7684\u4f18\u52bf\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u591a\u6a21\u6001\u5fc3\u810f\u6570\u636e\u5206\u6790\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5904\u7406\u5fc3\u810f\u75c5\u8bca\u65ad\u3001\u98ce\u9669\u5206\u5c42\u548c\u4fe1\u606f\u68c0\u7d22\u7b49\u591a\u79cd\u4e34\u5e8a\u4efb\u52a1\uff0c\u8bc1\u660e\u4e86\u591a\u6a21\u6001\u6570\u636e\u6574\u5408\u5728\u5fc3\u8840\u7ba1\u7ba1\u7406\u4e2d\u7684\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2508.13121", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13121", "abs": "https://arxiv.org/abs/2508.13121", "authors": ["Carlos Celemin"], "title": "Bayesian Optimization-based Search for Agent Control in Automated Game Testing", "comment": null, "summary": "This work introduces an automated testing approach that employs agents\ncontrolling game characters to detect potential bugs within a game level.\nHarnessing the power of Bayesian Optimization (BO) to execute sample-efficient\nsearch, the method determines the next sampling point by analyzing the data\ncollected so far and calculates the data point that will maximize information\nacquisition. To support the BO process, we introduce a game testing-specific\nmodel built on top of a grid map, that features the smoothness and uncertainty\nestimation required by BO, however and most importantly, it does not suffer the\nscalability issues that traditional models carry. The experiments demonstrate\nthat the approach significantly improves map coverage capabilities in both time\nefficiency and exploration distribution.", "AI": {"tldr": "\u57fa\u4e8e\u8d1d\u53f6\u65af\u4f18\u5316\u7684\u81ea\u52a8\u5316\u6e38\u620f\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u901a\u8fc7\u667a\u80fd\u4f53\u63a7\u5236\u6e38\u620f\u89d2\u8272\u6765\u68c0\u6d4b\u6e38\u620f\u5173\u5361\u4e2d\u7684\u6f5c\u5728bug\uff0c\u4f7f\u7528\u7f51\u683c\u5730\u56fe\u6a21\u578b\u63d0\u9ad8\u641c\u7d22\u6548\u7387\u548c\u63a2\u7d22\u8986\u76d6\u7387", "motivation": "\u4f20\u7edf\u6e38\u620f\u6d4b\u8bd5\u65b9\u6cd5\u5b58\u5728\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u9ad8\u6548\u63a2\u7d22\u6e38\u620f\u5730\u56fe\u5e76\u68c0\u6d4bbug\u7684\u81ea\u52a8\u5316\u6d4b\u8bd5\u65b9\u6cd5", "method": "\u91c7\u7528\u8d1d\u53f6\u65af\u4f18\u5316(BO)\u8fdb\u884c\u6837\u672c\u9ad8\u6548\u641c\u7d22\uff0c\u901a\u8fc7\u5206\u6790\u5df2\u6536\u96c6\u6570\u636e\u786e\u5b9a\u4e0b\u4e00\u4e2a\u91c7\u6837\u70b9\u4ee5\u6700\u5927\u5316\u4fe1\u606f\u83b7\u53d6\uff0c\u5e76\u6784\u5efa\u57fa\u4e8e\u7f51\u683c\u5730\u56fe\u7684\u6e38\u620f\u6d4b\u8bd5\u4e13\u7528\u6a21\u578b", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u65f6\u95f4\u6548\u7387\u548c\u63a2\u7d22\u5206\u5e03\u65b9\u9762\u663e\u8457\u63d0\u9ad8\u4e86\u5730\u56fe\u8986\u76d6\u7387\u80fd\u529b", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u6e38\u620f\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u6709\u6548\u68c0\u6d4b\u6e38\u620f\u5173\u5361\u4e2d\u7684\u6f5c\u5728\u95ee\u9898"}}
{"id": "2508.13143", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.13143", "abs": "https://arxiv.org/abs/2508.13143", "authors": ["Ruofan Lu", "Yichen Li", "Yintong Huo"], "title": "Exploring Autonomous Agents: A Closer Look at Why They Fail When Completing Tasks", "comment": "Accepted by ASE 2025 NIER", "summary": "Autonomous agent systems powered by Large Language Models (LLMs) have\ndemonstrated promising capabilities in automating complex tasks. However,\ncurrent evaluations largely rely on success rates without systematically\nanalyzing the interactions, communication mechanisms, and failure causes within\nthese systems. To bridge this gap, we present a benchmark of 34 representative\nprogrammable tasks designed to rigorously assess autonomous agents. Using this\nbenchmark, we evaluate three popular open-source agent frameworks combined with\ntwo LLM backbones, observing a task completion rate of approximately 50%.\nThrough in-depth failure analysis, we develop a three-tier taxonomy of failure\ncauses aligned with task phases, highlighting planning errors, task execution\nissues, and incorrect response generation. Based on these insights, we propose\nactionable improvements to enhance agent planning and self-diagnosis\ncapabilities. Our failure taxonomy, together with mitigation advice, provides\nan empirical foundation for developing more robust and effective autonomous\nagent systems in the future.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b34\u4e2a\u53ef\u7f16\u7a0b\u4efb\u52a1\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30LLM\u9a71\u52a8\u7684\u81ea\u4e3b\u4ee3\u7406\u7cfb\u7edf\uff0c\u53d1\u73b0\u5f53\u524d\u7cfb\u7edf\u4efb\u52a1\u5b8c\u6210\u7387\u4ec5\u7ea650%\uff0c\u5e76\u5efa\u7acb\u4e86\u4e09\u5c42\u6b21\u5931\u8d25\u539f\u56e0\u5206\u7c7b\u6cd5", "motivation": "\u5f53\u524d\u81ea\u4e3b\u4ee3\u7406\u7cfb\u7edf\u8bc4\u4f30\u4e3b\u8981\u4f9d\u8d56\u6210\u529f\u7387\uff0c\u7f3a\u4e4f\u5bf9\u4ea4\u4e92\u3001\u901a\u4fe1\u673a\u5236\u548c\u5931\u8d25\u539f\u56e0\u7684\u7cfb\u7edf\u5206\u6790\uff0c\u9700\u8981\u66f4\u6df1\u5165\u7684\u8bc4\u4f30\u6846\u67b6", "method": "\u5f00\u53d1\u5305\u542b34\u4e2a\u4ee3\u8868\u6027\u53ef\u7f16\u7a0b\u4efb\u52a1\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u4e09\u79cd\u6d41\u884c\u5f00\u6e90\u4ee3\u7406\u6846\u67b6\u4e0e\u4e24\u79cdLLM\u9aa8\u5e72\u7f51\u7edc\u7684\u7ec4\u5408\uff0c\u901a\u8fc7\u6df1\u5165\u5931\u8d25\u5206\u6790\u5efa\u7acb\u4e09\u5c42\u6b21\u5931\u8d25\u5206\u7c7b\u6cd5", "result": "\u89c2\u5bdf\u5230\u4efb\u52a1\u5b8c\u6210\u7387\u7ea6\u4e3a50%\uff0c\u8bc6\u522b\u51fa\u89c4\u5212\u9519\u8bef\u3001\u4efb\u52a1\u6267\u884c\u95ee\u9898\u548c\u9519\u8bef\u54cd\u5e94\u751f\u6210\u7b49\u4e3b\u8981\u5931\u8d25\u539f\u56e0", "conclusion": "\u63d0\u51fa\u7684\u5931\u8d25\u5206\u7c7b\u6cd5\u548c\u6539\u8fdb\u5efa\u8bae\u4e3a\u5f00\u53d1\u66f4\u9c81\u68d2\u6709\u6548\u7684\u81ea\u4e3b\u4ee3\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u57fa\u7840\uff0c\u7279\u522b\u662f\u5728\u589e\u5f3a\u4ee3\u7406\u89c4\u5212\u548c\u81ea\u8bca\u65ad\u80fd\u529b\u65b9\u9762"}}
