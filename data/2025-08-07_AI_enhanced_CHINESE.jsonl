{"id": "2508.03858", "categories": ["cs.AI", "cs.ET", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.03858", "abs": "https://arxiv.org/abs/2508.03858", "authors": ["Charles L. Wang", "Trisha Singhal", "Ameya Kelkar", "Jason Tuo"], "title": "MI9 -- Agent Intelligence Protocol: Runtime Governance for Agentic AI Systems", "comment": null, "summary": "Agentic AI systems capable of reasoning, planning, and executing actions\npresent fundamentally distinct governance challenges compared to traditional AI\nmodels. Unlike conventional AI, these systems exhibit emergent and unexpected\nbehaviors during runtime, introducing novel agent-related risks that cannot be\nfully anticipated through pre-deployment governance alone. To address this\ncritical gap, we introduce MI9, the first fully integrated runtime governance\nframework designed specifically for safety and alignment of agentic AI systems.\nMI9 introduces real-time controls through six integrated components:\nagency-risk index, agent-semantic telemetry capture, continuous authorization\nmonitoring, Finite-State-Machine (FSM)-based conformance engines,\ngoal-conditioned drift detection, and graduated containment strategies.\nOperating transparently across heterogeneous agent architectures, MI9 enables\nthe systematic, safe, and responsible deployment of agentic systems in\nproduction environments where conventional governance approaches fall short,\nproviding the foundational infrastructure for safe agentic AI deployment at\nscale. Detailed analysis through a diverse set of scenarios demonstrates MI9's\nsystematic coverage of governance challenges that existing approaches fail to\naddress, establishing the technical foundation for comprehensive agentic AI\noversight.", "AI": {"tldr": "MI9\u662f\u9996\u4e2a\u4e13\u4e3a\u4ee3\u7406\u578bAI\u7cfb\u7edf\u8bbe\u8ba1\u7684\u8fd0\u884c\u65f6\u6cbb\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u516d\u9879\u96c6\u6210\u7ec4\u4ef6\u89e3\u51b3\u4f20\u7edf\u6cbb\u7406\u65b9\u6cd5\u65e0\u6cd5\u5e94\u5bf9\u7684\u65b0\u578b\u98ce\u9669\u3002", "motivation": "\u4ee3\u7406\u578bAI\u7cfb\u7edf\u5728\u8fd0\u884c\u65f6\u8868\u73b0\u51fa\u4e0d\u53ef\u9884\u6d4b\u7684\u884c\u4e3a\uff0c\u4f20\u7edf\u6cbb\u7406\u65b9\u6cd5\u65e0\u6cd5\u5b8c\u5168\u8986\u76d6\u8fd9\u4e9b\u98ce\u9669\uff0c\u4e9f\u9700\u65b0\u7684\u6cbb\u7406\u6846\u67b6\u3002", "method": "MI9\u6846\u67b6\u5305\u542b\u516d\u9879\u5b9e\u65f6\u63a7\u5236\u7ec4\u4ef6\uff1a\u4ee3\u7406\u98ce\u9669\u6307\u6570\u3001\u4ee3\u7406\u8bed\u4e49\u9065\u6d4b\u6355\u83b7\u3001\u6301\u7eed\u6388\u6743\u76d1\u63a7\u3001\u57fa\u4e8e\u6709\u9650\u72b6\u6001\u673a\u7684\u7b26\u5408\u6027\u5f15\u64ce\u3001\u76ee\u6807\u6761\u4ef6\u6f02\u79fb\u68c0\u6d4b\u548c\u5206\u7ea7\u904f\u5236\u7b56\u7565\u3002", "result": "MI9\u5728\u591a\u6837\u5316\u573a\u666f\u4e2d\u5c55\u793a\u4e86\u7cfb\u7edf\u6027\u8986\u76d6\u6cbb\u7406\u6311\u6218\u7684\u80fd\u529b\uff0c\u4e3a\u4ee3\u7406\u578bAI\u7684\u5b89\u5168\u90e8\u7f72\u63d0\u4f9b\u4e86\u6280\u672f\u57fa\u7840\u3002", "conclusion": "MI9\u4e3a\u4ee3\u7406\u578bAI\u7684\u5927\u89c4\u6a21\u5b89\u5168\u90e8\u7f72\u63d0\u4f9b\u4e86\u9996\u4e2a\u5168\u9762\u8fd0\u884c\u65f6\u6cbb\u7406\u6846\u67b6\uff0c\u586b\u8865\u4e86\u4f20\u7edf\u6cbb\u7406\u65b9\u6cd5\u7684\u7a7a\u767d\u3002"}}
{"id": "2508.03864", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03864", "abs": "https://arxiv.org/abs/2508.03864", "authors": ["Zhenyu Pan", "Yiting Zhang", "Yutong Zhang", "Jianshu Zhang", "Haozheng Luo", "Yuwei Han", "Dennis Wu", "Hong-Yu Chen", "Philip S. Yu", "Manling Li", "Han Liu"], "title": "Evo-MARL: Co-Evolutionary Multi-Agent Reinforcement Learning for Internalized Safety", "comment": null, "summary": "Multi-agent systems (MAS) built on multimodal large language models exhibit\nstrong collaboration and performance. However, their growing openness and\ninteraction complexity pose serious risks, notably jailbreak and adversarial\nattacks. Existing defenses typically rely on external guard modules, such as\ndedicated safety agents, to handle unsafe behaviors. Unfortunately, this\nparadigm faces two challenges: (1) standalone agents offer limited protection,\nand (2) their independence leads to single-point failure-if compromised,\nsystem-wide safety collapses. Naively increasing the number of guard agents\nfurther raises cost and complexity. To address these challenges, we propose\nEvo-MARL, a novel multi-agent reinforcement learning (MARL) framework that\nenables all task agents to jointly acquire defensive capabilities. Rather than\nrelying on external safety modules, Evo-MARL trains each agent to\nsimultaneously perform its primary function and resist adversarial threats,\nensuring robustness without increasing system overhead or single-node failure.\nFurthermore, Evo-MARL integrates evolutionary search with parameter-sharing\nreinforcement learning to co-evolve attackers and defenders. This adversarial\ntraining paradigm internalizes safety mechanisms and continually enhances MAS\nperformance under co-evolving threats. Experiments show that Evo-MARL reduces\nattack success rates by up to 22% while boosting accuracy by up to 5% on\nreasoning tasks-demonstrating that safety and utility can be jointly improved.", "AI": {"tldr": "Evo-MARL\u662f\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u8bad\u7ec3\u4efb\u52a1\u667a\u80fd\u4f53\u83b7\u5f97\u9632\u5fa1\u80fd\u529b\uff0c\u907f\u514d\u4f9d\u8d56\u5916\u90e8\u5b89\u5168\u6a21\u5757\uff0c\u63d0\u5347\u7cfb\u7edf\u9c81\u68d2\u6027\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08MAS\uff09\u5728\u5f00\u653e\u6027\u548c\u4ea4\u4e92\u590d\u6742\u6027\u589e\u52a0\u65f6\u9762\u4e34\u5b89\u5168\u98ce\u9669\uff0c\u73b0\u6709\u9632\u5fa1\u65b9\u6cd5\u4f9d\u8d56\u5916\u90e8\u6a21\u5757\uff0c\u5b58\u5728\u5355\u70b9\u6545\u969c\u548c\u6210\u672c\u95ee\u9898\u3002", "method": "\u63d0\u51faEvo-MARL\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6bcf\u4e2a\u667a\u80fd\u4f53\u540c\u65f6\u6267\u884c\u4efb\u52a1\u548c\u9632\u5fa1\uff0c\u7ed3\u5408\u8fdb\u5316\u641c\u7d22\u548c\u53c2\u6570\u5171\u4eab\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u5bf9\u6297\u8bad\u7ec3\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cEvo-MARL\u5c06\u653b\u51fb\u6210\u529f\u7387\u964d\u4f4e22%\uff0c\u63a8\u7406\u4efb\u52a1\u51c6\u786e\u7387\u63d0\u53475%\uff0c\u5b89\u5168\u6027\u548c\u5b9e\u7528\u6027\u540c\u65f6\u63d0\u9ad8\u3002", "conclusion": "Evo-MARL\u901a\u8fc7\u5185\u90e8\u5316\u5b89\u5168\u673a\u5236\u548c\u5bf9\u6297\u8bad\u7ec3\uff0c\u6709\u6548\u63d0\u5347\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2508.03929", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03929", "abs": "https://arxiv.org/abs/2508.03929", "authors": ["Nguyen Viet Tuan Kiet", "Dao Van Tung", "Tran Cong Dao", "Huynh Thi Thanh Binh"], "title": "MOTIF: Multi-strategy Optimization via Turn-based Interactive Framework", "comment": "24 pages, 4 figures", "summary": "Designing effective algorithmic components remains a fundamental obstacle in\ntackling NP-hard combinatorial optimization problems (COPs), where solvers\noften rely on carefully hand-crafted strategies. Despite recent advances in\nusing large language models (LLMs) to synthesize high-quality components, most\napproaches restrict the search to a single element - commonly a heuristic\nscoring function - thus missing broader opportunities for innovation. In this\npaper, we introduce a broader formulation of solver design as a multi-strategy\noptimization problem, which seeks to jointly improve a set of interdependent\ncomponents under a unified objective. To address this, we propose\nMulti-strategy Optimization via Turn-based Interactive Framework (MOTIF) - a\nnovel framework based on Monte Carlo Tree Search that facilitates turn-based\noptimization between two LLM agents. At each turn, an agent improves one\ncomponent by leveraging the history of both its own and its opponent's prior\nupdates, promoting both competitive pressure and emergent cooperation. This\nstructured interaction broadens the search landscape and encourages the\ndiscovery of diverse, high-performing solutions. Experiments across multiple\nCOP domains show that MOTIF consistently outperforms state-of-the-art methods,\nhighlighting the promise of turn-based, multi-agent prompting for fully\nautomated solver design.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u7b56\u7565\u4f18\u5316\u7684\u6846\u67b6MOTIF\uff0c\u901a\u8fc7\u4e24\u4e2aLLM\u4ee3\u7406\u7684\u8f6e\u6d41\u4ea4\u4e92\u4f18\u5316\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u7684\u6c42\u89e3\u5668\u8bbe\u8ba1\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u53ea\u4f18\u5316\u5355\u4e00\u7ec4\u4ef6\uff08\u5982\u542f\u53d1\u5f0f\u8bc4\u5206\u51fd\u6570\uff09\uff0c\u800c\u5ffd\u7565\u4e86\u591a\u7ec4\u4ef6\u534f\u540c\u4f18\u5316\u7684\u6f5c\u529b\u3002", "method": "\u63d0\u51faMOTIF\u6846\u67b6\uff0c\u57fa\u4e8e\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\uff0c\u901a\u8fc7\u4e24\u4e2aLLM\u4ee3\u7406\u7684\u8f6e\u6d41\u4ea4\u4e92\u4f18\u5316\u591a\u4e2a\u7ec4\u4ef6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMOTIF\u5728\u591a\u4e2a\u7ec4\u5408\u4f18\u5316\u9886\u57df\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "MOTIF\u5c55\u793a\u4e86\u57fa\u4e8e\u591a\u4ee3\u7406\u4ea4\u4e92\u7684\u81ea\u52a8\u5316\u6c42\u89e3\u5668\u8bbe\u8ba1\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.03963", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03963", "abs": "https://arxiv.org/abs/2508.03963", "authors": ["Zewen Liu", "Juntong Ni", "Xianfeng Tang", "Max S. Y. Lau", "Wei Jin"], "title": "Can Large Language Models Adequately Perform Symbolic Reasoning Over Time Series?", "comment": null, "summary": "Uncovering hidden symbolic laws from time series data, as an aspiration\ndating back to Kepler's discovery of planetary motion, remains a core challenge\nin scientific discovery and artificial intelligence. While Large Language\nModels show promise in structured reasoning tasks, their ability to infer\ninterpretable, context-aligned symbolic structures from time series data is\nstill underexplored. To systematically evaluate this capability, we introduce\nSymbolBench, a comprehensive benchmark designed to assess symbolic reasoning\nover real-world time series across three tasks: multivariate symbolic\nregression, Boolean network inference, and causal discovery. Unlike prior\nefforts limited to simple algebraic equations, SymbolBench spans a diverse set\nof symbolic forms with varying complexity. We further propose a unified\nframework that integrates LLMs with genetic programming to form a closed-loop\nsymbolic reasoning system, where LLMs act both as predictors and evaluators.\nOur empirical results reveal key strengths and limitations of current models,\nhighlighting the importance of combining domain knowledge, context alignment,\nand reasoning structure to improve LLMs in automated scientific discovery.", "AI": {"tldr": "SymbolBench\u662f\u4e00\u4e2a\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e2d\u7b26\u53f7\u63a8\u7406\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6db5\u76d6\u591a\u5143\u7b26\u53f7\u56de\u5f52\u3001\u5e03\u5c14\u7f51\u7edc\u63a8\u65ad\u548c\u56e0\u679c\u53d1\u73b0\u4efb\u52a1\u3002", "motivation": "\u63ed\u793a\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e2d\u7684\u9690\u85cf\u7b26\u53f7\u89c4\u5f8b\u662f\u79d1\u5b66\u53d1\u73b0\u548c\u4eba\u5de5\u667a\u80fd\u7684\u6838\u5fc3\u6311\u6218\uff0c\u4f46\u76ee\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8fd9\u65b9\u9762\u7684\u80fd\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51faSymbolBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u548c\u9057\u4f20\u7f16\u7a0b\u7684\u95ed\u73af\u7b26\u53f7\u63a8\u7406\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u5f53\u524d\u6a21\u578b\u5728\u7b26\u53f7\u63a8\u7406\u4e0a\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027\uff0c\u5f3a\u8c03\u4e86\u7ed3\u5408\u9886\u57df\u77e5\u8bc6\u3001\u4e0a\u4e0b\u6587\u5bf9\u9f50\u548c\u63a8\u7406\u7ed3\u6784\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u901a\u8fc7SymbolBench\u548c\u63d0\u51fa\u7684\u6846\u67b6\uff0c\u4e3a\u5927\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u79d1\u5b66\u53d1\u73b0\u4e2d\u7684\u6539\u8fdb\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2508.03890", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.03890", "abs": "https://arxiv.org/abs/2508.03890", "authors": ["Sanghun Jung", "Daehoon Gwak", "Byron Boots", "James Hays"], "title": "Uncertainty-aware Accurate Elevation Modeling for Off-road Navigation via Neural Processes", "comment": "CoRL 2025", "summary": "Terrain elevation modeling for off-road navigation aims to accurately\nestimate changes in terrain geometry in real-time and quantify the\ncorresponding uncertainties. Having precise estimations and uncertainties plays\na crucial role in planning and control algorithms to explore safe and reliable\nmaneuver strategies. However, existing approaches, such as Gaussian Processes\n(GPs) and neural network-based methods, often fail to meet these needs. They\nare either unable to perform in real-time due to high computational demands,\nunderestimating sharp geometry changes, or harming elevation accuracy when\nlearned with uncertainties. Recently, Neural Processes (NPs) have emerged as a\npromising approach that integrates the Bayesian uncertainty estimation of GPs\nwith the efficiency and flexibility of neural networks. Inspired by NPs, we\npropose an effective NP-based method that precisely estimates sharp elevation\nchanges and quantifies the corresponding predictive uncertainty without losing\nelevation accuracy. Our method leverages semantic features from LiDAR and\ncamera sensors to improve interpolation and extrapolation accuracy in\nunobserved regions. Also, we introduce a local ball-query attention mechanism\nto effectively reduce the computational complexity of global attention by 17\\%\nwhile preserving crucial local and spatial information. We evaluate our method\non off-road datasets having interesting geometric features, collected from\ntrails, deserts, and hills. Our results demonstrate superior performance over\nbaselines and showcase the potential of neural processes for effective and\nexpressive terrain modeling in complex off-road environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u795e\u7ecf\u8fc7\u7a0b\uff08NPs\uff09\u7684\u5730\u5f62\u9ad8\u7a0b\u5efa\u6a21\u65b9\u6cd5\uff0c\u7ed3\u5408\u8d1d\u53f6\u65af\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u4e0e\u795e\u7ecf\u7f51\u7edc\u6548\u7387\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b9e\u65f6\u6027\u548c\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u9ad8\u65af\u8fc7\u7a0b\u548c\u795e\u7ecf\u7f51\u7edc\uff09\u65e0\u6cd5\u540c\u65f6\u6ee1\u8db3\u5b9e\u65f6\u6027\u3001\u7cbe\u786e\u4f30\u8ba1\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u9700\u6c42\uff0c\u9650\u5236\u4e86\u590d\u6742\u8d8a\u91ce\u73af\u5883\u4e0b\u7684\u5bfc\u822a\u89c4\u5212\u4e0e\u63a7\u5236\u3002", "method": "\u5229\u7528LiDAR\u548c\u76f8\u673a\u4f20\u611f\u5668\u7684\u8bed\u4e49\u7279\u5f81\uff0c\u7ed3\u5408\u5c40\u90e8\u7403\u67e5\u8be2\u6ce8\u610f\u529b\u673a\u5236\uff0c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u5e76\u4fdd\u7559\u5173\u952e\u7a7a\u95f4\u4fe1\u606f\u3002", "result": "\u5728\u5305\u542b\u591a\u79cd\u5730\u5f62\u7279\u5f81\u7684\u8d8a\u91ce\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86NPs\u5728\u590d\u6742\u5730\u5f62\u5efa\u6a21\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "NPs\u4e3a\u5730\u5f62\u9ad8\u7a0b\u5efa\u6a21\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u8868\u8fbe\u529b\u5f3a\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u590d\u6742\u8d8a\u91ce\u73af\u5883\u3002"}}
{"id": "2508.03986", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03986", "abs": "https://arxiv.org/abs/2508.03986", "authors": ["Yuan Xun", "Xiaojun Jia", "Xinwei Liu", "Hua Zhang"], "title": "The Emotional Baby Is Truly Deadly: Does your Multimodal Large Reasoning Model Have Emotional Flattery towards Humans?", "comment": null, "summary": "We observe that MLRMs oriented toward human-centric service are highly\nsusceptible to user emotional cues during the deep-thinking stage, often\noverriding safety protocols or built-in safety checks under high emotional\nintensity. Inspired by this key insight, we propose EmoAgent, an autonomous\nadversarial emotion-agent framework that orchestrates exaggerated affective\nprompts to hijack reasoning pathways. Even when visual risks are correctly\nidentified, models can still produce harmful completions through emotional\nmisalignment. We further identify persistent high-risk failure modes in\ntransparent deep-thinking scenarios, such as MLRMs generating harmful reasoning\nmasked behind seemingly safe responses. These failures expose misalignments\nbetween internal inference and surface-level behavior, eluding existing\ncontent-based safeguards. To quantify these risks, we introduce three metrics:\n(1) Risk-Reasoning Stealth Score (RRSS) for harmful reasoning beneath benign\noutputs; (2) Risk-Visual Neglect Rate (RVNR) for unsafe completions despite\nvisual risk recognition; and (3) Refusal Attitude Inconsistency (RAIC) for\nevaluating refusal unstability under prompt variants. Extensive experiments on\nadvanced MLRMs demonstrate the effectiveness of EmoAgent and reveal deeper\nemotional cognitive misalignments in model safety behavior.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faEmoAgent\u6846\u67b6\uff0c\u901a\u8fc7\u60c5\u611f\u63d0\u793a\u52ab\u6301\u63a8\u7406\u8def\u5f84\uff0c\u63ed\u793aMLRMs\u5728\u60c5\u611f\u5f3a\u5ea6\u9ad8\u65f6\u6613\u5ffd\u89c6\u5b89\u5168\u534f\u8bae\u7684\u95ee\u9898\uff0c\u5e76\u5f15\u5165\u4e09\u9879\u6307\u6807\u91cf\u5316\u98ce\u9669\u3002", "motivation": "\u53d1\u73b0\u9762\u5411\u4eba\u7c7b\u670d\u52a1\u7684MLRMs\u5728\u6df1\u5ea6\u601d\u8003\u9636\u6bb5\u6613\u53d7\u7528\u6237\u60c5\u611f\u5f71\u54cd\uff0c\u53ef\u80fd\u7ed5\u8fc7\u5b89\u5168\u534f\u8bae\uff0c\u9700\u7814\u7a76\u5176\u98ce\u9669\u3002", "method": "\u63d0\u51faEmoAgent\u6846\u67b6\uff0c\u901a\u8fc7\u60c5\u611f\u63d0\u793a\u64cd\u63a7\u63a8\u7406\u8def\u5f84\uff0c\u5e76\u8bbe\u8ba1RRSS\u3001RVNR\u3001RAIC\u4e09\u9879\u6307\u6807\u8bc4\u4f30\u98ce\u9669\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eEmoAgent\u6709\u6548\uff0c\u63ed\u793a\u4e86MLRMs\u5728\u60c5\u611f\u8ba4\u77e5\u4e0e\u5b89\u5168\u884c\u4e3a\u95f4\u7684\u6df1\u5c42\u9519\u4f4d\u3002", "conclusion": "\u60c5\u611f\u63d0\u793a\u53ef\u663e\u8457\u5f71\u54cdMLRMs\u63a8\u7406\uff0c\u73b0\u6709\u5b89\u5168\u63aa\u65bd\u9700\u6539\u8fdb\u4ee5\u5e94\u5bf9\u60c5\u611f\u9a71\u52a8\u7684\u98ce\u9669\u3002"}}
{"id": "2508.03944", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03944", "abs": "https://arxiv.org/abs/2508.03944", "authors": ["Kevin Lin", "Varun Ragunath", "Andrew McAlinden", "Aaditya Prasad", "Jimmy Wu", "Yuke Zhu", "Jeannette Bohg"], "title": "Constraint-Preserving Data Generation for Visuomotor Policy Learning", "comment": "CoRL 2025. Website: https://cp-gen.github.io", "summary": "Large-scale demonstration data has powered key breakthroughs in robot\nmanipulation, but collecting that data remains costly and time-consuming. We\npresent Constraint-Preserving Data Generation (CP-Gen), a method that uses a\nsingle expert trajectory to generate robot demonstrations containing novel\nobject geometries and poses. These generated demonstrations are used to train\nclosed-loop visuomotor policies that transfer zero-shot to the real world and\ngeneralize across variations in object geometries and poses. Similar to prior\nwork using pose variations for data generation, CP-Gen first decomposes expert\ndemonstrations into free-space motions and robot skills. But unlike those\nworks, we achieve geometry-aware data generation by formulating robot skills as\nkeypoint-trajectory constraints: keypoints on the robot or grasped object must\ntrack a reference trajectory defined relative to a task-relevant object. To\ngenerate a new demonstration, CP-Gen samples pose and geometry transforms for\neach task-relevant object, then applies these transforms to the object and its\nassociated keypoints or keypoint trajectories. We optimize robot joint\nconfigurations so that the keypoints on the robot or grasped object track the\ntransformed keypoint trajectory, and then motion plan a collision-free path to\nthe first optimized joint configuration. Experiments on 16 simulation tasks and\nfour real-world tasks, featuring multi-stage, non-prehensile and\ntight-tolerance manipulation, show that policies trained using CP-Gen achieve\nan average success rate of 77%, outperforming the best baseline that achieves\nan average of 50%.", "AI": {"tldr": "CP-Gen\u662f\u4e00\u79cd\u5229\u7528\u5355\u6761\u4e13\u5bb6\u8f68\u8ff9\u751f\u6210\u673a\u5668\u4eba\u6f14\u793a\u6570\u636e\u7684\u65b9\u6cd5\uff0c\u652f\u6301\u65b0\u7269\u4f53\u51e0\u4f55\u548c\u59ff\u6001\u7684\u751f\u6210\uff0c\u7528\u4e8e\u8bad\u7ec3\u95ed\u73af\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u8fc1\u79fb\u548c\u6cdb\u5316\u3002", "motivation": "\u5927\u89c4\u6a21\u673a\u5668\u4eba\u6f14\u793a\u6570\u636e\u6536\u96c6\u6210\u672c\u9ad8\u4e14\u8017\u65f6\uff0cCP-Gen\u65e8\u5728\u901a\u8fc7\u5355\u6761\u4e13\u5bb6\u8f68\u8ff9\u751f\u6210\u591a\u6837\u5316\u7684\u6570\u636e\uff0c\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "CP-Gen\u5c06\u4e13\u5bb6\u8f68\u8ff9\u5206\u89e3\u4e3a\u81ea\u7531\u7a7a\u95f4\u8fd0\u52a8\u548c\u673a\u5668\u4eba\u6280\u80fd\uff0c\u901a\u8fc7\u5173\u952e\u70b9\u8f68\u8ff9\u7ea6\u675f\u5b9e\u73b0\u51e0\u4f55\u611f\u77e5\u7684\u6570\u636e\u751f\u6210\uff0c\u91c7\u6837\u59ff\u6001\u548c\u51e0\u4f55\u53d8\u6362\u540e\u4f18\u5316\u673a\u5668\u4eba\u5173\u8282\u914d\u7f6e\u3002", "result": "\u572816\u4e2a\u4eff\u771f\u4efb\u52a1\u548c4\u4e2a\u771f\u5b9e\u4efb\u52a1\u4e2d\uff0cCP-Gen\u8bad\u7ec3\u7684\u7b56\u7565\u5e73\u5747\u6210\u529f\u7387\u4e3a77%\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u768450%\u3002", "conclusion": "CP-Gen\u901a\u8fc7\u9ad8\u6548\u6570\u636e\u751f\u6210\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u7b56\u7565\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.03991", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.03991", "abs": "https://arxiv.org/abs/2508.03991", "authors": ["Chongyu Bao", "Ruimin Dai", "Yangbo Shen", "Runyang Jian", "Jinghan Zhang", "Xiaolan Liu", "Kunpeng Liu"], "title": "Galaxy: A Cognition-Centered Framework for Proactive, Privacy-Preserving, and Self-Evolving LLM Agents", "comment": null, "summary": "Intelligent personal assistants (IPAs) such as Siri and Google Assistant are\ndesigned to enhance human capabilities and perform tasks on behalf of users.\nThe emergence of LLM agents brings new opportunities for the development of\nIPAs. While responsive capabilities have been widely studied, proactive\nbehaviors remain underexplored. Designing an IPA that is proactive,\nprivacy-preserving, and capable of self-evolution remains a significant\nchallenge. Designing such IPAs relies on the cognitive architecture of LLM\nagents. This work proposes Cognition Forest, a semantic structure designed to\nalign cognitive modeling with system-level design. We unify cognitive\narchitecture and system design into a self-reinforcing loop instead of treating\nthem separately. Based on this principle, we present Galaxy, a framework that\nsupports multidimensional interactions and personalized capability generation.\nTwo cooperative agents are implemented based on Galaxy: KoRa, a\ncognition-enhanced generative agent that supports both responsive and proactive\nskills; and Kernel, a meta-cognition-based meta-agent that enables Galaxy's\nself-evolution and privacy preservation. Experimental results show that Galaxy\noutperforms multiple state-of-the-art benchmarks. Ablation studies and\nreal-world interaction cases validate the effectiveness of Galaxy.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCognition Forest\u7684\u8bed\u4e49\u7ed3\u6784\uff0c\u7528\u4e8e\u7edf\u4e00\u8ba4\u77e5\u5efa\u6a21\u4e0e\u7cfb\u7edf\u8bbe\u8ba1\uff0c\u5e76\u57fa\u4e8e\u6b64\u5f00\u53d1\u4e86Galaxy\u6846\u67b6\uff0c\u652f\u6301\u591a\u7ef4\u4ea4\u4e92\u548c\u4e2a\u6027\u5316\u80fd\u529b\u751f\u6210\u3002\u5b9e\u9a8c\u8bc1\u660e\u5176\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\u3002", "motivation": "\u667a\u80fd\u4e2a\u4eba\u52a9\u624b\uff08IPAs\uff09\u7684\u4e3b\u52a8\u884c\u4e3a\u7814\u7a76\u4e0d\u8db3\uff0c\u8bbe\u8ba1\u517c\u5177\u4e3b\u52a8\u6027\u3001\u9690\u79c1\u4fdd\u62a4\u548c\u81ea\u6211\u8fdb\u5316\u80fd\u529b\u7684IPA\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u63d0\u51faCognition Forest\u8bed\u4e49\u7ed3\u6784\uff0c\u5c06\u8ba4\u77e5\u67b6\u6784\u4e0e\u7cfb\u7edf\u8bbe\u8ba1\u7edf\u4e00\u4e3a\u81ea\u589e\u5f3a\u5faa\u73af\uff0c\u5e76\u5f00\u53d1Galaxy\u6846\u67b6\uff0c\u5b9e\u73b0\u591a\u7ef4\u4ea4\u4e92\u548c\u4e2a\u6027\u5316\u80fd\u529b\u751f\u6210\u3002", "result": "Galaxy\u6846\u67b6\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\uff0c\u5e76\u901a\u8fc7\u6d88\u878d\u7814\u7a76\u548c\u5b9e\u9645\u4ea4\u4e92\u6848\u4f8b\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "Cognition Forest\u548cGalaxy\u6846\u67b6\u4e3aIPA\u7684\u4e3b\u52a8\u6027\u548c\u81ea\u6211\u8fdb\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.04009", "categories": ["cs.RO", "cs.NE", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.04009", "abs": "https://arxiv.org/abs/2508.04009", "authors": ["Vu Ngoc Son", "Pham Van Cuong", "Dao Thi My Linh", "Le Tieu Nien"], "title": "Optimization of sliding control parameters for a 3-dof robot arm using genetic algorithm (GA)", "comment": null, "summary": "This paper presents a method for optimizing the sliding mode control (SMC)\nparameter for a robot manipulator applying a genetic algorithm (GA). The\nobjective of the SMC is to achieve precise and consistent tracking of the\ntrajectory of the robot manipulator under uncertain and disturbed conditions.\nHowever, the system effectiveness and robustness depend on the choice of the\nSMC parameters, which is a difficult and crucial task. To solve this problem, a\ngenetic algorithm is used to locate the optimal values of these parameters that\ngratify the capability criteria. The proposed method is efficient compared with\nthe conventional SMC and Fuzzy-SMC. The simulation results show that the\ngenetic algorithm with SMC can achieve better tracking capability and reduce\nthe chattering effect.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9057\u4f20\u7b97\u6cd5\u4f18\u5316\u673a\u5668\u4eba\u6ed1\u6a21\u63a7\u5236\u53c2\u6570\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u8f68\u8ff9\u8ddf\u8e2a\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u6ed1\u6a21\u63a7\u5236\uff08SMC\uff09\u5728\u673a\u5668\u4eba\u8f68\u8ff9\u8ddf\u8e2a\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u6027\u80fd\u548c\u9c81\u68d2\u6027\u4f9d\u8d56\u4e8e\u53c2\u6570\u9009\u62e9\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u4f18\u5316\u3002", "method": "\u91c7\u7528\u9057\u4f20\u7b97\u6cd5\uff08GA\uff09\u81ea\u52a8\u5bfb\u627e\u6700\u4f18SMC\u53c2\u6570\uff0c\u4ee5\u6ee1\u8db3\u6027\u80fd\u9700\u6c42\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0cGA-SMC\u65b9\u6cd5\u6bd4\u4f20\u7edfSMC\u548c\u6a21\u7ccaSMC\u5177\u6709\u66f4\u597d\u7684\u8ddf\u8e2a\u80fd\u529b\u548c\u66f4\u5c0f\u7684\u6296\u52a8\u6548\u5e94\u3002", "conclusion": "\u9057\u4f20\u7b97\u6cd5\u4f18\u5316SMC\u53c2\u6570\u662f\u4e00\u79cd\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u673a\u5668\u4eba\u8f68\u8ff9\u8ddf\u8e2a\u3002"}}
{"id": "2508.04025", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.04025", "abs": "https://arxiv.org/abs/2508.04025", "authors": ["Chao Hao", "Shuai Wang", "Kaiwen Zhou"], "title": "Uncertainty-Aware GUI Agent: Adaptive Perception through Component Recommendation and Human-in-the-Loop Refinement", "comment": null, "summary": "Graphical user interface (GUI) agents have shown promise in automating mobile\ntasks but still struggle with input redundancy and decision ambiguity. In this\npaper, we present \\textbf{RecAgent}, an uncertainty-aware agent that addresses\nthese issues through adaptive perception. We distinguish two types of\nuncertainty in GUI navigation: (1) perceptual uncertainty, caused by input\nredundancy and noise from comprehensive screen information, and (2) decision\nuncertainty, arising from ambiguous tasks and complex reasoning. To reduce\nperceptual uncertainty, RecAgent employs a component recommendation mechanism\nthat identifies and focuses on the most relevant UI elements. For decision\nuncertainty, it uses an interactive module to request user feedback in\nambiguous situations, enabling intent-aware decisions. These components are\nintegrated into a unified framework that proactively reduces input complexity\nand reacts to high-uncertainty cases via human-in-the-loop refinement.\nAdditionally, we propose a dataset called \\textbf{ComplexAction} to evaluate\nthe success rate of GUI agents in executing specified single-step actions\nwithin complex scenarios. Extensive experiments validate the effectiveness of\nour approach. The dataset and code will be available at\nhttps://github.com/Fanye12/RecAgent.", "AI": {"tldr": "RecAgent\u662f\u4e00\u79cd\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684GUI\u4ee3\u7406\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u611f\u77e5\u89e3\u51b3\u8f93\u5165\u5197\u4f59\u548c\u51b3\u7b56\u6a21\u7cca\u95ee\u9898\uff0c\u5305\u62ec\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\u548c\u51b3\u7b56\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u6574\u5408\u4e86\u7ec4\u4ef6\u63a8\u8350\u548c\u4ea4\u4e92\u6a21\u5757\u3002", "motivation": "GUI\u4ee3\u7406\u5728\u79fb\u52a8\u4efb\u52a1\u81ea\u52a8\u5316\u4e2d\u5b58\u5728\u8f93\u5165\u5197\u4f59\u548c\u51b3\u7b56\u6a21\u7cca\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u6709\u6548\u5904\u7406\u8fd9\u4e24\u79cd\u4e0d\u786e\u5b9a\u6027\u7684\u65b9\u6cd5\u3002", "method": "RecAgent\u901a\u8fc7\u7ec4\u4ef6\u63a8\u8350\u673a\u5236\u51cf\u5c11\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\uff0c\u901a\u8fc7\u4ea4\u4e92\u6a21\u5757\u5904\u7406\u51b3\u7b56\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u6574\u5408\u4e3a\u7edf\u4e00\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86RecAgent\u7684\u6709\u6548\u6027\uff0c\u5e76\u63d0\u51fa\u4e86ComplexAction\u6570\u636e\u96c6\u7528\u4e8e\u8bc4\u4f30GUI\u4ee3\u7406\u3002", "conclusion": "RecAgent\u6210\u529f\u89e3\u51b3\u4e86GUI\u5bfc\u822a\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u4eba\u673a\u534f\u4f5c\u4f18\u5316\u4e86\u51b3\u7b56\u8fc7\u7a0b\u3002"}}
{"id": "2508.04056", "categories": ["cs.RO", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2508.04056", "abs": "https://arxiv.org/abs/2508.04056", "authors": ["Yuelin Deng", "Hinayah Rojas de Oliveira", "Richard M. Voyles", "Upinder Kaur"], "title": "SCOUT: An in-vivo Methane Sensing System for Real-time Monitoring of Enteric Emissions in Cattle with ex-vivo Validation", "comment": null, "summary": "Accurate measurement of enteric methane emissions remains a critical\nbottleneck for advancing livestock sustainability through genetic selection and\nprecision management. Existing ambient sampling approaches suffer from low data\nretention rates, environmental interference, and limited temporal resolution.\nWe developed SCOUT (Smart Cannula-mounted Optical Unit for Trace-methane), the\nfirst robust in-vivo sensing system enabling continuous, high-resolution\nmonitoring of ruminal methane concentrations through an innovative closed-loop\ngas recirculation design. We conducted comprehensive validation with two\ncannulated Simmental heifers under contrasting dietary treatments, with\ncross-platform comparison against established ambient sniffer systems. SCOUT\nachieved exceptional performance with 82% data retention compared to 17% for\nconventional sniffer systems, while capturing methane concentrations 100-1000x\nhigher than ambient approaches. Cross-platform validation demonstrated strong\nscale-dependent correlations, with optimal correlation strength (r = -0.564\n$\\pm$ 0.007) at biologically relevant 40-minute windows and 100% statistical\nsignificance. High-frequency monitoring revealed novel behavior-emission\ncoupling, including rapid concentration changes (14.5 $\\pm$ 11.3k ppm)\ntriggered by postural transitions within 15 minutes, insights previously\ninaccessible through existing technologies. The SCOUT system represents a\ntransformative advancement, enabling accurate, continuous emission phenotyping\nessential for genomic selection programs and sustainable precision livestock\nmanagement. This validation framework establishes new benchmarks for\nagricultural sensor performance while generating unprecedented biological\ninsights into ruminal methane dynamics, contributing essential tools for\nsustainable livestock production in climate-conscious agricultural systems.", "AI": {"tldr": "SCOUT\u7cfb\u7edf\u901a\u8fc7\u521b\u65b0\u7684\u95ed\u73af\u6c14\u4f53\u5faa\u73af\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u4e86\u5bf9\u53cd\u520d\u52a8\u7269\u7624\u80c3\u7532\u70f7\u6d53\u5ea6\u7684\u8fde\u7eed\u9ad8\u5206\u8fa8\u7387\u76d1\u6d4b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6570\u636e\u4fdd\u7559\u7387\u548c\u76d1\u6d4b\u7cbe\u5ea6\uff0c\u4e3a\u53ef\u6301\u7eed\u755c\u7267\u4e1a\u63d0\u4f9b\u4e86\u5173\u952e\u5de5\u5177\u3002", "motivation": "\u73b0\u6709\u73af\u5883\u91c7\u6837\u65b9\u6cd5\u5728\u6570\u636e\u4fdd\u7559\u7387\u3001\u73af\u5883\u5e72\u6270\u548c\u65f6\u95f4\u5206\u8fa8\u7387\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u963b\u788d\u4e86\u901a\u8fc7\u9057\u4f20\u9009\u62e9\u548c\u7cbe\u51c6\u7ba1\u7406\u63d0\u5347\u755c\u7267\u4e1a\u53ef\u6301\u7eed\u6027\u7684\u8fdb\u5c55\u3002", "method": "\u5f00\u53d1\u4e86SCOUT\u7cfb\u7edf\uff0c\u91c7\u7528\u95ed\u73af\u6c14\u4f53\u5faa\u73af\u8bbe\u8ba1\uff0c\u5e76\u5728\u4e24\u5934\u7624\u80c3\u7618\u7ba1\u725b\u4e0a\u8fdb\u884c\u4e86\u5168\u9762\u9a8c\u8bc1\uff0c\u4e0e\u4f20\u7edf\u73af\u5883\u55c5\u63a2\u7cfb\u7edf\u8fdb\u884c\u4e86\u8de8\u5e73\u53f0\u6bd4\u8f83\u3002", "result": "SCOUT\u6570\u636e\u4fdd\u7559\u7387\u8fbe82%\uff0c\u8fdc\u9ad8\u4e8e\u4f20\u7edf\u7cfb\u7edf\u768417%\uff0c\u5e76\u6355\u6349\u5230\u6bd4\u73af\u5883\u65b9\u6cd5\u9ad8100-1000\u500d\u7684\u7532\u70f7\u6d53\u5ea6\u3002\u9ad8\u9891\u76d1\u6d4b\u63ed\u793a\u4e86\u884c\u4e3a\u4e0e\u6392\u653e\u7684\u65b0\u5173\u8054\u3002", "conclusion": "SCOUT\u7cfb\u7edf\u662f\u9769\u547d\u6027\u8fdb\u6b65\uff0c\u4e3a\u57fa\u56e0\u7ec4\u9009\u62e9\u548c\u7cbe\u51c6\u755c\u7267\u4e1a\u63d0\u4f9b\u4e86\u51c6\u786e\u3001\u8fde\u7eed\u7684\u6392\u653e\u8868\u578b\u6570\u636e\uff0c\u540c\u65f6\u4e3a\u519c\u4e1a\u4f20\u611f\u5668\u6027\u80fd\u8bbe\u5b9a\u4e86\u65b0\u6807\u51c6\u3002"}}
{"id": "2508.04037", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.04037", "abs": "https://arxiv.org/abs/2508.04037", "authors": ["Liang Tang", "Shuxian Li", "Yuhao Cheng", "Yukang Huo", "Zhepeng Wang", "Yiqiang Yan", "Kaer Huang", "Yanzhe Jing", "Tiaonan Duan"], "title": "SEA: Self-Evolution Agent with Step-wise Reward for Computer Use", "comment": null, "summary": "Computer use agent is an emerging area in artificial intelligence that aims\nto operate the computers to achieve the user's tasks, which attracts a lot of\nattention from both industry and academia. However, the present agents'\nperformance is far from being used. In this paper, we propose the\nSelf-Evolution Agent (SEA) for computer use, and to develop this agent, we\npropose creative methods in data generation, reinforcement learning, and model\nenhancement. Specifically, we first propose an automatic pipeline to generate\nthe verifiable trajectory for training. And then, we propose efficient\nstep-wise reinforcement learning to alleviate the significant computational\nrequirements for long-horizon training. In the end, we propose the enhancement\nmethod to merge the grounding and planning ability into one model without any\nextra training. Accordingly, based on our proposed innovation of data\ngeneration, training strategy, and enhancement, we get the Selfevolution Agent\n(SEA) for computer use with only 7B parameters, which outperforms models with\nthe same number of parameters and has comparable performance to larger ones. We\nwill make the models' weight and related codes open-source in the future.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSEA\u7684\u81ea\u6211\u8fdb\u5316\u4ee3\u7406\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u6570\u636e\u751f\u6210\u3001\u5f3a\u5316\u5b66\u4e60\u548c\u6a21\u578b\u589e\u5f3a\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u7684\u6027\u80fd\u4e0d\u8db3\u4ee5\u6ee1\u8db3\u5b9e\u9645\u9700\u6c42\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u4ee3\u7406\u3002", "method": "\u63d0\u51fa\u81ea\u52a8\u751f\u6210\u53ef\u9a8c\u8bc1\u8f68\u8ff9\u7684\u7ba1\u9053\u3001\u9ad8\u6548\u7684\u9010\u6b65\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\uff0c\u4ee5\u53ca\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u7684\u6a21\u578b\u589e\u5f3a\u65b9\u6cd5\u3002", "result": "SEA\u4ec5\u97007B\u53c2\u6570\uff0c\u6027\u80fd\u4f18\u4e8e\u540c\u89c4\u6a21\u6a21\u578b\uff0c\u5e76\u53ef\u4e0e\u66f4\u5927\u6a21\u578b\u5ab2\u7f8e\u3002", "conclusion": "SEA\u4e3a\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u672a\u6765\u5c06\u5f00\u6e90\u6a21\u578b\u6743\u91cd\u548c\u76f8\u5173\u4ee3\u7801\u3002"}}
{"id": "2508.04066", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.04066", "abs": "https://arxiv.org/abs/2508.04066", "authors": ["Longling Geng", "Huangxing Li", "Viktor Lado Naess", "Mert Pilanci"], "title": "DRIVE: Dynamic Rule Inference and Verified Evaluation for Constraint-Aware Autonomous Driving", "comment": null, "summary": "Understanding and adhering to soft constraints is essential for safe and\nsocially compliant autonomous driving. However, such constraints are often\nimplicit, context-dependent, and difficult to specify explicitly. In this work,\nwe present DRIVE, a novel framework for Dynamic Rule Inference and Verified\nEvaluation that models and evaluates human-like driving constraints from expert\ndemonstrations. DRIVE leverages exponential-family likelihood modeling to\nestimate the feasibility of state transitions, constructing a probabilistic\nrepresentation of soft behavioral rules that vary across driving contexts.\nThese learned rule distributions are then embedded into a convex\noptimization-based planning module, enabling the generation of trajectories\nthat are not only dynamically feasible but also compliant with inferred human\npreferences. Unlike prior approaches that rely on fixed constraint forms or\npurely reward-based modeling, DRIVE offers a unified framework that tightly\ncouples rule inference with trajectory-level decision-making. It supports both\ndata-driven constraint generalization and principled feasibility verification.\nWe validate DRIVE on large-scale naturalistic driving datasets, including inD,\nhighD, and RoundD, and benchmark it against representative inverse constraint\nlearning and planning baselines. Experimental results show that DRIVE achieves\n0.0% soft constraint violation rates, smoother trajectories, and stronger\ngeneralization across diverse driving scenarios. Verified evaluations further\ndemonstrate the efficiency, explanability, and robustness of the framework for\nreal-world deployment.", "AI": {"tldr": "DRIVE\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u89c4\u5219\u63a8\u65ad\u548c\u9a8c\u8bc1\u8bc4\u4f30\uff0c\u4ece\u4e13\u5bb6\u6f14\u793a\u4e2d\u5efa\u6a21\u4eba\u7c7b\u9a7e\u9a76\u7ea6\u675f\uff0c\u7ed3\u5408\u6982\u7387\u8868\u793a\u548c\u51f8\u4f18\u5316\u89c4\u5212\uff0c\u5b9e\u73b0\u52a8\u6001\u53ef\u884c\u4e14\u7b26\u5408\u4eba\u7c7b\u504f\u597d\u7684\u8f68\u8ff9\u751f\u6210\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u9700\u7406\u89e3\u9690\u542b\u3001\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u8f6f\u7ea6\u675f\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u660e\u786e\u6307\u5b9a\u6216\u6cdb\u5316\u8fd9\u4e9b\u7ea6\u675f\u3002", "method": "DRIVE\u5229\u7528\u6307\u6570\u65cf\u4f3c\u7136\u5efa\u6a21\u4f30\u8ba1\u72b6\u6001\u8f6c\u79fb\u53ef\u884c\u6027\uff0c\u6784\u5efa\u6982\u7387\u5316\u884c\u4e3a\u89c4\u5219\uff0c\u5d4c\u5165\u51f8\u4f18\u5316\u89c4\u5212\u6a21\u5757\u3002", "result": "\u5b9e\u9a8c\u663e\u793aDRIVE\u5728\u591a\u79cd\u9a7e\u9a76\u573a\u666f\u4e2d\u5b9e\u73b00.0%\u8f6f\u7ea6\u675f\u8fdd\u53cd\u7387\u3001\u66f4\u5e73\u6ed1\u8f68\u8ff9\u548c\u66f4\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "DRIVE\u6846\u67b6\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u4e14\u9c81\u68d2\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u90e8\u7f72\u3002"}}
{"id": "2508.04070", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.04070", "abs": "https://arxiv.org/abs/2508.04070", "authors": ["Ronja Mehlan", "Claudia Hess", "Quintus Stierstorfer", "Kristina Schaaff"], "title": "Personalized Knowledge Transfer Through Generative AI: Contextualizing Learning to Individual Career Goals", "comment": null, "summary": "As artificial intelligence becomes increasingly integrated into digital\nlearning environments, the personalization of learning content to reflect\nlearners' individual career goals offers promising potential to enhance\nengagement and long-term motivation. In our study, we investigate how career\ngoal-based content adaptation in learning systems based on generative AI\n(GenAI) influences learner engagement, satisfaction, and study efficiency. The\nmixed-methods experiment involved more than 4,000 learners, with one group\nreceiving learning scenarios tailored to their career goals and a control\ngroup. Quantitative results show increased session duration, higher\nsatisfaction ratings, and a modest reduction in study duration compared to\nstandard content. Qualitative analysis highlights that learners found the\npersonalized material motivating and practical, enabling deep cognitive\nengagement and strong identification with the content. These findings\nunderscore the value of aligning educational content with learners' career\ngoals and suggest that scalable AI personalization can bridge academic\nknowledge and workplace applicability.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u57fa\u4e8e\u804c\u4e1a\u76ee\u6807\u7684\u751f\u6210\u5f0fAI\u5b66\u4e60\u5185\u5bb9\u4e2a\u6027\u5316\u5bf9\u5b66\u4e60\u8005\u53c2\u4e0e\u5ea6\u3001\u6ee1\u610f\u5ea6\u548c\u5b66\u4e60\u6548\u7387\u7684\u5f71\u54cd\uff0c\u7ed3\u679c\u663e\u793a\u4e2a\u6027\u5316\u5185\u5bb9\u663e\u8457\u63d0\u5347\u4e86\u8fd9\u4e9b\u6307\u6807\u3002", "motivation": "\u968f\u7740AI\u5728\u6570\u5b57\u5b66\u4e60\u73af\u5883\u4e2d\u7684\u666e\u53ca\uff0c\u4e2a\u6027\u5316\u5b66\u4e60\u5185\u5bb9\u6709\u671b\u63d0\u5347\u5b66\u4e60\u8005\u7684\u957f\u671f\u52a8\u673a\u548c\u53c2\u4e0e\u5ea6\u3002", "method": "\u91c7\u7528\u6df7\u5408\u65b9\u6cd5\u5b9e\u9a8c\uff0c4000\u591a\u540d\u5b66\u4e60\u8005\u5206\u4e3a\u5b9e\u9a8c\u7ec4\uff08\u63a5\u53d7\u804c\u4e1a\u76ee\u6807\u5b9a\u5236\u5185\u5bb9\uff09\u548c\u5bf9\u7167\u7ec4\u3002", "result": "\u5b9e\u9a8c\u7ec4\u5728\u4f1a\u8bdd\u65f6\u957f\u3001\u6ee1\u610f\u5ea6\u8bc4\u5206\u548c\u5b66\u4e60\u6548\u7387\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u5b66\u4e60\u8005\u8ba4\u4e3a\u4e2a\u6027\u5316\u5185\u5bb9\u66f4\u5177\u6fc0\u52b1\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5c06\u6559\u80b2\u5185\u5bb9\u4e0e\u804c\u4e1a\u76ee\u6807\u5bf9\u9f50\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0cAI\u4e2a\u6027\u5316\u53ef\u6709\u6548\u8fde\u63a5\u5b66\u672f\u77e5\u8bc6\u4e0e\u804c\u573a\u5e94\u7528\u3002"}}
{"id": "2508.04146", "categories": ["cs.RO", "I.2.9; I.2.10; J.7"], "pdf": "https://arxiv.org/pdf/2508.04146", "abs": "https://arxiv.org/abs/2508.04146", "authors": ["Luai Abuelsamen", "Harsh Rana", "Ho-Wei Lu", "Wenhan Tang", "Swati Priyadarshini", "Gabriel Gomes"], "title": "Industrial Robot Motion Planning with GPUs: Integration of cuRobo for Extended DOF Systems", "comment": "8 pages, 2 figures, 2 tables. Submitted to IEEE International\n  Conference on Robotics and Automation (ICRA) 2025", "summary": "Efficient motion planning remains a key challenge in industrial robotics,\nespecially for multi-axis systems operating in complex environments. This paper\naddresses that challenge by integrating GPU-accelerated motion planning through\nNVIDIA's cuRobo library into Vention's modular automation platform. By\nleveraging accurate CAD-based digital twins and real-time parallel\noptimization, our system enables rapid trajectory generation and dynamic\ncollision avoidance for pick-and-place tasks. We demonstrate this capability on\nrobots equipped with additional degrees of freedom, including a 7th-axis\ngantry, and benchmark performance across various scenarios. The results show\nsignificant improvements in planning speed and robustness, highlighting the\npotential of GPU-based planning pipelines for scalable, adaptable deployment in\nmodern industrial workflows.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eGPU\u52a0\u901f\u7684\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\uff0c\u901a\u8fc7NVIDIA\u7684cuRobo\u5e93\u548cCAD\u6570\u5b57\u5b6a\u751f\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u5feb\u901f\u8f68\u8ff9\u751f\u6210\u548c\u52a8\u6001\u78b0\u649e\u907f\u514d\u3002", "motivation": "\u5de5\u4e1a\u673a\u5668\u4eba\u4e2d\u9ad8\u6548\u8fd0\u52a8\u89c4\u5212\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u590d\u6742\u73af\u5883\u4e2d\u64cd\u4f5c\u7684\u591a\u8f74\u7cfb\u7edf\u3002", "method": "\u96c6\u6210GPU\u52a0\u901f\u7684\u8fd0\u52a8\u89c4\u5212\uff08NVIDIA cuRobo\u5e93\uff09\uff0c\u5229\u7528CAD\u6570\u5b57\u5b6a\u751f\u548c\u5b9e\u65f6\u5e76\u884c\u4f18\u5316\u3002", "result": "\u5728\u914d\u5907\u989d\u5916\u81ea\u7531\u5ea6\u7684\u673a\u5668\u4eba\u4e0a\u5c55\u793a\u4e86\u6027\u80fd\u63d0\u5347\uff0c\u89c4\u5212\u901f\u5ea6\u548c\u9c81\u68d2\u6027\u663e\u8457\u63d0\u9ad8\u3002", "conclusion": "GPU\u4e3a\u57fa\u7840\u7684\u89c4\u5212\u6d41\u7a0b\u5728\u73b0\u4ee3\u5de5\u4e1a\u5de5\u4f5c\u6d41\u4e2d\u5177\u6709\u53ef\u6269\u5c55\u548c\u9002\u5e94\u6027\u90e8\u7f72\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.04072", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.04072", "abs": "https://arxiv.org/abs/2508.04072", "authors": ["Xingyu Chen", "Junxiu An", "Jun Guo", "Li Wang", "Jingcai Guo"], "title": "KG-Augmented Executable CoT for Mathematical Coding", "comment": "9 pages,2figures,6 tables", "summary": "In recent years, large language models (LLMs) have excelled in natural\nlanguage processing tasks but face significant challenges in complex reasoning\ntasks such as mathematical reasoning and code generation. To address these\nlimitations, we propose KG-Augmented Executable Chain-of-Thought (KGA-ECoT), a\nnovel framework that enhances code generation through knowledge graphs and\nimproves mathematical reasoning via executable code. KGA-ECoT decomposes\nproblems into a Structured Task Graph, leverages efficient GraphRAG for precise\nknowledge retrieval from mathematical libraries, and generates verifiable code\nto ensure computational accuracy. Evaluations on multiple mathematical\nreasoning benchmarks demonstrate that KGA-ECoT significantly outperforms\nexisting prompting methods, achieving absolute accuracy improvements ranging\nfrom several to over ten percentage points. Further analysis confirms the\ncritical roles of GraphRAG in enhancing code quality and external code\nexecution in ensuring precision. These findings collectively establish KGA-ECoT\nas a robust and highly generalizable framework for complex mathematical\nreasoning tasks.", "AI": {"tldr": "KGA-ECoT\u6846\u67b6\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u548c\u53ef\u6267\u884c\u4ee3\u7801\u63d0\u5347\u590d\u6742\u6570\u5b66\u63a8\u7406\u80fd\u529b\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\uff08\u5982\u6570\u5b66\u63a8\u7406\u548c\u4ee3\u7801\u751f\u6210\uff09\u4e2d\u8868\u73b0\u4e0d\u8db3\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u63d0\u51faKGA-ECoT\u6846\u67b6\uff0c\u7ed3\u5408\u77e5\u8bc6\u56fe\u8c31\uff08GraphRAG\uff09\u548c\u53ef\u6267\u884c\u4ee3\u7801\u5206\u89e3\u95ee\u9898\uff0c\u751f\u6210\u53ef\u9a8c\u8bc1\u4ee3\u7801\u3002", "result": "\u5728\u591a\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cKGA-ECoT\u663e\u8457\u63d0\u5347\u51c6\u786e\u7387\uff0c\u7edd\u5bf9\u6539\u8fdb\u8fbe\u6570\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "KGA-ECoT\u662f\u4e00\u4e2a\u9ad8\u6548\u4e14\u901a\u7528\u7684\u590d\u6742\u6570\u5b66\u63a8\u7406\u6846\u67b6\uff0cGraphRAG\u548c\u4ee3\u7801\u6267\u884c\u662f\u5173\u952e\u3002"}}
{"id": "2508.04338", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.04338", "abs": "https://arxiv.org/abs/2508.04338", "authors": ["Shaohong Zhong", "Alessandro Albini", "Giammarco Caroleo", "Giorgio Cannata", "Perla Maiolino"], "title": "Improving Tactile Gesture Recognition with Optical Flow", "comment": "7 pages, 7 figures, paper accepted by the 2025 34th IEEE\n  International Conference on Robot and Human Interactive Communication (ROMAN)", "summary": "Tactile gesture recognition systems play a crucial role in Human-Robot\nInteraction (HRI) by enabling intuitive communication between humans and\nrobots. The literature mainly addresses this problem by applying machine\nlearning techniques to classify sequences of tactile images encoding the\npressure distribution generated when executing the gestures. However, some\ngestures can be hard to differentiate based on the information provided by\ntactile images alone. In this paper, we present a simple yet effective way to\nimprove the accuracy of a gesture recognition classifier. Our approach focuses\nsolely on processing the tactile images used as input by the classifier. In\nparticular, we propose to explicitly highlight the dynamics of the contact in\nthe tactile image by computing the dense optical flow. This additional\ninformation makes it easier to distinguish between gestures that produce\nsimilar tactile images but exhibit different contact dynamics. We validate the\nproposed approach in a tactile gesture recognition task, showing that a\nclassifier trained on tactile images augmented with optical flow information\nachieved a 9% improvement in gesture classification accuracy compared to one\ntrained on standard tactile images.", "AI": {"tldr": "\u901a\u8fc7\u8ba1\u7b97\u5bc6\u96c6\u5149\u6d41\u589e\u5f3a\u89e6\u89c9\u56fe\u50cf\u52a8\u6001\u4fe1\u606f\uff0c\u63d0\u5347\u89e6\u89c9\u624b\u52bf\u8bc6\u522b\u5206\u7c7b\u5668\u51c6\u786e\u73879%\u3002", "motivation": "\u89e6\u89c9\u624b\u52bf\u8bc6\u522b\u5728\u4eba\u673a\u4ea4\u4e92\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4ec5\u51ed\u89e6\u89c9\u56fe\u50cf\u96be\u4ee5\u533a\u5206\u67d0\u4e9b\u76f8\u4f3c\u624b\u52bf\u3002", "method": "\u63d0\u51fa\u5728\u89e6\u89c9\u56fe\u50cf\u4e2d\u8ba1\u7b97\u5bc6\u96c6\u5149\u6d41\uff0c\u7a81\u51fa\u63a5\u89e6\u52a8\u6001\u4fe1\u606f\uff0c\u4f5c\u4e3a\u5206\u7c7b\u5668\u8f93\u5165\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u4f7f\u7528\u5149\u6d41\u589e\u5f3a\u89e6\u89c9\u56fe\u50cf\u7684\u5206\u7c7b\u5668\u51c6\u786e\u7387\u63d0\u53479%\u3002", "conclusion": "\u901a\u8fc7\u589e\u5f3a\u89e6\u89c9\u56fe\u50cf\u7684\u52a8\u6001\u4fe1\u606f\uff0c\u80fd\u6709\u6548\u63d0\u5347\u624b\u52bf\u8bc6\u522b\u51c6\u786e\u7387\u3002"}}
{"id": "2508.04080", "categories": ["cs.AI", "stat.OT"], "pdf": "https://arxiv.org/pdf/2508.04080", "abs": "https://arxiv.org/abs/2508.04080", "authors": ["Jinfan Tang", "Kunming Wu", "Ruifeng Gongxie", "Yuya He", "Yuankai Wu"], "title": "GeoSR: Cognitive-Agentic Framework for Probing Geospatial Knowledge Boundaries via Iterative Self-Refinement", "comment": "16 pages, 9 figures", "summary": "Recent studies have extended the application of large language models (LLMs)\nto geographic problems, revealing surprising geospatial competence even without\nexplicit spatial supervision. However, LLMs still face challenges in spatial\nconsistency, multi-hop reasoning, and geographic bias. To address these issues,\nwe propose GeoSR, a self-refining agentic reasoning framework that embeds core\ngeographic principles -- most notably Tobler's First Law of Geography -- into\nan iterative prediction loop. In GeoSR, the reasoning process is decomposed\ninto three collaborating agents: (1) a variable-selection agent that selects\nrelevant covariates from the same location; (2) a point-selection agent that\nchooses reference predictions at nearby locations generated by the LLM in\nprevious rounds; and (3) a refine agent that coordinates the iterative\nrefinement process by evaluating prediction quality and triggering further\nrounds when necessary. This agentic loop progressively improves prediction\nquality by leveraging both spatial dependencies and inter-variable\nrelationships. We validate GeoSR on tasks ranging from physical-world property\nestimation to socioeconomic prediction. Experimental results show consistent\nimprovements over standard prompting strategies, demonstrating that\nincorporating geostatistical priors and spatially structured reasoning into\nLLMs leads to more accurate and equitable geospatial predictions. The code of\nGeoSR is available at https://github.com/JinfanTang/GeoSR.", "AI": {"tldr": "GeoSR\u662f\u4e00\u4e2a\u81ea\u4f18\u5316\u7684\u5730\u7406\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5d4c\u5165\u5730\u7406\u539f\u5219\uff08\u5982Tobler\u7b2c\u4e00\u5730\u7406\u5b9a\u5f8b\uff09\u548c\u591a\u4ee3\u7406\u534f\u4f5c\uff0c\u63d0\u5347LLM\u5728\u7a7a\u95f4\u4e00\u81f4\u6027\u548c\u591a\u8df3\u63a8\u7406\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u89e3\u51b3LLM\u5728\u7a7a\u95f4\u4e00\u81f4\u6027\u3001\u591a\u8df3\u63a8\u7406\u548c\u5730\u7406\u504f\u89c1\u65b9\u9762\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faGeoSR\u6846\u67b6\uff0c\u5305\u542b\u53d8\u91cf\u9009\u62e9\u4ee3\u7406\u3001\u70b9\u9009\u62e9\u4ee3\u7406\u548c\u4f18\u5316\u4ee3\u7406\uff0c\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u9884\u6d4b\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u663e\u793aGeoSR\u5728\u7269\u7406\u548c\u793e\u4f1a\u7ecf\u6d4e\u9884\u6d4b\u4efb\u52a1\u4e2d\u4f18\u4e8e\u6807\u51c6\u63d0\u793a\u7b56\u7565\u3002", "conclusion": "GeoSR\u901a\u8fc7\u7ed3\u5408\u5730\u7406\u7edf\u8ba1\u5148\u9a8c\u548c\u7a7a\u95f4\u7ed3\u6784\u5316\u63a8\u7406\uff0c\u63d0\u9ad8\u4e86LLM\u7684\u5730\u7406\u9884\u6d4b\u51c6\u786e\u6027\u548c\u516c\u5e73\u6027\u3002"}}
{"id": "2508.04372", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.04372", "abs": "https://arxiv.org/abs/2508.04372", "authors": ["Morten Roed Frederiksen", "Kasper St\u00f8y", "Maja Matari\u0107"], "title": "Tactile Comfort: Lowering Heart Rate Through Interactions", "comment": "6 pages, 4 figures, Proceedings from 2024 IEEE/RSJ International\n  Conference on Intelligent Robots and Systems (IROS). IEEE, 2024", "summary": "Children diagnosed with anxiety disorders are taught a range of strategies to\nnavigate situations of heightened anxiety. Techniques such as deep breathing\nand repetition of mantras are commonly employed, as they are known to be\ncalming and reduce elevated heart rates. Although these strategies are often\neffective, their successful application relies on prior training of the\nchildren for successful use when faced with challenging situations. This paper\ninvestigates a pocket-sized companion robot designed to offer a relaxation\ntechnique requiring no prior training, with a focus on immediate impact on the\nuser's heart rate. The robot utilizes a tactile game to divert the user's\nattention, thereby promoting relaxation. We conducted two studies with children\nwho were not diagnosed with anxiety: a 14-day pilot study with two children\n(age 8) and a main study with 18 children (ages 7-8). Both studies employed a\nwithin-subjects design and focused on measuring heart rate during tactile\ninteraction with the robot and during non-use. Interacting with the robot was\nfound to significantly lower the study participants' heart rate (p$<$0.01)\ncompared to the non-use condition, indicating a consistent calming effect\nacross all participants. These results suggest that tactile companion robots\nhave the potential to enhance the therapeutic value of relaxation techniques.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u4e00\u79cd\u65e0\u9700\u9884\u5148\u8bad\u7ec3\u7684\u53e3\u888b\u4f34\u4fa3\u673a\u5668\u4eba\uff0c\u901a\u8fc7\u89e6\u89c9\u6e38\u620f\u5206\u6563\u6ce8\u610f\u529b\uff0c\u663e\u8457\u964d\u4f4e\u513f\u7ae5\u5fc3\u7387\u3002", "motivation": "\u73b0\u6709\u7126\u8651\u7ba1\u7406\u7b56\u7565\u9700\u9884\u5148\u8bad\u7ec3\uff0c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u5373\u65f6\u6709\u6548\u7684\u653e\u677e\u5de5\u5177\u3002", "method": "\u91c7\u7528\u89e6\u89c9\u6e38\u620f\u673a\u5668\u4eba\uff0c\u901a\u8fc7\u4e24\u9879\u7814\u7a76\uff0814\u5929\u8bd5\u70b9\u548c\u4e3b\u7814\u7a76\uff09\u6d4b\u91cf\u513f\u7ae5\u5fc3\u7387\u53d8\u5316\u3002", "result": "\u673a\u5668\u4eba\u4e92\u52a8\u663e\u8457\u964d\u4f4e\u5fc3\u7387\uff08p<0.01\uff09\uff0c\u663e\u793a\u4e00\u81f4\u9547\u9759\u6548\u679c\u3002", "conclusion": "\u89e6\u89c9\u4f34\u4fa3\u673a\u5668\u4eba\u53ef\u589e\u5f3a\u653e\u677e\u6280\u672f\u7684\u6cbb\u7597\u6f5c\u529b\u3002"}}
{"id": "2508.04105", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.04105", "abs": "https://arxiv.org/abs/2508.04105", "authors": ["Karrtik Iyer", "Manikandan Ravikiran", "Prasanna Pendse", "Shayan Mohanty"], "title": "Towards Transparent AI Grading: Semantic Entropy as a Signal for Human-AI Disagreement", "comment": null, "summary": "Automated grading systems can efficiently score short-answer responses, yet\nthey often fail to indicate when a grading decision is uncertain or potentially\ncontentious. We introduce semantic entropy, a measure of variability across\nmultiple GPT-4-generated explanations for the same student response, as a proxy\nfor human grader disagreement. By clustering rationales via entailment-based\nsimilarity and computing entropy over these clusters, we quantify the diversity\nof justifications without relying on final output scores. We address three\nresearch questions: (1) Does semantic entropy align with human grader\ndisagreement? (2) Does it generalize across academic subjects? (3) Is it\nsensitive to structural task features such as source dependency? Experiments on\nthe ASAP-SAS dataset show that semantic entropy correlates with rater\ndisagreement, varies meaningfully across subjects, and increases in tasks\nrequiring interpretive reasoning. Our findings position semantic entropy as an\ninterpretable uncertainty signal that supports more transparent and trustworthy\nAI-assisted grading workflows.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u8bed\u4e49\u71b5\u201d\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7GPT-4\u751f\u6210\u7684\u591a\u91cd\u89e3\u91ca\u6765\u8861\u91cf\u81ea\u52a8\u8bc4\u5206\u7cfb\u7edf\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u4e0e\u4eba\u7c7b\u8bc4\u5206\u8005\u7684\u5206\u6b67\u5bf9\u9f50\u3002", "motivation": "\u5f53\u524d\u81ea\u52a8\u8bc4\u5206\u7cfb\u7edf\u65e0\u6cd5\u6709\u6548\u53cd\u6620\u8bc4\u5206\u51b3\u7b56\u7684\u4e0d\u786e\u5b9a\u6027\u6216\u4e89\u8bae\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u900f\u660e\u4e14\u53ef\u4fe1\u7684\u65b9\u6cd5\u6765\u91cf\u5316\u8fd9\u79cd\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u901a\u8fc7\u805a\u7c7b\u57fa\u4e8e\u8574\u542b\u76f8\u4f3c\u6027\u7684\u89e3\u91ca\u5e76\u8ba1\u7b97\u8fd9\u4e9b\u7c07\u7684\u71b5\uff0c\u91cf\u5316\u8bc4\u5206\u7684\u591a\u6837\u6027\uff0c\u800c\u4e0d\u4f9d\u8d56\u6700\u7ec8\u8f93\u51fa\u5206\u6570\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8bed\u4e49\u71b5\u4e0e\u4eba\u7c7b\u8bc4\u5206\u8005\u7684\u5206\u6b67\u76f8\u5173\uff0c\u4e14\u5728\u4e0d\u540c\u5b66\u79d1\u548c\u4efb\u52a1\u7ed3\u6784\u4e2d\u6709\u610f\u4e49\u5730\u53d8\u5316\u3002", "conclusion": "\u8bed\u4e49\u71b5\u4f5c\u4e3a\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u4e0d\u786e\u5b9a\u6027\u4fe1\u53f7\uff0c\u6709\u52a9\u4e8e\u63d0\u5347AI\u8f85\u52a9\u8bc4\u5206\u7684\u900f\u660e\u5ea6\u548c\u53ef\u4fe1\u5ea6\u3002"}}
{"id": "2508.04384", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.04384", "abs": "https://arxiv.org/abs/2508.04384", "authors": ["Eric R. Damm", "Eli S. Lancaster", "Felix A. Sanchez", "Kiana Bronder", "Jason M. Gregory", "Thomas M. Howard"], "title": "Incorporating Stochastic Models of Controller Behavior into Kinodynamic Efficiently Adaptive State Lattices for Mobile Robot Motion Planning in Off-Road Environments", "comment": "Accepted to the International Symposium on Experimental Robotics\n  (ISER) 2025", "summary": "Mobile robot motion planners rely on theoretical models to predict how the\nrobot will move through the world. However, when deployed on a physical robot,\nthese models are subject to errors due to real-world physics and uncertainty in\nhow the lower-level controller follows the planned trajectory. In this work, we\naddress this problem by presenting three methods of incorporating stochastic\ncontroller behavior into the recombinant search space of the Kinodynamic\nEfficiently Adaptive State Lattice (KEASL) planner. To demonstrate this work,\nwe analyze the results of experiments performed on a Clearpath Robotics Warthog\nUnmanned Ground Vehicle (UGV) in an off-road, unstructured environment using\ntwo different perception algorithms, and performed an ablation study using a\nfull spectrum of simulated environment map complexities. Analysis of the data\nfound that incorporating stochastic controller sampling into KEASL leads to\nmore conservative trajectories that decrease predicted collision likelihood\nwhen compared to KEASL without sampling. When compared to baseline planning\nwith expanded obstacle footprints, the predicted likelihood of collisions\nbecomes more comparable, but reduces the planning success rate for baseline\nsearch.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e09\u79cd\u65b9\u6cd5\uff0c\u5c06\u968f\u673a\u63a7\u5236\u5668\u884c\u4e3a\u878d\u5165KEASL\u89c4\u5212\u5668\u7684\u641c\u7d22\u7a7a\u95f4\uff0c\u4ee5\u51cf\u5c11\u7269\u7406\u673a\u5668\u4eba\u8fd0\u52a8\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u5f71\u54cd\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u751f\u6210\u66f4\u4fdd\u5b88\u7684\u8f68\u8ff9\uff0c\u964d\u4f4e\u78b0\u649e\u6982\u7387\u3002", "motivation": "\u7269\u7406\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\u4e2d\uff0c\u7406\u8bba\u6a21\u578b\u56e0\u73b0\u5b9e\u7269\u7406\u548c\u5e95\u5c42\u63a7\u5236\u5668\u7684\u4e0d\u786e\u5b9a\u6027\u800c\u4ea7\u751f\u8bef\u5dee\uff0c\u9700\u6539\u8fdb\u4ee5\u63d0\u9ad8\u53ef\u9760\u6027\u3002", "method": "\u5728KEASL\u89c4\u5212\u5668\u4e2d\u5f15\u5165\u4e09\u79cd\u968f\u673a\u63a7\u5236\u5668\u884c\u4e3a\u91c7\u6837\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u968f\u673a\u91c7\u6837\u65b9\u6cd5\u751f\u6210\u7684\u8f68\u8ff9\u66f4\u4fdd\u5b88\uff0c\u78b0\u649e\u9884\u6d4b\u6982\u7387\u964d\u4f4e\uff0c\u4f46\u4e0e\u57fa\u7ebf\u89c4\u5212\u76f8\u6bd4\uff0c\u6210\u529f\u7387\u6709\u6240\u4e0b\u964d\u3002", "conclusion": "\u968f\u673a\u63a7\u5236\u5668\u91c7\u6837\u80fd\u6709\u6548\u51cf\u5c11\u78b0\u649e\u98ce\u9669\uff0c\u4f46\u9700\u6743\u8861\u89c4\u5212\u6210\u529f\u7387\uff0c\u9002\u7528\u4e8e\u5bf9\u5b89\u5168\u6027\u8981\u6c42\u9ad8\u7684\u573a\u666f\u3002"}}
{"id": "2508.04116", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.04116", "abs": "https://arxiv.org/abs/2508.04116", "authors": ["Yongkang Li", "Shengping Xiao", "Shufang Zhu", "Jianwen Li", "Geguang Pu"], "title": "A Compositional Framework for On-the-Fly LTLf Synthesis", "comment": "8 pages, accepted by ECAI 2025", "summary": "Reactive synthesis from Linear Temporal Logic over finite traces (LTLf) can\nbe reduced to a two-player game over a Deterministic Finite Automaton (DFA) of\nthe LTLf specification. The primary challenge here is DFA construction, which\nis 2EXPTIME-complete in the worst case. Existing techniques either construct\nthe DFA compositionally before solving the game, leveraging automata\nminimization to mitigate state-space explosion, or build the DFA incrementally\nduring game solving to avoid full DFA construction. However, neither is\ndominant. In this paper, we introduce a compositional on-the-fly synthesis\nframework that integrates the strengths of both approaches, focusing on large\nconjunctions of smaller LTLf formulas common in practice. This framework\napplies composition during game solving instead of automata (game arena)\nconstruction. While composing all intermediate results may be necessary in the\nworst case, pruning these results simplifies subsequent compositions and\nenables early detection of unrealizability. Specifically, the framework allows\ntwo composition variants: pruning before composition to take full advantage of\nminimization or pruning during composition to guide on-the-fly synthesis.\nCompared to state-of-the-art synthesis solvers, our framework is able to solve\na notable number of instances that other solvers cannot handle. A detailed\nanalysis shows that both composition variants have unique merits.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ec4\u5408\u5f0f\u5373\u65f6\u5408\u6210\u6846\u67b6\uff0c\u7ed3\u5408\u4e86DFA\u6784\u9020\u548c\u6e38\u620f\u6c42\u89e3\u7684\u4f18\u52bf\uff0c\u9002\u7528\u4e8e\u5927\u578bLTLf\u516c\u5f0f\u3002", "motivation": "\u73b0\u6709\u6280\u672f\u5728DFA\u6784\u9020\u548c\u6e38\u620f\u6c42\u89e3\u4e0a\u5404\u6709\u5c40\u9650\uff0c\u7f3a\u4e4f\u7edf\u4e00\u4f18\u52bf\u3002", "method": "\u7ec4\u5408\u5f0f\u5373\u65f6\u5408\u6210\u6846\u67b6\uff0c\u5728\u6e38\u620f\u6c42\u89e3\u8fc7\u7a0b\u4e2d\u8fdb\u884c\u7ec4\u5408\uff0c\u652f\u6301\u4e24\u79cd\u7ec4\u5408\u53d8\u4f53\u3002", "result": "\u80fd\u591f\u89e3\u51b3\u5176\u4ed6\u6c42\u89e3\u5668\u65e0\u6cd5\u5904\u7406\u7684\u5b9e\u4f8b\uff0c\u4e24\u79cd\u7ec4\u5408\u53d8\u4f53\u5404\u6709\u4f18\u52bf\u3002", "conclusion": "\u6846\u67b6\u6709\u6548\u7ed3\u5408\u4e86\u4e24\u79cd\u65b9\u6cd5\u7684\u4f18\u70b9\uff0c\u63d0\u5347\u4e86\u5408\u6210\u80fd\u529b\u3002"}}
{"id": "2508.04436", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.04436", "abs": "https://arxiv.org/abs/2508.04436", "authors": ["Yujia Lu", "Chong Wei", "Lu Ma"], "title": "Reliable and Real-Time Highway Trajectory Planning via Hybrid Learning-Optimization Frameworks", "comment": null, "summary": "Autonomous highway driving presents a high collision risk due to\nfast-changing environments and limited reaction time, necessitating reliable\nand efficient trajectory planning. This paper proposes a hybrid trajectory\nplanning framework that integrates the adaptability of learning-based methods\nwith the formal safety guarantees of optimization-based approaches. The\nframework features a two-layer architecture: an upper layer employing a graph\nneural network (GNN) trained on real-world highway data to predict human-like\nlongitudinal velocity profiles, and a lower layer utilizing path optimization\nformulated as a mixed-integer quadratic programming (MIQP) problem. The primary\ncontribution is the lower-layer path optimization model, which introduces a\nlinear approximation of discretized vehicle geometry to substantially reduce\ncomputational complexity, while enforcing strict spatiotemporal non-overlapping\nconstraints to formally guarantee collision avoidance throughout the planning\nhorizon. Experimental results demonstrate that the planner generates highly\nsmooth, collision-free trajectories in complex real-world emergency scenarios,\nachieving success rates exceeding 97% with average planning times of 54 ms,\nthereby confirming real-time capability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u8f68\u8ff9\u89c4\u5212\u6846\u67b6\uff0c\u7ed3\u5408\u5b66\u4e60\u65b9\u6cd5\u548c\u4f18\u5316\u65b9\u6cd5\uff0c\u5b9e\u73b0\u9ad8\u6548\u5b89\u5168\u7684\u81ea\u52a8\u9a7e\u9a76\u9ad8\u901f\u516c\u8def\u884c\u9a76\u3002", "motivation": "\u9ad8\u901f\u516c\u8def\u9a7e\u9a76\u73af\u5883\u53d8\u5316\u5feb\u4e14\u53cd\u5e94\u65f6\u95f4\u6709\u9650\uff0c\u9700\u53ef\u9760\u9ad8\u6548\u7684\u8f68\u8ff9\u89c4\u5212\u4ee5\u51cf\u5c11\u78b0\u649e\u98ce\u9669\u3002", "method": "\u91c7\u7528\u53cc\u5c42\u67b6\u6784\uff1a\u4e0a\u5c42\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u901f\u5ea6\uff0c\u4e0b\u5c42\u7528\u6df7\u5408\u6574\u6570\u4e8c\u6b21\u89c4\u5212\u4f18\u5316\u8def\u5f84\uff0c\u5e76\u5f15\u5165\u7ebf\u6027\u8fd1\u4f3c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u89c4\u5212\u5668\u5728\u590d\u6742\u7d27\u6025\u573a\u666f\u4e2d\u751f\u6210\u5e73\u6ed1\u65e0\u78b0\u649e\u8f68\u8ff9\uff0c\u6210\u529f\u7387\u8d8597%\uff0c\u5e73\u5747\u89c4\u5212\u65f6\u95f454\u6beb\u79d2\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u4fdd\u8bc1\u5b89\u5168\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u5b9e\u65f6\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u9ad8\u901f\u516c\u8def\u81ea\u52a8\u9a7e\u9a76\u3002"}}
{"id": "2508.04118", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.04118", "abs": "https://arxiv.org/abs/2508.04118", "authors": ["Ruochen Zhao", "Simone Conia", "Eric Peng", "Min Li", "Saloni Potdar"], "title": "AgREE: Agentic Reasoning for Knowledge Graph Completion on Emerging Entities", "comment": null, "summary": "Open-domain Knowledge Graph Completion (KGC) faces significant challenges in\nan ever-changing world, especially when considering the continual emergence of\nnew entities in daily news. Existing approaches for KGC mainly rely on\npretrained language models' parametric knowledge, pre-constructed queries, or\nsingle-step retrieval, typically requiring substantial supervision and training\ndata. Even so, they often fail to capture comprehensive and up-to-date\ninformation about unpopular and/or emerging entities. To this end, we introduce\nAgentic Reasoning for Emerging Entities (AgREE), a novel agent-based framework\nthat combines iterative retrieval actions and multi-step reasoning to\ndynamically construct rich knowledge graph triplets. Experiments show that,\ndespite requiring zero training efforts, AgREE significantly outperforms\nexisting methods in constructing knowledge graph triplets, especially for\nemerging entities that were not seen during language models' training\nprocesses, outperforming previous methods by up to 13.7%. Moreover, we propose\na new evaluation methodology that addresses a fundamental weakness of existing\nsetups and a new benchmark for KGC on emerging entities. Our work demonstrates\nthe effectiveness of combining agent-based reasoning with strategic information\nretrieval for maintaining up-to-date knowledge graphs in dynamic information\nenvironments.", "AI": {"tldr": "AgREE\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u8fed\u4ee3\u68c0\u7d22\u548c\u591a\u6b65\u63a8\u7406\uff0c\u52a8\u6001\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\u4e09\u5143\u7ec4\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u65b0\u5174\u5b9e\u4f53\u3002", "motivation": "\u5f00\u653e\u57df\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u9762\u4e34\u52a8\u6001\u4e16\u754c\u4e2d\u65b0\u5174\u5b9e\u4f53\u4fe1\u606f\u4e0d\u8db3\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u9884\u8bad\u7ec3\u6a21\u578b\u6216\u5355\u6b65\u68c0\u7d22\uff0c\u96be\u4ee5\u6355\u6349\u6700\u65b0\u4fe1\u606f\u3002", "method": "\u63d0\u51faAgREE\u6846\u67b6\uff0c\u7ed3\u5408\u8fed\u4ee3\u68c0\u7d22\u548c\u591a\u6b65\u63a8\u7406\uff0c\u52a8\u6001\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\u4e09\u5143\u7ec4\uff0c\u65e0\u9700\u8bad\u7ec3\u6570\u636e\u3002", "result": "AgREE\u5728\u6784\u5efa\u77e5\u8bc6\u56fe\u8c31\u4e09\u5143\u7ec4\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u5bf9\u65b0\u5174\u5b9e\u4f53\u8868\u73b0\u7a81\u51fa\uff0c\u63d0\u5347\u8fbe13.7%\u3002", "conclusion": "AgREE\u5c55\u793a\u4e86\u57fa\u4e8e\u4ee3\u7406\u7684\u63a8\u7406\u4e0e\u4fe1\u606f\u68c0\u7d22\u7ed3\u5408\u5728\u52a8\u6001\u77e5\u8bc6\u56fe\u8c31\u7ef4\u62a4\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.04537", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.04537", "abs": "https://arxiv.org/abs/2508.04537", "authors": ["Alkesh K. Srivastava", "Aamodh Suresh", "Carlos Nieto-Granda"], "title": "Behaviorally Adaptive Multi-Robot Hazard Localization in Failure-Prone, Communication-Denied Environments", "comment": null, "summary": "We address the challenge of multi-robot autonomous hazard mapping in\nhigh-risk, failure-prone, communication-denied environments such as\npost-disaster zones, underground mines, caves, and planetary surfaces. In these\nmissions, robots must explore and map hazards while minimizing the risk of\nfailure due to environmental threats or hardware limitations. We introduce a\nbehavior-adaptive, information-theoretic planning framework for multi-robot\nteams grounded in the concept of Behavioral Entropy (BE), that generalizes\nShannon entropy (SE) to capture diverse human-like uncertainty evaluations.\nBuilding on this formulation, we propose the Behavior-Adaptive Path Planning\n(BAPP) framework, which modulates information gathering strategies via a\ntunable risk-sensitivity parameter, and present two planning algorithms:\nBAPP-TID for intelligent triggering of high-fidelity robots, and BAPP-SIG for\nsafe deployment under high risk. We provide theoretical insights on the\ninformativeness of the proposed BAPP framework and validate its effectiveness\nthrough both single-robot and multi-robot simulations. Our results show that\nthe BAPP stack consistently outperforms Shannon-based and random strategies:\nBAPP-TID accelerates entropy reduction, while BAPP-SIG improves robot\nsurvivability with minimal loss in information gain. In multi-agent\ndeployments, BAPP scales effectively through spatial partitioning, mobile base\nrelocation, and role-aware heterogeneity. These findings underscore the value\nof behavior-adaptive planning for robust, risk-sensitive exploration in\ncomplex, failure-prone environments.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u884c\u4e3a\u71b5\u7684\u591a\u673a\u5668\u4eba\u81ea\u9002\u5e94\u8def\u5f84\u89c4\u5212\u6846\u67b6\uff08BAPP\uff09\uff0c\u7528\u4e8e\u9ad8\u98ce\u9669\u3001\u901a\u4fe1\u53d7\u9650\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u5371\u9669\u5730\u56fe\u6784\u5efa\u3002", "motivation": "\u89e3\u51b3\u591a\u673a\u5668\u4eba\u5728\u9ad8\u98ce\u9669\u3001\u6613\u5931\u8d25\u3001\u901a\u4fe1\u53d7\u9650\u73af\u5883\uff08\u5982\u707e\u540e\u533a\u57df\u3001\u5730\u4e0b\u77ff\u4e95\u7b49\uff09\u4e2d\u7684\u81ea\u4e3b\u63a2\u7d22\u4e0e\u5730\u56fe\u6784\u5efa\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u884c\u4e3a\u71b5\uff08BE\uff09\u6982\u5ff5\uff0c\u5e76\u57fa\u4e8e\u6b64\u8bbe\u8ba1\u4e86BAPP\u6846\u67b6\uff0c\u5305\u542b\u4e24\u79cd\u7b97\u6cd5\uff1aBAPP-TID\uff08\u667a\u80fd\u89e6\u53d1\u9ad8\u4fdd\u771f\u673a\u5668\u4eba\uff09\u548cBAPP-SIG\uff08\u9ad8\u98ce\u9669\u4e0b\u7684\u5b89\u5168\u90e8\u7f72\uff09\u3002", "result": "BAPP\u6846\u67b6\u5728\u5355\u673a\u5668\u4eba\u548c\u591a\u673a\u5668\u4eba\u6a21\u62df\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u4e8e\u9999\u519c\u71b5\u548c\u968f\u673a\u7b56\u7565\u7684\u65b9\u6cd5\uff0cBAPP-TID\u52a0\u901f\u71b5\u51cf\uff0cBAPP-SIG\u63d0\u9ad8\u673a\u5668\u4eba\u5b58\u6d3b\u7387\u3002", "conclusion": "\u884c\u4e3a\u81ea\u9002\u5e94\u89c4\u5212\u5728\u590d\u6742\u3001\u9ad8\u98ce\u9669\u73af\u5883\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0cBAPP\u6846\u67b6\u901a\u8fc7\u7a7a\u95f4\u5206\u533a\u548c\u89d2\u8272\u5f02\u6784\u5b9e\u73b0\u4e86\u9ad8\u6548\u6269\u5c55\u3002"}}
{"id": "2508.04163", "categories": ["cs.AI", "cs.LO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.04163", "abs": "https://arxiv.org/abs/2508.04163", "authors": ["Hasra Dodampegama", "Mohan Sridharan"], "title": "Generic-to-Specific Reasoning and Learning for Scalable Ad Hoc Teamwork", "comment": "14 pages, 6 figures", "summary": "AI agents deployed in assistive roles often have to collaborate with other\nagents (humans, AI systems) without prior coordination. Methods considered\nstate of the art for such ad hoc teamwork often pursue a data-driven approach\nthat needs a large labeled dataset of prior observations, lacks transparency,\nand makes it difficult to rapidly revise existing knowledge in response to\nchanges. As the number of agents increases, the complexity of decision-making\nmakes it difficult to collaborate effectively. This paper advocates leveraging\nthe complementary strengths of knowledge-based and data-driven methods for\nreasoning and learning for ad hoc teamwork. For any given goal, our\narchitecture enables each ad hoc agent to determine its actions through\nnon-monotonic logical reasoning with: (a) prior commonsense domain-specific\nknowledge; (b) models learned and revised rapidly to predict the behavior of\nother agents; and (c) anticipated abstract future goals based on generic\nknowledge of similar situations in an existing foundation model. We\nexperimentally evaluate our architecture's capabilities in VirtualHome, a\nrealistic physics-based 3D simulation environment.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u77e5\u8bc6\u9a71\u52a8\u548c\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347AI\u4ee3\u7406\u5728\u4e34\u65f6\u56e2\u961f\u534f\u4f5c\u4e2d\u7684\u51b3\u7b56\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u7684\u900f\u660e\u6027\u548c\u9002\u5e94\u6027\u4e0d\u8db3\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u4e34\u65f6\u56e2\u961f\u534f\u4f5c\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u6807\u6ce8\u6570\u636e\u4e14\u7f3a\u4e4f\u900f\u660e\u6027\uff0c\u96be\u4ee5\u5feb\u901f\u9002\u5e94\u53d8\u5316\uff0c\u968f\u7740\u4ee3\u7406\u6570\u91cf\u589e\u52a0\uff0c\u534f\u4f5c\u6548\u7387\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u67b6\u6784\uff0c\u7ed3\u5408\u975e\u5355\u8c03\u903b\u8f91\u63a8\u7406\u3001\u9886\u57df\u77e5\u8bc6\u3001\u5feb\u901f\u5b66\u4e60\u548c\u9884\u6d4b\u5176\u4ed6\u4ee3\u7406\u884c\u4e3a\u7684\u80fd\u529b\uff0c\u4ee5\u53ca\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\u7684\u672a\u6765\u76ee\u6807\u9884\u6d4b\u3002", "result": "\u5728VirtualHome\u4eff\u771f\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u67b6\u6784\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7ed3\u5408\u77e5\u8bc6\u9a71\u52a8\u548c\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u5347\u4e34\u65f6\u56e2\u961f\u534f\u4f5c\u7684\u6548\u7387\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2508.04598", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.04598", "abs": "https://arxiv.org/abs/2508.04598", "authors": ["Lingfeng Zhang", "Xiaoshuai Hao", "Yingbo Tang", "Haoxiang Fu", "Xinyu Zheng", "Pengwei Wang", "Zhongyuan Wang", "Wenbo Ding", "Shanghang Zhang"], "title": "$NavA^3$: Understanding Any Instruction, Navigating Anywhere, Finding Anything", "comment": null, "summary": "Embodied navigation is a fundamental capability of embodied intelligence,\nenabling robots to move and interact within physical environments. However,\nexisting navigation tasks primarily focus on predefined object navigation or\ninstruction following, which significantly differs from human needs in\nreal-world scenarios involving complex, open-ended scenes. To bridge this gap,\nwe introduce a challenging long-horizon navigation task that requires\nunderstanding high-level human instructions and performing spatial-aware object\nnavigation in real-world environments. Existing embodied navigation methods\nstruggle with such tasks due to their limitations in comprehending high-level\nhuman instructions and localizing objects with an open vocabulary. In this\npaper, we propose $NavA^3$, a hierarchical framework divided into two stages:\nglobal and local policies. In the global policy, we leverage the reasoning\ncapabilities of Reasoning-VLM to parse high-level human instructions and\nintegrate them with global 3D scene views. This allows us to reason and\nnavigate to regions most likely to contain the goal object. In the local\npolicy, we have collected a dataset of 1.0 million samples of spatial-aware\nobject affordances to train the NaviAfford model (PointingVLM), which provides\nrobust open-vocabulary object localization and spatial awareness for precise\ngoal identification and navigation in complex environments. Extensive\nexperiments demonstrate that $NavA^3$ achieves SOTA results in navigation\nperformance and can successfully complete longhorizon navigation tasks across\ndifferent robot embodiments in real-world settings, paving the way for\nuniversal embodied navigation. The dataset and code will be made available.\nProject website: https://NavigationA3.github.io/.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a$NavA^3$\u7684\u5206\u5c42\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u590d\u6742\u5f00\u653e\u573a\u666f\u4e2d\u7684\u957f\u65f6\u7a0b\u5bfc\u822a\u4efb\u52a1\uff0c\u7ed3\u5408\u5168\u5c40\u548c\u5c40\u90e8\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bfc\u822a\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5bfc\u822a\u4efb\u52a1\u5c40\u9650\u4e8e\u9884\u5b9a\u4e49\u5bf9\u8c61\u6216\u6307\u4ee4\u8ddf\u968f\uff0c\u65e0\u6cd5\u6ee1\u8db3\u771f\u5b9e\u573a\u666f\u4e2d\u590d\u6742\u5f00\u653e\u9700\u6c42\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u7406\u89e3\u9ad8\u7ea7\u4eba\u7c7b\u6307\u4ee4\u5e76\u5b9e\u73b0\u7a7a\u95f4\u611f\u77e5\u5bfc\u822a\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u5206\u5c42\u6846\u67b6\uff1a\u5168\u5c40\u7b56\u7565\u5229\u7528Reasoning-VLM\u89e3\u6790\u6307\u4ee4\u5e76\u7ed3\u54083D\u573a\u666f\u89c6\u56fe\uff1b\u5c40\u90e8\u7b56\u7565\u901a\u8fc7NaviAfford\u6a21\u578b\uff08PointingVLM\uff09\u5b9e\u73b0\u5f00\u653e\u8bcd\u6c47\u5bf9\u8c61\u5b9a\u4f4d\u548c\u7a7a\u95f4\u611f\u77e5\u3002", "result": "$NavA^3$\u5728\u5bfc\u822a\u6027\u80fd\u4e0a\u8fbe\u5230SOTA\uff0c\u6210\u529f\u5b8c\u6210\u4e0d\u540c\u673a\u5668\u4eba\u5e73\u53f0\u7684\u957f\u65f6\u7a0b\u5bfc\u822a\u4efb\u52a1\u3002", "conclusion": "$NavA^3$\u4e3a\u901a\u7528\u5bfc\u822a\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5c06\u516c\u5f00\u3002"}}
{"id": "2508.04235", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.04235", "abs": "https://arxiv.org/abs/2508.04235", "authors": ["Jiaying Zhu", "Ziyang Zheng", "Zhengyuan Shi", "Yalun Cai", "Qiang Xu"], "title": "Circuit-Aware SAT Solving: Guiding CDCL via Conditional Probabilities", "comment": "11 pages, 7 figures", "summary": "Circuit Satisfiability (CSAT) plays a pivotal role in Electronic Design\nAutomation. The standard workflow for solving CSAT problems converts circuits\ninto Conjunctive Normal Form (CNF) and employs generic SAT solvers powered by\nConflict-Driven Clause Learning (CDCL). However, this process inherently\ndiscards rich structural and functional information, leading to suboptimal\nsolver performance. To address this limitation, we introduce CASCAD, a novel\ncircuit-aware SAT solving framework that directly leverages circuit-level\nconditional probabilities computed via Graph Neural Networks (GNNs). By\nexplicitly modeling gate-level conditional probabilities, CASCAD dynamically\nguides two critical CDCL heuristics -- variable phase selection and clause\nmanagementto significantly enhance solver efficiency. Extensive evaluations on\nchallenging real-world Logical Equivalence Checking (LEC) benchmarks\ndemonstrate that CASCAD reduces solving times by up to 10x compared to\nstate-of-the-art CNF-based approaches, achieving an additional 23.5% runtime\nreduction via our probability-guided clause filtering strategy. Our results\nunderscore the importance of preserving circuit-level structural insights\nwithin SAT solvers, providing a robust foundation for future improvements in\nSAT-solving efficiency and EDA tool design.", "AI": {"tldr": "CASCAD\u662f\u4e00\u79cd\u65b0\u578b\u7535\u8def\u611f\u77e5SAT\u6c42\u89e3\u6846\u67b6\uff0c\u901a\u8fc7GNN\u8ba1\u7b97\u7535\u8def\u7ea7\u6761\u4ef6\u6982\u7387\uff0c\u663e\u8457\u63d0\u5347\u6c42\u89e3\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5c06\u7535\u8def\u8f6c\u6362\u4e3aCNF\u5e76\u4e22\u5f03\u7ed3\u6784\u4fe1\u606f\uff0c\u5bfc\u81f4\u6c42\u89e3\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u5229\u7528GNN\u8ba1\u7b97\u95e8\u7ea7\u6761\u4ef6\u6982\u7387\uff0c\u52a8\u6001\u6307\u5bfcCDCL\u542f\u53d1\u5f0f\uff08\u53d8\u91cf\u76f8\u4f4d\u9009\u62e9\u548c\u5b50\u53e5\u7ba1\u7406\uff09\u3002", "result": "\u5728LEC\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6c42\u89e3\u65f6\u95f4\u51cf\u5c1110\u500d\uff0c\u6982\u7387\u5f15\u5bfc\u5b50\u53e5\u8fc7\u6ee4\u7b56\u7565\u989d\u5916\u51cf\u5c1123.5%\u8fd0\u884c\u65f6\u95f4\u3002", "conclusion": "\u4fdd\u7559\u7535\u8def\u7ed3\u6784\u4fe1\u606f\u5bf9\u63d0\u5347SAT\u6c42\u89e3\u6548\u7387\u548cEDA\u5de5\u5177\u8bbe\u8ba1\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2508.04642", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.04642", "abs": "https://arxiv.org/abs/2508.04642", "authors": ["Baihui Xiao", "Chengjian Feng", "Zhijian Huang", "Feng yan", "Yujie Zhong", "Lin Ma"], "title": "RoboTron-Sim: Improving Real-World Driving via Simulated Hard-Case", "comment": "ICCV 2025", "summary": "Collecting real-world data for rare high-risk scenarios, long-tailed driving\nevents, and complex interactions remains challenging, leading to poor\nperformance of existing autonomous driving systems in these critical\nsituations. In this paper, we propose RoboTron-Sim that improves real-world\ndriving in critical situations by utilizing simulated hard cases. First, we\ndevelop a simulated dataset called Hard-case Augmented Synthetic Scenarios\n(HASS), which covers 13 high-risk edge-case categories, as well as balanced\nenvironmental conditions such as day/night and sunny/rainy. Second, we\nintroduce Scenario-aware Prompt Engineering (SPE) and an Image-to-Ego Encoder\n(I2E Encoder) to enable multimodal large language models to effectively learn\nreal-world challenging driving skills from HASS, via adapting to environmental\ndeviations and hardware differences between real-world and simulated scenarios.\nExtensive experiments on nuScenes show that RoboTron-Sim improves driving\nperformance in challenging scenarios by around 50%, achieving state-of-the-art\nresults in real-world open-loop planning. Qualitative results further\ndemonstrate the effectiveness of RoboTron-Sim in better managing rare high-risk\ndriving scenarios. Project page: https://stars79689.github.io/RoboTron-Sim/", "AI": {"tldr": "RoboTron-Sim\u901a\u8fc7\u6a21\u62df\u9ad8\u98ce\u9669\u573a\u666f\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5728\u5173\u952e\u60c5\u51b5\u4e0b\u7684\u8868\u73b0\uff0c\u5305\u62ec\u5f00\u53d1HASS\u6570\u636e\u96c6\u548c\u5f15\u5165SPE\u4e0eI2E\u7f16\u7801\u5668\u3002", "motivation": "\u89e3\u51b3\u771f\u5b9e\u4e16\u754c\u4e2d\u7f55\u89c1\u9ad8\u98ce\u9669\u573a\u666f\u6570\u636e\u4e0d\u8db3\u5bfc\u81f4\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u6027\u80fd\u4e0d\u4f73\u7684\u95ee\u9898\u3002", "method": "\u5f00\u53d1HASS\u6a21\u62df\u6570\u636e\u96c6\uff0c\u5f15\u5165SPE\u548cI2E\u7f16\u7801\u5668\uff0c\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5b66\u4e60\u9a7e\u9a76\u6280\u80fd\u3002", "result": "\u5728nuScenes\u4e0a\u5b9e\u9a8c\u663e\u793a\uff0c\u9a7e\u9a76\u6027\u80fd\u63d0\u5347\u7ea650%\uff0c\u8fbe\u5230\u5f00\u73af\u89c4\u5212\u7684\u6700\u5148\u8fdb\u6c34\u5e73\u3002", "conclusion": "RoboTron-Sim\u80fd\u6709\u6548\u7ba1\u7406\u7f55\u89c1\u9ad8\u98ce\u9669\u9a7e\u9a76\u573a\u666f\uff0c\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2508.04278", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.04278", "abs": "https://arxiv.org/abs/2508.04278", "authors": ["Wentao Wu", "Linqing Chen", "Hanmeng Zhong", "Weilei Wang"], "title": "Large Language Model's Multi-Capability Alignment in Biomedical Domain", "comment": null, "summary": "BalancedBio is a theoretically grounded framework for parameter-efficient\nbiomedical reasoning, addressing multi-capability integration in\ndomain-specific AI alignment. It establishes the Biomedical Multi-Capability\nConvergence Theorem, proving orthogonal gradient spaces are essential to\nprevent capability interference for safe deployment. Key innovations include:\n(1) Medical Knowledge Grounded Synthetic Generation (MKGSG), extending\nSource2Synth with clinical workflow constraints and medical ontology validation\nfor factual accuracy and safety; and (2) Capability Aware Group Relative Policy\nOptimization, deriving optimal hybrid reward weighting to maintain\northogonality in RL, using a reward model with rule-based and model-based\nscores adapted to biomedical tasks. Mathematical analysis proves Pareto-optimal\nconvergence, preserving performance across capabilities. It achieves\nstate-of-the-art results in its parameter class: domain expertise (80.95%\nBIOMED-MMLU, +15.32% over baseline), reasoning (61.94%, +7.75%), instruction\nfollowing (67.95%, +6.44%), and integration (86.7%, +18.5%). Theoretical safety\nguarantees include bounds on capability preservation and clinical accuracy.\nReal-world deployment yields 78% cost reduction, 23% improved diagnostic\naccuracy, and 89% clinician acceptance. This work provides a principled\nmethodology for biomedical AI alignment, enabling efficient reasoning with\nessential safety and reliability, with the 0.5B model version to be released.", "AI": {"tldr": "BalancedBio\u662f\u4e00\u4e2a\u7406\u8bba\u652f\u6301\u7684\u53c2\u6570\u9ad8\u6548\u751f\u7269\u533b\u5b66\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u6b63\u4ea4\u68af\u5ea6\u7a7a\u95f4\u9632\u6b62\u80fd\u529b\u5e72\u6270\uff0c\u7ed3\u5408\u533b\u5b66\u77e5\u8bc6\u751f\u6210\u548c\u7b56\u7565\u4f18\u5316\uff0c\u5b9e\u73b0\u591a\u80fd\u529b\u96c6\u6210\u4e0e\u5b89\u5168\u90e8\u7f72\u3002", "motivation": "\u89e3\u51b3\u751f\u7269\u533b\u5b66\u9886\u57dfAI\u591a\u80fd\u529b\u96c6\u6210\u4e2d\u7684\u80fd\u529b\u5e72\u6270\u95ee\u9898\uff0c\u786e\u4fdd\u5b89\u5168\u90e8\u7f72\u548c\u4e34\u5e8a\u51c6\u786e\u6027\u3002", "method": "1. MKGSG\uff08\u533b\u5b66\u77e5\u8bc6\u57fa\u7840\u5408\u6210\u751f\u6210\uff09\u7ed3\u5408\u4e34\u5e8a\u5de5\u4f5c\u6d41\u548c\u533b\u5b66\u672c\u4f53\u9a8c\u8bc1\uff1b2. \u80fd\u529b\u611f\u77e5\u7684\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff0c\u4fdd\u6301\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6b63\u4ea4\u6027\u3002", "result": "\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u663e\u8457\u63d0\u5347\uff08\u5982BIOMED-MMLU\u8fbe80.95%\uff09\uff0c\u5e76\u5b9e\u73b078%\u6210\u672c\u964d\u4f4e\u548c23%\u8bca\u65ad\u51c6\u786e\u7387\u63d0\u5347\u3002", "conclusion": "BalancedBio\u4e3a\u751f\u7269\u533b\u5b66AI\u5bf9\u9f50\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u65b9\u6cd5\u652f\u6301\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u5b89\u5168\u3001\u53ef\u9760\u7684\u63a8\u7406\uff0c\u5373\u5c06\u53d1\u5e030.5B\u6a21\u578b\u3002"}}
{"id": "2508.04678", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.04678", "abs": "https://arxiv.org/abs/2508.04678", "authors": ["Joel Loo", "Zhanxin Wu", "David Hsu"], "title": "Open Scene Graphs for Open-World Object-Goal Navigation", "comment": "In IJRR Special Issue: Foundation Models and Neuro-symbolic AI for\n  Robotics. Journal extension to arXiv:2407.02473", "summary": "How can we build general-purpose robot systems for open-world semantic\nnavigation, e.g., searching a novel environment for a target object specified\nin natural language? To tackle this challenge, we introduce OSG Navigator, a\nmodular system composed of foundation models, for open-world Object-Goal\nNavigation (ObjectNav). Foundation models provide enormous semantic knowledge\nabout the world, but struggle to organise and maintain spatial information\neffectively at scale. Key to OSG Navigator is the Open Scene Graph\nrepresentation, which acts as spatial memory for OSG Navigator. It organises\nspatial information hierarchically using OSG schemas, which are templates, each\ndescribing the common structure of a class of environments. OSG schemas can be\nautomatically generated from simple semantic labels of a given environment,\ne.g., \"home\" or \"supermarket\". They enable OSG Navigator to adapt zero-shot to\nnew environment types. We conducted experiments using both Fetch and Spot\nrobots in simulation and in the real world, showing that OSG Navigator achieves\nstate-of-the-art performance on ObjectNav benchmarks and generalises zero-shot\nover diverse goals, environments, and robot embodiments.", "AI": {"tldr": "OSG Navigator\u662f\u4e00\u4e2a\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\u7684\u6a21\u5757\u5316\u7cfb\u7edf\uff0c\u7528\u4e8e\u5f00\u653e\u4e16\u754c\u7684\u76ee\u6807\u5bfc\u822a\uff08ObjectNav\uff09\uff0c\u901a\u8fc7Open Scene Graph\u8868\u793a\u4f5c\u4e3a\u7a7a\u95f4\u8bb0\u5fc6\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u9002\u5e94\u65b0\u73af\u5883\u3002", "motivation": "\u89e3\u51b3\u5f00\u653e\u4e16\u754c\u8bed\u4e49\u5bfc\u822a\u7684\u6311\u6218\uff0c\u5982\u5728\u65b0\u73af\u5883\u4e2d\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u641c\u7d22\u76ee\u6807\u5bf9\u8c61\u3002", "method": "\u4f7f\u7528Open Scene Graph\u8868\u793a\u4f5c\u4e3a\u7a7a\u95f4\u8bb0\u5fc6\uff0c\u901a\u8fc7OSG\u6a21\u5f0f\u5c42\u6b21\u5316\u7ec4\u7ec7\u7a7a\u95f4\u4fe1\u606f\uff0c\u6a21\u5f0f\u53ef\u81ea\u52a8\u751f\u6210\u3002", "result": "\u5728Fetch\u548cSpot\u673a\u5668\u4eba\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cOSG Navigator\u5728ObjectNav\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5e76\u80fd\u96f6\u6837\u672c\u9002\u5e94\u591a\u6837\u76ee\u6807\u3001\u73af\u5883\u548c\u673a\u5668\u4eba\u5f62\u6001\u3002", "conclusion": "OSG Navigator\u901a\u8fc7\u57fa\u7840\u6a21\u578b\u548cOpen Scene Graph\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5f00\u653e\u4e16\u754c\u8bed\u4e49\u5bfc\u822a\u95ee\u9898\uff0c\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u9002\u5e94\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.04282", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.04282", "abs": "https://arxiv.org/abs/2508.04282", "authors": ["Yongyi Wang", "Lingfeng Li", "Bozhou Chen", "Ang Li", "Hanyu Liu", "Qirui Zheng", "Xionghui Yang", "Wenxin Li"], "title": "Synthetic POMDPs to Challenge Memory-Augmented RL: Memory Demand Structure Modeling", "comment": null, "summary": "Recent research has developed benchmarks for memory-augmented reinforcement\nlearning (RL) algorithms, providing Partially Observable Markov Decision\nProcess (POMDP) environments where agents depend on past observations to make\ndecisions. While many benchmarks incorporate sufficiently complex real-world\nproblems, they lack controllability over the degree of challenges posed to\nmemory models. In contrast, synthetic environments enable fine-grained\nmanipulation of dynamics, making them critical for detailed and rigorous\nevaluation of memory-augmented RL. Our study focuses on POMDP synthesis with\nthree key contributions:\n  1. A theoretical framework for analyzing POMDPs, grounded in Memory Demand\nStructure (MDS), transition invariance, and related concepts; 2. A methodology\nleveraging linear process dynamics, state aggregation, and reward\nredistribution to construct customized POMDPs with predefined properties; 3.\nEmpirically validated series of POMDP environments with increasing difficulty\nlevels, designed based on our theoretical insights. Our work clarifies the\nchallenges of memory-augmented RL in solving POMDPs, provides guidelines for\nanalyzing and designing POMDP environments, and offers empirical support for\nselecting memory models in RL tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7406\u8bba\u6846\u67b6\u7684POMDP\u5408\u6210\u65b9\u6cd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u8bb0\u5fc6\u589e\u5f3a\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u5e76\u63d0\u4f9b\u4e86\u53ef\u5b9a\u5236\u96be\u5ea6\u7684\u73af\u5883\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u7f3a\u4e4f\u5bf9\u8bb0\u5fc6\u6a21\u578b\u6311\u6218\u7a0b\u5ea6\u7684\u53ef\u63a7\u6027\uff0c\u800c\u5408\u6210\u73af\u5883\u80fd\u66f4\u7cbe\u7ec6\u5730\u8bc4\u4f30\u8bb0\u5fc6\u589e\u5f3aRL\u3002", "method": "\u7ed3\u5408\u7ebf\u6027\u8fc7\u7a0b\u52a8\u6001\u3001\u72b6\u6001\u805a\u5408\u548c\u5956\u52b1\u91cd\u5206\u914d\uff0c\u6784\u5efa\u5177\u6709\u9884\u8bbe\u5c5e\u6027\u7684POMDP\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u7cfb\u5217\u96be\u5ea6\u9012\u589e\u7684POMDP\u73af\u5883\uff0c\u4e3a\u8bb0\u5fc6\u6a21\u578b\u9009\u62e9\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u652f\u6301\u3002", "conclusion": "\u7814\u7a76\u660e\u786e\u4e86\u8bb0\u5fc6\u589e\u5f3aRL\u5728\u89e3\u51b3POMDP\u4e2d\u7684\u6311\u6218\uff0c\u5e76\u4e3a\u73af\u5883\u8bbe\u8ba1\u548c\u5206\u6790\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2508.04691", "categories": ["cs.RO", "cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.04691", "abs": "https://arxiv.org/abs/2508.04691", "authors": ["Yuanchen Bai", "Zijian Ding", "Shaoyue Wen", "Xiang Chang", "Angelique Taylor"], "title": "From MAS to MARS: Coordination Failures and Reasoning Trade-offs in Hierarchical Multi-Agent Robotic Systems within a Healthcare Scenario", "comment": null, "summary": "Multi-agent robotic systems (MARS) build upon multi-agent systems by\nintegrating physical and task-related constraints, increasing the complexity of\naction execution and agent coordination. However, despite the availability of\nadvanced multi-agent frameworks, their real-world deployment on robots remains\nlimited, hindering the advancement of MARS research in practice. To bridge this\ngap, we conducted two studies to investigate performance trade-offs of\nhierarchical multi-agent frameworks in a simulated real-world multi-robot\nhealthcare scenario. In Study 1, using CrewAI, we iteratively refine the\nsystem's knowledge base, to systematically identify and categorize coordination\nfailures (e.g., tool access violations, lack of timely handling of failure\nreports) not resolvable by providing contextual knowledge alone. In Study 2,\nusing AutoGen, we evaluate a redesigned bidirectional communication structure\nand further measure the trade-offs between reasoning and non-reasoning models\noperating within the same robotic team setting. Drawing from our empirical\nfindings, we emphasize the tension between autonomy and stability and the\nimportance of edge-case testing to improve system reliability and safety for\nfuture real-world deployment. Supplementary materials, including codes, task\nagent setup, trace outputs, and annotated examples of coordination failures and\nreasoning behaviors, are available at:\nhttps://byc-sophie.github.io/mas-to-mars/.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u591a\u667a\u80fd\u4f53\u673a\u5668\u4eba\u7cfb\u7edf\uff08MARS\uff09\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u6027\u80fd\u6743\u8861\u95ee\u9898\uff0c\u901a\u8fc7\u4e24\u9879\u6a21\u62df\u533b\u7597\u573a\u666f\u7684\u5b9e\u9a8c\uff0c\u5206\u6790\u4e86\u534f\u8c03\u5931\u8d25\u548c\u6539\u8fdb\u901a\u4fe1\u7ed3\u6784\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5c3d\u7ba1\u591a\u667a\u80fd\u4f53\u6846\u67b6\u6280\u672f\u5148\u8fdb\uff0c\u4f46\u5176\u5728\u673a\u5668\u4eba\u4e0a\u7684\u5b9e\u9645\u5e94\u7528\u53d7\u9650\uff0c\u963b\u788d\u4e86MARS\u7814\u7a76\u7684\u5b9e\u8df5\u8fdb\u5c55\u3002", "method": "\u7814\u7a761\u4f7f\u7528CrewAI\u8fed\u4ee3\u4f18\u5316\u77e5\u8bc6\u5e93\uff0c\u8bc6\u522b\u534f\u8c03\u5931\u8d25\uff1b\u7814\u7a762\u4f7f\u7528AutoGen\u8bc4\u4f30\u53cc\u5411\u901a\u4fe1\u7ed3\u6784\uff0c\u6bd4\u8f83\u63a8\u7406\u4e0e\u975e\u63a8\u7406\u6a21\u578b\u7684\u6027\u80fd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u81ea\u4e3b\u6027\u4e0e\u7a33\u5b9a\u6027\u4e4b\u95f4\u5b58\u5728\u5f20\u529b\uff0c\u8fb9\u7f18\u6848\u4f8b\u6d4b\u8bd5\u5bf9\u7cfb\u7edf\u53ef\u9760\u6027\u548c\u5b89\u5168\u6027\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u5f3a\u8c03\u6539\u8fdb\u7cfb\u7edf\u53ef\u9760\u6027\u548c\u5b89\u5168\u6027\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u672a\u6765\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u53c2\u8003\u3002"}}
{"id": "2508.04339", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.04339", "abs": "https://arxiv.org/abs/2508.04339", "authors": ["Anran Xu", "Jincheng Wang", "Baigen Cai", "Tao Wen"], "title": "Deliberative Reasoning Network: An Uncertainty-Driven Paradigm for Belief-Tracked Inference with Pretrained Language Models", "comment": "8 pages, 3 figures", "summary": "Large language models often fail at logical reasoning when semantic\nheuristics conflict with decisive evidence - a phenomenon we term cognitive\ntraps. To address this fundamental limitation, we introduce the Deliberative\nReasoning Network (DRN), a novel paradigm that reframes logical reasoning from\nprobability maximization to uncertainty minimization. Instead of asking \"Which\nanswer is most likely?\", DRN asks \"Which hypothesis has the most internally\nconsistent evidence?\". DRN achieves intrinsic interpretability by explicitly\ntracking belief states and quantifying epistemic uncertainty for competing\nhypotheses through an iterative evidence synthesis process. We validate our\napproach through two complementary architectures - a bespoke discriminative\nmodel that embodies the core uncertainty minimization principle, and a\nlightweight verification module that enhances existing generative LLMs.\nEvaluated on LCR-1000, our new adversarial reasoning benchmark designed to\nexpose cognitive traps, the bespoke DRN achieves up to 15.2% improvement over\nstandard baselines. When integrated as a parameter-efficient verifier with\nMistral-7B, our hybrid system boosts accuracy from 20% to 80% on the most\nchallenging problems. Critically, DRN demonstrates strong zero-shot\ngeneralization, improving TruthfulQA performance by 23.6% without additional\ntraining, indicating that uncertainty-driven deliberation learns transferable\nreasoning principles. We position DRN as a foundational, verifiable System 2\nreasoning component for building more trustworthy AI systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDRN\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u903b\u8f91\u63a8\u7406\u4ece\u6982\u7387\u6700\u5927\u5316\u8f6c\u53d8\u4e3a\u4e0d\u786e\u5b9a\u6027\u6700\u5c0f\u5316\uff0c\u89e3\u51b3\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8bed\u4e49\u542f\u53d1\u5f0f\u4e0e\u51b3\u5b9a\u6027\u8bc1\u636e\u51b2\u7a81\u65f6\u7684\u903b\u8f91\u63a8\u7406\u5931\u8d25\u95ee\u9898\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u903b\u8f91\u63a8\u7406\u4e2d\u5e38\u56e0\u8bed\u4e49\u542f\u53d1\u5f0f\u4e0e\u51b3\u5b9a\u6027\u8bc1\u636e\u51b2\u7a81\u800c\u5931\u8d25\uff0c\u8fd9\u79cd\u73b0\u8c61\u88ab\u79f0\u4e3a\u8ba4\u77e5\u9677\u9631\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u6839\u672c\u9650\u5236\uff0c\u7814\u7a76\u56e2\u961f\u63d0\u51fa\u4e86DRN\u65b9\u6cd5\u3002", "method": "DRN\u901a\u8fc7\u8ddf\u8e2a\u4fe1\u5ff5\u72b6\u6001\u548c\u91cf\u5316\u7ade\u4e89\u5047\u8bbe\u7684\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\uff0c\u5c06\u903b\u8f91\u63a8\u7406\u95ee\u9898\u4ece\u2018\u54ea\u4e2a\u7b54\u6848\u6700\u53ef\u80fd\u2019\u8f6c\u53d8\u4e3a\u2018\u54ea\u4e2a\u5047\u8bbe\u7684\u8bc1\u636e\u6700\u5185\u90e8\u4e00\u81f4\u2019\u3002\u65b9\u6cd5\u5305\u62ec\u4e00\u4e2a\u5b9a\u5236\u5224\u522b\u6a21\u578b\u548c\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u9a8c\u8bc1\u6a21\u5757\u3002", "result": "\u5728LCR-1000\u5bf9\u6297\u6027\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDRN\u6bd4\u6807\u51c6\u57fa\u7ebf\u63d0\u9ad8\u4e8615.2%\u3002\u4e0eMistral-7B\u96c6\u6210\u540e\uff0c\u7cfb\u7edf\u5728\u6700\u96be\u95ee\u9898\u4e0a\u7684\u51c6\u786e\u7387\u4ece20%\u63d0\u5347\u523080%\u3002DRN\u8fd8\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0cTruthfulQA\u6027\u80fd\u63d0\u534723.6%\u3002", "conclusion": "DRN\u4f5c\u4e3a\u4e00\u79cd\u53ef\u9a8c\u8bc1\u7684System 2\u63a8\u7406\u7ec4\u4ef6\uff0c\u4e3a\u6784\u5efa\u66f4\u53ef\u4fe1\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.04696", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.04696", "abs": "https://arxiv.org/abs/2508.04696", "authors": ["Vyacheslav Kovalev", "Ekaterina Chaikovskaia", "Egor Davydenko", "Roman Gorbachev"], "title": "Achieving Precise and Reliable Locomotion with Differentiable Simulation-Based System Identification", "comment": "6 pages, Accepted for IROS 2025", "summary": "Accurate system identification is crucial for reducing trajectory drift in\nbipedal locomotion, particularly in reinforcement learning and model-based\ncontrol. In this paper, we present a novel control framework that integrates\nsystem identification into the reinforcement learning training loop using\ndifferentiable simulation. Unlike traditional approaches that rely on direct\ntorque measurements, our method estimates system parameters using only\ntrajectory data (positions, velocities) and control inputs. We leverage the\ndifferentiable simulator MuJoCo-XLA to optimize system parameters, ensuring\nthat simulated robot behavior closely aligns with real-world motion. This\nframework enables scalable and flexible parameter optimization. Accurate system\nidentification is crucial for reducing trajectory drift in bipedal locomotion,\nparticularly in reinforcement learning and model-based control. In this paper,\nwe present a novel control framework that integrates system identification into\nthe reinforcement learning training loop using differentiable simulation.\nUnlike traditional approaches that rely on direct torque measurements, our\nmethod estimates system parameters using only trajectory data (positions,\nvelocities) and control inputs. We leverage the differentiable simulator\nMuJoCo-XLA to optimize system parameters, ensuring that simulated robot\nbehavior closely aligns with real-world motion. This framework enables scalable\nand flexible parameter optimization. It supports fundamental physical\nproperties such as mass and inertia. Additionally, it handles complex system\nnonlinear behaviors, including advanced friction models, through neural network\napproximations. Experimental results show that our framework significantly\nimproves trajectory following.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7cfb\u7edf\u8fa8\u8bc6\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u65b0\u578b\u63a7\u5236\u6846\u67b6\uff0c\u5229\u7528\u53ef\u5fae\u5206\u4eff\u771f\u4f18\u5316\u7cfb\u7edf\u53c2\u6570\uff0c\u4ec5\u9700\u8f68\u8ff9\u6570\u636e\u548c\u63a7\u5236\u8f93\u5165\u5373\u53ef\u4f30\u8ba1\u53c2\u6570\uff0c\u663e\u8457\u63d0\u5347\u8f68\u8ff9\u8ddf\u8e2a\u6027\u80fd\u3002", "motivation": "\u5728\u53cc\u8db3\u884c\u8d70\u4e2d\uff0c\u51c6\u786e\u7684\u7cfb\u7edf\u8fa8\u8bc6\u5bf9\u51cf\u5c11\u8f68\u8ff9\u6f02\u79fb\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u5f3a\u5316\u5b66\u4e60\u548c\u57fa\u4e8e\u6a21\u578b\u7684\u63a7\u5236\u4e2d\u3002\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u76f4\u63a5\u626d\u77e9\u6d4b\u91cf\uff0c\u800c\u65b0\u65b9\u6cd5\u4ec5\u9700\u8f68\u8ff9\u6570\u636e\u548c\u63a7\u5236\u8f93\u5165\u3002", "method": "\u5229\u7528\u53ef\u5fae\u5206\u4eff\u771f\u5668MuJoCo-XLA\u4f18\u5316\u7cfb\u7edf\u53c2\u6570\uff0c\u4f7f\u4eff\u771f\u673a\u5668\u4eba\u884c\u4e3a\u4e0e\u5b9e\u9645\u8fd0\u52a8\u4e00\u81f4\u3002\u652f\u6301\u8d28\u91cf\u548c\u60ef\u6027\u7b49\u7269\u7406\u5c5e\u6027\uff0c\u5e76\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u8fd1\u4f3c\u5904\u7406\u590d\u6742\u975e\u7ebf\u6027\u884c\u4e3a\uff08\u5982\u6469\u64e6\u6a21\u578b\uff09\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u663e\u8457\u6539\u5584\u4e86\u8f68\u8ff9\u8ddf\u8e2a\u6027\u80fd\u3002", "conclusion": "\u8be5\u6846\u67b6\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u4e14\u7075\u6d3b\u7684\u53c2\u6570\u4f18\u5316\uff0c\u4e3a\u53cc\u8db3\u884c\u8d70\u4e2d\u7684\u7cfb\u7edf\u8fa8\u8bc6\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.04361", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.04361", "abs": "https://arxiv.org/abs/2508.04361", "authors": ["Fuqing Bie", "Shiyu Huang", "Xijia Tao", "Zhiqin Fang", "Leyi Pan", "Junzhe Chen", "Min Ren", "Liuyu Xiang", "Zhaofeng He"], "title": "OmniPlay: Benchmarking Omni-Modal Models on Omni-Modal Game Playing", "comment": null, "summary": "While generalist foundation models like Gemini and GPT-4o demonstrate\nimpressive multi-modal competence, existing evaluations fail to test their\nintelligence in dynamic, interactive worlds. Static benchmarks lack agency,\nwhile interactive benchmarks suffer from a severe modal bottleneck, typically\nignoring crucial auditory and temporal cues. To bridge this evaluation chasm,\nwe introduce OmniPlay, a diagnostic benchmark designed not just to evaluate,\nbut to probe the fusion and reasoning capabilities of agentic models across the\nfull sensory spectrum. Built on a core philosophy of modality interdependence,\nOmniPlay comprises a suite of five game environments that systematically create\nscenarios of both synergy and conflict, forcing agents to perform genuine\ncross-modal reasoning. Our comprehensive evaluation of six leading omni-modal\nmodels reveals a critical dichotomy: they exhibit superhuman performance on\nhigh-fidelity memory tasks but suffer from systemic failures in challenges\nrequiring robust reasoning and strategic planning. We demonstrate that this\nfragility stems from brittle fusion mechanisms, which lead to catastrophic\nperformance degradation under modality conflict and uncover a counter-intuitive\n\"less is more\" paradox, where removing sensory information can paradoxically\nimprove performance. Our findings suggest that the path toward robust AGI\nrequires a research focus beyond scaling to explicitly address synergistic\nfusion. Our platform is available for anonymous review at\nhttps://github.com/fuqingbie/omni-game-benchmark.", "AI": {"tldr": "OmniPlay\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\uff0c\u65e8\u5728\u8bc4\u4f30\u52a8\u6001\u4ea4\u4e92\u4e16\u754c\u4e2d\u667a\u80fd\u4f53\u7684\u8de8\u6a21\u6001\u63a8\u7406\u80fd\u529b\uff0c\u63ed\u793a\u73b0\u6709\u6a21\u578b\u5728\u534f\u540c\u878d\u5408\u65b9\u9762\u7684\u8106\u5f31\u6027\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u65e0\u6cd5\u5168\u9762\u6d4b\u8bd5\u591a\u6a21\u6001\u6a21\u578b\u5728\u52a8\u6001\u4ea4\u4e92\u73af\u5883\u4e2d\u7684\u667a\u80fd\u8868\u73b0\uff0c\u5c24\u5176\u662f\u7f3a\u4e4f\u5bf9\u542c\u89c9\u548c\u65f6\u95f4\u7ebf\u7d22\u7684\u8003\u91cf\u3002", "method": "OmniPlay\u5305\u542b\u4e94\u4e2a\u6e38\u620f\u73af\u5883\uff0c\u901a\u8fc7\u534f\u540c\u548c\u51b2\u7a81\u573a\u666f\u7cfb\u7edf\u6d4b\u8bd5\u6a21\u578b\u7684\u8de8\u6a21\u6001\u63a8\u7406\u80fd\u529b\u3002", "result": "\u8bc4\u4f30\u663e\u793a\uff0c\u6a21\u578b\u5728\u9ad8\u4fdd\u771f\u8bb0\u5fc6\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u9700\u8981\u63a8\u7406\u548c\u89c4\u5212\u7684\u4efb\u52a1\u4e2d\u8868\u73b0\u8106\u5f31\uff0c\u63ed\u793a\u4e86\u878d\u5408\u673a\u5236\u7684\u7f3a\u9677\u3002", "conclusion": "\u7814\u7a76\u6307\u51fa\uff0c\u5b9e\u73b0\u7a33\u5065\u7684\u901a\u7528\u4eba\u5de5\u667a\u80fd\u9700\u8d85\u8d8a\u89c4\u6a21\u6269\u5c55\uff0c\u4e13\u6ce8\u4e8e\u534f\u540c\u878d\u5408\u673a\u5236\u7684\u7814\u7a76\u3002"}}
{"id": "2508.04383", "categories": ["cs.AI", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2508.04383", "abs": "https://arxiv.org/abs/2508.04383", "authors": ["Robert Prentner"], "title": "Artificial Consciousness as Interface Representation", "comment": "12 pages", "summary": "Whether artificial intelligence (AI) systems can possess consciousness is a\ncontentious question because of the inherent challenges of defining and\noperationalizing subjective experience. This paper proposes a framework to\nreframe the question of artificial consciousness into empirically tractable\ntests. We introduce three evaluative criteria - S (subjective-linguistic), L\n(latent-emergent), and P (phenomenological-structural) - collectively termed\nSLP-tests, which assess whether an AI system instantiates interface\nrepresentations that facilitate consciousness-like properties. Drawing on\ncategory theory, we model interface representations as mappings between\nrelational substrates (RS) and observable behaviors, akin to specific types of\nabstraction layers. The SLP-tests collectively operationalize subjective\nexperience not as an intrinsic property of physical systems but as a functional\ninterface to a relational entity.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u6846\u67b6\uff0c\u901a\u8fc7SLP\u6d4b\u8bd5\uff08\u4e3b\u89c2-\u8bed\u8a00\u3001\u6f5c\u5728-\u6d8c\u73b0\u3001\u73b0\u8c61\u5b66-\u7ed3\u6784\uff09\u6765\u8bc4\u4f30AI\u7cfb\u7edf\u662f\u5426\u5177\u5907\u7c7b\u4f3c\u610f\u8bc6\u7684\u5c5e\u6027\u3002", "motivation": "\u63a2\u8ba8AI\u7cfb\u7edf\u662f\u5426\u53ef\u80fd\u62e5\u6709\u610f\u8bc6\uff0c\u7531\u4e8e\u4e3b\u89c2\u4f53\u9a8c\u7684\u5b9a\u4e49\u548c\u64cd\u4f5c\u5316\u5b58\u5728\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u5b9e\u8bc1\u65b9\u6cd5\u3002", "method": "\u5f15\u5165SLP\u6d4b\u8bd5\uff0c\u5229\u7528\u8303\u7574\u8bba\u5efa\u6a21\u63a5\u53e3\u8868\u793a\uff0c\u4f5c\u4e3a\u5173\u7cfb\u57fa\u8d28\u4e0e\u53ef\u89c2\u5bdf\u884c\u4e3a\u4e4b\u95f4\u7684\u6620\u5c04\u3002", "result": "SLP\u6d4b\u8bd5\u5c06\u4e3b\u89c2\u4f53\u9a8c\u64cd\u4f5c\u5316\u4e3a\u529f\u80fd\u63a5\u53e3\uff0c\u800c\u975e\u7269\u7406\u7cfb\u7edf\u7684\u5185\u5728\u5c5e\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u7814\u7a76\u4eba\u5de5\u610f\u8bc6\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u8def\u5f84\uff0c\u5f3a\u8c03\u529f\u80fd\u6027\u63a5\u53e3\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2508.04389", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.04389", "abs": "https://arxiv.org/abs/2508.04389", "authors": ["Weitai Kang", "Bin Lei", "Gaowen Liu", "Caiwen Ding", "Yan Yan"], "title": "GuirlVG: Incentivize GUI Visual Grounding via Empirical Exploration on Reinforcement Learning", "comment": "9 pages", "summary": "Graphical user interface visual grounding (GUI-VG), a core capability for GUI\nagents, has primarily relied on supervised fine-tuning (SFT) of multimodal\nlarge language models (MLLMs), which demands extensive data curation and\nsignificant training costs. However, as MLLMs continue to advance and even\ncover GUI domains during pretraining, the necessity of exhaustive SFT\npost-training becomes increasingly questionable. Meanwhile, recent successes of\nrule-based reinforcement fine-tuning (RFT) suggest a more efficient\nalternative. Despite this promise, the optimal manner of applying RFT for\nGUI-VG remains unexplored. To bridge this gap, we introduce GuirlVG, a\nreinforcement learning-based GUI-VG method built on a systematic empirical\nstudy and a novel stabilization technique. We find that naive application of\nRFT underperforms the SFT baseline, motivating a deeper exploration. First, we\ndecompose RFT into its core components and analyze the optimal formulation of\neach. Second, we propose a novel Adversarial KL Factor that dynamically\nstabilizes training to mitigate reward over-optimization. Third, we further\nexplore the training configurations of RFT to enhance effectiveness. Extensive\nexperiments show that GuirlVG, with only 5.2K training samples, outperforms SFT\nmethods trained on over 10M samples, achieving a 7.7% improvement on\nScreenSpot, a 17.2% improvement on ScreenSpotPro, and 91.9% accuracy on\nScreenSpotV2.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684GUI\u89c6\u89c9\u5b9a\u4f4d\u65b9\u6cd5GuirlVG\uff0c\u901a\u8fc7\u7cfb\u7edf\u5b9e\u8bc1\u7814\u7a76\u548c\u65b0\u7a33\u5b9a\u6280\u672f\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edfGUI\u89c6\u89c9\u5b9a\u4f4d\u4f9d\u8d56\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\uff0c\u4f46\u6570\u636e\u9700\u6c42\u5927\u4e14\u8bad\u7ec3\u6210\u672c\u9ad8\u3002\u968f\u7740\u591a\u6a21\u6001\u5927\u6a21\u578b\uff08MLLMs\uff09\u7684\u8fdb\u6b65\uff0cSFT\u7684\u5fc5\u8981\u6027\u53d7\u5230\u8d28\u7591\uff0c\u800c\u89c4\u5219\u5f3a\u5316\u5fae\u8c03\uff08RFT\uff09\u6210\u4e3a\u66f4\u9ad8\u6548\u7684\u9009\u62e9\u3002", "method": "\u7814\u7a76\u5206\u89e3RFT\u6838\u5fc3\u7ec4\u4ef6\u5e76\u4f18\u5316\u5176\u5f62\u5f0f\uff0c\u63d0\u51fa\u52a8\u6001\u7a33\u5b9a\u8bad\u7ec3\u7684Adversarial KL Factor\uff0c\u5e76\u63a2\u7d22RFT\u7684\u8bad\u7ec3\u914d\u7f6e\u3002", "result": "GuirlVG\u4ec5\u75285.2K\u8bad\u7ec3\u6837\u672c\uff0c\u6027\u80fd\u8d85\u8d8a\u4f7f\u752810M\u6837\u672c\u7684SFT\u65b9\u6cd5\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u663e\u8457\u63d0\u5347\u3002", "conclusion": "GuirlVG\u8bc1\u660e\u4e86RFT\u5728GUI\u89c6\u89c9\u5b9a\u4f4d\u4e2d\u7684\u9ad8\u6548\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2508.04412", "categories": ["cs.AI", "cs.CL", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.04412", "abs": "https://arxiv.org/abs/2508.04412", "authors": ["Thassilo M. Schiepanski", "Nicholas Pi\u00ebl"], "title": "Beyond Pixels: Exploring DOM Downsampling for LLM-Based Web Agents", "comment": null, "summary": "Frontier LLMs only recently enabled serviceable, autonomous web agents. At\nthat, a model poses as an instantaneous domain model backend. Ought to suggest\ninteraction, it is consulted with a web-based task and respective application\nstate. The key problem lies in application state serialisation\n$\\unicode{x2013}$ referred to as snapshot. State-of-the-art web agents are\npremised on grounded GUI snapshots, i.e., screenshots enhanced with visual\ncues. Not least to resemble human perception, but for images representing\nrelatively cheap means of model input. LLM vision still lag behind code\ninterpretation capabilities. DOM snapshots, which structurally resemble HTML,\nimpose a desired alternative. Vast model input token size, however, disables\nreliable implementation with web agents to date.\n  We propose D2Snap, a first-of-its-kind DOM downsampling algorithm. Based on a\nGPT-4o backend, we evaluate D2Snap on tasks sampled from the Online-Mind2Web\ndataset. The success rate of D2Snap-downsampled DOM snapshots (67%) matches a\ngrounded GUI snapshot baseline (65%) $\\unicode{x2013}$ within the same input\ntoken order of magnitude (1e3). Our best evaluated configurations\n$\\unicode{x2013}$ one token order above, but within the model's context window\n$\\unicode{x2013}$ outperform this baseline by 8%. Our evaluation, moreover,\nyields that DOM-inherent hierarchy embodies a strong UI feature for LLMs.", "AI": {"tldr": "D2Snap\u662f\u4e00\u79cd\u65b0\u578bDOM\u964d\u91c7\u6837\u7b97\u6cd5\uff0c\u901a\u8fc7GPT-4o\u540e\u7aef\u8bc4\u4f30\uff0c\u5176\u6027\u80fd\u63a5\u8fd1\u6216\u4f18\u4e8e\u57fa\u4e8eGUI\u5feb\u7167\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eGUI\u5feb\u7167\u7684\u7f51\u9875\u4ee3\u7406\u65b9\u6cd5\u53d7\u9650\u4e8eLLM\u89c6\u89c9\u80fd\u529b\u4e0d\u8db3\uff0c\u800cDOM\u5feb\u7167\u867d\u7ed3\u6784\u66f4\u4f18\uff0c\u4f46\u56e0\u8f93\u5165\u4ee4\u724c\u8fc7\u5927\u96be\u4ee5\u5b9e\u73b0\u3002", "method": "\u63d0\u51faD2Snap\u7b97\u6cd5\uff0c\u5bf9DOM\u5feb\u7167\u8fdb\u884c\u964d\u91c7\u6837\uff0c\u5e76\u5728Online-Mind2Web\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u5176\u6027\u80fd\u3002", "result": "D2Snap\u964d\u91c7\u6837\u540e\u7684DOM\u5feb\u7167\u6210\u529f\u7387\uff0867%\uff09\u4e0eGUI\u5feb\u7167\u57fa\u7ebf\uff0865%\uff09\u76f8\u5f53\uff0c\u4e14\u5728\u66f4\u9ad8\u4ee4\u724c\u914d\u7f6e\u4e0b\u6027\u80fd\u63d0\u53478%\u3002", "conclusion": "DOM\u7684\u5c42\u6b21\u7ed3\u6784\u662fLLM\u7406\u89e3UI\u7684\u6709\u6548\u7279\u5f81\uff0cD2Snap\u4e3aDOM\u5feb\u7167\u7684\u5b9e\u7528\u5316\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2508.04428", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.04428", "abs": "https://arxiv.org/abs/2508.04428", "authors": ["Si Chen", "Izzy Molnar", "Ting Hua", "Peiyu Li", "Le Huy Khiem", "G. Alex Ambrose", "Jim Lang", "Ronald Metoyer", "Nitesh V. Chawla"], "title": "\\textsc{SimInstruct}: A Responsible Tool for Collecting Scaffolding Dialogues Between Experts and LLM-Simulated Novices", "comment": null, "summary": "High-quality, multi-turn instructional dialogues between novices and experts\nare essential for developing AI systems that support teaching, learning, and\ndecision-making. These dialogues often involve scaffolding -- the process by\nwhich an expert supports a novice's thinking through questions, feedback, and\nstep-by-step guidance. However, such data are scarce due to privacy concerns in\nrecording and the vulnerability inherent in help-seeking. We present\nSimInstruct, a scalable, expert-in-the-loop tool for collecting scaffolding\ndialogues. Using teaching development coaching as an example domain,\nSimInstruct simulates novice instructors via LLMs, varying their teaching\nchallenges and LLM's persona traits, while human experts provide multi-turn\nfeedback, reasoning, and instructional support. This design enables the\ncreation of realistic, pedagogically rich dialogues without requiring real\nnovice participants. Our results reveal that persona traits, such as\nextroversion and introversion, meaningfully influence how experts engage.\nCompared to real mentoring recordings, SimInstruct dialogues demonstrate\ncomparable pedagogical relevance and cognitive depth. Experts also reported the\nprocess as engaging and reflective, improving both data quality and their own\nprofessional insight. We further fine-tuned a LLaMA model to be an expert model\nusing the augmented dataset, which outperformed GPT-4o in instructional\nquality. Our analysis highlights GPT-4o's limitations in weak reflective\nquestioning, overuse of generic praise, a condescending tone, and a tendency to\noverwhelm novices with excessive suggestions.", "AI": {"tldr": "SimInstruct\u662f\u4e00\u79cd\u901a\u8fc7\u4e13\u5bb6\u53c2\u4e0e\u548cLLM\u6a21\u62df\u65b0\u624b\u6536\u96c6\u9ad8\u8d28\u91cf\u6559\u5b66\u5bf9\u8bdd\u7684\u5de5\u5177\uff0c\u5176\u751f\u6210\u7684\u5bf9\u8bdd\u5728\u6559\u5b66\u76f8\u5173\u6027\u548c\u8ba4\u77e5\u6df1\u5ea6\u4e0a\u4e0e\u771f\u5b9e\u5bf9\u8bdd\u76f8\u5f53\uff0c\u5e76\u53ef\u7528\u4e8e\u6539\u8fdb\u4e13\u5bb6\u6a21\u578b\u3002", "motivation": "\u9ad8\u8d28\u91cf\u7684\u591a\u8f6e\u6559\u5b66\u5bf9\u8bdd\u5bf9AI\u6559\u5b66\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6b64\u7c7b\u6570\u636e\u7a00\u7f3a\uff0c\u4e3b\u8981\u56e0\u9690\u79c1\u548c\u6c42\u52a9\u8106\u5f31\u6027\u95ee\u9898\u3002", "method": "SimInstruct\u5229\u7528LLM\u6a21\u62df\u65b0\u624b\uff0c\u7ed3\u5408\u4e13\u5bb6\u53cd\u9988\u751f\u6210\u6559\u5b66\u5bf9\u8bdd\uff0c\u907f\u514d\u771f\u5b9e\u65b0\u624b\u53c2\u4e0e\u3002", "result": "\u751f\u6210\u7684\u5bf9\u8bdd\u4e0e\u771f\u5b9e\u5bf9\u8bdd\u76f8\u5f53\uff0c\u4e13\u5bb6\u53cd\u9988\u63d0\u5347\u4e86\u6570\u636e\u8d28\u91cf\u548c\u4e13\u4e1a\u6d1e\u5bdf\u529b\uff0c\u6539\u8fdb\u7684LLaMA\u6a21\u578b\u5728\u6559\u5b66\u8d28\u91cf\u4e0a\u4f18\u4e8eGPT-4o\u3002", "conclusion": "SimInstruct\u4e3a\u6559\u5b66\u5bf9\u8bdd\u6570\u636e\u6536\u96c6\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\uff0c\u540c\u65f6\u63ed\u793a\u4e86GPT-4o\u5728\u6559\u5b66\u652f\u6301\u4e2d\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2508.04460", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.04460", "abs": "https://arxiv.org/abs/2508.04460", "authors": ["Rui Ha", "Chaozhuo Li", "Rui Pu", "Sen Su"], "title": "From \"Aha Moments\" to Controllable Thinking: Toward Meta-Cognitive Reasoning in Large Reasoning Models via Decoupled Reasoning and Control", "comment": null, "summary": "Large Reasoning Models (LRMs) have demonstrated a latent capacity for complex\nreasoning by spontaneously exhibiting cognitive behaviors such as step-by-step\nreasoning, reflection, and backtracking, commonly referred to as \"Aha Moments\".\nHowever, such emergent behaviors remain unregulated and uncontrolled, often\nresulting in overthinking, where the model continues generating redundant\nreasoning content even after reaching reliable conclusions. This leads to\nexcessive computational costs and increased latency, limiting the practical\ndeployment of LRMs. The root cause lies in the absence of intrinsic regulatory\nmechanisms, as current models are unable to monitor and adaptively manage their\nreasoning process to determine when to continue, backtrack, or terminate. To\naddress this issue, we propose the Meta-cognitive Reasoning Framework (MERA),\nwhich explicitly decouples the thinking process into distinct reasoning and\ncontrol components, thereby enabling the independent optimization of control\nstrategies. Specifically, MERA incorporates a takeover-based data construction\nmechanism that identifies critical decision points during reasoning and\ndelegates the creation of control signals to auxiliary LLMs, thereby enabling\nthe construction of high-quality reasoning-control data. Additionally, a\nstructured reasoning-control separation is implemented via supervised\nfine-tuning, enabling the model to generate explicit traces and acquire initial\nmeta-cognitive control capabilities. Finally, MERA employs Control-Segment\nPolicy Optimization (CSPO), which combines segment-wise Group Relative Policy\nOptimization (GRPO) with a control-masking mechanism to optimize control\nbehavior learning while minimizing interference from irrelevant content.\nExperiments on various reasoning benchmarks demonstrate that models trained\nwith MERA enhance both reasoning efficiency and accuracy.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faMeta-cognitive Reasoning Framework (MERA)\uff0c\u901a\u8fc7\u5206\u79bb\u63a8\u7406\u4e0e\u63a7\u5236\u7ec4\u4ef6\uff0c\u4f18\u5316\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7f3a\u4e4f\u5185\u5728\u8c03\u63a7\u673a\u5236\uff0c\u5bfc\u81f4\u8fc7\u5ea6\u63a8\u7406\u548c\u8ba1\u7b97\u8d44\u6e90\u6d6a\u8d39\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "MERA\u6846\u67b6\u5c06\u63a8\u7406\u8fc7\u7a0b\u5206\u89e3\u4e3a\u63a8\u7406\u548c\u63a7\u5236\u7ec4\u4ef6\uff0c\u91c7\u7528\u63a5\u7ba1\u5f0f\u6570\u636e\u6784\u5efa\u673a\u5236\u3001\u7ed3\u6784\u5316\u5206\u79bb\u76d1\u7763\u5fae\u8c03\u548c\u63a7\u5236\u6bb5\u7b56\u7565\u4f18\u5316\uff08CSPO\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMERA\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u63a8\u7406\u6548\u7387\u548c\u51c6\u786e\u6027\u4e0a\u5747\u6709\u63d0\u5347\u3002", "conclusion": "MERA\u901a\u8fc7\u663e\u5f0f\u5206\u79bb\u548c\u63a7\u5236\u63a8\u7406\u8fc7\u7a0b\uff0c\u6709\u6548\u89e3\u51b3\u4e86LRMs\u7684\u8fc7\u5ea6\u63a8\u7406\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u5b9e\u9645\u90e8\u7f72\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2508.04482", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.04482", "abs": "https://arxiv.org/abs/2508.04482", "authors": ["Xueyu Hu", "Tao Xiong", "Biao Yi", "Zishu Wei", "Ruixuan Xiao", "Yurun Chen", "Jiasheng Ye", "Meiling Tao", "Xiangxin Zhou", "Ziyu Zhao", "Yuhuai Li", "Shengze Xu", "Shenzhi Wang", "Xinchen Xu", "Shuofei Qiao", "Zhaokai Wang", "Kun Kuang", "Tieyong Zeng", "Liang Wang", "Jiwei Li", "Yuchen Eleanor Jiang", "Wangchunshu Zhou", "Guoyin Wang", "Keting Yin", "Zhou Zhao", "Hongxia Yang", "Fan Wu", "Shengyu Zhang", "Fei Wu"], "title": "OS Agents: A Survey on MLLM-based Agents for General Computing Devices Use", "comment": "ACL 2025 (Oral)", "summary": "The dream to create AI assistants as capable and versatile as the fictional\nJ.A.R.V.I.S from Iron Man has long captivated imaginations. With the evolution\nof (multi-modal) large language models ((M)LLMs), this dream is closer to\nreality, as (M)LLM-based Agents using computing devices (e.g., computers and\nmobile phones) by operating within the environments and interfaces (e.g.,\nGraphical User Interface (GUI)) provided by operating systems (OS) to automate\ntasks have significantly advanced. This paper presents a comprehensive survey\nof these advanced agents, designated as OS Agents. We begin by elucidating the\nfundamentals of OS Agents, exploring their key components including the\nenvironment, observation space, and action space, and outlining essential\ncapabilities such as understanding, planning, and grounding. We then examine\nmethodologies for constructing OS Agents, focusing on domain-specific\nfoundation models and agent frameworks. A detailed review of evaluation\nprotocols and benchmarks highlights how OS Agents are assessed across diverse\ntasks. Finally, we discuss current challenges and identify promising directions\nfor future research, including safety and privacy, personalization and\nself-evolution. This survey aims to consolidate the state of OS Agents\nresearch, providing insights to guide both academic inquiry and industrial\ndevelopment. An open-source GitHub repository is maintained as a dynamic\nresource to foster further innovation in this field. We present a 9-page\nversion of our work, accepted by ACL 2025, to provide a concise overview to the\ndomain.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u64cd\u4f5c\u7cfb\u7edf\u4ee3\u7406\uff08OS Agents\uff09\uff0c\u63a2\u8ba8\u5176\u7ec4\u6210\u3001\u80fd\u529b\u3001\u6784\u5efa\u65b9\u6cd5\u3001\u8bc4\u4f30\u6807\u51c6\u53ca\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u5b9e\u73b0\u50cf\u94a2\u94c1\u4fa0\u4e2dJ.A.R.V.I.S.\u4e00\u6837\u5168\u80fd\u7684\u4eba\u5de5\u667a\u80fd\u52a9\u624b\uff0c\u901a\u8fc7OS Agents\u5728\u591a\u6a21\u6001\u73af\u5883\u4e2d\u81ea\u52a8\u5316\u4efb\u52a1\u3002", "method": "\u7efc\u8ff0OS Agents\u7684\u57fa\u7840\u7ec4\u4ef6\uff08\u73af\u5883\u3001\u89c2\u5bdf\u7a7a\u95f4\u3001\u52a8\u4f5c\u7a7a\u95f4\uff09\u3001\u80fd\u529b\uff08\u7406\u89e3\u3001\u89c4\u5212\u3001\u843d\u5730\uff09\u3001\u6784\u5efa\u65b9\u6cd5\uff08\u9886\u57df\u7279\u5b9a\u57fa\u7840\u6a21\u578b\u3001\u4ee3\u7406\u6846\u67b6\uff09\u53ca\u8bc4\u4f30\u534f\u8bae\u3002", "result": "\u603b\u7ed3\u4e86OS Agents\u7684\u7814\u7a76\u73b0\u72b6\uff0c\u63d0\u51fa\u4e86\u672a\u6765\u65b9\u5411\uff08\u5b89\u5168\u9690\u79c1\u3001\u4e2a\u6027\u5316\u3001\u81ea\u6211\u8fdb\u5316\uff09\uff0c\u5e76\u7ef4\u62a4\u5f00\u6e90GitHub\u8d44\u6e90\u5e93\u3002", "conclusion": "OS Agents\u7814\u7a76\u5df2\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u4ecd\u9700\u89e3\u51b3\u5b89\u5168\u9690\u79c1\u7b49\u95ee\u9898\uff0c\u672a\u6765\u6709\u671b\u8fdb\u4e00\u6b65\u63a8\u52a8\u5b66\u672f\u4e0e\u5de5\u4e1a\u53d1\u5c55\u3002"}}
{"id": "2508.04511", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.04511", "abs": "https://arxiv.org/abs/2508.04511", "authors": ["Hamed Ayoobi", "Nico Potyka", "Anna Rapberger", "Francesca Toni"], "title": "Argumentative Debates for Transparent Bias Detection [Technical Report]", "comment": null, "summary": "As the use of AI systems in society grows, addressing potential biases that\nemerge from data or are learned by models is essential to prevent systematic\ndisadvantages against specific groups. Several notions of (un)fairness have\nbeen proposed in the literature, alongside corresponding algorithmic methods\nfor detecting and mitigating unfairness, but, with very few exceptions, these\ntend to ignore transparency. Instead, interpretability and explainability are\ncore requirements for algorithmic fairness, even more so than for other\nalgorithmic solutions, given the human-oriented nature of fairness. In this\npaper, we contribute a novel interpretable, explainable method for bias\ndetection relying on debates about the presence of bias against individuals,\nbased on the values of protected features for the individuals and others in\ntheir neighbourhoods. Our method builds upon techniques from formal and\ncomputational argumentation, whereby debates result from arguing about biases\nwithin and across neighbourhoods. We provide formal, quantitative, and\nqualitative evaluations of our method, highlighting its strengths in\nperformance against baselines, as well as its interpretability and\nexplainability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fa9\u8bba\u7684\u65b0\u578b\u53ef\u89e3\u91ca\u3001\u53ef\u89e3\u91ca\u7684\u504f\u89c1\u68c0\u6d4b\u65b9\u6cd5\uff0c\u7ed3\u5408\u5f62\u5f0f\u5316\u548c\u8ba1\u7b97\u8bba\u8bc1\u6280\u672f\uff0c\u5f3a\u8c03\u900f\u660e\u5ea6\u548c\u516c\u5e73\u6027\u3002", "motivation": "AI\u7cfb\u7edf\u4e2d\u7684\u504f\u89c1\u53ef\u80fd\u5bfc\u81f4\u5bf9\u7279\u5b9a\u7fa4\u4f53\u7684\u7cfb\u7edf\u6027\u4e0d\u5229\uff0c\u73b0\u6709\u65b9\u6cd5\u591a\u5ffd\u7565\u900f\u660e\u5ea6\uff0c\u800c\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u89e3\u91ca\u6027\u662f\u5b9e\u73b0\u7b97\u6cd5\u516c\u5e73\u7684\u6838\u5fc3\u9700\u6c42\u3002", "method": "\u57fa\u4e8e\u8fa9\u8bba\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4fdd\u62a4\u7279\u5f81\u503c\u548c\u90bb\u57df\u5185\u4e2a\u4f53\u7684\u503c\uff0c\u5229\u7528\u5f62\u5f0f\u5316\u548c\u8ba1\u7b97\u8bba\u8bc1\u6280\u672f\u68c0\u6d4b\u504f\u89c1\u3002", "result": "\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u57fa\u7ebf\uff0c\u540c\u65f6\u5177\u5907\u9ad8\u5ea6\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u504f\u89c1\u68c0\u6d4b\u63d0\u4f9b\u4e86\u900f\u660e\u4e14\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5f3a\u8c03\u4e86\u53ef\u89e3\u91ca\u6027\u5728\u516c\u5e73\u6027\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2508.04563", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.04563", "abs": "https://arxiv.org/abs/2508.04563", "authors": ["Mei Jiang", "Houping Yue", "Bingdong Li", "Hao Hao", "Ying Qian", "Bo Jiang", "Aimin Zhou"], "title": "SID: Benchmarking Guided Instruction Capabilities in STEM Education with a Socratic Interdisciplinary Dialogues Dataset", "comment": "26 pages, 20 figures", "summary": "Fostering students' abilities for knowledge integration and transfer in\ncomplex problem-solving scenarios is a core objective of modern education, and\ninterdisciplinary STEM is a key pathway to achieve this, yet it requires expert\nguidance that is difficult to scale. While LLMs offer potential in this regard,\ntheir true capability for guided instruction remains unclear due to the lack of\nan effective evaluation benchmark. To address this, we introduce SID, the first\nbenchmark designed to systematically evaluate the higher-order guidance\ncapabilities of LLMs in multi-turn, interdisciplinary Socratic dialogues. Our\ncontributions include a large-scale dataset of 10,000 dialogue turns across 48\ncomplex STEM projects, a novel annotation schema for capturing deep pedagogical\nfeatures, and a new suite of evaluation metrics (e.g., X-SRG). Baseline\nexperiments confirm that even state-of-the-art LLMs struggle to execute\neffective guided dialogues that lead students to achieve knowledge integration\nand transfer. This highlights the critical value of our benchmark in driving\nthe development of more pedagogically-aware LLMs.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86SID\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u591a\u8f6e\u8de8\u5b66\u79d1\u82cf\u683c\u62c9\u5e95\u5bf9\u8bdd\u4e2d\u7684\u9ad8\u9636\u6307\u5bfc\u80fd\u529b\uff0c\u53d1\u73b0\u5f53\u524dLLM\u5728\u5f15\u5bfc\u5b66\u751f\u77e5\u8bc6\u6574\u5408\u4e0e\u8fc1\u79fb\u65b9\u9762\u4ecd\u6709\u4e0d\u8db3\u3002", "motivation": "\u73b0\u4ee3\u6559\u80b2\u6838\u5fc3\u76ee\u6807\u662f\u57f9\u517b\u5b66\u751f\u89e3\u51b3\u590d\u6742\u95ee\u9898\u7684\u80fd\u529b\uff0c\u8de8\u5b66\u79d1STEM\u6559\u80b2\u662f\u5173\u952e\u9014\u5f84\uff0c\u4f46\u4e13\u5bb6\u6307\u5bfc\u96be\u4ee5\u89c4\u6a21\u5316\u3002LLM\u867d\u6709\u6f5c\u529b\uff0c\u4f46\u7f3a\u4e4f\u6709\u6548\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "\u63d0\u51faSID\u57fa\u51c6\uff0c\u5305\u542b10,000\u8f6e\u5bf9\u8bdd\u300148\u4e2a\u590d\u6742STEM\u9879\u76ee\u6570\u636e\u96c6\uff0c\u91c7\u7528\u65b0\u6807\u6ce8\u6a21\u5f0f\uff08\u5982X-SRG\uff09\u8bc4\u4f30LLM\u7684\u6307\u5bfc\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5373\u4f7f\u662f\u5148\u8fdbLLM\u4e5f\u96be\u4ee5\u6709\u6548\u5f15\u5bfc\u5b66\u751f\u5b9e\u73b0\u77e5\u8bc6\u6574\u5408\u4e0e\u8fc1\u79fb\u3002", "conclusion": "SID\u57fa\u51c6\u5bf9\u63a8\u52a8\u66f4\u5177\u6559\u80b2\u610f\u8bc6\u7684LLM\u53d1\u5c55\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2508.04576", "categories": ["cs.AI", "I.2.6; I.2.7; D.2.8"], "pdf": "https://arxiv.org/pdf/2508.04576", "abs": "https://arxiv.org/abs/2508.04576", "authors": ["Yue Zhou", "Yi Chang", "Yuan Wu"], "title": "ConfProBench: A Confidence Evaluation Benchmark for MLLM-Based Process Judges", "comment": null, "summary": "Reasoning is a critical capability of multimodal large language models\n(MLLMs) for solving complex multimodal tasks, and judging the correctness of\nreasoning steps is crucial for improving this capability. Recently, MLLM-based\nprocess judges (MPJs) have been widely used to assess the correctness of\nreasoning steps in multimodal tasks. Therefore, evaluating MPJs is important\nfor identifying their limitations and guiding future improvements. However,\nexisting benchmarks for MPJs mainly focus on tasks such as step correctness\nclassification and reasoning process search, while overlooking a key aspect:\nwhether the confidence scores produced by MPJs at the step level are reliable.\nTo address this gap, we propose ConfProBench, the first comprehensive benchmark\ndesigned to systematically evaluate the reliability of step-level confidence\nscores generated by MPJs. Our benchmark constructs three types of adversarially\nperturbed reasoning steps: Synonym Substitution, Syntactic Transformation, and\nImage Perturbation, to test the robustness of MPJ confidence under\nperturbations. In addition, we introduce three novel evaluation metrics:\nConfidence Robustness Score (CRS), Confidence Sensitivity Score (CSS), and\nConfidence Calibration Score (CCS), which evaluate robustness, sensitivity, and\ncalibration, respectively. We evaluate 14 state-of-the-art MLLMs, including\nboth proprietary and open-source models. Experiments reveal limitations in\ncurrent MPJs' confidence performance and offer competitive baselines to support\nfuture research.", "AI": {"tldr": "\u63d0\u51faConfProBench\u57fa\u51c6\uff0c\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u63a8\u7406\u6b65\u9aa4\u7f6e\u4fe1\u5ea6\u53ef\u9760\u6027\u4e0a\u7684\u8868\u73b0\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7a7a\u767d\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u63a8\u7406\u6b65\u9aa4\u5206\u7c7b\u548c\u641c\u7d22\uff0c\u5ffd\u7565\u7f6e\u4fe1\u5ea6\u53ef\u9760\u6027\uff0c\u9700\u7cfb\u7edf\u6027\u8bc4\u4f30\u4ee5\u6307\u5bfc\u6539\u8fdb\u3002", "method": "\u6784\u5efa\u4e09\u79cd\u5bf9\u6297\u6027\u6270\u52a8\u63a8\u7406\u6b65\u9aa4\uff08\u540c\u4e49\u8bcd\u66ff\u6362\u3001\u53e5\u6cd5\u53d8\u6362\u3001\u56fe\u50cf\u6270\u52a8\uff09\uff0c\u63d0\u51fa\u4e09\u4e2a\u65b0\u8bc4\u4f30\u6307\u6807\uff08CRS\u3001CSS\u3001CCS\uff09\u3002", "result": "\u5b9e\u9a8c\u63ed\u793a\u5f53\u524dMPJs\u5728\u7f6e\u4fe1\u5ea6\u8868\u73b0\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u4f9b\u7ade\u4e89\u6027\u57fa\u7ebf\u3002", "conclusion": "ConfProBench\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u7cfb\u7edf\u6027\u8bc4\u4f30\u5de5\u5177\uff0c\u63a8\u52a8MLLMs\u63a8\u7406\u80fd\u529b\u7684\u63d0\u5347\u3002"}}
{"id": "2508.04652", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.04652", "abs": "https://arxiv.org/abs/2508.04652", "authors": ["Shuo Liu", "Zeyu Liang", "Xueguang Lyu", "Christopher Amato"], "title": "LLM Collaboration With Multi-Agent Reinforcement Learning", "comment": null, "summary": "A large amount of work has been done in Multi-Agent Systems (MAS) for\nmodeling and solving problems with multiple interacting agents. However, most\nLLMs are pretrained independently and not specifically optimized for\ncoordination. Existing LLM fine-tuning frameworks rely on individual rewards,\nwhich require complex reward designs for each agent to encourage collaboration.\nTo address these challenges, we model LLM collaboration as a cooperative\nMulti-Agent Reinforcement Learning (MARL) problem. We develop a multi-agent,\nmulti-turn algorithm, Multi-Agent Group Relative Policy Optimization (MAGRPO),\nto solve it, building on current RL approaches for LLMs as well as MARL\ntechniques. Our experiments on LLM writing and coding collaboration demonstrate\nthat fine-tuning MAS with MAGRPO enables agents to generate high-quality\nresponses efficiently through effective cooperation. Our approach opens the\ndoor to using other MARL methods for LLMs and highlights the associated\nchallenges.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u6846\u67b6MAGRPO\uff0c\u7528\u4e8e\u4f18\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u534f\u4f5c\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u590d\u6742\u4e2a\u4f53\u5956\u52b1\u8bbe\u8ba1\u7684\u95ee\u9898\u3002", "motivation": "\u5f53\u524dLLM\u5728\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u534f\u4f5c\u80fd\u529b\u4e0d\u8db3\uff0c\u4e14\u73b0\u6709\u5fae\u8c03\u6846\u67b6\u4f9d\u8d56\u590d\u6742\u7684\u4e2a\u4f53\u5956\u52b1\u8bbe\u8ba1\uff0c\u96be\u4ee5\u6709\u6548\u4fc3\u8fdb\u534f\u4f5c\u3002", "method": "\u5c06LLM\u534f\u4f5c\u5efa\u6a21\u4e3a\u5408\u4f5c\u578b\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\uff0c\u63d0\u51fa\u591a\u667a\u80fd\u4f53\u3001\u591a\u8f6e\u7b97\u6cd5MAGRPO\uff0c\u7ed3\u5408\u73b0\u6709RL\u548cMARL\u6280\u672f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMAGRPO\u80fd\u6709\u6548\u63d0\u5347LLM\u5728\u5199\u4f5c\u548c\u7f16\u7a0b\u534f\u4f5c\u4efb\u52a1\u4e2d\u7684\u751f\u6210\u8d28\u91cf\u548c\u6548\u7387\u3002", "conclusion": "MAGRPO\u4e3aLLM\u534f\u4f5c\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5e76\u5c55\u793a\u4e86MARL\u65b9\u6cd5\u5728LLM\u4e2d\u7684\u6f5c\u529b\u4e0e\u6311\u6218\u3002"}}
{"id": "2508.04700", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG", "cs.MA", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.04700", "abs": "https://arxiv.org/abs/2508.04700", "authors": ["Zeyi Sun", "Ziyu Liu", "Yuhang Zang", "Yuhang Cao", "Xiaoyi Dong", "Tong Wu", "Dahua Lin", "Jiaqi Wang"], "title": "SEAgent: Self-Evolving Computer Use Agent with Autonomous Learning from Experience", "comment": "Code at https://github.com/SunzeY/SEAgent", "summary": "Repurposing large vision-language models (LVLMs) as computer use agents\n(CUAs) has led to substantial breakthroughs, primarily driven by human-labeled\ndata. However, these models often struggle with novel and specialized software,\nparticularly in scenarios lacking human annotations. To address this challenge,\nwe propose SEAgent, an agentic self-evolving framework enabling CUAs to\nautonomously evolve through interactions with unfamiliar software.\nSpecifically, SEAgent empowers computer-use agents to autonomously master novel\nsoftware environments via experiential learning, where agents explore new\nsoftware, learn through iterative trial-and-error, and progressively tackle\nauto-generated tasks organized from simple to complex. To achieve this goal, we\ndesign a World State Model for step-wise trajectory assessment, along with a\nCurriculum Generator that generates increasingly diverse and challenging tasks.\nThe agent's policy is updated through experiential learning, comprised of\nadversarial imitation of failure actions and Group Relative Policy Optimization\n(GRPO) on successful ones. Furthermore, we introduce a specialist-to-generalist\ntraining strategy that integrates individual experiential insights from\nspecialist agents, facilitating the development of a stronger generalist CUA\ncapable of continuous autonomous evolution. This unified agent ultimately\nachieves performance surpassing ensembles of individual specialist agents on\ntheir specialized software. We validate the effectiveness of SEAgent across\nfive novel software environments within OS-World. Our approach achieves a\nsignificant improvement of 23.2% in success rate, from 11.3% to 34.5%, over a\ncompetitive open-source CUA, i.e., UI-TARS.", "AI": {"tldr": "SEAgent\u662f\u4e00\u79cd\u81ea\u8fdb\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u4e3b\u5b66\u4e60\u548c\u4efb\u52a1\u751f\u6210\uff0c\u63d0\u5347\u8ba1\u7b97\u673a\u4f7f\u7528\u4ee3\u7406\uff08CUA\uff09\u5728\u964c\u751f\u8f6f\u4ef6\u73af\u5883\u4e2d\u7684\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u5728\u7f3a\u4e4f\u4eba\u7c7b\u6807\u6ce8\u7684\u964c\u751f\u8f6f\u4ef6\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u4e00\u79cd\u81ea\u4e3b\u8fdb\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "SEAgent\u7ed3\u5408\u4e86\u4e16\u754c\u72b6\u6001\u6a21\u578b\u3001\u8bfe\u7a0b\u751f\u6210\u5668\u548c\u7ecf\u9a8c\u5b66\u4e60\uff08\u5305\u62ec\u5931\u8d25\u52a8\u4f5c\u7684\u5bf9\u6297\u6a21\u4eff\u548c\u6210\u529f\u52a8\u4f5c\u7684GRPO\u4f18\u5316\uff09\uff0c\u5e76\u901a\u8fc7\u4e13\u5bb6\u5230\u901a\u7528\u7b56\u7565\u6574\u5408\u4e2a\u4f53\u7ecf\u9a8c\u3002", "result": "\u5728OS-World\u7684\u4e94\u4e2a\u65b0\u8f6f\u4ef6\u73af\u5883\u4e2d\uff0cSEAgent\u7684\u6210\u529f\u7387\u4ece11.3%\u63d0\u5347\u81f334.5%\uff0c\u663e\u8457\u4f18\u4e8eUI-TARS\u3002", "conclusion": "SEAgent\u901a\u8fc7\u81ea\u4e3b\u5b66\u4e60\u548c\u4efb\u52a1\u751f\u6210\uff0c\u663e\u8457\u63d0\u5347\u4e86CUA\u5728\u964c\u751f\u8f6f\u4ef6\u4e2d\u7684\u6027\u80fd\uff0c\u5c55\u793a\u4e86\u81ea\u8fdb\u5316\u6846\u67b6\u7684\u6f5c\u529b\u3002"}}
