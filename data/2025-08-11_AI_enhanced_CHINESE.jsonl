{"id": "2508.05731", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.05731", "abs": "https://arxiv.org/abs/2508.05731", "authors": ["Yuhang Liu", "Zeyu Liu", "Shuanghe Zhu", "Pengxiang Li", "Congkai Xie", "Jiasheng Wang", "Xueyu Hu", "Xiaotian Han", "Jianbo Yuan", "Xinyao Wang", "Shengyu Zhang", "Hongxia Yang", "Fei Wu"], "title": "InfiGUI-G1: Advancing GUI Grounding with Adaptive Exploration Policy Optimization", "comment": "11 pages, 3 figures", "summary": "The emergence of Multimodal Large Language Models (MLLMs) has propelled the\ndevelopment of autonomous agents that operate on Graphical User Interfaces\n(GUIs) using pure visual input. A fundamental challenge is robustly grounding\nnatural language instructions. This requires a precise spatial alignment, which\naccurately locates the coordinates of each element, and, more critically, a\ncorrect semantic alignment, which matches the instructions to the functionally\nappropriate UI element. Although Reinforcement Learning with Verifiable Rewards\n(RLVR) has proven to be effective at improving spatial alignment for these\nMLLMs, we find that inefficient exploration bottlenecks semantic alignment,\nwhich prevent models from learning difficult semantic associations. To address\nthis exploration problem, we present Adaptive Exploration Policy Optimization\n(AEPO), a new policy optimization framework. AEPO employs a multi-answer\ngeneration strategy to enforce broader exploration, which is then guided by a\ntheoretically grounded Adaptive Exploration Reward (AER) function derived from\nfirst principles of efficiency eta=U/C. Our AEPO-trained models, InfiGUI-G1-3B\nand InfiGUI-G1-7B, establish new state-of-the-art results across multiple\nchallenging GUI grounding benchmarks, achieving significant relative\nimprovements of up to 9.0% against the naive RLVR baseline on benchmarks\ndesigned to test generalization and semantic understanding. Resources are\navailable at https://github.com/InfiXAI/InfiGUI-G1.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAEPO\u7684\u65b0\u7b56\u7565\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u7b54\u6848\u751f\u6210\u7b56\u7565\u548c\u81ea\u9002\u5e94\u63a2\u7d22\u5956\u52b1\u51fd\u6570\uff0c\u89e3\u51b3\u4e86MLLMs\u5728GUI\u64cd\u4f5c\u4e2d\u8bed\u4e49\u5bf9\u9f50\u7684\u63a2\u7d22\u74f6\u9888\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728GUI\u64cd\u4f5c\u4e2d\u9762\u4e34\u8bed\u4e49\u5bf9\u9f50\u7684\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u63a2\u7d22\u6548\u7387\u4e0a\u5b58\u5728\u74f6\u9888\u3002", "method": "\u63d0\u51fa\u81ea\u9002\u5e94\u63a2\u7d22\u7b56\u7565\u4f18\u5316\uff08AEPO\uff09\uff0c\u7ed3\u5408\u591a\u7b54\u6848\u751f\u6210\u7b56\u7565\u548c\u7406\u8bba\u9a71\u52a8\u7684\u81ea\u9002\u5e94\u63a2\u7d22\u5956\u52b1\u51fd\u6570\uff08AER\uff09\u3002", "result": "AEPO\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u591a\u4e2aGUI\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u65b0SOTA\uff0c\u76f8\u5bf9\u57fa\u7ebf\u63d0\u5347\u9ad8\u8fbe9.0%\u3002", "conclusion": "AEPO\u6709\u6548\u89e3\u51b3\u4e86\u8bed\u4e49\u5bf9\u9f50\u7684\u63a2\u7d22\u95ee\u9898\uff0c\u4e3aMLLMs\u5728GUI\u64cd\u4f5c\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2508.05766", "categories": ["cs.AI", "cs.LG", "cs.SY", "eess.SY", "nlin.AO"], "pdf": "https://arxiv.org/pdf/2508.05766", "abs": "https://arxiv.org/abs/2508.05766", "authors": ["Bo Wen"], "title": "A Framework for Inherently Safer AGI through Language-Mediated Active Inference", "comment": null, "summary": "This paper proposes a novel framework for developing safe Artificial General\nIntelligence (AGI) by combining Active Inference principles with Large Language\nModels (LLMs). We argue that traditional approaches to AI safety, focused on\npost-hoc interpretability and reward engineering, have fundamental limitations.\nWe present an architecture where safety guarantees are integrated into the\nsystem's core design through transparent belief representations and\nhierarchical value alignment. Our framework leverages natural language as a\nmedium for representing and manipulating beliefs, enabling direct human\noversight while maintaining computational tractability. The architecture\nimplements a multi-agent system where agents self-organize according to Active\nInference principles, with preferences and safety constraints flowing through\nhierarchical Markov blankets. We outline specific mechanisms for ensuring\nsafety, including: (1) explicit separation of beliefs and preferences in\nnatural language, (2) bounded rationality through resource-aware free energy\nminimization, and (3) compositional safety through modular agent structures.\nThe paper concludes with a research agenda centered on the Abstraction and\nReasoning Corpus (ARC) benchmark, proposing experiments to validate our\nframework's safety properties. Our approach offers a path toward AGI\ndevelopment that is inherently safer, rather than retrofitted with safety\nmeasures.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4e3b\u52a8\u63a8\u7406\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u65b0\u578b\u6846\u67b6\uff0c\u65e8\u5728\u5f00\u53d1\u5b89\u5168\u7684\u901a\u7528\u4eba\u5de5\u667a\u80fd\uff08AGI\uff09\uff0c\u901a\u8fc7\u900f\u660e\u4fe1\u5ff5\u8868\u793a\u548c\u5c42\u6b21\u5316\u4ef7\u503c\u5bf9\u9f50\u5b9e\u73b0\u6838\u5fc3\u8bbe\u8ba1\u7684\u5b89\u5168\u6027\u3002", "motivation": "\u4f20\u7edfAI\u5b89\u5168\u65b9\u6cd5\uff08\u5982\u4e8b\u540e\u53ef\u89e3\u91ca\u6027\u548c\u5956\u52b1\u5de5\u7a0b\uff09\u5b58\u5728\u6839\u672c\u6027\u5c40\u9650\uff0c\u9700\u5728\u7cfb\u7edf\u6838\u5fc3\u8bbe\u8ba1\u4e2d\u96c6\u6210\u5b89\u5168\u6027\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u57fa\u4e8e\u4e3b\u52a8\u63a8\u7406\u539f\u5219\u81ea\u7ec4\u7ec7\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u8868\u793a\u4fe1\u5ff5\u548c\u504f\u597d\uff0c\u5b9e\u73b0\u5c42\u6b21\u5316\u5b89\u5168\u7ea6\u675f\u3002", "result": "\u63d0\u51fa\u5177\u4f53\u5b89\u5168\u673a\u5236\uff0c\u5305\u62ec\u4fe1\u5ff5\u4e0e\u504f\u597d\u7684\u8bed\u8a00\u5206\u79bb\u3001\u8d44\u6e90\u611f\u77e5\u7684\u81ea\u7531\u80fd\u6700\u5c0f\u5316\u53ca\u6a21\u5757\u5316\u667a\u80fd\u4f53\u7ed3\u6784\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aAGI\u5f00\u53d1\u63d0\u4f9b\u4e86\u4e00\u6761\u66f4\u5b89\u5168\u7684\u53d1\u5c55\u8def\u5f84\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8eARC\u57fa\u51c6\u7684\u9a8c\u8bc1\u5b9e\u9a8c\u3002"}}
{"id": "2508.05776", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05776", "abs": "https://arxiv.org/abs/2508.05776", "authors": ["Thomas L. Griffiths", "Brenden M. Lake", "R. Thomas McCoy", "Ellie Pavlick", "Taylor W. Webb"], "title": "Whither symbols in the era of advanced neural networks?", "comment": null, "summary": "Some of the strongest evidence that human minds should be thought about in\nterms of symbolic systems has been the way they combine ideas, produce novelty,\nand learn quickly. We argue that modern neural networks -- and the artificial\nintelligence systems built upon them -- exhibit similar abilities. This\nundermines the argument that the cognitive processes and representations used\nby human minds are symbolic, although the fact that these neural networks are\ntypically trained on data generated by symbolic systems illustrates that such\nsystems play an important role in characterizing the abstract problems that\nhuman minds have to solve. This argument leads us to offer a new agenda for\nresearch on the symbolic basis of human thought.", "AI": {"tldr": "\u73b0\u4ee3\u795e\u7ecf\u7f51\u7edc\u8868\u73b0\u51fa\u7c7b\u4f3c\u4eba\u7c7b\u601d\u7ef4\u7684\u7ec4\u5408\u3001\u521b\u65b0\u548c\u5feb\u901f\u5b66\u4e60\u80fd\u529b\uff0c\u524a\u5f31\u4e86\u4eba\u7c7b\u8ba4\u77e5\u8fc7\u7a0b\u662f\u7b26\u53f7\u5316\u7684\u8bba\u70b9\uff0c\u4f46\u7b26\u53f7\u7cfb\u7edf\u5728\u63cf\u8ff0\u4eba\u7c7b\u601d\u7ef4\u89e3\u51b3\u7684\u62bd\u8c61\u95ee\u9898\u4e2d\u4ecd\u8d77\u91cd\u8981\u4f5c\u7528\u3002", "motivation": "\u63a2\u8ba8\u795e\u7ecf\u7f51\u7edc\u662f\u5426\u5177\u5907\u7c7b\u4f3c\u4eba\u7c7b\u601d\u7ef4\u7684\u7b26\u53f7\u5316\u80fd\u529b\uff0c\u5e76\u91cd\u65b0\u8bc4\u4f30\u7b26\u53f7\u7cfb\u7edf\u5728\u4eba\u7c7b\u601d\u7ef4\u4e2d\u7684\u4f5c\u7528\u3002", "method": "\u901a\u8fc7\u5206\u6790\u73b0\u4ee3\u795e\u7ecf\u7f51\u7edc\u7684\u8868\u73b0\uff0c\u5bf9\u6bd4\u4eba\u7c7b\u601d\u7ef4\u7684\u7b26\u53f7\u5316\u7279\u5f81\u3002", "result": "\u795e\u7ecf\u7f51\u7edc\u8868\u73b0\u51fa\u7c7b\u4f3c\u4eba\u7c7b\u601d\u7ef4\u7684\u7ec4\u5408\u548c\u521b\u65b0\u80fd\u529b\uff0c\u4f46\u7b26\u53f7\u7cfb\u7edf\u5728\u95ee\u9898\u62bd\u8c61\u4e2d\u4ecd\u4e0d\u53ef\u6216\u7f3a\u3002", "conclusion": "\u63d0\u51fa\u7814\u7a76\u4eba\u7c7b\u601d\u7ef4\u7b26\u53f7\u57fa\u7840\u7684\u65b0\u8bae\u7a0b\uff0c\u5f3a\u8c03\u7b26\u53f7\u7cfb\u7edf\u4e0e\u795e\u7ecf\u7f51\u7edc\u7684\u4e92\u8865\u6027\u3002"}}
{"id": "2508.05773", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.05773", "abs": "https://arxiv.org/abs/2508.05773", "authors": ["Keyvan Majd", "Hardik Parwana", "Bardh Hoxha", "Steven Hong", "Hideki Okamoto", "Georgios Fainekos"], "title": "GPU-Accelerated Barrier-Rate Guided MPPI Control for Tractor-Trailer Systems", "comment": "Accepted to IEEE ITSC 2025", "summary": "Articulated vehicles such as tractor-trailers, yard trucks, and similar\nplatforms must often reverse and maneuver in cluttered spaces where pedestrians\nare present. We present how Barrier-Rate guided Model Predictive Path Integral\n(BR-MPPI) control can solve navigation in such challenging environments.\nBR-MPPI embeds Control Barrier Function (CBF) constraints directly into the\npath-integral update. By steering the importance-sampling distribution toward\ncollision-free, dynamically feasible trajectories, BR-MPPI enhances the\nexploration strength of MPPI and improves robustness of resulting trajectories.\nThe method is evaluated in the high-fidelity CarMaker simulator on a 12 [m]\ntractor-trailer tasked with reverse and forward parking in a parking lot.\nBR-MPPI computes control inputs in above 100 [Hz] on a single GPU (for\nscenarios with eight obstacles) and maintains better parking clearance than a\nstandard MPPI baseline and an MPPI with collision cost baseline.", "AI": {"tldr": "BR-MPPI\u65b9\u6cd5\u901a\u8fc7\u5d4c\u5165CBF\u7ea6\u675f\u6539\u8fdbMPPI\u63a7\u5236\uff0c\u63d0\u5347\u590d\u6742\u73af\u5883\u4e2d\u94f0\u63a5\u8f66\u8f86\u7684\u5bfc\u822a\u80fd\u529b\uff0c\u5b9e\u9a8c\u663e\u793a\u5176\u5728\u505c\u8f66\u573a\u666f\u4e2d\u4f18\u4e8e\u4f20\u7edfMPPI\u65b9\u6cd5\u3002", "motivation": "\u94f0\u63a5\u8f66\u8f86\uff08\u5982\u62d6\u8f66\u3001\u5806\u573a\u5361\u8f66\u7b49\uff09\u5728\u62e5\u6324\u73af\u5883\u4e2d\u5012\u8f66\u548c\u673a\u52a8\u65f6\u9762\u4e34\u6311\u6218\uff0c\u9700\u4e00\u79cd\u66f4\u9c81\u68d2\u7684\u63a7\u5236\u65b9\u6cd5\u3002", "method": "BR-MPPI\u5c06CBF\u7ea6\u675f\u76f4\u63a5\u5d4c\u5165\u8def\u5f84\u79ef\u5206\u66f4\u65b0\u4e2d\uff0c\u5f15\u5bfc\u91c7\u6837\u5206\u5e03\u671d\u5411\u65e0\u78b0\u649e\u4e14\u52a8\u6001\u53ef\u884c\u7684\u8f68\u8ff9\u3002", "result": "\u5728\u9ad8\u4fdd\u771fCarMaker\u6a21\u62df\u5668\u4e2d\uff0cBR-MPPI\u572812\u7c73\u62d6\u8f66\u5012\u8f66\u548c\u524d\u8fdb\u505c\u8f66\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edfMPPI\uff0c\u63a7\u5236\u9891\u7387\u8d85\u8fc7100Hz\u3002", "conclusion": "BR-MPPI\u901a\u8fc7\u6539\u8fdb\u91c7\u6837\u7b56\u7565\u548c\u5d4c\u5165CBF\u7ea6\u675f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u94f0\u63a5\u8f66\u8f86\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u9c81\u68d2\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2508.05792", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05792", "abs": "https://arxiv.org/abs/2508.05792", "authors": ["Kausik Lakkaraju", "Siva Likitha Valluru", "Biplav Srivastava"], "title": "Holistic Explainable AI (H-XAI): Extending Transparency Beyond Developers in AI-Driven Decision Making", "comment": null, "summary": "Current eXplainable AI (XAI) methods largely serve developers, often focusing\non justifying model outputs rather than supporting diverse stakeholder needs. A\nrecent shift toward Evaluative AI reframes explanation as a tool for hypothesis\ntesting, but still focuses primarily on operational organizations. We introduce\nHolistic-XAI (H-XAI), a unified framework that integrates causal rating methods\nwith traditional XAI methods to support explanation as an interactive,\nmulti-method process. H-XAI allows stakeholders to ask a series of questions,\ntest hypotheses, and compare model behavior against automatically constructed\nrandom and biased baselines. It combines instance-level and global\nexplanations, adapting to each stakeholder's goals, whether understanding\nindividual decisions, assessing group-level bias, or evaluating robustness\nunder perturbations. We demonstrate the generality of our approach through two\ncase studies spanning six scenarios: binary credit risk classification and\nfinancial time-series forecasting. H-XAI fills critical gaps left by existing\nXAI methods by combining causal ratings and post-hoc explanations to answer\nstakeholder-specific questions at both the individual decision level and the\noverall model level.", "AI": {"tldr": "H-XAI\u662f\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u7ed3\u5408\u56e0\u679c\u8bc4\u7ea7\u4e0e\u4f20\u7edfXAI\u65b9\u6cd5\uff0c\u652f\u6301\u4ea4\u4e92\u5f0f\u3001\u591a\u65b9\u6cd5\u7684\u89e3\u91ca\u8fc7\u7a0b\uff0c\u6ee1\u8db3\u4e0d\u540c\u5229\u76ca\u76f8\u5173\u8005\u7684\u9700\u6c42\u3002", "motivation": "\u73b0\u6709XAI\u65b9\u6cd5\u4e3b\u8981\u670d\u52a1\u4e8e\u5f00\u53d1\u8005\uff0c\u7f3a\u4e4f\u5bf9\u591a\u6837\u5316\u5229\u76ca\u76f8\u5173\u8005\u9700\u6c42\u7684\u652f\u6301\u3002H-XAI\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u63d0\u4f9b\u66f4\u5168\u9762\u7684\u89e3\u91ca\u5de5\u5177\u3002", "method": "H-XAI\u6574\u5408\u56e0\u679c\u8bc4\u7ea7\u4e0e\u4f20\u7edfXAI\u65b9\u6cd5\uff0c\u652f\u6301\u4ea4\u4e92\u5f0f\u63d0\u95ee\u3001\u5047\u8bbe\u6d4b\u8bd5\uff0c\u5e76\u5bf9\u6bd4\u968f\u673a\u548c\u504f\u7f6e\u57fa\u7ebf\u3002", "result": "\u901a\u8fc7\u4e24\u4e2a\u6848\u4f8b\u7814\u7a76\uff08\u4fe1\u7528\u98ce\u9669\u5206\u7c7b\u548c\u91d1\u878d\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff09\uff0c\u9a8c\u8bc1\u4e86H-XAI\u7684\u901a\u7528\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "H-XAI\u901a\u8fc7\u7ed3\u5408\u56e0\u679c\u8bc4\u7ea7\u548c\u540e\u9a8c\u89e3\u91ca\uff0c\u586b\u8865\u4e86\u73b0\u6709XAI\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u6ee1\u8db3\u4e2a\u4f53\u548c\u6a21\u578b\u5c42\u9762\u7684\u9700\u6c42\u3002"}}
{"id": "2508.05838", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "cs.SY", "eess.SY", "68T07, 68T40, 90C40, 93E35", "I.2.6; I.2.9; I.2.10"], "pdf": "https://arxiv.org/pdf/2508.05838", "abs": "https://arxiv.org/abs/2508.05838", "authors": ["Ahmad Farooq", "Kamran Iqbal"], "title": "Integrating Vision Foundation Models with Reinforcement Learning for Enhanced Object Interaction", "comment": "Published in the Proceedings of the 2025 3rd International Conference\n  on Robotics, Control and Vision Engineering (RCVE'25). 6 pages, 3 figures, 1\n  table", "summary": "This paper presents a novel approach that integrates vision foundation models\nwith reinforcement learning to enhance object interaction capabilities in\nsimulated environments. By combining the Segment Anything Model (SAM) and\nYOLOv5 with a Proximal Policy Optimization (PPO) agent operating in the\nAI2-THOR simulation environment, we enable the agent to perceive and interact\nwith objects more effectively. Our comprehensive experiments, conducted across\nfour diverse indoor kitchen settings, demonstrate significant improvements in\nobject interaction success rates and navigation efficiency compared to a\nbaseline agent without advanced perception. The results show a 68% increase in\naverage cumulative reward, a 52.5% improvement in object interaction success\nrate, and a 33% increase in navigation efficiency. These findings highlight the\npotential of integrating foundation models with reinforcement learning for\ncomplex robotic tasks, paving the way for more sophisticated and capable\nautonomous agents.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u4e0e\u5f3a\u5316\u5b66\u4e60\u7684\u65b0\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u62df\u73af\u5883\u4e2d\u5bf9\u8c61\u7684\u4ea4\u4e92\u80fd\u529b\u3002", "motivation": "\u901a\u8fc7\u6574\u5408\u89c6\u89c9\u57fa\u7840\u6a21\u578b\uff08\u5982SAM\u548cYOLOv5\uff09\u4e0e\u5f3a\u5316\u5b66\u4e60\uff08PPO\uff09\uff0c\u65e8\u5728\u63d0\u5347\u667a\u80fd\u4f53\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u611f\u77e5\u4e0e\u4ea4\u4e92\u80fd\u529b\u3002", "method": "\u7ed3\u5408Segment Anything Model (SAM)\u548cYOLOv5\u4f5c\u4e3a\u611f\u77e5\u6a21\u5757\uff0c\u4f7f\u7528PPO\u7b97\u6cd5\u5728AI2-THOR\u6a21\u62df\u73af\u5883\u4e2d\u8bad\u7ec3\u667a\u80fd\u4f53\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5bf9\u8c61\u4ea4\u4e92\u6210\u529f\u7387\u63d0\u534752.5%\uff0c\u5bfc\u822a\u6548\u7387\u63d0\u534733%\uff0c\u5e73\u5747\u7d2f\u79ef\u5956\u52b1\u589e\u52a068%\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u4e0e\u5f3a\u5316\u5b66\u4e60\u7684\u7ed3\u5408\u5728\u590d\u6742\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4e3a\u66f4\u5148\u8fdb\u7684\u81ea\u4e3b\u667a\u80fd\u4f53\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2508.05855", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.05855", "abs": "https://arxiv.org/abs/2508.05855", "authors": ["Zixia Wang", "Jia Hu", "Ronghui Mu"], "title": "Safety of Embodied Navigation: A Survey", "comment": null, "summary": "As large language models (LLMs) continue to advance and gain influence, the\ndevelopment of embodied AI has accelerated, drawing significant attention,\nparticularly in navigation scenarios. Embodied navigation requires an agent to\nperceive, interact with, and adapt to its environment while moving toward a\nspecified target in unfamiliar settings. However, the integration of embodied\nnavigation into critical applications raises substantial safety concerns. Given\ntheir deployment in dynamic, real-world environments, ensuring the safety of\nsuch systems is critical. This survey provides a comprehensive analysis of\nsafety in embodied navigation from multiple perspectives, encompassing attack\nstrategies, defense mechanisms, and evaluation methodologies. Beyond conducting\na comprehensive examination of existing safety challenges, mitigation\ntechnologies, and various datasets and metrics that assess effectiveness and\nrobustness, we explore unresolved issues and future research directions in\nembodied navigation safety. These include potential attack methods, mitigation\nstrategies, more reliable evaluation techniques, and the implementation of\nverification frameworks. By addressing these critical gaps, this survey aims to\nprovide valuable insights that can guide future research toward the development\nof safer and more reliable embodied navigation systems. Furthermore, the\nfindings of this study have broader implications for enhancing societal safety\nand increasing industrial efficiency.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u5177\u8eab\u5bfc\u822a\u4e2d\u7684\u5b89\u5168\u95ee\u9898\uff0c\u5206\u6790\u4e86\u653b\u51fb\u7b56\u7565\u3001\u9632\u5fa1\u673a\u5236\u548c\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5e76\u63a2\u8ba8\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u548c\u5177\u8eabAI\u7684\u53d1\u5c55\uff0c\u5177\u8eab\u5bfc\u822a\u5728\u5173\u952e\u5e94\u7528\u4e2d\u7684\u5b89\u5168\u6027\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\uff0c\u9700\u5168\u9762\u7814\u7a76\u4ee5\u786e\u4fdd\u7cfb\u7edf\u5b89\u5168\u3002", "method": "\u901a\u8fc7\u7efc\u5408\u5206\u6790\u73b0\u6709\u5b89\u5168\u6311\u6218\u3001\u7f13\u89e3\u6280\u672f\u3001\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6307\u6807\uff0c\u63a2\u8ba8\u672a\u89e3\u51b3\u95ee\u9898\u548c\u672a\u6765\u65b9\u5411\u3002", "result": "\u63d0\u51fa\u4e86\u6f5c\u5728\u653b\u51fb\u65b9\u6cd5\u3001\u7f13\u89e3\u7b56\u7565\u3001\u53ef\u9760\u8bc4\u4f30\u6280\u672f\u548c\u9a8c\u8bc1\u6846\u67b6\u7684\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u672c\u6587\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u6307\u5bfc\uff0c\u65e8\u5728\u5f00\u53d1\u66f4\u5b89\u5168\u53ef\u9760\u7684\u5177\u8eab\u5bfc\u822a\u7cfb\u7edf\uff0c\u540c\u65f6\u63d0\u5347\u793e\u4f1a\u5b89\u5168\u548c\u5de5\u4e1a\u6548\u7387\u3002"}}
{"id": "2508.05936", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.05936", "abs": "https://arxiv.org/abs/2508.05936", "authors": ["Haohui Pan", "Takuya Kiyokawa", "Tomoki Ishikura", "Shingo Hamada", "Genichiro Matsuda", "Kensuke Harada"], "title": "Modular Vacuum-Based Fixturing System for Adaptive Disassembly Workspace Integration", "comment": "8 pages, 9 figures", "summary": "The disassembly of small household appliances poses significant challenges\ndue to their complex and curved geometries, which render traditional rigid\nfixtures inadequate. In this paper, we propose a modular vacuum-based fixturing\nsystem that leverages commercially available balloon-type soft grippers to\nconform to arbitrarily shaped surfaces and provide stable support during\nscrew-removal tasks. To enable a reliable deployment of the system, we develop\na stability-aware planning framework that samples the bottom surface of the\ntarget object, filters candidate contact points based on geometric continuity,\nand evaluates support configurations using convex hull-based static stability\ncriteria. We compare the quality of object placement under different numbers\nand configurations of balloon hands. In addition, real-world experiments were\nconducted to compare the success rates of traditional rigid fixtures with our\nproposed system. The results demonstrate that our method consistently achieves\nhigher success rates and superior placement stability during screw removal\ntasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u5757\u5316\u771f\u7a7a\u5939\u5177\u7684\u7cfb\u7edf\uff0c\u5229\u7528\u8f6f\u6c14\u7403\u5939\u6301\u5668\u9002\u5e94\u590d\u6742\u51e0\u4f55\u5f62\u72b6\uff0c\u63d0\u9ad8\u62c6\u5378\u5c0f\u5bb6\u7535\u7684\u6210\u529f\u7387\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u4f20\u7edf\u521a\u6027\u5939\u5177\u96be\u4ee5\u9002\u5e94\u5c0f\u5bb6\u7535\u7684\u590d\u6742\u66f2\u9762\u51e0\u4f55\u5f62\u72b6\uff0c\u5bfc\u81f4\u62c6\u5378\u4efb\u52a1\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u5f00\u53d1\u4e86\u7a33\u5b9a\u6027\u611f\u77e5\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u91c7\u6837\u76ee\u6807\u7269\u4f53\u5e95\u9762\u3001\u7b5b\u9009\u63a5\u89e6\u70b9\u5e76\u8bc4\u4f30\u652f\u6491\u914d\u7f6e\uff0c\u7ed3\u5408\u6c14\u7403\u5939\u6301\u5668\u5b9e\u73b0\u7a33\u5b9a\u652f\u6491\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u87ba\u4e1d\u62c6\u5378\u4efb\u52a1\u4e2d\u6bd4\u4f20\u7edf\u521a\u6027\u5939\u5177\u5177\u6709\u66f4\u9ad8\u7684\u6210\u529f\u7387\u548c\u7a33\u5b9a\u6027\u3002", "conclusion": "\u6a21\u5757\u5316\u771f\u7a7a\u5939\u5177\u7cfb\u7edf\u4e3a\u590d\u6742\u51e0\u4f55\u5f62\u72b6\u7269\u4f53\u7684\u62c6\u5378\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u3001\u7a33\u5b9a\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.05888", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.05888", "abs": "https://arxiv.org/abs/2508.05888", "authors": ["Sahil Bansal", "Sai Shruthi Sistla", "Aarti Arikatala", "Sebastian Schreiber"], "title": "Planning Agents on an Ego-Trip: Leveraging Hybrid Ego-Graph Ensembles for Improved Tool Retrieval in Enterprise Task Planning", "comment": null, "summary": "Effective tool retrieval is essential for AI agents to select from a vast\narray of tools when identifying and planning actions in the context of complex\nuser queries. Despite its central role in planning, this aspect remains\nunderexplored in the literature. Traditional approaches rely primarily on\nsimilarities between user queries and tool descriptions, which significantly\nlimits retrieval accuracy, specifically when handling multi-step user requests.\nTo address these limitations, we propose a Knowledge Graph (KG)-based tool\nretrieval framework that captures the semantic relationships between tools and\ntheir functional dependencies. Our retrieval algorithm leverages ensembles of\n1-hop ego tool graphs to model direct and indirect connections between tools,\nenabling more comprehensive and contextual tool selection for multi-step tasks.\nWe evaluate our approach on a synthetically generated internal dataset across\nsix defined user classes, extending previous work on coherent dialogue\nsynthesis and too retrieval benchmarks. Results demonstrate that our tool\ngraph-based method achieves 91.85% tool coverage on the micro-average Complete\nRecall metric, compared to 89.26% for re-ranked semantic-lexical hybrid\nretrieval, the strongest non-KG baseline in our experiments. These findings\nsupport our hypothesis that the structural information in the KG provides\ncomplementary signals to pure similarity matching, particularly for queries\nrequiring sequential tool composition.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\uff08KG\uff09\u7684\u5de5\u5177\u68c0\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u6355\u6349\u5de5\u5177\u95f4\u7684\u8bed\u4e49\u5173\u7cfb\u548c\u529f\u80fd\u4f9d\u8d56\uff0c\u63d0\u5347\u591a\u6b65\u9aa4\u4efb\u52a1\u4e2d\u7684\u5de5\u5177\u9009\u62e9\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u7528\u6237\u67e5\u8be2\u4e0e\u5de5\u5177\u63cf\u8ff0\u7684\u76f8\u4f3c\u6027\uff0c\u9650\u5236\u4e86\u591a\u6b65\u9aa4\u8bf7\u6c42\u7684\u68c0\u7d22\u51c6\u786e\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u5168\u9762\u7684\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u77e5\u8bc6\u56fe\u8c31\u4e2d\u76841-hop ego\u5de5\u5177\u56fe\u96c6\u5408\u5efa\u6a21\u5de5\u5177\u95f4\u7684\u76f4\u63a5\u548c\u95f4\u63a5\u8fde\u63a5\uff0c\u5b9e\u73b0\u66f4\u5168\u9762\u7684\u5de5\u5177\u9009\u62e9\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728Complete Recall\u6307\u6807\u4e0a\u8fbe\u523091.85%\u7684\u5de5\u5177\u8986\u76d6\u7387\uff0c\u4f18\u4e8e\u975eKG\u57fa\u7ebf\uff0889.26%\uff09\u3002", "conclusion": "KG\u7684\u7ed3\u6784\u4fe1\u606f\u4e3a\u7eaf\u76f8\u4f3c\u6027\u5339\u914d\u63d0\u4f9b\u4e86\u8865\u5145\u4fe1\u53f7\uff0c\u7279\u522b\u9002\u7528\u4e8e\u9700\u8981\u987a\u5e8f\u5de5\u5177\u7ec4\u5408\u7684\u67e5\u8be2\u3002"}}
{"id": "2508.05937", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.05937", "abs": "https://arxiv.org/abs/2508.05937", "authors": ["Gen Sako", "Takuya Kiyokawa", "Kensuke Harada", "Tomoki Ishikura", "Naoya Miyaji", "Genichiro Matsuda"], "title": "Affordance-Guided Dual-Armed Disassembly Teleoperation for Mating Parts", "comment": "6 pages, 9 figures", "summary": "Robotic non-destructive disassembly of mating parts remains challenging due\nto the need for flexible manipulation and the limited visibility of internal\nstructures. This study presents an affordance-guided teleoperation system that\nenables intuitive human demonstrations for dual-arm fix-and-disassemble tasks\nfor mating parts. The system visualizes feasible grasp poses and disassembly\ndirections in a virtual environment, both derived from the object's geometry,\nto address occlusions and structural complexity. To prevent excessive position\ntracking under load when following the affordance, we integrate a hybrid\ncontroller that combines position and impedance control into the teleoperated\ndisassembly arm. Real-world experiments validate the effectiveness of the\nproposed system, showing improved task success rates and reduced object pose\ndeviation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u611f\u77e5\u5f15\u5bfc\u7684\u8fdc\u7a0b\u64cd\u4f5c\u7cfb\u7edf\uff0c\u7528\u4e8e\u89e3\u51b3\u673a\u5668\u4eba\u62c6\u5378\u914d\u5408\u96f6\u4ef6\u65f6\u7684\u7075\u6d3b\u64cd\u4f5c\u548c\u5185\u90e8\u7ed3\u6784\u53ef\u89c1\u6027\u95ee\u9898\u3002", "motivation": "\u914d\u5408\u96f6\u4ef6\u7684\u975e\u7834\u574f\u6027\u62c6\u5378\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u5177\u6709\u6311\u6218\u6027\uff0c\u4e3b\u8981\u7531\u4e8e\u7075\u6d3b\u64cd\u4f5c\u9700\u6c42\u548c\u5185\u90e8\u7ed3\u6784\u4e0d\u53ef\u89c1\u6027\u3002", "method": "\u7cfb\u7edf\u901a\u8fc7\u865a\u62df\u73af\u5883\u53ef\u89c6\u5316\u53ef\u884c\u6293\u53d6\u4f4d\u59ff\u548c\u62c6\u5378\u65b9\u5411\uff0c\u5e76\u91c7\u7528\u6df7\u5408\u63a7\u5236\u5668\uff08\u4f4d\u7f6e\u4e0e\u963b\u6297\u63a7\u5236\uff09\u4ee5\u51cf\u5c11\u8d1f\u8f7d\u4e0b\u7684\u4f4d\u7f6e\u8ddf\u8e2a\u8bef\u5dee\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u7684\u6709\u6548\u6027\uff0c\u4efb\u52a1\u6210\u529f\u7387\u63d0\u9ad8\u4e14\u7269\u4f53\u4f4d\u59ff\u504f\u5dee\u51cf\u5c11\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u914d\u5408\u96f6\u4ef6\u7684\u62c6\u5378\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u76f4\u89c2\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.05996", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05996", "abs": "https://arxiv.org/abs/2508.05996", "authors": ["Kaitao Chen", "Mianxin Liu", "Daoming Zong", "Chaoyue Ding", "Shaohao Rui", "Yankai Jiang", "Mu Zhou", "Xiaosong Wang"], "title": "Mediator-Guided Multi-Agent Collaboration among Open-Source Models for Medical Decision-Making", "comment": "14 pages, 4 figures", "summary": "Complex medical decision-making involves cooperative workflows operated by\ndifferent clinicians. Designing AI multi-agent systems can expedite and augment\nhuman-level clinical decision-making. Existing multi-agent researches primarily\nfocus on language-only tasks, yet their extension to multimodal scenarios\nremains challenging. A blind combination of diverse vision-language models\n(VLMs) can amplify an erroneous outcome interpretation. VLMs in general are\nless capable in instruction following and importantly self-reflection, compared\nto large language models (LLMs) of comparable sizes. This disparity largely\nconstrains VLMs' ability in cooperative workflows. In this study, we propose\nMedOrch, a mediator-guided multi-agent collaboration framework for medical\nmultimodal decision-making. MedOrch employs an LLM-based mediator agent that\nenables multiple VLM-based expert agents to exchange and reflect on their\noutputs towards collaboration. We utilize multiple open-source general-purpose\nand domain-specific VLMs instead of costly GPT-series models, revealing the\nstrength of heterogeneous models. We show that the collaboration within\ndistinct VLM-based agents can surpass the capabilities of any individual agent.\nWe validate our approach on five medical vision question answering benchmarks,\ndemonstrating superior collaboration performance without model training. Our\nfindings underscore the value of mediator-guided multi-agent collaboration in\nadvancing medical multimodal intelligence. Our code will be made publicly\navailable.", "AI": {"tldr": "\u63d0\u51faMedOrch\u6846\u67b6\uff0c\u901a\u8fc7LLM\u4e2d\u4ecb\u534f\u8c03\u591aVLM\u4e13\u5bb6\u4ee3\u7406\uff0c\u63d0\u5347\u533b\u7597\u591a\u6a21\u6001\u51b3\u7b56\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u591a\u4ee3\u7406\u7814\u7a76\u5c40\u9650\u4e8e\u8bed\u8a00\u4efb\u52a1\uff0c\u591a\u6a21\u6001\u573a\u666f\u4e0bVLM\u534f\u4f5c\u80fd\u529b\u4e0d\u8db3\uff0c\u9700\u89e3\u51b3\u9519\u8bef\u8f93\u51fa\u548c\u6307\u4ee4\u8ddf\u968f\u95ee\u9898\u3002", "method": "\u91c7\u7528LLM\u4e2d\u4ecb\u4ee3\u7406\u534f\u8c03\u591a\u4e2aVLM\u4e13\u5bb6\u4ee3\u7406\uff0c\u5229\u7528\u5f00\u6e90VLM\u5b9e\u73b0\u5f02\u6784\u6a21\u578b\u534f\u4f5c\u3002", "result": "\u5728\u4e94\u4e2a\u533b\u7597\u89c6\u89c9\u95ee\u7b54\u57fa\u51c6\u4e0a\u9a8c\u8bc1\uff0c\u534f\u4f5c\u6027\u80fd\u8d85\u8d8a\u5355\u4e00\u4ee3\u7406\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "conclusion": "\u4e2d\u4ecb\u5f15\u5bfc\u7684\u591a\u4ee3\u7406\u534f\u4f5c\u53ef\u63a8\u52a8\u533b\u7597\u591a\u6a21\u6001\u667a\u80fd\u53d1\u5c55\uff0c\u4ee3\u7801\u5c06\u5f00\u6e90\u3002"}}
{"id": "2508.05941", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.05941", "abs": "https://arxiv.org/abs/2508.05941", "authors": ["Zhanyi Sun", "Shuran Song"], "title": "Latent Policy Barrier: Learning Robust Visuomotor Policies by Staying In-Distribution", "comment": null, "summary": "Visuomotor policies trained via behavior cloning are vulnerable to covariate\nshift, where small deviations from expert trajectories can compound into\nfailure. Common strategies to mitigate this issue involve expanding the\ntraining distribution through human-in-the-loop corrections or synthetic data\naugmentation. However, these approaches are often labor-intensive, rely on\nstrong task assumptions, or compromise the quality of imitation. We introduce\nLatent Policy Barrier, a framework for robust visuomotor policy learning.\nInspired by Control Barrier Functions, LPB treats the latent embeddings of\nexpert demonstrations as an implicit barrier separating safe, in-distribution\nstates from unsafe, out-of-distribution (OOD) ones. Our approach decouples the\nrole of precise expert imitation and OOD recovery into two separate modules: a\nbase diffusion policy solely on expert data, and a dynamics model trained on\nboth expert and suboptimal policy rollout data. At inference time, the dynamics\nmodel predicts future latent states and optimizes them to stay within the\nexpert distribution. Both simulated and real-world experiments show that LPB\nimproves both policy robustness and data efficiency, enabling reliable\nmanipulation from limited expert data and without additional human correction\nor annotation.", "AI": {"tldr": "LPB\u6846\u67b6\u901a\u8fc7\u5c06\u4e13\u5bb6\u6f14\u793a\u7684\u6f5c\u5728\u5d4c\u5165\u4f5c\u4e3a\u9690\u5f0f\u5c4f\u969c\uff0c\u5206\u79bb\u5b89\u5168\u4e0e\u4e0d\u5b89\u5168\u72b6\u6001\uff0c\u63d0\u5347\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u7684\u9c81\u68d2\u6027\u548c\u6570\u636e\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u884c\u4e3a\u514b\u9686\u4e2d\u56e0\u534f\u53d8\u91cf\u504f\u79fb\u5bfc\u81f4\u7684\u7b56\u7565\u8106\u5f31\u6027\u95ee\u9898\uff0c\u907f\u514d\u4f9d\u8d56\u4eba\u5de5\u4fee\u6b63\u6216\u6570\u636e\u589e\u5f3a\u3002", "method": "\u7ed3\u5408\u6269\u6563\u7b56\u7565\u548c\u52a8\u6001\u6a21\u578b\uff0c\u524d\u8005\u57fa\u4e8e\u4e13\u5bb6\u6570\u636e\uff0c\u540e\u8005\u5229\u7528\u4e13\u5bb6\u548c\u6b21\u4f18\u7b56\u7565\u6570\u636e\u9884\u6d4b\u672a\u6765\u72b6\u6001\u5e76\u4f18\u5316\u5176\u4fdd\u6301\u5728\u4e13\u5bb6\u5206\u5e03\u5185\u3002", "result": "\u6a21\u62df\u548c\u5b9e\u9645\u5b9e\u9a8c\u8868\u660e\uff0cLPB\u63d0\u5347\u4e86\u7b56\u7565\u9c81\u68d2\u6027\u548c\u6570\u636e\u6548\u7387\uff0c\u65e0\u9700\u989d\u5916\u4eba\u5de5\u5e72\u9884\u3002", "conclusion": "LPB\u4e3a\u6709\u9650\u4e13\u5bb6\u6570\u636e\u4e0b\u7684\u53ef\u9760\u64cd\u4f5c\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.06042", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06042", "abs": "https://arxiv.org/abs/2508.06042", "authors": ["Daechul Ahn", "San Kim", "Jonghyun Choi"], "title": "Society of Mind Meets Real-Time Strategy: A Hierarchical Multi-Agent Framework for Strategic Reasoning", "comment": "COLM 2025", "summary": "Large Language Models (LLMs) have recently demonstrated impressive action\nsequence prediction capabilities but often struggle with dynamic, long-horizon\ntasks such as real-time strategic games. In a game such as StarCraftII (SC2),\nagents need to manage resource constraints and adapt to evolving battlefield\nsituations in a partially observable environment. This often overwhelms\nexisiting LLM-based approaches. To address these challenges, we propose a\nhierarchical multi-agent framework that employs specialized imitation learning\nagents under a meta-controller called Strategic Planner (SP). By expert\ndemonstrations, each specialized agent learns a distinctive strategy, such as\naerial support or defensive maneuvers, and produces coherent, structured\nmultistep action sequences. The SP then orchestrates these proposals into a\nsingle, environmentally adaptive plan that ensures local decisions aligning\nwith long-term strategies. We call this HIMA (Hierarchical Imitation\nMulti-Agent). We also present TEXTSCII-ALL, a comprehensive SC2 testbed that\nencompasses all race match combinations in SC2. Our empirical results show that\nHIMA outperforms state of the arts in strategic clarity, adaptability, and\ncomputational efficiency, underscoring the potential of combining specialized\nimitation modules with meta-level orchestration to develop more robust,\ngeneral-purpose AI agents.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHIMA\u7684\u5206\u5c42\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u4eff\u5b66\u4e60\u548c\u5143\u63a7\u5236\u5668\uff08Strategic Planner\uff09\u89e3\u51b3LLM\u5728\u52a8\u6001\u957f\u65f6\u4efb\u52a1\uff08\u5982\u300a\u661f\u9645\u4e89\u9738II\u300b\uff09\u4e2d\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709LLM\u5728\u52a8\u6001\u3001\u957f\u65f6\u4efb\u52a1\uff08\u5982\u300a\u661f\u9645\u4e89\u9738II\u300b\uff09\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u96be\u4ee5\u5e94\u5bf9\u8d44\u6e90\u7ea6\u675f\u548c\u90e8\u5206\u53ef\u89c2\u6d4b\u73af\u5883\u4e0b\u7684\u6218\u573a\u53d8\u5316\u3002", "method": "\u91c7\u7528\u5206\u5c42\u591a\u667a\u80fd\u4f53\u6846\u67b6HIMA\uff0c\u7ed3\u5408\u6a21\u4eff\u5b66\u4e60\u8bad\u7ec3\u4e13\u7528\u667a\u80fd\u4f53\uff08\u5982\u7a7a\u4e2d\u652f\u63f4\u6216\u9632\u5fa1\u673a\u52a8\uff09\uff0c\u5e76\u7531\u5143\u63a7\u5236\u5668SP\u534f\u8c03\u751f\u6210\u9002\u5e94\u6027\u8ba1\u5212\u3002", "result": "HIMA\u5728\u6218\u7565\u6e05\u6670\u5ea6\u3001\u9002\u5e94\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u4e86\u4e13\u7528\u6a21\u4eff\u6a21\u5757\u4e0e\u5143\u7ea7\u534f\u8c03\u7ed3\u5408\u7684\u6f5c\u529b\u3002", "conclusion": "HIMA\u6846\u67b6\u5c55\u793a\u4e86\u901a\u8fc7\u4e13\u7528\u6a21\u4eff\u6a21\u5757\u4e0e\u5143\u63a7\u5236\u5668\u7ed3\u5408\uff0c\u5f00\u53d1\u66f4\u9c81\u68d2\u3001\u901a\u7528AI\u667a\u80fd\u4f53\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2508.05946", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.05946", "abs": "https://arxiv.org/abs/2508.05946", "authors": ["Nello Balossino", "Rossana Damiano", "Cristina Gena", "Alberto Lillo", "Anna Maria Marras", "Claudio Mattutino", "Antonio Pizzo", "Alessia Prin", "Fabiana Vernero"], "title": "Social and Telepresence Robots for Accessibility and Inclusion in Small Museums", "comment": null, "summary": "There are still many museums that present accessibility barriers,\nparticularly regarding perceptual, cultural, and cognitive aspects. This is\nespecially evident in low-density population areas. The aim of the ROBSO-PM\nproject is to improve the accessibility of small museums through the use of\nsocial robots and social telepresence robots, focusing on three museums as case\nstudies: the Museum of the Holy Shroud in Turin, a small but globally known\ninstitution, and two lesser known mountain museums: the Museum of the Champlas\ndu Col Carnival and the Pragelato Museum of Alpine Peoples' Costumes and\nTraditions. The project explores two main applications for robots: as guides\nsupporting inclusive visits for foreign or disabled visitors, and as\ntelepresence tools allowing people with limited mobility to access museums\nremotely. From a research perspective, key topics include storytelling, robot\npersonality, empathy, personalization, and, in the case of telepresence,\ncollaboration between the robot and the person, with clearly defined roles and\nautonomy.", "AI": {"tldr": "ROBSO-PM\u9879\u76ee\u65e8\u5728\u901a\u8fc7\u793e\u4ea4\u673a\u5668\u4eba\u548c\u8fdc\u7a0b\u793e\u4ea4\u673a\u5668\u4eba\u63d0\u5347\u5c0f\u578b\u535a\u7269\u9986\u7684\u53ef\u8bbf\u95ee\u6027\uff0c\u91cd\u70b9\u5173\u6ce8\u611f\u77e5\u3001\u6587\u5316\u548c\u8ba4\u77e5\u969c\u788d\u3002", "motivation": "\u8bb8\u591a\u535a\u7269\u9986\u5b58\u5728\u53ef\u8bbf\u95ee\u6027\u969c\u788d\uff0c\u5c24\u5176\u662f\u5728\u4f4e\u4eba\u53e3\u5bc6\u5ea6\u5730\u533a\uff0c\u9879\u76ee\u5e0c\u671b\u901a\u8fc7\u6280\u672f\u624b\u6bb5\u6539\u5584\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u9879\u76ee\u4ee5\u4e09\u4e2a\u535a\u7269\u9986\u4e3a\u6848\u4f8b\u7814\u7a76\uff0c\u63a2\u7d22\u673a\u5668\u4eba\u4f5c\u4e3a\u5bfc\u89c8\u5de5\u5177\u548c\u8fdc\u7a0b\u8bbf\u95ee\u5de5\u5177\u7684\u5e94\u7528\uff0c\u6d89\u53ca\u8bb2\u6545\u4e8b\u3001\u673a\u5668\u4eba\u4e2a\u6027\u3001\u5171\u60c5\u7b49\u6280\u672f\u3002", "result": "\u673a\u5668\u4eba\u53ef\u7528\u4e8e\u652f\u6301\u5305\u5bb9\u6027\u53c2\u89c2\u548c\u8fdc\u7a0b\u8bbf\u95ee\uff0c\u7814\u7a76\u91cd\u70b9\u5305\u62ec\u89d2\u8272\u5b9a\u4e49\u548c\u81ea\u4e3b\u6027\u3002", "conclusion": "\u793e\u4ea4\u673a\u5668\u4eba\u6709\u671b\u63d0\u5347\u5c0f\u578b\u535a\u7269\u9986\u7684\u53ef\u8bbf\u95ee\u6027\uff0c\u5c24\u5176\u5728\u611f\u77e5\u548c\u6587\u5316\u969c\u788d\u65b9\u9762\u3002"}}
{"id": "2508.06060", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06060", "abs": "https://arxiv.org/abs/2508.06060", "authors": ["Sankarshan Damle", "Boi Faltings"], "title": "LLMs for Resource Allocation: A Participatory Budgeting Approach to Inferring Preferences", "comment": "Published in the Proceedings of the 28th European Conference on\n  Artificial Intelligence (ECAI 2025)", "summary": "Large Language Models (LLMs) are increasingly expected to handle complex\ndecision-making tasks, yet their ability to perform structured resource\nallocation remains underexplored. Evaluating their reasoning is also difficult\ndue to data contamination and the static nature of existing benchmarks. We\npresent a dual-purpose framework leveraging Participatory Budgeting (PB) both\nas (i) a practical setting for LLM-based resource allocation and (ii) an\nadaptive benchmark for evaluating their reasoning capabilities. We task LLMs\nwith selecting project subsets under feasibility (e.g., budget) constraints via\nthree prompting strategies: greedy selection, direct optimization, and a\nhill-climbing-inspired refinement. We benchmark LLMs' allocations against a\nutility-maximizing oracle. Interestingly, we also test whether LLMs can infer\nstructured preferences from natural-language voter input or metadata, without\nexplicit votes. By comparing allocations based on inferred preferences to those\nfrom ground-truth votes, we evaluate LLMs' ability to extract preferences from\nopen-ended input. Our results underscore the role of prompt design and show\nthat LLMs hold promise for mechanism design with unstructured inputs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u53cc\u7528\u9014\u6846\u67b6\uff0c\u5229\u7528\u53c2\u4e0e\u5f0f\u9884\u7b97\uff08PB\uff09\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8d44\u6e90\u5206\u914d\u548c\u63a8\u7406\u80fd\u529b\u65b9\u9762\u7684\u8868\u73b0\u3002", "motivation": "\u63a2\u7d22LLMs\u5728\u7ed3\u6784\u5316\u8d44\u6e90\u5206\u914d\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\uff0c\u5e76\u89e3\u51b3\u73b0\u6709\u8bc4\u6d4b\u65b9\u6cd5\u56e0\u6570\u636e\u6c61\u67d3\u548c\u9759\u6001\u6027\u5e26\u6765\u7684\u5c40\u9650\u6027\u3002", "method": "\u901a\u8fc7\u4e09\u79cd\u63d0\u793a\u7b56\u7565\uff08\u8d2a\u5a6a\u9009\u62e9\u3001\u76f4\u63a5\u4f18\u5316\u548c\u722c\u5c71\u5f0f\u6539\u8fdb\uff09\u8ba9LLMs\u5728\u9884\u7b97\u7ea6\u675f\u4e0b\u9009\u62e9\u9879\u76ee\u5b50\u96c6\uff0c\u5e76\u5bf9\u6bd4\u6548\u7528\u6700\u5927\u5316\u57fa\u51c6\u3002\u8fd8\u6d4b\u8bd5LLMs\u80fd\u5426\u4ece\u81ea\u7136\u8bed\u8a00\u8f93\u5165\u63a8\u65ad\u7ed3\u6784\u5316\u504f\u597d\u3002", "result": "\u7ed3\u679c\u8868\u660e\u63d0\u793a\u8bbe\u8ba1\u5bf9\u6027\u80fd\u6709\u91cd\u8981\u5f71\u54cd\uff0cLLMs\u5728\u673a\u5236\u8bbe\u8ba1\u4e2d\u5904\u7406\u975e\u7ed3\u6784\u5316\u8f93\u5165\u65b9\u9762\u5177\u6709\u6f5c\u529b\u3002", "conclusion": "LLMs\u5728\u8d44\u6e90\u5206\u914d\u548c\u504f\u597d\u63a8\u65ad\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u6f5c\u529b\uff0c\u63d0\u793a\u8bbe\u8ba1\u662f\u5173\u952e\u56e0\u7d20\u3002"}}
{"id": "2508.05972", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.05972", "abs": "https://arxiv.org/abs/2508.05972", "authors": ["Shaoting Liu", "Zhou Liu"], "title": "Dynamical Trajectory Planning of Disturbance Consciousness for Air-Land Bimodal Unmanned Aerial Vehicles", "comment": null, "summary": "Air-land bimodal vehicles provide a promising solution for navigating complex\nenvironments by combining the flexibility of aerial locomotion with the energy\nefficiency of ground mobility. To enhance the robustness of trajectory planning\nunder environmental disturbances, this paper presents a disturbance-aware\nplanning framework that incorporates real-time disturbance estimation into both\npath searching and trajectory optimization. A key component of the framework is\na disturbance-adaptive safety boundary adjustment mechanism, which dynamically\nmodifies the vehicle's feasible dynamic boundaries based on estimated\ndisturbances to ensure trajectory feasibility. Leveraging the dynamics model of\nthe bimodal vehicle, the proposed approach achieves adaptive and reliable\nmotion planning across different terrains and operating conditions. A series of\nreal-world experiments and benchmark comparisons on a custom-built platform\nvalidate the effectiveness and robustness of the method, demonstrating\nimprovements in tracking accuracy, task efficiency, and energy performance\nunder both ground and aerial disturbances.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6270\u52a8\u611f\u77e5\u7684\u8f68\u8ff9\u89c4\u5212\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u7a7a\u9646\u53cc\u6a21\u8f66\u8f86\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u7ed3\u5408\u7a7a\u4e2d\u548c\u5730\u9762\u79fb\u52a8\u7684\u4f18\u52bf\uff0c\u7a7a\u9646\u53cc\u6a21\u8f66\u8f86\u5728\u590d\u6742\u73af\u5883\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u73af\u5883\u6270\u52a8\u4f1a\u5f71\u54cd\u8f68\u8ff9\u89c4\u5212\u7684\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6270\u52a8\u611f\u77e5\u7684\u89c4\u5212\u6846\u67b6\uff0c\u5305\u62ec\u5b9e\u65f6\u6270\u52a8\u4f30\u8ba1\u3001\u8def\u5f84\u641c\u7d22\u548c\u8f68\u8ff9\u4f18\u5316\uff0c\u5e76\u5f15\u5165\u6270\u52a8\u81ea\u9002\u5e94\u5b89\u5168\u8fb9\u754c\u8c03\u6574\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5728\u8ddf\u8e2a\u7cbe\u5ea6\u3001\u4efb\u52a1\u6548\u7387\u548c\u80fd\u91cf\u6027\u80fd\u65b9\u9762\u5747\u6709\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u9002\u5e94\u4e0d\u540c\u5730\u5f62\u548c\u64cd\u4f5c\u6761\u4ef6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u53cc\u6a21\u8f66\u8f86\u7684\u8fd0\u52a8\u89c4\u5212\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.06062", "categories": ["cs.AI", "cs.LG", "cs.LO", "68T27, 68T30"], "pdf": "https://arxiv.org/pdf/2508.06062", "abs": "https://arxiv.org/abs/2508.06062", "authors": ["Evgenii E. Vityaev", "Andrei Mantsivoda"], "title": "Don't Forget Imagination!", "comment": "14 pages, 2 figures", "summary": "Cognitive imagination is a type of imagination that plays a key role in human\nthinking. It is not a ``picture-in-the-head'' imagination. It is a faculty to\nmentally visualize coherent and holistic systems of concepts and causal links\nthat serve as semantic contexts for reasoning, decision making and prediction.\nOur position is that the role of cognitive imagination is still greatly\nunderestimated, and this creates numerous problems and diminishes the current\ncapabilities of AI. For instance, when reasoning, humans rely on imaginary\ncontexts to retrieve background info. They also constantly return to the\ncontext for semantic verification that their reasoning is still reasonable.\nThus, reasoning without imagination is blind. This paper is a call for greater\nattention to cognitive imagination as the next promising breakthrough in\nartificial intelligence. As an instrument for simulating cognitive imagination,\nwe propose semantic models -- a new approach to mathematical models that can\nlearn, like neural networks, and are based on probabilistic causal\nrelationships. Semantic models can simulate cognitive imagination because they\nensure the consistency of imaginary contexts and implement a glass-box approach\nthat allows the context to be manipulated as a holistic and coherent system of\ninterrelated facts glued together with causal relations.", "AI": {"tldr": "\u672c\u6587\u547c\u5401\u91cd\u89c6\u8ba4\u77e5\u60f3\u8c61\u529b\u5728\u4eba\u5de5\u667a\u80fd\u4e2d\u7684\u5173\u952e\u4f5c\u7528\uff0c\u5e76\u63d0\u51fa\u8bed\u4e49\u6a21\u578b\u4f5c\u4e3a\u6a21\u62df\u8ba4\u77e5\u60f3\u8c61\u529b\u7684\u5de5\u5177\u3002", "motivation": "\u8ba4\u77e5\u60f3\u8c61\u529b\u5728\u4eba\u7c7b\u601d\u7ef4\u4e2d\u626e\u6f14\u91cd\u8981\u89d2\u8272\uff0c\u4f46\u5f53\u524dAI\u9886\u57df\u5bf9\u5176\u91cd\u89c6\u4e0d\u8db3\uff0c\u5bfc\u81f4\u63a8\u7406\u548c\u51b3\u7b56\u80fd\u529b\u53d7\u9650\u3002", "method": "\u63d0\u51fa\u8bed\u4e49\u6a21\u578b\uff0c\u4e00\u79cd\u57fa\u4e8e\u6982\u7387\u56e0\u679c\u5173\u7cfb\u7684\u6570\u5b66\u6a21\u578b\uff0c\u80fd\u591f\u5b66\u4e60\u5e76\u786e\u4fdd\u60f3\u8c61\u4e0a\u4e0b\u6587\u7684\u8fde\u8d2f\u6027\u3002", "result": "\u8bed\u4e49\u6a21\u578b\u80fd\u591f\u6a21\u62df\u8ba4\u77e5\u60f3\u8c61\u529b\uff0c\u652f\u6301\u4e00\u81f4\u4e14\u53ef\u64cd\u4f5c\u7684\u60f3\u8c61\u4e0a\u4e0b\u6587\u3002", "conclusion": "\u8ba4\u77e5\u60f3\u8c61\u529b\u662fAI\u672a\u6765\u7684\u91cd\u8981\u7a81\u7834\u65b9\u5411\uff0c\u8bed\u4e49\u6a21\u578b\u4e3a\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2508.06053", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06053", "abs": "https://arxiv.org/abs/2508.06053", "authors": ["Kaixuan Wu", "Yuanzhuo Xu", "Zejun Zhang", "Weiping Zhu", "Steve Drew", "Xiaoguang Niu"], "title": "ReNiL: Relative Neural Inertial Locator with Any-Scale Bayesian Inference", "comment": null, "summary": "Pedestrian inertial localization is key for mobile and IoT services because\nit provides infrastructure-free positioning. Yet most learning-based methods\ndepend on fixed sliding-window integration, struggle to adapt to diverse motion\nscales and cadences, and yield inconsistent uncertainty, limiting real-world\nuse. We present ReNiL, a Bayesian deep-learning framework for accurate,\nefficient, and uncertainty-aware pedestrian localization. ReNiL introduces\nInertial Positioning Demand Points (IPDPs) to estimate motion at contextually\nmeaningful waypoints instead of dense tracking, and supports inference on IMU\nsequences at any scale so cadence can match application needs. It couples a\nmotion-aware orientation filter with an Any-Scale Laplace Estimator (ASLE), a\ndual-task network that blends patch-based self-supervision with Bayesian\nregression. By modeling displacements with a Laplace distribution, ReNiL\nprovides homogeneous Euclidean uncertainty that integrates cleanly with other\nsensors. A Bayesian inference chain links successive IPDPs into consistent\ntrajectories. On RoNIN-ds and a new WUDataset covering indoor and outdoor\nmotion from 28 participants, ReNiL achieves state-of-the-art displacement\naccuracy and uncertainty consistency, outperforming TLIO, CTIN, iMoT, and RoNIN\nvariants while reducing computation. Application studies further show\nrobustness and practicality for mobile and IoT localization, making ReNiL a\nscalable, uncertainty-aware foundation for next-generation positioning.", "AI": {"tldr": "ReNiL\u662f\u4e00\u4e2a\u8d1d\u53f6\u65af\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u6548\u3001\u51c6\u786e\u4e14\u5177\u6709\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u884c\u4eba\u60ef\u6027\u5b9a\u4f4d\uff0c\u901a\u8fc7IPDPs\u548cASLE\u6280\u672f\u5b9e\u73b0\u591a\u5c3a\u5ea6\u8fd0\u52a8\u9002\u5e94\u548c\u4e00\u81f4\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3002", "motivation": "\u73b0\u6709\u5b66\u4e60\u578b\u65b9\u6cd5\u4f9d\u8d56\u56fa\u5b9a\u6ed1\u52a8\u7a97\u53e3\uff0c\u96be\u4ee5\u9002\u5e94\u591a\u6837\u5316\u8fd0\u52a8\u5c3a\u5ea6\u548c\u6b65\u9891\uff0c\u4e14\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u4e0d\u4e00\u81f4\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "ReNiL\u5f15\u5165IPDPs\u5728\u5173\u952e\u8def\u5f84\u70b9\u4f30\u8ba1\u8fd0\u52a8\uff0c\u7ed3\u5408ASLE\u7f51\u7edc\uff08\u878d\u5408\u81ea\u76d1\u7763\u4e0e\u8d1d\u53f6\u65af\u56de\u5f52\uff09\u548c\u8fd0\u52a8\u611f\u77e5\u65b9\u5411\u6ee4\u6ce2\u5668\uff0c\u652f\u6301\u4efb\u610f\u5c3a\u5ea6\u7684IMU\u5e8f\u5217\u63a8\u7406\u3002", "result": "\u5728RoNIN-ds\u548cWUDataset\u4e0a\uff0cReNiL\u5728\u4f4d\u79fb\u7cbe\u5ea6\u548c\u4e0d\u786e\u5b9a\u6027\u4e00\u81f4\u6027\u4e0a\u8fbe\u5230SOTA\uff0c\u4f18\u4e8eTLIO\u3001CTIN\u7b49\u65b9\u6cd5\uff0c\u540c\u65f6\u964d\u4f4e\u8ba1\u7b97\u91cf\u3002", "conclusion": "ReNiL\u4e3a\u79fb\u52a8\u548cIoT\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u3001\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u57fa\u7840\uff0c\u5c55\u793a\u4e86\u5b9e\u9645\u5e94\u7528\u7684\u9c81\u68d2\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2508.06064", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06064", "abs": "https://arxiv.org/abs/2508.06064", "authors": ["Harold Silv\u00e8re Kiossou", "Siegfried Nijssen", "Pierre Schaus"], "title": "A Generic Complete Anytime Beam Search for Optimal Decision Tree", "comment": null, "summary": "Finding an optimal decision tree that minimizes classification error is known\nto be NP-hard. While exact algorithms based on MILP, CP, SAT, or dynamic\nprogramming guarantee optimality, they often suffer from poor anytime behavior\n-- meaning they struggle to find high-quality decision trees quickly when the\nsearch is stopped before completion -- due to unbalanced search space\nexploration. To address this, several anytime extensions of exact methods have\nbeen proposed, such as LDS-DL8.5, Top-k-DL8.5, and Blossom, but they have not\nbeen systematically compared, making it difficult to assess their relative\neffectiveness. In this paper, we propose CA-DL8.5, a generic, complete, and\nanytime beam search algorithm that extends the DL8.5 framework and unifies some\nexisting anytime strategies. In particular, CA-DL8.5 generalizes previous\napproaches LDS-DL8.5 and Top-k-DL8.5, by allowing the integration of various\nheuristics and relaxation mechanisms through a modular design. The algorithm\nreuses DL8.5's efficient branch-and-bound pruning and trie-based caching,\ncombined with a restart-based beam search that gradually relaxes pruning\ncriteria to improve solution quality over time. Our contributions are twofold:\n(1) We introduce this new generic framework for exact and anytime decision tree\nlearning, enabling the incorporation of diverse heuristics and search\nstrategies; (2) We conduct a rigorous empirical comparison of several\ninstantiations of CA-DL8.5 -- based on Purity, Gain, Discrepancy, and Top-k\nheuristics -- using an anytime evaluation metric called the primal gap\nintegral. Experimental results on standard classification benchmarks show that\nCA-DL8.5 using LDS (limited discrepancy) consistently provides the best anytime\nperformance, outperforming both other CA-DL8.5 variants and the Blossom\nalgorithm while maintaining completeness and optimality guarantees.", "AI": {"tldr": "CA-DL8.5\u662f\u4e00\u79cd\u901a\u7528\u7684\u3001\u5b8c\u6574\u7684\u3001\u968f\u65f6\u53ef\u7528\u7684\u6ce2\u675f\u641c\u7d22\u7b97\u6cd5\uff0c\u6269\u5c55\u4e86DL8.5\u6846\u67b6\uff0c\u7edf\u4e00\u4e86\u73b0\u6709\u7684\u968f\u65f6\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u6a21\u5757\u5316\u8bbe\u8ba1\u6574\u5408\u591a\u79cd\u542f\u53d1\u5f0f\u548c\u677e\u5f1b\u673a\u5236\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u7cbe\u786e\u7b97\u6cd5\u5728\u672a\u5b8c\u6210\u641c\u7d22\u65f6\u96be\u4ee5\u5feb\u901f\u627e\u5230\u9ad8\u8d28\u91cf\u51b3\u7b56\u6811\u7684\u95ee\u9898\uff0c\u5e76\u7cfb\u7edf\u6bd4\u8f83\u73b0\u6709\u968f\u65f6\u6269\u5c55\u65b9\u6cd5\u7684\u76f8\u5bf9\u6709\u6548\u6027\u3002", "method": "\u63d0\u51faCA-DL8.5\u7b97\u6cd5\uff0c\u7ed3\u5408DL8.5\u7684\u9ad8\u6548\u526a\u679d\u548c\u7f13\u5b58\u6280\u672f\uff0c\u91c7\u7528\u57fa\u4e8e\u91cd\u542f\u7684\u6ce2\u675f\u641c\u7d22\u9010\u6b65\u653e\u5bbd\u526a\u679d\u6807\u51c6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8eLDS\u542f\u53d1\u5f0f\u7684CA-DL8.5\u5728\u968f\u65f6\u6027\u80fd\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u4f18\u4e8e\u5176\u4ed6\u53d8\u4f53\u548cBlossom\u7b97\u6cd5\u3002", "conclusion": "CA-DL8.5\u4e3a\u51b3\u7b56\u6811\u5b66\u4e60\u63d0\u4f9b\u4e86\u901a\u7528\u6846\u67b6\uff0c\u652f\u6301\u591a\u6837\u542f\u53d1\u5f0f\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002"}}
{"id": "2508.06095", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06095", "abs": "https://arxiv.org/abs/2508.06095", "authors": ["Mitchell Abrams", "Thies Oelerich", "Christian Hartl-Nesic", "Andreas Kugi", "Matthias Scheutz"], "title": "Incremental Language Understanding for Online Motion Planning of Robot Manipulators", "comment": "8 pages, 9 figures, accepted at IROS 2025", "summary": "Human-robot interaction requires robots to process language incrementally,\nadapting their actions in real-time based on evolving speech input. Existing\napproaches to language-guided robot motion planning typically assume fully\nspecified instructions, resulting in inefficient stop-and-replan behavior when\ncorrections or clarifications occur. In this paper, we introduce a novel\nreasoning-based incremental parser which integrates an online motion planning\nalgorithm within the cognitive architecture. Our approach enables continuous\nadaptation to dynamic linguistic input, allowing robots to update motion plans\nwithout restarting execution. The incremental parser maintains multiple\ncandidate parses, leveraging reasoning mechanisms to resolve ambiguities and\nrevise interpretations when needed. By combining symbolic reasoning with online\nmotion planning, our system achieves greater flexibility in handling speech\ncorrections and dynamically changing constraints. We evaluate our framework in\nreal-world human-robot interaction scenarios, demonstrating online adaptions of\ngoal poses, constraints, or task objectives. Our results highlight the\nadvantages of integrating incremental language understanding with real-time\nmotion planning for natural and fluid human-robot collaboration. The\nexperiments are demonstrated in the accompanying video at\nwww.acin.tuwien.ac.at/42d5.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63a8\u7406\u7684\u589e\u91cf\u89e3\u6790\u5668\uff0c\u7ed3\u5408\u5728\u7ebf\u8fd0\u52a8\u89c4\u5212\u7b97\u6cd5\uff0c\u5b9e\u73b0\u673a\u5668\u4eba\u5bf9\u52a8\u6001\u8bed\u8a00\u8f93\u5165\u7684\u5b9e\u65f6\u9002\u5e94\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5047\u8bbe\u6307\u4ee4\u5b8c\u5168\u660e\u786e\uff0c\u5bfc\u81f4\u4fee\u6b63\u6216\u6f84\u6e05\u65f6\u9700\u505c\u6b62\u5e76\u91cd\u65b0\u89c4\u5212\uff0c\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u5f15\u5165\u589e\u91cf\u89e3\u6790\u5668\uff0c\u7ef4\u62a4\u591a\u4e2a\u5019\u9009\u89e3\u6790\uff0c\u7ed3\u5408\u7b26\u53f7\u63a8\u7406\u4e0e\u5728\u7ebf\u8fd0\u52a8\u89c4\u5212\u3002", "result": "\u7cfb\u7edf\u80fd\u5b9e\u65f6\u8c03\u6574\u8fd0\u52a8\u8ba1\u5212\uff0c\u9002\u5e94\u76ee\u6807\u4f4d\u59ff\u3001\u7ea6\u675f\u6216\u4efb\u52a1\u76ee\u6807\u7684\u52a8\u6001\u53d8\u5316\u3002", "conclusion": "\u589e\u91cf\u8bed\u8a00\u7406\u89e3\u4e0e\u5b9e\u65f6\u8fd0\u52a8\u89c4\u5212\u7684\u7ed3\u5408\u63d0\u5347\u4e86\u4eba\u673a\u534f\u4f5c\u7684\u81ea\u7136\u6027\u548c\u6d41\u7545\u6027\u3002"}}
{"id": "2508.06074", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06074", "abs": "https://arxiv.org/abs/2508.06074", "authors": ["Siyi Lu", "Run Liu", "Dongsheng Yang", "Lei He"], "title": "ME$^3$-BEV: Mamba-Enhanced Deep Reinforcement Learning for End-to-End Autonomous Driving with BEV-Perception", "comment": null, "summary": "Autonomous driving systems face significant challenges in perceiving complex\nenvironments and making real-time decisions. Traditional modular approaches,\nwhile offering interpretability, suffer from error propagation and coordination\nissues, whereas end-to-end learning systems can simplify the design but face\ncomputational bottlenecks. This paper presents a novel approach to autonomous\ndriving using deep reinforcement learning (DRL) that integrates bird's-eye view\n(BEV) perception for enhanced real-time decision-making. We introduce the\n\\texttt{Mamba-BEV} model, an efficient spatio-temporal feature extraction\nnetwork that combines BEV-based perception with the Mamba framework for\ntemporal feature modeling. This integration allows the system to encode vehicle\nsurroundings and road features in a unified coordinate system and accurately\nmodel long-range dependencies. Building on this, we propose the\n\\texttt{ME$^3$-BEV} framework, which utilizes the \\texttt{Mamba-BEV} model as a\nfeature input for end-to-end DRL, achieving superior performance in dynamic\nurban driving scenarios. We further enhance the interpretability of the model\nby visualizing high-dimensional features through semantic segmentation,\nproviding insight into the learned representations. Extensive experiments on\nthe CARLA simulator demonstrate that \\texttt{ME$^3$-BEV} outperforms existing\nmodels across multiple metrics, including collision rate and trajectory\naccuracy, offering a promising solution for real-time autonomous driving.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u548c\u9e1f\u77b0\u56fe\uff08BEV\uff09\u611f\u77e5\u7684\u65b0\u578b\u81ea\u52a8\u9a7e\u9a76\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408BEV\u611f\u77e5\u548cMamba\u6846\u67b6\u7684\u65f6\u7a7a\u7279\u5f81\u63d0\u53d6\u7f51\u7edc\uff0c\u63d0\u5347\u4e86\u5b9e\u65f6\u51b3\u7b56\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u6a21\u5757\u5316\u65b9\u6cd5\u5b58\u5728\u9519\u8bef\u4f20\u64ad\u548c\u534f\u8c03\u95ee\u9898\uff0c\u7aef\u5230\u7aef\u5b66\u4e60\u7cfb\u7edf\u5219\u9762\u4e34\u8ba1\u7b97\u74f6\u9888\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u7b80\u5316\u8bbe\u8ba1\u53c8\u80fd\u63d0\u5347\u6027\u80fd\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51faMamba-BEV\u6a21\u578b\uff0c\u7ed3\u5408BEV\u611f\u77e5\u548cMamba\u6846\u67b6\u8fdb\u884c\u65f6\u7a7a\u7279\u5f81\u5efa\u6a21\uff0c\u5e76\u8fdb\u4e00\u6b65\u63d0\u51faME\u00b3-BEV\u6846\u67b6\uff0c\u7528\u4e8e\u7aef\u5230\u7aefDRL\u3002", "result": "\u5728CARLA\u6a21\u62df\u5668\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cME\u00b3-BEV\u5728\u78b0\u649e\u7387\u548c\u8f68\u8ff9\u7cbe\u5ea6\u7b49\u591a\u9879\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "conclusion": "ME\u00b3-BEV\u4e3a\u5b9e\u65f6\u81ea\u52a8\u9a7e\u9a76\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.06096", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06096", "abs": "https://arxiv.org/abs/2508.06096", "authors": ["Eric Jing", "Abdeslam Boularias"], "title": "Bounding Distributional Shifts in World Modeling through Novelty Detection", "comment": "7 pages, 6 figures", "summary": "Recent work on visual world models shows significant promise in latent state\ndynamics obtained from pre-trained image backbones. However, most of the\ncurrent approaches are sensitive to training quality, requiring near-complete\ncoverage of the action and state space during training to prevent divergence\nduring inference. To make a model-based planning algorithm more robust to the\nquality of the learned world model, we propose in this work to use a\nvariational autoencoder as a novelty detector to ensure that proposed action\ntrajectories during planning do not cause the learned model to deviate from the\ntraining data distribution. To evaluate the effectiveness of this approach, a\nseries of experiments in challenging simulated robot environments was carried\nout, with the proposed method incorporated into a model-predictive control\npolicy loop extending the DINO-WM architecture. The results clearly show that\nthe proposed method improves over state-of-the-art solutions in terms of data\nefficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53d8\u5206\u81ea\u7f16\u7801\u5668\u7684\u65b0\u9896\u6027\u68c0\u6d4b\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u6a21\u578b\u89c4\u5212\u5bf9\u5b66\u4e60\u4e16\u754c\u6a21\u578b\u8d28\u91cf\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u4e16\u754c\u6a21\u578b\u5bf9\u8bad\u7ec3\u8d28\u91cf\u654f\u611f\uff0c\u9700\u8981\u8fd1\u4e4e\u5b8c\u6574\u7684\u52a8\u4f5c\u548c\u72b6\u6001\u7a7a\u95f4\u8986\u76d6\u4ee5\u9632\u6b62\u63a8\u7406\u65f6\u53d1\u6563\u3002", "method": "\u4f7f\u7528\u53d8\u5206\u81ea\u7f16\u7801\u5668\u4f5c\u4e3a\u65b0\u9896\u6027\u68c0\u6d4b\u5668\uff0c\u786e\u4fdd\u89c4\u5212\u4e2d\u7684\u52a8\u4f5c\u8f68\u8ff9\u4e0d\u504f\u79bb\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u3002", "result": "\u5728\u6a21\u62df\u673a\u5668\u4eba\u73af\u5883\u4e2d\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u6570\u636e\u6548\u7387\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u89c4\u5212\u5bf9\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u7684\u9c81\u68d2\u6027\u548c\u6570\u636e\u6548\u7387\u3002"}}
{"id": "2508.06091", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06091", "abs": "https://arxiv.org/abs/2508.06091", "authors": ["Stan P Hauke", "Przemys\u0142aw Andrzej Wa\u0142\u0119ga"], "title": "Aggregate-Combine-Readout GNNs Are More Expressive Than Logic C2", "comment": "18 pages", "summary": "In recent years, there has been growing interest in understanding the\nexpressive power of graph neural networks (GNNs) by relating them to logical\nlanguages. This research has been been initialised by an influential result of\nBarcel\\'o et al. (2020), who showed that the graded modal logic (or a guarded\nfragment of the logic C2), characterises the logical expressiveness of\naggregate-combine GNNs. As a ``challenging open problem'' they left the\nquestion whether full C2 characterises the logical expressiveness of\naggregate-combine-readout GNNs. This question has remained unresolved despite\nseveral attempts. In this paper, we solve the above open problem by proving\nthat the logical expressiveness of aggregate-combine-readout GNNs strictly\nexceeds that of C2. This result holds over both undirected and directed graphs.\nBeyond its implications for GNNs, our work also leads to purely logical\ninsights on the expressive power of infinitary logics.", "AI": {"tldr": "\u672c\u6587\u89e3\u51b3\u4e86\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u903b\u8f91\u8868\u8fbe\u80fd\u529b\u662f\u5426\u5b8c\u5168\u7531C2\u903b\u8f91\u63cf\u8ff0\u7684\u95ee\u9898\uff0c\u8bc1\u660e\u5176\u8868\u8fbe\u80fd\u529b\u4e25\u683c\u8d85\u8fc7C2\u3002", "motivation": "\u7814\u7a76GNNs\u7684\u903b\u8f91\u8868\u8fbe\u80fd\u529b\uff0c\u89e3\u51b3Barcel\u00f3\u7b49\u4eba\u63d0\u51fa\u7684\u672a\u89e3\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\uff0c\u6bd4\u8f83GNNs\u4e0eC2\u903b\u8f91\u7684\u8868\u8fbe\u80fd\u529b\u3002", "result": "\u8bc1\u660eGNNs\u7684\u903b\u8f91\u8868\u8fbe\u80fd\u529b\u4e25\u683c\u8d85\u8fc7C2\u903b\u8f91\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e0d\u4ec5\u5bf9GNNs\u6709\u91cd\u8981\u610f\u4e49\uff0c\u8fd8\u4e3a\u65e0\u7a77\u903b\u8f91\u7684\u8868\u8fbe\u80fd\u529b\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\u3002"}}
{"id": "2508.06181", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06181", "abs": "https://arxiv.org/abs/2508.06181", "authors": ["Jan W\u0119grzynowski", "Piotr Kicki", "Grzegorz Czechmanowski", "Maciej Krupka", "Krzysztof Walas"], "title": "Beyond Constant Parameters: Hyper Prediction Models and HyperMPC", "comment": null, "summary": "Model Predictive Control (MPC) is among the most widely adopted and reliable\nmethods for robot control, relying critically on an accurate dynamics model.\nHowever, existing dynamics models used in the gradient-based MPC are limited by\ncomputational complexity and state representation. To address this limitation,\nwe propose the Hyper Prediction Model (HyperPM) - a novel approach in which we\nproject the unmodeled dynamics onto a time-dependent dynamics model. This\ntime-dependency is captured through time-varying model parameters, whose\nevolution over the MPC prediction horizon is learned using a neural network.\nSuch formulation preserves the computational efficiency and robustness of the\nbase model while equipping it with the capacity to anticipate previously\nunmodeled phenomena. We evaluated the proposed approach on several challenging\nsystems, including real-world F1TENTH autonomous racing, and demonstrated that\nit significantly reduces long-horizon prediction errors. Moreover, when\nintegrated within the MPC framework (HyperMPC), our method consistently\noutperforms existing state-of-the-art techniques.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65f6\u95f4\u4f9d\u8d56\u52a8\u6001\u6a21\u578b\u7684Hyper Prediction Model\uff08HyperPM\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u68af\u5ea6MPC\u4e2d\u52a8\u6001\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u957f\u671f\u9884\u6d4b\u8bef\u5dee\u3002", "motivation": "\u73b0\u6709\u68af\u5ea6MPC\u4e2d\u7684\u52a8\u6001\u6a21\u578b\u53d7\u9650\u4e8e\u8ba1\u7b97\u590d\u6742\u6027\u548c\u72b6\u6001\u8868\u793a\uff0c\u65e0\u6cd5\u51c6\u786e\u9884\u6d4b\u672a\u5efa\u6a21\u7684\u52a8\u6001\u73b0\u8c61\u3002", "method": "\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u65f6\u95f4\u53d8\u5316\u7684\u6a21\u578b\u53c2\u6570\uff0c\u5c06\u672a\u5efa\u6a21\u52a8\u6001\u6295\u5f71\u5230\u65f6\u95f4\u4f9d\u8d56\u7684\u52a8\u6001\u6a21\u578b\u4e2d\uff0c\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002", "result": "\u5728F1TENTH\u81ea\u52a8\u9a7e\u9a76\u8d5b\u8f66\u7b49\u6311\u6218\u6027\u7cfb\u7edf\u4e2d\uff0cHyperPM\u663e\u8457\u51cf\u5c11\u4e86\u957f\u671f\u9884\u6d4b\u8bef\u5dee\uff0c\u5e76\u5728MPC\u6846\u67b6\uff08HyperMPC\uff09\u4e2d\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "HyperPM\u901a\u8fc7\u65f6\u95f4\u4f9d\u8d56\u7684\u52a8\u6001\u6a21\u578b\u63d0\u5347\u4e86MPC\u7684\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u590d\u6742\u7cfb\u7edf\u7684\u63a7\u5236\u3002"}}
{"id": "2508.06110", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.06110", "abs": "https://arxiv.org/abs/2508.06110", "authors": ["Yiran Rex Ma"], "title": "PanelTR: Zero-Shot Table Reasoning Framework Through Multi-Agent Scientific Discussion", "comment": "Accepted at IJCNN 2025", "summary": "Table reasoning, including tabular QA and fact verification, often depends on\nannotated data or complex data augmentation, limiting flexibility and\ngeneralization. LLMs, despite their versatility, often underperform compared to\nsimple supervised models. To approach these issues, we introduce PanelTR, a\nframework utilizing LLM agent scientists for robust table reasoning through a\nstructured scientific approach. PanelTR's workflow involves agent scientists\nconducting individual investigations, engaging in self-review, and\nparticipating in collaborative peer-review discussions. This process, driven by\nfive scientist personas, enables semantic-level transfer without relying on\ndata augmentation or parametric optimization. Experiments across four\nbenchmarks show that PanelTR outperforms vanilla LLMs and rivals fully\nsupervised models, all while remaining independent of training data. Our\nfindings indicate that structured scientific methodology can effectively handle\ncomplex tasks beyond table reasoning with flexible semantic understanding in a\nzero-shot context.", "AI": {"tldr": "PanelTR\u6846\u67b6\u901a\u8fc7LLM\u4ee3\u7406\u79d1\u5b66\u5bb6\u7684\u7ed3\u6784\u5316\u79d1\u5b66\u65b9\u6cd5\u63d0\u5347\u8868\u683c\u63a8\u7406\u80fd\u529b\uff0c\u65e0\u9700\u4f9d\u8d56\u6807\u6ce8\u6570\u636e\u6216\u590d\u6742\u6570\u636e\u589e\u5f3a\u3002", "motivation": "\u89e3\u51b3\u8868\u683c\u63a8\u7406\u4efb\u52a1\u4e2d\u4f9d\u8d56\u6807\u6ce8\u6570\u636e\u548c\u590d\u6742\u6570\u636e\u589e\u5f3a\u7684\u95ee\u9898\uff0c\u4ee5\u53caLLMs\u5728\u6b64\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\u7684\u5c40\u9650\u6027\u3002", "method": "PanelTR\u6846\u67b6\u5229\u7528\u4e94\u4e2a\u79d1\u5b66\u5bb6\u89d2\u8272\u8fdb\u884c\u72ec\u7acb\u8c03\u67e5\u3001\u81ea\u6211\u5ba1\u67e5\u548c\u534f\u4f5c\u540c\u884c\u8bc4\u5ba1\uff0c\u5b9e\u73b0\u8bed\u4e49\u7ea7\u8fc1\u79fb\u3002", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cPanelTR\u4f18\u4e8e\u666e\u901aLLMs\uff0c\u5e76\u4e0e\u5b8c\u5168\u76d1\u7763\u6a21\u578b\u76f8\u5f53\uff0c\u4e14\u65e0\u9700\u8bad\u7ec3\u6570\u636e\u3002", "conclusion": "\u7ed3\u6784\u5316\u79d1\u5b66\u65b9\u6cd5\u53ef\u6709\u6548\u5904\u7406\u590d\u6742\u4efb\u52a1\uff0c\u5e76\u5728\u96f6\u6837\u672c\u60c5\u5883\u4e0b\u5b9e\u73b0\u7075\u6d3b\u7684\u8bed\u4e49\u7406\u89e3\u3002"}}
{"id": "2508.06206", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06206", "abs": "https://arxiv.org/abs/2508.06206", "authors": ["Hanqing Wang", "Shaoyang Wang", "Yiming Zhong", "Zemin Yang", "Jiamin Wang", "Zhiqing Cui", "Jiahao Yuan", "Yifan Han", "Mingyu Liu", "Yuexin Ma"], "title": "Affordance-R1: Reinforcement Learning for Generalizable Affordance Reasoning in Multimodal Large Language Model", "comment": null, "summary": "Affordance grounding focuses on predicting the specific regions of objects\nthat are associated with the actions to be performed by robots. It plays a\nvital role in the fields of human-robot interaction, human-object interaction,\nembodied manipulation, and embodied perception. Existing models often neglect\nthe affordance shared among different objects because they lack the\nChain-of-Thought(CoT) reasoning abilities, limiting their out-of-domain (OOD)\ngeneralization and explicit reasoning capabilities. To address these\nchallenges, we propose Affordance-R1, the first unified affordance grounding\nframework that integrates cognitive CoT guided Group Relative Policy\nOptimization (GRPO) within a reinforcement learning paradigm. Specifically, we\ndesigned a sophisticated affordance function, which contains format,\nperception, and cognition rewards to effectively guide optimization directions.\nFurthermore, we constructed a high-quality affordance-centric reasoning\ndataset, ReasonAff, to support training. Trained exclusively via reinforcement\nlearning with GRPO and without explicit reasoning data, Affordance-R1 achieves\nrobust zero-shot generalization and exhibits emergent test-time reasoning\ncapabilities. Comprehensive experiments demonstrate that our model outperforms\nwell-established methods and exhibits open-world generalization. To the best of\nour knowledge, Affordance-R1 is the first to integrate GRPO-based RL with\nreasoning into affordance reasoning. The code of our method and our dataset is\nreleased on https://github.com/hq-King/Affordance-R1.", "AI": {"tldr": "Affordance-R1\u662f\u4e00\u4e2a\u7edf\u4e00\u7684affordance grounding\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u8ba4\u77e5\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u548c\u5f3a\u5316\u5b66\u4e60\uff08GRPO\uff09\uff0c\u63d0\u5347\u4e86\u8de8\u9886\u57df\u6cdb\u5316\u548c\u663e\u5f0f\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u56e0\u7f3a\u4e4f\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u80fd\u529b\uff0c\u5ffd\u89c6\u4e86\u4e0d\u540c\u5bf9\u8c61\u95f4\u7684affordance\u5171\u4eab\uff0c\u9650\u5236\u4e86\u6cdb\u5316\u548c\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51faAffordance-R1\u6846\u67b6\uff0c\u8bbe\u8ba1\u5305\u542b\u683c\u5f0f\u3001\u611f\u77e5\u548c\u8ba4\u77e5\u5956\u52b1\u7684affordance\u51fd\u6570\uff0c\u5e76\u6784\u5efa\u9ad8\u8d28\u91cf\u6570\u636e\u96c6ReasonAff\u3002", "result": "\u6a21\u578b\u5728\u96f6\u6837\u672c\u6cdb\u5316\u548c\u63a8\u7406\u80fd\u529b\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u5f00\u653e\u4e16\u754c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "Affordance-R1\u9996\u6b21\u5c06GRPO\u5f3a\u5316\u5b66\u4e60\u4e0e\u63a8\u7406\u7ed3\u5408\uff0c\u63a8\u52a8\u4e86affordance\u63a8\u7406\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.06111", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06111", "abs": "https://arxiv.org/abs/2508.06111", "authors": ["Dewi S. W. Gould", "Bruno Mlodozeniec", "Samuel F. Brown"], "title": "SKATE, a Scalable Tournament Eval: Weaker LLMs differentiate between stronger ones using verifiable challenges", "comment": "7 pages and appendices", "summary": "Evaluating the capabilities and risks of foundation models is paramount, yet\ncurrent methods demand extensive domain expertise, hindering their scalability\nas these models rapidly evolve. We introduce SKATE: a novel evaluation\nframework in which large language models (LLMs) compete by generating and\nsolving verifiable tasks for one another. Our core insight is to treat\nevaluation as a game: models act as both task-setters and solvers, incentivized\nto create questions which highlight their own strengths while exposing others'\nweaknesses. SKATE offers several key advantages, balancing scalability,\nopen-endedness, and objectivity. It is fully automated, data-free, and\nscalable, requiring no human input or domain expertise. By using verifiable\ntasks rather than LLM judges, scoring is objective. Unlike domain-limited\nprogrammatically-generated benchmarks (e.g. chess-playing or spatial\nreasoning), having LLMs creatively pose challenges enables open-ended and\nscalable evaluation. As a proof of concept, we introduce LLM-set\ncode-output-prediction (COP) challenges as a verifiable and extensible\nframework in which to test our approach. Using a TrueSkill-based ranking\nsystem, we evaluate six frontier LLMs and find that: (1) weaker models can\nreliably differentiate and score stronger ones, (2) LLM-based systems are\ncapable of self-preferencing behavior, generating questions that align with\ntheir own capabilities, and (3) SKATE automatically surfaces fine-grained\ncapability differences between models. Our findings are an important step\ntowards general, scalable evaluation frameworks which can keep pace with LLM\nprogress.", "AI": {"tldr": "SKATE\u662f\u4e00\u79cd\u65b0\u578b\u8bc4\u4f30\u6846\u67b6\uff0c\u5229\u7528LLM\u76f8\u4e92\u751f\u6210\u548c\u89e3\u51b3\u53ef\u9a8c\u8bc1\u4efb\u52a1\uff0c\u5b9e\u73b0\u81ea\u52a8\u5316\u3001\u53ef\u6269\u5c55\u4e14\u5ba2\u89c2\u7684\u6a21\u578b\u8bc4\u4f30\u3002", "motivation": "\u5f53\u524d\u8bc4\u4f30\u65b9\u6cd5\u4f9d\u8d56\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\uff0c\u96be\u4ee5\u9002\u5e94LLM\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u901a\u7528\u4e14\u53ef\u6269\u5c55\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "SKATE\u6846\u67b6\u5c06\u8bc4\u4f30\u89c6\u4e3a\u6e38\u620f\uff0cLLM\u65e2\u4f5c\u4e3a\u4efb\u52a1\u751f\u6210\u8005\u53c8\u4f5c\u4e3a\u89e3\u51b3\u8005\uff0c\u901a\u8fc7\u751f\u6210\u53ef\u9a8c\u8bc1\u4efb\u52a1\uff08\u5982\u4ee3\u7801\u8f93\u51fa\u9884\u6d4b\uff09\u8fdb\u884c\u4e92\u8bc4\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cSKATE\u80fd\u6709\u6548\u533a\u5206\u6a21\u578b\u80fd\u529b\uff0c\u53d1\u73b0\u6a21\u578b\u81ea\u6211\u504f\u597d\u884c\u4e3a\uff0c\u5e76\u81ea\u52a8\u63ed\u793a\u6a21\u578b\u95f4\u7684\u7ec6\u7c92\u5ea6\u5dee\u5f02\u3002", "conclusion": "SKATE\u4e3a\u901a\u7528\u3001\u53ef\u6269\u5c55\u7684LLM\u8bc4\u4f30\u6846\u67b6\u63d0\u4f9b\u4e86\u91cd\u8981\u8fdb\u5c55\uff0c\u80fd\u591f\u8ddf\u4e0a\u6a21\u578b\u53d1\u5c55\u7684\u6b65\u4f10\u3002"}}
{"id": "2508.06207", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06207", "abs": "https://arxiv.org/abs/2508.06207", "authors": ["Andrea Dal Prete", "Seyram Ofori", "Chan Yon Sin", "Ashwin Narayan", "Francesco Braghin", "Marta Gandolla", "Haoyong Yu"], "title": "Computer Vision-based Adaptive Control for Back Exoskeleton Performance Optimization", "comment": null, "summary": "Back exoskeletons can reduce musculoskeletal strain, but their effectiveness\ndepends on support modulation and adaptive control. This study addresses two\nchallenges: defining optimal support strategies and developing adaptive control\nbased on payload estimation. We introduce an optimization space based on muscle\nactivity reduction, perceived discomfort, and user preference, constructing\nfunctions to identify optimal strategies. Experiments with 12 subjects revealed\noptimal operating regions, highlighting the need for dynamic modulation. Based\non these insights, we developed a vision-based adaptive control pipeline that\nestimates payloads in real-time by enhancing exoskeleton contextual\nunderstanding, minimising latency and enabling support adaptation within the\ndefined optimisation space. Validation with 12 more subjects showed over 80%\naccuracy and improvements across all metrics. Compared to static control,\nadaptive modulation reduced peak back muscle activation by up to 23% while\npreserving user preference and minimising discomfort. These findings validate\nthe proposed framework and highlight the potential of intelligent,\ncontext-aware control in industrial exoskeletons.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u808c\u8089\u6d3b\u52a8\u51cf\u5c11\u3001\u4e0d\u9002\u611f\u548c\u7528\u6237\u504f\u597d\u7684\u4f18\u5316\u7a7a\u95f4\uff0c\u5f00\u53d1\u4e86\u5b9e\u65f6\u8d1f\u8f7d\u4f30\u8ba1\u7684\u81ea\u9002\u5e94\u63a7\u5236\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u80cc\u90e8\u808c\u8089\u6fc0\u6d3b\u5cf0\u503c\u3002", "motivation": "\u89e3\u51b3\u80cc\u90e8\u5916\u9aa8\u9abc\u652f\u6301\u7b56\u7565\u4f18\u5316\u548c\u81ea\u9002\u5e94\u63a7\u5236\u7684\u6311\u6218\uff0c\u4ee5\u63d0\u9ad8\u5176\u5de5\u4e1a\u5e94\u7528\u6548\u679c\u3002", "method": "\u6784\u5efa\u4f18\u5316\u7a7a\u95f4\uff0c\u7ed3\u5408\u808c\u8089\u6d3b\u52a8\u3001\u4e0d\u9002\u611f\u548c\u7528\u6237\u504f\u597d\uff1b\u5f00\u53d1\u57fa\u4e8e\u89c6\u89c9\u7684\u81ea\u9002\u5e94\u63a7\u5236\u7ba1\u9053\uff0c\u5b9e\u65f6\u4f30\u8ba1\u8d1f\u8f7d\u5e76\u52a8\u6001\u8c03\u6574\u652f\u6301\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u81ea\u9002\u5e94\u63a7\u5236\u5c06\u5cf0\u503c\u80cc\u90e8\u808c\u8089\u6fc0\u6d3b\u964d\u4f4e23%\uff0c\u51c6\u786e\u7387\u8d8580%\uff0c\u4e14\u7528\u6237\u504f\u597d\u548c\u8212\u9002\u5ea6\u5f97\u5230\u4fdd\u6301\u3002", "conclusion": "\u9a8c\u8bc1\u4e86\u667a\u80fd\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u63a7\u5236\u5728\u5de5\u4e1a\u5916\u9aa8\u9abc\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u52a8\u6001\u652f\u6301\u8c03\u5236\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\u3002"}}
{"id": "2508.06129", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06129", "abs": "https://arxiv.org/abs/2508.06129", "authors": ["Bachtiar Herdianto", "Romain Billot", "Flavien Lucas", "Marc Sevaux"], "title": "Study of Robust Features in Formulating Guidance for Heuristic Algorithms for Solving the Vehicle Routing Problem", "comment": "22 pages, 14 figures", "summary": "The Vehicle Routing Problem (VRP) is a complex optimization problem with\nnumerous real-world applications, mostly solved using metaheuristic algorithms\ndue to its $\\mathcal{NP}$-Hard nature. Traditionally, these metaheuristics rely\non human-crafted designs developed through empirical studies. However, recent\nresearch shows that machine learning methods can be used the structural\ncharacteristics of solutions in combinatorial optimization, thereby aiding in\ndesigning more efficient algorithms, particularly for solving VRP. Building on\nthis advancement, this study extends the previous research by conducting a\nsensitivity analysis using multiple classifier models that are capable of\npredicting the quality of VRP solutions. Hence, by leveraging explainable AI,\nthis research is able to extend the understanding of how these models make\ndecisions. Finally, our findings indicate that while feature importance varies,\ncertain features consistently emerge as strong predictors. Furthermore, we\npropose a unified framework able of ranking feature impact across different\nscenarios to illustrate this finding. These insights highlight the potential of\nfeature importance analysis as a foundation for developing a guidance mechanism\nof metaheuristic algorithms for solving the VRP.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u53ef\u89e3\u91caAI\u5206\u6790VRP\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u9884\u6d4b\u6a21\u578b\u7684\u654f\u611f\u6027\uff0c\u63d0\u51fa\u7edf\u4e00\u6846\u67b6\u4ee5\u8bc4\u4f30\u7279\u5f81\u91cd\u8981\u6027\uff0c\u4e3a\u6539\u8fdb\u5143\u542f\u53d1\u5f0f\u7b97\u6cd5\u63d0\u4f9b\u6307\u5bfc\u3002", "motivation": "\u4f20\u7edf\u5143\u542f\u53d1\u5f0f\u7b97\u6cd5\u4f9d\u8d56\u4eba\u5de5\u8bbe\u8ba1\uff0c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u80fd\u5229\u7528\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u7684\u7ed3\u6784\u7279\u5f81\uff0c\u63d0\u5347\u7b97\u6cd5\u6548\u7387\u3002\u672c\u7814\u7a76\u8fdb\u4e00\u6b65\u63a2\u7d22\u6a21\u578b\u51b3\u7b56\u673a\u5236\u3002", "method": "\u4f7f\u7528\u591a\u79cd\u5206\u7c7b\u5668\u6a21\u578b\u9884\u6d4bVRP\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\uff0c\u7ed3\u5408\u53ef\u89e3\u91caAI\u8fdb\u884c\u654f\u611f\u6027\u5206\u6790\uff0c\u63d0\u51fa\u7279\u5f81\u91cd\u8981\u6027\u6392\u540d\u6846\u67b6\u3002", "result": "\u7279\u5f81\u91cd\u8981\u6027\u56e0\u6a21\u578b\u800c\u5f02\uff0c\u4f46\u67d0\u4e9b\u7279\u5f81\u59cb\u7ec8\u662f\u5f3a\u9884\u6d4b\u56e0\u5b50\u3002\u7edf\u4e00\u6846\u67b6\u80fd\u8de8\u573a\u666f\u8bc4\u4f30\u7279\u5f81\u5f71\u54cd\u3002", "conclusion": "\u7279\u5f81\u91cd\u8981\u6027\u5206\u6790\u4e3a\u5143\u542f\u53d1\u5f0f\u7b97\u6cd5\u8bbe\u8ba1\u63d0\u4f9b\u65b0\u601d\u8def\uff0c\u6709\u671b\u63d0\u5347VRP\u6c42\u89e3\u6548\u7387\u3002"}}
{"id": "2508.06229", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06229", "abs": "https://arxiv.org/abs/2508.06229", "authors": ["Zihao Xu", "Ce Hao", "Chunzheng Wang", "Kuankuan Sima", "Fan Shi", "Jin Song Dong"], "title": "REBot: Reflexive Evasion Robot for Instantaneous Dynamic Obstacle Avoidance", "comment": null, "summary": "Dynamic obstacle avoidance (DOA) is critical for quadrupedal robots operating\nin environments with moving obstacles or humans. Existing approaches typically\nrely on navigation-based trajectory replanning, which assumes sufficient\nreaction time and leading to fails when obstacles approach rapidly. In such\nscenarios, quadrupedal robots require reflexive evasion capabilities to perform\ninstantaneous, low-latency maneuvers. This paper introduces Reflexive Evasion\nRobot (REBot), a control framework that enables quadrupedal robots to achieve\nreal-time reflexive obstacle avoidance. REBot integrates an avoidance policy\nand a recovery policy within a finite-state machine. With carefully designed\nlearning curricula and by incorporating regularization and adaptive rewards,\nREBot achieves robust evasion and rapid stabilization in instantaneous DOA\ntasks. We validate REBot through extensive simulations and real-world\nexperiments, demonstrating notable improvements in avoidance success rates,\nenergy efficiency, and robustness to fast-moving obstacles. Videos and appendix\nare available on https://rebot-2025.github.io/.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aREBot\u7684\u63a7\u5236\u6846\u67b6\uff0c\u7528\u4e8e\u56db\u8db3\u673a\u5668\u4eba\u5728\u52a8\u6001\u969c\u788d\u7269\u73af\u5883\u4e2d\u7684\u5b9e\u65f6\u53cd\u5c04\u6027\u907f\u969c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u5bfc\u822a\u8f68\u8ff9\u91cd\u89c4\u5212\uff0c\u53cd\u5e94\u65f6\u95f4\u4e0d\u8db3\uff0c\u65e0\u6cd5\u5e94\u5bf9\u5feb\u901f\u63a5\u8fd1\u7684\u969c\u788d\u7269\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u4f4e\u5ef6\u8fdf\u7684\u53cd\u5c04\u6027\u907f\u969c\u80fd\u529b\u3002", "method": "REBot\u7ed3\u5408\u907f\u969c\u7b56\u7565\u548c\u6062\u590d\u7b56\u7565\uff0c\u91c7\u7528\u6709\u9650\u72b6\u6001\u673a\u8bbe\u8ba1\uff0c\u5e76\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5b66\u4e60\u8bfe\u7a0b\u3001\u6b63\u5219\u5316\u548c\u81ea\u9002\u5e94\u5956\u52b1\u5b9e\u73b0\u9c81\u68d2\u907f\u969c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cREBot\u5728\u907f\u969c\u6210\u529f\u7387\u3001\u80fd\u6548\u548c\u5bf9\u5feb\u901f\u79fb\u52a8\u969c\u788d\u7269\u7684\u9c81\u68d2\u6027\u65b9\u9762\u5747\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "REBot\u4e3a\u56db\u8db3\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u5b9e\u65f6\u53cd\u5c04\u6027\u907f\u969c\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.06145", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06145", "abs": "https://arxiv.org/abs/2508.06145", "authors": ["Byeonghun Bang", "Jongsuk Yoon", "Dong-Jin Chang", "Seho Park", "Yong Oh Lee"], "title": "Retrieval Augmented Large Language Model System for Comprehensive Drug Contraindications", "comment": null, "summary": "The versatility of large language models (LLMs) has been explored across\nvarious sectors, but their application in healthcare poses challenges,\nparticularly in the domain of pharmaceutical contraindications where accurate\nand reliable information is required. This study enhances the capability of\nLLMs to address contraindications effectively by implementing a Retrieval\nAugmented Generation (RAG) pipeline. Utilizing OpenAI's GPT-4o-mini as the base\nmodel, and the text-embedding-3-small model for embeddings, our approach\nintegrates Langchain to orchestrate a hybrid retrieval system with re-ranking.\nThis system leverages Drug Utilization Review (DUR) data from public databases,\nfocusing on contraindications for specific age groups, pregnancy, and\nconcomitant drug use. The dataset includes 300 question-answer pairs across\nthree categories, with baseline model accuracy ranging from 0.49 to 0.57.\nPost-integration of the RAG pipeline, we observed a significant improvement in\nmodel accuracy, achieving rates of 0.94, 0.87, and 0.89 for contraindications\nrelated to age groups, pregnancy, and concomitant drug use, respectively. The\nresults indicate that augmenting LLMs with a RAG framework can substantially\nreduce uncertainty in prescription and drug intake decisions by providing more\nprecise and reliable drug contraindication information.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u5f15\u5165\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u836f\u7269\u7981\u5fcc\u9886\u57df\u7684\u51c6\u786e\u6027\uff0c\u51cf\u5c11\u4e86\u5904\u65b9\u51b3\u7b56\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "motivation": "\u63a2\u7d22LLMs\u5728\u533b\u7597\u9886\u57df\u7684\u5e94\u7528\uff0c\u7279\u522b\u662f\u5728\u836f\u7269\u7981\u5fcc\u65b9\u9762\uff0c\u9700\u8981\u9ad8\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "\u4f7f\u7528OpenAI\u7684GPT-4o-mini\u4f5c\u4e3a\u57fa\u7840\u6a21\u578b\uff0c\u7ed3\u5408text-embedding-3-small\u6a21\u578b\u548cLangchain\uff0c\u6784\u5efa\u6df7\u5408\u68c0\u7d22\u7cfb\u7edf\uff0c\u5e76\u5229\u7528\u516c\u5f00\u7684DUR\u6570\u636e\u3002", "result": "RAG\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u51c6\u786e\u6027\uff0c\u5e74\u9f84\u7ec4\u3001\u598a\u5a20\u548c\u8054\u5408\u7528\u836f\u7684\u7981\u5fcc\u4fe1\u606f\u51c6\u786e\u7387\u5206\u522b\u8fbe\u52300.94\u30010.87\u548c0.89\u3002", "conclusion": "RAG\u6846\u67b6\u80fd\u6709\u6548\u589e\u5f3aLLMs\u5728\u836f\u7269\u7981\u5fcc\u9886\u57df\u7684\u8868\u73b0\uff0c\u4e3a\u4e34\u5e8a\u51b3\u7b56\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u652f\u6301\u3002"}}
{"id": "2508.06266", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06266", "abs": "https://arxiv.org/abs/2508.06266", "authors": ["Zezeng Li", "Rui Yang", "Ruochen Chen", "ZhongXuan Luo", "Liming Chen"], "title": "ADPro: a Test-time Adaptive Diffusion Policy for Robot Manipulation via Manifold and Initial Noise Constraints", "comment": null, "summary": "Diffusion policies have recently emerged as a powerful class of visuomotor\ncontrollers for robot manipulation, offering stable training and expressive\nmulti-modal action modeling. However, existing approaches typically treat\naction generation as an unconstrained denoising process, ignoring valuable a\npriori knowledge about geometry and control structure. In this work, we propose\nthe Adaptive Diffusion Policy (ADP), a test-time adaptation method that\nintroduces two key inductive biases into the diffusion. First, we embed a\ngeometric manifold constraint that aligns denoising updates with task-relevant\nsubspaces, leveraging the fact that the relative pose between the end-effector\nand target scene provides a natural gradient direction, and guiding denoising\nalong the geodesic path of the manipulation manifold. Then, to reduce\nunnecessary exploration and accelerate convergence, we propose an analytically\nguided initialization: rather than sampling from an uninformative prior, we\ncompute a rough registration between the gripper and target scenes to propose a\nstructured initial noisy action. ADP is compatible with pre-trained diffusion\npolicies and requires no retraining, enabling test-time adaptation that tailors\nthe policy to specific tasks, thereby enhancing generalization across novel\ntasks and environments. Experiments on RLBench, CALVIN, and real-world dataset\nshow that ADPro, an implementation of ADP, improves success rates,\ngeneralization, and sampling efficiency, achieving up to 25% faster execution\nand 9% points over strong diffusion baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u6269\u6563\u7b56\u7565\uff08ADP\uff09\uff0c\u901a\u8fc7\u51e0\u4f55\u6d41\u5f62\u7ea6\u675f\u548c\u89e3\u6790\u5f15\u5bfc\u521d\u59cb\u5316\u4f18\u5316\u6269\u6563\u7b56\u7565\uff0c\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u6269\u6563\u7b56\u7565\u5728\u52a8\u4f5c\u751f\u6210\u4e2d\u5ffd\u7565\u4e86\u51e0\u4f55\u548c\u63a7\u5236\u7ed3\u6784\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u9650\u5236\u4e86\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "ADP\u5f15\u5165\u51e0\u4f55\u6d41\u5f62\u7ea6\u675f\u548c\u89e3\u6790\u5f15\u5bfc\u521d\u59cb\u5316\uff0c\u4f18\u5316\u6269\u6563\u8fc7\u7a0b\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3002", "result": "\u5728RLBench\u3001CALVIN\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\uff0cADP\uff08ADPro\uff09\u63d0\u5347\u4e86\u6210\u529f\u7387\u3001\u6cdb\u5316\u80fd\u529b\u548c\u91c7\u6837\u6548\u7387\uff0c\u6267\u884c\u901f\u5ea6\u63d0\u534725%\uff0c\u6210\u529f\u7387\u63d0\u9ad89%\u3002", "conclusion": "ADP\u901a\u8fc7\u5f15\u5165\u51e0\u4f55\u548c\u521d\u59cb\u5316\u7ea6\u675f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6269\u6563\u7b56\u7565\u7684\u6027\u80fd\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2508.06225", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06225", "abs": "https://arxiv.org/abs/2508.06225", "authors": ["Zailong Tian", "Zhuoheng Han", "Yanzhe Chen", "Haozhe Xu", "Xi Yang", "richeng xuan", "Hongfeng Wang", "Lizi Liao"], "title": "Overconfidence in LLM-as-a-Judge: Diagnosis and Confidence-Driven Solution", "comment": null, "summary": "Large Language Models (LLMs) are widely used as automated judges, where\npractical value depends on both accuracy and trustworthy, risk-aware judgments.\nExisting approaches predominantly focus on accuracy, overlooking the necessity\nof well-calibrated confidence, which is vital for adaptive and reliable\nevaluation pipelines. In this work, we advocate a shift from accuracy-centric\nevaluation to confidence-driven, risk-aware LLM-as-a-Judge systems, emphasizing\nthe necessity of well-calibrated confidence for trustworthy and adaptive\nevaluation. We systematically identify the **Overconfidence Phenomenon** in\ncurrent LLM-as-a-Judges, where predicted confidence significantly overstates\nactual correctness, undermining reliability in practical deployment. To\nquantify this phenomenon, we introduce **TH-Score**, a novel metric measuring\nconfidence-accuracy alignment. Furthermore, we propose **LLM-as-a-Fuser**, an\nensemble framework that transforms LLMs into reliable, risk-aware evaluators.\nExtensive experiments demonstrate that our approach substantially improves\ncalibration and enables adaptive, confidence-driven evaluation pipelines,\nachieving superior reliability and accuracy compared to existing baselines.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4ece\u51c6\u786e\u6027\u4e3a\u4e2d\u5fc3\u8f6c\u5411\u7f6e\u4fe1\u5ea6\u9a71\u52a8\u7684LLM\u8bc4\u4f30\u7cfb\u7edf\uff0c\u5f15\u5165TH-Score\u91cf\u5316\u8fc7\u5ea6\u81ea\u4fe1\u73b0\u8c61\uff0c\u5e76\u63d0\u51faLLM-as-a-Fuser\u6846\u67b6\u63d0\u5347\u53ef\u9760\u6027\u548c\u6821\u51c6\u3002", "motivation": "\u73b0\u6709LLM\u8bc4\u4f30\u7cfb\u7edf\u8fc7\u4e8e\u5173\u6ce8\u51c6\u786e\u6027\uff0c\u5ffd\u89c6\u4e86\u7f6e\u4fe1\u5ea6\u6821\u51c6\u7684\u91cd\u8981\u6027\uff0c\u5bfc\u81f4\u5b9e\u9645\u90e8\u7f72\u4e2d\u53ef\u9760\u6027\u4e0d\u8db3\u3002", "method": "\u63d0\u51faTH-Score\u91cf\u5316\u8fc7\u5ea6\u81ea\u4fe1\u73b0\u8c61\uff0c\u5e76\u8bbe\u8ba1LLM-as-a-Fuser\u6846\u67b6\uff0c\u901a\u8fc7\u96c6\u6210\u65b9\u6cd5\u63d0\u5347\u7f6e\u4fe1\u5ea6\u6821\u51c6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u6539\u5584\u4e86\u6821\u51c6\u6548\u679c\uff0c\u5b9e\u73b0\u4e86\u81ea\u9002\u5e94\u3001\u7f6e\u4fe1\u5ea6\u9a71\u52a8\u7684\u8bc4\u4f30\uff0c\u53ef\u9760\u6027\u548c\u51c6\u786e\u6027\u4f18\u4e8e\u57fa\u7ebf\u3002", "conclusion": "\u7f6e\u4fe1\u5ea6\u9a71\u52a8\u7684LLM\u8bc4\u4f30\u7cfb\u7edf\u66f4\u53ef\u9760\uff0cLLM-as-a-Fuser\u6846\u67b6\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u98ce\u9669\u611f\u77e5\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.06276", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06276", "abs": "https://arxiv.org/abs/2508.06276", "authors": ["Juan Heredia", "Christian Schlette", "Mikkel Baun Kj\u00e6rgaard"], "title": "EcBot: Data-Driven Energy Consumption Open-Source MATLAB Library for Manipulators", "comment": null, "summary": "Existing literature proposes models for estimating the electrical power of\nmanipulators, yet two primary limitations prevail. First, most models are\npredominantly tested using traditional industrial robots. Second, these models\noften lack accuracy. To address these issues, we introduce an open source\nMatlab-based library designed to automatically generate \\ac{ec} models for\nmanipulators. The necessary inputs for the library are Denavit-Hartenberg\nparameters, link masses, and centers of mass. Additionally, our model is\ndata-driven and requires real operational data, including joint positions,\nvelocities, accelerations, electrical power, and corresponding timestamps. We\nvalidated our methodology by testing on four lightweight robots sourced from\nthree distinct manufacturers: Universal Robots, Franka Emika, and Kinova. The\nmodel underwent testing, and the results demonstrated an RMSE ranging from 1.42\nW to 2.80 W for the training dataset and from 1.45 W to 5.25 W for the testing\ndataset.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMatlab\u7684\u5f00\u6e90\u5e93\uff0c\u7528\u4e8e\u81ea\u52a8\u751f\u6210\u673a\u68b0\u81c2\u7684\u80fd\u8017\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u4e3b\u8981\u9488\u5bf9\u4f20\u7edf\u5de5\u4e1a\u673a\u5668\u4eba\u4e14\u7cbe\u5ea6\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u673a\u68b0\u81c2\u80fd\u8017\u6a21\u578b\u591a\u9488\u5bf9\u4f20\u7edf\u5de5\u4e1a\u673a\u5668\u4eba\u4e14\u7cbe\u5ea6\u4e0d\u8db3\uff0c\u9700\u66f4\u666e\u9002\u4e14\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5229\u7528Denavit-Hartenberg\u53c2\u6570\u3001\u8fde\u6746\u8d28\u91cf\u3001\u8d28\u5fc3\u4f4d\u7f6e\u7b49\u8f93\u5165\uff0c\u7ed3\u5408\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff08\u9700\u5b9e\u9645\u8fd0\u884c\u6570\u636e\uff09\uff0c\u5f00\u53d1\u5f00\u6e90Matlab\u5e93\u3002", "result": "\u5728\u56db\u79cd\u8f7b\u91cf\u7ea7\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\uff0c\u8bad\u7ec3\u96c6RMSE\u4e3a1.42-2.80 W\uff0c\u6d4b\u8bd5\u96c6\u4e3a1.45-5.25 W\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u673a\u68b0\u81c2\u80fd\u8017\u6a21\u578b\u7684\u666e\u9002\u6027\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2508.06226", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06226", "abs": "https://arxiv.org/abs/2508.06226", "authors": ["Yumeng Fu", "Jiayin Zhu", "Lingling Zhang", "Bo Zhao", "Shaoxuan Ma", "Yushun Zhang", "Yanrui Wu", "Wenjun Wu"], "title": "GeoLaux: A Benchmark for Evaluating MLLMs' Geometry Performance on Long-Step Problems Requiring Auxiliary Lines", "comment": null, "summary": "Geometry problem solving (GPS) requires models to master diagram\ncomprehension, logical reasoning, knowledge application, numerical computation,\nand auxiliary line construction. This presents a significant challenge for\nMultimodal Large Language Models (MLLMs). However, existing benchmarks for\nevaluating MLLM geometry skills overlook auxiliary line construction and lack\nfine-grained process evaluation, making them insufficient for assessing MLLMs'\nlong-step reasoning abilities. To bridge these gaps, we present the GeoLaux\nbenchmark, comprising 2,186 geometry problems, incorporating both calculation\nand proving questions. Notably, the problems require an average of 6.51\nreasoning steps, with a maximum of 24 steps, and 41.8% of them need auxiliary\nline construction. Building on the dataset, we design a novel five-dimensional\nevaluation strategy assessing answer correctness, process correctness, process\nquality, auxiliary line impact, and error causes. Extensive experiments on 13\nleading MLLMs (including thinking models and non-thinking models) yield three\npivotal findings: First, models exhibit substantial performance degradation in\nextended reasoning steps (nine models demonstrate over 50% performance drop).\nSecond, compared to calculation problems, MLLMs tend to take shortcuts when\nsolving proving problems. Third, models lack auxiliary line awareness, and\nenhancing this capability proves particularly beneficial for overall geometry\nreasoning improvement. These findings establish GeoLaux as both a benchmark for\nevaluating MLLMs' long-step geometric reasoning with auxiliary lines and a\nguide for capability advancement. Our dataset and code are included in\nsupplementary materials and will be released.", "AI": {"tldr": "GeoLaux\u57fa\u51c6\u6d4b\u8bd5\u586b\u8865\u4e86\u73b0\u6709MLLM\u51e0\u4f55\u80fd\u529b\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u91cd\u70b9\u5173\u6ce8\u8f85\u52a9\u7ebf\u6784\u5efa\u548c\u957f\u6b65\u63a8\u7406\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4e2d\u7684\u8868\u73b0\u4e0b\u964d\u548c\u8f85\u52a9\u7ebf\u610f\u8bc6\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5ffd\u89c6\u8f85\u52a9\u7ebf\u6784\u5efa\u548c\u7ec6\u7c92\u5ea6\u8fc7\u7a0b\u8bc4\u4f30\uff0c\u65e0\u6cd5\u5168\u9762\u8bc4\u4f30MLLM\u7684\u957f\u6b65\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51faGeoLaux\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b2,186\u9053\u51e0\u4f55\u9898\uff0c\u8bbe\u8ba1\u4e94\u7ef4\u8bc4\u4f30\u7b56\u7565\uff08\u7b54\u6848\u6b63\u786e\u6027\u3001\u8fc7\u7a0b\u6b63\u786e\u6027\u3001\u8fc7\u7a0b\u8d28\u91cf\u3001\u8f85\u52a9\u7ebf\u5f71\u54cd\u3001\u9519\u8bef\u539f\u56e0\uff09\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u6a21\u578b\u5728\u957f\u6b65\u63a8\u7406\u4e2d\u8868\u73b0\u663e\u8457\u4e0b\u964d\uff0c\u8bc1\u660e\u9898\u4e2d\u6613\u8d70\u6377\u5f84\uff0c\u4e14\u7f3a\u4e4f\u8f85\u52a9\u7ebf\u610f\u8bc6\u3002", "conclusion": "GeoLaux\u53ef\u4f5c\u4e3a\u8bc4\u4f30MLLM\u51e0\u4f55\u63a8\u7406\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u5e76\u6307\u5bfc\u80fd\u529b\u63d0\u5347\u3002"}}
{"id": "2508.06278", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06278", "abs": "https://arxiv.org/abs/2508.06278", "authors": ["Petr Novak", "Stefan Biffl", "Marek Obitko", "Petr Kadera"], "title": "Mitigating Undesired Conditions in Flexible Production with Product-Process-Resource Asset Knowledge Graphs", "comment": "3 pages, 1 figure", "summary": "Contemporary industrial cyber-physical production systems (CPPS) composed of\nrobotic workcells face significant challenges in the analysis of undesired\nconditions due to the flexibility of Industry 4.0 that disrupts traditional\nquality assurance mechanisms. This paper presents a novel industry-oriented\nsemantic model called Product-Process-Resource Asset Knowledge Graph (PPR-AKG),\nwhich is designed to analyze and mitigate undesired conditions in flexible\nCPPS. Built on top of the well-proven Product-Process-Resource (PPR) model\noriginating from ISA-95 and VDI-3682, a comprehensive OWL ontology addresses\nshortcomings of conventional model-driven engineering for CPPS, particularly\ninadequate undesired condition and error handling representation. The\nintegration of semantic technologies with large language models (LLMs) provides\nintuitive interfaces for factory operators, production planners, and engineers\nto interact with the entire model using natural language. Evaluation with the\nuse case addressing electric vehicle battery remanufacturing demonstrates that\nthe PPR-AKG approach efficiently supports resource allocation based on\nexplicitly represented capabilities as well as identification and mitigation of\nundesired conditions in production. The key contributions include (1) a\nholistic PPR-AKG model capturing multi-dimensional production knowledge, and\n(2) the useful combination of the PPR-AKG with LLM-based chatbots for human\ninteraction.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u4e49\u6a21\u578bPPR-AKG\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5206\u6790\u548c\u7f13\u89e3\u5de5\u4e1a4.0\u4e2d\u67d4\u6027CPPS\u7684\u4e0d\u826f\u6761\u4ef6\uff0c\u7ed3\u5408LLM\u63d0\u4f9b\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u3002", "motivation": "\u5de5\u4e1a4.0\u7684\u7075\u6d3b\u6027\u5bfc\u81f4\u4f20\u7edf\u8d28\u91cf\u4fdd\u8bc1\u673a\u5236\u5931\u6548\uff0c\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u5206\u6790\u548c\u5904\u7406\u4e0d\u826f\u6761\u4ef6\u3002", "method": "\u57fa\u4e8ePPR\u6a21\u578b\u6784\u5efaPPR-AKG\u8bed\u4e49\u6a21\u578b\uff0c\u7ed3\u5408OWL\u672c\u4f53\u548cLLM\u6280\u672f\uff0c\u63d0\u4f9b\u81ea\u7136\u8bed\u8a00\u63a5\u53e3\u3002", "result": "\u5728\u7535\u52a8\u6c7d\u8f66\u7535\u6c60\u518d\u5236\u9020\u6848\u4f8b\u4e2d\uff0cPPR-AKG\u6709\u6548\u652f\u6301\u8d44\u6e90\u5206\u914d\u548c\u4e0d\u826f\u6761\u4ef6\u7684\u8bc6\u522b\u4e0e\u7f13\u89e3\u3002", "conclusion": "PPR-AKG\u6a21\u578b\u53ca\u5176\u4e0eLLM\u7684\u7ed3\u5408\u4e3a\u67d4\u6027CPPS\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u77e5\u8bc6\u8868\u793a\u548c\u4ea4\u4e92\u65b9\u5f0f\u3002"}}
{"id": "2508.06230", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06230", "abs": "https://arxiv.org/abs/2508.06230", "authors": ["Ruben Sharma", "Sebastijan Duman\u010di\u0107", "Ross D. King", "Andrew Cropper"], "title": "Learning Logical Rules using Minimum Message Length", "comment": null, "summary": "Unifying probabilistic and logical learning is a key challenge in AI. We\nintroduce a Bayesian inductive logic programming approach that learns minimum\nmessage length programs from noisy data. Our approach balances hypothesis\ncomplexity and data fit through priors, which explicitly favour more general\nprograms, and a likelihood that favours accurate programs. Our experiments on\nseveral domains, including game playing and drug design, show that our method\nsignificantly outperforms previous methods, notably those that learn minimum\ndescription length programs. Our results also show that our approach is\ndata-efficient and insensitive to example balance, including the ability to\nlearn from exclusively positive examples.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8d1d\u53f6\u65af\u5f52\u7eb3\u903b\u8f91\u7f16\u7a0b\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u5c0f\u6d88\u606f\u957f\u5ea6\u7a0b\u5e8f\u4ece\u566a\u58f0\u6570\u636e\u4e2d\u5b66\u4e60\uff0c\u5e73\u8861\u5047\u8bbe\u590d\u6742\u6027\u548c\u6570\u636e\u62df\u5408\u3002", "motivation": "\u7edf\u4e00\u6982\u7387\u548c\u903b\u8f91\u5b66\u4e60\u662fAI\u7684\u5173\u952e\u6311\u6218\u3002", "method": "\u4f7f\u7528\u5148\u9a8c\u548c\u4f3c\u7136\u51fd\u6570\uff0c\u524d\u8005\u504f\u597d\u66f4\u901a\u7528\u7684\u7a0b\u5e8f\uff0c\u540e\u8005\u504f\u597d\u51c6\u786e\u7a0b\u5e8f\u3002", "result": "\u5728\u6e38\u620f\u548c\u836f\u7269\u8bbe\u8ba1\u7b49\u591a\u4e2a\u9886\u57df\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u6700\u5c0f\u63cf\u8ff0\u957f\u5ea6\u7a0b\u5e8f\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6570\u636e\u9ad8\u6548\uff0c\u5bf9\u793a\u4f8b\u5e73\u8861\u4e0d\u654f\u611f\uff0c\u751a\u81f3\u80fd\u4ece\u7eaf\u6b63\u4f8b\u4e2d\u5b66\u4e60\u3002"}}
{"id": "2508.06283", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06283", "abs": "https://arxiv.org/abs/2508.06283", "authors": ["Saad Ejaz", "Marco Giberna", "Muhammad Shaheer", "Jose Andres Millan-Romera", "Ali Tourani", "Paul Kremer", "Holger Voos", "Jose Luis Sanchez-Lopez"], "title": "Situationally-aware Path Planning Exploiting 3D Scene Graphs", "comment": null, "summary": "3D Scene Graphs integrate both metric and semantic information, yet their\nstructure remains underutilized for improving path planning efficiency and\ninterpretability. In this work, we present S-Path, a situationally-aware path\nplanner that leverages the metric-semantic structure of indoor 3D Scene Graphs\nto significantly enhance planning efficiency. S-Path follows a two-stage\nprocess: it first performs a search over a semantic graph derived from the\nscene graph to yield a human-understandable high-level path. This also\nidentifies relevant regions for planning, which later allows the decomposition\nof the problem into smaller, independent subproblems that can be solved in\nparallel. We also introduce a replanning mechanism that, in the event of an\ninfeasible path, reuses information from previously solved subproblems to\nupdate semantic heuristics and prioritize reuse to further improve the\nefficiency of future planning attempts. Extensive experiments on both\nreal-world and simulated environments show that S-Path achieves average\nreductions of 5.7x in planning time while maintaining comparable path\noptimality to classical sampling-based planners and surpassing them in complex\nscenarios, making it an efficient and interpretable path planner for\nenvironments represented by indoor 3D Scene Graphs.", "AI": {"tldr": "S-Path\u5229\u75283D\u573a\u666f\u56fe\u7684\u8bed\u4e49\u7ed3\u6784\u63d0\u5347\u8def\u5f84\u89c4\u5212\u6548\u7387\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u89c4\u5212\u548c\u5b50\u95ee\u9898\u5206\u89e3\u5b9e\u73b0\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u8def\u5f84\u89c4\u5212\u3002", "motivation": "3D\u573a\u666f\u56fe\u7684\u8bed\u4e49\u7ed3\u6784\u672a\u88ab\u5145\u5206\u5229\u7528\u4ee5\u63d0\u5347\u8def\u5f84\u89c4\u5212\u7684\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "S-Path\u91c7\u7528\u4e24\u9636\u6bb5\u89c4\u5212\uff1a\u5148\u5728\u8bed\u4e49\u56fe\u4e0a\u641c\u7d22\u9ad8\u5c42\u8def\u5f84\uff0c\u518d\u5206\u89e3\u4e3a\u5e76\u884c\u5b50\u95ee\u9898\uff0c\u5e76\u5f15\u5165\u91cd\u89c4\u5212\u673a\u5236\u4f18\u5316\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u663e\u793aS-Path\u5e73\u5747\u51cf\u5c115.7\u500d\u89c4\u5212\u65f6\u95f4\uff0c\u8def\u5f84\u6700\u4f18\u6027\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u5f53\uff0c\u590d\u6742\u573a\u666f\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "S-Path\u662f\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u8def\u5f84\u89c4\u5212\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5ba4\u51853D\u573a\u666f\u56fe\u73af\u5883\u3002"}}
{"id": "2508.06263", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06263", "abs": "https://arxiv.org/abs/2508.06263", "authors": ["Andrew Cropper", "David M. Cerna", "Matti J\u00e4rvisalo"], "title": "Symmetry breaking for inductive logic programming", "comment": null, "summary": "The goal of inductive logic programming is to search for a hypothesis that\ngeneralises training data and background knowledge. The challenge is searching\nvast hypothesis spaces, which is exacerbated because many logically equivalent\nhypotheses exist. To address this challenge, we introduce a method to break\nsymmetries in the hypothesis space. We implement our idea in answer set\nprogramming. Our experiments on multiple domains, including visual reasoning\nand game playing, show that our approach can reduce solving times from over an\nhour to just 17 seconds.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b9\u6cd5\uff0c\u901a\u8fc7\u6253\u7834\u5047\u8bbe\u7a7a\u95f4\u4e2d\u7684\u5bf9\u79f0\u6027\u6765\u4f18\u5316\u5f52\u7eb3\u903b\u8f91\u7f16\u7a0b\u7684\u641c\u7d22\u6548\u7387\u3002", "motivation": "\u5f52\u7eb3\u903b\u8f91\u7f16\u7a0b\u9700\u8981\u641c\u7d22\u5e9e\u5927\u7684\u5047\u8bbe\u7a7a\u95f4\uff0c\u4e14\u5b58\u5728\u5927\u91cf\u903b\u8f91\u7b49\u6548\u5047\u8bbe\uff0c\u5bfc\u81f4\u641c\u7d22\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u91c7\u7528\u7b54\u6848\u96c6\u7f16\u7a0b\u5b9e\u73b0\u5bf9\u79f0\u6027\u6253\u7834\u7684\u65b9\u6cd5\u3002", "result": "\u5728\u89c6\u89c9\u63a8\u7406\u548c\u6e38\u620f\u7b49\u591a\u4e2a\u9886\u57df\u7684\u5b9e\u9a8c\u4e2d\uff0c\u6c42\u89e3\u65f6\u95f4\u4ece\u8d85\u8fc7\u4e00\u5c0f\u65f6\u7f29\u77ed\u81f3\u4ec517\u79d2\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u5f52\u7eb3\u903b\u8f91\u7f16\u7a0b\u7684\u641c\u7d22\u6548\u7387\u3002"}}
{"id": "2508.06291", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06291", "abs": "https://arxiv.org/abs/2508.06291", "authors": ["Christian Rauch", "Bj\u00f6rn Ellensohn", "Linus Nwankwo", "Vedant Dave", "Elmar Rueckert"], "title": "Real-Time 3D Vision-Language Embedding Mapping", "comment": null, "summary": "A metric-accurate semantic 3D representation is essential for many robotic\ntasks. This work proposes a simple, yet powerful, way to integrate the 2D\nembeddings of a Vision-Language Model in a metric-accurate 3D representation at\nreal-time. We combine a local embedding masking strategy, for a more distinct\nembedding distribution, with a confidence-weighted 3D integration for more\nreliable 3D embeddings. The resulting metric-accurate embedding representation\nis task-agnostic and can represent semantic concepts on a global multi-room, as\nwell as on a local object-level. This enables a variety of interactive robotic\napplications that require the localisation of objects-of-interest via natural\nlanguage. We evaluate our approach on a variety of real-world sequences and\ndemonstrate that these strategies achieve a more accurate object-of-interest\nlocalisation while improving the runtime performance in order to meet our\nreal-time constraints. We further demonstrate the versatility of our approach\nin a variety of interactive handheld, mobile robotics and manipulation tasks,\nrequiring only raw image data.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c062D\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5d4c\u5165\u5230\u5b9e\u65f6\u3001\u9ad8\u7cbe\u5ea6\u76843D\u8868\u793a\u4e2d\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u5c40\u90e8\u5d4c\u5165\u63a9\u7801\u7b56\u7565\u548c\u7f6e\u4fe1\u5ea6\u52a0\u67433D\u96c6\u6210\uff0c\u5b9e\u73b0\u4e86\u4efb\u52a1\u65e0\u5173\u7684\u8bed\u4e493D\u8868\u793a\u3002", "motivation": "\u4e3a\u673a\u5668\u4eba\u4efb\u52a1\u63d0\u4f9b\u9ad8\u7cbe\u5ea6\u7684\u8bed\u4e493D\u8868\u793a\uff0c\u652f\u6301\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u3002", "method": "\u91c7\u7528\u5c40\u90e8\u5d4c\u5165\u63a9\u7801\u7b56\u7565\u548c\u7f6e\u4fe1\u5ea6\u52a0\u67433D\u96c6\u6210\uff0c\u4f18\u5316\u5d4c\u5165\u5206\u5e03\u548c3D\u8868\u793a\u3002", "result": "\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u63d0\u5347\u4e86\u76ee\u6807\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u5b9e\u65f6\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u591a\u79cd\u4ea4\u4e92\u5f0f\u673a\u5668\u4eba\u4efb\u52a1\uff0c\u4ec5\u9700\u539f\u59cb\u56fe\u50cf\u6570\u636e\u3002"}}
{"id": "2508.06296", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06296", "abs": "https://arxiv.org/abs/2508.06296", "authors": ["Pierre Peign\u00e9 - Lefebvre", "Quentin Feuillade-Montixi", "Tom David", "Nicolas Miailhe"], "title": "LLM Robustness Leaderboard v1 --Technical report", "comment": null, "summary": "This technical report accompanies the LLM robustness leaderboard published by\nPRISM Eval for the Paris AI Action Summit. We introduce PRISM Eval Behavior\nElicitation Tool (BET), an AI system performing automated red-teaming through\nDynamic Adversarial Optimization that achieves 100% Attack Success Rate (ASR)\nagainst 37 of 41 state-of-the-art LLMs. Beyond binary success metrics, we\npropose a fine-grained robustness metric estimating the average number of\nattempts required to elicit harmful behaviors, revealing that attack difficulty\nvaries by over 300-fold across models despite universal vulnerability. We\nintroduce primitive-level vulnerability analysis to identify which jailbreaking\ntechniques are most effective for specific hazard categories. Our collaborative\nevaluation with trusted third parties from the AI Safety Network demonstrates\npractical pathways for distributed robustness assessment across the community.", "AI": {"tldr": "PRISM Eval\u5f00\u53d1\u4e86\u884c\u4e3a\u8bf1\u5bfc\u5de5\u5177BET\uff0c\u901a\u8fc7\u52a8\u6001\u5bf9\u6297\u4f18\u5316\u5b9e\u73b0100%\u653b\u51fb\u6210\u529f\u7387\uff0c\u5e76\u63d0\u51fa\u7ec6\u7c92\u5ea6\u9c81\u68d2\u6027\u6307\u6807\uff0c\u63ed\u793a\u4e0d\u540c\u6a21\u578b\u653b\u51fb\u96be\u5ea6\u5dee\u5f02\u663e\u8457\u3002", "motivation": "\u8bc4\u4f30\u548c\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u9c81\u68d2\u6027\uff0c\u8bc6\u522b\u6f0f\u6d1e\u5e76\u63a8\u52a8\u793e\u533a\u5206\u5e03\u5f0f\u8bc4\u4f30\u3002", "method": "\u4f7f\u7528BET\u5de5\u5177\u8fdb\u884c\u81ea\u52a8\u5316\u7ea2\u961f\u6d4b\u8bd5\uff0c\u7ed3\u5408\u52a8\u6001\u5bf9\u6297\u4f18\u5316\u548c\u7ec6\u7c92\u5ea6\u9c81\u68d2\u6027\u6307\u6807\u5206\u6790\u3002", "result": "BET\u5bf941\u4e2aLLM\u4e2d\u768437\u4e2a\u5b9e\u73b0100%\u653b\u51fb\u6210\u529f\u7387\uff0c\u653b\u51fb\u96be\u5ea6\u5dee\u5f02\u8fbe300\u500d\u4ee5\u4e0a\u3002", "conclusion": "\u7814\u7a76\u4e3aLLM\u9c81\u68d2\u6027\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u548c\u6307\u6807\uff0c\u652f\u6301\u793e\u533a\u534f\u4f5c\u63d0\u5347AI\u5b89\u5168\u6027\u3002"}}
{"id": "2508.06295", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06295", "abs": "https://arxiv.org/abs/2508.06295", "authors": ["Juan Heredia", "Emil Stubbe Kolvig-Raun", "Sune Lundo Sorensen", "Mikkel Baun Kjaergaard"], "title": "Evaluating Robot Program Performance with Power Consumption Driven Metrics in Lightweight Industrial Robots", "comment": null, "summary": "The code performance of industrial robots is typically analyzed through CPU\nmetrics, which overlook the physical impact of code on robot behavior. This\nstudy introduces a novel framework for assessing robot program performance from\nan embodiment perspective by analyzing the robot's electrical power profile.\nOur approach diverges from conventional CPU based evaluations and instead\nleverages a suite of normalized metrics, namely, the energy utilization\ncoefficient, the energy conversion metric, and the reliability coefficient, to\ncapture how efficiently and reliably energy is used during task execution.\nComplementing these metrics, the established robot wear metric provides further\ninsight into long term reliability. Our approach is demonstrated through an\nexperimental case study in machine tending, comparing four programs with\ndiverse strategies using a UR5e robot. The proposed metrics directly compare\nand categorize different robot programs, regardless of the specific task, by\nlinking code performance to its physical manifestation through power\nconsumption patterns. Our results reveal the strengths and weaknesses of each\nstrategy, offering actionable insights for optimizing robot programming\npractices. Enhancing energy efficiency and reliability through this embodiment\ncentric approach not only improves individual robot performance but also\nsupports broader industrial objectives such as sustainable manufacturing and\ncost reduction.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u4eba\u7535\u529b\u80fd\u8017\u7684\u65b0\u578b\u6846\u67b6\uff0c\u4ece\u4f53\u73b0\u89c6\u89d2\u8bc4\u4f30\u673a\u5668\u4eba\u7a0b\u5e8f\u6027\u80fd\uff0c\u66ff\u4ee3\u4f20\u7edfCPU\u6307\u6807\u3002", "motivation": "\u4f20\u7edfCPU\u6307\u6807\u5ffd\u89c6\u4e86\u4ee3\u7801\u5bf9\u673a\u5668\u4eba\u7269\u7406\u884c\u4e3a\u7684\u5f71\u54cd\uff0c\u9700\u4ece\u80fd\u8017\u89d2\u5ea6\u66f4\u5168\u9762\u8bc4\u4f30\u6027\u80fd\u3002", "method": "\u91c7\u7528\u5f52\u4e00\u5316\u6307\u6807\uff08\u80fd\u91cf\u5229\u7528\u7cfb\u6570\u3001\u80fd\u91cf\u8f6c\u6362\u6307\u6807\u3001\u53ef\u9760\u6027\u7cfb\u6570\uff09\u548c\u673a\u5668\u4eba\u78e8\u635f\u6307\u6807\uff0c\u5206\u6790\u4efb\u52a1\u6267\u884c\u4e2d\u7684\u80fd\u8017\u6548\u7387\u4e0e\u53ef\u9760\u6027\u3002", "result": "\u901a\u8fc7UR5e\u673a\u5668\u4eba\u5b9e\u9a8c\u6848\u4f8b\uff0c\u6bd4\u8f83\u56db\u79cd\u7a0b\u5e8f\u7b56\u7565\uff0c\u63ed\u793a\u5404\u7b56\u7565\u4f18\u7f3a\u70b9\uff0c\u4e3a\u4f18\u5316\u7f16\u7a0b\u63d0\u4f9b\u4f9d\u636e\u3002", "conclusion": "\u57fa\u4e8e\u80fd\u8017\u7684\u4f53\u73b0\u89c6\u89d2\u65b9\u6cd5\u63d0\u5347\u4e86\u673a\u5668\u4eba\u6027\u80fd\uff0c\u652f\u6301\u53ef\u6301\u7eed\u5236\u9020\u548c\u6210\u672c\u964d\u4f4e\u7b49\u5de5\u4e1a\u76ee\u6807\u3002"}}
{"id": "2508.06326", "categories": ["cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.06326", "abs": "https://arxiv.org/abs/2508.06326", "authors": ["Nathaniel Virgo", "Martin Biehl", "Manuel Baltieri", "Matteo Capucci"], "title": "A \"good regulator theorem\" for embodied agents", "comment": "Accepted at the Artificial Life conference 2025 (ALife 2025). 10\n  pages, 1 figure", "summary": "In a classic paper, Conant and Ashby claimed that \"every good regulator of a\nsystem must be a model of that system.\" Artificial Life has produced many\nexamples of systems that perform tasks with apparently no model in sight; these\nsuggest Conant and Ashby's theorem doesn't easily generalise beyond its\nrestricted setup. Nevertheless, here we show that a similar intuition can be\nfleshed out in a different way: whenever an agent is able to perform a\nregulation task, it is possible for an observer to interpret it as having\n\"beliefs\" about its environment, which it \"updates\" in response to sensory\ninput. This notion of belief updating provides a notion of model that is more\nsophisticated than Conant and Ashby's, as well as a theorem that is more\nbroadly applicable. However, it necessitates a change in perspective, in that\nthe observer plays an essential role in the theory: models are not a mere\nproperty of the system but are imposed on it from outside. Our theorem holds\nregardless of whether the system is regulating its environment in a classic\ncontrol theory setup, or whether it's regulating its own internal state; the\nmodel is of its environment either way. The model might be trivial, however,\nand this is how the apparent counterexamples are resolved.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86Conant\u548cAshby\u7684\u5b9a\u7406\u5728\u66f4\u5e7f\u6cdb\u7cfb\u7edf\u4e2d\u7684\u9002\u7528\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u2018\u4fe1\u5ff5\u66f4\u65b0\u2019\u6a21\u578b\u6982\u5ff5\uff0c\u5f3a\u8c03\u89c2\u5bdf\u8005\u5728\u6a21\u578b\u5b9a\u4e49\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002", "motivation": "\u7814\u7a76Conant\u548cAshby\u5b9a\u7406\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63a2\u7d22\u5982\u4f55\u5728\u66f4\u5e7f\u6cdb\u7684\u7cfb\u7edf\u4e2d\u5b9a\u4e49\u2018\u6a21\u578b\u2019\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u2018\u4fe1\u5ff5\u66f4\u65b0\u2019\u7684\u6982\u5ff5\uff0c\u5c06\u6a21\u578b\u5b9a\u4e49\u4e3a\u89c2\u5bdf\u8005\u5bf9\u7cfb\u7edf\u7684\u89e3\u91ca\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u5e7f\u6cdb\u9002\u7528\u7684\u5b9a\u7406\uff0c\u89e3\u51b3\u4e86\u539f\u6709\u5b9a\u7406\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u6a21\u578b\u7684\u5b9a\u4e49\u9700\u8981\u89c2\u5bdf\u8005\u7684\u53c2\u4e0e\uff0c\u4e14\u9002\u7528\u4e8e\u66f4\u5e7f\u6cdb\u7684\u7cfb\u7edf\uff0c\u5305\u62ec\u7ecf\u5178\u63a7\u5236\u7406\u8bba\u548c\u5185\u90e8\u72b6\u6001\u8c03\u8282\u3002"}}
{"id": "2508.06313", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06313", "abs": "https://arxiv.org/abs/2508.06313", "authors": ["Amir Hossein Barjini", "Mohammad Bahari", "Mahdi Hejrati", "Jouni Mattila"], "title": "Surrogate-Enhanced Modeling and Adaptive Modular Control of All-Electric Heavy-Duty Robotic Manipulators", "comment": "This is submitted to IEEE T-ASE", "summary": "This paper presents a unified system-level modeling and control framework for\nan all-electric heavy-duty robotic manipulator (HDRM) driven by\nelectromechanical linear actuators (EMLAs). A surrogate-enhanced actuator\nmodel, combining integrated electromechanical dynamics with a neural network\ntrained on a dedicated testbed, is integrated into an extended virtual\ndecomposition control (VDC) architecture augmented by a natural adaptation law.\nThe derived analytical HDRM model supports a hierarchical control structure\nthat seamlessly maps high-level force and velocity objectives to real-time\nactuator commands, accompanied by a Lyapunov-based stability proof. In\nmulti-domain simulations of both cubic and a custom planar triangular\ntrajectory, the proposed adaptive modular controller achieves sub-centimeter\nCartesian tracking accuracy. Experimental validation of the same 1-DoF platform\nunder realistic load emulation confirms the efficacy of the proposed control\nstrategy. These findings demonstrate that a surrogate-enhanced EMLA model\nembedded in the VDC approach can enable modular, real-time control of an\nall-electric HDRM, supporting its deployment in next-generation mobile working\nmachines.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5168\u7535\u52a8\u91cd\u578b\u673a\u68b0\u81c2\u7684\u7edf\u4e00\u7cfb\u7edf\u7ea7\u5efa\u6a21\u4e0e\u63a7\u5236\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u589e\u5f3a\u7684\u4ee3\u7406\u6a21\u578b\u548c\u81ea\u9002\u5e94\u865a\u62df\u5206\u89e3\u63a7\u5236\uff08VDC\uff09\u67b6\u6784\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u5b9e\u65f6\u63a7\u5236\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u6a21\u5757\u5316\u3001\u5b9e\u65f6\u63a7\u5236\u7684\u5168\u7535\u52a8\u91cd\u578b\u673a\u68b0\u81c2\uff08HDRM\uff09\uff0c\u4ee5\u652f\u6301\u4e0b\u4e00\u4ee3\u79fb\u52a8\u5de5\u4f5c\u673a\u5668\u7684\u90e8\u7f72\u3002", "method": "\u91c7\u7528\u4ee3\u7406\u589e\u5f3a\u7684\u7535\u52a8\u7ebf\u6027\u6267\u884c\u5668\uff08EMLA\uff09\u6a21\u578b\uff0c\u7ed3\u5408\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u548c\u6269\u5c55\u7684VDC\u67b6\u6784\uff0c\u5e76\u5f15\u5165\u81ea\u7136\u9002\u5e94\u5f8b\u3002\u901a\u8fc7Lyapunov\u7a33\u5b9a\u6027\u8bc1\u660e\uff0c\u5b9e\u73b0\u5206\u5c42\u63a7\u5236\u7ed3\u6784\u3002", "result": "\u5728\u591a\u57df\u4eff\u771f\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e2d\uff0c\u63a7\u5236\u5668\u5b9e\u73b0\u4e86\u4e9a\u5398\u7c73\u7ea7\u7684\u7b1b\u5361\u5c14\u8ddf\u8e2a\u7cbe\u5ea6\u3002", "conclusion": "\u4ee3\u7406\u589e\u5f3a\u7684EMLA\u6a21\u578b\u4e0eVDC\u65b9\u6cd5\u7ed3\u5408\uff0c\u80fd\u591f\u5b9e\u73b0\u5168\u7535\u52a8HDRM\u7684\u6a21\u5757\u5316\u5b9e\u65f6\u63a7\u5236\uff0c\u9002\u7528\u4e8e\u672a\u6765\u79fb\u52a8\u5de5\u4f5c\u673a\u5668\u3002"}}
{"id": "2508.06348", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06348", "abs": "https://arxiv.org/abs/2508.06348", "authors": ["Mille Mei Zhen Loo", "Gert Luzkov", "Paolo Burelli"], "title": "AntiCheatPT: A Transformer-Based Approach to Cheat Detection in Competitive Computer Games", "comment": null, "summary": "Cheating in online video games compromises the integrity of gaming\nexperiences. Anti-cheat systems, such as VAC (Valve Anti-Cheat), face\nsignificant challenges in keeping pace with evolving cheating methods without\nimposing invasive measures on users' systems. This paper presents\nAntiCheatPT\\_256, a transformer-based machine learning model designed to detect\ncheating behaviour in Counter-Strike 2 using gameplay data. To support this, we\nintroduce and publicly release CS2CD: A labelled dataset of 795 matches. Using\nthis dataset, 90,707 context windows were created and subsequently augmented to\naddress class imbalance. The transformer model, trained on these windows,\nachieved an accuracy of 89.17\\% and an AUC of 93.36\\% on an unaugmented test\nset. This approach emphasizes reproducibility and real-world applicability,\noffering a robust baseline for future research in data-driven cheat detection.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578bAntiCheatPT_256\uff0c\u7528\u4e8e\u68c0\u6d4b\u300aCS2\u300b\u4e2d\u7684\u4f5c\u5f0a\u884c\u4e3a\uff0c\u5e76\u516c\u5f00\u4e86\u6570\u636e\u96c6CS2CD\u3002\u6a21\u578b\u5728\u6d4b\u8bd5\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5728\u7ebf\u6e38\u620f\u4f5c\u5f0a\u7834\u574f\u6e38\u620f\u4f53\u9a8c\uff0c\u73b0\u6709\u53cd\u4f5c\u5f0a\u7cfb\u7edf\u96be\u4ee5\u5728\u4e0d\u4fb5\u72af\u7528\u6237\u9690\u79c1\u7684\u60c5\u51b5\u4e0b\u5e94\u5bf9\u4e0d\u65ad\u6f14\u53d8\u7684\u4f5c\u5f0a\u624b\u6bb5\u3002", "method": "\u4f7f\u7528CS2CD\u6570\u636e\u96c6\uff08795\u573a\u6bd4\u8d5b\uff09\u751f\u621090,707\u4e2a\u4e0a\u4e0b\u6587\u7a97\u53e3\uff0c\u5e76\u901a\u8fc7\u6570\u636e\u589e\u5f3a\u89e3\u51b3\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u8bad\u7ec3Transformer\u6a21\u578b\u3002", "result": "\u6a21\u578b\u5728\u672a\u589e\u5f3a\u6d4b\u8bd5\u96c6\u4e0a\u51c6\u786e\u7387\u8fbe89.17%\uff0cAUC\u4e3a93.36%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5f3a\u8c03\u53ef\u91cd\u590d\u6027\u548c\u5b9e\u9645\u5e94\u7528\u6027\uff0c\u4e3a\u6570\u636e\u9a71\u52a8\u7684\u4f5c\u5f0a\u68c0\u6d4b\u7814\u7a76\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\u3002"}}
{"id": "2508.06319", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06319", "abs": "https://arxiv.org/abs/2508.06319", "authors": ["Sagar Parekh", "Heramb Nemlekar", "Dylan P. Losey"], "title": "Towards Balanced Behavior Cloning from Imbalanced Datasets", "comment": null, "summary": "Robots should be able to learn complex behaviors from human demonstrations.\nIn practice, these human-provided datasets are inevitably imbalanced: i.e., the\nhuman demonstrates some subtasks more frequently than others. State-of-the-art\nmethods default to treating each element of the human's dataset as equally\nimportant. So if -- for instance -- the majority of the human's data focuses on\nreaching a goal, and only a few state-action pairs move to avoid an obstacle,\nthe learning algorithm will place greater emphasis on goal reaching. More\ngenerally, misalignment between the relative amounts of data and the importance\nof that data causes fundamental problems for imitation learning approaches. In\nthis paper we analyze and develop learning methods that automatically account\nfor mixed datasets. We formally prove that imbalanced data leads to imbalanced\npolicies when each state-action pair is weighted equally; these policies\nemulate the most represented behaviors, and not the human's complex, multi-task\ndemonstrations. We next explore algorithms that rebalance offline datasets\n(i.e., reweight the importance of different state-action pairs) without human\noversight. Reweighting the dataset can enhance the overall policy performance.\nHowever, there is no free lunch: each method for autonomously rebalancing\nbrings its own pros and cons. We formulate these advantages and disadvantages,\nhelping other researchers identify when each type of approach is most\nappropriate. We conclude by introducing a novel meta-gradient rebalancing\nalgorithm that addresses the primary limitations behind existing approaches.\nOur experiments show that dataset rebalancing leads to better downstream\nlearning, improving the performance of general imitation learning algorithms\nwithout requiring additional data collection. See our project website:\nhttps://collab.me.vt.edu/data_curation/.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u51b3\u6a21\u4eff\u5b66\u4e60\u4e2d\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u65b0\u52a0\u6743\u6570\u636e\u96c6\u4e2d\u7684\u72b6\u6001-\u52a8\u4f5c\u5bf9\uff0c\u63d0\u5347\u7b56\u7565\u6027\u80fd\u3002", "motivation": "\u4eba\u7c7b\u6f14\u793a\u7684\u6570\u636e\u96c6\u901a\u5e38\u4e0d\u5e73\u8861\uff0c\u5bfc\u81f4\u73b0\u6709\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u8fc7\u5ea6\u5173\u6ce8\u9ad8\u9891\u5b50\u4efb\u52a1\uff0c\u5ffd\u7565\u4f4e\u9891\u4f46\u91cd\u8981\u7684\u884c\u4e3a\u3002", "method": "\u5206\u6790\u6570\u636e\u4e0d\u5e73\u8861\u5bf9\u7b56\u7565\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u591a\u79cd\u81ea\u52a8\u91cd\u65b0\u52a0\u6743\u6570\u636e\u96c6\u7684\u7b97\u6cd5\uff0c\u5e76\u5f15\u5165\u4e00\u79cd\u65b0\u7684\u5143\u68af\u5ea6\u91cd\u65b0\u5e73\u8861\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u91cd\u65b0\u5e73\u8861\u6570\u636e\u96c6\u80fd\u663e\u8457\u63d0\u5347\u6a21\u4eff\u5b66\u4e60\u7b97\u6cd5\u7684\u6027\u80fd\uff0c\u65e0\u9700\u989d\u5916\u6570\u636e\u6536\u96c6\u3002", "conclusion": "\u6570\u636e\u91cd\u65b0\u5e73\u8861\u662f\u89e3\u51b3\u6a21\u4eff\u5b66\u4e60\u6570\u636e\u4e0d\u5e73\u8861\u95ee\u9898\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u65b0\u7b97\u6cd5\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2508.06352", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.06352", "abs": "https://arxiv.org/abs/2508.06352", "authors": ["Christian Meske", "Justin Brenne", "Erdi Uenal", "Sabahat Oelcer", "Ayseguel Doganguen"], "title": "From Explainable to Explanatory Artificial Intelligence: Toward a New Paradigm for Human-Centered Explanations through Generative AI", "comment": null, "summary": "Current explainable AI (XAI) approaches prioritize algorithmic transparency\nand present explanations in abstract, non-adaptive formats that often fail to\nsupport meaningful end-user understanding. This paper introduces \"Explanatory\nAI\" as a complementary paradigm that leverages generative AI capabilities to\nserve as explanatory partners for human understanding rather than providers of\nalgorithmic transparency. While XAI reveals algorithmic decision processes for\nmodel validation, Explanatory AI addresses contextual reasoning to support\nhuman decision-making in sociotechnical contexts. We develop a definition and\nsystematic eight-dimensional conceptual model distinguishing Explanatory AI\nthrough narrative communication, adaptive personalization, and progressive\ndisclosure principles. Empirical validation through Rapid Contextual Design\nmethodology with healthcare professionals demonstrates that users consistently\nprefer context-sensitive, multimodal explanations over technical transparency.\nOur findings reveal the practical urgency for AI systems designed for human\ncomprehension rather than algorithmic introspection, establishing a\ncomprehensive research agenda for advancing user-centered AI explanation\napproaches across diverse domains and cultural contexts.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u201c\u89e3\u91ca\u6027AI\u201d\u4f5c\u4e3a\u53ef\u89e3\u91caAI\uff08XAI\uff09\u7684\u8865\u5145\u8303\u5f0f\uff0c\u901a\u8fc7\u751f\u6210\u5f0fAI\u80fd\u529b\u63d0\u4f9b\u4e0a\u4e0b\u6587\u63a8\u7406\u652f\u6301\uff0c\u800c\u975e\u4ec5\u5173\u6ce8\u7b97\u6cd5\u900f\u660e\u6027\u3002", "motivation": "\u5f53\u524dXAI\u65b9\u6cd5\u8fc7\u4e8e\u62bd\u8c61\u4e14\u7f3a\u4e4f\u9002\u5e94\u6027\uff0c\u672a\u80fd\u6709\u6548\u652f\u6301\u7528\u6237\u7406\u89e3\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u89e3\u91ca\u6027AI\u63d0\u5347\u4eba\u7c7b\u51b3\u7b56\u652f\u6301\u3002", "method": "\u63d0\u51fa\u516b\u7ef4\u6982\u5ff5\u6a21\u578b\uff0c\u7ed3\u5408\u53d9\u4e8b\u6c9f\u901a\u3001\u81ea\u9002\u5e94\u4e2a\u6027\u5316\u548c\u6e10\u8fdb\u62ab\u9732\u539f\u5219\uff0c\u5e76\u901a\u8fc7\u533b\u7597\u9886\u57df\u7684\u5feb\u901f\u60c5\u5883\u8bbe\u8ba1\u65b9\u6cd5\u8fdb\u884c\u5b9e\u8bc1\u9a8c\u8bc1\u3002", "result": "\u7528\u6237\u66f4\u504f\u597d\u4e0a\u4e0b\u6587\u654f\u611f\u7684\u591a\u6a21\u6001\u89e3\u91ca\uff0c\u800c\u975e\u6280\u672f\u900f\u660e\u6027\u3002", "conclusion": "\u89e3\u91ca\u6027AI\u4e3a\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684AI\u89e3\u91ca\u65b9\u6cd5\u63d0\u4f9b\u4e86\u7814\u7a76\u8bae\u7a0b\uff0c\u5f3a\u8c03\u8de8\u9886\u57df\u548c\u6587\u5316\u80cc\u666f\u7684\u5e94\u7528\u3002"}}
{"id": "2508.06330", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06330", "abs": "https://arxiv.org/abs/2508.06330", "authors": ["Baorun Li", "Chengrui Zhu", "Siyi Du", "Bingran Chen", "Jie Ren", "Wenfei Wang", "Yong Liu", "Jiajun Lv"], "title": "L2Calib: $SE(3)$-Manifold Reinforcement Learning for Robust Extrinsic Calibration with Degenerate Motion Resilience", "comment": "IROS2025", "summary": "Extrinsic calibration is essential for multi-sensor fusion, existing methods\nrely on structured targets or fully-excited data, limiting real-world\napplicability. Online calibration further suffers from weak excitation, leading\nto unreliable estimates. To address these limitations, we propose a\nreinforcement learning (RL)-based extrinsic calibration framework that\nformulates extrinsic calibration as a decision-making problem, directly\noptimizes $SE(3)$ extrinsics to enhance odometry accuracy. Our approach\nleverages a probabilistic Bingham distribution to model 3D rotations, ensuring\nstable optimization while inherently retaining quaternion symmetry. A\ntrajectory alignment reward mechanism enables robust calibration without\nstructured targets by quantitatively evaluating estimated tightly-coupled\ntrajectory against a reference trajectory. Additionally, an automated data\nselection module filters uninformative samples, significantly improving\nefficiency and scalability for large-scale datasets. Extensive experiments on\nUAVs, UGVs, and handheld platforms demonstrate that our method outperforms\ntraditional optimization-based approaches, achieving high-precision calibration\neven under weak excitation conditions. Our framework simplifies deployment on\ndiverse robotic platforms by eliminating the need for high-quality initial\nextrinsics and enabling calibration from routine operating data. The code is\navailable at https://github.com/APRIL-ZJU/learn-to-calibrate.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u5916\u53c2\u6807\u5b9a\u6846\u67b6\uff0c\u901a\u8fc7\u51b3\u7b56\u95ee\u9898\u76f4\u63a5\u4f18\u5316SE(3)\u5916\u53c2\uff0c\u63d0\u5347\u91cc\u7a0b\u8ba1\u7cbe\u5ea6\uff0c\u65e0\u9700\u7ed3\u6784\u5316\u76ee\u6807\u6216\u9ad8\u8d28\u91cf\u521d\u59cb\u5916\u53c2\u3002", "motivation": "\u73b0\u6709\u5916\u53c2\u6807\u5b9a\u65b9\u6cd5\u4f9d\u8d56\u7ed3\u6784\u5316\u76ee\u6807\u6216\u5b8c\u5168\u6fc0\u52b1\u6570\u636e\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\uff1b\u5728\u7ebf\u6807\u5b9a\u56e0\u5f31\u6fc0\u52b1\u5bfc\u81f4\u4f30\u8ba1\u4e0d\u53ef\u9760\u3002", "method": "\u5c06\u5916\u53c2\u6807\u5b9a\u5efa\u6a21\u4e3a\u51b3\u7b56\u95ee\u9898\uff0c\u5229\u7528Bingham\u5206\u5e03\u5efa\u6a213D\u65cb\u8f6c\uff0c\u5f15\u5165\u8f68\u8ff9\u5bf9\u9f50\u5956\u52b1\u673a\u5236\u548c\u81ea\u52a8\u6570\u636e\u9009\u62e9\u6a21\u5757\u3002", "result": "\u5728\u65e0\u4eba\u673a\u3001\u65e0\u4eba\u8f66\u548c\u624b\u6301\u8bbe\u5907\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u4f20\u7edf\u4f18\u5316\u65b9\u6cd5\uff0c\u5728\u5f31\u6fc0\u52b1\u6761\u4ef6\u4e0b\u4ecd\u80fd\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u6807\u5b9a\u3002", "conclusion": "\u8be5\u6846\u67b6\u7b80\u5316\u4e86\u591a\u6837\u673a\u5668\u4eba\u5e73\u53f0\u7684\u90e8\u7f72\uff0c\u652f\u6301\u4ece\u5e38\u89c4\u64cd\u4f5c\u6570\u636e\u4e2d\u5b9e\u73b0\u6807\u5b9a\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2508.06368", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06368", "abs": "https://arxiv.org/abs/2508.06368", "authors": ["Claudia dAmato", "Giuseppe Rubini", "Francesco Didio", "Donato Francioso", "Fatima Zahra Amara", "Nicola Fanizzi"], "title": "Automated Creation of the Legal Knowledge Graph Addressing Legislation on Violence Against Women: Resource, Methodology and Lessons Learned", "comment": null, "summary": "Legal decision-making process requires the availability of comprehensive and\ndetailed legislative background knowledge and up-to-date information on legal\ncases and related sentences/decisions. Legal Knowledge Graphs (KGs) would be a\nvaluable tool to facilitate access to legal information, to be queried and\nexploited for the purpose, and to enable advanced reasoning and machine\nlearning applications. Indeed, legal KGs may act as knowledge intensive\ncomponent to be used by pre-dictive machine learning solutions supporting the\ndecision process of the legal expert. Nevertheless, a few KGs can be found in\nthe legal domain. To fill this gap, we developed a legal KG targeting legal\ncases of violence against women, along with clear adopted methodologies.\nSpecifically, the paper introduces two complementary approaches for automated\nlegal KG construction; a systematic bottom-up approach, customized for the\nlegal domain, and a new solution leveraging Large Language Models. Starting\nfrom legal sentences publicly available from the European Court of Justice, the\nsolutions integrate structured data extraction, ontology development, and\nsemantic enrichment to produce KGs tailored for legal cases involving violence\nagainst women. After analyzing and comparing the results of the two approaches,\nthe developed KGs are validated via suitable competency questions. The obtained\nKG may be impactful for multiple purposes: can improve the accessibility to\nlegal information both to humans and machine, can enable complex queries and\nmay constitute an important knowledge component to be possibly exploited by\nmachine learning tools tailored for predictive justice.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u6784\u5efa\u6cd5\u5f8b\u77e5\u8bc6\u56fe\u8c31\u7684\u65b9\u6cd5\uff0c\u586b\u8865\u6cd5\u5f8b\u9886\u57df\u77e5\u8bc6\u56fe\u8c31\u7684\u7a7a\u767d\uff0c\u5e76\u9a8c\u8bc1\u5176\u5728\u66b4\u529b\u4fb5\u5bb3\u5987\u5973\u6848\u4ef6\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u6cd5\u5f8b\u51b3\u7b56\u9700\u8981\u5168\u9762\u7684\u7acb\u6cd5\u80cc\u666f\u77e5\u8bc6\u548c\u6700\u65b0\u7684\u6cd5\u5f8b\u6848\u4f8b\u4fe1\u606f\uff0c\u4f46\u6cd5\u5f8b\u9886\u57df\u7684\u77e5\u8bc6\u56fe\u8c31\u8f83\u5c11\u3002", "method": "\u91c7\u7528\u4e24\u79cd\u4e92\u8865\u65b9\u6cd5\uff1a\u7cfb\u7edf\u5316\u7684\u81ea\u5e95\u5411\u4e0a\u65b9\u6cd5\u548c\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u65b0\u65b9\u6848\uff0c\u7ed3\u5408\u7ed3\u6784\u5316\u6570\u636e\u63d0\u53d6\u3001\u672c\u4f53\u5f00\u53d1\u548c\u8bed\u4e49\u4e30\u5bcc\u3002", "result": "\u6784\u5efa\u4e86\u9488\u5bf9\u66b4\u529b\u4fb5\u5bb3\u5987\u5973\u6848\u4ef6\u7684\u6cd5\u5f8b\u77e5\u8bc6\u56fe\u8c31\uff0c\u5e76\u901a\u8fc7\u80fd\u529b\u95ee\u9898\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u77e5\u8bc6\u56fe\u8c31\u53ef\u63d0\u5347\u6cd5\u5f8b\u4fe1\u606f\u7684\u53ef\u8bbf\u95ee\u6027\uff0c\u652f\u6301\u590d\u6742\u67e5\u8be2\uff0c\u5e76\u4e3a\u9884\u6d4b\u6027\u53f8\u6cd5\u673a\u5668\u5b66\u4e60\u5de5\u5177\u63d0\u4f9b\u77e5\u8bc6\u652f\u6301\u3002"}}
{"id": "2508.06404", "categories": ["cs.RO", "math.OC"], "pdf": "https://arxiv.org/pdf/2508.06404", "abs": "https://arxiv.org/abs/2508.06404", "authors": ["Abdullah Zareh Andaryan", "Michael G. H. Bell", "Mohsen Ramezani", "Glenn Geers"], "title": "V*: An Efficient Motion Planning Algorithm for Autonomous Vehicles", "comment": null, "summary": "Autonomous vehicle navigation in structured environments requires planners\ncapable of generating time-optimal, collision-free trajectories that satisfy\ndynamic and kinematic constraints. We introduce V*, a graph-based motion\nplanner that represents speed and direction as explicit state variables within\na discretised space-time-velocity lattice. Unlike traditional methods that\ndecouple spatial search from dynamic feasibility or rely on post-hoc smoothing,\nV* integrates both motion dimensions directly into graph construction through\ndynamic graph generation during search expansion. To manage the complexity of\nhigh-dimensional search, we employ a hexagonal discretisation strategy and\nprovide formal mathematical proofs establishing optimal waypoint spacing and\nminimal node redundancy under constrained heading transitions for\nvelocity-aware motion planning. We develop a mathematical formulation for\ntransient steering dynamics in the kinematic bicycle model, modelling steering\nangle convergence with exponential behaviour, and deriving the relationship for\nconvergence rate parameters. This theoretical foundation, combined with\ngeometric pruning strategies that eliminate expansions leading to infeasible\nsteering configurations, enables V* to evaluate dynamically admissible\nmanoeuvres, ensuring each trajectory is physically realisable without further\nrefinement. We further demonstrate V*'s performance in simulation studies with\ncluttered and dynamic environments involving moving obstacles, showing its\nability to avoid conflicts, yield proactively, and generate safe, efficient\ntrajectories with temporal reasoning capabilities for waiting behaviours and\ndynamic coordination.", "AI": {"tldr": "V*\u662f\u4e00\u79cd\u57fa\u4e8e\u56fe\u7684\u8fd0\u52a8\u89c4\u5212\u5668\uff0c\u901a\u8fc7\u5c06\u901f\u5ea6\u548c\u65b9\u5411\u4f5c\u4e3a\u663e\u5f0f\u72b6\u6001\u53d8\u91cf\u6574\u5408\u5230\u65f6\u7a7a\u901f\u5ea6\u7f51\u683c\u4e2d\uff0c\u76f4\u63a5\u751f\u6210\u52a8\u6001\u53ef\u884c\u7684\u8f68\u8ff9\uff0c\u907f\u514d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u89e3\u8026\u548c\u540e\u5904\u7406\u5e73\u6ed1\u3002", "motivation": "\u5728\u7ed3\u6784\u5316\u73af\u5883\u4e2d\uff0c\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u9700\u8981\u80fd\u591f\u751f\u6210\u65f6\u95f4\u6700\u4f18\u3001\u65e0\u78b0\u649e\u4e14\u6ee1\u8db3\u52a8\u6001\u548c\u8fd0\u52a8\u5b66\u7ea6\u675f\u7684\u8f68\u8ff9\u3002\u4f20\u7edf\u65b9\u6cd5\u901a\u5e38\u89e3\u8026\u7a7a\u95f4\u641c\u7d22\u548c\u52a8\u6001\u53ef\u884c\u6027\uff0c\u6216\u4f9d\u8d56\u540e\u5904\u7406\u5e73\u6ed1\uff0c\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u6216\u8f68\u8ff9\u4e0d\u53ef\u884c\u3002", "method": "V*\u91c7\u7528\u516d\u8fb9\u5f62\u79bb\u6563\u5316\u7b56\u7565\uff0c\u52a8\u6001\u751f\u6210\u56fe\u7ed3\u6784\uff0c\u7ed3\u5408\u6570\u5b66\u5efa\u6a21\u7684\u8f6c\u5411\u52a8\u529b\u5b66\u548c\u51e0\u4f55\u526a\u679d\u7b56\u7565\uff0c\u786e\u4fdd\u8f68\u8ff9\u7684\u52a8\u6001\u53ef\u884c\u6027\u3002", "result": "\u4eff\u771f\u7814\u7a76\u8868\u660e\uff0cV*\u80fd\u591f\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u751f\u6210\u5b89\u5168\u3001\u9ad8\u6548\u7684\u8f68\u8ff9\uff0c\u5177\u5907\u65f6\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u5982\u7b49\u5f85\u884c\u4e3a\u548c\u52a8\u6001\u534f\u8c03\u3002", "conclusion": "V*\u901a\u8fc7\u76f4\u63a5\u6574\u5408\u52a8\u6001\u548c\u8fd0\u52a8\u5b66\u7ea6\u675f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8f68\u8ff9\u89c4\u5212\u7684\u6548\u7387\u548c\u53ef\u884c\u6027\uff0c\u9002\u7528\u4e8e\u590d\u6742\u52a8\u6001\u73af\u5883\u3002"}}
{"id": "2508.06443", "categories": ["cs.AI", "cs.CY", "cs.ET", "cs.GT"], "pdf": "https://arxiv.org/pdf/2508.06443", "abs": "https://arxiv.org/abs/2508.06443", "authors": ["Debabrota Basu", "Udvas Das"], "title": "The Fair Game: Auditing & Debiasing AI Algorithms Over Time", "comment": null, "summary": "An emerging field of AI, namely Fair Machine Learning (ML), aims to quantify\ndifferent types of bias (also known as unfairness) exhibited in the predictions\nof ML algorithms, and to design new algorithms to mitigate them. Often, the\ndefinitions of bias used in the literature are observational, i.e. they use the\ninput and output of a pre-trained algorithm to quantify a bias under concern.\nIn reality,these definitions are often conflicting in nature and can only be\ndeployed if either the ground truth is known or only in retrospect after\ndeploying the algorithm. Thus,there is a gap between what we want Fair ML to\nachieve and what it does in a dynamic social environment. Hence, we propose an\nalternative dynamic mechanism,\"Fair Game\",to assure fairness in the predictions\nof an ML algorithm and to adapt its predictions as the society interacts with\nthe algorithm over time. \"Fair Game\" puts together an Auditor and a Debiasing\nalgorithm in a loop around an ML algorithm. The \"Fair Game\" puts these two\ncomponents in a loop by leveraging Reinforcement Learning (RL). RL algorithms\ninteract with an environment to take decisions, which yields new observations\n(also known as data/feedback) from the environment and in turn, adapts future\ndecisions. RL is already used in algorithms with pre-fixed long-term fairness\ngoals. \"Fair Game\" provides a unique framework where the fairness goals can be\nadapted over time by only modifying the auditor and the different biases it\nquantifies. Thus,\"Fair Game\" aims to simulate the evolution of ethical and\nlegal frameworks in the society by creating an auditor which sends feedback to\na debiasing algorithm deployed around an ML system. This allows us to develop a\nflexible and adaptive-over-time framework to build Fair ML systems pre- and\npost-deployment.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u673a\u5236\u201cFair Game\u201d\uff0c\u901a\u8fc7\u7ed3\u5408\u5ba1\u8ba1\u5458\u548c\u53bb\u504f\u7b97\u6cd5\uff0c\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u7684\u516c\u5e73\u6027\uff0c\u5e76\u968f\u65f6\u95f4\u8c03\u6574\u5176\u9884\u6d4b\u3002", "motivation": "\u73b0\u6709\u516c\u5e73\u673a\u5668\u5b66\u4e60\u7684\u5b9a\u4e49\u591a\u4e3a\u89c2\u5bdf\u6027\uff0c\u4e14\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5b58\u5728\u51b2\u7a81\u548c\u5c40\u9650\u6027\uff0c\u65e0\u6cd5\u9002\u5e94\u52a8\u6001\u793e\u4f1a\u73af\u5883\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u52a8\u6001\u8c03\u6574\u516c\u5e73\u76ee\u6807\u7684\u673a\u5236\u3002", "method": "\u63d0\u51fa\u201cFair Game\u201d\u6846\u67b6\uff0c\u5c06\u5ba1\u8ba1\u5458\u548c\u53bb\u504f\u7b97\u6cd5\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5faa\u73af\u7ed3\u5408\uff0c\u52a8\u6001\u8c03\u6574\u516c\u5e73\u76ee\u6807\u3002", "result": "\u201cFair Game\u201d\u80fd\u591f\u6a21\u62df\u793e\u4f1a\u4f26\u7406\u548c\u6cd5\u5f8b\u6846\u67b6\u7684\u6f14\u53d8\uff0c\u63d0\u4f9b\u7075\u6d3b\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u516c\u5e73\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u3002", "conclusion": "\u201cFair Game\u201d\u4e3a\u516c\u5e73\u673a\u5668\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u52a8\u6001\u3001\u9002\u5e94\u6027\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u90e8\u7f72\u524d\u540e\u7684\u7cfb\u7edf\u3002"}}
{"id": "2508.06426", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06426", "abs": "https://arxiv.org/abs/2508.06426", "authors": ["Youguang Xing", "Xu Luo", "Junlin Xie", "Lianli Gao", "Hengtao Shen", "Jingkuan Song"], "title": "Shortcut Learning in Generalist Robot Policies: The Role of Dataset Diversity and Fragmentation", "comment": "CoRL 2025", "summary": "Generalist robot policies trained on large-scale datasets such as Open\nX-Embodiment (OXE) demonstrate strong performance across a wide range of tasks.\nHowever, they often struggle to generalize beyond the distribution of their\ntraining data. In this paper, we investigate the underlying cause of this\nlimited generalization capability. We identify shortcut learning -- the\nreliance on task-irrelevant features -- as a key impediment to generalization.\nThrough comprehensive theoretical and empirical analysis, we uncover two\nprimary contributors to shortcut learning: (1) limited diversity within\nindividual sub-datasets, and (2) significant distributional disparities across\nsub-datasets, leading to dataset fragmentation. These issues arise from the\ninherent structure of large-scale datasets like OXE, which are typically\ncomposed of multiple sub-datasets collected independently across varied\nenvironments and embodiments. Our findings provide critical insights into\ndataset collection strategies that can reduce shortcut learning and enhance the\ngeneralization ability of generalist robot policies. Moreover, in scenarios\nwhere acquiring new large-scale data is impractical, we demonstrate that\ncarefully selected robotic data augmentation strategies can effectively reduce\nshortcut learning in existing offline datasets, thereby improving\ngeneralization capabilities of generalist robot policies, e.g., $\\pi_0$, in\nboth simulation and real-world environments. More information at\nhttps://lucky-light-sun.github.io/proj/shortcut-learning-in-grps/.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u901a\u7528\u673a\u5668\u4eba\u7b56\u7565\u5728\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u5916\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u539f\u56e0\uff0c\u53d1\u73b0\u2018\u6377\u5f84\u5b66\u4e60\u2019\u662f\u4e3b\u8981\u969c\u788d\uff0c\u5e76\u63d0\u51fa\u6570\u636e\u96c6\u6536\u96c6\u548c\u589e\u5f3a\u7b56\u7565\u4ee5\u6539\u5584\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u901a\u7528\u673a\u5668\u4eba\u7b56\u7565\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff08\u5982OXE\uff09\u4e0a\u8bad\u7ec3\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u6570\u636e\u5206\u5e03\u5916\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u7814\u7a76\u65e8\u5728\u627e\u51fa\u539f\u56e0\u5e76\u63d0\u51fa\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u548c\u5b9e\u8bc1\u5206\u6790\uff0c\u8bc6\u522b\u4e86\u5bfc\u81f4\u6377\u5f84\u5b66\u4e60\u7684\u4e24\u4e2a\u4e3b\u8981\u56e0\u7d20\uff1a\u5b50\u6570\u636e\u96c6\u5185\u591a\u6837\u6027\u4e0d\u8db3\u548c\u5b50\u6570\u636e\u96c6\u95f4\u5206\u5e03\u5dee\u5f02\u5927\u3002", "result": "\u53d1\u73b0\u6570\u636e\u96c6\u7ed3\u6784\u548c\u6536\u96c6\u65b9\u5f0f\u662f\u6cdb\u5316\u80fd\u529b\u53d7\u9650\u7684\u5173\u952e\uff0c\u5e76\u63d0\u51fa\u6570\u636e\u589e\u5f3a\u7b56\u7565\u53ef\u6709\u6548\u51cf\u5c11\u6377\u5f84\u5b66\u4e60\u3002", "conclusion": "\u4f18\u5316\u6570\u636e\u96c6\u6536\u96c6\u7b56\u7565\u548c\u91c7\u7528\u6570\u636e\u589e\u5f3a\u6280\u672f\u53ef\u63d0\u5347\u901a\u7528\u673a\u5668\u4eba\u7b56\u7565\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5c24\u5176\u5728\u65e0\u6cd5\u83b7\u53d6\u65b0\u6570\u636e\u65f6\u3002"}}
{"id": "2508.06454", "categories": ["cs.AI", "cs.GT"], "pdf": "https://arxiv.org/pdf/2508.06454", "abs": "https://arxiv.org/abs/2508.06454", "authors": ["Joshua Caiata", "Ben Armstrong", "Kate Larson"], "title": "What Voting Rules Actually Do: A Data-Driven Analysis of Multi-Winner Voting", "comment": "41 pages", "summary": "Committee-selection problems arise in many contexts and applications, and\nthere has been increasing interest within the social choice research community\non identifying which properties are satisfied by different multi-winner voting\nrules. In this work, we propose a data-driven framework to evaluate how\nfrequently voting rules violate axioms across diverse preference distributions\nin practice, shifting away from the binary perspective of axiom satisfaction\ngiven by worst-case analysis. Using this framework, we analyze the relationship\nbetween multi-winner voting rules and their axiomatic performance under several\npreference distributions. We then show that neural networks, acting as voting\nrules, can outperform traditional rules in minimizing axiom violations. Our\nresults suggest that data-driven approaches to social choice can inform the\ndesign of new voting systems and support the continuation of data-driven\nresearch in social choice.", "AI": {"tldr": "\u63d0\u51fa\u6570\u636e\u9a71\u52a8\u6846\u67b6\u8bc4\u4f30\u591a\u8d62\u5bb6\u6295\u7968\u89c4\u5219\u5728\u5b9e\u9645\u504f\u597d\u5206\u5e03\u4e2d\u8fdd\u53cd\u516c\u7406\u7684\u9891\u7387\uff0c\u5e76\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u4f18\u5316\u4f20\u7edf\u89c4\u5219\u3002", "motivation": "\u7814\u7a76\u591a\u8d62\u5bb6\u6295\u7968\u89c4\u5219\u5728\u4e0d\u540c\u504f\u597d\u5206\u5e03\u4e0b\u7684\u516c\u7406\u8868\u73b0\uff0c\u6446\u8131\u6700\u574f\u60c5\u51b5\u5206\u6790\u7684\u4e8c\u5143\u89c6\u89d2\u3002", "method": "\u63d0\u51fa\u6570\u636e\u9a71\u52a8\u6846\u67b6\uff0c\u5206\u6790\u6295\u7968\u89c4\u5219\u4e0e\u516c\u7406\u8868\u73b0\u7684\u5173\u7cfb\uff0c\u5e76\u5229\u7528\u795e\u7ecf\u7f51\u7edc\u4f18\u5316\u3002", "result": "\u795e\u7ecf\u7f51\u7edc\u4f5c\u4e3a\u6295\u7968\u89c4\u5219\u4f18\u4e8e\u4f20\u7edf\u89c4\u5219\uff0c\u80fd\u51cf\u5c11\u516c\u7406\u8fdd\u53cd\u3002", "conclusion": "\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u53ef\u4e3a\u65b0\u6295\u7968\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u652f\u6301\uff0c\u63a8\u52a8\u793e\u4f1a\u9009\u62e9\u9886\u57df\u7684\u6570\u636e\u9a71\u52a8\u7814\u7a76\u3002"}}
