{"id": "2508.04714", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MA", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.04714", "abs": "https://arxiv.org/abs/2508.04714", "authors": ["Chitranshu Harbola", "Anupam Purwar"], "title": "Prescriptive Agents based on Rag for Automated Maintenance (PARAM)", "comment": null, "summary": "Industrial machinery maintenance requires timely intervention to prevent\ncatastrophic failures and optimize operational efficiency. This paper presents\nan integrated Large Language Model (LLM)-based intelligent system for\nprescriptive maintenance that extends beyond traditional anomaly detection to\nprovide actionable maintenance recommendations. Building upon our prior LAMP\nframework for numerical data analysis, we develop a comprehensive solution that\ncombines bearing vibration frequency analysis with multi agentic generation for\nintelligent maintenance planning. Our approach serializes bearing vibration\ndata (BPFO, BPFI, BSF, FTF frequencies) into natural language for LLM\nprocessing, enabling few-shot anomaly detection with high accuracy. The system\nclassifies fault types (inner race, outer race, ball/roller, cage faults) and\nassesses severity levels. A multi-agentic component processes maintenance\nmanuals using vector embeddings and semantic search, while also conducting web\nsearches to retrieve comprehensive procedural knowledge and access up-to-date\nmaintenance practices for more accurate and in-depth recommendations. The\nGemini model then generates structured maintenance recommendations includes\nimmediate actions, inspection checklists, corrective measures, parts\nrequirements, and timeline specifications. Experimental validation in bearing\nvibration datasets demonstrates effective anomaly detection and contextually\nrelevant maintenance guidance. The system successfully bridges the gap between\ncondition monitoring and actionable maintenance planning, providing industrial\npractitioners with intelligent decision support. This work advances the\napplication of LLMs in industrial maintenance, offering a scalable framework\nfor prescriptive maintenance across machinery components and industrial\nsectors.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u667a\u80fd\u7cfb\u7edf\uff0c\u7528\u4e8e\u5de5\u4e1a\u673a\u68b0\u7684\u9884\u6d4b\u6027\u7ef4\u62a4\uff0c\u7ed3\u5408\u632f\u52a8\u9891\u7387\u5206\u6790\u548c\u591a\u4ee3\u7406\u751f\u6210\u6280\u672f\uff0c\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u7ef4\u62a4\u5efa\u8bae\u3002", "motivation": "\u5de5\u4e1a\u673a\u68b0\u7ef4\u62a4\u9700\u8981\u53ca\u65f6\u5e72\u9884\u4ee5\u9632\u6b62\u707e\u96be\u6027\u6545\u969c\u5e76\u4f18\u5316\u8fd0\u884c\u6548\u7387\uff0c\u4f20\u7edf\u65b9\u6cd5\u4ec5\u80fd\u68c0\u6d4b\u5f02\u5e38\uff0c\u7f3a\u4e4f\u5177\u4f53\u7ef4\u62a4\u5efa\u8bae\u3002", "method": "\u7cfb\u7edf\u5c06\u8f74\u627f\u632f\u52a8\u6570\u636e\uff08BPFO\u3001BPFI\u7b49\uff09\u8f6c\u5316\u4e3a\u81ea\u7136\u8bed\u8a00\u4f9bLLM\u5904\u7406\uff0c\u7ed3\u5408\u591a\u4ee3\u7406\u6280\u672f\u5206\u6790\u7ef4\u62a4\u624b\u518c\u548c\u7f51\u7edc\u4fe1\u606f\uff0c\u751f\u6210\u7ed3\u6784\u5316\u7ef4\u62a4\u5efa\u8bae\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0c\u7cfb\u7edf\u80fd\u6709\u6548\u68c0\u6d4b\u5f02\u5e38\u5e76\u63d0\u4f9b\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u7ef4\u62a4\u6307\u5bfc\uff0c\u586b\u8865\u4e86\u72b6\u6001\u76d1\u6d4b\u4e0e\u53ef\u64cd\u4f5c\u7ef4\u62a4\u8ba1\u5212\u4e4b\u95f4\u7684\u7a7a\u767d\u3002", "conclusion": "\u8be5\u7814\u7a76\u63a8\u52a8\u4e86LLM\u5728\u5de5\u4e1a\u7ef4\u62a4\u4e2d\u7684\u5e94\u7528\uff0c\u4e3a\u8de8\u884c\u4e1a\u673a\u68b0\u7ec4\u4ef6\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u9884\u6d4b\u6027\u7ef4\u62a4\u6846\u67b6\u3002"}}
{"id": "2508.04719", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.04719", "abs": "https://arxiv.org/abs/2508.04719", "authors": ["Amulya Bhattaram", "Justin Chung", "Stanley Chung", "Ranit Gupta", "Janani Ramamoorthy", "Kartikeya Gullapalli", "Diana Marculescu", "Dimitrios Stamoulis"], "title": "GeoFlow: Agentic Workflow Automation for Geospatial Tasks", "comment": "Accepted to ACM SIGSPATIAL 2025", "summary": "We present GeoFlow, a method that automatically generates agentic workflows\nfor geospatial tasks. Unlike prior work that focuses on reasoning decomposition\nand leaves API selection implicit, our method provides each agent with detailed\ntool-calling objectives to guide geospatial API invocation at runtime. GeoFlow\nincreases agentic success by 6.8% and reduces token usage by up to fourfold\nacross major LLM families compared to state-of-the-art approaches.", "AI": {"tldr": "GeoFlow\u662f\u4e00\u79cd\u81ea\u52a8\u751f\u6210\u5730\u7406\u7a7a\u95f4\u4efb\u52a1\u4ee3\u7406\u5de5\u4f5c\u6d41\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u660e\u786e\u5de5\u5177\u8c03\u7528\u76ee\u6807\u63d0\u5347\u4ee3\u7406\u6210\u529f\u7387\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u63a8\u7406\u5206\u89e3\u4e0a\u8868\u73b0\u4e0d\u8db3\uff0c\u4e14API\u9009\u62e9\u9690\u542b\uff0cGeoFlow\u65e8\u5728\u901a\u8fc7\u660e\u786e\u76ee\u6807\u6539\u8fdb\u4ee3\u7406\u6027\u80fd\u3002", "method": "\u4e3a\u6bcf\u4e2a\u4ee3\u7406\u63d0\u4f9b\u8be6\u7ec6\u7684\u5de5\u5177\u8c03\u7528\u76ee\u6807\uff0c\u6307\u5bfc\u8fd0\u884c\u65f6\u5730\u7406\u7a7a\u95f4API\u7684\u8c03\u7528\u3002", "result": "GeoFlow\u5c06\u4ee3\u7406\u6210\u529f\u7387\u63d0\u53476.8%\uff0c\u5e76\u5728\u4e3b\u6d41LLM\u5bb6\u65cf\u4e2d\u51cf\u5c11\u9ad8\u8fbe\u56db\u500d\u7684token\u4f7f\u7528\u3002", "conclusion": "GeoFlow\u901a\u8fc7\u660e\u786e\u5de5\u5177\u8c03\u7528\u76ee\u6807\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ee3\u7406\u5de5\u4f5c\u6d41\u7684\u6548\u7387\u548c\u6210\u529f\u7387\u3002"}}
{"id": "2508.04720", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.04720", "abs": "https://arxiv.org/abs/2508.04720", "authors": ["Yingjie Zhou", "Jiezhang Cao", "Farong Wen", "Li Xu", "Yanwei Jiang", "Jun Jia", "Ronghui Li", "Xiaohong Liu", "Yu Zhou", "Xiongkuo Min", "Jie Guo", "Zicheng Zhang", "Guangtao Zhai"], "title": "Who is a Better Player: LLM against LLM", "comment": null, "summary": "Adversarial board games, as a paradigmatic domain of strategic reasoning and\nintelligence, have long served as both a popular competitive activity and a\nbenchmark for evaluating artificial intelligence (AI) systems. Building on this\nfoundation, we propose an adversarial benchmarking framework to assess the\ncomprehensive performance of Large Language Models (LLMs) through board games\ncompetition, compensating the limitation of data dependency of the mainstream\nQuestion-and-Answer (Q&A) based benchmark method. We introduce Qi Town, a\nspecialized evaluation platform that supports 5 widely played games and\ninvolves 20 LLM-driven players. The platform employs both the Elo rating system\nand a novel Performance Loop Graph (PLG) to quantitatively evaluate the\ntechnical capabilities of LLMs, while also capturing Positive Sentiment Score\n(PSS) throughout gameplay to assess mental fitness. The evaluation is\nstructured as a round-robin tournament, enabling systematic comparison across\nplayers. Experimental results indicate that, despite technical differences,\nmost LLMs remain optimistic about winning and losing, demonstrating greater\nadaptability to high-stress adversarial environments than humans. On the other\nhand, the complex relationship between cyclic wins and losses in PLGs exposes\nthe instability of LLMs' skill play during games, warranting further\nexplanation and exploration.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bf9\u6297\u6027\u68cb\u76d8\u6e38\u620f\u7684LLM\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u6bd4\u8d5b\u548c\u60c5\u611f\u8bc4\u5206\u5168\u9762\u8bc4\u4f30LLM\u7684\u6027\u80fd\u3002", "motivation": "\u5f25\u8865\u4e3b\u6d41\u95ee\u7b54\u57fa\u51c6\u65b9\u6cd5\u7684\u6570\u636e\u4f9d\u8d56\u6027\u9650\u5236\uff0c\u901a\u8fc7\u5bf9\u6297\u6027\u73af\u5883\u66f4\u5168\u9762\u5730\u8bc4\u4f30LLM\u7684\u6218\u7565\u63a8\u7406\u548c\u9002\u5e94\u80fd\u529b\u3002", "method": "\u5f00\u53d1Qi Town\u5e73\u53f0\uff0c\u652f\u63015\u79cd\u6e38\u620f\u548c20\u4e2aLLM\u73a9\u5bb6\uff0c\u4f7f\u7528Elo\u8bc4\u5206\u548cPLG\u5b9a\u91cf\u8bc4\u4f30\u6280\u672f\u80fd\u529b\uff0c\u5e76\u901a\u8fc7PSS\u8bc4\u4f30\u5fc3\u7406\u9002\u5e94\u6027\u3002", "result": "\u5b9e\u9a8c\u663e\u793aLLM\u5728\u9ad8\u538b\u5bf9\u6297\u73af\u5883\u4e2d\u8868\u73b0\u4e50\u89c2\u4e14\u9002\u5e94\u6027\u5f3a\uff0c\u4f46PLG\u63ed\u793a\u4e86\u6280\u80fd\u53d1\u6325\u7684\u4e0d\u7a33\u5b9a\u6027\u3002", "conclusion": "\u5bf9\u6297\u6027\u68cb\u76d8\u6e38\u620f\u662f\u8bc4\u4f30LLM\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u5176\u6280\u80fd\u4e0d\u7a33\u5b9a\u6027\u7684\u539f\u56e0\u3002"}}
{"id": "2508.04846", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.04846", "abs": "https://arxiv.org/abs/2508.04846", "authors": ["Mahdi Nazari Ashani", "Ali Asghar Alesheikh", "Saba Kazemi", "Kimya Kheirkhah", "Yasin Mohammadi", "Fatemeh Rezaie", "Amir Mahdi Manafi", "Hedieh Zarkesh"], "title": "Fine-Tuning Small Language Models (SLMs) for Autonomous Web-based Geographical Information Systems (AWebGIS)", "comment": null, "summary": "Autonomous web-based geographical information systems (AWebGIS) aim to\nperform geospatial operations from natural language input, providing intuitive,\nintelligent, and hands-free interaction. However, most current solutions rely\non cloud-based large language models (LLMs), which require continuous internet\naccess and raise users' privacy and scalability issues due to centralized\nserver processing. This study compares three approaches to enabling AWebGIS:\n(1) a fully-automated online method using cloud-based LLMs (e.g., Cohere); (2)\na semi-automated offline method using classical machine learning classifiers\nsuch as support vector machine and random forest; and (3) a fully autonomous\noffline (client-side) method based on a fine-tuned small language model (SLM),\nspecifically T5-small model, executed in the client's web browser. The third\napproach, which leverages SLMs, achieved the highest accuracy among all\nmethods, with an exact matching accuracy of 0.93, Levenshtein similarity of\n0.99, and recall-oriented understudy for gisting evaluation ROUGE-1 and ROUGE-L\nscores of 0.98. Crucially, this client-side computation strategy reduces the\nload on backend servers by offloading processing to the user's device,\neliminating the need for server-based inference. These results highlight the\nfeasibility of browser-executable models for AWebGIS solutions.", "AI": {"tldr": "\u6bd4\u8f83\u4e09\u79cdAWebGIS\u65b9\u6cd5\uff0c\u53d1\u73b0\u57fa\u4e8e\u5ba2\u6237\u7aef\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\u7684\u65b9\u6cd5\u5728\u7cbe\u5ea6\u548c\u9690\u79c1\u4fdd\u62a4\u4e0a\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709AWebGIS\u4f9d\u8d56\u4e91\u7aefLLM\u5bfc\u81f4\u7684\u9690\u79c1\u548c\u53ef\u6269\u5c55\u6027\u95ee\u9898\u3002", "method": "\u6bd4\u8f83\u4e09\u79cd\u65b9\u6cd5\uff1a\u4e91\u7aefLLM\u3001\u79bb\u7ebf\u7ecf\u5178\u673a\u5668\u5b66\u4e60\u3001\u5ba2\u6237\u7aefSLM\u3002", "result": "\u5ba2\u6237\u7aefSLM\u65b9\u6cd5\u7cbe\u5ea6\u6700\u9ad8\uff08\u51c6\u786e\u73870.93\uff09\uff0c\u5e76\u51cf\u5c11\u670d\u52a1\u5668\u8d1f\u8f7d\u3002", "conclusion": "\u6d4f\u89c8\u5668\u53ef\u6267\u884c\u6a21\u578b\u5728AWebGIS\u4e2d\u5177\u6709\u53ef\u884c\u6027\u3002"}}
{"id": "2508.04834", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.04834", "abs": "https://arxiv.org/abs/2508.04834", "authors": ["Morten Roed Frederiksen", "Kasper St\u00f8y"], "title": "On the causality between affective impact and coordinated human-robot reactions", "comment": "7 pages, 5 figures, 29th IEEE International Workshop on Robot and\n  Human Communication (ROMAN)", "summary": "In an effort to improve how robots function in social contexts, this paper\ninvestigates if a robot that actively shares a reaction to an event with a\nhuman alters how the human perceives the robot's affective impact. To verify\nthis, we created two different test setups. One to highlight and isolate the\nreaction element of affective robot expressions, and one to investigate the\neffects of applying specific timing delays to a robot reacting to a physical\nencounter with a human. The first test was conducted with two different groups\n(n=84) of human observers, a test group and a control group both interacting\nwith the robot. The second test was performed with 110 participants using\nincreasingly longer reaction delays for the robot with every ten participants.\nThe results show a statistically significant change (p$<$.05) in perceived\naffective impact for the robots when they react to an event shared with a human\nobserver rather than reacting at random. The result also shows for shared\nphysical interaction, the near-human reaction times from the robot are most\nappropriate for the scenario. The paper concludes that a delay time around\n200ms may render the biggest impact on human observers for small-sized\nnon-humanoid robots. It further concludes that a slightly shorter reaction time\naround 100ms is most effective when the goal is to make the human observers\nfeel they made the biggest impact on the robot.", "AI": {"tldr": "\u7814\u7a76\u673a\u5668\u4eba\u901a\u8fc7\u4e3b\u52a8\u5206\u4eab\u4e8b\u4ef6\u53cd\u5e94\u5bf9\u4eba\u7c7b\u611f\u77e5\u5176\u60c5\u611f\u5f71\u54cd\u7684\u6548\u679c\uff0c\u53d1\u73b0\u5171\u4eab\u53cd\u5e94\u65f6\u95f4\u5bf9\u611f\u77e5\u6709\u663e\u8457\u5f71\u54cd\u3002", "motivation": "\u63d0\u5347\u673a\u5668\u4eba\u5728\u793e\u4ea4\u73af\u5883\u4e2d\u7684\u8868\u73b0\uff0c\u63a2\u7a76\u5176\u53cd\u5e94\u884c\u4e3a\u5bf9\u4eba\u7c7b\u611f\u77e5\u7684\u5f71\u54cd\u3002", "method": "\u8bbe\u8ba1\u4e24\u7ec4\u5b9e\u9a8c\uff1a\u4e00\u7ec4\u9694\u79bb\u673a\u5668\u4eba\u60c5\u611f\u8868\u8fbe\u7684\u53cd\u5e94\u5143\u7d20\uff0c\u53e6\u4e00\u7ec4\u6d4b\u8bd5\u4e0d\u540c\u53cd\u5e94\u5ef6\u8fdf\u65f6\u95f4\u5bf9\u7269\u7406\u4e92\u52a8\u7684\u5f71\u54cd\u3002", "result": "\u5171\u4eab\u4e8b\u4ef6\u7684\u673a\u5668\u4eba\u53cd\u5e94\u663e\u8457\u6539\u53d8\u4eba\u7c7b\u611f\u77e5\uff08p<0.05\uff09\uff1b200ms\u5ef6\u8fdf\u5bf9\u5c0f\u578b\u975e\u4eba\u5f62\u673a\u5668\u4eba\u6700\u6709\u6548\uff0c100ms\u5ef6\u8fdf\u5219\u8ba9\u4eba\u7c7b\u611f\u89c9\u5f71\u54cd\u6700\u5927\u3002", "conclusion": "\u673a\u5668\u4eba\u53cd\u5e94\u65f6\u95f4\u5bf9\u4eba\u7c7b\u611f\u77e5\u6709\u663e\u8457\u5f71\u54cd\uff0c200ms\u548c100ms\u5206\u522b\u4e3a\u4e0d\u540c\u76ee\u6807\u7684\u6700\u4f18\u5ef6\u8fdf\u3002"}}
{"id": "2508.04848", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.04848", "abs": "https://arxiv.org/abs/2508.04848", "authors": ["Chang Tian", "Matthew B. Blaschko", "Mingzhe Xing", "Xiuxing Li", "Yinliang Yue", "Marie-Francine Moens"], "title": "Large Language Models Reasoning Abilities Under Non-Ideal Conditions After RL-Fine-Tuning", "comment": "large language models, large vision-language model, reasoning,\n  non-ideal conditions, reinforcement learning", "summary": "Reinforcement learning (RL) has become a key technique for enhancing the\nreasoning abilities of large language models (LLMs), with policy-gradient\nalgorithms dominating the post-training stage because of their efficiency and\neffectiveness. However, most existing benchmarks evaluate large-language-model\nreasoning under idealized settings, overlooking performance in realistic,\nnon-ideal scenarios. We identify three representative non-ideal scenarios with\npractical relevance: summary inference, fine-grained noise suppression, and\ncontextual filtering. We introduce a new research direction guided by\nbrain-science findings that human reasoning remains reliable under imperfect\ninputs. We formally define and evaluate these challenging scenarios. We\nfine-tune three LLMs and a state-of-the-art large vision-language model (LVLM)\nusing RL with a representative policy-gradient algorithm and then test their\nperformance on eight public datasets. Our results reveal that while RL\nfine-tuning improves baseline reasoning under idealized settings, performance\ndeclines significantly across all three non-ideal scenarios, exposing critical\nlimitations in advanced reasoning capabilities. Although we propose a\nscenario-specific remediation method, our results suggest current methods leave\nthese reasoning deficits largely unresolved. This work highlights that the\nreasoning abilities of large models are often overstated and underscores the\nimportance of evaluating models under non-ideal scenarios. The code and data\nwill be released at XXXX.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5728\u975e\u7406\u60f3\u573a\u666f\u4e0b\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u63a8\u7406\u80fd\u529b\u7684\u63d0\u5347\u6709\u9650\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u591a\u5173\u6ce8\u7406\u60f3\u5316\u573a\u666f\uff0c\u5ffd\u7565\u4e86\u5b9e\u9645\u5e94\u7528\u4e2d\u975e\u7406\u60f3\u573a\u666f\u7684\u6027\u80fd\uff0c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7RL\u5fae\u8c03LLMs\u548c\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\uff0c\u5e76\u5728\u4e09\u79cd\u975e\u7406\u60f3\u573a\u666f\uff08\u6458\u8981\u63a8\u7406\u3001\u7ec6\u7c92\u5ea6\u566a\u58f0\u6291\u5236\u3001\u4e0a\u4e0b\u6587\u8fc7\u6ee4\uff09\u4e0b\u6d4b\u8bd5\u6027\u80fd\u3002", "result": "RL\u5fae\u8c03\u5728\u7406\u60f3\u573a\u666f\u4e0b\u63d0\u5347\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5728\u975e\u7406\u60f3\u573a\u666f\u4e2d\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u6709\u6548\u89e3\u51b3\u8fd9\u4e9b\u7f3a\u9677\u3002", "conclusion": "\u5927\u578b\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u5e38\u88ab\u9ad8\u4f30\uff0c\u9700\u5728\u975e\u7406\u60f3\u573a\u666f\u4e0b\u8bc4\u4f30\uff0c\u5f53\u524d\u65b9\u6cd5\u4ecd\u9700\u6539\u8fdb\u3002"}}
{"id": "2508.04931", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.04931", "abs": "https://arxiv.org/abs/2508.04931", "authors": ["Jin Wang", "Weijie Wang", "Boyuan Deng", "Heng Zhang", "Rui Dai", "Nikos Tsagarakis"], "title": "INTENTION: Inferring Tendencies of Humanoid Robot Motion Through Interactive Intuition and Grounded VLM", "comment": "Project Web: https://robo-intention.github.io", "summary": "Traditional control and planning for robotic manipulation heavily rely on\nprecise physical models and predefined action sequences. While effective in\nstructured environments, such approaches often fail in real-world scenarios due\nto modeling inaccuracies and struggle to generalize to novel tasks. In\ncontrast, humans intuitively interact with their surroundings, demonstrating\nremarkable adaptability, making efficient decisions through implicit physical\nunderstanding. In this work, we propose INTENTION, a novel framework enabling\nrobots with learned interactive intuition and autonomous manipulation in\ndiverse scenarios, by integrating Vision-Language Models (VLMs) based scene\nreasoning with interaction-driven memory. We introduce Memory Graph to record\nscenes from previous task interactions which embodies human-like understanding\nand decision-making about different tasks in real world. Meanwhile, we design\nan Intuitive Perceptor that extracts physical relations and affordances from\nvisual scenes. Together, these components empower robots to infer appropriate\ninteraction behaviors in new scenes without relying on repetitive instructions.\nVideos: https://robo-intention.github.io", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faINTENTION\u6846\u67b6\uff0c\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u4ea4\u4e92\u9a71\u52a8\u8bb0\u5fc6\uff0c\u8d4b\u4e88\u673a\u5668\u4eba\u76f4\u89c9\u5f0f\u4ea4\u4e92\u548c\u81ea\u4e3b\u64cd\u4f5c\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u4eba\u63a7\u5236\u4f9d\u8d56\u7cbe\u786e\u6a21\u578b\u548c\u9884\u5b9a\u4e49\u52a8\u4f5c\uff0c\u96be\u4ee5\u9002\u5e94\u73b0\u5b9e\u4e16\u754c\u7684\u4e0d\u786e\u5b9a\u6027\u548c\u65b0\u4efb\u52a1\u3002\u4eba\u7c7b\u5219\u901a\u8fc7\u76f4\u89c9\u548c\u7269\u7406\u7406\u89e3\u9ad8\u6548\u51b3\u7b56\uff0c\u542f\u53d1\u7814\u7a76\u8005\u5f00\u53d1\u7c7b\u4f3c\u80fd\u529b\u7684\u673a\u5668\u4eba\u6846\u67b6\u3002", "method": "INTENTION\u6846\u67b6\u6574\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u573a\u666f\u63a8\u7406\u548c\u4ea4\u4e92\u9a71\u52a8\u8bb0\u5fc6\uff08Memory Graph\uff09\uff0c\u5e76\u8bbe\u8ba1Intuitive Perceptor\u63d0\u53d6\u7269\u7406\u5173\u7cfb\u548c\u529f\u80fd\u5c5e\u6027\u3002", "result": "\u673a\u5668\u4eba\u80fd\u5728\u65b0\u573a\u666f\u4e2d\u63a8\u65ad\u5408\u9002\u7684\u4ea4\u4e92\u884c\u4e3a\uff0c\u65e0\u9700\u91cd\u590d\u6307\u4ee4\uff0c\u8868\u73b0\u51fa\u7c7b\u4f3c\u4eba\u7c7b\u7684\u9002\u5e94\u6027\u548c\u51b3\u7b56\u80fd\u529b\u3002", "conclusion": "INTENTION\u6846\u67b6\u901a\u8fc7\u6a21\u4eff\u4eba\u7c7b\u76f4\u89c9\u548c\u8bb0\u5fc6\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5728\u591a\u6837\u5316\u573a\u666f\u4e2d\u7684\u81ea\u4e3b\u64cd\u4f5c\u80fd\u529b\u3002"}}
{"id": "2508.04915", "categories": ["cs.AI", "cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.04915", "abs": "https://arxiv.org/abs/2508.04915", "authors": ["Huiya Zhao", "Yinghao Zhu", "Zixiang Wang", "Yasha Wang", "Junyi Gao", "Liantao Ma"], "title": "ConfAgents: A Conformal-Guided Multi-Agent Framework for Cost-Efficient Medical Diagnosis", "comment": "Code: https://github.com/PKU-AICare/ConfAgents", "summary": "The efficacy of AI agents in healthcare research is hindered by their\nreliance on static, predefined strategies. This creates a critical limitation:\nagents can become better tool-users but cannot learn to become better strategic\nplanners, a crucial skill for complex domains like healthcare. We introduce\nHealthFlow, a self-evolving AI agent that overcomes this limitation through a\nnovel meta-level evolution mechanism. HealthFlow autonomously refines its own\nhigh-level problem-solving policies by distilling procedural successes and\nfailures into a durable, strategic knowledge base. To anchor our research and\nfacilitate reproducible evaluation, we introduce EHRFlowBench, a new benchmark\nfeaturing complex, realistic health data analysis tasks derived from\npeer-reviewed clinical research. Our comprehensive experiments demonstrate that\nHealthFlow's self-evolving approach significantly outperforms state-of-the-art\nagent frameworks. This work marks a necessary shift from building better\ntool-users to designing smarter, self-evolving task-managers, paving the way\nfor more autonomous and effective AI for scientific discovery.", "AI": {"tldr": "HealthFlow\u662f\u4e00\u79cd\u81ea\u8fdb\u5316\u7684AI\u4ee3\u7406\uff0c\u901a\u8fc7\u5143\u7ea7\u8fdb\u5316\u673a\u5236\u63d0\u5347\u6218\u7565\u89c4\u5212\u80fd\u529b\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6846\u67b6\u3002", "motivation": "\u73b0\u6709AI\u4ee3\u7406\u4f9d\u8d56\u9759\u6001\u7b56\u7565\uff0c\u65e0\u6cd5\u5728\u590d\u6742\u9886\u57df\uff08\u5982\u533b\u7597\uff09\u4e2d\u6210\u4e3a\u66f4\u597d\u7684\u6218\u7565\u89c4\u5212\u8005\u3002", "method": "\u5f15\u5165HealthFlow\u548cEHRFlowBench\u57fa\u51c6\uff0c\u901a\u8fc7\u63d0\u70bc\u6210\u529f\u4e0e\u5931\u8d25\u7ecf\u9a8c\u81ea\u4e3b\u4f18\u5316\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660eHealthFlow\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u4ee3\u7406\u6846\u67b6\u3002", "conclusion": "\u8be5\u7814\u7a76\u6807\u5fd7\u7740\u4ece\u5de5\u5177\u4f7f\u7528\u8005\u8f6c\u5411\u81ea\u8fdb\u5316\u4efb\u52a1\u7ba1\u7406\u8005\u7684\u91cd\u8981\u8f6c\u53d8\uff0c\u63a8\u52a8\u66f4\u81ea\u4e3b\u7684AI\u79d1\u5b66\u53d1\u5c55\u3002"}}
{"id": "2508.04981", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.04981", "abs": "https://arxiv.org/abs/2508.04981", "authors": ["Tianyuan Zheng", "Jingang Yi", "Kaiyan Yu"], "title": "Optimal Planning for Multi-Robot Simultaneous Area and Line Coverage Using Hierarchical Cyclic Merging Regulation", "comment": null, "summary": "The double coverage problem focuses on determining efficient, collision-free\nroutes for multiple robots to simultaneously cover linear features (e.g.,\nsurface cracks or road routes) and survey areas (e.g., parking lots or local\nregions) in known environments. In these problems, each robot carries two\nfunctional roles: service (linear feature footprint coverage) and exploration\n(complete area coverage). Service has a smaller operational footprint but\nincurs higher costs (e.g., time) compared to exploration. We present optimal\nplanning algorithms for the double coverage problems using hierarchical cyclic\nmerging regulation (HCMR). To reduce the complexity for optimal planning\nsolutions, we analyze the manifold attachment process during graph traversal\nfrom a Morse theory perspective. We show that solutions satisfying minimum path\nlength and collision-free constraints must belong to a Morse-bounded\ncollection. To identify this collection, we introduce the HCMR algorithm. In\nHCMR, cyclic merging search regulates traversal behavior, while edge sequence\nback propagation converts these regulations into graph edge traversal\nsequences. Incorporating balanced partitioning, the optimal sequence is\nselected to generate routes for each robot. We prove the optimality of the HCMR\nalgorithm under a fixed sweep direction. The multi-robot simulation results\ndemonstrate that the HCMR algorithm significantly improves planned path length\nby at least 10.0%, reduces task time by at least 16.9% in average, and ensures\nconflict-free operation compared to other state-of-the-art planning methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5c42\u6b21\u5faa\u73af\u5408\u5e76\u8c03\u8282\uff08HCMR\uff09\u7684\u53cc\u91cd\u8986\u76d6\u95ee\u9898\u4f18\u5316\u89c4\u5212\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8def\u5f84\u957f\u5ea6\u548c\u4efb\u52a1\u65f6\u95f4\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u591a\u673a\u5668\u4eba\u5728\u5df2\u77e5\u73af\u5883\u4e2d\u540c\u65f6\u8986\u76d6\u7ebf\u6027\u7279\u5f81\u548c\u533a\u57df\u65f6\u7684\u9ad8\u6548\u3001\u65e0\u78b0\u649e\u8def\u5f84\u89c4\u5212\u95ee\u9898\u3002", "method": "\u91c7\u7528HCMR\u7b97\u6cd5\uff0c\u7ed3\u5408Morse\u7406\u8bba\u5206\u6790\u6d41\u5f62\u9644\u7740\u8fc7\u7a0b\uff0c\u901a\u8fc7\u5faa\u73af\u5408\u5e76\u641c\u7d22\u548c\u8fb9\u5e8f\u5217\u53cd\u5411\u4f20\u64ad\u751f\u6210\u6700\u4f18\u8def\u5f84\u3002", "result": "HCMR\u7b97\u6cd5\u5728\u8def\u5f84\u957f\u5ea6\u4e0a\u81f3\u5c11\u63d0\u534710.0%\uff0c\u4efb\u52a1\u65f6\u95f4\u5e73\u5747\u51cf\u5c1116.9%\uff0c\u4e14\u786e\u4fdd\u65e0\u51b2\u7a81\u64cd\u4f5c\u3002", "conclusion": "HCMR\u7b97\u6cd5\u5728\u56fa\u5b9a\u626b\u63cf\u65b9\u5411\u4e0b\u5177\u6709\u6700\u4f18\u6027\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u89c4\u5212\u65b9\u6cd5\u3002"}}
{"id": "2508.05006", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05006", "abs": "https://arxiv.org/abs/2508.05006", "authors": ["Youzhi Zhang", "Yufei Li", "Gaofeng Meng", "Hongbin Liu", "Jiebo Luo"], "title": "The Docking Game: Loop Self-Play for Fast, Dynamic, and Accurate Prediction of Flexible Protein--Ligand Binding", "comment": "21 pages, 9 figures", "summary": "Molecular docking is a crucial aspect of drug discovery, as it predicts the\nbinding interactions between small-molecule ligands and protein pockets.\nHowever, current multi-task learning models for docking often show inferior\nperformance in ligand docking compared to protein pocket docking. This\ndisparity arises largely due to the distinct structural complexities of ligands\nand proteins. To address this issue, we propose a novel game-theoretic\nframework that models the protein-ligand interaction as a two-player game\ncalled the Docking Game, with the ligand docking module acting as the ligand\nplayer and the protein pocket docking module as the protein player. To solve\nthis game, we develop a novel Loop Self-Play (LoopPlay) algorithm, which\nalternately trains these players through a two-level loop. In the outer loop,\nthe players exchange predicted poses, allowing each to incorporate the other's\nstructural predictions, which fosters mutual adaptation over multiple\niterations. In the inner loop, each player dynamically refines its predictions\nby incorporating its own predicted ligand or pocket poses back into its model.\nWe theoretically show the convergence of LoopPlay, ensuring stable\noptimization. Extensive experiments conducted on public benchmark datasets\ndemonstrate that LoopPlay achieves approximately a 10\\% improvement in\npredicting accurate binding modes compared to previous state-of-the-art\nmethods. This highlights its potential to enhance the accuracy of molecular\ndocking in drug discovery.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u535a\u5f08\u8bba\u7684\u65b0\u578b\u5206\u5b50\u5bf9\u63a5\u6846\u67b6\uff08Docking Game\uff09\uff0c\u901a\u8fc7LoopPlay\u7b97\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u914d\u4f53\u5bf9\u63a5\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u5206\u5b50\u5bf9\u63a5\u6a21\u578b\u5728\u914d\u4f53\u5bf9\u63a5\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u7531\u4e8e\u914d\u4f53\u548c\u86cb\u767d\u8d28\u7684\u7ed3\u6784\u590d\u6742\u6027\u5dee\u5f02\u3002", "method": "\u5c06\u86cb\u767d-\u914d\u4f53\u4ea4\u4e92\u5efa\u6a21\u4e3a\u53cc\u73a9\u5bb6\u535a\u5f08\uff0c\u5f00\u53d1\u4e86LoopPlay\u7b97\u6cd5\uff0c\u901a\u8fc7\u5185\u5916\u5faa\u73af\u4ea4\u66ff\u8bad\u7ec3\u73a9\u5bb6\u6a21\u5757\u3002", "result": "\u5b9e\u9a8c\u663e\u793aLoopPlay\u5728\u9884\u6d4b\u7ed3\u5408\u6a21\u5f0f\u4e0a\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u5347\u7ea610%\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u671b\u63d0\u5347\u836f\u7269\u53d1\u73b0\u4e2d\u5206\u5b50\u5bf9\u63a5\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2508.04994", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.04994", "abs": "https://arxiv.org/abs/2508.04994", "authors": ["Wenjie Hu", "Ye Zhou", "Hann Woei Ho"], "title": "Hierarchical Deep Deterministic Policy Gradient for Autonomous Maze Navigation of Mobile Robots", "comment": null, "summary": "Maze navigation is a fundamental challenge in robotics, requiring agents to\ntraverse complex environments efficiently. While the Deep Deterministic Policy\nGradient (DDPG) algorithm excels in control tasks, its performance in maze\nnavigation suffers from sparse rewards, inefficient exploration, and\nlong-horizon planning difficulties, often leading to low success rates and\naverage rewards, sometimes even failing to achieve effective navigation. To\naddress these limitations, this paper proposes an efficient Hierarchical DDPG\n(HDDPG) algorithm, which includes high-level and low-level policies. The\nhigh-level policy employs an advanced DDPG framework to generate intermediate\nsubgoals from a long-term perspective and on a higher temporal scale. The\nlow-level policy, also powered by the improved DDPG algorithm, generates\nprimitive actions by observing current states and following the subgoal\nassigned by the high-level policy. The proposed method enhances stability with\noff-policy correction, refining subgoal assignments by relabeling historical\nexperiences. Additionally, adaptive parameter space noise is utilized to\nimprove exploration, and a reshaped intrinsic-extrinsic reward function is\nemployed to boost learning efficiency. Further optimizations, including\ngradient clipping and Xavier initialization, are employed to improve\nrobustness. The proposed algorithm is rigorously evaluated through numerical\nsimulation experiments executed using the Robot Operating System (ROS) and\nGazebo. Regarding the three distinct final targets in autonomous maze\nnavigation tasks, HDDPG significantly overcomes the limitations of standard\nDDPG and its variants, improving the success rate by at least 56.59% and\nboosting the average reward by a minimum of 519.03 compared to baseline\nalgorithms.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u5c42\u6b21\u5316DDPG\uff08HDDPG\uff09\u7b97\u6cd5\uff0c\u901a\u8fc7\u9ad8\u4f4e\u7ea7\u7b56\u7565\u7ed3\u5408\u3001\u6539\u8fdb\u63a2\u7d22\u548c\u5956\u52b1\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8ff7\u5bab\u5bfc\u822a\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u6807\u51c6DDPG\u7b97\u6cd5\u5728\u8ff7\u5bab\u5bfc\u822a\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5b58\u5728\u7a00\u758f\u5956\u52b1\u3001\u63a2\u7d22\u6548\u7387\u4f4e\u548c\u957f\u65f6\u89c4\u5212\u56f0\u96be\u7b49\u95ee\u9898\uff0c\u5bfc\u81f4\u6210\u529f\u7387\u4f4e\u548c\u5956\u52b1\u4e0d\u8db3\u3002", "method": "HDDPG\u91c7\u7528\u9ad8\u4f4e\u7ea7\u7b56\u7565\uff1a\u9ad8\u7ea7\u7b56\u7565\u751f\u6210\u957f\u671f\u5b50\u76ee\u6807\uff0c\u4f4e\u7ea7\u7b56\u7565\u6267\u884c\u5177\u4f53\u52a8\u4f5c\uff1b\u7ed3\u5408\u79bb\u7b56\u7565\u6821\u6b63\u3001\u81ea\u9002\u5e94\u53c2\u6570\u566a\u58f0\u548c\u4f18\u5316\u5956\u52b1\u51fd\u6570\u3002", "result": "\u5728ROS\u548cGazebo\u4eff\u771f\u5b9e\u9a8c\u4e2d\uff0cHDDPG\u76f8\u6bd4\u57fa\u7ebf\u7b97\u6cd5\uff0c\u6210\u529f\u7387\u63d0\u5347\u81f3\u5c1156.59%\uff0c\u5e73\u5747\u5956\u52b1\u589e\u52a0\u81f3\u5c11519.03\u3002", "conclusion": "HDDPG\u6709\u6548\u89e3\u51b3\u4e86\u6807\u51c6DDPG\u5728\u8ff7\u5bab\u5bfc\u822a\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u6027\u80fd\u3002"}}
{"id": "2508.05009", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.05009", "abs": "https://arxiv.org/abs/2508.05009", "authors": ["Bin Han", "Robert Wolfe", "Anat Caspi", "Bill Howe"], "title": "Can Large Language Models Integrate Spatial Data? Empirical Insights into Reasoning Strengths and Computational Weaknesses", "comment": null, "summary": "We explore the application of large language models (LLMs) to empower domain\nexperts in integrating large, heterogeneous, and noisy urban spatial datasets.\nTraditional rule-based integration methods are unable to cover all edge cases,\nrequiring manual verification and repair. Machine learning approaches require\ncollecting and labeling of large numbers of task-specific samples. In this\nstudy, we investigate the potential of LLMs for spatial data integration. Our\nanalysis first considers how LLMs reason about environmental spatial\nrelationships mediated by human experience, such as between roads and\nsidewalks. We show that while LLMs exhibit spatial reasoning capabilities, they\nstruggle to connect the macro-scale environment with the relevant computational\ngeometry tasks, often producing logically incoherent responses. But when\nprovided relevant features, thereby reducing dependence on spatial reasoning,\nLLMs are able to generate high-performing results. We then adapt a\nreview-and-refine method, which proves remarkably effective in correcting\nerroneous initial responses while preserving accurate responses. We discuss\npractical implications of employing LLMs for spatial data integration in\nreal-world contexts and outline future research directions, including\npost-training, multi-modal integration methods, and support for diverse data\nformats. Our findings position LLMs as a promising and flexible alternative to\ntraditional rule-based heuristics, advancing the capabilities of adaptive\nspatial data integration.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u6574\u5408\u57ce\u5e02\u7a7a\u95f4\u6570\u636e\u7684\u6f5c\u529b\uff0c\u53d1\u73b0\u5176\u5728\u63d0\u4f9b\u76f8\u5173\u7279\u5f81\u65f6\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\u6709\u9650\u3002", "motivation": "\u4f20\u7edf\u89c4\u5219\u65b9\u6cd5\u548c\u673a\u5668\u5b66\u4e60\u5728\u6574\u5408\u5f02\u6784\u3001\u566a\u58f0\u7a7a\u95f4\u6570\u636e\u65f6\u5b58\u5728\u4e0d\u8db3\uff0cLLMs\u53ef\u80fd\u63d0\u4f9b\u66f4\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5206\u6790LLMs\u7684\u7a7a\u95f4\u63a8\u7406\u80fd\u529b\uff0c\u63d0\u51fa\u57fa\u4e8e\u7279\u5f81\u589e\u5f3a\u548c\u201c\u5ba1\u67e5-\u4f18\u5316\u201d\u65b9\u6cd5\u7684\u6539\u8fdb\u7b56\u7565\u3002", "result": "LLMs\u5728\u51cf\u5c11\u7a7a\u95f4\u63a8\u7406\u4f9d\u8d56\u65f6\u8868\u73b0\u826f\u597d\uff0c\u4f46\u9700\u8f85\u52a9\u65b9\u6cd5\u4fee\u6b63\u521d\u59cb\u9519\u8bef\u3002", "conclusion": "LLMs\u662f\u4f20\u7edf\u89c4\u5219\u65b9\u6cd5\u7684\u6709\u6548\u66ff\u4ee3\uff0c\u672a\u6765\u53ef\u7ed3\u5408\u591a\u6a21\u6001\u548c\u591a\u6837\u5316\u6570\u636e\u652f\u6301\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2508.05021", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.05021", "abs": "https://arxiv.org/abs/2508.05021", "authors": ["Weifan Zhang", "Tingguang Li", "Yuzhen Liu"], "title": "MAG-Nav: Language-Driven Object Navigation Leveraging Memory-Reserved Active Grounding", "comment": null, "summary": "Visual navigation in unknown environments based solely on natural language\ndescriptions is a key capability for intelligent robots. In this work, we\npropose a navigation framework built upon off-the-shelf Visual Language Models\n(VLMs), enhanced with two human-inspired mechanisms: perspective-based active\ngrounding, which dynamically adjusts the robot's viewpoint for improved visual\ninspection, and historical memory backtracking, which enables the system to\nretain and re-evaluate uncertain observations over time. Unlike existing\napproaches that passively rely on incidental visual inputs, our method actively\noptimizes perception and leverages memory to resolve ambiguity, significantly\nimproving vision-language grounding in complex, unseen environments. Our\nframework operates in a zero-shot manner, achieving strong generalization to\ndiverse and open-ended language descriptions without requiring labeled data or\nmodel fine-tuning. Experimental results on Habitat-Matterport 3D (HM3D) show\nthat our method outperforms state-of-the-art approaches in language-driven\nobject navigation. We further demonstrate its practicality through real-world\ndeployment on a quadruped robot, achieving robust and effective navigation\nperformance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u5bfc\u822a\u6846\u67b6\uff0c\u901a\u8fc7\u4e3b\u52a8\u8c03\u6574\u89c6\u89d2\u548c\u5386\u53f2\u8bb0\u5fc6\u56de\u6eaf\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u672a\u77e5\u73af\u5883\u4e2d\u7684\u89c6\u89c9\u8bed\u8a00\u5b9a\u4f4d\u80fd\u529b\u3002", "motivation": "\u667a\u80fd\u673a\u5668\u4eba\u5728\u672a\u77e5\u73af\u5883\u4e2d\u4ec5\u4f9d\u9760\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u8fdb\u884c\u89c6\u89c9\u5bfc\u822a\u7684\u80fd\u529b\u662f\u5173\u952e\u9700\u6c42\u3002\u73b0\u6709\u65b9\u6cd5\u88ab\u52a8\u4f9d\u8d56\u89c6\u89c9\u8f93\u5165\uff0c\u65e0\u6cd5\u6709\u6548\u89e3\u51b3\u6a21\u7cca\u6027\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u89c6\u89d2\u4e3b\u52a8\u5b9a\u4f4d\u548c\u5386\u53f2\u8bb0\u5fc6\u56de\u6eaf\u673a\u5236\uff0c\u52a8\u6001\u4f18\u5316\u611f\u77e5\u5e76\u5229\u7528\u8bb0\u5fc6\u89e3\u51b3\u6a21\u7cca\u6027\uff0c\u65e0\u9700\u6807\u6ce8\u6570\u636e\u6216\u6a21\u578b\u5fae\u8c03\u3002", "result": "\u5728HM3D\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728\u56db\u8db3\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u4e86\u7a33\u5065\u7684\u5bfc\u822a\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u4e3b\u52a8\u611f\u77e5\u548c\u8bb0\u5fc6\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bed\u8a00\u9a71\u52a8\u5bfc\u822a\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2508.05081", "categories": ["cs.AI", "cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.05081", "abs": "https://arxiv.org/abs/2508.05081", "authors": ["Jiarun Liu", "Chunhong Zhang", "Zheng Hu"], "title": "Cognitive Duality for Adaptive Web Agents", "comment": null, "summary": "Web navigation represents a critical and challenging domain for evaluating\nartificial general intelligence (AGI), demanding complex decision-making within\nhigh-entropy, dynamic environments with combinatorially explosive action\nspaces. Current approaches to building autonomous web agents either focus on\noffline imitation learning or online exploration, but rarely integrate both\nparadigms effectively. Inspired by the dual-process theory of human cognition,\nwe derive a principled decomposition into fast System 1 and slow System 2\ncognitive processes. This decomposition provides a unifying perspective on\nexisting web agent methodologies, bridging the gap between offline learning of\nintuitive reactive behaviors and online acquisition of deliberative planning\ncapabilities. We implement this framework in CogniWeb, a modular agent\narchitecture that adaptively toggles between fast intuitive processing and\ndeliberate reasoning based on task complexity. Our evaluation on WebArena\ndemonstrates that CogniWeb achieves competitive performance (43.96% success\nrate) while maintaining significantly higher efficiency (75% reduction in token\nusage).", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u79bb\u7ebf\u6a21\u4eff\u5b66\u4e60\u548c\u5728\u7ebf\u63a2\u7d22\u7684Web\u5bfc\u822a\u667a\u80fd\u4f53\u6846\u67b6CogniWeb\uff0c\u901a\u8fc7\u53cc\u7cfb\u7edf\u8ba4\u77e5\u7406\u8bba\u5b9e\u73b0\u9ad8\u6548\u51b3\u7b56\u3002", "motivation": "Web\u5bfc\u822a\u662f\u8bc4\u4f30\u901a\u7528\u4eba\u5de5\u667a\u80fd\uff08AGI\uff09\u7684\u5173\u952e\u9886\u57df\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u6709\u6548\u6574\u5408\u79bb\u7ebf\u5b66\u4e60\u548c\u5728\u7ebf\u63a2\u7d22\u3002", "method": "\u57fa\u4e8e\u4eba\u7c7b\u8ba4\u77e5\u7684\u53cc\u7cfb\u7edf\u7406\u8bba\uff0c\u5c06\u667a\u80fd\u4f53\u884c\u4e3a\u5206\u89e3\u4e3a\u5feb\u901f\u76f4\u89c9\uff08System 1\uff09\u548c\u6162\u901f\u63a8\u7406\uff08System 2\uff09\uff0c\u5e76\u5b9e\u73b0\u4e3a\u6a21\u5757\u5316\u67b6\u6784CogniWeb\u3002", "result": "\u5728WebArena\u6d4b\u8bd5\u4e2d\uff0cCogniWeb\u53d6\u5f97\u4e8643.96%\u7684\u6210\u529f\u7387\uff0c\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u4e86\u6548\u7387\uff08\u4ee4\u724c\u4f7f\u7528\u51cf\u5c1175%\uff09\u3002", "conclusion": "CogniWeb\u901a\u8fc7\u53cc\u7cfb\u7edf\u7406\u8bba\u6709\u6548\u6574\u5408\u4e86\u79bb\u7ebf\u4e0e\u5728\u7ebf\u5b66\u4e60\uff0c\u4e3aWeb\u5bfc\u822a\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.05027", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.05027", "abs": "https://arxiv.org/abs/2508.05027", "authors": ["Philip Huang", "Yorai Shaoul", "Jiaoyang Li"], "title": "Benchmarking Shortcutting Techniques for Multi-Robot-Arm Motion Planning", "comment": "9 pages, 6 figures, accepted for publication at 2025 IEEE/RSJ\n  International Conference on Intelligent Robots and Systems (IROS 2025)", "summary": "Generating high-quality motion plans for multiple robot arms is challenging\ndue to the high dimensionality of the system and the potential for inter-arm\ncollisions. Traditional motion planning methods often produce motions that are\nsuboptimal in terms of smoothness and execution time for multi-arm systems.\nPost-processing via shortcutting is a common approach to improve motion quality\nfor efficient and smooth execution. However, in multi-arm scenarios, optimizing\none arm's motion must not introduce collisions with other arms. Although\nexisting multi-arm planning works often use some form of shortcutting\ntechniques, their exact methodology and impact on performance are often vaguely\ndescribed. In this work, we present a comprehensive study quantitatively\ncomparing existing shortcutting methods for multi-arm trajectories across\ndiverse simulated scenarios. We carefully analyze the pros and cons of each\nshortcutting method and propose two simple strategies for combining these\nmethods to achieve the best performance-runtime tradeoff. Video, code, and\ndataset are available at https://philip-huang.github.io/mr-shortcut/.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u591a\u673a\u68b0\u81c2\u8fd0\u52a8\u89c4\u5212\u4e2d\u7684\u6377\u5f84\u4f18\u5316\u65b9\u6cd5\uff0c\u6bd4\u8f83\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u79cd\u7ec4\u5408\u7b56\u7565\u4ee5\u5b9e\u73b0\u6700\u4f73\u6027\u80fd\u4e0e\u8fd0\u884c\u65f6\u95f4\u7684\u5e73\u8861\u3002", "motivation": "\u591a\u673a\u68b0\u81c2\u7cfb\u7edf\u7684\u9ad8\u7ef4\u5ea6\u548c\u6f5c\u5728\u7684\u78b0\u649e\u98ce\u9669\u4f7f\u5f97\u9ad8\u8d28\u91cf\u8fd0\u52a8\u89c4\u5212\u5177\u6709\u6311\u6218\u6027\uff0c\u4f20\u7edf\u65b9\u6cd5\u5e38\u4ea7\u751f\u6b21\u4f18\u8fd0\u52a8\uff0c\u73b0\u6709\u6377\u5f84\u4f18\u5316\u65b9\u6cd5\u7f3a\u4e4f\u8be6\u7ec6\u63cf\u8ff0\u548c\u6027\u80fd\u5206\u6790\u3002", "method": "\u901a\u8fc7\u5b9a\u91cf\u6bd4\u8f83\u73b0\u6709\u6377\u5f84\u4f18\u5316\u65b9\u6cd5\uff0c\u5206\u6790\u5176\u4f18\u7f3a\u70b9\uff0c\u5e76\u63d0\u51fa\u4e24\u79cd\u7ec4\u5408\u7b56\u7565\u3002", "result": "\u7814\u7a76\u63d0\u4f9b\u4e86\u5bf9\u4e0d\u540c\u6377\u5f84\u4f18\u5316\u65b9\u6cd5\u7684\u6027\u80fd\u8bc4\u4f30\uff0c\u5e76\u5c55\u793a\u4e86\u7ec4\u5408\u7b56\u7565\u5728\u6027\u80fd\u4e0e\u8fd0\u884c\u65f6\u95f4\u4e0a\u7684\u4f18\u52bf\u3002", "conclusion": "\u672c\u6587\u4e3a\u591a\u673a\u68b0\u81c2\u8fd0\u52a8\u89c4\u5212\u4e2d\u7684\u6377\u5f84\u4f18\u5316\u63d0\u4f9b\u4e86\u5168\u9762\u5206\u6790\u548c\u5b9e\u7528\u7b56\u7565\uff0c\u63d0\u5347\u4e86\u8fd0\u52a8\u8d28\u91cf\u548c\u6548\u7387\u3002"}}
{"id": "2508.05083", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05083", "abs": "https://arxiv.org/abs/2508.05083", "authors": ["Dexuan Xu", "Jieyi Wang", "Zhongyan Chai", "Yongzhi Cao", "Hanpin Wang", "Huamin Zhang", "Yu Huang"], "title": "MedMKEB: A Comprehensive Knowledge Editing Benchmark for Medical Multimodal Large Language Models", "comment": "18 pages", "summary": "Recent advances in multimodal large language models (MLLMs) have\nsignificantly improved medical AI, enabling it to unify the understanding of\nvisual and textual information. However, as medical knowledge continues to\nevolve, it is critical to allow these models to efficiently update outdated or\nincorrect information without retraining from scratch. Although textual\nknowledge editing has been widely studied, there is still a lack of systematic\nbenchmarks for multimodal medical knowledge editing involving image and text\nmodalities. To fill this gap, we present MedMKEB, the first comprehensive\nbenchmark designed to evaluate the reliability, generality, locality,\nportability, and robustness of knowledge editing in medical multimodal large\nlanguage models. MedMKEB is built on a high-quality medical visual\nquestion-answering dataset and enriched with carefully constructed editing\ntasks, including counterfactual correction, semantic generalization, knowledge\ntransfer, and adversarial robustness. We incorporate human expert validation to\nensure the accuracy and reliability of the benchmark. Extensive single editing\nand sequential editing experiments on state-of-the-art general and medical\nMLLMs demonstrate the limitations of existing knowledge-based editing\napproaches in medicine, highlighting the need to develop specialized editing\nstrategies. MedMKEB will serve as a standard benchmark to promote the\ndevelopment of trustworthy and efficient medical knowledge editing algorithms.", "AI": {"tldr": "MedMKEB\u662f\u9996\u4e2a\u9488\u5bf9\u533b\u5b66\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u77e5\u8bc6\u7f16\u8f91\u7684\u7efc\u5408\u6027\u57fa\u51c6\uff0c\u8bc4\u4f30\u53ef\u9760\u6027\u3001\u901a\u7528\u6027\u3001\u5c40\u90e8\u6027\u3001\u53ef\u79fb\u690d\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u533b\u5b66\u77e5\u8bc6\u4e0d\u65ad\u66f4\u65b0\uff0c\u73b0\u6709\u6a21\u578b\u9700\u9ad8\u6548\u4fee\u6b63\u8fc7\u65f6\u6216\u9519\u8bef\u4fe1\u606f\uff0c\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u6027\u591a\u6a21\u6001\u533b\u5b66\u77e5\u8bc6\u7f16\u8f91\u57fa\u51c6\u3002", "method": "\u57fa\u4e8e\u9ad8\u8d28\u91cf\u533b\u5b66\u89c6\u89c9\u95ee\u7b54\u6570\u636e\u96c6\uff0c\u6784\u5efa\u5305\u62ec\u53cd\u4e8b\u5b9e\u4fee\u6b63\u3001\u8bed\u4e49\u6cdb\u5316\u3001\u77e5\u8bc6\u8fc1\u79fb\u548c\u5bf9\u6297\u9c81\u68d2\u6027\u7b49\u7f16\u8f91\u4efb\u52a1\uff0c\u5e76\u5f15\u5165\u4e13\u5bb6\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u73b0\u6709\u77e5\u8bc6\u7f16\u8f91\u65b9\u6cd5\u5728\u533b\u5b66\u9886\u57df\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u5f00\u53d1\u4e13\u95e8\u7b56\u7565\u3002", "conclusion": "MedMKEB\u5c06\u4f5c\u4e3a\u6807\u51c6\u57fa\u51c6\uff0c\u63a8\u52a8\u53ef\u4fe1\u8d56\u4e14\u9ad8\u6548\u7684\u533b\u5b66\u77e5\u8bc6\u7f16\u8f91\u7b97\u6cd5\u53d1\u5c55\u3002"}}
{"id": "2508.05040", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.05040", "abs": "https://arxiv.org/abs/2508.05040", "authors": ["Boyang Zhang", "Jiahui Zuo", "Zeyu Duan", "Fumin Zhang"], "title": "A Vision-Based Collision Sensing Method for Stable Circular Object Grasping with A Soft Gripper System", "comment": null, "summary": "External collisions to robot actuators typically pose risks to grasping\ncircular objects. This work presents a vision-based sensing module capable of\ndetecting collisions to maintain stable grasping with a soft gripper system.\nThe system employs an eye-in-palm camera with a broad field of view to\nsimultaneously monitor the motion of fingers and the grasped object.\nFurthermore, we have developed a collision-rich grasping strategy to ensure the\nstability and security of the entire dynamic grasping process. A physical soft\ngripper was manufactured and affixed to a collaborative robotic arm to evaluate\nthe performance of the collision detection mechanism. An experiment regarding\ntesting the response time of the mechanism confirmed the system has the\ncapability to react to the collision instantaneously. A dodging test was\nconducted to demonstrate the gripper can detect the direction and scale of\nexternal collisions precisely.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u7684\u78b0\u649e\u68c0\u6d4b\u6a21\u5757\uff0c\u7528\u4e8e\u8f6f\u5939\u6301\u5668\u7cfb\u7edf\uff0c\u786e\u4fdd\u7a33\u5b9a\u6293\u53d6\u5706\u5f62\u7269\u4f53\u3002", "motivation": "\u5916\u90e8\u78b0\u649e\u5bf9\u673a\u5668\u4eba\u6267\u884c\u5668\u6784\u6210\u98ce\u9669\uff0c\u5c24\u5176\u662f\u6293\u53d6\u5706\u5f62\u7269\u4f53\u65f6\uff0c\u9700\u8981\u4e00\u79cd\u53ef\u9760\u7684\u78b0\u649e\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u773c\u5728\u624b\u76f8\u673a\u76d1\u6d4b\u624b\u6307\u548c\u88ab\u6293\u7269\u4f53\u7684\u8fd0\u52a8\uff0c\u5f00\u53d1\u4e86\u78b0\u649e\u4e30\u5bcc\u7684\u6293\u53d6\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u80fd\u5373\u65f6\u54cd\u5e94\u78b0\u649e\uff0c\u5e76\u80fd\u7cbe\u786e\u68c0\u6d4b\u78b0\u649e\u65b9\u5411\u548c\u5927\u5c0f\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u80fd\u6709\u6548\u63d0\u5347\u8f6f\u5939\u6301\u5668\u5728\u52a8\u6001\u6293\u53d6\u4e2d\u7684\u7a33\u5b9a\u6027\u548c\u5b89\u5168\u6027\u3002"}}
{"id": "2508.05113", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05113", "abs": "https://arxiv.org/abs/2508.05113", "authors": ["Xinyue Wu", "Fan Hu", "Shaik Jani Babu", "Yi Zhao", "Xinfei Guo"], "title": "EasySize: Elastic Analog Circuit Sizing via LLM-Guided Heuristic Search", "comment": null, "summary": "Analog circuit design is a time-consuming, experience-driven task in chip\ndevelopment. Despite advances in AI, developing universal, fast, and stable\ngate sizing methods for analog circuits remains a significant challenge. Recent\napproaches combine Large Language Models (LLMs) with heuristic search\ntechniques to enhance generalizability, but they often depend on large model\nsizes and lack portability across different technology nodes. To overcome these\nlimitations, we propose EasySize, the first lightweight gate sizing framework\nbased on a finetuned Qwen3-8B model, designed for universal applicability\nacross process nodes, design specifications, and circuit topologies. EasySize\nexploits the varying Ease of Attainability (EOA) of performance metrics to\ndynamically construct task-specific loss functions, enabling efficient\nheuristic search through global Differential Evolution (DE) and local Particle\nSwarm Optimization (PSO) within a feedback-enhanced flow. Although finetuned\nsolely on 350nm node data, EasySize achieves strong performance on 5\noperational amplifier (Op-Amp) netlists across 180nm, 45nm, and 22nm technology\nnodes without additional targeted training, and outperforms AutoCkt, a\nwidely-used Reinforcement Learning based sizing framework, on 86.67\\% of tasks\nwith more than 96.67\\% of simulation resources reduction. We argue that\nEasySize can significantly reduce the reliance on human expertise and\ncomputational resources in gate sizing, thereby accelerating and simplifying\nthe analog circuit design process. EasySize will be open-sourced at a later\ndate.", "AI": {"tldr": "EasySize\u662f\u4e00\u4e2a\u57fa\u4e8eQwen3-8B\u6a21\u578b\u7684\u8f7b\u91cf\u7ea7\u95e8\u5c3a\u5bf8\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u6784\u5efa\u4efb\u52a1\u7279\u5b9a\u635f\u5931\u51fd\u6570\u548c\u7ed3\u5408\u5168\u5c40\u4e0e\u5c40\u90e8\u4f18\u5316\u65b9\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u6a21\u62df\u7535\u8def\u8bbe\u8ba1\u4e2d\u5bf9\u4eba\u529b\u548c\u8ba1\u7b97\u8d44\u6e90\u7684\u4f9d\u8d56\u3002", "motivation": "\u6a21\u62df\u7535\u8def\u8bbe\u8ba1\u8017\u65f6\u4e14\u4f9d\u8d56\u7ecf\u9a8c\uff0c\u73b0\u6709AI\u65b9\u6cd5\u901a\u7528\u6027\u5dee\u4e14\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u5927\u3002EasySize\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u4f9b\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u901a\u7528\u7684\u95e8\u5c3a\u5bf8\u4f18\u5316\u65b9\u6848\u3002", "method": "EasySize\u57fa\u4e8e\u5fae\u8c03\u7684Qwen3-8B\u6a21\u578b\uff0c\u5229\u7528\u6027\u80fd\u6307\u6807\u7684\u6613\u8fbe\u6027\uff08EOA\uff09\u52a8\u6001\u6784\u5efa\u635f\u5931\u51fd\u6570\uff0c\u7ed3\u5408\u5168\u5c40\u5dee\u5206\u8fdb\u5316\uff08DE\uff09\u548c\u5c40\u90e8\u7c92\u5b50\u7fa4\u4f18\u5316\uff08PSO\uff09\u8fdb\u884c\u9ad8\u6548\u641c\u7d22\u3002", "result": "EasySize\u5728\u591a\u4e2a\u5de5\u827a\u8282\u70b9\uff08180nm\u300145nm\u300122nm\uff09\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8eAutoCkt\u6846\u67b6\uff0c\u8282\u7701\u4e8696.67%\u7684\u4eff\u771f\u8d44\u6e90\u3002", "conclusion": "EasySize\u663e\u8457\u51cf\u5c11\u4e86\u6a21\u62df\u7535\u8def\u8bbe\u8ba1\u4e2d\u5bf9\u4eba\u529b\u548c\u8ba1\u7b97\u8d44\u6e90\u7684\u4f9d\u8d56\uff0c\u6709\u671b\u52a0\u901f\u548c\u7b80\u5316\u8bbe\u8ba1\u6d41\u7a0b\u3002"}}
{"id": "2508.05104", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.05104", "abs": "https://arxiv.org/abs/2508.05104", "authors": ["Andrej L\u00fa\u010dny", "Matilde Antonj", "Carlo Mazzola", "Hana Horn\u00e1\u010dkov\u00e1", "Ana Fari\u0107", "Krist\u00edna Malinovsk\u00e1", "Michal Vavrecka", "Igor Farka\u0161"], "title": "Examining the legibility of humanoid robot arm movements in a pointing task", "comment": "Published at ICSR 2025", "summary": "Human--robot interaction requires robots whose actions are legible, allowing\nhumans to interpret, predict, and feel safe around them. This study\ninvestigates the legibility of humanoid robot arm movements in a pointing task,\naiming to understand how humans predict robot intentions from truncated\nmovements and bodily cues. We designed an experiment using the NICO humanoid\nrobot, where participants observed its arm movements towards targets on a\ntouchscreen. Robot cues varied across conditions: gaze, pointing, and pointing\nwith congruent or incongruent gaze. Arm trajectories were stopped at 60\\% or\n80\\% of their full length, and participants predicted the final target. We\ntested the multimodal superiority and ocular primacy hypotheses, both of which\nwere supported by the experiment.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4eba\u5f62\u673a\u5668\u4eba\u624b\u81c2\u52a8\u4f5c\u7684\u53ef\u8bfb\u6027\uff0c\u6d4b\u8bd5\u4eba\u7c7b\u5982\u4f55\u901a\u8fc7\u622a\u65ad\u52a8\u4f5c\u548c\u8eab\u4f53\u7ebf\u7d22\u9884\u6d4b\u673a\u5668\u4eba\u610f\u56fe\u3002\u5b9e\u9a8c\u652f\u6301\u591a\u6a21\u6001\u4f18\u8d8a\u6027\u548c\u89c6\u89c9\u4e3b\u5bfc\u5047\u8bf4\u3002", "motivation": "\u63d0\u5347\u4eba\u673a\u4ea4\u4e92\u4e2d\u673a\u5668\u4eba\u52a8\u4f5c\u7684\u53ef\u8bfb\u6027\uff0c\u4f7f\u4eba\u7c7b\u80fd\u66f4\u51c6\u786e\u9884\u6d4b\u673a\u5668\u4eba\u610f\u56fe\u5e76\u611f\u5230\u5b89\u5168\u3002", "method": "\u4f7f\u7528NICO\u4eba\u5f62\u673a\u5668\u4eba\u8fdb\u884c\u5b9e\u9a8c\uff0c\u53c2\u4e0e\u8005\u89c2\u5bdf\u5176\u624b\u81c2\u6307\u5411\u89e6\u6478\u5c4f\u76ee\u6807\u7684\u52a8\u4f5c\uff0c\u6d4b\u8bd5\u4e0d\u540c\u7ebf\u7d22\uff08\u6ce8\u89c6\u3001\u6307\u5411\u3001\u6ce8\u89c6\u4e0e\u6307\u5411\u4e00\u81f4/\u4e0d\u4e00\u81f4\uff09\u548c\u622a\u65ad\u8f68\u8ff9\uff0860%\u621680%\uff09\u5bf9\u9884\u6d4b\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u652f\u6301\u591a\u6a21\u6001\u4f18\u8d8a\u6027\u548c\u89c6\u89c9\u4e3b\u5bfc\u5047\u8bf4\uff0c\u8868\u660e\u4eba\u7c7b\u80fd\u901a\u8fc7\u8eab\u4f53\u7ebf\u7d22\u9884\u6d4b\u673a\u5668\u4eba\u610f\u56fe\u3002", "conclusion": "\u673a\u5668\u4eba\u52a8\u4f5c\u7684\u53ef\u8bfb\u6027\u53ef\u901a\u8fc7\u591a\u6a21\u6001\u7ebf\u7d22\uff08\u5982\u6ce8\u89c6\u548c\u6307\u5411\uff09\u63d0\u5347\uff0c\u89c6\u89c9\u7ebf\u7d22\u5728\u9884\u6d4b\u4e2d\u8d77\u4e3b\u5bfc\u4f5c\u7528\u3002"}}
{"id": "2508.05116", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.05116", "abs": "https://arxiv.org/abs/2508.05116", "authors": ["Peer-Benedikt Degen", "Igor Asanov"], "title": "Beyond Automation: Socratic AI, Epistemic Agency, and the Implications of the Emergence of Orchestrated Multi-Agent Learning Architectures", "comment": null, "summary": "Generative AI is no longer a peripheral tool in higher education. It is\nrapidly evolving into a general-purpose infrastructure that reshapes how\nknowledge is generated, mediated, and validated. This paper presents findings\nfrom a controlled experiment evaluating a Socratic AI Tutor, a large language\nmodel designed to scaffold student research question development through\nstructured dialogue grounded in constructivist theory. Conducted with 65\npre-service teacher students in Germany, the study compares interaction with\nthe Socratic Tutor to engagement with an uninstructed AI chatbot. Students\nusing the Socratic Tutor reported significantly greater support for critical,\nindependent, and reflective thinking, suggesting that dialogic AI can stimulate\nmetacognitive engagement and challenging recent narratives of de-skilling due\nto generative AI usage. These findings serve as a proof of concept for a\nbroader pedagogical shift: the use of multi-agent systems (MAS) composed of\nspecialised AI agents. To conceptualise this, we introduce the notion of\norchestrated MAS, modular, pedagogically aligned agent constellations, curated\nby educators, that support diverse learning trajectories through differentiated\nroles and coordinated interaction. To anchor this shift, we propose an adapted\noffer-and-use model, in which students appropriate instructional offers from\nthese agents. Beyond technical feasibility, we examine system-level\nimplications for higher education institutions and students, including funding\nnecessities, changes to faculty roles, curriculars, competencies and assessment\npractices. We conclude with a comparative cost-effectiveness analysis\nhighlighting the scalability of such systems. In sum, this study contributes\nboth empirical evidence and a conceptual roadmap for hybrid learning ecosystems\nthat embed human-AI co-agency and pedagogical alignment.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u751f\u6210\u5f0fAI\u5728\u9ad8\u7b49\u6559\u80b2\u4e2d\u7684\u6838\u5fc3\u4f5c\u7528\uff0c\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u82cf\u683c\u62c9\u5e95\u5f0fAI\u5bfc\u5e08\u5bf9\u5b66\u751f\u6279\u5224\u6027\u601d\u7ef4\u7684\u4fc3\u8fdb\u4f5c\u7528\uff0c\u5e76\u63d0\u51fa\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u6559\u80b2\u5e94\u7528\u6846\u67b6\u3002", "motivation": "\u751f\u6210\u5f0fAI\u6b63\u6210\u4e3a\u9ad8\u7b49\u6559\u80b2\u7684\u57fa\u7840\u8bbe\u65bd\uff0c\u4f46\u5bf9\u5176\u662f\u5426\u5bfc\u81f4\u5b66\u751f\u80fd\u529b\u9000\u5316\u7684\u62c5\u5fe7\u5b58\u5728\u3002\u7814\u7a76\u65e8\u5728\u9a8c\u8bc1AI\u80fd\u5426\u901a\u8fc7\u7ed3\u6784\u5316\u5bf9\u8bdd\u4fc3\u8fdb\u5b66\u751f\u6279\u5224\u6027\u601d\u7ef4\u3002", "method": "\u7814\u7a76\u91c7\u7528\u5bf9\u7167\u5b9e\u9a8c\uff0c65\u540d\u5fb7\u56fd\u5e08\u8303\u751f\u5206\u522b\u4e0e\u82cf\u683c\u62c9\u5e95\u5f0fAI\u5bfc\u5e08\u548c\u666e\u901aAI\u804a\u5929\u673a\u5668\u4eba\u4e92\u52a8\uff0c\u8bc4\u4f30\u5176\u5bf9\u6279\u5224\u6027\u601d\u7ef4\u7684\u652f\u6301\u6548\u679c\u3002", "result": "\u4f7f\u7528\u82cf\u683c\u62c9\u5e95\u5f0fAI\u5bfc\u5e08\u7684\u5b66\u751f\u5728\u6279\u5224\u6027\u3001\u72ec\u7acb\u6027\u548c\u53cd\u601d\u6027\u601d\u7ef4\u65b9\u9762\u8868\u73b0\u663e\u8457\u66f4\u597d\uff0c\u8bc1\u660eAI\u5bf9\u8bdd\u80fd\u4fc3\u8fdb\u5143\u8ba4\u77e5\u53c2\u4e0e\u3002", "conclusion": "\u7814\u7a76\u63d0\u51fa\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08MAS\uff09\u7684\u6559\u80b2\u5e94\u7528\u6846\u67b6\uff0c\u5f3a\u8c03AI\u4e0e\u4eba\u7c7b\u534f\u4f5c\u7684\u6df7\u5408\u5b66\u4e60\u751f\u6001\u7cfb\u7edf\uff0c\u5e76\u5206\u6790\u5176\u6210\u672c\u6548\u76ca\u548c\u5236\u5ea6\u5f71\u54cd\u3002"}}
{"id": "2508.05143", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.05143", "abs": "https://arxiv.org/abs/2508.05143", "authors": ["Sim\u00e9on Capy", "Thomas M. Kwok", "Kevin Joseph", "Yuichiro Kawasumi", "Koichi Nagashima", "Tomoya Sasaki", "Yue Hu", "Eiichi Yoshida"], "title": "From Canada to Japan: How 10,000 km Affect User Perception in Robot Teleoperation", "comment": "Author preprint - Accepted for Humanoids 2025", "summary": "Robot teleoperation (RTo) has emerged as a viable alternative to local\ncontrol, particularly when human intervention is still necessary. This research\naims to study the distance effect on user perception in RTo, exploring the\npotential of teleoperated robots for older adult care. We propose an evaluation\nof non-expert users' perception of long-distance RTo, examining how their\nperception changes before and after interaction, as well as comparing it to\nthat of locally operated robots. We have designed a specific protocol\nconsisting of multiple questionnaires, along with a dedicated software\narchitecture using the Robotics Operating System (ROS) and Unity. The results\nrevealed no statistically significant differences between the local and remote\nrobot conditions, suggesting that robots may be a viable alternative to\ntraditional local control.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u8fdc\u7a0b\u673a\u5668\u4eba\u64cd\u4f5c\uff08RTo\uff09\u5bf9\u7528\u6237\u611f\u77e5\u7684\u5f71\u54cd\uff0c\u5c24\u5176\u662f\u5728\u8001\u5e74\u62a4\u7406\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b0\u8fdc\u7a0b\u4e0e\u672c\u5730\u64cd\u4f5c\u5728\u611f\u77e5\u4e0a\u65e0\u663e\u8457\u5dee\u5f02\u3002", "motivation": "\u63a2\u7d22\u8fdc\u7a0b\u673a\u5668\u4eba\u64cd\u4f5c\u5728\u8001\u5e74\u62a4\u7406\u4e2d\u7684\u6f5c\u529b\uff0c\u7814\u7a76\u8ddd\u79bb\u5bf9\u975e\u4e13\u5bb6\u7528\u6237\u611f\u77e5\u7684\u5f71\u54cd\u3002", "method": "\u8bbe\u8ba1\u4e86\u5305\u542b\u95ee\u5377\u8c03\u67e5\u7684\u534f\u8bae\uff0c\u5e76\u4f7f\u7528ROS\u548cUnity\u5f00\u53d1\u8f6f\u4ef6\u67b6\u6784\uff0c\u6bd4\u8f83\u8fdc\u7a0b\u4e0e\u672c\u5730\u64cd\u4f5c\u7684\u7528\u6237\u611f\u77e5\u3002", "result": "\u8fdc\u7a0b\u4e0e\u672c\u5730\u673a\u5668\u4eba\u64cd\u4f5c\u5728\u7528\u6237\u611f\u77e5\u4e0a\u65e0\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "\u8fdc\u7a0b\u673a\u5668\u4eba\u64cd\u4f5c\u53ef\u80fd\u662f\u4f20\u7edf\u672c\u5730\u63a7\u5236\u7684\u6709\u6548\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2508.05145", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05145", "abs": "https://arxiv.org/abs/2508.05145", "authors": ["Sebastiano Dissegna", "Chiara Di Francescomarino", "Massimiliano Ronzani"], "title": "Graph-based Event Log Repair", "comment": null, "summary": "The quality of event logs in Process Mining is crucial when applying any form\nof analysis to them. In real-world event logs, the acquisition of data can be\nnon-trivial (e.g., due to the execution of manual activities and related manual\nrecording or to issues in collecting, for each event, all its attributes), and\noften may end up with events recorded with some missing information. Standard\napproaches to the problem of trace (or log) reconstruction either require the\navailability of a process model that is used to fill missing values by\nleveraging different reasoning techniques or employ a Machine Learning/Deep\nLearning model to restore the missing values by learning from similar cases. In\nrecent years, a new type of Deep Learning model that is capable of handling\ninput data encoded as graphs has emerged, namely Graph Neural Networks. Graph\nNeural Network models, and even more so Heterogeneous Graph Neural Networks,\noffer the advantage of working with a more natural representation of complex\nmulti-modal sequences like the execution traces in Process Mining, allowing for\nmore expressive and semantically rich encodings.\n  In this work, we focus on the development of a Heterogeneous Graph Neural\nNetwork model that, given a trace containing some incomplete events, will\nreturn the full set of attributes missing from those events. We evaluate our\nwork against a state-of-the-art approach leveraging autoencoders on two\nsynthetic logs and four real event logs, on different types of missing values.\nDifferent from state-of-the-art model-free approaches, which mainly focus on\nrepairing a subset of event attributes, the proposed approach shows very good\nperformance in reconstructing all different event attributes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f02\u6784\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u6a21\u578b\uff0c\u7528\u4e8e\u4fee\u590d\u8fc7\u7a0b\u6316\u6398\u4e2d\u4e8b\u4ef6\u65e5\u5fd7\u7684\u7f3a\u5931\u5c5e\u6027\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u4e8b\u4ef6\u65e5\u5fd7\u5e38\u56e0\u6570\u636e\u91c7\u96c6\u95ee\u9898\u5bfc\u81f4\u4fe1\u606f\u7f3a\u5931\uff0c\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u8fc7\u7a0b\u6a21\u578b\u6216\u673a\u5668\u5b66\u4e60\uff0c\u800c\u56fe\u795e\u7ecf\u7f51\u7edc\u80fd\u66f4\u81ea\u7136\u5730\u5904\u7406\u590d\u6742\u591a\u6a21\u6001\u5e8f\u5217\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u5f02\u6784\u56fe\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u7528\u4e8e\u4fee\u590d\u4e8b\u4ef6\u65e5\u5fd7\u4e2d\u7684\u7f3a\u5931\u5c5e\u6027\uff0c\u5e76\u5728\u5408\u6210\u548c\u771f\u5b9e\u65e5\u5fd7\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4fee\u590d\u6240\u6709\u4e8b\u4ef6\u5c5e\u6027\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u65e0\u6a21\u578b\u65b9\u6cd5\u3002", "conclusion": "\u5f02\u6784\u56fe\u795e\u7ecf\u7f51\u7edc\u4e3a\u4e8b\u4ef6\u65e5\u5fd7\u4fee\u590d\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u548c\u5168\u9762\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.05148", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05148", "abs": "https://arxiv.org/abs/2508.05148", "authors": ["Francisco Munguia-Galeano", "Zhengxue Zhou", "Satheeshkumar Veeramani", "Hatem Fakhruldeen", "Louis Longley", "Rob Clowes", "Andrew I. Cooper"], "title": "Chemist Eye: A Visual Language Model-Powered System for Safety Monitoring and Robot Decision-Making in Self-Driving Laboratories", "comment": null, "summary": "The integration of robotics and automation into self-driving laboratories\n(SDLs) can introduce additional safety complexities, in addition to those that\nalready apply to conventional research laboratories. Personal protective\nequipment (PPE) is an essential requirement for ensuring the safety and\nwell-being of workers in laboratories, self-driving or otherwise. Fires are\nanother important risk factor in chemical laboratories. In SDLs, fires that\noccur close to mobile robots, which use flammable lithium batteries, could have\nincreased severity. Here, we present Chemist Eye, a distributed safety\nmonitoring system designed to enhance situational awareness in SDLs. The system\nintegrates multiple stations equipped with RGB, depth, and infrared cameras,\ndesigned to monitor incidents in SDLs. Chemist Eye is also designed to spot\nworkers who have suffered a potential accident or medical emergency, PPE\ncompliance and fire hazards. To do this, Chemist Eye uses decision-making\ndriven by a vision-language model (VLM). Chemist Eye is designed for seamless\nintegration, enabling real-time communication with robots. Based on the VLM\nrecommendations, the system attempts to drive mobile robots away from potential\nfire locations, exits, or individuals not wearing PPE, and issues audible\nwarnings where necessary. It also integrates with third-party messaging\nplatforms to provide instant notifications to lab personnel. We tested Chemist\nEye with real-world data from an SDL equipped with three mobile robots and\nfound that the spotting of possible safety hazards and decision-making\nperformances reached 97 % and 95 %, respectively.", "AI": {"tldr": "Chemist Eye\u662f\u4e00\u4e2a\u5206\u5e03\u5f0f\u5b89\u5168\u76d1\u63a7\u7cfb\u7edf\uff0c\u7528\u4e8e\u589e\u5f3a\u81ea\u9a71\u52a8\u5b9e\u9a8c\u5ba4\uff08SDL\uff09\u7684\u5b89\u5168\u610f\u8bc6\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u6444\u50cf\u5934\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5b9e\u65f6\u76d1\u6d4b\u5371\u9669\u5e76\u6307\u5bfc\u673a\u5668\u4eba\u884c\u52a8\u3002", "motivation": "\u81ea\u9a71\u52a8\u5b9e\u9a8c\u5ba4\u4e2d\uff0c\u673a\u5668\u4eba\u4f7f\u7528\u6613\u71c3\u9502\u7535\u6c60\u589e\u52a0\u4e86\u706b\u707e\u98ce\u9669\uff0c\u540c\u65f6\u4e2a\u4eba\u9632\u62a4\u88c5\u5907\uff08PPE\uff09\u7684\u5408\u89c4\u6027\u4e5f\u9700\u76d1\u63a7\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u7684\u5b89\u5168\u7cfb\u7edf\u3002", "method": "\u7cfb\u7edf\u6574\u5408RGB\u3001\u6df1\u5ea6\u548c\u7ea2\u5916\u6444\u50cf\u5934\uff0c\u5229\u7528VLM\u8fdb\u884c\u51b3\u7b56\uff0c\u5b9e\u65f6\u76d1\u6d4b\u5371\u9669\u5e76\u6307\u5bfc\u673a\u5668\u4eba\u8fdc\u79bb\u5371\u9669\u533a\u57df\uff0c\u540c\u65f6\u901a\u8fc7\u7b2c\u4e09\u65b9\u5e73\u53f0\u901a\u77e5\u4eba\u5458\u3002", "result": "\u5728\u771f\u5b9eSDL\u73af\u5883\u4e2d\u6d4b\u8bd5\uff0cChemist Eye\u5bf9\u5b89\u5168\u5371\u9669\u7684\u8bc6\u522b\u548c\u51b3\u7b56\u6027\u80fd\u5206\u522b\u8fbe\u523097%\u548c95%\u3002", "conclusion": "Chemist Eye\u80fd\u6709\u6548\u63d0\u5347SDL\u7684\u5b89\u5168\u6027\uff0c\u51cf\u5c11\u706b\u707e\u548cPPE\u8fdd\u89c4\u98ce\u9669\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.05197", "categories": ["cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05197", "abs": "https://arxiv.org/abs/2508.05197", "authors": ["Zhuohang Jiang", "Pangjing Wu", "Xu Yuan", "Wenqi Fan", "Qing Li"], "title": "QA-Dragon: Query-Aware Dynamic RAG System for Knowledge-Intensive Visual Question Answering", "comment": "The source code for our system is released in\n  https://github.com/jzzzzh/QA-Dragon", "summary": "Retrieval-Augmented Generation (RAG) has been introduced to mitigate\nhallucinations in Multimodal Large Language Models (MLLMs) by incorporating\nexternal knowledge into the generation process, and it has become a widely\nadopted approach for knowledge-intensive Visual Question Answering (VQA).\nHowever, existing RAG methods typically retrieve from either text or images in\nisolation, limiting their ability to address complex queries that require\nmulti-hop reasoning or up-to-date factual knowledge. To address this\nlimitation, we propose QA-Dragon, a Query-Aware Dynamic RAG System for\nKnowledge-Intensive VQA. Specifically, QA-Dragon introduces a domain router to\nidentify the query's subject domain for domain-specific reasoning, along with a\nsearch router that dynamically selects optimal retrieval strategies. By\norchestrating both text and image search agents in a hybrid setup, our system\nsupports multimodal, multi-turn, and multi-hop reasoning, enabling it to tackle\ncomplex VQA tasks effectively. We evaluate our QA-Dragon on the Meta CRAG-MM\nChallenge at KDD Cup 2025, where it significantly enhances the reasoning\nperformance of base models under challenging scenarios. Our framework achieves\nsubstantial improvements in both answer accuracy and knowledge overlap scores,\noutperforming baselines by 5.06% on the single-source task, 6.35% on the\nmulti-source task, and 5.03% on the multi-turn task.", "AI": {"tldr": "QA-Dragon\u662f\u4e00\u79cd\u52a8\u6001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7cfb\u7edf\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u548c\u591a\u8df3\u63a8\u7406\u63d0\u5347\u590d\u6742\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709RAG\u65b9\u6cd5\u4ec5\u4ece\u6587\u672c\u6216\u56fe\u50cf\u4e2d\u68c0\u7d22\uff0c\u65e0\u6cd5\u5904\u7406\u9700\u8981\u591a\u8df3\u63a8\u7406\u6216\u6700\u65b0\u77e5\u8bc6\u7684\u590d\u6742\u67e5\u8be2\u3002", "method": "QA-Dragon\u5f15\u5165\u9886\u57df\u8def\u7531\u5668\u548c\u641c\u7d22\u8def\u7531\u5668\uff0c\u52a8\u6001\u9009\u62e9\u68c0\u7d22\u7b56\u7565\uff0c\u652f\u6301\u591a\u6a21\u6001\u3001\u591a\u8f6e\u548c\u591a\u8df3\u63a8\u7406\u3002", "result": "\u5728KDD Cup 2025\u7684Meta CRAG-MM\u6311\u6218\u4e2d\uff0cQA-Dragon\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u5355\u6e90\u3001\u591a\u6e90\u548c\u591a\u8f6e\u4efb\u52a1\u5206\u522b\u4f18\u4e8e\u57fa\u7ebf5.06%\u30016.35%\u548c5.03%\u3002", "conclusion": "QA-Dragon\u901a\u8fc7\u52a8\u6001\u68c0\u7d22\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u590d\u6742VQA\u4efb\u52a1\u4e2d\u7684\u77e5\u8bc6\u83b7\u53d6\u548c\u63a8\u7406\u95ee\u9898\u3002"}}
{"id": "2508.05153", "categories": ["cs.RO", "cs.AI", "I.2.9; I.2.6; I.4.8"], "pdf": "https://arxiv.org/pdf/2508.05153", "abs": "https://arxiv.org/abs/2508.05153", "authors": ["Mohammed Daba", "Jing Qiu"], "title": "FCBV-Net: Category-Level Robotic Garment Smoothing via Feature-Conditioned Bimanual Value Prediction", "comment": "7 pages, 3 figures, 1 table. Submitted to IEEE Robotics and\n  Automation Letters", "summary": "Category-level generalization for robotic garment manipulation, such as\nbimanual smoothing, remains a significant hurdle due to high dimensionality,\ncomplex dynamics, and intra-category variations. Current approaches often\nstruggle, either overfitting with concurrently learned visual features for a\nspecific instance or, despite category-level perceptual generalization, failing\nto predict the value of synergistic bimanual actions. We propose the\nFeature-Conditioned Bimanual Value Network (FCBV-Net), operating on 3D point\nclouds to specifically enhance category-level policy generalization for garment\nsmoothing. FCBV-Net conditions bimanual action value prediction on pre-trained,\nfrozen dense geometric features, ensuring robustness to intra-category garment\nvariations. Trainable downstream components then learn a task-specific policy\nusing these static features. In simulated GarmentLab experiments with the\nCLOTH3D dataset, FCBV-Net demonstrated superior category-level generalization.\nIt exhibited only an 11.5% efficiency drop (Steps80) on unseen garments\ncompared to 96.2% for a 2D image-based baseline, and achieved 89% final\ncoverage, outperforming an 83% coverage from a 3D correspondence-based baseline\nthat uses identical per-point geometric features but a fixed primitive. These\nresults highlight that the decoupling of geometric understanding from bimanual\naction value learning enables better category-level generalization.", "AI": {"tldr": "FCBV-Net\u901a\u8fc7\u9884\u8bad\u7ec3\u7684\u51e0\u4f55\u7279\u5f81\u589e\u5f3a\u7c7b\u522b\u7ea7\u7b56\u7565\u6cdb\u5316\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u673a\u5668\u4eba\u670d\u88c5\u5e73\u6ed1\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u7c7b\u522b\u7ea7\u673a\u5668\u4eba\u670d\u88c5\u64cd\u4f5c\u4e2d\u7684\u9ad8\u7ef4\u5ea6\u548c\u52a8\u6001\u590d\u6742\u6027\uff0c\u907f\u514d\u73b0\u6709\u65b9\u6cd5\u7684\u8fc7\u62df\u5408\u6216\u6cdb\u5316\u4e0d\u8db3\u95ee\u9898\u3002", "method": "\u63d0\u51faFCBV-Net\uff0c\u5229\u75283D\u70b9\u4e91\u548c\u51bb\u7ed3\u7684\u51e0\u4f55\u7279\u5f81\uff0c\u5206\u79bb\u51e0\u4f55\u7406\u89e3\u4e0e\u52a8\u4f5c\u4ef7\u503c\u5b66\u4e60\uff0c\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5728\u6a21\u62df\u5b9e\u9a8c\u4e2d\uff0cFCBV-Net\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6548\u7387\u4e0b\u964d\u4ec511.5%\uff0c\u6700\u7ec8\u8986\u76d6\u7387\u8fbe89%\u3002", "conclusion": "\u51e0\u4f55\u7279\u5f81\u4e0e\u52a8\u4f5c\u4ef7\u503c\u5b66\u4e60\u7684\u89e3\u8026\u662f\u5b9e\u73b0\u7c7b\u522b\u7ea7\u6cdb\u5316\u7684\u5173\u952e\u3002"}}
{"id": "2508.05267", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05267", "abs": "https://arxiv.org/abs/2508.05267", "authors": ["V\u00edtor N. Louren\u00e7o", "Mohnish Dubey", "Yunfei Bai", "Audrey Depeige", "Vivek Jain"], "title": "An Explainable Natural Language Framework for Identifying and Notifying Target Audiences In Enterprise Communication", "comment": "Accepted to publication at the 24th International Semantic Web\n  Conference Industry Track, ISWC 2025", "summary": "In large-scale maintenance organizations, identifying subject matter experts\nand managing communications across complex entities relationships poses\nsignificant challenges -- including information overload and longer response\ntimes -- that traditional communication approaches fail to address effectively.\nWe propose a novel framework that combines RDF graph databases with LLMs to\nprocess natural language queries for precise audience targeting, while\nproviding transparent reasoning through a planning-orchestration architecture.\nOur solution enables communication owners to formulate intuitive queries\ncombining concepts such as equipment, manufacturers, maintenance engineers, and\nfacilities, delivering explainable results that maintain trust in the system\nwhile improving communication efficiency across the organization.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408RDF\u56fe\u6570\u636e\u5e93\u548cLLM\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u7cbe\u51c6\u76ee\u6807\u53d7\u4f17\u5b9a\u4f4d\u548c\u900f\u660e\u63a8\u7406\uff0c\u4ee5\u89e3\u51b3\u5927\u89c4\u6a21\u7ef4\u62a4\u7ec4\u7ec7\u4e2d\u7684\u901a\u4fe1\u7ba1\u7406\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u901a\u4fe1\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u5e94\u5bf9\u4fe1\u606f\u8fc7\u8f7d\u548c\u54cd\u5e94\u65f6\u95f4\u957f\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u7684\u5b9e\u4f53\u5173\u7cfb\u4e2d\u8bc6\u522b\u4e13\u5bb6\u548c\u7ba1\u7406\u901a\u4fe1\u3002", "method": "\u91c7\u7528RDF\u56fe\u6570\u636e\u5e93\u4e0eLLM\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u5b9e\u73b0\u7cbe\u51c6\u53d7\u4f17\u5b9a\u4f4d\uff0c\u5e76\u901a\u8fc7\u89c4\u5212-\u7f16\u6392\u67b6\u6784\u63d0\u4f9b\u900f\u660e\u63a8\u7406\u3002", "result": "\u89e3\u51b3\u65b9\u6848\u5141\u8bb8\u901a\u4fe1\u6240\u6709\u8005\u901a\u8fc7\u76f4\u89c2\u67e5\u8be2\u7ec4\u5408\u591a\u79cd\u6982\u5ff5\uff0c\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u7ed3\u679c\uff0c\u63d0\u9ad8\u7ec4\u7ec7\u901a\u4fe1\u6548\u7387\u5e76\u4fdd\u6301\u7cfb\u7edf\u4fe1\u4efb\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u7ef4\u62a4\u7ec4\u7ec7\u4e2d\u7684\u901a\u4fe1\u6311\u6218\uff0c\u540c\u65f6\u63d0\u5347\u4e86\u900f\u660e\u5ea6\u548c\u6548\u7387\u3002"}}
{"id": "2508.05186", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05186", "abs": "https://arxiv.org/abs/2508.05186", "authors": ["Yongjie Bai", "Zhouxia Wang", "Yang Liu", "Weixing Chen", "Ziliang Chen", "Mingtong Dai", "Yongsen Zheng", "Lingbo Liu", "Guanbin Li", "Liang Lin"], "title": "Learning to See and Act: Task-Aware View Planning for Robotic Manipulation", "comment": "7 pages, 9 figures, project page: https://hcplab-sysu.github.io/TAVP", "summary": "Recent vision-language-action (VLA) models for multi-task robotic\nmanipulation commonly rely on static viewpoints and shared visual encoders,\nwhich limit 3D perception and cause task interference, hindering robustness and\ngeneralization. In this work, we propose Task-Aware View Planning (TAVP), a\nframework designed to overcome these challenges by integrating active view\nplanning with task-specific representation learning. TAVP employs an efficient\nexploration policy, accelerated by a novel pseudo-environment, to actively\nacquire informative views. Furthermore, we introduce a Mixture-of-Experts (MoE)\nvisual encoder to disentangle features across different tasks, boosting both\nrepresentation fidelity and task generalization. By learning to see the world\nin a task-aware way, TAVP generates more complete and discriminative visual\nrepresentations, demonstrating significantly enhanced action prediction across\na wide array of manipulation challenges. Extensive experiments on RLBench tasks\nshow that our proposed TAVP model achieves superior performance over\nstate-of-the-art fixed-view approaches. Visual results and code are provided\nat: https://hcplab-sysu.github.io/TAVP.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4efb\u52a1\u611f\u77e5\u89c6\u56fe\u89c4\u5212\uff08TAVP\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u4e3b\u52a8\u89c6\u56fe\u89c4\u5212\u548c\u4efb\u52a1\u7279\u5b9a\u8868\u793a\u5b66\u4e60\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u57283D\u611f\u77e5\u548c\u4efb\u52a1\u5e72\u6270\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709VLA\u6a21\u578b\u4f9d\u8d56\u9759\u6001\u89c6\u89d2\u548c\u5171\u4eab\u89c6\u89c9\u7f16\u7801\u5668\uff0c\u9650\u5236\u4e863D\u611f\u77e5\u5e76\u5bfc\u81f4\u4efb\u52a1\u5e72\u6270\uff0c\u5f71\u54cd\u4e86\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "TAVP\u7ed3\u5408\u4e3b\u52a8\u89c6\u56fe\u89c4\u5212\u548c\u4efb\u52a1\u7279\u5b9a\u8868\u793a\u5b66\u4e60\uff0c\u91c7\u7528\u9ad8\u6548\u63a2\u7d22\u7b56\u7565\u548cMixture-of-Experts\uff08MoE\uff09\u89c6\u89c9\u7f16\u7801\u5668\uff0c\u4ee5\u89e3\u8026\u4e0d\u540c\u4efb\u52a1\u7684\u7279\u5f81\u3002", "result": "\u5728RLBench\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTAVP\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u56fa\u5b9a\u89c6\u89d2\u65b9\u6cd5\uff0c\u751f\u6210\u4e86\u66f4\u5b8c\u6574\u548c\u533a\u5206\u6027\u7684\u89c6\u89c9\u8868\u793a\u3002", "conclusion": "TAVP\u901a\u8fc7\u4efb\u52a1\u611f\u77e5\u7684\u89c6\u89c9\u8868\u793a\u5b66\u4e60\uff0c\u63d0\u5347\u4e86\u591a\u4efb\u52a1\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.05311", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.05311", "abs": "https://arxiv.org/abs/2508.05311", "authors": ["Andrew Kiruluta"], "title": "A Novel Architecture for Symbolic Reasoning with Decision Trees and LLM Agents", "comment": null, "summary": "We propose a hybrid architecture that integrates decision tree-based symbolic\nreasoning with the generative capabilities of large language models (LLMs)\nwithin a coordinated multi-agent framework. Unlike prior approaches that\nloosely couple symbolic and neural modules, our design embeds decision trees\nand random forests as callable oracles within a unified reasoning system.\nTree-based modules enable interpretable rule inference and causal logic, while\nLLM agents handle abductive reasoning, generalization, and interactive\nplanning. A central orchestrator maintains belief state consistency and\nmediates communication across agents and external tools, enabling reasoning\nover both structured and unstructured inputs.\n  The system achieves strong performance on reasoning benchmarks. On\n\\textit{ProofWriter}, it improves entailment consistency by +7.2\\% through\nlogic-grounded tree validation. On GSM8k, it achieves +5.3\\% accuracy gains in\nmultistep mathematical problems via symbolic augmentation. On \\textit{ARC}, it\nboosts abstraction accuracy by +6.0\\% through integration of symbolic oracles.\nApplications in clinical decision support and scientific discovery show how the\nsystem encodes domain rules symbolically while leveraging LLMs for contextual\ninference and hypothesis generation. This architecture offers a robust,\ninterpretable, and extensible solution for general-purpose neuro-symbolic\nreasoning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u67b6\u6784\uff0c\u5c06\u51b3\u7b56\u6811\u7b26\u53f7\u63a8\u7406\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u80fd\u529b\u7ed3\u5408\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u6846\u67b6\u5b9e\u73b0\u534f\u8c03\u63a8\u7406\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u677e\u6563\u7ed3\u5408\u7b26\u53f7\u4e0e\u795e\u7ecf\u6a21\u5757\uff0c\u7f3a\u4e4f\u7edf\u4e00\u6027\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5d4c\u5165\u51b3\u7b56\u6811\u548c\u968f\u673a\u68ee\u6797\u4f5c\u4e3a\u53ef\u8c03\u7528\u9884\u8a00\u673a\uff0c\u6784\u5efa\u66f4\u7d27\u5bc6\u7684\u63a8\u7406\u7cfb\u7edf\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u4e2d\u592e\u534f\u8c03\u5668\uff0c\u7ef4\u62a4\u4fe1\u5ff5\u72b6\u6001\u4e00\u81f4\u6027\uff0c\u5e76\u534f\u8c03\u7b26\u53f7\u6a21\u5757\uff08\u51b3\u7b56\u6811\uff09\u4e0eLLM\u667a\u80fd\u4f53\u7684\u4ea4\u4e92\uff0c\u652f\u6301\u7ed3\u6784\u5316\u4e0e\u975e\u7ed3\u6784\u5316\u8f93\u5165\u63a8\u7406\u3002", "result": "\u5728ProofWriter\u3001GSM8k\u548cARC\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5206\u522b\u63d0\u53477.2%\u30015.3%\u548c6.0%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u8be5\u67b6\u6784\u4e3a\u901a\u7528\u795e\u7ecf\u7b26\u53f7\u63a8\u7406\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u4e34\u5e8a\u51b3\u7b56\u548c\u79d1\u5b66\u53d1\u73b0\u7b49\u9886\u57df\u3002"}}
{"id": "2508.05208", "categories": ["cs.RO", "cs.PF"], "pdf": "https://arxiv.org/pdf/2508.05208", "abs": "https://arxiv.org/abs/2508.05208", "authors": ["Victor Ngo", "Rachel", "Ramchurn", "Roma Patel", "Alan Chamberlain", "Ayse Kucukyilmaz"], "title": "Dancing with a Robot: An Experimental Study of Child-Robot Interaction in a Performative Art Setting", "comment": "published by Springer", "summary": "This paper presents an evaluation of 18 children's in-the-wild experiences\nwith the autonomous robot arm performer NED (Never-Ending Dancer) within the\nThingamabobas installation, showcased across the UK. We detail NED's design,\nincluding costume, behaviour, and human interactions, all integral to the\ninstallation. Our observational analysis revealed three key challenges in\nchild-robot interactions: 1) Initiating and maintaining engagement, 2) Lack of\nrobot expressivity and reciprocity, and 3) Unmet expectations. Our findings\nshow that children are naturally curious, and adept at interacting with a\nrobotic art performer. However, our observations emphasise the critical need to\noptimise human-robot interaction (HRI) systems through careful consideration of\naudience's capabilities, perceptions, and expectations, within the performative\narts context, to enable engaging and meaningful experiences, especially for\nyoung audiences.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e8618\u540d\u513f\u7ae5\u4e0e\u81ea\u4e3b\u673a\u5668\u4eba\u624b\u81c2NED\u7684\u4e92\u52a8\u4f53\u9a8c\uff0c\u63ed\u793a\u4e86\u513f\u7ae5\u4e0e\u673a\u5668\u4eba\u4e92\u52a8\u7684\u4e09\u5927\u6311\u6218\uff0c\u5e76\u5f3a\u8c03\u4e86\u4f18\u5316\u4eba\u673a\u4ea4\u4e92\u7cfb\u7edf\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u7814\u7a76\u513f\u7ae5\u4e0e\u673a\u5668\u4ebaNED\u5728\u827a\u672f\u8868\u6f14\u4e2d\u7684\u4e92\u52a8\u4f53\u9a8c\uff0c\u4ee5\u4e86\u89e3\u5982\u4f55\u63d0\u5347\u4eba\u673a\u4ea4\u4e92\u7684\u5438\u5f15\u529b\u548c\u610f\u4e49\u3002", "method": "\u901a\u8fc7\u89c2\u5bdf\u5206\u6790\u513f\u7ae5\u4e0eNED\u7684\u4e92\u52a8\uff0c\u603b\u7ed3\u51fa\u4e92\u52a8\u4e2d\u7684\u5173\u952e\u6311\u6218\u3002", "result": "\u53d1\u73b0\u513f\u7ae5\u5bf9\u673a\u5668\u4eba\u8868\u6f14\u8005\u8868\u73b0\u51fa\u81ea\u7136\u597d\u5947\u5fc3\uff0c\u4f46\u4e92\u52a8\u4e2d\u5b58\u5728\u4e09\u5927\u6311\u6218\uff1a1) \u542f\u52a8\u548c\u7ef4\u6301\u4e92\u52a8\u56f0\u96be\uff0c2) \u673a\u5668\u4eba\u8868\u8fbe\u6027\u548c\u4e92\u60e0\u6027\u4e0d\u8db3\uff0c3) \u671f\u671b\u672a\u6ee1\u8db3\u3002", "conclusion": "\u4f18\u5316\u4eba\u673a\u4ea4\u4e92\u7cfb\u7edf\u9700\u8003\u8651\u89c2\u4f17\u7684\u80fd\u529b\u3001\u611f\u77e5\u548c\u671f\u671b\uff0c\u5c24\u5176\u662f\u5728\u827a\u672f\u8868\u6f14\u80cc\u666f\u4e0b\uff0c\u4ee5\u521b\u9020\u66f4\u6709\u5438\u5f15\u529b\u7684\u4f53\u9a8c\u3002"}}
{"id": "2508.05338", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.05338", "abs": "https://arxiv.org/abs/2508.05338", "authors": ["Brinnae Bent"], "title": "The Term 'Agent' Has Been Diluted Beyond Utility and Requires Redefinition", "comment": "Accepted to AIES 2025", "summary": "The term 'agent' in artificial intelligence has long carried multiple\ninterpretations across different subfields. Recent developments in AI\ncapabilities, particularly in large language model systems, have amplified this\nambiguity, creating significant challenges in research communication, system\nevaluation and reproducibility, and policy development. This paper argues that\nthe term 'agent' requires redefinition. Drawing from historical analysis and\ncontemporary usage patterns, we propose a framework that defines clear minimum\nrequirements for a system to be considered an agent while characterizing\nsystems along a multidimensional spectrum of environmental interaction,\nlearning and adaptation, autonomy, goal complexity, and temporal coherence.\nThis approach provides precise vocabulary for system description while\npreserving the term's historically multifaceted nature. After examining\npotential counterarguments and implementation challenges, we provide specific\nrecommendations for moving forward as a field, including suggestions for\nterminology standardization and framework adoption. The proposed approach\noffers practical tools for improving research clarity and reproducibility while\nsupporting more effective policy development.", "AI": {"tldr": "\u8bba\u6587\u4e3b\u5f20\u91cd\u65b0\u5b9a\u4e49AI\u4e2d\u7684'agent'\u4e00\u8bcd\uff0c\u63d0\u51fa\u591a\u7ef4\u6846\u67b6\u4ee5\u660e\u786e\u5176\u6700\u4f4e\u8981\u6c42\uff0c\u5e76\u5efa\u8bae\u6807\u51c6\u5316\u672f\u8bed\u4ee5\u63d0\u5347\u7814\u7a76\u6e05\u6670\u5ea6\u548c\u653f\u7b56\u5236\u5b9a\u6548\u679c\u3002", "motivation": "AI\u4e2d'agent'\u4e00\u8bcd\u7684\u591a\u4e49\u6027\u5bfc\u81f4\u7814\u7a76\u4ea4\u6d41\u3001\u7cfb\u7edf\u8bc4\u4f30\u548c\u653f\u7b56\u5236\u5b9a\u7684\u56f0\u96be\uff0c\u9700\u91cd\u65b0\u5b9a\u4e49\u4ee5\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5386\u53f2\u5206\u6790\u548c\u5f53\u4ee3\u4f7f\u7528\u6a21\u5f0f\uff0c\u63d0\u51fa\u591a\u7ef4\u6846\u67b6\u5b9a\u4e49'agent'\u7684\u6700\u4f4e\u8981\u6c42\uff0c\u6db5\u76d6\u73af\u5883\u4ea4\u4e92\u3001\u5b66\u4e60\u4e0e\u9002\u5e94\u3001\u81ea\u4e3b\u6027\u3001\u76ee\u6807\u590d\u6742\u6027\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u3002", "result": "\u6846\u67b6\u4e3a\u7cfb\u7edf\u63cf\u8ff0\u63d0\u4f9b\u7cbe\u786e\u8bcd\u6c47\uff0c\u540c\u65f6\u4fdd\u7559\u672f\u8bed\u7684\u591a\u9762\u6027\uff0c\u5e76\u63d0\u51fa\u672f\u8bed\u6807\u51c6\u5316\u548c\u6846\u67b6\u91c7\u7eb3\u7684\u5177\u4f53\u5efa\u8bae\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u52a9\u4e8e\u63d0\u5347\u7814\u7a76\u6e05\u6670\u5ea6\u548c\u53ef\u91cd\u590d\u6027\uff0c\u652f\u6301\u66f4\u6709\u6548\u7684\u653f\u7b56\u5236\u5b9a\u3002"}}
{"id": "2508.05294", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.05294", "abs": "https://arxiv.org/abs/2508.05294", "authors": ["Sahar Salimpour", "Lei Fu", "Farhad Keramat", "Leonardo Militano", "Giovanni Toffetti", "Harry Edelman", "Jorge Pe\u00f1a Queralta"], "title": "Towards Embodied Agentic AI: Review and Classification of LLM- and VLM-Driven Robot Autonomy and Interaction", "comment": null, "summary": "Foundation models, including large language models (LLMs) and vision-language\nmodels (VLMs), have recently enabled novel approaches to robot autonomy and\nhuman-robot interfaces. In parallel, vision-language-action models (VLAs) or\nlarge behavior models (BLMs) are increasing the dexterity and capabilities of\nrobotic systems. This survey paper focuses on those words advancing towards\nagentic applications and architectures. This includes initial efforts exploring\nGPT-style interfaces to tooling, as well as more complex system where AI agents\nare coordinators, planners, perception actors, or generalist interfaces. Such\nagentic architectures allow robots to reason over natural language\ninstructions, invoke APIs, plan task sequences, or assist in operations and\ndiagnostics. In addition to peer-reviewed research, due to the fast-evolving\nnature of the field, we highlight and include community-driven projects, ROS\npackages, and industrial frameworks that show emerging trends. We propose a\ntaxonomy for classifying model integration approaches and present a comparative\nanalysis of the role that agents play in different solutions in today's\nliterature.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u57fa\u7840\u6a21\u578b\uff08\u5982LLMs\u548cVLMs\uff09\u5728\u673a\u5668\u4eba\u81ea\u4e3b\u6027\u548c\u4eba\u673a\u4ea4\u4e92\u4e2d\u7684\u5e94\u7528\uff0c\u63a2\u8ba8\u4e86VLAs\u548cBLMs\u5982\u4f55\u63d0\u5347\u673a\u5668\u4eba\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u4e86\u5206\u7c7b\u6a21\u578b\u96c6\u6210\u65b9\u6cd5\u7684\u5206\u7c7b\u6cd5\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u63a2\u7d22\u57fa\u7840\u6a21\u578b\u5982\u4f55\u63a8\u52a8\u673a\u5668\u4eba\u6280\u672f\u7684\u8fdb\u6b65\uff0c\u7279\u522b\u662f\u5728\u4ee3\u7406\u5f0f\u5e94\u7528\u548c\u67b6\u6784\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u7efc\u8ff0\u73b0\u6709\u7814\u7a76\u3001\u793e\u533a\u9879\u76ee\u548c\u5de5\u4e1a\u6846\u67b6\uff0c\u63d0\u51fa\u5206\u7c7b\u6cd5\uff0c\u5e76\u5bf9\u4e0d\u540c\u89e3\u51b3\u65b9\u6848\u4e2d\u4ee3\u7406\u7684\u89d2\u8272\u8fdb\u884c\u6bd4\u8f83\u5206\u6790\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u4ee3\u7406\u5f0f\u67b6\u6784\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u63a8\u7406\u3001\u8c03\u7528API\u3001\u89c4\u5212\u4efb\u52a1\u5e8f\u5217\u7b49\uff0c\u5c55\u793a\u4e86\u57fa\u7840\u6a21\u578b\u7684\u5e7f\u6cdb\u5e94\u7528\u524d\u666f\u3002", "conclusion": "\u7ed3\u8bba\u5f3a\u8c03\u57fa\u7840\u6a21\u578b\u5728\u673a\u5668\u4eba\u9886\u57df\u7684\u5feb\u901f\u53d1\u5c55\u548c\u591a\u6837\u5316\u5e94\u7528\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u7684\u6f5c\u5728\u65b9\u5411\u3002"}}
{"id": "2508.05344", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05344", "abs": "https://arxiv.org/abs/2508.05344", "authors": ["Asutosh Hota", "Jussi P. P. Jokinen"], "title": "NomicLaw: Emergent Trust and Strategic Argumentation in LLMs During Collaborative Law-Making", "comment": null, "summary": "Recent advancements in large language models (LLMs) have extended their\ncapabilities from basic text processing to complex reasoning tasks, including\nlegal interpretation, argumentation, and strategic interaction. However,\nempirical understanding of LLM behavior in open-ended, multi-agent settings\nespecially those involving deliberation over legal and ethical dilemmas remains\nlimited. We introduce NomicLaw, a structured multi-agent simulation where LLMs\nengage in collaborative law-making, responding to complex legal vignettes by\nproposing rules, justifying them, and voting on peer proposals. We\nquantitatively measure trust and reciprocity via voting patterns and\nqualitatively assess how agents use strategic language to justify proposals and\ninfluence outcomes. Experiments involving homogeneous and heterogeneous LLM\ngroups demonstrate how agents spontaneously form alliances, betray trust, and\nadapt their rhetoric to shape collective decisions. Our results highlight the\nlatent social reasoning and persuasive capabilities of ten open-source LLMs and\nprovide insights into the design of future AI systems capable of autonomous\nnegotiation, coordination and drafting legislation in legal settings.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86NomicLaw\uff0c\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6a21\u62df\u7cfb\u7edf\uff0c\u7528\u4e8e\u7814\u7a76LLMs\u5728\u6cd5\u5f8b\u548c\u4f26\u7406\u56f0\u5883\u4e2d\u7684\u534f\u4f5c\u7acb\u6cd5\u884c\u4e3a\uff0c\u63ed\u793a\u4e86LLMs\u7684\u793e\u4f1a\u63a8\u7406\u548c\u8bf4\u670d\u80fd\u529b\u3002", "motivation": "\u7814\u7a76LLMs\u5728\u5f00\u653e\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e2d\u5904\u7406\u6cd5\u5f8b\u548c\u4f26\u7406\u95ee\u9898\u7684\u884c\u4e3a\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u901a\u8fc7NomicLaw\u6a21\u62df\uff0cLLMs\u534f\u4f5c\u5236\u5b9a\u6cd5\u5f8b\u89c4\u5219\uff0c\u63d0\u51fa\u3001\u8bba\u8bc1\u5e76\u6295\u7968\u8868\u51b3\u63d0\u6848\uff0c\u5b9a\u91cf\u5206\u6790\u4fe1\u4efb\u4e0e\u4e92\u60e0\uff0c\u5b9a\u6027\u8bc4\u4f30\u7b56\u7565\u8bed\u8a00\u4f7f\u7528\u3002", "result": "\u5b9e\u9a8c\u663e\u793aLLMs\u80fd\u81ea\u53d1\u5f62\u6210\u8054\u76df\u3001\u80cc\u53db\u4fe1\u4efb\u5e76\u8c03\u6574\u4fee\u8f9e\u4ee5\u5f71\u54cd\u51b3\u7b56\uff0c\u5c55\u793a\u4e86\u5176\u793e\u4f1a\u63a8\u7406\u548c\u8bf4\u670d\u80fd\u529b\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86LLMs\u5728\u81ea\u4e3b\u534f\u5546\u548c\u7acb\u6cd5\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u672a\u6765AI\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u53c2\u8003\u3002"}}
{"id": "2508.05298", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.05298", "abs": "https://arxiv.org/abs/2508.05298", "authors": ["Jian Gong", "Youwei Huang", "Bo Yuan", "Ming Zhu", "Juncheng Zhan", "Jinke Wang", "Hang Shu", "Mingyue Xiong", "Yanjun Ye", "Yufan Zu", "Yang Zhou", "Yihan Ding", "Xuannian Chen", "Xingyu Lu", "Runjie Ban", "Bingchao Huang", "Fusen Liu"], "title": "GhostShell: Streaming LLM Function Calls for Concurrent Embodied Programming", "comment": "17 pages, 5 figures, conference", "summary": "We present GhostShell, a novel approach that leverages Large Language Models\n(LLMs) to enable streaming and concurrent behavioral programming for embodied\nsystems. In contrast to conventional methods that rely on pre-scheduled action\nsequences or behavior trees, GhostShell drives embodied systems to act\non-the-fly by issuing function calls incrementally as tokens are streamed from\nthe LLM. GhostShell features a streaming XML function token parser, a dynamic\nfunction interface mapper, and a multi-channel scheduler that orchestrates\nintra-channel synchronous and inter-channel asynchronous function calls,\nthereby coordinating serial-parallel embodied actions across multiple robotic\ncomponents as directed by the LLM. We evaluate GhostShell on our robot\nprototype COCO through comprehensive grounded experiments across 34 real-world\ninteraction tasks and multiple LLMs. The results demonstrate that our approach\nachieves state-of-the-art Behavioral Correctness Metric of 0.85 with Claude-4\nSonnet and up to 66X faster response times compared to LLM native function\ncalling APIs. GhostShell also proves effective in long-horizon multimodal\ntasks, demonstrating strong robustness and generalization.", "AI": {"tldr": "GhostShell\u5229\u7528LLMs\u5b9e\u73b0\u5b9e\u65f6\u884c\u4e3a\u7f16\u7a0b\uff0c\u901a\u8fc7\u52a8\u6001\u51fd\u6570\u8c03\u7528\u548c\u8c03\u5ea6\u5668\u534f\u8c03\u591a\u673a\u5668\u4eba\u7ec4\u4ef6\uff0c\u663e\u8457\u63d0\u5347\u54cd\u5e94\u901f\u5ea6\u548c\u884c\u4e3a\u6b63\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u9884\u5b9a\u4e49\u884c\u4e3a\u5e8f\u5217\u6216\u884c\u4e3a\u6811\uff0c\u7f3a\u4e4f\u7075\u6d3b\u6027\u3002GhostShell\u65e8\u5728\u901a\u8fc7LLMs\u5b9e\u73b0\u5b9e\u65f6\u3001\u5e76\u53d1\u7684\u884c\u4e3a\u7f16\u7a0b\u3002", "method": "\u7ed3\u5408\u6d41\u5f0fXML\u51fd\u6570\u89e3\u6790\u5668\u3001\u52a8\u6001\u51fd\u6570\u63a5\u53e3\u6620\u5c04\u5668\u548c\u591a\u901a\u9053\u8c03\u5ea6\u5668\uff0c\u652f\u6301\u540c\u6b65\u548c\u5f02\u6b65\u51fd\u6570\u8c03\u7528\u3002", "result": "\u572834\u4e2a\u5b9e\u9645\u4efb\u52a1\u4e2d\uff0cGhostShell\u7684\u884c\u4e3a\u6b63\u786e\u6027\u8fbe0.85\uff0c\u54cd\u5e94\u901f\u5ea6\u63d0\u534766\u500d\uff0c\u4e14\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u8868\u73b0\u7a33\u5065\u3002", "conclusion": "GhostShell\u4e3a\u5b9e\u65f6\u884c\u4e3a\u7f16\u7a0b\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u590d\u6742\u673a\u5668\u4eba\u7cfb\u7edf\u3002"}}
{"id": "2508.05350", "categories": ["cs.AI", "cs.CC", "cs.LO"], "pdf": "https://arxiv.org/pdf/2508.05350", "abs": "https://arxiv.org/abs/2508.05350", "authors": ["Federica Di Stefano", "Quentin Mani\u00e8re", "Magdalena Ortiz", "Mantas \u0160imkus"], "title": "Minimal Model Reasoning in Description Logics: Don't Try This at Home!", "comment": "44 pages", "summary": "Reasoning with minimal models has always been at the core of many knowledge\nrepresentation techniques, but we still have only a limited understanding of\nthis problem in Description Logics (DLs). Minimization of some selected\npredicates, letting the remaining predicates vary or be fixed, as proposed in\ncircumscription, has been explored and exhibits high complexity. The case of\n`pure' minimal models, where the extension of all predicates must be minimal,\nhas remained largely uncharted. We address this problem in popular DLs and\nobtain surprisingly negative results: concept satisfiability in minimal models\nis undecidable already for $\\mathcal{EL}$. This undecidability also extends to\na very restricted fragment of tuple-generating dependencies. To regain\ndecidability, we impose acyclicity conditions on the TBox that bring the\nworst-case complexity below double exponential time and allow us to establish a\nconnection with the recently studied pointwise circumscription; we also derive\nresults in data complexity. We conclude with a brief excursion to the DL-Lite\nfamily, where a positive result was known for DL-Lite$_{\\text{core}}$, but our\ninvestigation establishes ExpSpace-hardness already for its extension\nDL-Lite$_{\\text{horn}}$.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.05342", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05342", "abs": "https://arxiv.org/abs/2508.05342", "authors": ["Shunlei Li", "Longsen Gao", "Jin Wang", "Chang Che", "Xi Xiao", "Jiuwen Cao", "Yingbai Hu", "Hamid Reza Karimi"], "title": "Information-Theoretic Graph Fusion with Vision-Language-Action Model for Policy Reasoning and Dual Robotic Control", "comment": "Journal under review", "summary": "Teaching robots dexterous skills from human videos remains challenging due to\nthe reliance on low-level trajectory imitation, which fails to generalize\nacross object types, spatial layouts, and manipulator configurations. We\npropose Graph-Fused Vision-Language-Action (GF-VLA), a framework that enables\ndual-arm robotic systems to perform task-level reasoning and execution directly\nfrom RGB and Depth human demonstrations. GF-VLA first extracts\nShannon-information-based cues to identify hands and objects with the highest\ntask relevance, then encodes these cues into temporally ordered scene graphs\nthat capture both hand-object and object-object interactions. These graphs are\nfused with a language-conditioned transformer that generates hierarchical\nbehavior trees and interpretable Cartesian motion commands. To improve\nexecution efficiency in bimanual settings, we further introduce a cross-hand\nselection policy that infers optimal gripper assignment without explicit\ngeometric reasoning. We evaluate GF-VLA on four structured dual-arm block\nassembly tasks involving symbolic shape construction and spatial\ngeneralization. Experimental results show that the information-theoretic scene\nrepresentation achieves over 95 percent graph accuracy and 93 percent subtask\nsegmentation, supporting the LLM planner in generating reliable and\nhuman-readable task policies. When executed by the dual-arm robot, these\npolicies yield 94 percent grasp success, 89 percent placement accuracy, and 90\npercent overall task success across stacking, letter-building, and geometric\nreconfiguration scenarios, demonstrating strong generalization and robustness\nacross diverse spatial and semantic variations.", "AI": {"tldr": "GF-VLA\u6846\u67b6\u901a\u8fc7\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u878d\u5408\uff0c\u4f7f\u53cc\u81c2\u673a\u5668\u4eba\u80fd\u591f\u4ece\u4eba\u7c7b\u6f14\u793a\u4e2d\u76f4\u63a5\u8fdb\u884c\u4efb\u52a1\u7ea7\u63a8\u7406\u548c\u6267\u884c\uff0c\u663e\u8457\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u548c\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u4f4e\u7ea7\u522b\u8f68\u8ff9\u6a21\u4eff\u65b9\u6cd5\u5728\u7269\u4f53\u7c7b\u578b\u3001\u7a7a\u95f4\u5e03\u5c40\u548c\u673a\u68b0\u81c2\u914d\u7f6e\u4e0a\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u53d6\u4efb\u52a1\u76f8\u5173\u7684\u624b\u548c\u7269\u4f53\u4fe1\u606f\uff0c\u7f16\u7801\u4e3a\u65f6\u5e8f\u573a\u666f\u56fe\uff0c\u7ed3\u5408\u8bed\u8a00\u6761\u4ef6\u53d8\u6362\u5668\u751f\u6210\u884c\u4e3a\u6811\u548c\u8fd0\u52a8\u6307\u4ee4\uff0c\u5e76\u5f15\u5165\u8de8\u624b\u9009\u62e9\u7b56\u7565\u4f18\u5316\u6267\u884c\u6548\u7387\u3002", "result": "\u5728\u53cc\u81c2\u79ef\u6728\u7ec4\u88c5\u4efb\u52a1\u4e2d\uff0c\u56fe\u8868\u793a\u51c6\u786e\u7387\u8d8595%\uff0c\u5b50\u4efb\u52a1\u5206\u5272\u8fbe93%\uff0c\u4efb\u52a1\u6210\u529f\u738790%\uff0c\u8868\u73b0\u51fa\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "GF-VLA\u6846\u67b6\u901a\u8fc7\u4fe1\u606f\u8bba\u548c\u8bed\u8a00\u878d\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u4ece\u4eba\u7c7b\u6f14\u793a\u4e2d\u5b66\u4e60\u590d\u6742\u4efb\u52a1\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.05383", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05383", "abs": "https://arxiv.org/abs/2508.05383", "authors": ["Xiangxiang Zhang", "Jingxuan Wei", "Donghong Zhong", "Qi Chen", "Caijun Jia", "Cheng Tan", "Jinming Gu", "Xiaobo Qin", "Zhiping Liu", "Liang Hu", "Tong Sun", "Yuchen Wu", "Zewei Sun", "Chenwei Lou", "Hua Zheng", "Tianyang Zhan", "Changbao Wang", "Shuangzhi Wu", "Zefa Lin", "Chang Guo", "Sihang Yuan", "Riwei Chen", "Shixiong Zhao", "Yingping Zhang", "Gaowei Wu", "Bihui Yu", "Jiahui Wu", "Zhehui Zhao", "Qianqian Liu", "Ruofeng Tang", "Xingyue Huang", "Bing Zhao", "Mengyang Zhang", "Youqiang Zhou"], "title": "StructVRM: Aligning Multimodal Reasoning with Structured and Verifiable Reward Models", "comment": null, "summary": "Existing Vision-Language Models often struggle with complex, multi-question\nreasoning tasks where partial correctness is crucial for effective learning.\nTraditional reward mechanisms, which provide a single binary score for an\nentire response, are too coarse to guide models through intricate problems with\nmultiple sub-parts. To address this, we introduce StructVRM, a method that\naligns multimodal reasoning with Structured and Verifiable Reward Models. At\nits core is a model-based verifier trained to provide fine-grained,\nsub-question-level feedback, assessing semantic and mathematical equivalence\nrather than relying on rigid string matching. This allows for nuanced, partial\ncredit scoring in previously intractable problem formats. Extensive experiments\ndemonstrate the effectiveness of StructVRM. Our trained model, Seed-StructVRM,\nachieves state-of-the-art performance on six out of twelve public multimodal\nbenchmarks and our newly curated, high-difficulty STEM-Bench. The success of\nStructVRM validates that training with structured, verifiable rewards is a\nhighly effective approach for advancing the capabilities of multimodal models\nin complex, real-world reasoning domains.", "AI": {"tldr": "StructVRM\u901a\u8fc7\u7ed3\u6784\u5316\u53ef\u9a8c\u8bc1\u5956\u52b1\u6a21\u578b\u63d0\u5347\u591a\u6a21\u6001\u6a21\u578b\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u4f20\u7edf\u5956\u52b1\u673a\u5236\u5bf9\u590d\u6742\u591a\u95ee\u9898\u63a8\u7406\u4efb\u52a1\u7684\u652f\u6301\u4e0d\u8db3\uff0c\u65e0\u6cd5\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u53cd\u9988\u3002", "method": "\u63d0\u51faStructVRM\u65b9\u6cd5\uff0c\u4f7f\u7528\u6a21\u578b\u9a8c\u8bc1\u5668\u63d0\u4f9b\u5b50\u95ee\u9898\u7ea7\u522b\u7684\u7ec6\u7c92\u5ea6\u53cd\u9988\uff0c\u8bc4\u4f30\u8bed\u4e49\u548c\u6570\u5b66\u7b49\u4ef7\u6027\u3002", "result": "Seed-StructVRM\u572812\u4e2a\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u4e2d6\u4e2a\u8868\u73b0\u6700\u4f73\uff0c\u5e76\u5728\u65b0STEM-Bench\u4e0a\u53d6\u5f97\u4f18\u5f02\u7ed3\u679c\u3002", "conclusion": "\u7ed3\u6784\u5316\u53ef\u9a8c\u8bc1\u5956\u52b1\u8bad\u7ec3\u662f\u63d0\u5347\u591a\u6a21\u6001\u6a21\u578b\u590d\u6742\u63a8\u7406\u80fd\u529b\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2508.05359", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.05359", "abs": "https://arxiv.org/abs/2508.05359", "authors": ["Morten Roed Frederiksen", "Kasper St\u00f8y"], "title": "Affecta-Context: The Context-Guided Behavior Adaptation Framework", "comment": "6 pages, Intelligent Autonomous Systems 18. IAS 2023", "summary": "This paper presents Affecta-context, a general framework to facilitate\nbehavior adaptation for social robots. The framework uses information about the\nphysical context to guide its behaviors in human-robot interactions. It\nconsists of two parts: one that represents encountered contexts and one that\nlearns to prioritize between behaviors through human-robot interactions. As\nphysical contexts are encountered the framework clusters them by their measured\nphysical properties. In each context, the framework learns to prioritize\nbetween behaviors to optimize the physical attributes of the robot's behavior\nin line with its current environment and the preferences of the users it\ninteracts with. This paper illlustrates the abilities of the Affecta-context\nframework by enabling a robot to autonomously learn the prioritization of\ndiscrete behaviors. This was achieved by training across 72 interactions in two\ndifferent physical contexts with 6 different human test participants. The paper\ndemonstrates the trained Affecta-context framework by verifying the robot's\nability to generalize over the input and to match its behaviors to a previously\nunvisited physical context.", "AI": {"tldr": "Affecta-context\u6846\u67b6\u901a\u8fc7\u7269\u7406\u4e0a\u4e0b\u6587\u4fe1\u606f\u4f18\u5316\u793e\u4ea4\u673a\u5668\u4eba\u884c\u4e3a\uff0c\u5305\u62ec\u4e0a\u4e0b\u6587\u8868\u793a\u548c\u884c\u4e3a\u4f18\u5148\u7ea7\u5b66\u4e60\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u5176\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u63d0\u5347\u793e\u4ea4\u673a\u5668\u4eba\u5728\u4e0d\u540c\u7269\u7406\u73af\u5883\u4e2d\u7684\u884c\u4e3a\u9002\u5e94\u6027\uff0c\u4ee5\u6ee1\u8db3\u7528\u6237\u504f\u597d\u548c\u73af\u5883\u9700\u6c42\u3002", "method": "\u6846\u67b6\u5206\u4e3a\u4e24\u90e8\u5206\uff1a\u4e0a\u4e0b\u6587\u8868\u793a\u548c\u884c\u4e3a\u4f18\u5148\u7ea7\u5b66\u4e60\uff0c\u901a\u8fc7\u805a\u7c7b\u7269\u7406\u5c5e\u6027\u548c\u4ea4\u4e92\u8bad\u7ec3\u4f18\u5316\u884c\u4e3a\u3002", "result": "\u572872\u6b21\u4ea4\u4e92\u5b9e\u9a8c\u4e2d\uff0c\u673a\u5668\u4eba\u6210\u529f\u5b66\u4e60\u884c\u4e3a\u4f18\u5148\u7ea7\uff0c\u5e76\u80fd\u6cdb\u5316\u5230\u672a\u89c1\u7684\u7269\u7406\u4e0a\u4e0b\u6587\u3002", "conclusion": "Affecta-context\u6846\u67b6\u6709\u6548\u63d0\u5347\u673a\u5668\u4eba\u884c\u4e3a\u9002\u5e94\u6027\uff0c\u5177\u5907\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.05388", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05388", "abs": "https://arxiv.org/abs/2508.05388", "authors": ["Silvia Garc\u00eda-M\u00e9ndez", "Francisco de Arriba-P\u00e9rez", "F\u00e1tima Leal", "Bruno Veloso", "Benedita Malheiro", "Juan Carlos Burguillo-Rial"], "title": "An Explainable Machine Learning Framework for Railway Predictive Maintenance using Data Streams from the Metro Operator of Portugal", "comment": null, "summary": "This work contributes to a real-time data-driven predictive maintenance\nsolution for Intelligent Transportation Systems. The proposed method implements\na processing pipeline comprised of sample pre-processing, incremental\nclassification with Machine Learning models, and outcome explanation. This\nnovel online processing pipeline has two main highlights: (i) a dedicated\nsample pre-processing module, which builds statistical and frequency-related\nfeatures on the fly, and (ii) an explainability module. This work is the first\nto perform online fault prediction with natural language and visual\nexplainability. The experiments were performed with the MetroPT data set from\nthe metro operator of Porto, Portugal. The results are above 98 % for F-measure\nand 99 % for accuracy. In the context of railway predictive maintenance,\nachieving these high values is crucial due to the practical and operational\nimplications of accurate failure prediction. In the specific case of a high\nF-measure, this ensures that the system maintains an optimal balance between\ndetecting the highest possible number of real faults and minimizing false\nalarms, which is crucial for maximizing service availability. Furthermore, the\naccuracy obtained enables reliability, directly impacting cost reduction and\nincreased safety. The analysis demonstrates that the pipeline maintains high\nperformance even in the presence of class imbalance and noise, and its\nexplanations effectively reflect the decision-making process. These findings\nvalidate the methodological soundness of the approach and confirm its practical\napplicability for supporting proactive maintenance decisions in real-world\nrailway operations. Therefore, by identifying the early signs of failure, this\npipeline enables decision-makers to understand the underlying problems and act\naccordingly swiftly.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u65f6\u6570\u636e\u9a71\u52a8\u7684\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u9884\u6d4b\u6027\u7ef4\u62a4\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u7ebf\u5904\u7406\u6d41\u7a0b\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u6545\u969c\u9884\u6d4b\uff0c\u5e76\u9996\u6b21\u7ed3\u5408\u81ea\u7136\u8bed\u8a00\u548c\u89c6\u89c9\u89e3\u91ca\u3002", "motivation": "\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u7684\u9884\u6d4b\u6027\u7ef4\u62a4\u5bf9\u786e\u4fdd\u9ad8\u670d\u52a1\u53ef\u7528\u6027\u548c\u5b89\u5168\u6027\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u5b9e\u65f6\u6027\u548c\u89e3\u91ca\u6027\u3002", "method": "\u91c7\u7528\u5305\u542b\u6837\u672c\u9884\u5904\u7406\u3001\u589e\u91cf\u5206\u7c7b\u548c\u7ed3\u679c\u89e3\u91ca\u7684\u5728\u7ebf\u5904\u7406\u6d41\u7a0b\uff0c\u7ed3\u5408\u7edf\u8ba1\u548c\u9891\u7387\u7279\u5f81\u63d0\u53d6\u4ee5\u53ca\u89e3\u91ca\u6027\u6a21\u5757\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793aF-measure\u8d85\u8fc798%\uff0c\u51c6\u786e\u7387\u8fbe99%\uff0c\u4e14\u5728\u7c7b\u4e0d\u5e73\u8861\u548c\u566a\u58f0\u60c5\u51b5\u4e0b\u4fdd\u6301\u9ad8\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9a8c\u8bc1\u4e86\u5176\u65b9\u6cd5\u8bba\u7684\u7a33\u5065\u6027\u548c\u5b9e\u9645\u9002\u7528\u6027\uff0c\u80fd\u591f\u652f\u6301\u94c1\u8def\u8fd0\u8425\u4e2d\u7684\u4e3b\u52a8\u7ef4\u62a4\u51b3\u7b56\u3002"}}
{"id": "2508.05368", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.05368", "abs": "https://arxiv.org/abs/2508.05368", "authors": ["Tong Hua", "Jiale Han", "Wei Ouyang"], "title": "A Multi-view Landmark Representation Approach with Application to GNSS-Visual-Inertial Odometry", "comment": null, "summary": "Invariant Extended Kalman Filter (IEKF) has been a significant technique in\nvision-aided sensor fusion. However, it usually suffers from high computational\nburden when jointly optimizing camera poses and the landmarks. To improve its\nefficiency and applicability for multi-sensor fusion, we present a multi-view\npose-only estimation approach with its application to GNSS-Visual-Inertial\nOdometry (GVIO) in this paper. Our main contribution is deriving a visual\nmeasurement model which directly associates landmark representation with\nmultiple camera poses and observations. Such a pose-only measurement is proven\nto be tightly-coupled between landmarks and poses, and maintain a perfect null\nspace that is independent of estimated poses. Finally, we apply the proposed\napproach to a filter based GVIO with a novel feature management strategy. Both\nsimulation tests and real-world experiments are conducted to demonstrate the\nsuperiority of the proposed method in terms of efficiency and accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u89c6\u89d2\u4ec5\u59ff\u6001\u4f30\u8ba1\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347GNSS-\u89c6\u89c9-\u60ef\u6027\u91cc\u7a0b\u8ba1\uff08GVIO\uff09\u7684\u6548\u7387\uff0c\u901a\u8fc7\u76f4\u63a5\u5173\u8054\u5730\u6807\u4e0e\u591a\u76f8\u673a\u59ff\u6001\u7684\u89c6\u89c9\u6d4b\u91cf\u6a21\u578b\uff0c\u51cf\u5c11\u8ba1\u7b97\u8d1f\u62c5\u3002", "motivation": "\u4f20\u7edfIEKF\u5728\u8054\u5408\u4f18\u5316\u76f8\u673a\u59ff\u6001\u548c\u5730\u6807\u65f6\u8ba1\u7b97\u8d1f\u62c5\u9ad8\uff0c\u9650\u5236\u4e86\u5176\u5728\u591a\u4f20\u611f\u5668\u878d\u5408\u4e2d\u7684\u6548\u7387\u548c\u9002\u7528\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ec5\u59ff\u6001\u7684\u89c6\u89c9\u6d4b\u91cf\u6a21\u578b\uff0c\u76f4\u63a5\u5173\u8054\u5730\u6807\u4e0e\u591a\u76f8\u673a\u59ff\u6001\uff0c\u4fdd\u6301\u7d27\u5bc6\u8026\u5408\u548c\u5b8c\u7f8e\u96f6\u7a7a\u95f4\u3002", "result": "\u4eff\u771f\u548c\u5b9e\u9645\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6548\u7387\u548c\u7cbe\u5ea6\u4e0a\u5747\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86GVIO\u7684\u8ba1\u7b97\u6548\u7387\u548c\u7cbe\u5ea6\uff0c\u9002\u7528\u4e8e\u591a\u4f20\u611f\u5668\u878d\u5408\u573a\u666f\u3002"}}
{"id": "2508.05405", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05405", "abs": "https://arxiv.org/abs/2508.05405", "authors": ["Xinrun Xu", "Pi Bu", "Ye Wang", "B\u00f6rje F. Karlsson", "Ziming Wang", "Tengtao Song", "Qi Zhu", "Jun Song", "Zhiming Ding", "Bo Zheng"], "title": "DeepPHY: Benchmarking Agentic VLMs on Physical Reasoning", "comment": "48 pages", "summary": "Although Vision Language Models (VLMs) exhibit strong perceptual abilities\nand impressive visual reasoning, they struggle with attention to detail and\nprecise action planning in complex, dynamic environments, leading to subpar\nperformance. Real-world tasks typically require complex interactions, advanced\nspatial reasoning, long-term planning, and continuous strategy refinement,\nusually necessitating understanding the physics rules of the target scenario.\nHowever, evaluating these capabilities in real-world scenarios is often\nprohibitively expensive. To bridge this gap, we introduce DeepPHY, a novel\nbenchmark framework designed to systematically evaluate VLMs' understanding and\nreasoning about fundamental physical principles through a series of challenging\nsimulated environments. DeepPHY integrates multiple physical reasoning\nenvironments of varying difficulty levels and incorporates fine-grained\nevaluation metrics. Our evaluation finds that even state-of-the-art VLMs\nstruggle to translate descriptive physical knowledge into precise, predictive\ncontrol.", "AI": {"tldr": "DeepPHY\u662f\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5bf9\u7269\u7406\u539f\u7406\u7684\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u5c06\u63cf\u8ff0\u6027\u7269\u7406\u77e5\u8bc6\u8f6c\u5316\u4e3a\u7cbe\u786e\u9884\u6d4b\u63a7\u5236\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "VLMs\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u7f3a\u4e4f\u7ec6\u8282\u5173\u6ce8\u548c\u7cbe\u786e\u884c\u52a8\u89c4\u5212\u80fd\u529b\uff0c\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u6210\u672c\u9ad8\u6602\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u7cfb\u7edf\u5316\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u63d0\u51faDeepPHY\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u96be\u5ea6\u7269\u7406\u63a8\u7406\u73af\u5883\u548c\u7ec6\u7c92\u5ea6\u8bc4\u4f30\u6307\u6807\uff0c\u7cfb\u7edf\u6d4b\u8bd5VLMs\u7684\u7269\u7406\u7406\u89e3\u80fd\u529b\u3002", "result": "\u8bc4\u4f30\u663e\u793a\uff0c\u5373\u4f7f\u662f\u5148\u8fdb\u7684VLMs\u4e5f\u96be\u4ee5\u5c06\u7269\u7406\u77e5\u8bc6\u8f6c\u5316\u4e3a\u7cbe\u786e\u7684\u9884\u6d4b\u63a7\u5236\u3002", "conclusion": "DeepPHY\u4e3aVLMs\u7684\u7269\u7406\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u8bc4\u4f30\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2508.05373", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.05373", "abs": "https://arxiv.org/abs/2508.05373", "authors": ["Morten Roed Frederiksen", "Kasper St\u00f8y"], "title": "Robots can defuse high-intensity conflict situations", "comment": "7 pages, 6 figures, 2020 IEEE/RSJ International Conference on\n  Intelligent Robots and Systems (IROS) October 25-29, 2020, Las Vegas, NV, USA", "summary": "This paper investigates the specific scenario of high-intensity\nconfrontations between humans and robots, to understand how robots can defuse\nthe conflict. It focuses on the effectiveness of using five different affective\nexpression modalities as main drivers for defusing the conflict. The aim is to\ndiscover any strengths or weaknesses in using each modality to mitigate the\nhostility that people feel towards a poorly performing robot. The defusing of\nthe situation is accomplished by making the robot better at acknowledging the\nconflict and by letting it express remorse. To facilitate the tests, we used a\ncustom affective robot in a simulated conflict situation with 105 test\nparticipants. The results show that all tested expression modalities can\nsuccessfully be used to defuse the situation and convey an acknowledgment of\nthe confrontation. The ratings were remarkably similar, but the movement\nmodality was different (ANON p$<$.05) than the other modalities. The test\nparticipants also had similar affective interpretations on how impacted the\nrobot was of the confrontation across all expression modalities. This indicates\nthat defusing a high-intensity interaction may not demand special attention to\nthe expression abilities of the robot, but rather require attention to the\nabilities of being socially aware of the situation and reacting in accordance\nwith it.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u673a\u5668\u4eba\u5728\u9ad8\u5f3a\u5ea6\u51b2\u7a81\u4e2d\u5982\u4f55\u901a\u8fc7\u4e94\u79cd\u60c5\u611f\u8868\u8fbe\u65b9\u5f0f\u7f13\u89e3\u51b2\u7a81\uff0c\u53d1\u73b0\u6240\u6709\u65b9\u5f0f\u5747\u6709\u6548\uff0c\u4f46\u8fd0\u52a8\u65b9\u5f0f\u8868\u73b0\u4e0d\u540c\u3002", "motivation": "\u4e86\u89e3\u673a\u5668\u4eba\u5982\u4f55\u901a\u8fc7\u60c5\u611f\u8868\u8fbe\u7f13\u89e3\u4eba\u7c7b\u5bf9\u5176\u7684\u654c\u610f\uff0c\u5c24\u5176\u662f\u5728\u5176\u8868\u73b0\u4e0d\u4f73\u65f6\u3002", "method": "\u4f7f\u7528\u5b9a\u5236\u60c5\u611f\u673a\u5668\u4eba\u5728\u6a21\u62df\u51b2\u7a81\u573a\u666f\u4e2d\u6d4b\u8bd5105\u540d\u53c2\u4e0e\u8005\uff0c\u8bc4\u4f30\u4e94\u79cd\u8868\u8fbe\u65b9\u5f0f\u7684\u6548\u679c\u3002", "result": "\u6240\u6709\u8868\u8fbe\u65b9\u5f0f\u5747\u80fd\u6210\u529f\u7f13\u89e3\u51b2\u7a81\uff0c\u8fd0\u52a8\u65b9\u5f0f\u8868\u73b0\u663e\u8457\u4e0d\u540c\uff0c\u53c2\u4e0e\u8005\u5bf9\u5404\u65b9\u5f0f\u7684\u611f\u77e5\u76f8\u4f3c\u3002", "conclusion": "\u7f13\u89e3\u9ad8\u5f3a\u5ea6\u51b2\u7a81\u66f4\u9700\u673a\u5668\u4eba\u5177\u5907\u793e\u4f1a\u60c5\u5883\u610f\u8bc6\uff0c\u800c\u975e\u7279\u5b9a\u8868\u8fbe\u65b9\u5f0f\u3002"}}
{"id": "2508.05427", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05427", "abs": "https://arxiv.org/abs/2508.05427", "authors": ["Kartar Kumar Lohana Tharwani", "Rajesh Kumar", "Sumita", "Numan Ahmed", "Yong Tang"], "title": "Large Language Models Transform Organic Synthesis From Reaction Prediction to Automation", "comment": null, "summary": "Large language models (LLMs) are beginning to reshape how chemists plan and\nrun reactions in organic synthesis. Trained on millions of reported\ntransformations, these text-based models can propose synthetic routes, forecast\nreaction outcomes and even instruct robots that execute experiments without\nhuman supervision. Here we survey the milestones that turned LLMs from\nspeculative tools into practical lab partners. We show how coupling LLMs with\ngraph neural networks, quantum calculations and real-time spectroscopy shrinks\ndiscovery cycles and supports greener, data-driven chemistry. We discuss\nlimitations, including biased datasets, opaque reasoning and the need for\nsafety gates that prevent unintentional hazards. Finally, we outline community\ninitiatives open benchmarks, federated learning and explainable interfaces that\naim to democratize access while keeping humans firmly in control. These\nadvances chart a path towards rapid, reliable and inclusive molecular\ninnovation powered by artificial intelligence and automation.", "AI": {"tldr": "LLMs\u6b63\u5728\u6539\u53d8\u6709\u673a\u5408\u6210\u5316\u5b66\u5bb6\u7684\u5b9e\u9a8c\u65b9\u5f0f\uff0c\u7ed3\u5408\u56fe\u795e\u7ecf\u7f51\u7edc\u3001\u91cf\u5b50\u8ba1\u7b97\u548c\u5b9e\u65f6\u5149\u8c31\u6280\u672f\uff0c\u52a0\u901f\u53d1\u73b0\u5468\u671f\u5e76\u63a8\u52a8\u7eff\u8272\u5316\u5b66\u3002", "motivation": "\u63a2\u8ba8LLMs\u5982\u4f55\u4ece\u7406\u8bba\u5de5\u5177\u53d1\u5c55\u4e3a\u5b9e\u7528\u7684\u5b9e\u9a8c\u5ba4\u52a9\u624b\uff0c\u63a8\u52a8\u5206\u5b50\u521b\u65b0\u7684\u5feb\u901f\u3001\u53ef\u9760\u548c\u5305\u5bb9\u6027\u53d1\u5c55\u3002", "method": "\u7ed3\u5408LLMs\u4e0e\u56fe\u795e\u7ecf\u7f51\u7edc\u3001\u91cf\u5b50\u8ba1\u7b97\u548c\u5b9e\u65f6\u5149\u8c31\u6280\u672f\uff0c\u4f18\u5316\u5408\u6210\u8def\u7ebf\u9884\u6d4b\u548c\u5b9e\u9a8c\u6267\u884c\u3002", "result": "LLMs\u663e\u8457\u7f29\u77ed\u4e86\u53d1\u73b0\u5468\u671f\uff0c\u652f\u6301\u6570\u636e\u9a71\u52a8\u7684\u7eff\u8272\u5316\u5b66\uff0c\u4f46\u4ecd\u5b58\u5728\u6570\u636e\u96c6\u504f\u89c1\u548c\u5b89\u5168\u6027\u95ee\u9898\u3002", "conclusion": "\u901a\u8fc7\u5f00\u653e\u57fa\u51c6\u3001\u8054\u90a6\u5b66\u4e60\u548c\u53ef\u89e3\u91ca\u754c\u9762\u7b49\u793e\u533a\u5021\u8bae\uff0c\u786e\u4fdd\u4eba\u7c7b\u63a7\u5236\u4e0b\u7684\u4eba\u5de5\u667a\u80fd\u548c\u81ea\u52a8\u5316\u5206\u5b50\u521b\u65b0\u3002"}}
{"id": "2508.05396", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05396", "abs": "https://arxiv.org/abs/2508.05396", "authors": ["Yufei Duan", "Hang Yin", "Danica Kragic"], "title": "Real-Time Iteration Scheme for Diffusion Policy", "comment": "\\c{opyright} 2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "Diffusion Policies have demonstrated impressive performance in robotic\nmanipulation tasks. However, their long inference time, resulting from an\nextensive iterative denoising process, and the need to execute an action chunk\nbefore the next prediction to maintain consistent actions limit their\napplicability to latency-critical tasks or simple tasks with a short cycle\ntime. While recent methods explored distillation or alternative policy\nstructures to accelerate inference, these often demand additional training,\nwhich can be resource-intensive for large robotic models. In this paper, we\nintroduce a novel approach inspired by the Real-Time Iteration (RTI) Scheme, a\nmethod from optimal control that accelerates optimization by leveraging\nsolutions from previous time steps as initial guesses for subsequent\niterations. We explore the application of this scheme in diffusion inference\nand propose a scaling-based method to effectively handle discrete actions, such\nas grasping, in robotic manipulation. The proposed scheme significantly reduces\nruntime computational costs without the need for distillation or policy\nredesign. This enables a seamless integration into many pre-trained\ndiffusion-based models, in particular, to resource-demanding large models. We\nalso provide theoretical conditions for the contractivity which could be useful\nfor estimating the initial denoising step. Quantitative results from extensive\nsimulation experiments show a substantial reduction in inference time, with\ncomparable overall performance compared with Diffusion Policy using full-step\ndenoising. Our project page with additional resources is available at:\nhttps://rti-dp.github.io/.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b9e\u65f6\u8fed\u4ee3\uff08RTI\uff09\u65b9\u6848\u7684\u65b0\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u6269\u6563\u7b56\u7565\u7684\u63a8\u7406\u65f6\u95f4\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u7b56\u7565\u91cd\u65b0\u8bbe\u8ba1\u3002", "motivation": "\u6269\u6563\u7b56\u7565\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u8fed\u4ee3\u53bb\u566a\u8fc7\u7a0b\u5bfc\u81f4\u7684\u63a8\u7406\u65f6\u95f4\u957f\uff0c\u9650\u5236\u4e86\u5176\u5728\u5ef6\u8fdf\u654f\u611f\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u501f\u9274\u6700\u4f18\u63a7\u5236\u4e2d\u7684\u5b9e\u65f6\u8fed\u4ee3\uff08RTI\uff09\u65b9\u6848\uff0c\u5229\u7528\u524d\u4e00\u65f6\u95f4\u6b65\u7684\u89e3\u4f5c\u4e3a\u540e\u7eed\u8fed\u4ee3\u7684\u521d\u59cb\u731c\u6d4b\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u7f29\u653e\u7684\u65b9\u6cd5\u5904\u7406\u79bb\u6563\u52a8\u4f5c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5927\u5e45\u964d\u4f4e\u63a8\u7406\u65f6\u95f4\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u5168\u6b65\u53bb\u566a\u6269\u6563\u7b56\u7565\u76f8\u5f53\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u7684\u96c6\u6210\u63d0\u4f9b\u4e86\u65e0\u7f1d\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u8d44\u6e90\u5bc6\u96c6\u578b\u5927\u6a21\u578b\u3002"}}
{"id": "2508.05432", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.05432", "abs": "https://arxiv.org/abs/2508.05432", "authors": ["Krzysztof Janowicz", "Zilong Liu", "Gengchen Mai", "Zhangyu Wang", "Ivan Majic", "Alexandra Fortacz", "Grant McKenzie", "Song Gao"], "title": "Whose Truth? Pluralistic Geo-Alignment for (Agentic) AI", "comment": null, "summary": "AI (super) alignment describes the challenge of ensuring (future) AI systems\nbehave in accordance with societal norms and goals. While a quickly evolving\nliterature is addressing biases and inequalities, the geographic variability of\nalignment remains underexplored. Simply put, what is considered appropriate,\ntruthful, or legal can differ widely across regions due to cultural norms,\npolitical realities, and legislation. Alignment measures applied to AI/ML\nworkflows can sometimes produce outcomes that diverge from statistical\nrealities, such as text-to-image models depicting balanced gender ratios in\ncompany leadership despite existing imbalances. Crucially, some model outputs\nare globally acceptable, while others, e.g., questions about Kashmir, depend on\nknowing the user's location and their context. This geographic sensitivity is\nnot new. For instance, Google Maps renders Kashmir's borders differently based\non user location. What is new is the unprecedented scale and automation with\nwhich AI now mediates knowledge, expresses opinions, and represents geographic\nreality to millions of users worldwide, often with little transparency about\nhow context is managed. As we approach Agentic AI, the need for\nspatio-temporally aware alignment, rather than one-size-fits-all approaches, is\nincreasingly urgent. This paper reviews key geographic research problems,\nsuggests topics for future work, and outlines methods for assessing alignment\nsensitivity.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86AI\u5bf9\u9f50\u4e2d\u7684\u5730\u7406\u5dee\u5f02\u95ee\u9898\uff0c\u5f3a\u8c03\u9700\u6839\u636e\u5730\u57df\u6587\u5316\u3001\u653f\u6cbb\u548c\u6cd5\u5f8b\u5dee\u5f02\u8c03\u6574\u5bf9\u9f50\u63aa\u65bd\uff0c\u907f\u514d\u4e00\u5200\u5207\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709AI\u5bf9\u9f50\u7814\u7a76\u5ffd\u89c6\u5730\u7406\u591a\u6837\u6027\uff0c\u5bfc\u81f4\u6a21\u578b\u8f93\u51fa\u53ef\u80fd\u4e0e\u5b9e\u9645\u7edf\u8ba1\u6216\u5730\u57df\u89c4\u8303\u4e0d\u7b26\uff0c\u4e9f\u9700\u89e3\u51b3\u3002", "method": "\u56de\u987e\u5173\u952e\u5730\u7406\u7814\u7a76\u95ee\u9898\uff0c\u63d0\u51fa\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u5e76\u8bbe\u8ba1\u8bc4\u4f30\u5bf9\u9f50\u654f\u611f\u6027\u7684\u65b9\u6cd5\u3002", "result": "\u6307\u51faAI\u5bf9\u9f50\u9700\u8003\u8651\u65f6\u7a7a\u80cc\u666f\uff0c\u907f\u514d\u4e00\u5200\u5207\uff0c\u90e8\u5206\u6a21\u578b\u8f93\u51fa\u9700\u6839\u636e\u7528\u6237\u5730\u7406\u4f4d\u7f6e\u8c03\u6574\u3002", "conclusion": "\u547c\u5401\u5f00\u53d1\u65f6\u7a7a\u611f\u77e5\u7684\u5bf9\u9f50\u65b9\u6cd5\uff0c\u4ee5\u9002\u5e94\u5168\u7403\u5316AI\u5e94\u7528\u4e2d\u7684\u5730\u57df\u591a\u6837\u6027\u3002"}}
{"id": "2508.05402", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05402", "abs": "https://arxiv.org/abs/2508.05402", "authors": ["Rui Yu", "Xianghang Zhang", "Runkai Zhao", "Huaicheng Yan", "Meng Wang"], "title": "DistillDrive: End-to-End Multi-Mode Autonomous Driving Distillation by Isomorphic Hetero-Source Planning Model", "comment": null, "summary": "End-to-end autonomous driving has been recently seen rapid development,\nexerting a profound influence on both industry and academia. However, the\nexisting work places excessive focus on ego-vehicle status as their sole\nlearning objectives and lacks of planning-oriented understanding, which limits\nthe robustness of the overall decision-making prcocess. In this work, we\nintroduce DistillDrive, an end-to-end knowledge distillation-based autonomous\ndriving model that leverages diversified instance imitation to enhance\nmulti-mode motion feature learning. Specifically, we employ a planning model\nbased on structured scene representations as the teacher model, leveraging its\ndiversified planning instances as multi-objective learning targets for the\nend-to-end model. Moreover, we incorporate reinforcement learning to enhance\nthe optimization of state-to-decision mappings, while utilizing generative\nmodeling to construct planning-oriented instances, fostering intricate\ninteractions within the latent space. We validate our model on the nuScenes and\nNAVSIM datasets, achieving a 50\\% reduction in collision rate and a 3-point\nimprovement in closed-loop performance compared to the baseline model. Code and\nmodel are publicly available at https://github.com/YuruiAI/DistillDrive", "AI": {"tldr": "DistillDrive\u662f\u4e00\u79cd\u57fa\u4e8e\u77e5\u8bc6\u84b8\u998f\u7684\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u6837\u5316\u5b9e\u4f8b\u6a21\u4eff\u589e\u5f3a\u591a\u6a21\u6001\u8fd0\u52a8\u7279\u5f81\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u51b3\u7b56\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u8fc7\u4e8e\u5173\u6ce8\u81ea\u8f66\u72b6\u6001\u4f5c\u4e3a\u5355\u4e00\u5b66\u4e60\u76ee\u6807\uff0c\u7f3a\u4e4f\u89c4\u5212\u5bfc\u5411\u7684\u7406\u89e3\uff0c\u9650\u5236\u4e86\u51b3\u7b56\u8fc7\u7a0b\u7684\u9c81\u68d2\u6027\u3002", "method": "\u5229\u7528\u57fa\u4e8e\u7ed3\u6784\u5316\u573a\u666f\u8868\u793a\u7684\u89c4\u5212\u6a21\u578b\u4f5c\u4e3a\u6559\u5e08\u6a21\u578b\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u548c\u751f\u6210\u5efa\u6a21\u4f18\u5316\u72b6\u6001\u5230\u51b3\u7b56\u7684\u6620\u5c04\u3002", "result": "\u5728nuScenes\u548cNAVSIM\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u78b0\u649e\u7387\u964d\u4f4e50%\uff0c\u95ed\u73af\u6027\u80fd\u63d0\u53473\u70b9\u3002", "conclusion": "DistillDrive\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u548c\u591a\u6837\u5316\u5b9e\u4f8b\u6a21\u4eff\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd\u3002"}}
{"id": "2508.05464", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.05464", "abs": "https://arxiv.org/abs/2508.05464", "authors": ["Matteo Prandi", "Vincenzo Suriani", "Federico Pierucci", "Marcello Galisai", "Daniele Nardi", "Piercosma Bisconti"], "title": "Bench-2-CoP: Can We Trust Benchmarking for EU AI Compliance?", "comment": null, "summary": "The rapid advancement of General Purpose AI (GPAI) models necessitates robust\nevaluation frameworks, especially with emerging regulations like the EU AI Act\nand its associated Code of Practice (CoP). Current AI evaluation practices\ndepend heavily on established benchmarks, but these tools were not designed to\nmeasure the systemic risks that are the focus of the new regulatory landscape.\nThis research addresses the urgent need to quantify this \"benchmark-regulation\ngap.\" We introduce Bench-2-CoP, a novel, systematic framework that uses\nvalidated LLM-as-judge analysis to map the coverage of 194,955 questions from\nwidely-used benchmarks against the EU AI Act's taxonomy of model capabilities\nand propensities. Our findings reveal a profound misalignment: the evaluation\necosystem is overwhelmingly focused on a narrow set of behavioral propensities,\nsuch as \"Tendency to hallucinate\" (53.7% of the corpus) and \"Discriminatory\nbias\" (28.9%), while critical functional capabilities are dangerously\nneglected. Crucially, capabilities central to loss-of-control scenarios,\nincluding evading human oversight, self-replication, and autonomous AI\ndevelopment, receive zero coverage in the entire benchmark corpus. This\ntranslates to a near-total evaluation gap for systemic risks like \"Loss of\nControl\" (0.4% coverage) and \"Cyber Offence\" (0.8% coverage). This study\nprovides the first comprehensive, quantitative analysis of this gap, offering\ncritical insights for policymakers to refine the CoP and for developers to\nbuild the next generation of evaluation tools, ultimately fostering safer and\nmore compliant AI.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51faBench-2-CoP\u6846\u67b6\uff0c\u91cf\u5316AI\u8bc4\u4f30\u57fa\u51c6\u4e0e\u6b27\u76dfAI\u6cd5\u6848\u95f4\u7684\u5dee\u8ddd\uff0c\u53d1\u73b0\u5f53\u524d\u8bc4\u4f30\u8fc7\u5ea6\u5173\u6ce8\u884c\u4e3a\u503e\u5411\uff0c\u800c\u5ffd\u89c6\u5173\u952e\u529f\u80fd\u80fd\u529b\u3002", "motivation": "\u73b0\u6709AI\u8bc4\u4f30\u57fa\u51c6\u65e0\u6cd5\u6ee1\u8db3\u6b27\u76dfAI\u6cd5\u6848\u5bf9\u7cfb\u7edf\u6027\u98ce\u9669\u7684\u5173\u6ce8\uff0c\u4e9f\u9700\u91cf\u5316\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "\u4f7f\u7528LLM-as-judge\u5206\u6790\uff0c\u5c06194,955\u4e2a\u57fa\u51c6\u95ee\u9898\u6620\u5c04\u5230\u6b27\u76dfAI\u6cd5\u6848\u7684\u80fd\u529b\u5206\u7c7b\u4e2d\u3002", "result": "\u8bc4\u4f30\u751f\u6001\u4e25\u91cd\u504f\u5411\u884c\u4e3a\u503e\u5411\uff08\u5982\u5e7b\u89c9\u503e\u5411\u536053.7%\uff09\uff0c\u5173\u952e\u529f\u80fd\u80fd\u529b\uff08\u5982\u81ea\u4e3bAI\u5f00\u53d1\uff09\u5b8c\u5168\u672a\u8986\u76d6\u3002", "conclusion": "\u7814\u7a76\u4e3a\u653f\u7b56\u5236\u5b9a\u8005\u548c\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u6539\u8fdb\u8bc4\u4f30\u5de5\u5177\u7684\u5173\u952e\u4f9d\u636e\uff0c\u4ee5\u4fc3\u8fdb\u66f4\u5b89\u5168\u5408\u89c4\u7684AI\u3002"}}
{"id": "2508.05410", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.05410", "abs": "https://arxiv.org/abs/2508.05410", "authors": ["Manas Bhargava", "Takefumi Hiraki", "Malina Strugaru", "Michal Piovarci", "Chiara Daraio", "Daisuke Iwai", "Bernd Bickel"], "title": "Computational Design and Fabrication of Modular Robots with Untethered Control", "comment": null, "summary": "Natural organisms use distributed actuation via their musculoskeletal systems\nto adapt their gait for traversing diverse terrains or to morph their bodies to\nperform varied tasks. A longstanding challenge in the field of robotics is to\nmimic this extensive adaptability and range of motion. This has led humans to\ndevelop various soft robotic systems that emulate natural organisms. However,\nsuch systems are generally optimized for a single functionality, lack the\nability to change form or function on demand, or are often tethered to bulky\ncontrol systems. To address these challenges, we present our framework for\ndesigning and controlling robots that mimic nature's blueprint by utilizing\ndistributed actuation. We propose a novel building block that combines\n3D-printed bones with liquid crystal elastomer (LCE) muscles as lightweight\nactuators and enables the modular assembly of musculoskeletal robots. We\ndeveloped LCE rods that contract in response to infrared radiation, thereby\nachieving local and untethered control over the distributed network of bones,\nwhich in turn results in global deformation of the robot. Furthermore, to\ncapitalize on the extensive design space, we develop two computational tools:\none to optimize the robot's skeletal graph, enabling multiple target\ndeformations, and another to co-optimize the skeletal designs and control gaits\nto achieve target locomotion. We validate our system by building several robots\nthat show complex shape morphing, varying control schemes, and adaptability to\ntheir environment. Our system integrates advances in modular material building,\nuntethered and distributed control, and computational design to introduce a new\ngeneration of robots that brings us closer to the capabilities of living\norganisms.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u5e03\u5f0f\u9a71\u52a8\u7684\u673a\u5668\u4eba\u8bbe\u8ba1\u6846\u67b6\uff0c\u7ed3\u54083D\u6253\u5370\u9aa8\u9abc\u548c\u6db2\u6676\u5f39\u6027\u4f53\u808c\u8089\uff0c\u5b9e\u73b0\u6a21\u5757\u5316\u7ec4\u88c5\u548c\u65e0\u7ebf\u63a7\u5236\uff0c\u5e76\u901a\u8fc7\u8ba1\u7b97\u5de5\u5177\u4f18\u5316\u8bbe\u8ba1\u548c\u8fd0\u52a8\u3002", "motivation": "\u6a21\u4eff\u81ea\u7136\u751f\u7269\u7684\u9002\u5e94\u6027\u548c\u8fd0\u52a8\u8303\u56f4\uff0c\u89e3\u51b3\u73b0\u6709\u8f6f\u673a\u5668\u4eba\u7cfb\u7edf\u529f\u80fd\u5355\u4e00\u3001\u65e0\u6cd5\u52a8\u6001\u6539\u53d8\u5f62\u6001\u6216\u4f9d\u8d56\u7b28\u91cd\u63a7\u5236\u7684\u95ee\u9898\u3002", "method": "\u5229\u75283D\u6253\u5370\u9aa8\u9abc\u548c\u6db2\u6676\u5f39\u6027\u4f53\uff08LCE\uff09\u808c\u8089\u4f5c\u4e3a\u8f7b\u91cf\u7ea7\u9a71\u52a8\u5668\uff0c\u5f00\u53d1\u7ea2\u5916\u54cd\u5e94LCE\u6746\u5b9e\u73b0\u65e0\u7ebf\u63a7\u5236\uff0c\u5e76\u901a\u8fc7\u8ba1\u7b97\u5de5\u5177\u4f18\u5316\u9aa8\u9abc\u56fe\u548c\u8fd0\u52a8\u6b65\u6001\u3002", "result": "\u6784\u5efa\u4e86\u591a\u4e2a\u673a\u5668\u4eba\uff0c\u5c55\u793a\u4e86\u590d\u6742\u5f62\u6001\u53d8\u5316\u3001\u591a\u6837\u5316\u63a7\u5236\u65b9\u6848\u548c\u73af\u5883\u9002\u5e94\u6027\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u7ed3\u5408\u6a21\u5757\u5316\u6750\u6599\u3001\u65e0\u7ebf\u5206\u5e03\u5f0f\u63a7\u5236\u548c\u8ba1\u7b97\u8bbe\u8ba1\uff0c\u63a8\u52a8\u4e86\u673a\u5668\u4eba\u6280\u672f\u5411\u751f\u7269\u80fd\u529b\u7684\u9760\u8fd1\u3002"}}
{"id": "2508.05474", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.05474", "abs": "https://arxiv.org/abs/2508.05474", "authors": ["Burak Can Kaplan", "Hugo Cesar De Castro Carneiro", "Stefan Wermter"], "title": "Can Large Language Models Generate Effective Datasets for Emotion Recognition in Conversations?", "comment": "8 pages, 4 figures", "summary": "Emotion recognition in conversations (ERC) focuses on identifying emotion\nshifts within interactions, representing a significant step toward advancing\nmachine intelligence. However, ERC data remains scarce, and existing datasets\nface numerous challenges due to their highly biased sources and the inherent\nsubjectivity of soft labels. Even though Large Language Models (LLMs) have\ndemonstrated their quality in many affective tasks, they are typically\nexpensive to train, and their application to ERC tasks--particularly in data\ngeneration--remains limited. To address these challenges, we employ a small,\nresource-efficient, and general-purpose LLM to synthesize ERC datasets with\ndiverse properties, supplementing the three most widely used ERC benchmarks. We\ngenerate six novel datasets, with two tailored to enhance each benchmark. We\nevaluate the utility of these datasets to (1) supplement existing datasets for\nERC classification, and (2) analyze the effects of label imbalance in ERC. Our\nexperimental results indicate that ERC classifier models trained on the\ngenerated datasets exhibit strong robustness and consistently achieve\nstatistically significant performance improvements on existing ERC benchmarks.", "AI": {"tldr": "\u4f7f\u7528\u5c0f\u578b\u3001\u9ad8\u6548\u7684\u901a\u7528LLM\u751f\u6210\u591a\u6837\u5316\u7684ERC\u6570\u636e\u96c6\uff0c\u8865\u5145\u73b0\u6709\u57fa\u51c6\uff0c\u63d0\u5347\u5206\u7c7b\u6027\u80fd\u5e76\u5206\u6790\u6807\u7b7e\u4e0d\u5e73\u8861\u5f71\u54cd\u3002", "motivation": "\u89e3\u51b3ERC\u6570\u636e\u7a00\u7f3a\u3001\u6765\u6e90\u504f\u89c1\u548c\u6807\u7b7e\u4e3b\u89c2\u6027\u95ee\u9898\uff0c\u63a2\u7d22LLM\u5728\u6570\u636e\u751f\u6210\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u5229\u7528\u5c0f\u578b\u901a\u7528LLM\u751f\u6210\u516d\u4e2a\u65b0\u6570\u636e\u96c6\uff0c\u8865\u5145\u4e09\u4e2a\u4e3b\u6d41ERC\u57fa\u51c6\uff0c\u8bc4\u4f30\u5176\u5728\u5206\u7c7b\u548c\u6807\u7b7e\u4e0d\u5e73\u8861\u5206\u6790\u4e2d\u7684\u6548\u679c\u3002", "result": "\u751f\u6210\u7684\u663e\u8457\u63d0\u5347\u4e86ERC\u5206\u7c7b\u5668\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u5728\u73b0\u6709\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u7edf\u8ba1\u663e\u8457\u7684\u6027\u80fd\u6539\u8fdb\u3002", "conclusion": "\u5c0f\u578bLLM\u751f\u6210\u7684\u6570\u636e\u96c6\u80fd\u6709\u6548\u8865\u5145ERC\u4efb\u52a1\uff0c\u63d0\u5347\u6a21\u578b\u6027\u80fd\u5e76\u89e3\u51b3\u6570\u636e\u4e0d\u8db3\u95ee\u9898\u3002"}}
{"id": "2508.05415", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.05415", "abs": "https://arxiv.org/abs/2508.05415", "authors": ["Alexander Fabisch", "Wadhah Zai El Amri", "Chandandeep Singh", "Nicol\u00e1s Navarro-Guerrero"], "title": "Do Robots Really Need Anthropomorphic Hands?", "comment": null, "summary": "Human manipulation skills represent a pinnacle of their voluntary motor\nfunctions, requiring the coordination of many degrees of freedom and processing\nof high-dimensional sensor input to achieve such a high level of dexterity.\nThus, we set out to answer whether the human hand, with its associated\nbiomechanical properties, sensors, and control mechanisms, is an ideal that we\nshould strive for in robotics-do we really need anthropomorphic robotic hands?\n  This survey can help practitioners to make the trade-off between hand\ncomplexity and potential manipulation skills. We provide an overview of the\nhuman hand, a comparison of commercially available robotic and prosthetic\nhands, and a systematic review of hand mechanisms and skills that they are\ncapable of. This leads to follow-up questions. What is the minimum requirement\nfor mechanisms and sensors to implement most skills that a robot needs? What is\nmissing to reach human-level dexterity? Can we improve upon human dexterity?\n  Although complex five-fingered hands are often used as the ultimate goal for\nrobotic manipulators, they are not necessary for all tasks. We found that wrist\nflexibility and finger abduction/adduction are important for manipulation\ncapabilities. On the contrary, increasing the number of fingers, actuators, or\ndegrees of freedom is often not necessary. Three fingers are a good compromise\nbetween simplicity and dexterity. Non-anthropomorphic hand designs with two\nopposing pairs of fingers or human hands with six fingers can further increase\ndexterity, suggesting that the human hand may not be the optimum.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u4eba\u7c7b\u624b\u7684\u7075\u5de7\u6027\u662f\u5426\u5e94\u4e3a\u673a\u5668\u4eba\u624b\u7684\u7406\u60f3\u76ee\u6807\uff0c\u5206\u6790\u4e86\u73b0\u6709\u673a\u5668\u4eba\u624b\u7684\u6027\u80fd\u4e0e\u8bbe\u8ba1\uff0c\u5e76\u63d0\u51fa\u4e86\u7b80\u5316\u8bbe\u8ba1\u7684\u53ef\u80fd\u6027\u3002", "motivation": "\u7814\u7a76\u4eba\u7c7b\u624b\u7684\u9ad8\u81ea\u7531\u5ea6\u534f\u8c03\u548c\u9ad8\u7ef4\u4f20\u611f\u5668\u8f93\u5165\u5904\u7406\u80fd\u529b\uff0c\u63a2\u8ba8\u662f\u5426\u9700\u8981\u5728\u673a\u5668\u4eba\u624b\u4e2d\u8ffd\u6c42\u7c7b\u4f3c\u7684\u8bbe\u8ba1\u3002", "method": "\u901a\u8fc7\u7efc\u8ff0\u4eba\u7c7b\u624b\u7684\u7279\u6027\u3001\u5546\u4e1a\u673a\u5668\u4eba\u624b\u7684\u6bd4\u8f83\u4ee5\u53ca\u7cfb\u7edf\u8bc4\u4f30\u5176\u673a\u5236\u548c\u6280\u80fd\uff0c\u5206\u6790\u673a\u5668\u4eba\u624b\u7684\u8bbe\u8ba1\u9700\u6c42\u3002", "result": "\u53d1\u73b0\u624b\u8155\u7075\u6d3b\u6027\u548c\u624b\u6307\u5916\u5c55/\u5185\u6536\u5bf9\u64cd\u4f5c\u80fd\u529b\u66f4\u91cd\u8981\uff0c\u800c\u589e\u52a0\u624b\u6307\u6570\u91cf\u6216\u81ea\u7531\u5ea6\u5e76\u975e\u5fc5\u8981\uff1b\u4e09\u6307\u8bbe\u8ba1\u662f\u7b80\u5355\u4e0e\u7075\u5de7\u7684\u6298\u8877\u3002", "conclusion": "\u4eba\u7c7b\u624b\u53ef\u80fd\u5e76\u975e\u673a\u5668\u4eba\u624b\u7684\u7406\u60f3\u76ee\u6807\uff0c\u975e\u4eff\u751f\u8bbe\u8ba1\u6216\u80fd\u63d0\u4f9b\u66f4\u9ad8\u7684\u7075\u5de7\u6027\u3002"}}
{"id": "2508.05496", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05496", "abs": "https://arxiv.org/abs/2508.05496", "authors": ["Shuo Cai", "Su Lu", "Qi Zhou", "Kejing Yang", "Zhijie Sang", "Congkai Xie", "Hongxia Yang"], "title": "InfiAlign: A Scalable and Sample-Efficient Framework for Aligning LLMs to Enhance Reasoning Capabilities", "comment": null, "summary": "Large language models (LLMs) have exhibited impressive reasoning abilities on\na wide range of complex tasks. However, enhancing these capabilities through\npost-training remains resource intensive, particularly in terms of data and\ncomputational cost. Although recent efforts have sought to improve sample\nefficiency through selective data curation, existing methods often rely on\nheuristic or task-specific strategies that hinder scalability. In this work, we\nintroduce InfiAlign, a scalable and sample-efficient post-training framework\nthat integrates supervised fine-tuning (SFT) with Direct Preference\nOptimization (DPO) to align LLMs for enhanced reasoning. At the core of\nInfiAlign is a robust data selection pipeline that automatically curates\nhigh-quality alignment data from open-source reasoning datasets using\nmultidimensional quality metrics. This pipeline enables significant performance\ngains while drastically reducing data requirements and remains extensible to\nnew data sources. When applied to the Qwen2.5-Math-7B-Base model, our SFT model\nachieves performance on par with DeepSeek-R1-Distill-Qwen-7B, while using only\napproximately 12% of the training data, and demonstrates strong generalization\nacross diverse reasoning tasks. Additional improvements are obtained through\nthe application of DPO, with particularly notable gains in mathematical\nreasoning tasks. The model achieves an average improvement of 3.89% on AIME\n24/25 benchmarks. Our results highlight the effectiveness of combining\nprincipled data selection with full-stage post-training, offering a practical\nsolution for aligning large reasoning models in a scalable and data-efficient\nmanner. The model checkpoints are available at\nhttps://huggingface.co/InfiX-ai/InfiAlign-Qwen-7B-SFT.", "AI": {"tldr": "InfiAlign\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u6837\u672c\u9ad8\u6548\u7684\u540e\u8bad\u7ec3\u6846\u67b6\uff0c\u7ed3\u5408\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u548c\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u6765\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u63a8\u7406\u80fd\u529b\u3002\u5176\u6838\u5fc3\u662f\u4e00\u4e2a\u81ea\u52a8\u7b5b\u9009\u9ad8\u8d28\u91cf\u6570\u636e\u7684\u6570\u636e\u9009\u62e9\u7ba1\u9053\uff0c\u663e\u8457\u51cf\u5c11\u6570\u636e\u9700\u6c42\u5e76\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u540e\u8bad\u7ec3\u8fc7\u7a0b\u901a\u5e38\u9700\u8981\u5927\u91cf\u6570\u636e\u548c\u8ba1\u7b97\u8d44\u6e90\u3002\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u542f\u53d1\u5f0f\u6216\u4efb\u52a1\u7279\u5b9a\u7b56\u7565\uff0c\u96be\u4ee5\u6269\u5c55\u3002", "method": "InfiAlign\u6846\u67b6\u7ed3\u5408SFT\u548cDPO\uff0c\u901a\u8fc7\u591a\u7ef4\u8d28\u91cf\u6307\u6807\u81ea\u52a8\u7b5b\u9009\u5f00\u6e90\u63a8\u7406\u6570\u636e\u96c6\u4e2d\u7684\u9ad8\u8d28\u91cf\u5bf9\u9f50\u6570\u636e\u3002", "result": "\u5728Qwen2.5-Math-7B-Base\u6a21\u578b\u4e0a\uff0c\u4ec5\u4f7f\u752812%\u7684\u8bad\u7ec3\u6570\u636e\u5373\u8fbe\u5230\u4e0eDeepSeek-R1-Distill-Qwen-7B\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u5e76\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u5e73\u5747\u63d0\u53473.89%\u3002", "conclusion": "InfiAlign\u901a\u8fc7\u7ed3\u5408\u6570\u636e\u9009\u62e9\u548c\u5168\u9636\u6bb5\u540e\u8bad\u7ec3\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u6570\u636e\u9ad8\u6548\u7684\u5bf9\u9f50\u5927\u63a8\u7406\u6a21\u578b\u7684\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.05535", "categories": ["cs.RO", "cs.CL", "cs.HC", "cs.LG", "cs.MA", "I.2.9; I.2.7; I.2.6"], "pdf": "https://arxiv.org/pdf/2508.05535", "abs": "https://arxiv.org/abs/2508.05535", "authors": ["Albert Yu", "Chengshu Li", "Luca Macesanu", "Arnav Balaji", "Ruchira Ray", "Raymond Mooney", "Roberto Mart\u00edn-Mart\u00edn"], "title": "Mixed-Initiative Dialog for Human-Robot Collaborative Manipulation", "comment": "Project website at https://robin-lab.cs.utexas.edu/MicoBot/", "summary": "Effective robotic systems for long-horizon human-robot collaboration must\nadapt to a wide range of human partners, whose physical behavior, willingness\nto assist, and understanding of the robot's capabilities may change over time.\nThis demands a tightly coupled communication loop that grants both agents the\nflexibility to propose, accept, or decline requests as they coordinate toward\ncompleting the task effectively. We apply a Mixed-Initiative dialog paradigm to\nCollaborative human-roBot teaming and propose MICoBot, a system that handles\nthe common scenario where both agents, using natural language, take initiative\nin formulating, accepting, or rejecting proposals on who can best complete\ndifferent steps of a task. To handle diverse, task-directed dialog, and find\nsuccessful collaborative strategies that minimize human effort, MICoBot makes\ndecisions at three levels: (1) a meta-planner considers human dialog to\nformulate and code a high-level collaboration strategy, (2) a planner optimally\nallocates the remaining steps to either agent based on the robot's capabilities\n(measured by a simulation-pretrained affordance model) and the human's\nestimated availability to help, and (3) an action executor decides the\nlow-level actions to perform or words to say to the human. Our extensive\nevaluations in simulation and real-world -- on a physical robot with 18 unique\nhuman participants over 27 hours -- demonstrate the ability of our method to\neffectively collaborate with diverse human users, yielding significantly\nimproved task success and user experience than a pure LLM baseline and other\nagent allocation models. See additional videos and materials at\nhttps://robin-lab.cs.utexas.edu/MicoBot/.", "AI": {"tldr": "MICoBot\u662f\u4e00\u4e2a\u7528\u4e8e\u4eba\u673a\u534f\u4f5c\u7684\u6df7\u5408\u4e3b\u52a8\u5bf9\u8bdd\u7cfb\u7edf\uff0c\u901a\u8fc7\u4e09\u5c42\u51b3\u7b56\u673a\u5236\u4f18\u5316\u4efb\u52a1\u5206\u914d\u548c\u534f\u4f5c\u7b56\u7565\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4efb\u52a1\u6210\u529f\u7387\u548c\u7528\u6237\u4f53\u9a8c\u3002", "motivation": "\u89e3\u51b3\u4eba\u673a\u534f\u4f5c\u4e2d\u56e0\u4eba\u7c7b\u884c\u4e3a\u591a\u6837\u6027\u548c\u52a8\u6001\u53d8\u5316\u5bfc\u81f4\u7684\u6c9f\u901a\u4e0e\u4efb\u52a1\u5206\u914d\u6311\u6218\u3002", "method": "\u91c7\u7528\u4e09\u5c42\u51b3\u7b56\u673a\u5236\uff1a\u5143\u89c4\u5212\u5668\u5236\u5b9a\u534f\u4f5c\u7b56\u7565\uff0c\u89c4\u5212\u5668\u4f18\u5316\u4efb\u52a1\u5206\u914d\uff0c\u6267\u884c\u5668\u5904\u7406\u5177\u4f53\u884c\u52a8\u6216\u5bf9\u8bdd\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u5b9e\u9a8c\u4e2d\uff0cMICoBot\u663e\u8457\u4f18\u4e8e\u7eafLLM\u57fa\u7ebf\u548c\u5176\u4ed6\u4efb\u52a1\u5206\u914d\u6a21\u578b\uff0c\u63d0\u9ad8\u4e86\u4efb\u52a1\u6210\u529f\u7387\u548c\u7528\u6237\u4f53\u9a8c\u3002", "conclusion": "MICoBot\u901a\u8fc7\u6df7\u5408\u4e3b\u52a8\u5bf9\u8bdd\u548c\u591a\u5c42\u51b3\u7b56\u673a\u5236\uff0c\u6709\u6548\u652f\u6301\u591a\u6837\u5316\u4eba\u673a\u534f\u4f5c\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.05498", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05498", "abs": "https://arxiv.org/abs/2508.05498", "authors": ["Ge Chang", "Jinbo Su", "Jiacheng Liu", "Pengfei Yang", "Yuhao Shang", "Huiwen Zheng", "Hongli Ma", "Yan Liang", "Yuanchun Li", "Yunxin Liu"], "title": "GRAIL:Learning to Interact with Large Knowledge Graphs for Retrieval Augmented Reasoning", "comment": "9 pages,3 figures", "summary": "Large Language Models (LLMs) integrated with Retrieval-Augmented Generation\n(RAG) techniques have exhibited remarkable performance across a wide range of\ndomains. However, existing RAG approaches primarily operate on unstructured\ndata and demonstrate limited capability in handling structured knowledge such\nas knowledge graphs. Meanwhile, current graph retrieval methods fundamentally\nstruggle to capture holistic graph structures while simultaneously facing\nprecision control challenges that manifest as either critical information gaps\nor excessive redundant connections, collectively undermining reasoning\nperformance. To address this challenge, we propose GRAIL: Graph-Retrieval\nAugmented Interactive Learning, a framework designed to interact with\nlarge-scale graphs for retrieval-augmented reasoning. Specifically, GRAIL\nintegrates LLM-guided random exploration with path filtering to establish a\ndata synthesis pipeline, where a fine-grained reasoning trajectory is\nautomatically generated for each task. Based on the synthesized data, we then\nemploy a two-stage training process to learn a policy that dynamically decides\nthe optimal actions at each reasoning step. The overall objective of\nprecision-conciseness balance in graph retrieval is decoupled into fine-grained\nprocess-supervised rewards to enhance data efficiency and training stability.\nIn practical deployment, GRAIL adopts an interactive retrieval paradigm,\nenabling the model to autonomously explore graph paths while dynamically\nbalancing retrieval breadth and precision. Extensive experiments have shown\nthat GRAIL achieves an average accuracy improvement of 21.01% and F1\nimprovement of 22.43% on three knowledge graph question-answering datasets. Our\nsource code and datasets is available at https://github.com/Changgeww/GRAIL.", "AI": {"tldr": "GRAIL\u6846\u67b6\u901a\u8fc7\u7ed3\u5408LLM\u5f15\u5bfc\u7684\u968f\u673a\u63a2\u7d22\u548c\u8def\u5f84\u8fc7\u6ee4\uff0c\u89e3\u51b3\u4e86\u73b0\u6709RAG\u65b9\u6cd5\u5728\u5904\u7406\u7ed3\u6784\u5316\u77e5\u8bc6\uff08\u5982\u77e5\u8bc6\u56fe\u8c31\uff09\u65f6\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u73b0\u6709RAG\u65b9\u6cd5\u4e3b\u8981\u9488\u5bf9\u975e\u7ed3\u6784\u5316\u6570\u636e\uff0c\u5bf9\u7ed3\u6784\u5316\u77e5\u8bc6\uff08\u5982\u77e5\u8bc6\u56fe\u8c31\uff09\u5904\u7406\u80fd\u529b\u6709\u9650\uff0c\u4e14\u5f53\u524d\u56fe\u8c31\u68c0\u7d22\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u6574\u4f53\u7ed3\u6784\u5e76\u63a7\u5236\u7cbe\u5ea6\u3002", "method": "GRAIL\u6846\u67b6\u901a\u8fc7LLM\u5f15\u5bfc\u7684\u968f\u673a\u63a2\u7d22\u548c\u8def\u5f84\u8fc7\u6ee4\u751f\u6210\u63a8\u7406\u8f68\u8ff9\uff0c\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u5b66\u4e60\u52a8\u6001\u51b3\u7b56\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u8fc7\u7a0b\u76d1\u7763\u5956\u52b1\u5e73\u8861\u68c0\u7d22\u7cbe\u5ea6\u4e0e\u7b80\u6d01\u6027\u3002", "result": "\u5728\u4e09\u4e2a\u77e5\u8bc6\u56fe\u8c31\u95ee\u7b54\u6570\u636e\u96c6\u4e0a\uff0cGRAIL\u5e73\u5747\u51c6\u786e\u7387\u63d0\u534721.01%\uff0cF1\u503c\u63d0\u534722.43%\u3002", "conclusion": "GRAIL\u901a\u8fc7\u4ea4\u4e92\u5f0f\u68c0\u7d22\u8303\u5f0f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u56fe\u8c31\u68c0\u7d22\u7684\u63a8\u7406\u6027\u80fd\uff0c\u4e3a\u7ed3\u6784\u5316\u77e5\u8bc6\u5904\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.05543", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.05543", "abs": "https://arxiv.org/abs/2508.05543", "authors": ["Wenbo Li", "Guanting Chen", "Tao Zhao", "Jiyao Wang", "Tianxin Hu", "Yuwen Liao", "Weixiang Guo", "Shenghai Yuan"], "title": "CleanUpBench: Embodied Sweeping and Grasping Benchmark", "comment": null, "summary": "Embodied AI benchmarks have advanced navigation, manipulation, and reasoning,\nbut most target complex humanoid agents or large-scale simulations that are far\nfrom real-world deployment. In contrast, mobile cleaning robots with dual mode\ncapabilities, such as sweeping and grasping, are rapidly emerging as realistic\nand commercially viable platforms. However, no benchmark currently exists that\nsystematically evaluates these agents in structured, multi-target cleaning\ntasks, revealing a critical gap between academic research and real-world\napplications. We introduce CleanUpBench, a reproducible and extensible\nbenchmark for evaluating embodied agents in realistic indoor cleaning\nscenarios. Built on NVIDIA Isaac Sim, CleanUpBench simulates a mobile service\nrobot equipped with a sweeping mechanism and a six-degree-of-freedom robotic\narm, enabling interaction with heterogeneous objects. The benchmark includes\nmanually designed environments and one procedurally generated layout to assess\ngeneralization, along with a comprehensive evaluation suite covering task\ncompletion, spatial efficiency, motion quality, and control performance. To\nsupport comparative studies, we provide baseline agents based on heuristic\nstrategies and map-based planning. CleanUpBench bridges the gap between\nlow-level skill evaluation and full-scene testing, offering a scalable testbed\nfor grounded, embodied intelligence in everyday settings.", "AI": {"tldr": "CleanUpBench\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u79fb\u52a8\u6e05\u6d01\u673a\u5668\u4eba\u5728\u73b0\u5b9e\u5ba4\u5185\u6e05\u6d01\u4efb\u52a1\u4e2d\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u586b\u8865\u4e86\u5b66\u672f\u7814\u7a76\u4e0e\u5b9e\u9645\u5e94\u7528\u4e4b\u95f4\u7684\u7a7a\u767d\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u591a\u9488\u5bf9\u590d\u6742\u4eba\u5f62\u4ee3\u7406\u6216\u5927\u89c4\u6a21\u6a21\u62df\uff0c\u800c\u7f3a\u4e4f\u5bf9\u73b0\u5b9e\u53ef\u884c\u7684\u79fb\u52a8\u6e05\u6d01\u673a\u5668\u4eba\u7684\u7cfb\u7edf\u8bc4\u4f30\u3002", "method": "\u57fa\u4e8eNVIDIA Isaac Sim\u6784\u5efa\uff0c\u6a21\u62df\u914d\u5907\u6e05\u626b\u673a\u5236\u548c\u516d\u81ea\u7531\u5ea6\u673a\u68b0\u81c2\u7684\u673a\u5668\u4eba\uff0c\u63d0\u4f9b\u624b\u52a8\u8bbe\u8ba1\u548c\u7a0b\u5e8f\u751f\u6210\u7684\u73af\u5883\uff0c\u4ee5\u53ca\u5168\u9762\u7684\u8bc4\u4f30\u6307\u6807\u3002", "result": "CleanUpBench\u652f\u6301\u4efb\u52a1\u5b8c\u6210\u5ea6\u3001\u7a7a\u95f4\u6548\u7387\u3001\u8fd0\u52a8\u8d28\u91cf\u548c\u63a7\u5236\u6027\u80fd\u7684\u8bc4\u4f30\uff0c\u5e76\u63d0\u4f9b\u57fa\u4e8e\u542f\u53d1\u5f0f\u7b56\u7565\u548c\u5730\u56fe\u89c4\u5212\u7684\u57fa\u7ebf\u4ee3\u7406\u3002", "conclusion": "CleanUpBench\u4e3a\u65e5\u5e38\u573a\u666f\u4e2d\u7684\u5177\u4f53\u5316\u667a\u80fd\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u8fde\u63a5\u4e86\u4f4e\u5c42\u6b21\u6280\u80fd\u8bc4\u4f30\u4e0e\u5168\u573a\u666f\u6d4b\u8bd5\u3002"}}
{"id": "2508.05508", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05508", "abs": "https://arxiv.org/abs/2508.05508", "authors": ["Roshita Bhonsle", "Rishav Dutta", "Sneha Vavilapalli", "Harsh Seth", "Abubakarr Jaye", "Yapei Chang", "Mukund Rungta", "Emmanuel Aboah Boateng", "Sadid Hasan", "Ehi Nosakhare", "Soundar Srinivasan"], "title": "Auto-Eval Judge: Towards a General Agentic Framework for Task Completion Evaluation", "comment": null, "summary": "The increasing adoption of foundation models as agents across diverse domains\nnecessitates a robust evaluation framework. Current methods, such as\nLLM-as-a-Judge, focus only on final outputs, overlooking the step-by-step\nreasoning that drives agentic decision-making. Meanwhile, existing\nAgent-as-a-Judge systems, where one agent evaluates another's task completion,\nare typically designed for narrow, domain-specific settings. To address this\ngap, we propose a generalizable, modular framework for evaluating agent task\ncompletion independent of the task domain. The framework emulates human-like\nevaluation by decomposing tasks into sub-tasks and validating each step using\navailable information, such as the agent's output and reasoning. Each module\ncontributes to a specific aspect of the evaluation process, and their outputs\nare aggregated to produce a final verdict on task completion. We validate our\nframework by evaluating the Magentic-One Actor Agent on two benchmarks, GAIA\nand BigCodeBench. Our Judge Agent predicts task success with closer agreement\nto human evaluations, achieving 4.76% and 10.52% higher alignment accuracy,\nrespectively, compared to the GPT-4o based LLM-as-a-Judge baseline. This\ndemonstrates the potential of our proposed general-purpose evaluation\nframework.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u3001\u6a21\u5757\u5316\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u4ee3\u7406\u4efb\u52a1\u5b8c\u6210\u60c5\u51b5\uff0c\u901a\u8fc7\u5206\u89e3\u4efb\u52a1\u5e76\u9a8c\u8bc1\u6bcf\u4e00\u6b65\uff0c\u63d0\u9ad8\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u7684\u4e00\u81f4\u6027\u3002", "motivation": "\u5f53\u524d\u8bc4\u4f30\u65b9\u6cd5\uff08\u5982LLM-as-a-Judge\uff09\u4ec5\u5173\u6ce8\u6700\u7ec8\u8f93\u51fa\uff0c\u5ffd\u7565\u4e86\u9010\u6b65\u63a8\u7406\u8fc7\u7a0b\uff1b\u73b0\u6709Agent-as-a-Judge\u7cfb\u7edf\u5c40\u9650\u4e8e\u7279\u5b9a\u9886\u57df\u3002", "method": "\u8bbe\u8ba1\u6a21\u5757\u5316\u6846\u67b6\uff0c\u5c06\u4efb\u52a1\u5206\u89e3\u4e3a\u5b50\u4efb\u52a1\uff0c\u9a8c\u8bc1\u6bcf\u4e00\u6b65\uff0c\u5e76\u805a\u5408\u6a21\u5757\u8f93\u51fa\u4ee5\u751f\u6210\u6700\u7ec8\u8bc4\u4f30\u7ed3\u679c\u3002", "result": "\u5728GAIA\u548cBigCodeBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cJudge Agent\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u7684\u4e00\u81f4\u6027\u5206\u522b\u6bd4GPT-4o\u57fa\u7ebf\u9ad84.76%\u548c10.52%\u3002", "conclusion": "\u8be5\u6846\u67b6\u5c55\u793a\u4e86\u901a\u7528\u8bc4\u4f30\u65b9\u6cd5\u7684\u6f5c\u529b\uff0c\u80fd\u591f\u66f4\u51c6\u786e\u5730\u9884\u6d4b\u4efb\u52a1\u6210\u529f\u3002"}}
{"id": "2508.05584", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.05584", "abs": "https://arxiv.org/abs/2508.05584", "authors": ["Van Cuong Pham", "Minh Hai Tran", "Phuc Anh Nguyen", "Ngoc Son Vu", "Nga Nguyen Thi"], "title": "Robust adaptive fuzzy sliding mode control for trajectory tracking for of cylindrical manipulator", "comment": null, "summary": "This research proposes a robust adaptive fuzzy sliding mode control (AFSMC)\napproach to enhance the trajectory tracking performance of cylindrical robotic\nmanipulators, extensively utilized in applications such as CNC and 3D printing.\nThe proposed approach integrates fuzzy logic with sliding mode control (SMC) to\nbolster adaptability and robustness, with fuzzy logic approximating the\nuncertain dynamics of the system, while SMC ensures strong performance.\nSimulation results in MATLAB/Simulink demonstrate that AFSMC significantly\nimproves trajectory tracking accuracy, stability, and disturbance rejection\ncompared to traditional methods. This research underscores the effectiveness of\nAFSMC in controlling robotic manipulators, contributing to enhanced precision\nin industrial robotic applications.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u9c81\u68d2\u81ea\u9002\u5e94\u6a21\u7cca\u6ed1\u6a21\u63a7\u5236\uff08AFSMC\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u5706\u67f1\u5f62\u673a\u5668\u4eba\u673a\u68b0\u81c2\u7684\u8f68\u8ff9\u8ddf\u8e2a\u6027\u80fd\u3002", "motivation": "\u5706\u67f1\u5f62\u673a\u5668\u4eba\u673a\u68b0\u81c2\u5728CNC\u548c3D\u6253\u5370\u7b49\u5e94\u7528\u4e2d\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u4f20\u7edf\u63a7\u5236\u65b9\u6cd5\u5728\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u7ed3\u5408\u6a21\u7cca\u903b\u8f91\u4e0e\u6ed1\u6a21\u63a7\u5236\uff08SMC\uff09\uff0c\u6a21\u7cca\u903b\u8f91\u7528\u4e8e\u8fd1\u4f3c\u7cfb\u7edf\u7684\u4e0d\u786e\u5b9a\u6027\u52a8\u6001\uff0cSMC\u786e\u4fdd\u5f3a\u9c81\u68d2\u6027\u3002", "result": "MATLAB/Simulink\u4eff\u771f\u663e\u793a\uff0cAFSMC\u5728\u8f68\u8ff9\u8ddf\u8e2a\u7cbe\u5ea6\u3001\u7a33\u5b9a\u6027\u548c\u6297\u5e72\u6270\u80fd\u529b\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "AFSMC\u80fd\u6709\u6548\u63d0\u5347\u673a\u5668\u4eba\u673a\u68b0\u81c2\u7684\u63a7\u5236\u6027\u80fd\uff0c\u4e3a\u5de5\u4e1a\u5e94\u7528\u63d0\u4f9b\u66f4\u9ad8\u7cbe\u5ea6\u3002"}}
{"id": "2508.05513", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.05513", "abs": "https://arxiv.org/abs/2508.05513", "authors": ["Meryem Yilmaz Soylu", "Adrian Gallard", "Jeonghyun Lee", "Gayane Grigoryan", "Rushil Desai", "Stephen Harmon"], "title": "Streamlining Admission with LOR Insights: AI-Based Leadership Assessment in Online Master's Program", "comment": null, "summary": "Letters of recommendation (LORs) provide valuable insights into candidates'\ncapabilities and experiences beyond standardized test scores. However,\nreviewing these text-heavy materials is time-consuming and labor-intensive. To\naddress this challenge and support the admission committee in providing\nfeedback for students' professional growth, our study introduces LORI: LOR\nInsights, a novel AI-based detection tool for assessing leadership skills in\nLORs submitted by online master's program applicants. By employing natural\nlanguage processing and leveraging large language models using RoBERTa and\nLLAMA, we seek to identify leadership attributes such as teamwork,\ncommunication, and innovation. Our latest RoBERTa model achieves a weighted F1\nscore of 91.6%, a precision of 92.4%, and a recall of 91.6%, showing a strong\nlevel of consistency in our test data. With the growing importance of\nleadership skills in the STEM sector, integrating LORI into the graduate\nadmissions process is crucial for accurately assessing applicants' leadership\ncapabilities. This approach not only streamlines the admissions process but\nalso automates and ensures a more comprehensive evaluation of candidates'\ncapabilities.", "AI": {"tldr": "LORI\u662f\u4e00\u79cd\u57fa\u4e8eAI\u7684\u5de5\u5177\uff0c\u7528\u4e8e\u4ece\u63a8\u8350\u4fe1\u4e2d\u81ea\u52a8\u68c0\u6d4b\u9886\u5bfc\u529b\u6280\u80fd\uff0c\u91c7\u7528RoBERTa\u548cLLAMA\u6a21\u578b\uff0c\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u63a8\u8350\u4fe1\u8bc4\u4f30\u8017\u65f6\u4e14\u4e3b\u89c2\uff0cLORI\u65e8\u5728\u81ea\u52a8\u5316\u8fd9\u4e00\u8fc7\u7a0b\uff0c\u63d0\u4f9b\u5ba2\u89c2\u53cd\u9988\u3002", "method": "\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff08RoBERTa\u548cLLAMA\uff09\u8bc6\u522b\u9886\u5bfc\u529b\u5c5e\u6027\u3002", "result": "RoBERTa\u6a21\u578b\u5728\u6d4b\u8bd5\u6570\u636e\u4e2d\u8868\u73b0\u4f18\u5f02\uff0cF1\u5206\u657091.6%\uff0c\u7cbe\u5ea692.4%\uff0c\u53ec\u56de\u738791.6%\u3002", "conclusion": "LORI\u53ef\u4f18\u5316\u7814\u7a76\u751f\u62db\u751f\u6d41\u7a0b\uff0c\u66f4\u5168\u9762\u8bc4\u4f30\u7533\u8bf7\u8005\u7684\u9886\u5bfc\u529b\u3002"}}
{"id": "2508.05634", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.05634", "abs": "https://arxiv.org/abs/2508.05634", "authors": ["Jianpeng Yao", "Xiaopan Zhang", "Yu Xia", "Zejin Wang", "Amit K. Roy-Chowdhury", "Jiachen Li"], "title": "Towards Generalizable Safety in Crowd Navigation via Conformal Uncertainty Handling", "comment": "9th Conference on Robot Learning (CoRL 2025); Project website:\n  https://gen-safe-nav.github.io/. arXiv admin note: text overlap with\n  arXiv:2407.17460", "summary": "Mobile robots navigating in crowds trained using reinforcement learning are\nknown to suffer performance degradation when faced with out-of-distribution\nscenarios. We propose that by properly accounting for the uncertainties of\npedestrians, a robot can learn safe navigation policies that are robust to\ndistribution shifts. Our method augments agent observations with prediction\nuncertainty estimates generated by adaptive conformal inference, and it uses\nthese estimates to guide the agent's behavior through constrained reinforcement\nlearning. The system helps regulate the agent's actions and enables it to adapt\nto distribution shifts. In the in-distribution setting, our approach achieves a\n96.93% success rate, which is over 8.80% higher than the previous\nstate-of-the-art baselines with over 3.72 times fewer collisions and 2.43 times\nfewer intrusions into ground-truth human future trajectories. In three\nout-of-distribution scenarios, our method shows much stronger robustness when\nfacing distribution shifts in velocity variations, policy changes, and\ntransitions from individual to group dynamics. We deploy our method on a real\nrobot, and experiments show that the robot makes safe and robust decisions when\ninteracting with both sparse and dense crowds. Our code and videos are\navailable on https://gen-safe-nav.github.io/.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u8003\u8651\u884c\u4eba\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u6765\u589e\u5f3a\u79fb\u52a8\u673a\u5668\u4eba\u5728\u4eba\u7fa4\u5bfc\u822a\u4e2d\u9c81\u68d2\u6027\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528\u81ea\u9002\u5e94\u5171\u5f62\u63a8\u7406\u548c\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u79fb\u52a8\u673a\u5668\u4eba\u5728\u9762\u5bf9\u5206\u5e03\u5916\u573a\u666f\u65f6\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u81ea\u9002\u5e94\u5171\u5f62\u63a8\u7406\u751f\u6210\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff0c\u5e76\u5229\u7528\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\u5f15\u5bfc\u673a\u5668\u4eba\u884c\u4e3a\u3002", "result": "\u5728\u5206\u5e03\u5185\u573a\u666f\u4e2d\uff0c\u6210\u529f\u7387\u63d0\u53478.80%\uff0c\u78b0\u649e\u548c\u4fb5\u5165\u8f68\u8ff9\u6b21\u6570\u663e\u8457\u51cf\u5c11\uff1b\u5728\u5206\u5e03\u5916\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u771f\u5b9e\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u5b89\u5168\u6027\u548c\u9c81\u68d2\u6027\uff0c\u9002\u7528\u4e8e\u7a00\u758f\u548c\u5bc6\u96c6\u4eba\u7fa4\u3002"}}
{"id": "2508.05557", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05557", "abs": "https://arxiv.org/abs/2508.05557", "authors": ["Rui Lu", "Jinhe Bi", "Yunpu Ma", "Feng Xiao", "Yuntao Du", "Yijun Tian"], "title": "MV-Debate: Multi-view Agent Debate with Dynamic Reflection Gating for Multimodal Harmful Content Detection in Social Media", "comment": null, "summary": "Social media has evolved into a complex multimodal environment where text,\nimages, and other signals interact to shape nuanced meanings, often concealing\nharmful intent. Identifying such intent, whether sarcasm, hate speech, or\nmisinformation, remains challenging due to cross-modal contradictions, rapid\ncultural shifts, and subtle pragmatic cues. To address these challenges, we\npropose MV-Debate, a multi-view agent debate framework with dynamic reflection\ngating for unified multimodal harmful content detection. MV-Debate assembles\nfour complementary debate agents, a surface analyst, a deep reasoner, a\nmodality contrast, and a social contextualist, to analyze content from diverse\ninterpretive perspectives. Through iterative debate and reflection, the agents\nrefine responses under a reflection-gain criterion, ensuring both accuracy and\nefficiency. Experiments on three benchmark datasets demonstrate that MV-Debate\nsignificantly outperforms strong single-model and existing multi-agent debate\nbaselines. This work highlights the promise of multi-agent debate in advancing\nreliable social intent detection in safety-critical online contexts.", "AI": {"tldr": "MV-Debate\u662f\u4e00\u79cd\u591a\u89c6\u89d2\u4ee3\u7406\u8fa9\u8bba\u6846\u67b6\uff0c\u7528\u4e8e\u7edf\u4e00\u591a\u6a21\u6001\u6709\u5bb3\u5185\u5bb9\u68c0\u6d4b\uff0c\u901a\u8fc7\u52a8\u6001\u53cd\u601d\u95e8\u63a7\u63d0\u5347\u51c6\u786e\u6027\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u4e2d\u591a\u6a21\u6001\u5185\u5bb9\u7684\u590d\u6742\u6027\u4f7f\u5f97\u6709\u5bb3\u610f\u56fe\uff08\u5982\u8bbd\u523a\u3001\u4ec7\u6068\u8a00\u8bba\u6216\u865a\u5047\u4fe1\u606f\uff09\u96be\u4ee5\u8bc6\u522b\uff0c\u9700\u89e3\u51b3\u8de8\u6a21\u6001\u77db\u76fe\u548c\u6587\u5316\u5feb\u901f\u53d8\u5316\u7b49\u95ee\u9898\u3002", "method": "MV-Debate\u6574\u5408\u56db\u79cd\u4e92\u8865\u4ee3\u7406\uff08\u8868\u9762\u5206\u6790\u5e08\u3001\u6df1\u5ea6\u63a8\u7406\u8005\u3001\u6a21\u6001\u5bf9\u6bd4\u8005\u548c\u793e\u4f1a\u60c5\u5883\u5206\u6790\u8005\uff09\uff0c\u901a\u8fc7\u8fed\u4ee3\u8fa9\u8bba\u548c\u52a8\u6001\u53cd\u601d\u95e8\u63a7\u4f18\u5316\u68c0\u6d4b\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0cMV-Debate\u663e\u8457\u4f18\u4e8e\u5355\u6a21\u578b\u548c\u73b0\u6709\u591a\u4ee3\u7406\u8fa9\u8bba\u57fa\u7ebf\u3002", "conclusion": "\u591a\u4ee3\u7406\u8fa9\u8bba\u6846\u67b6\u5728\u63d0\u5347\u793e\u4ea4\u5a92\u4f53\u610f\u56fe\u68c0\u6d4b\u7684\u53ef\u9760\u6027\u65b9\u9762\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2508.05635", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.05635", "abs": "https://arxiv.org/abs/2508.05635", "authors": ["Yue Liao", "Pengfei Zhou", "Siyuan Huang", "Donglin Yang", "Shengcong Chen", "Yuxin Jiang", "Yue Hu", "Jingbin Cai", "Si Liu", "Jianlan Luo", "Liliang Chen", "Shuicheng Yan", "Maoqing Yao", "Guanghui Ren"], "title": "Genie Envisioner: A Unified World Foundation Platform for Robotic Manipulation", "comment": "https://genie-envisioner.github.io/", "summary": "We introduce Genie Envisioner (GE), a unified world foundation platform for\nrobotic manipulation that integrates policy learning, evaluation, and\nsimulation within a single video-generative framework. At its core, GE-Base is\na large-scale, instruction-conditioned video diffusion model that captures the\nspatial, temporal, and semantic dynamics of real-world robotic interactions in\na structured latent space. Built upon this foundation, GE-Act maps latent\nrepresentations to executable action trajectories through a lightweight,\nflow-matching decoder, enabling precise and generalizable policy inference\nacross diverse embodiments with minimal supervision. To support scalable\nevaluation and training, GE-Sim serves as an action-conditioned neural\nsimulator, producing high-fidelity rollouts for closed-loop policy development.\nThe platform is further equipped with EWMBench, a standardized benchmark suite\nmeasuring visual fidelity, physical consistency, and instruction-action\nalignment. Together, these components establish Genie Envisioner as a scalable\nand practical foundation for instruction-driven, general-purpose embodied\nintelligence. All code, models, and benchmarks will be released publicly.", "AI": {"tldr": "Genie Envisioner (GE) \u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u5e73\u53f0\uff0c\u96c6\u6210\u4e86\u7b56\u7565\u5b66\u4e60\u3001\u8bc4\u4f30\u548c\u4eff\u771f\uff0c\u901a\u8fc7\u89c6\u9891\u751f\u6210\u6846\u67b6\u5b9e\u73b0\u3002", "motivation": "\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u5b9e\u7528\u7684\u57fa\u7840\u5e73\u53f0\uff0c\u652f\u6301\u6307\u4ee4\u9a71\u52a8\u7684\u901a\u7528\u667a\u80fd\u4f53\u3002", "method": "GE-Base \u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u7684\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0cGE-Act \u901a\u8fc7\u8f7b\u91cf\u7ea7\u89e3\u7801\u5668\u751f\u6210\u53ef\u6267\u884c\u52a8\u4f5c\uff0cGE-Sim \u4f5c\u4e3a\u795e\u7ecf\u6a21\u62df\u5668\u652f\u6301\u95ed\u73af\u7b56\u7565\u5f00\u53d1\u3002", "result": "\u5e73\u53f0\u5177\u5907\u9ad8\u4fdd\u771f\u4eff\u771f\u80fd\u529b\uff0c\u652f\u6301\u8de8\u591a\u6837\u5316\u7684\u673a\u5668\u4eba\u5b9e\u73b0\u901a\u7528\u7b56\u7565\u63a8\u65ad\u3002", "conclusion": "Genie Envisioner \u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u3001\u5b9e\u7528\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u57fa\u7840\u5e73\u53f0\uff0c\u76f8\u5173\u4ee3\u7801\u548c\u6a21\u578b\u5c06\u516c\u5f00\u3002"}}
{"id": "2508.05619", "categories": ["cs.AI", "nlin.AO", "physics.bio-ph", "physics.comp-ph", "physics.hist-ph"], "pdf": "https://arxiv.org/pdf/2508.05619", "abs": "https://arxiv.org/abs/2508.05619", "authors": ["Bo Wen"], "title": "The Missing Reward: Active Inference in the Era of Experience", "comment": null, "summary": "This paper argues that Active Inference (AIF) provides a crucial foundation\nfor developing autonomous AI agents capable of learning from experience without\ncontinuous human reward engineering. As AI systems begin to exhaust\nhigh-quality training data and rely on increasingly large human workforces for\nreward design, the current paradigm faces significant scalability challenges\nthat could impede progress toward genuinely autonomous intelligence. The\nproposal for an ``Era of Experience,'' where agents learn from self-generated\ndata, is a promising step forward. However, this vision still depends on\nextensive human engineering of reward functions, effectively shifting the\nbottleneck from data curation to reward curation. This highlights what we\nidentify as the \\textbf{grounded-agency gap}: the inability of contemporary AI\nsystems to autonomously formulate, adapt, and pursue objectives in response to\nchanging circumstances. We propose that AIF can bridge this gap by replacing\nexternal reward signals with an intrinsic drive to minimize free energy,\nallowing agents to naturally balance exploration and exploitation through a\nunified Bayesian objective. By integrating Large Language Models as generative\nworld models with AIF's principled decision-making framework, we can create\nagents that learn efficiently from experience while remaining aligned with\nhuman values. This synthesis offers a compelling path toward AI systems that\ncan develop autonomously while adhering to both computational and physical\nconstraints.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e3b\u52a8\u63a8\u7406\uff08AIF\uff09\u4e3a\u89e3\u51b3\u81ea\u4e3bAI\u4ee3\u7406\u5b66\u4e60\u4e2d\u7684\u5956\u52b1\u5de5\u7a0b\u74f6\u9888\u95ee\u9898\u63d0\u4f9b\u4e86\u5173\u952e\u57fa\u7840\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u81ea\u7531\u80fd\u91cf\u7684\u5185\u5728\u9a71\u52a8\u529b\u66ff\u4ee3\u5916\u90e8\u5956\u52b1\u4fe1\u53f7\u3002", "motivation": "\u5f53\u524dAI\u7cfb\u7edf\u4f9d\u8d56\u5927\u91cf\u4eba\u5de5\u8bbe\u8ba1\u7684\u5956\u52b1\u51fd\u6570\uff0c\u9762\u4e34\u53ef\u6269\u5c55\u6027\u6311\u6218\uff0c\u963b\u788d\u4e86\u771f\u6b63\u81ea\u4e3b\u667a\u80fd\u7684\u53d1\u5c55\u3002", "method": "\u7ed3\u5408\u4e3b\u52a8\u63a8\u7406\uff08AIF\uff09\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u5229\u7528\u5185\u5728\u9a71\u52a8\u529b\uff08\u6700\u5c0f\u5316\u81ea\u7531\u80fd\u91cf\uff09\u66ff\u4ee3\u5916\u90e8\u5956\u52b1\u4fe1\u53f7\uff0c\u5b9e\u73b0\u81ea\u4e3b\u5b66\u4e60\u548c\u76ee\u6807\u9002\u5e94\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u80fd\u591f\u9ad8\u6548\u4ece\u7ecf\u9a8c\u4e2d\u5b66\u4e60\u5e76\u4fdd\u6301\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u4e00\u81f4\u7684AI\u4ee3\u7406\u6846\u67b6\u3002", "conclusion": "\u4e3b\u52a8\u63a8\u7406\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u7ed3\u5408\u4e3a\u81ea\u4e3bAI\u7cfb\u7edf\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u53ef\u884c\u8def\u5f84\uff0c\u89e3\u51b3\u4e86\u5956\u52b1\u5de5\u7a0b\u74f6\u9888\u95ee\u9898\u3002"}}
{"id": "2508.05622", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.05622", "abs": "https://arxiv.org/abs/2508.05622", "authors": ["Yu Yuan", "Lili Zhao", "Wei Chen", "Guangting Zheng", "Kai Zhang", "Mengdi Zhang", "Qi Liu"], "title": "Simulating Human-Like Learning Dynamics with LLM-Empowered Agents", "comment": null, "summary": "Capturing human learning behavior based on deep learning methods has become a\nmajor research focus in both psychology and intelligent systems. Recent\napproaches rely on controlled experiments or rule-based models to explore\ncognitive processes. However, they struggle to capture learning dynamics, track\nprogress over time, or provide explainability. To address these challenges, we\nintroduce LearnerAgent, a novel multi-agent framework based on Large Language\nModels (LLMs) to simulate a realistic teaching environment. To explore\nhuman-like learning dynamics, we construct learners with psychologically\ngrounded profiles-such as Deep, Surface, and Lazy-as well as a persona-free\nGeneral Learner to inspect the base LLM's default behavior. Through weekly\nknowledge acquisition, monthly strategic choices, periodic tests, and peer\ninteraction, we can track the dynamic learning progress of individual learners\nover a full-year journey. Our findings are fourfold: 1) Longitudinal analysis\nreveals that only Deep Learner achieves sustained cognitive growth. Our\nspecially designed \"trap questions\" effectively diagnose Surface Learner's\nshallow knowledge. 2) The behavioral and cognitive patterns of distinct\nlearners align closely with their psychological profiles. 3) Learners'\nself-concept scores evolve realistically, with the General Learner developing\nsurprisingly high self-efficacy despite its cognitive limitations. 4)\nCritically, the default profile of base LLM is a \"diligent but brittle Surface\nLearner\"-an agent that mimics the behaviors of a good student but lacks true,\ngeneralizable understanding. Extensive simulation experiments demonstrate that\nLearnerAgent aligns well with real scenarios, yielding more insightful findings\nabout LLMs' behavior.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faLearnerAgent\u6846\u67b6\uff0c\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u6a21\u62df\u771f\u5b9e\u6559\u5b66\u73af\u5883\uff0c\u5206\u6790\u4e0d\u540c\u5b66\u4e60\u8005\u7684\u52a8\u6001\u5b66\u4e60\u884c\u4e3a\uff0c\u63ed\u793a\u5176\u8ba4\u77e5\u6210\u957f\u4e0e\u5fc3\u7406\u7279\u5f81\u7684\u5173\u7cfb\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u6355\u6349\u5b66\u4e60\u52a8\u6001\u6216\u63d0\u4f9b\u89e3\u91ca\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u6846\u67b6\u6765\u6a21\u62df\u771f\u5b9e\u5b66\u4e60\u73af\u5883\u5e76\u5206\u6790\u5b66\u4e60\u884c\u4e3a\u3002", "method": "\u6784\u5efa\u591a\u667a\u80fd\u4f53\u6846\u67b6LearnerAgent\uff0c\u57fa\u4e8e\u5fc3\u7406\u7279\u5f81\u8bbe\u8ba1\u5b66\u4e60\u8005\u7c7b\u578b\uff08\u5982\u6df1\u5ea6\u3001\u8868\u9762\u3001\u61d2\u60f0\u5b66\u4e60\u8005\uff09\uff0c\u901a\u8fc7\u77e5\u8bc6\u83b7\u53d6\u3001\u6d4b\u8bd5\u548c\u4e92\u52a8\u8ddf\u8e2a\u5b66\u4e60\u52a8\u6001\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff1a1\uff09\u6df1\u5ea6\u5b66\u4e60\u8005\u6301\u7eed\u8ba4\u77e5\u6210\u957f\uff1b2\uff09\u5b66\u4e60\u8005\u884c\u4e3a\u4e0e\u5fc3\u7406\u7279\u5f81\u4e00\u81f4\uff1b3\uff09\u81ea\u6211\u6548\u80fd\u611f\u6f14\u5316\u771f\u5b9e\uff1b4\uff09\u57fa\u7840LLM\u9ed8\u8ba4\u884c\u4e3a\u4e3a\u201c\u52e4\u594b\u4f46\u8106\u5f31\u7684\u8868\u9762\u5b66\u4e60\u8005\u201d\u3002", "conclusion": "LearnerAgent\u80fd\u6709\u6548\u6a21\u62df\u771f\u5b9e\u5b66\u4e60\u573a\u666f\uff0c\u4e3a\u7406\u89e3LLM\u884c\u4e3a\u63d0\u4f9b\u65b0\u89c6\u89d2\u3002"}}
