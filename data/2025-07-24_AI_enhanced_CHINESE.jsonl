{"id": "2507.17012", "categories": ["cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2507.17012", "abs": "https://arxiv.org/abs/2507.17012", "authors": ["Zhihan Zhang", "Alexander Metzger", "Yuxuan Mei", "Felix H\u00e4hnlein", "Zachary Englhardt", "Tingyu Cheng", "Gregory D. Abowd", "Shwetak Patel", "Adriana Schulz", "Vikram Iyer"], "title": "Towards Autonomous Sustainability Assessment via Multimodal AI Agents", "comment": null, "summary": "Interest in sustainability information has surged in recent years. However,\nthe data required for a life cycle assessment (LCA) that maps the materials and\nprocesses from product manufacturing to disposal into environmental impacts\n(EI) are often unavailable. Here we reimagine conventional LCA by introducing\nmultimodal AI agents that emulate interactions between LCA experts and\nstakeholders like product managers and engineers to calculate the\ncradle-to-gate (production) carbon emissions of electronic devices. The AI\nagents iteratively generate a detailed life-cycle inventory leveraging a custom\ndata abstraction and software tools that extract information from online text\nand images from repair communities and government certifications. This approach\nreduces weeks or months of expert time to under one minute and closes data\navailability gaps while yielding carbon footprint estimates within 19% of\nexpert LCAs with zero proprietary data. Additionally, we develop a method to\ndirectly estimate EI by comparing an input to a cluster of products with\nsimilar descriptions and known carbon footprints. This runs in 3 ms on a laptop\nwith a MAPE of 12.28% on electronic products. Further, we develop a data-driven\nmethod to generate emission factors. We use the properties of an unknown\nmaterial to represent it as a weighted sum of emission factors for similar\nmaterials. Compared to human experts picking the closest LCA database entry,\nthis improves MAPE by 120.26%. We analyze the data and compute scaling of this\napproach and discuss its implications for future LCA workflows.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u591a\u6a21\u6001AI\u4ee3\u7406\u7684\u521b\u65b0\u751f\u547d\u5468\u671f\u8bc4\u4f30(LCA)\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u62df\u4e13\u5bb6\u4e0e\u5229\u76ca\u76f8\u5173\u8005\u7684\u4ea4\u4e92\uff0c\u81ea\u52a8\u8ba1\u7b97\u7535\u5b50\u8bbe\u5907\u7684\u78b3\u6392\u653e\uff0c\u5c06\u4f20\u7edf\u9700\u8981\u6570\u5468\u6216\u6570\u6708\u7684\u4e13\u5bb6\u5de5\u4f5c\u65f6\u95f4\u7f29\u77ed\u81f31\u5206\u949f\u4ee5\u5185\u3002", "motivation": "\u4f20\u7edfLCA\u8bc4\u4f30\u9762\u4e34\u6570\u636e\u53ef\u7528\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4ece\u4ea7\u54c1\u5236\u9020\u5230\u5904\u7f6e\u7684\u6750\u6599\u548c\u5de5\u827a\u8fc7\u7a0b\u7684\u73af\u5883\u5f71\u54cd\u6570\u636e\u5f80\u5f80\u7f3a\u5931\uff0c\u800c\u8fd1\u5e74\u6765\u5bf9\u53ef\u6301\u7eed\u6027\u4fe1\u606f\u7684\u9700\u6c42\u6fc0\u589e\uff0c\u8feb\u5207\u9700\u8981\u4e00\u79cd\u80fd\u591f\u89e3\u51b3\u6570\u636e\u7f3a\u53e3\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1\u591a\u6a21\u6001AI\u4ee3\u7406\u7cfb\u7edf\uff0c\u6a21\u62dfLCA\u4e13\u5bb6\u4e0e\u4ea7\u54c1\u7ecf\u7406\u3001\u5de5\u7a0b\u5e08\u7b49\u5229\u76ca\u76f8\u5173\u8005\u7684\u4ea4\u4e92\uff1b\u91c7\u7528\u81ea\u5b9a\u4e49\u6570\u636e\u62bd\u8c61\u548c\u8f6f\u4ef6\u5de5\u5177\u4ece\u5728\u7ebf\u6587\u672c\u3001\u7ef4\u4fee\u793e\u533a\u56fe\u50cf\u548c\u653f\u5e9c\u8ba4\u8bc1\u4e2d\u63d0\u53d6\u4fe1\u606f\uff1b\u5f00\u53d1\u76f4\u63a5\u73af\u5883\u5f71\u54cd\u4f30\u7b97\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e0e\u76f8\u4f3c\u4ea7\u54c1\u96c6\u7fa4\u6bd4\u8f83\u8fdb\u884c\u78b3\u8db3\u8ff9\u4f30\u7b97\uff1b\u6784\u5efa\u6570\u636e\u9a71\u52a8\u7684\u6392\u653e\u56e0\u5b50\u751f\u6210\u65b9\u6cd5\uff0c\u4f7f\u7528\u672a\u77e5\u6750\u6599\u7684\u5c5e\u6027\u5c06\u5176\u8868\u793a\u4e3a\u76f8\u4f3c\u6750\u6599\u6392\u653e\u56e0\u5b50\u7684\u52a0\u6743\u548c\u3002", "result": "AI\u4ee3\u7406\u65b9\u6cd5\u5c06\u4e13\u5bb6\u5de5\u4f5c\u65f6\u95f4\u4ece\u6570\u5468/\u6570\u6708\u7f29\u77ed\u81f31\u5206\u949f\u4ee5\u5185\uff1b\u5728\u96f6\u4e13\u6709\u6570\u636e\u60c5\u51b5\u4e0b\uff0c\u78b3\u8db3\u8ff9\u4f30\u7b97\u7ed3\u679c\u4e0e\u4e13\u5bb6LCA\u76f8\u6bd4\u8bef\u5dee\u572819%\u4ee5\u5185\uff1b\u76f4\u63a5\u4f30\u7b97\u65b9\u6cd5\u5728\u7b14\u8bb0\u672c\u7535\u8111\u4e0a3\u6beb\u79d2\u5185\u5b8c\u6210\uff0c\u7535\u5b50\u4ea7\u54c1MAPE\u4e3a12.28%\uff1b\u6570\u636e\u9a71\u52a8\u6392\u653e\u56e0\u5b50\u751f\u6210\u65b9\u6cd5\u76f8\u6bd4\u4eba\u7c7b\u4e13\u5bb6\u9009\u62e9\u6700\u63a5\u8fd1LCA\u6570\u636e\u5e93\u6761\u76ee\u7684\u65b9\u6cd5\uff0cMAPE\u6539\u5584\u4e86120.26%\u3002", "conclusion": "\u7814\u7a76\u6210\u529f\u5f00\u53d1\u4e86\u4e00\u5957\u5b8c\u6574\u7684AI\u9a71\u52a8LCA\u8bc4\u4f30\u7cfb\u7edf\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8bc4\u4f30\u6548\u7387\u5e76\u89e3\u51b3\u4e86\u6570\u636e\u53ef\u7528\u6027\u95ee\u9898\uff0c\u4e3a\u672a\u6765LCA\u5de5\u4f5c\u6d41\u7a0b\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\uff0c\u5177\u6709\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u548c\u5b9e\u9645\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2507.17054", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17054", "abs": "https://arxiv.org/abs/2507.17054", "authors": ["Shao-Hung Chan", "Thomy Phan", "Jiaoyang Li", "Sven Koenig"], "title": "New Mechanisms in Flex Distribution for Bounded Suboptimal Multi-Agent Path Finding", "comment": "9 pages, 10 figures, International Symposium on Combinatorial Search,\n  2025", "summary": "Multi-Agent Path Finding (MAPF) is the problem of finding a set of\ncollision-free paths, one for each agent in a shared environment. Its objective\nis to minimize the sum of path costs (SOC), where the path cost of each agent\nis defined as the travel time from its start location to its target location.\nExplicit Estimation Conflict-Based Search (EECBS) is the leading algorithm for\nbounded-suboptimal MAPF, with the SOC of the solution being at most a\nuser-specified factor $w$ away from optimal. EECBS maintains sets of paths and\na lower bound $LB$ on the optimal SOC. Then, it iteratively selects a set of\npaths whose SOC is at most $w \\cdot LB$ and introduces constraints to resolve\ncollisions. For each path in a set, EECBS maintains a lower bound on its\noptimal path that satisfies constraints. By finding an individually\nbounded-suboptimal path with cost at most a threshold of $w$ times its lower\nbound, EECBS guarantees to find a bounded-suboptimal solution. To speed up\nEECBS, previous work uses flex distribution to increase the threshold. Though\nEECBS with flex distribution guarantees to find a bounded-suboptimal solution,\nincreasing the thresholds may push the SOC beyond $w \\cdot LB$, forcing EECBS\nto switch among different sets of paths instead of resolving collisions on a\nparticular set of paths, and thus reducing efficiency. To address this issue,\nwe propose Conflict-Based Flex Distribution that distributes flex in proportion\nto the number of collisions. We also estimate the delays needed to satisfy\nconstraints and propose Delay-Based Flex Distribution. On top of that, we\npropose Mixed-Strategy Flex Distribution, combining both in a hierarchical\nframework. We prove that EECBS with our new flex distribution mechanisms is\ncomplete and bounded-suboptimal. Our experiments show that our approaches\noutperform the original (greedy) flex distribution.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u591a\u667a\u80fd\u4f53\u8def\u5f84\u89c4\u5212(MAPF)\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u6539\u8fdb\u7684EECBS\u7b97\u6cd5\u4e2d\u7684flex\u5206\u914d\u673a\u5236\uff0c\u5305\u62ec\u57fa\u4e8e\u51b2\u7a81\u7684flex\u5206\u914d\u3001\u57fa\u4e8e\u5ef6\u8fdf\u7684flex\u5206\u914d\u548c\u6df7\u5408\u7b56\u7565flex\u5206\u914d\uff0c\u4ee5\u63d0\u9ad8\u7b97\u6cd5\u6548\u7387\u540c\u65f6\u4fdd\u6301\u6709\u754c\u6b21\u4f18\u89e3\u7684\u4fdd\u8bc1\u3002", "motivation": "\u73b0\u6709EECBS\u7b97\u6cd5\u4f7f\u7528\u8d2a\u5fc3flex\u5206\u914d\u867d\u7136\u80fd\u4fdd\u8bc1\u627e\u5230\u6709\u754c\u6b21\u4f18\u89e3\uff0c\u4f46\u589e\u52a0\u9608\u503c\u53ef\u80fd\u5bfc\u81f4\u89e3\u7684\u6210\u672c\u8d85\u51fa\u754c\u9650\uff0c\u8feb\u4f7f\u7b97\u6cd5\u5728\u4e0d\u540c\u8def\u5f84\u96c6\u5408\u95f4\u5207\u6362\u800c\u975e\u89e3\u51b3\u7279\u5b9a\u96c6\u5408\u7684\u51b2\u7a81\uff0c\u4ece\u800c\u964d\u4f4e\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e09\u79cd\u65b0\u7684flex\u5206\u914d\u673a\u5236\uff1a1) \u57fa\u4e8e\u51b2\u7a81\u7684flex\u5206\u914d(Conflict-Based Flex Distribution)\uff0c\u6309\u51b2\u7a81\u6570\u91cf\u6bd4\u4f8b\u5206\u914dflex\uff1b2) \u57fa\u4e8e\u5ef6\u8fdf\u7684flex\u5206\u914d(Delay-Based Flex Distribution)\uff0c\u4f30\u8ba1\u6ee1\u8db3\u7ea6\u675f\u6240\u9700\u7684\u5ef6\u8fdf\uff1b3) \u6df7\u5408\u7b56\u7565flex\u5206\u914d(Mixed-Strategy Flex Distribution)\uff0c\u5728\u5206\u5c42\u6846\u67b6\u4e2d\u7ed3\u5408\u524d\u4e24\u79cd\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684flex\u5206\u914d\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u539f\u59cb\u7684\u8d2a\u5fc3flex\u5206\u914d\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7b97\u6cd5\u7684\u5b8c\u6574\u6027\u548c\u6709\u754c\u6b21\u4f18\u6027\u4fdd\u8bc1\u3002", "conclusion": "\u65b0\u63d0\u51fa\u7684flex\u5206\u914d\u673a\u5236\u6709\u6548\u6539\u8fdb\u4e86EECBS\u7b97\u6cd5\u7684\u6548\u7387\uff0c\u5728\u4fdd\u6301\u7406\u8bba\u4fdd\u8bc1\u7684\u524d\u63d0\u4e0b\uff0c\u901a\u8fc7\u66f4\u5408\u7406\u7684flex\u5206\u914d\u7b56\u7565\u51cf\u5c11\u4e86\u4e0d\u5fc5\u8981\u7684\u8def\u5f84\u96c6\u5408\u5207\u6362\uff0c\u63d0\u9ad8\u4e86\u51b2\u7a81\u89e3\u51b3\u7684\u6548\u7387\u3002"}}
{"id": "2507.17075", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17075", "abs": "https://arxiv.org/abs/2507.17075", "authors": ["Yihao Xue", "Baharan Mirzasoleiman"], "title": "LoRA is All You Need for Safety Alignment of Reasoning LLMs", "comment": null, "summary": "Reasoning LLMs have demonstrated remarkable breakthroughs in solving complex\nproblems that were previously out of reach. To ensure LLMs do not assist with\nharmful requests, safety alignment fine-tuning is necessary in the\npost-training phase. However, safety alignment fine-tuning has recently been\nshown to significantly degrade reasoning abilities, a phenomenon known as the\n\"Safety Tax\". In this work, we show that using LoRA for SFT on refusal datasets\neffectively aligns the model for safety without harming its reasoning\ncapabilities. This is because restricting the safety weight updates to a\nlow-rank space minimizes the interference with the reasoning weights. Our\nextensive experiments across four benchmarks covering math, science, and coding\nshow that this approach produces highly safe LLMs -- with safety levels\ncomparable to full-model fine-tuning -- without compromising their reasoning\nabilities. Additionally, we observe that LoRA induces weight updates with\nsmaller overlap with the initial weights compared to full-model fine-tuning. We\nalso explore methods that further reduce such overlap -- via regularization or\nduring weight merging -- and observe some improvement on certain tasks. We hope\nthis result motivates designing approaches that yield more consistent\nimprovements in the reasoning-safety trade-off.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4f7f\u7528LoRA\u8fdb\u884c\u5b89\u5168\u5bf9\u9f50\u5fae\u8c03\u53ef\u4ee5\u5728\u4e0d\u635f\u5bb3\u63a8\u7406\u80fd\u529b\u7684\u524d\u63d0\u4e0b\u786e\u4fdd\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u6027\uff0c\u6709\u6548\u89e3\u51b3\u4e86\"\u5b89\u5168\u7a0e\"\u95ee\u9898", "motivation": "\u5f53\u524d\u7684\u5b89\u5168\u5bf9\u9f50\u5fae\u8c03\u4f1a\u663e\u8457\u964d\u4f4e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4ea7\u751f\"\u5b89\u5168\u7a0e\"\u73b0\u8c61\u3002\u9700\u8981\u627e\u5230\u4e00\u79cd\u65e2\u80fd\u4fdd\u8bc1\u6a21\u578b\u5b89\u5168\u53c8\u4e0d\u635f\u5bb3\u63a8\u7406\u80fd\u529b\u7684\u65b9\u6cd5", "method": "\u4f7f\u7528LoRA\uff08\u4f4e\u79e9\u9002\u5e94\uff09\u5728\u62d2\u7edd\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff0c\u5c06\u5b89\u5168\u6743\u91cd\u66f4\u65b0\u9650\u5236\u5728\u4f4e\u79e9\u7a7a\u95f4\u4e2d\uff0c\u4ece\u800c\u6700\u5c0f\u5316\u5bf9\u63a8\u7406\u6743\u91cd\u7684\u5e72\u6270\u3002\u8fd8\u63a2\u7d22\u4e86\u901a\u8fc7\u6b63\u5219\u5316\u6216\u6743\u91cd\u5408\u5e76\u6765\u8fdb\u4e00\u6b65\u51cf\u5c11\u6743\u91cd\u91cd\u53e0\u7684\u65b9\u6cd5", "result": "\u5728\u6570\u5b66\u3001\u79d1\u5b66\u548c\u7f16\u7a0b\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLoRA\u65b9\u6cd5\u4ea7\u751f\u4e86\u9ad8\u5ea6\u5b89\u5168\u7684LLM\uff0c\u5b89\u5168\u6c34\u5e73\u4e0e\u5168\u6a21\u578b\u5fae\u8c03\u76f8\u5f53\uff0c\u4f46\u4e0d\u4f1a\u635f\u5bb3\u63a8\u7406\u80fd\u529b\u3002LoRA\u76f8\u6bd4\u5168\u6a21\u578b\u5fae\u8c03\u4ea7\u751f\u7684\u6743\u91cd\u66f4\u65b0\u4e0e\u521d\u59cb\u6743\u91cd\u7684\u91cd\u53e0\u66f4\u5c0f", "conclusion": "LoRA\u5fae\u8c03\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u63a8\u7406\u80fd\u529b\u4e0e\u5b89\u5168\u6027\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\uff0c\u4e3a\u8bbe\u8ba1\u66f4\u4e00\u81f4\u6539\u8fdb\u63a8\u7406-\u5b89\u5168\u6743\u8861\u7684\u65b9\u6cd5\u63d0\u4f9b\u4e86\u91cd\u8981\u542f\u53d1"}}
{"id": "2507.17118", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17118", "abs": "https://arxiv.org/abs/2507.17118", "authors": ["Mandar Pitale", "Jelena Frtunikj", "Abhinaw Priyadershi", "Vasu Singh", "Maria Spence"], "title": "HySafe-AI: Hybrid Safety Architectural Analysis Framework for AI Systems: A Case Study", "comment": "7 pages", "summary": "AI has become integral to safety-critical areas like autonomous driving\nsystems (ADS) and robotics. The architecture of recent autonomous systems are\ntrending toward end-to-end (E2E) monolithic architectures such as large\nlanguage models (LLMs) and vision language models (VLMs). In this paper, we\nreview different architectural solutions and then evaluate the efficacy of\ncommon safety analyses such as failure modes and effect analysis (FMEA) and\nfault tree analysis (FTA). We show how these techniques can be improved for the\nintricate nature of the foundational models, particularly in how they form and\nutilize latent representations. We introduce HySAFE-AI, Hybrid Safety\nArchitectural Analysis Framework for AI Systems, a hybrid framework that adapts\ntraditional methods to evaluate the safety of AI systems. Lastly, we offer\nhints of future work and suggestions to guide the evolution of future AI safety\nstandards.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86HySAFE-AI\u6df7\u5408\u5b89\u5168\u67b6\u6784\u5206\u6790\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u7aef\u5230\u7aefAI\u7cfb\u7edf\uff08\u5982\u5927\u8bed\u8a00\u6a21\u578b\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff09\u5728\u5b89\u5168\u5173\u952e\u9886\u57df\u7684\u5b89\u5168\u6027\uff0c\u5e76\u6539\u8fdb\u4e86\u4f20\u7edf\u7684FMEA\u548cFTA\u5206\u6790\u65b9\u6cd5\u4ee5\u9002\u5e94\u57fa\u7840\u6a21\u578b\u7684\u590d\u6742\u6027\u3002", "motivation": "\u968f\u7740AI\u5728\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u548c\u673a\u5668\u4eba\u7b49\u5b89\u5168\u5173\u952e\u9886\u57df\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u7279\u522b\u662f\u7aef\u5230\u7aef\u5355\u4f53\u67b6\u6784\uff08\u5982\u5927\u8bed\u8a00\u6a21\u578b\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff09\u7684\u5174\u8d77\uff0c\u4f20\u7edf\u7684\u5b89\u5168\u5206\u6790\u65b9\u6cd5\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u9002\u5e94\u8fd9\u4e9b\u57fa\u7840\u6a21\u578b\u590d\u6742\u7684\u6f5c\u5728\u8868\u793a\u5f62\u6210\u548c\u5229\u7528\u673a\u5236\u3002", "method": "\u7814\u7a76\u8005\u56de\u987e\u4e86\u4e0d\u540c\u7684\u67b6\u6784\u89e3\u51b3\u65b9\u6848\uff0c\u8bc4\u4f30\u4e86\u5e38\u89c1\u5b89\u5168\u5206\u6790\u6280\u672f\uff08\u5982FMEA\u548cFTA\uff09\u7684\u6709\u6548\u6027\uff0c\u5e76\u9488\u5bf9\u57fa\u7840\u6a21\u578b\u7684\u590d\u6742\u6027\u8d28\u6539\u8fdb\u4e86\u8fd9\u4e9b\u6280\u672f\uff0c\u7279\u522b\u5173\u6ce8\u6f5c\u5728\u8868\u793a\u7684\u5f62\u6210\u548c\u5229\u7528\u3002\u5728\u6b64\u57fa\u7840\u4e0a\u63d0\u51fa\u4e86HySAFE-AI\u6df7\u5408\u5b89\u5168\u67b6\u6784\u5206\u6790\u6846\u67b6\u3002", "result": "\u63d0\u51fa\u4e86HySAFE-AI\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u4e2a\u6df7\u5408\u6846\u67b6\uff0c\u80fd\u591f\u5c06\u4f20\u7edf\u65b9\u6cd5\u9002\u914d\u7528\u4e8e\u8bc4\u4f30AI\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u3002\u8be5\u6846\u67b6\u6539\u8fdb\u4e86\u4f20\u7edf\u5b89\u5168\u5206\u6790\u6280\u672f\uff0c\u4f7f\u5176\u66f4\u9002\u7528\u4e8e\u590d\u6742\u7684\u57fa\u7840\u6a21\u578b\u3002", "conclusion": "\u7814\u7a76\u4e3aAI\u5b89\u5168\u6807\u51c6\u7684\u672a\u6765\u53d1\u5c55\u63d0\u4f9b\u4e86\u6307\u5bfc\u5efa\u8bae\uff0c\u5c55\u793a\u4e86\u5982\u4f55\u5c06\u4f20\u7edf\u5b89\u5168\u5206\u6790\u65b9\u6cd5\u9002\u914d\u5230\u73b0\u4ee3AI\u7cfb\u7edf\u4e2d\uff0c\u5e76\u4e3a\u672a\u6765\u5de5\u4f5c\u63d0\u4f9b\u4e86\u65b9\u5411\u63d0\u793a\uff0c\u63a8\u52a8AI\u5b89\u5168\u6807\u51c6\u7684\u6f14\u8fdb\u3002"}}
{"id": "2507.16839", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.16839", "abs": "https://arxiv.org/abs/2507.16839", "authors": ["Gregory Beale", "Gibran Ali"], "title": "Summarizing Normative Driving Behavior From Large-Scale NDS Datasets for Vehicle System Development", "comment": "Accepted to the 2025 IEEE International Conference on Intelligent\n  Transportation Systems (ITSC 2025)", "summary": "This paper presents a methodology to process large-scale naturalistic driving\nstudies (NDS) to describe the driving behavior for five vehicle metrics,\nincluding speed, speeding, lane keeping, following distance, and headway,\ncontextualized by roadway characteristics, vehicle classes, and driver\ndemographics. Such descriptions of normative driving behaviors can aid in the\ndevelopment of vehicle safety and intelligent transportation systems. The\nmethodology is demonstrated using data from the Second Strategic Highway\nResearch Program (SHRP 2) NDS, which includes over 34 million miles of driving\nacross more than 3,400 drivers. Summaries of each driving metric were generated\nusing vehicle, GPS, and forward radar data. Additionally, interactive online\nanalytics tools were developed to visualize and compare driving behavior across\ngroups through dynamic data selection and grouping. For example, among drivers\non 65-mph roads for the SHRP 2 NDS, females aged 16-19 exceeded the speed limit\nby 7.5 to 15 mph slightly more often than their male counterparts, and younger\ndrivers maintained headways under 1.5 seconds more frequently than older\ndrivers. This work supports better vehicle systems and safer infrastructure by\nquantifying normative driving behaviors and offers a methodology for analyzing\nNDS datasets for cross group comparisons.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5904\u7406\u5927\u89c4\u6a21\u81ea\u7136\u9a7e\u9a76\u7814\u7a76\u6570\u636e\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5206\u6790\u4e94\u4e2a\u5173\u952e\u9a7e\u9a76\u884c\u4e3a\u6307\u6807\uff08\u901f\u5ea6\u3001\u8d85\u901f\u3001\u8f66\u9053\u4fdd\u6301\u3001\u8ddf\u8f66\u8ddd\u79bb\u548c\u8f66\u5934\u65f6\u8ddd\uff09\uff0c\u5e76\u7ed3\u5408\u9053\u8def\u7279\u5f81\u3001\u8f66\u8f86\u7c7b\u578b\u548c\u9a7e\u9a76\u5458\u4eba\u53e3\u7edf\u8ba1\u5b66\u7279\u5f81\u8fdb\u884c\u5206\u6790\uff0c\u4ee5\u652f\u6301\u8f66\u8f86\u5b89\u5168\u7cfb\u7edf\u548c\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u7684\u5f00\u53d1\u3002", "motivation": "\u4e3a\u4e86\u5f00\u53d1\u66f4\u597d\u7684\u8f66\u8f86\u5b89\u5168\u7cfb\u7edf\u548c\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\uff0c\u9700\u8981\u91cf\u5316\u548c\u7406\u89e3\u5e38\u6001\u5316\u9a7e\u9a76\u884c\u4e3a\u3002\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5927\u89c4\u6a21\u3001\u591a\u7ef4\u5ea6\u7684\u9a7e\u9a76\u884c\u4e3a\u5206\u6790\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u7f3a\u4e4f\u8de8\u7fa4\u4f53\u6bd4\u8f83\u7684\u6709\u6548\u5de5\u5177\u3002", "method": "\u4f7f\u7528SHRP 2\u81ea\u7136\u9a7e\u9a76\u7814\u7a76\u6570\u636e\u96c6\uff08\u5305\u542b3400\u591a\u540d\u9a7e\u9a76\u5458\u8d85\u8fc73400\u4e07\u82f1\u91cc\u7684\u9a7e\u9a76\u6570\u636e\uff09\uff0c\u7ed3\u5408\u8f66\u8f86\u6570\u636e\u3001GPS\u6570\u636e\u548c\u524d\u5411\u96f7\u8fbe\u6570\u636e\uff0c\u5f00\u53d1\u4e86\u4e00\u5957\u5904\u7406\u548c\u5206\u6790\u5927\u89c4\u6a21\u81ea\u7136\u9a7e\u9a76\u6570\u636e\u7684\u65b9\u6cd5\u8bba\u3002\u540c\u65f6\u5f00\u53d1\u4e86\u4ea4\u4e92\u5f0f\u5728\u7ebf\u5206\u6790\u5de5\u5177\uff0c\u7528\u4e8e\u53ef\u89c6\u5316\u548c\u6bd4\u8f83\u4e0d\u540c\u7fa4\u4f53\u7684\u9a7e\u9a76\u884c\u4e3a\u3002", "result": "\u6210\u529f\u5206\u6790\u4e86\u4e94\u4e2a\u5173\u952e\u9a7e\u9a76\u6307\u6807\u7684\u884c\u4e3a\u6a21\u5f0f\uff0c\u53d1\u73b0\u4e86\u4e0d\u540c\u4eba\u7fa4\u95f4\u7684\u9a7e\u9a76\u884c\u4e3a\u5dee\u5f02\u3002\u4f8b\u5982\uff0c\u572865\u82f1\u91cc/\u5c0f\u65f6\u9650\u901f\u9053\u8def\u4e0a\uff0c16-19\u5c81\u5973\u6027\u9a7e\u9a76\u5458\u8d85\u901f7.5-15\u82f1\u91cc/\u5c0f\u65f6\u7684\u9891\u7387\u7565\u9ad8\u4e8e\u540c\u9f84\u7537\u6027\uff1b\u5e74\u8f7b\u9a7e\u9a76\u5458\u4fdd\u63011.5\u79d2\u4ee5\u4e0b\u8f66\u5934\u65f6\u8ddd\u7684\u9891\u7387\u9ad8\u4e8e\u5e74\u957f\u9a7e\u9a76\u5458\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u5957\u5b8c\u6574\u7684\u5927\u89c4\u6a21\u81ea\u7136\u9a7e\u9a76\u6570\u636e\u5206\u6790\u65b9\u6cd5\u8bba\uff0c\u901a\u8fc7\u91cf\u5316\u5e38\u6001\u5316\u9a7e\u9a76\u884c\u4e3a\uff0c\u4e3a\u5f00\u53d1\u66f4\u597d\u7684\u8f66\u8f86\u7cfb\u7edf\u548c\u66f4\u5b89\u5168\u7684\u57fa\u7840\u8bbe\u65bd\u63d0\u4f9b\u652f\u6301\uff0c\u5e76\u4e3a\u81ea\u7136\u9a7e\u9a76\u7814\u7a76\u6570\u636e\u96c6\u7684\u8de8\u7fa4\u4f53\u6bd4\u8f83\u5206\u6790\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2507.17168", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17168", "abs": "https://arxiv.org/abs/2507.17168", "authors": ["Qifan Zhang", "Nuo Chen", "Zehua Li", "Miao Peng", "Jing Tang", "Jia Li"], "title": "Improving LLMs' Generalized Reasoning Abilities by Graph Problems", "comment": "COLM2025", "summary": "Large Language Models (LLMs) have made remarkable strides in reasoning tasks,\nyet their performance often falters on novel and complex problems.\nDomain-specific continued pretraining (CPT) methods, such as those tailored for\nmathematical reasoning, have shown promise but lack transferability to broader\nreasoning tasks. In this work, we pioneer the use of Graph Problem Reasoning\n(GPR) to enhance the general reasoning capabilities of LLMs. GPR tasks,\nspanning pathfinding, network analysis, numerical computation, and topological\nreasoning, require sophisticated logical and relational reasoning, making them\nideal for teaching diverse reasoning patterns. To achieve this, we introduce\nGraphPile, the first large-scale corpus specifically designed for CPT using GPR\ndata. Spanning 10.9 billion tokens across 23 graph tasks, the dataset includes\nchain-of-thought, program-of-thought, trace of execution, and real-world graph\ndata. Using GraphPile, we train GraphMind on popular base models Llama 3 and\n3.1, as well as Gemma 2, achieving up to 4.9 percent higher accuracy in\nmathematical reasoning and up to 21.2 percent improvement in non-mathematical\nreasoning tasks such as logical and commonsense reasoning. By being the first\nto harness GPR for enhancing reasoning patterns and introducing the first\ndataset of its kind, our work bridges the gap between domain-specific\npretraining and universal reasoning capabilities, advancing the adaptability\nand robustness of LLMs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u56fe\u95ee\u9898\u63a8\u7406(GPR)\u65b9\u6cd5\u6765\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u901a\u7528\u63a8\u7406\u80fd\u529b\uff0c\u521b\u5efa\u4e86\u9996\u4e2a\u5927\u89c4\u6a21\u56fe\u63a8\u7406\u6570\u636e\u96c6GraphPile\uff0c\u5e76\u8bad\u7ec3\u4e86GraphMind\u6a21\u578b\uff0c\u5728\u6570\u5b66\u548c\u975e\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\u3002", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5728\u65b0\u9896\u590d\u6742\u95ee\u9898\u4e0a\u63a8\u7406\u6027\u80fd\u4e0d\u4f73\uff0c\u800c\u9886\u57df\u7279\u5b9a\u7684\u6301\u7eed\u9884\u8bad\u7ec3\u65b9\u6cd5\u7f3a\u4e4f\u5411\u66f4\u5e7f\u6cdb\u63a8\u7406\u4efb\u52a1\u7684\u8fc1\u79fb\u80fd\u529b\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u63d0\u5347\u901a\u7528\u63a8\u7406\u80fd\u529b\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u56fe\u95ee\u9898\u63a8\u7406(GPR)\u65b9\u6cd5\uff0c\u6784\u5efa\u4e86\u5305\u542b109\u4ebf\u4e2atoken\u3001\u6db5\u76d623\u4e2a\u56fe\u4efb\u52a1\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6GraphPile\uff0c\u5305\u542b\u601d\u7ef4\u94fe\u3001\u7a0b\u5e8f\u601d\u7ef4\u3001\u6267\u884c\u8f68\u8ff9\u548c\u771f\u5b9e\u4e16\u754c\u56fe\u6570\u636e\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u8bad\u7ec3GraphMind\u6a21\u578b\u3002", "result": "\u5728Llama 3\u30013.1\u548cGemma 2\u7b49\u57fa\u7840\u6a21\u578b\u4e0a\u8bad\u7ec3\u7684GraphMind\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u51c6\u786e\u7387\u63d0\u5347\u9ad8\u8fbe4.9%\uff0c\u5728\u903b\u8f91\u63a8\u7406\u548c\u5e38\u8bc6\u63a8\u7406\u7b49\u975e\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u6539\u8fdb\u9ad8\u8fbe21.2%\u3002", "conclusion": "\u672c\u7814\u7a76\u9996\u6b21\u5229\u7528\u56fe\u95ee\u9898\u63a8\u7406\u589e\u5f3a\u63a8\u7406\u6a21\u5f0f\uff0c\u521b\u5efa\u4e86\u9996\u4e2a\u6b64\u7c7b\u6570\u636e\u96c6\uff0c\u6210\u529f\u5f25\u5408\u4e86\u9886\u57df\u7279\u5b9a\u9884\u8bad\u7ec3\u4e0e\u901a\u7528\u63a8\u7406\u80fd\u529b\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u63d0\u5347\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u7684\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.16841", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.16841", "abs": "https://arxiv.org/abs/2507.16841", "authors": ["Waseem Akram", "Muhayy Ud Din", "Abdelhaleem Saad", "Irfan Hussain"], "title": "AquaChat: An LLM-Guided ROV Framework for Adaptive Inspection of Aquaculture Net Pens", "comment": null, "summary": "Inspection of aquaculture net pens is essential for maintaining the\nstructural integrity, biosecurity, and operational efficiency of fish farming\nsystems. Traditional inspection approaches rely on pre-programmed missions or\nmanual control, offering limited adaptability to dynamic underwater conditions\nand user-specific demands. In this study, we propose AquaChat, a novel Remotely\nOperated Vehicle (ROV) framework that integrates Large Language Models (LLMs)\nfor intelligent and adaptive net pen inspection. The system features a\nmulti-layered architecture: (1) a high-level planning layer that interprets\nnatural language user commands using an LLM to generate symbolic task plans;\n(2) a mid-level task manager that translates plans into ROV control sequences;\nand (3) a low-level motion control layer that executes navigation and\ninspection tasks with precision. Real-time feedback and event-triggered\nreplanning enhance robustness in challenging aquaculture environments. The\nframework is validated through experiments in both simulated and controlled\naquatic environments representative of aquaculture net pens. Results\ndemonstrate improved task flexibility, inspection accuracy, and operational\nefficiency. AquaChat illustrates the potential of integrating language-based AI\nwith marine robotics to enable intelligent, user-interactive inspection systems\nfor sustainable aquaculture operations.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86AquaChat\uff0c\u4e00\u4e2a\u96c6\u6210\u5927\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u6c34\u4ea7\u517b\u6b96\u7f51\u7bb1\u68c0\u6d4bROV\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u5b9e\u73b0\u81ea\u9002\u5e94\u7684\u6c34\u4e0b\u68c0\u6d4b\u4efb\u52a1", "motivation": "\u4f20\u7edf\u7684\u6c34\u4ea7\u517b\u6b96\u7f51\u7bb1\u68c0\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u9884\u7f16\u7a0b\u4efb\u52a1\u6216\u4eba\u5de5\u63a7\u5236\uff0c\u5bf9\u52a8\u6001\u6c34\u4e0b\u73af\u5883\u548c\u7528\u6237\u7279\u5b9a\u9700\u6c42\u7684\u9002\u5e94\u6027\u6709\u9650\uff0c\u9700\u8981\u66f4\u667a\u80fd\u548c\u81ea\u9002\u5e94\u7684\u68c0\u6d4b\u7cfb\u7edf", "method": "\u8bbe\u8ba1\u4e86\u591a\u5c42\u67b6\u6784\u7684AquaChat\u7cfb\u7edf\uff1a(1)\u9ad8\u5c42\u89c4\u5212\u5c42\u4f7f\u7528LLM\u89e3\u91ca\u81ea\u7136\u8bed\u8a00\u7528\u6237\u6307\u4ee4\u5e76\u751f\u6210\u7b26\u53f7\u5316\u4efb\u52a1\u8ba1\u5212\uff1b(2)\u4e2d\u5c42\u4efb\u52a1\u7ba1\u7406\u5668\u5c06\u8ba1\u5212\u8f6c\u6362\u4e3aROV\u63a7\u5236\u5e8f\u5217\uff1b(3)\u4f4e\u5c42\u8fd0\u52a8\u63a7\u5236\u5c42\u7cbe\u786e\u6267\u884c\u5bfc\u822a\u548c\u68c0\u6d4b\u4efb\u52a1\uff0c\u5e76\u5177\u5907\u5b9e\u65f6\u53cd\u9988\u548c\u4e8b\u4ef6\u89e6\u53d1\u91cd\u89c4\u5212\u529f\u80fd", "result": "\u5728\u6a21\u62df\u548c\u53d7\u63a7\u6c34\u73af\u5883\u4e2d\u8fdb\u884c\u9a8c\u8bc1\u5b9e\u9a8c\uff0c\u7ed3\u679c\u663e\u793a\u4efb\u52a1\u7075\u6d3b\u6027\u3001\u68c0\u6d4b\u7cbe\u5ea6\u548c\u64cd\u4f5c\u6548\u7387\u5747\u6709\u663e\u8457\u63d0\u5347", "conclusion": "AquaChat\u5c55\u793a\u4e86\u8bed\u8a00AI\u4e0e\u6d77\u6d0b\u673a\u5668\u4eba\u6280\u672f\u7ed3\u5408\u7684\u6f5c\u529b\uff0c\u80fd\u591f\u4e3a\u53ef\u6301\u7eed\u6c34\u4ea7\u517b\u6b96\u8fd0\u8425\u63d0\u4f9b\u667a\u80fd\u7684\u3001\u7528\u6237\u4ea4\u4e92\u5f0f\u7684\u68c0\u6d4b\u7cfb\u7edf"}}
{"id": "2507.17214", "categories": ["cs.AI", "cs.CY", "cs.NI", "cs.SY", "eess.SY", "I.2; B.8; C.2; I.5; J.7"], "pdf": "https://arxiv.org/pdf/2507.17214", "abs": "https://arxiv.org/abs/2507.17214", "authors": ["Amod Kant Agrawal"], "title": "Our Cars Can Talk: How IoT Brings AI to Vehicles", "comment": "3 pages, 1 figure; To appear in IEEE Computer (Nov 2025)", "summary": "Bringing AI to vehicles and enabling them as sensing platforms is key to\ntransforming maintenance from reactive to proactive. Now is the time to\nintegrate AI copilots that speak both languages: machine and driver. This\narticle offers a conceptual and technical perspective intended to spark\ninterdisciplinary dialogue and guide future research and development in\nintelligent vehicle systems, predictive maintenance, and AI-powered user\ninteraction.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5c06AI\u96c6\u6210\u5230\u8f66\u8f86\u4e2d\u4f5c\u4e3a\u611f\u77e5\u5e73\u53f0\uff0c\u901a\u8fc7AI\u526f\u9a7e\u9a76\u5b9e\u73b0\u673a\u5668\u548c\u9a7e\u9a76\u5458\u4e4b\u95f4\u7684\u53cc\u5411\u4ea4\u6d41\uff0c\u4ece\u800c\u5c06\u8f66\u8f86\u7ef4\u62a4\u4ece\u88ab\u52a8\u54cd\u5e94\u8f6c\u53d8\u4e3a\u4e3b\u52a8\u9884\u6d4b\u3002", "motivation": "\u4f20\u7edf\u8f66\u8f86\u7ef4\u62a4\u6a21\u5f0f\u662f\u88ab\u52a8\u54cd\u5e94\u5f0f\u7684\uff0c\u7f3a\u4e4f\u9884\u6d4b\u6027\u548c\u667a\u80fd\u5316\u3002\u9700\u8981\u5229\u7528AI\u6280\u672f\u5c06\u8f66\u8f86\u8f6c\u53d8\u4e3a\u667a\u80fd\u611f\u77e5\u5e73\u53f0\uff0c\u5b9e\u73b0\u4e3b\u52a8\u9884\u6d4b\u6027\u7ef4\u62a4\uff0c\u5e76\u5efa\u7acb\u673a\u5668\u4e0e\u9a7e\u9a76\u5458\u4e4b\u95f4\u7684\u6709\u6548\u6c9f\u901a\u6865\u6881\u3002", "method": "\u63d0\u51fa\u96c6\u6210AI\u526f\u9a7e\u9a76\u7cfb\u7edf\u7684\u6982\u5ff5\u6846\u67b6\uff0c\u8be5\u7cfb\u7edf\u80fd\u591f\u540c\u65f6\u7406\u89e3\u673a\u5668\u8bed\u8a00\u548c\u4eba\u7c7b\u9a7e\u9a76\u5458\u8bed\u8a00\uff0c\u4f5c\u4e3a\u8f66\u8f86\u667a\u80fd\u611f\u77e5\u5e73\u53f0\u7684\u6838\u5fc3\u7ec4\u4ef6\uff0c\u5b9e\u73b0\u9884\u6d4b\u6027\u7ef4\u62a4\u548cAI\u9a71\u52a8\u7684\u7528\u6237\u4ea4\u4e92\u3002", "result": "\u6587\u7ae0\u63d0\u4f9b\u4e86\u667a\u80fd\u8f66\u8f86\u7cfb\u7edf\u3001\u9884\u6d4b\u6027\u7ef4\u62a4\u548cAI\u7528\u6237\u4ea4\u4e92\u7684\u6982\u5ff5\u6027\u548c\u6280\u672f\u6027\u89c6\u89d2\uff0c\u4e3a\u8be5\u9886\u57df\u7684\u8de8\u5b66\u79d1\u5bf9\u8bdd\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u63d0\u4f9b\u6307\u5bfc\u6846\u67b6\u3002", "conclusion": "AI\u526f\u9a7e\u9a76\u6280\u672f\u7684\u96c6\u6210\u662f\u5b9e\u73b0\u8f66\u8f86\u4ece\u88ab\u52a8\u7ef4\u62a4\u5411\u4e3b\u52a8\u9884\u6d4b\u6027\u7ef4\u62a4\u8f6c\u53d8\u7684\u5173\u952e\uff0c\u8fd9\u79cd\u53cc\u8bed\u8a00AI\u7cfb\u7edf\u5c06\u63a8\u52a8\u667a\u80fd\u8f66\u8f86\u7cfb\u7edf\u3001\u9884\u6d4b\u6027\u7ef4\u62a4\u548c\u4eba\u673a\u4ea4\u4e92\u9886\u57df\u7684\u53d1\u5c55\u548c\u521b\u65b0\u3002"}}
{"id": "2507.16842", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.16842", "abs": "https://arxiv.org/abs/2507.16842", "authors": ["Yinan Meng", "Kun Qian", "Jiong Yang", "Renbo Su", "Zhenhong Li", "Charlie C. L. Wang"], "title": "Sensor-Space Based Robust Kinematic Control of Redundant Soft Manipulator by Learning", "comment": null, "summary": "The intrinsic compliance and high degree of freedom (DoF) of redundant soft\nmanipulators facilitate safe interaction and flexible task execution. However,\neffective kinematic control remains highly challenging, as it must handle\ndeformations caused by unknown external loads and avoid actuator saturation due\nto improper null-space regulation - particularly in confined environments. In\nthis paper, we propose a Sensor-Space Imitation Learning Kinematic Control\n(SS-ILKC) framework to enable robust kinematic control under actuator\nsaturation and restrictive environmental constraints. We employ a dual-learning\nstrategy: a multi-goal sensor-space control framework based on reinforcement\nlearning principle is trained in simulation to develop robust control policies\nfor open spaces, while a generative adversarial imitation learning approach\nenables effective policy learning from sparse expert demonstrations for\nconfined spaces. To enable zero-shot real-world deployment, a pre-processed\nsim-to-real transfer mechanism is proposed to mitigate the\nsimulation-to-reality gap and accurately characterize actuator saturation\nlimits. Experimental results demonstrate that our method can effectively\ncontrol a pneumatically actuated soft manipulator, achieving precise\npath-following and object manipulation in confined environments under unknown\nloading conditions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f20\u611f\u5668\u7a7a\u95f4\u6a21\u4eff\u5b66\u4e60\u8fd0\u52a8\u5b66\u63a7\u5236\u6846\u67b6(SS-ILKC)\uff0c\u901a\u8fc7\u53cc\u91cd\u5b66\u4e60\u7b56\u7565\u548c\u4eff\u771f\u5230\u73b0\u5b9e\u8f6c\u79fb\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u8f6f\u4f53\u673a\u68b0\u81c2\u5728\u53d7\u9650\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u8fd0\u52a8\u5b66\u63a7\u5236\u3002", "motivation": "\u5197\u4f59\u8f6f\u4f53\u673a\u68b0\u81c2\u867d\u7136\u5177\u6709\u5185\u5728\u987a\u5e94\u6027\u548c\u9ad8\u81ea\u7531\u5ea6\uff0c\u4fbf\u4e8e\u5b89\u5168\u4ea4\u4e92\u548c\u7075\u6d3b\u4efb\u52a1\u6267\u884c\uff0c\u4f46\u5176\u6709\u6548\u7684\u8fd0\u52a8\u5b66\u63a7\u5236\u9762\u4e34\u5de8\u5927\u6311\u6218\uff1a\u9700\u8981\u5904\u7406\u672a\u77e5\u5916\u90e8\u8f7d\u8377\u5f15\u8d77\u7684\u53d8\u5f62\uff0c\u5e76\u907f\u514d\u56e0\u4e0d\u5f53\u96f6\u7a7a\u95f4\u8c03\u8282\u5bfc\u81f4\u7684\u6267\u884c\u5668\u9971\u548c\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u53d7\u9650\u73af\u5883\u4e2d\u3002", "method": "\u63d0\u51fa\u4f20\u611f\u5668\u7a7a\u95f4\u6a21\u4eff\u5b66\u4e60\u8fd0\u52a8\u5b66\u63a7\u5236(SS-ILKC)\u6846\u67b6\uff0c\u91c7\u7528\u53cc\u91cd\u5b66\u4e60\u7b56\u7565\uff1a1)\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u539f\u7406\u7684\u591a\u76ee\u6807\u4f20\u611f\u5668\u7a7a\u95f4\u63a7\u5236\u6846\u67b6\u5728\u4eff\u771f\u4e2d\u8bad\u7ec3\uff0c\u4e3a\u5f00\u653e\u7a7a\u95f4\u5f00\u53d1\u9c81\u68d2\u63a7\u5236\u7b56\u7565\uff1b2)\u751f\u6210\u5bf9\u6297\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u4ece\u7a00\u758f\u4e13\u5bb6\u6f14\u793a\u4e2d\u5b66\u4e60\u53d7\u9650\u7a7a\u95f4\u7684\u6709\u6548\u7b56\u7565\u3002\u540c\u65f6\u63d0\u51fa\u9884\u5904\u7406\u7684\u4eff\u771f\u5230\u73b0\u5b9e\u8f6c\u79fb\u673a\u5236\u6765\u7f13\u89e3\u4eff\u771f\u4e0e\u73b0\u5b9e\u7684\u5dee\u8ddd\u5e76\u51c6\u786e\u8868\u5f81\u6267\u884c\u5668\u9971\u548c\u9650\u5236\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u63a7\u5236\u6c14\u52a8\u8f6f\u4f53\u673a\u68b0\u81c2\uff0c\u5728\u672a\u77e5\u8f7d\u8377\u6761\u4ef6\u4e0b\u7684\u53d7\u9650\u73af\u5883\u4e2d\u5b9e\u73b0\u7cbe\u786e\u7684\u8def\u5f84\u8ddf\u8e2a\u548c\u7269\u4f53\u64cd\u4f5c\u3002", "conclusion": "SS-ILKC\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u8f6f\u4f53\u673a\u68b0\u81c2\u5728\u53d7\u9650\u73af\u5883\u4e0b\u7684\u8fd0\u52a8\u5b66\u63a7\u5236\u95ee\u9898\uff0c\u901a\u8fc7\u53cc\u91cd\u5b66\u4e60\u7b56\u7565\u548c\u4eff\u771f\u5230\u73b0\u5b9e\u8f6c\u79fb\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u5728\u6267\u884c\u5668\u9971\u548c\u548c\u73af\u5883\u7ea6\u675f\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u63a7\u5236\uff0c\u4e3a\u8f6f\u4f53\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.17257", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.17257", "abs": "https://arxiv.org/abs/2507.17257", "authors": ["Elija Perrier", "Michael Timothy Bennett"], "title": "Agent Identity Evals: Measuring Agentic Identity", "comment": null, "summary": "Central to agentic capability and trustworthiness of language model agents\n(LMAs) is the extent they maintain stable, reliable, identity over time.\nHowever, LMAs inherit pathologies from large language models (LLMs)\n(statelessness, stochasticity, sensitivity to prompts and\nlinguistically-intermediation) which can undermine their identifiability,\ncontinuity, persistence and consistency. This attrition of identity can erode\ntheir reliability, trustworthiness and utility by interfering with their\nagentic capabilities such as reasoning, planning and action. To address these\nchallenges, we introduce \\textit{agent identity evals} (AIE), a rigorous,\nstatistically-driven, empirical framework for measuring the degree to which an\nLMA system exhibit and maintain their agentic identity over time, including\ntheir capabilities, properties and ability to recover from state perturbations.\nAIE comprises a set of novel metrics which can integrate with other measures of\nperformance, capability and agentic robustness to assist in the design of\noptimal LMA infrastructure and scaffolding such as memory and tools. We set out\nformal definitions and methods that can be applied at each stage of the LMA\nlife-cycle, and worked examples of how to apply them.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4ee3\u7406\u8eab\u4efd\u8bc4\u4f30(AIE)\u6846\u67b6\uff0c\u7528\u4e8e\u8861\u91cf\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u5728\u65f6\u95f4\u63a8\u79fb\u4e2d\u7ef4\u6301\u7a33\u5b9a\u8eab\u4efd\u7684\u80fd\u529b\uff0c\u4ee5\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\u56fa\u6709\u7f3a\u9677\u5bf9\u4ee3\u7406\u53ef\u4fe1\u5ea6\u548c\u529f\u80fd\u6027\u7684\u5f71\u54cd\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u4ee3\u7406(LMAs)\u7ee7\u627f\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u7684\u75c5\u7406\u7279\u5f81(\u65e0\u72b6\u6001\u6027\u3001\u968f\u673a\u6027\u3001\u5bf9\u63d0\u793a\u654f\u611f\u6027\u7b49)\uff0c\u8fd9\u4e9b\u7279\u5f81\u4f1a\u7834\u574f\u4ee3\u7406\u7684\u53ef\u8bc6\u522b\u6027\u3001\u8fde\u7eed\u6027\u3001\u6301\u4e45\u6027\u548c\u4e00\u81f4\u6027\uff0c\u4ece\u800c\u524a\u5f31\u5176\u63a8\u7406\u3001\u89c4\u5212\u548c\u884c\u52a8\u7b49\u4ee3\u7406\u80fd\u529b\uff0c\u6700\u7ec8\u5f71\u54cd\u53ef\u9760\u6027\u3001\u53ef\u4fe1\u5ea6\u548c\u5b9e\u7528\u6027\u3002", "method": "\u5f15\u5165\u4ee3\u7406\u8eab\u4efd\u8bc4\u4f30(AIE)\u6846\u67b6\u2014\u2014\u4e00\u4e2a\u4e25\u683c\u7684\u3001\u7edf\u8ba1\u9a71\u52a8\u7684\u7ecf\u9a8c\u6846\u67b6\uff0c\u5305\u542b\u4e00\u5957\u65b0\u9896\u7684\u6307\u6807\u6765\u6d4b\u91cfLMA\u7cfb\u7edf\u5c55\u73b0\u548c\u7ef4\u6301\u4ee3\u7406\u8eab\u4efd\u7684\u7a0b\u5ea6\uff0c\u5305\u62ec\u5176\u80fd\u529b\u3001\u5c5e\u6027\u548c\u4ece\u72b6\u6001\u6270\u52a8\u4e2d\u6062\u590d\u7684\u80fd\u529b\u3002\u8be5\u6846\u67b6\u53ef\u4ee5\u4e0e\u5176\u4ed6\u6027\u80fd\u3001\u80fd\u529b\u548c\u4ee3\u7406\u9c81\u68d2\u6027\u6d4b\u91cf\u6307\u6807\u96c6\u6210\u3002", "result": "\u63d0\u51fa\u4e86\u53ef\u5e94\u7528\u4e8eLMA\u751f\u547d\u5468\u671f\u5404\u4e2a\u9636\u6bb5\u7684\u6b63\u5f0f\u5b9a\u4e49\u548c\u65b9\u6cd5\uff0c\u5e76\u63d0\u4f9b\u4e86\u5982\u4f55\u5e94\u7528\u8fd9\u4e9b\u65b9\u6cd5\u7684\u5de5\u4f5c\u793a\u4f8b\u3002AIE\u6846\u67b6\u53ef\u4ee5\u5e2e\u52a9\u8bbe\u8ba1\u6700\u4f18\u7684LMA\u57fa\u7840\u8bbe\u65bd\u548c\u652f\u6491\u7ed3\u6784\uff0c\u5982\u5185\u5b58\u548c\u5de5\u5177\u3002", "conclusion": "AIE\u6846\u67b6\u4e3a\u8bc4\u4f30\u548c\u7ef4\u6301\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u7684\u8eab\u4efd\u7a33\u5b9a\u6027\u63d0\u4f9b\u4e86\u7cfb\u7edf\u6027\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u4ee3\u7406\u7cfb\u7edf\u7684\u53ef\u4fe1\u5ea6\u548c\u4ee3\u7406\u80fd\u529b\uff0c\u4e3a\u8bbe\u8ba1\u66f4\u53ef\u9760\u7684LMA\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u8df5\u6307\u5bfc\u3002"}}
{"id": "2507.16846", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.16846", "abs": "https://arxiv.org/abs/2507.16846", "authors": ["Qing Tang", "Xianbiao Hu"], "title": "Analytical Formulation of Autonomous Vehicle Freeway Merging Control with State-Dependent Discharge Rates", "comment": "Accepted for publication in IEEE Transactions on Intelligent\n  Transportation Systems (2025) as a regular paper (minor revision approved)", "summary": "The core of the freeway merging control problem lies in dynamic queue\npropagation and dissipation linked to merging vehicle behavior. Traditionally,\nqueuing is modeled through demand-supply interactions with time varying demand\nand fixed capacity. However, field observations show flow rates decrease during\ncongestion at freeway merges due to the impact of intersecting traffic, a\nfactor overlooked in fundamental diagrams. This manuscript introduces an\nanalytical approach to characterize and control the dynamic multi-stage merging\nof autonomous vehicles, prioritizing traffic efficiency and safety. For the\nfirst time, the effective discharge rate at the merging point, reduced by the\nmulti-stage dynamic merging process, is analytically derived using a closed\nform formulation. Leveraging this expression, performance metrics such as queue\nlength and traffic delay are derived as the first objective. Additionally, a\ncrash risk function is established to quantitatively assess potential\ncollisions during the merging process, serving as the second objective.\nFinally, the problem is formulated as a dynamic programming model to jointly\nminimize delay and crash risk, with the merging location and speed as decision\nvariables. Given the terminal state, the ramp vehicle merging task is\nformulated as a recursive optimization problem, employing backward induction to\nfind the minimum cost solution. Numerical experiments using the NGSIM dataset\nvalidate the derived effective discharge rate. The results indicate that the\nproposed model outperforms two benchmark algorithms, leading to a more\nefficient and safer merging process.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u591a\u9636\u6bb5\u52a8\u6001\u6c47\u5165\u63a7\u5236\u7684\u5206\u6790\u65b9\u6cd5\uff0c\u901a\u8fc7\u95ed\u5f0f\u516c\u5f0f\u63a8\u5bfc\u6709\u6548\u6392\u653e\u7387\uff0c\u5efa\u7acb\u4e86\u540c\u65f6\u6700\u5c0f\u5316\u5ef6\u8bef\u548c\u78b0\u649e\u98ce\u9669\u7684\u52a8\u6001\u89c4\u5212\u6a21\u578b\uff0c\u5728NGSIM\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u4f20\u7edf\u6392\u961f\u6a21\u578b\u5ffd\u7565\u4e86\u4ea4\u53c9\u4ea4\u901a\u5bf9\u9ad8\u901f\u516c\u8def\u6c47\u5165\u70b9\u6d41\u91cf\u7684\u5f71\u54cd\uff0c\u73b0\u6709\u57fa\u672c\u56fe\u65e0\u6cd5\u51c6\u786e\u53cd\u6620\u62e5\u5835\u65f6\u6d41\u91cf\u4e0b\u964d\u7684\u73b0\u8c61\uff0c\u9700\u8981\u5efa\u7acb\u66f4\u51c6\u786e\u7684\u52a8\u6001\u591a\u9636\u6bb5\u6c47\u5165\u63a7\u5236\u6a21\u578b\u6765\u63d0\u5347\u4ea4\u901a\u6548\u7387\u548c\u5b89\u5168\u6027\u3002", "method": "\u9996\u6b21\u4f7f\u7528\u95ed\u5f0f\u516c\u5f0f\u5206\u6790\u63a8\u5bfc\u591a\u9636\u6bb5\u52a8\u6001\u6c47\u5165\u8fc7\u7a0b\u4e2d\u7684\u6709\u6548\u6392\u653e\u7387\uff0c\u5efa\u7acb\u961f\u5217\u957f\u5ea6\u548c\u4ea4\u901a\u5ef6\u8bef\u7684\u6027\u80fd\u6307\u6807\uff0c\u6784\u5efa\u78b0\u649e\u98ce\u9669\u5b9a\u91cf\u8bc4\u4f30\u51fd\u6570\uff0c\u5c06\u95ee\u9898\u8868\u8ff0\u4e3a\u52a8\u6001\u89c4\u5212\u6a21\u578b\uff0c\u4ee5\u6c47\u5165\u4f4d\u7f6e\u548c\u901f\u5ea6\u4e3a\u51b3\u7b56\u53d8\u91cf\uff0c\u91c7\u7528\u53cd\u5411\u5f52\u7eb3\u6c42\u89e3\u6700\u5c0f\u6210\u672c\u65b9\u6848\u3002", "result": "\u4f7f\u7528NGSIM\u6570\u636e\u96c6\u9a8c\u8bc1\u4e86\u63a8\u5bfc\u7684\u6709\u6548\u6392\u653e\u7387\u7684\u51c6\u786e\u6027\uff0c\u6570\u503c\u5b9e\u9a8c\u8868\u660e\u6240\u63d0\u51fa\u7684\u6a21\u578b\u5728\u4e24\u4e2a\u57fa\u51c6\u7b97\u6cd5\u7684\u5bf9\u6bd4\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u548c\u66f4\u5b89\u5168\u7684\u6c47\u5165\u8fc7\u7a0b\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u5efa\u7acb\u4e86\u8003\u8651\u4ea4\u53c9\u4ea4\u901a\u5f71\u54cd\u7684\u52a8\u6001\u591a\u9636\u6bb5\u6c47\u5165\u63a7\u5236\u6a21\u578b\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u5ef6\u8bef\u548c\u78b0\u649e\u98ce\u9669\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u9ad8\u901f\u516c\u8def\u6c47\u5165\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u63a7\u5236\u7b56\u7565\uff0c\u5728\u6548\u7387\u548c\u5b89\u5168\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2507.17258", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17258", "abs": "https://arxiv.org/abs/2507.17258", "authors": ["Andreas Scholl", "Natalie Kiesler"], "title": "Students' Feedback Requests and Interactions with the SCRIPT Chatbot: Do They Get What They Ask For?", "comment": "Accepted at PPIG 2025", "summary": "Building on prior research on Generative AI (GenAI) and related tools for\nprogramming education, we developed SCRIPT, a chatbot based on ChatGPT-4o-mini,\nto support novice learners. SCRIPT allows for open-ended interactions and\nstructured guidance through predefined prompts. We evaluated the tool via an\nexperiment with 136 students from an introductory programming course at a large\nGerman university and analyzed how students interacted with SCRIPT while\nsolving programming tasks with a focus on their feedback preferences. The\nresults reveal that students' feedback requests seem to follow a specific\nsequence. Moreover, the chatbot responses aligned well with students' requested\nfeedback types (in 75%), and it adhered to the system prompt constraints. These\ninsights inform the design of GenAI-based learning support systems and\nhighlight challenges in balancing guidance and flexibility in AI-assisted\ntools.", "AI": {"tldr": "\u7814\u7a76\u8005\u5f00\u53d1\u4e86\u57fa\u4e8eChatGPT-4o-mini\u7684\u7f16\u7a0b\u5b66\u4e60\u804a\u5929\u673a\u5668\u4ebaSCRIPT\uff0c\u901a\u8fc7136\u540d\u5fb7\u56fd\u5927\u5b66\u751f\u7684\u5b9e\u9a8c\u8bc4\u4f30\uff0c\u53d1\u73b0\u5b66\u751f\u7684\u53cd\u9988\u8bf7\u6c42\u9075\u5faa\u7279\u5b9a\u5e8f\u5217\uff0c\u673a\u5668\u4eba\u54cd\u5e94\u4e0e\u5b66\u751f\u9700\u6c42\u5339\u914d\u5ea6\u8fbe75%\uff0c\u4e3a\u8bbe\u8ba1AI\u8f85\u52a9\u7f16\u7a0b\u6559\u80b2\u5de5\u5177\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002", "motivation": "\u73b0\u6709\u7f16\u7a0b\u6559\u80b2\u4e2d\u7f3a\u4e4f\u6709\u6548\u7684AI\u8f85\u52a9\u5b66\u4e60\u5de5\u5177\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u4e3a\u7f16\u7a0b\u65b0\u624b\u63d0\u4f9b\u5f00\u653e\u5f0f\u4ea4\u4e92\u548c\u7ed3\u6784\u5316\u6307\u5bfc\u7684\u667a\u80fd\u804a\u5929\u673a\u5668\u4eba\uff0c\u4ee5\u652f\u6301\u5b66\u751f\u5728\u7f16\u7a0b\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u83b7\u5f97\u4e2a\u6027\u5316\u53cd\u9988\u548c\u6307\u5bfc\u3002", "method": "\u57fa\u4e8eChatGPT-4o-mini\u5f00\u53d1\u4e86SCRIPT\u804a\u5929\u673a\u5668\u4eba\uff0c\u652f\u6301\u5f00\u653e\u5f0f\u4ea4\u4e92\u548c\u9884\u5b9a\u4e49\u63d0\u793a\u7684\u7ed3\u6784\u5316\u6307\u5bfc\u3002\u901a\u8fc7\u5728\u5fb7\u56fd\u67d0\u5927\u5b66\u7684\u7f16\u7a0b\u5165\u95e8\u8bfe\u7a0b\u4e2d\u5bf9136\u540d\u5b66\u751f\u8fdb\u884c\u5b9e\u9a8c\u8bc4\u4f30\uff0c\u5206\u6790\u5b66\u751f\u4e0eSCRIPT\u7684\u4ea4\u4e92\u65b9\u5f0f\uff0c\u91cd\u70b9\u5173\u6ce8\u4ed6\u4eec\u7684\u53cd\u9988\u504f\u597d\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5b66\u751f\u7684\u53cd\u9988\u8bf7\u6c42\u9075\u5faa\u7279\u5b9a\u7684\u5e8f\u5217\u6a21\u5f0f\uff0c\u804a\u5929\u673a\u5668\u4eba\u7684\u54cd\u5e94\u4e0e\u5b66\u751f\u8bf7\u6c42\u7684\u53cd\u9988\u7c7b\u578b\u5339\u914d\u5ea6\u8fbe\u523075%\uff0c\u5e76\u4e14\u80fd\u591f\u9075\u5b88\u7cfb\u7edf\u63d0\u793a\u7ea6\u675f\u3002\u5b9e\u9a8c\u7ed3\u679c\u63ed\u793a\u4e86\u5b66\u751f\u5728\u89e3\u51b3\u7f16\u7a0b\u4efb\u52a1\u65f6\u7684\u53cd\u9988\u9700\u6c42\u89c4\u5f8b\u3002", "conclusion": "\u7814\u7a76\u4e3a\u8bbe\u8ba1\u57fa\u4e8e\u751f\u6210\u5f0fAI\u7684\u5b66\u4e60\u652f\u6301\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\uff0c\u5f3a\u8c03\u4e86\u5728AI\u8f85\u52a9\u5de5\u5177\u4e2d\u5e73\u8861\u6307\u5bfc\u6027\u548c\u7075\u6d3b\u6027\u7684\u6311\u6218\u3002SCRIPT\u7684\u6210\u529f\u5e94\u7528\u5c55\u793a\u4e86AI\u804a\u5929\u673a\u5668\u4eba\u5728\u7f16\u7a0b\u6559\u80b2\u4e2d\u7684\u6f5c\u529b\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u672a\u6765\u6539\u8fdb\u7684\u65b9\u5411\u3002"}}
{"id": "2507.16853", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.16853", "abs": "https://arxiv.org/abs/2507.16853", "authors": ["Ning Li", "Xiangmou Qu", "Jiamu Zhou", "Jun Wang", "Muning Wen", "Kounianhua Du", "Xingyu Lou", "Qiuying Peng", "Jun Wang", "Weinan Zhang"], "title": "MobileUse: A GUI Agent with Hierarchical Reflection for Autonomous Mobile Operation", "comment": "A technical report on a GUI agent based on multi-agent systems", "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have enabled the\ndevelopment of mobile agents that can understand visual inputs and follow user\ninstructions, unlocking new possibilities for automating complex tasks on\nmobile devices. However, applying these models to real-world mobile scenarios\nremains a significant challenge due to the long-horizon task execution,\ndifficulty in error recovery, and the cold-start problem in unfamiliar\nenvironments. To address these challenges, we propose MobileUse, a GUI agent\ndesigned for robust and adaptive mobile task execution. To improve resilience\nin long-horizon tasks and dynamic environments, we introduce a hierarchical\nreflection architecture that enables the agent to self-monitor, detect, and\nrecover from errors across multiple temporal scales-ranging from individual\nactions to overall task completion-while maintaining efficiency through a\nreflection-on-demand strategy. To tackle cold-start issues, we further\nintroduce a proactive exploration module, which enriches the agent's\nunderstanding of the environment through self-planned exploration. Evaluations\non AndroidWorld and AndroidLab benchmarks demonstrate that MobileUse\nestablishes new state-of-the-art performance, achieving success rates of 62.9%\nand 44.2%, respectively. To facilitate real-world applications, we release an\nout-of-the-box toolkit for automated task execution on physical mobile devices,\nwhich is available at https://github.com/MadeAgents/mobile-use.", "AI": {"tldr": "\u63d0\u51fa\u4e86MobileUse\uff0c\u4e00\u4e2a\u7528\u4e8e\u79fb\u52a8\u8bbe\u5907GUI\u81ea\u52a8\u5316\u7684\u667a\u80fd\u4ee3\u7406\uff0c\u901a\u8fc7\u5206\u5c42\u53cd\u601d\u67b6\u6784\u548c\u4e3b\u52a8\u63a2\u7d22\u6a21\u5757\u89e3\u51b3\u957f\u671f\u4efb\u52a1\u6267\u884c\u3001\u9519\u8bef\u6062\u590d\u548c\u51b7\u542f\u52a8\u95ee\u9898\uff0c\u5728AndroidWorld\u548cAndroidLab\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u79fb\u52a8\u573a\u666f\u4e2d\u9762\u4e34\u957f\u671f\u4efb\u52a1\u6267\u884c\u56f0\u96be\u3001\u9519\u8bef\u6062\u590d\u80fd\u529b\u4e0d\u8db3\u4ee5\u53ca\u5728\u964c\u751f\u73af\u5883\u4e2d\u7684\u51b7\u542f\u52a8\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5728\u771f\u5b9e\u4e16\u754c\u79fb\u52a8\u8bbe\u5907\u81ea\u52a8\u5316\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u8bbe\u8ba1\u4e86MobileUse GUI\u4ee3\u7406\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a1) \u5206\u5c42\u53cd\u601d\u67b6\u6784\uff0c\u80fd\u591f\u5728\u591a\u4e2a\u65f6\u95f4\u5c3a\u5ea6\u4e0a\u8fdb\u884c\u81ea\u6211\u76d1\u63a7\u3001\u9519\u8bef\u68c0\u6d4b\u548c\u6062\u590d\uff0c\u5e76\u91c7\u7528\u6309\u9700\u53cd\u601d\u7b56\u7565\u4fdd\u6301\u6548\u7387\uff1b2) \u4e3b\u52a8\u63a2\u7d22\u6a21\u5757\uff0c\u901a\u8fc7\u81ea\u4e3b\u89c4\u5212\u7684\u63a2\u7d22\u6765\u4e30\u5bcc\u4ee3\u7406\u5bf9\u73af\u5883\u7684\u7406\u89e3\u3002", "result": "\u5728AndroidWorld\u548cAndroidLab\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5206\u522b\u8fbe\u523062.9%\u548c44.2%\u7684\u6210\u529f\u7387\uff0c\u5efa\u7acb\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\u8bb0\u5f55\uff0c\u5e76\u53d1\u5e03\u4e86\u53ef\u7528\u4e8e\u7269\u7406\u79fb\u52a8\u8bbe\u5907\u81ea\u52a8\u5316\u4efb\u52a1\u6267\u884c\u7684\u5f00\u7bb1\u5373\u7528\u5de5\u5177\u5305\u3002", "conclusion": "MobileUse\u901a\u8fc7\u5206\u5c42\u53cd\u601d\u548c\u4e3b\u52a8\u63a2\u7d22\u6709\u6548\u89e3\u51b3\u4e86\u79fb\u52a8\u8bbe\u5907GUI\u81ea\u52a8\u5316\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u7a81\u7834\u6027\u6027\u80fd\uff0c\u4e3a\u5b9e\u9645\u79fb\u52a8\u8bbe\u5907\u81ea\u52a8\u5316\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.17289", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17289", "abs": "https://arxiv.org/abs/2507.17289", "authors": ["Shitong Zhu", "Chenhao Fang", "Derek Larson", "Neel Reddy Pochareddy", "Rajeev Rao", "Sophie Zeng", "Yanqing Peng", "Wendy Summer", "Alex Goncalves", "Arya Pudota", "Herve Robert"], "title": "Compliance Brain Assistant: Conversational Agentic AI for Assisting Compliance Tasks in Enterprise Environments", "comment": null, "summary": "This paper presents Compliance Brain Assistant (CBA), a conversational,\nagentic AI assistant designed to boost the efficiency of daily compliance tasks\nfor personnel in enterprise environments. To strike a good balance between\nresponse quality and latency, we design a user query router that can\nintelligently choose between (i) FastTrack mode: to handle simple requests that\nonly need additional relevant context retrieved from knowledge corpora; and\n(ii) FullAgentic mode: to handle complicated requests that need composite\nactions and tool invocations to proactively discover context across various\ncompliance artifacts, and/or involving other APIs/models for accommodating\nrequests. A typical example would be to start with a user query, use its\ndescription to find a specific entity and then use the entity's information to\nquery other APIs for curating and enriching the final AI response.\n  Our experimental evaluations compared CBA against an out-of-the-box LLM on\nvarious real-world privacy/compliance-related queries targeting various\npersonas. We found that CBA substantially improved upon the vanilla LLM's\nperformance on metrics such as average keyword match rate (83.7% vs. 41.7%) and\nLLM-judge pass rate (82.0% vs. 20.0%). We also compared metrics for the full\nrouting-based design against the `fast-track only` and `full-agentic` modes and\nfound that it had a better average match-rate and pass-rate while keeping the\nrun-time approximately the same. This finding validated our hypothesis that the\nrouting mechanism leads to a good trade-off between the two worlds.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u5408\u89c4\u5927\u8111\u52a9\u624b(CBA)\uff0c\u4e00\u4e2a\u5bf9\u8bdd\u5f0fAI\u52a9\u624b\uff0c\u901a\u8fc7\u667a\u80fd\u8def\u7531\u673a\u5236\u5728\u5feb\u901f\u6a21\u5f0f\u548c\u5168\u4ee3\u7406\u6a21\u5f0f\u95f4\u9009\u62e9\uff0c\u663e\u8457\u63d0\u5347\u4f01\u4e1a\u5408\u89c4\u4efb\u52a1\u6548\u7387\uff0c\u5728\u5173\u952e\u8bcd\u5339\u914d\u7387\u548cLLM\u8bc4\u5224\u901a\u8fc7\u7387\u7b49\u6307\u6807\u4e0a\u5927\u5e45\u8d85\u8d8a\u666e\u901aLLM\u3002", "motivation": "\u4f01\u4e1a\u73af\u5883\u4e2d\u5408\u89c4\u4eba\u5458\u9700\u8981\u5904\u7406\u5927\u91cf\u65e5\u5e38\u5408\u89c4\u4efb\u52a1\uff0c\u73b0\u6709LLM\u5728\u5904\u7406\u590d\u6742\u5408\u89c4\u67e5\u8be2\u65f6\u5b58\u5728\u54cd\u5e94\u8d28\u91cf\u548c\u5ef6\u8fdf\u4e4b\u95f4\u7684\u5e73\u8861\u95ee\u9898\uff0c\u9700\u8981\u4e00\u4e2a\u80fd\u591f\u667a\u80fd\u5904\u7406\u4e0d\u540c\u590d\u6742\u5ea6\u5408\u89c4\u8bf7\u6c42\u7684AI\u52a9\u624b\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7528\u6237\u67e5\u8be2\u8def\u7531\u5668\uff0c\u80fd\u591f\u667a\u80fd\u9009\u62e9\u5904\u7406\u6a21\u5f0f\uff1a(1) FastTrack\u6a21\u5f0f\uff1a\u5904\u7406\u53ea\u9700\u4ece\u77e5\u8bc6\u5e93\u68c0\u7d22\u76f8\u5173\u4e0a\u4e0b\u6587\u7684\u7b80\u5355\u8bf7\u6c42\uff1b(2) FullAgentic\u6a21\u5f0f\uff1a\u5904\u7406\u9700\u8981\u590d\u5408\u884c\u52a8\u548c\u5de5\u5177\u8c03\u7528\u7684\u590d\u6742\u8bf7\u6c42\uff0c\u80fd\u591f\u4e3b\u52a8\u53d1\u73b0\u5404\u79cd\u5408\u89c4\u6587\u4ef6\u4e2d\u7684\u4e0a\u4e0b\u6587\uff0c\u5e76\u8c03\u7528\u5176\u4ed6API/\u6a21\u578b\u3002", "result": "\u4e0e\u5f00\u7bb1\u5373\u7528\u7684LLM\u76f8\u6bd4\uff0cCBA\u5728\u5404\u79cd\u771f\u5b9e\u4e16\u754c\u9690\u79c1/\u5408\u89c4\u76f8\u5173\u67e5\u8be2\u4e0a\u8868\u73b0\u663e\u8457\u63d0\u5347\uff1a\u5e73\u5747\u5173\u952e\u8bcd\u5339\u914d\u7387\u4ece41.7%\u63d0\u5347\u523083.7%\uff0cLLM\u8bc4\u5224\u901a\u8fc7\u7387\u4ece20.0%\u63d0\u5347\u523082.0%\u3002\u57fa\u4e8e\u8def\u7531\u7684\u5b8c\u6574\u8bbe\u8ba1\u5728\u4fdd\u6301\u76f8\u8fd1\u8fd0\u884c\u65f6\u95f4\u7684\u540c\u65f6\uff0c\u83b7\u5f97\u4e86\u66f4\u597d\u7684\u5e73\u5747\u5339\u914d\u7387\u548c\u901a\u8fc7\u7387\u3002", "conclusion": "\u8def\u7531\u673a\u5236\u6210\u529f\u5b9e\u73b0\u4e86\u54cd\u5e94\u8d28\u91cf\u548c\u5ef6\u8fdf\u4e4b\u95f4\u7684\u826f\u597d\u5e73\u8861\uff0c\u9a8c\u8bc1\u4e86\u8bbe\u8ba1\u5047\u8bbe\u3002CBA\u901a\u8fc7\u667a\u80fd\u9009\u62e9\u5904\u7406\u6a21\u5f0f\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u4f01\u4e1a\u5408\u89c4\u4efb\u52a1\u7684\u5904\u7406\u6548\u7387\uff0c\u4e3a\u4f01\u4e1a\u5408\u89c4\u5de5\u4f5c\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684AI\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.16859", "categories": ["cs.RO", "cs.AI", "62H30", "I.2"], "pdf": "https://arxiv.org/pdf/2507.16859", "abs": "https://arxiv.org/abs/2507.16859", "authors": ["Luobin Cui", "Yanlai Wu", "Tang Ying", "Weikai Li"], "title": "Leveraging multi-source and heterogeneous signals for fatigue detection", "comment": "1figures,32pages", "summary": "Fatigue detection plays a critical role in safety-critical applications such\nas aviation, mining, and long-haul transport. However, most existing methods\nrely on high-end sensors and controlled environments, limiting their\napplicability in real world settings. This paper formally defines a practical\nyet underexplored problem setting for real world fatigue detection, where\nsystems operating with context-appropriate sensors aim to leverage knowledge\nfrom differently instrumented sources including those using impractical sensors\ndeployed in controlled environments. To tackle this challenge, we propose a\nheterogeneous and multi-source fatigue detection framework that adaptively\nutilizes the available modalities in the target domain while benefiting from\nthe diverse configurations present in source domains. Our experiments,\nconducted using a realistic field-deployed sensor setup and two publicly\navailable datasets, demonstrate the practicality, robustness, and improved\ngeneralization of our approach, paving the practical way for effective fatigue\nmonitoring in sensor-constrained scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5f02\u6784\u591a\u6e90\u75b2\u52b3\u68c0\u6d4b\u6846\u67b6\uff0c\u80fd\u591f\u5728\u4f20\u611f\u5668\u53d7\u9650\u7684\u771f\u5b9e\u73af\u5883\u4e2d\u6709\u6548\u68c0\u6d4b\u75b2\u52b3\u72b6\u6001\uff0c\u901a\u8fc7\u5229\u7528\u4e0d\u540c\u4f20\u611f\u5668\u914d\u7f6e\u7684\u6e90\u57df\u77e5\u8bc6\u6765\u63d0\u5347\u76ee\u6807\u57df\u7684\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u75b2\u52b3\u68c0\u6d4b\u65b9\u6cd5\u4f9d\u8d56\u9ad8\u7aef\u4f20\u611f\u5668\u548c\u53d7\u63a7\u73af\u5883\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u5b58\u5728\u5c40\u9650\u6027\u3002\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u5728\u4f20\u611f\u5668\u53d7\u9650\u7684\u5b9e\u9645\u573a\u666f\u4e2d\u8fdb\u884c\u6709\u6548\u75b2\u52b3\u76d1\u6d4b\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u80fd\u591f\u5229\u7528\u6765\u81ea\u4e0d\u540c\u4f20\u611f\u5668\u914d\u7f6e\u73af\u5883\u7684\u77e5\u8bc6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5f02\u6784\u591a\u6e90\u75b2\u52b3\u68c0\u6d4b\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u81ea\u9002\u5e94\u5730\u5229\u7528\u76ee\u6807\u57df\u4e2d\u53ef\u7528\u7684\u6a21\u6001\uff0c\u540c\u65f6\u4ece\u6e90\u57df\u7684\u591a\u6837\u5316\u914d\u7f6e\u4e2d\u83b7\u76ca\u3002\u6846\u67b6\u901a\u8fc7\u8de8\u57df\u77e5\u8bc6\u8fc1\u79fb\u6765\u5904\u7406\u4e0d\u540c\u4f20\u611f\u5668\u914d\u7f6e\u4e4b\u95f4\u7684\u5dee\u5f02\u3002", "result": "\u5728\u771f\u5b9e\u90e8\u7f72\u7684\u4f20\u611f\u5668\u8bbe\u7f6e\u548c\u4e24\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u5b9e\u7528\u6027\u3001\u9c81\u68d2\u6027\u548c\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u80fd\u591f\u5728\u4f20\u611f\u5668\u53d7\u9650\u7684\u573a\u666f\u4e2d\u5b9e\u73b0\u6709\u6548\u7684\u75b2\u52b3\u76d1\u6d4b\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5728\u4f20\u611f\u5668\u53d7\u9650\u573a\u666f\u4e2d\u8fdb\u884c\u6709\u6548\u75b2\u52b3\u76d1\u6d4b\u94fa\u5e73\u4e86\u5b9e\u7528\u9053\u8def\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u771f\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u7684\u5c40\u9650\u6027\u95ee\u9898\uff0c\u4e3a\u822a\u7a7a\u3001\u91c7\u77ff\u3001\u957f\u9014\u8fd0\u8f93\u7b49\u5b89\u5168\u5173\u952e\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.17418", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.17418", "abs": "https://arxiv.org/abs/2507.17418", "authors": ["Joobin Jin", "Seokjun Hong", "Gyeongseon Baek", "Yeeun Kim", "Byeongjoon Noh"], "title": "Ctx2TrajGen: Traffic Context-Aware Microscale Vehicle Trajectories using Generative Adversarial Imitation Learning", "comment": null, "summary": "Precise modeling of microscopic vehicle trajectories is critical for traffic\nbehavior analysis and autonomous driving systems. We propose Ctx2TrajGen, a\ncontext-aware trajectory generation framework that synthesizes realistic urban\ndriving behaviors using GAIL. Leveraging PPO and WGAN-GP, our model addresses\nnonlinear interdependencies and training instability inherent in microscopic\nsettings. By explicitly conditioning on surrounding vehicles and road geometry,\nCtx2TrajGen generates interaction-aware trajectories aligned with real-world\ncontext. Experiments on the drone-captured DRIFT dataset demonstrate superior\nperformance over existing methods in terms of realism, behavioral diversity,\nand contextual fidelity, offering a robust solution to data scarcity and domain\nshift without simulation.", "AI": {"tldr": "\u63d0\u51fa\u4e86Ctx2TrajGen\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8eGAIL\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u8f68\u8ff9\u751f\u6210\u6a21\u578b\uff0c\u80fd\u591f\u5408\u6210\u771f\u5b9e\u7684\u57ce\u5e02\u9a7e\u9a76\u884c\u4e3a\uff0c\u89e3\u51b3\u4e86\u5fae\u89c2\u4ea4\u901a\u5efa\u6a21\u4e2d\u7684\u975e\u7ebf\u6027\u76f8\u4e92\u4f9d\u8d56\u548c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u95ee\u9898", "motivation": "\u7cbe\u786e\u5efa\u6a21\u5fae\u89c2\u8f66\u8f86\u8f68\u8ff9\u5bf9\u4ea4\u901a\u884c\u4e3a\u5206\u6790\u548c\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u5fae\u89c2\u73af\u5883\u4e2d\u7684\u975e\u7ebf\u6027\u76f8\u4e92\u4f9d\u8d56\u5173\u7cfb\u548c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u540c\u65f6\u9762\u4e34\u6570\u636e\u7a00\u7f3a\u548c\u9886\u57df\u504f\u79fb\u7684\u6311\u6218", "method": "\u63d0\u51faCtx2TrajGen\u6846\u67b6\uff0c\u7ed3\u5408\u751f\u6210\u5bf9\u6297\u6a21\u4eff\u5b66\u4e60(GAIL)\u3001\u8fd1\u7aef\u7b56\u7565\u4f18\u5316(PPO)\u548c\u6539\u8fdb\u7684Wasserstein\u751f\u6210\u5bf9\u6297\u7f51\u7edc(WGAN-GP)\uff0c\u901a\u8fc7\u663e\u5f0f\u5730\u5bf9\u5468\u56f4\u8f66\u8f86\u548c\u9053\u8def\u51e0\u4f55\u7ed3\u6784\u8fdb\u884c\u6761\u4ef6\u5316\u5efa\u6a21\uff0c\u751f\u6210\u5177\u6709\u4ea4\u4e92\u611f\u77e5\u80fd\u529b\u7684\u8f68\u8ff9", "result": "\u5728\u65e0\u4eba\u673a\u6355\u83b7\u7684DRIFT\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u771f\u5b9e\u6027\u3001\u884c\u4e3a\u591a\u6837\u6027\u548c\u4e0a\u4e0b\u6587\u4fdd\u771f\u5ea6\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u548c\u9886\u57df\u504f\u79fb\u95ee\u9898", "conclusion": "Ctx2TrajGen\u4e3a\u5fae\u89c2\u8f66\u8f86\u8f68\u8ff9\u751f\u6210\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u65e0\u9700\u4eff\u771f\u7684\u60c5\u51b5\u4e0b\u751f\u6210\u4e0e\u771f\u5b9e\u4e16\u754c\u4e0a\u4e0b\u6587\u4e00\u81f4\u7684\u4ea4\u4e92\u611f\u77e5\u8f68\u8ff9\uff0c\u4e3a\u4ea4\u901a\u884c\u4e3a\u5206\u6790\u548c\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6709\u6548\u652f\u6301"}}
{"id": "2507.16865", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.16865", "abs": "https://arxiv.org/abs/2507.16865", "authors": ["Shanshan Zhang", "Tianshui Wen", "Siyue Wang", "Qi Zhang", "Ziheng Zhou", "Huiru Zheng", "Lingxiang Zheng", "Yu Yang"], "title": "ResKACNNet: A Residual ChebyKAN Network for Inertial Odometry", "comment": null, "summary": "Inertial Measurement Unit (IMU) has become a key technology for achieving\nlow-cost and precise positioning. However, traditional CNN-based inertial\npositioning methods struggle to capture the nonlinear motion characteristics\nand long-term dependencies in IMU data. To address this limitation, we propose\na novel inertial positioning network with a generic backbone called\nResChebyKAN, which leverages the nonlinear approximation capabilities of\nChebyshev polynomials to model complex motion patterns. Additionally, we\nintroduce an Efficient Kernel-based Self-Attention (EKSA) module to effectively\ncapture contextual information and enhance long-term dependency modeling.\nExperimental results on public datasets (e.g., RIDI, RoNIN, RNIN-VIO, OxIOD,\nIMUNet, and TLIO) demonstrate that our method reduces the absolute trajectory\nerror by 3.79% to 42.32% compared to existing benchmark methods. Furthermore,\nwe release a preprocessed dataset and empirically show that removing the\ngravity component from acceleration data significantly improves inertial\npositioning performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eResChebyKAN\u9aa8\u5e72\u7f51\u7edc\u548cEKSA\u6ce8\u610f\u529b\u6a21\u5757\u7684\u60ef\u6027\u5b9a\u4f4d\u65b9\u6cd5\uff0c\u901a\u8fc7\u5207\u6bd4\u96ea\u592b\u591a\u9879\u5f0f\u5efa\u6a21\u975e\u7ebf\u6027\u8fd0\u52a8\u6a21\u5f0f\uff0c\u5728\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u663e\u8457\u964d\u4f4e\u4e86\u8f68\u8ff9\u8bef\u5dee", "motivation": "\u4f20\u7edf\u57fa\u4e8eCNN\u7684\u60ef\u6027\u5b9a\u4f4d\u65b9\u6cd5\u96be\u4ee5\u6355\u83b7IMU\u6570\u636e\u4e2d\u7684\u975e\u7ebf\u6027\u8fd0\u52a8\u7279\u5f81\u548c\u957f\u671f\u4f9d\u8d56\u5173\u7cfb\uff0c\u9650\u5236\u4e86\u5b9a\u4f4d\u7cbe\u5ea6\u7684\u63d0\u5347", "method": "\u63d0\u51faResChebyKAN\u901a\u7528\u9aa8\u5e72\u7f51\u7edc\uff0c\u5229\u7528\u5207\u6bd4\u96ea\u592b\u591a\u9879\u5f0f\u7684\u975e\u7ebf\u6027\u903c\u8fd1\u80fd\u529b\u5efa\u6a21\u590d\u6742\u8fd0\u52a8\u6a21\u5f0f\uff1b\u5f15\u5165\u9ad8\u6548\u6838\u57fa\u81ea\u6ce8\u610f\u529b(EKSA)\u6a21\u5757\u6355\u83b7\u4e0a\u4e0b\u6587\u4fe1\u606f\u5e76\u589e\u5f3a\u957f\u671f\u4f9d\u8d56\u5efa\u6a21", "result": "\u5728RIDI\u3001RoNIN\u3001RNIN-VIO\u3001OxIOD\u3001IMUNet\u548cTLIO\u7b49\u516c\u5f00\u6570\u636e\u96c6\u4e0a\uff0c\u76f8\u6bd4\u73b0\u6709\u57fa\u51c6\u65b9\u6cd5\uff0c\u7edd\u5bf9\u8f68\u8ff9\u8bef\u5dee\u964d\u4f4e\u4e863.79%\u523042.32%\uff1b\u5b9e\u9a8c\u8bc1\u660e\u53bb\u9664\u52a0\u901f\u5ea6\u6570\u636e\u4e2d\u7684\u91cd\u529b\u5206\u91cf\u53ef\u663e\u8457\u6539\u5584\u60ef\u6027\u5b9a\u4f4d\u6027\u80fd", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8eResChebyKAN\u548cEKSA\u7684\u60ef\u6027\u5b9a\u4f4d\u7f51\u7edc\u80fd\u591f\u6709\u6548\u5904\u7406IMU\u6570\u636e\u7684\u975e\u7ebf\u6027\u7279\u5f81\u548c\u957f\u671f\u4f9d\u8d56\uff0c\u663e\u8457\u63d0\u5347\u4e86\u60ef\u6027\u5b9a\u4f4d\u7684\u7cbe\u5ea6\uff0c\u4e3a\u4f4e\u6210\u672c\u7cbe\u786e\u5b9a\u4f4d\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2507.17477", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17477", "abs": "https://arxiv.org/abs/2507.17477", "authors": ["Haoran Sun", "Zekun Zhang", "Shaoning Zeng"], "title": "An Uncertainty-Driven Adaptive Self-Alignment Framework for Large Language Models", "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable progress in\ninstruction following and general-purpose reasoning. However, achieving\nhigh-quality alignment with human intent and safety norms without human\nannotations remains a fundamental challenge. In this work, we propose an\nUncertainty-Driven Adaptive Self-Alignment (UDASA) framework designed to\nimprove LLM alignment in a fully automated manner. UDASA first generates\nmultiple responses for each input and quantifies output uncertainty across\nthree dimensions: semantics, factuality, and value alignment. Based on these\nuncertainty scores, the framework constructs preference pairs and categorizes\ntraining samples into three stages, conservative, moderate, and exploratory,\naccording to their uncertainty difference. The model is then optimized\nprogressively across these stages. In addition, we conduct a series of\npreliminary studies to validate the core design assumptions and provide strong\nempirical motivation for the proposed framework. Experimental results show that\nUDASA outperforms existing alignment methods across multiple tasks, including\nharmlessness, helpfulness, truthfulness, and controlled sentiment generation,\nsignificantly improving model performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e0d\u786e\u5b9a\u6027\u9a71\u52a8\u7684\u81ea\u9002\u5e94\u81ea\u5bf9\u9f50\u6846\u67b6(UDASA)\uff0c\u901a\u8fc7\u91cf\u5316\u8f93\u51fa\u4e0d\u786e\u5b9a\u6027\u5e76\u5206\u9636\u6bb5\u8bad\u7ec3\u6765\u81ea\u52a8\u6539\u5584\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u4eba\u7c7b\u610f\u56fe\u7684\u5bf9\u9f50\uff0c\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\u5373\u53ef\u663e\u8457\u63d0\u5347\u6a21\u578b\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6307\u4ee4\u9075\u5faa\u548c\u901a\u7528\u63a8\u7406\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5728\u6ca1\u6709\u4eba\u5de5\u6807\u6ce8\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e0e\u4eba\u7c7b\u610f\u56fe\u548c\u5b89\u5168\u89c4\u8303\u7684\u9ad8\u8d28\u91cf\u5bf9\u9f50\u4ecd\u7136\u662f\u4e00\u4e2a\u6839\u672c\u6027\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5728\u5b8c\u5168\u81ea\u52a8\u5316\u7684\u60c5\u51b5\u4e0b\u6709\u6548\u63d0\u5347LLM\u7684\u5bf9\u9f50\u6027\u80fd\u3002", "method": "\u63d0\u51faUDASA\u6846\u67b6\uff1a(1)\u4e3a\u6bcf\u4e2a\u8f93\u5165\u751f\u6210\u591a\u4e2a\u54cd\u5e94\uff1b(2)\u4ece\u8bed\u4e49\u3001\u4e8b\u5b9e\u6027\u548c\u4ef7\u503c\u5bf9\u9f50\u4e09\u4e2a\u7ef4\u5ea6\u91cf\u5316\u8f93\u51fa\u4e0d\u786e\u5b9a\u6027\uff1b(3)\u57fa\u4e8e\u4e0d\u786e\u5b9a\u6027\u5206\u6570\u6784\u5efa\u504f\u597d\u5bf9\uff1b(4)\u6839\u636e\u4e0d\u786e\u5b9a\u6027\u5dee\u5f02\u5c06\u8bad\u7ec3\u6837\u672c\u5206\u4e3a\u4fdd\u5b88\u3001\u9002\u4e2d\u3001\u63a2\u7d22\u4e09\u4e2a\u9636\u6bb5\uff1b(5)\u5728\u8fd9\u4e9b\u9636\u6bb5\u4e2d\u9010\u6b65\u4f18\u5316\u6a21\u578b\u3002\u6b64\u5916\u8fd8\u8fdb\u884c\u4e86\u4e00\u7cfb\u5217\u9884\u5907\u7814\u7a76\u6765\u9a8c\u8bc1\u6838\u5fc3\u8bbe\u8ba1\u5047\u8bbe\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cUDASA\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u5bf9\u9f50\u65b9\u6cd5\uff0c\u5305\u62ec\u65e0\u5bb3\u6027\u3001\u6709\u7528\u6027\u3001\u771f\u5b9e\u6027\u548c\u53d7\u63a7\u60c5\u611f\u751f\u6210\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002\u6846\u67b6\u80fd\u591f\u5728\u5b8c\u5168\u81ea\u52a8\u5316\u7684\u60c5\u51b5\u4e0b\u6709\u6548\u6539\u5584LLM\u4e0e\u4eba\u7c7b\u610f\u56fe\u7684\u5bf9\u9f50\u3002", "conclusion": "UDASA\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u5728\u65e0\u4eba\u5de5\u6807\u6ce8\u60c5\u51b5\u4e0b\u63d0\u5347LLM\u5bf9\u9f50\u8d28\u91cf\u7684\u6311\u6218\uff0c\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u548c\u5206\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u6a21\u578b\u6027\u80fd\u7684\u663e\u8457\u6539\u5584\uff0c\u4e3a\u81ea\u52a8\u5316LLM\u5bf9\u9f50\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.16941", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.16941", "abs": "https://arxiv.org/abs/2507.16941", "authors": ["Daniel Correa", "Tero Kaarlela", "Jose Fuentes", "Paulo Padrao", "Alain Duran", "Leonardo Bobadilla"], "title": "Multi-agent Reinforcement Learning for Robotized Coral Reef Sample Collection", "comment": null, "summary": "This paper presents a reinforcement learning (RL) environment for developing\nan autonomous underwater robotic coral sampling agent, a crucial coral reef\nconservation and research task. Using software-in-the-loop (SIL) and\nhardware-in-the-loop (HIL), an RL-trained artificial intelligence (AI)\ncontroller is developed using a digital twin (DT) in simulation and\nsubsequently verified in physical experiments. An underwater motion capture\n(MOCAP) system provides real-time 3D position and orientation feedback during\nverification testing for precise synchronization between the digital and\nphysical domains. A key novelty of this approach is the combined use of a\ngeneral-purpose game engine for simulation, deep RL, and real-time underwater\nmotion capture for an effective zero-shot sim-to-real strategy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5f3a\u5316\u5b66\u4e60\u73af\u5883\uff0c\u7528\u4e8e\u5f00\u53d1\u81ea\u4e3b\u6c34\u4e0b\u673a\u5668\u4eba\u73ca\u745a\u91c7\u6837\u4ee3\u7406\uff0c\u901a\u8fc7\u8f6f\u4ef6\u5728\u73af\u548c\u786c\u4ef6\u5728\u73af\u65b9\u6cd5\uff0c\u7ed3\u5408\u6570\u5b57\u5b6a\u751f\u6280\u672f\u548c\u6c34\u4e0b\u8fd0\u52a8\u6355\u6349\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4e86\u4ece\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u96f6\u6837\u672c\u8fc1\u79fb\u7b56\u7565\u3002", "motivation": "\u73ca\u745a\u7901\u4fdd\u62a4\u548c\u7814\u7a76\u9700\u8981\u81ea\u4e3b\u6c34\u4e0b\u673a\u5668\u4eba\u8fdb\u884c\u73ca\u745a\u91c7\u6837\uff0c\u8fd9\u662f\u4e00\u9879\u5173\u952e\u4f46\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5728\u590d\u6742\u6c34\u4e0b\u73af\u5883\u4e2d\u7cbe\u786e\u64cd\u4f5c\u7684\u667a\u80fd\u63a7\u5236\u7cfb\u7edf\u3002", "method": "\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u7ed3\u5408\u8f6f\u4ef6\u5728\u73af(SIL)\u548c\u786c\u4ef6\u5728\u73af(HIL)\u6280\u672f\uff0c\u4f7f\u7528\u6570\u5b57\u5b6a\u751f\u5728\u4eff\u771f\u73af\u5883\u4e2d\u8bad\u7ec3AI\u63a7\u5236\u5668\uff0c\u5e76\u901a\u8fc7\u6c34\u4e0b\u8fd0\u52a8\u6355\u6349\u7cfb\u7edf\u63d0\u4f9b\u5b9e\u65f63D\u4f4d\u7f6e\u548c\u65b9\u5411\u53cd\u9988\uff0c\u5b9e\u73b0\u6570\u5b57\u57df\u548c\u7269\u7406\u57df\u4e4b\u95f4\u7684\u7cbe\u786e\u540c\u6b65\u3002", "result": "\u6210\u529f\u5f00\u53d1\u4e86\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u81ea\u4e3b\u6c34\u4e0b\u673a\u5668\u4eba\u63a7\u5236\u5668\uff0c\u5e76\u901a\u8fc7\u7269\u7406\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u5b9e\u73b0\u4e86\u4ece\u4eff\u771f\u5230\u73b0\u5b9e\u73af\u5883\u7684\u6709\u6548\u8fc1\u79fb\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u901a\u7528\u6e38\u620f\u5f15\u64ce\u4eff\u771f\u3001\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u548c\u5b9e\u65f6\u6c34\u4e0b\u8fd0\u52a8\u6355\u6349\u6280\u672f\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u96f6\u6837\u672c\u4eff\u771f\u5230\u73b0\u5b9e\u8fc1\u79fb\u7b56\u7565\uff0c\u4e3a\u6c34\u4e0b\u673a\u5668\u4eba\u73ca\u745a\u91c7\u6837\u4efb\u52a1\u63d0\u4f9b\u4e86\u521b\u65b0\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.17482", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17482", "abs": "https://arxiv.org/abs/2507.17482", "authors": ["Luca Salvatore Lorello", "Nikolaos Manginas", "Marco Lippi", "Stefano Melacci"], "title": "LTLZinc: a Benchmarking Framework for Continual Learning and Neuro-Symbolic Temporal Reasoning", "comment": null, "summary": "Neuro-symbolic artificial intelligence aims to combine neural architectures\nwith symbolic approaches that can represent knowledge in a human-interpretable\nformalism. Continual learning concerns with agents that expand their knowledge\nover time, improving their skills while avoiding to forget previously learned\nconcepts. Most of the existing approaches for neuro-symbolic artificial\nintelligence are applied to static scenarios only, and the challenging setting\nwhere reasoning along the temporal dimension is necessary has been seldom\nexplored. In this work we introduce LTLZinc, a benchmarking framework that can\nbe used to generate datasets covering a variety of different problems, against\nwhich neuro-symbolic and continual learning methods can be evaluated along the\ntemporal and constraint-driven dimensions. Our framework generates expressive\ntemporal reasoning and continual learning tasks from a linear temporal logic\nspecification over MiniZinc constraints, and arbitrary image classification\ndatasets. Fine-grained annotations allow multiple neural and neuro-symbolic\ntraining settings on the same generated datasets. Experiments on six\nneuro-symbolic sequence classification and four class-continual learning tasks\ngenerated by LTLZinc, demonstrate the challenging nature of temporal learning\nand reasoning, and highlight limitations of current state-of-the-art methods.\nWe release the LTLZinc generator and ten ready-to-use tasks to the\nneuro-symbolic and continual learning communities, in the hope of fostering\nresearch towards unified temporal learning and reasoning frameworks.", "AI": {"tldr": "\u7814\u7a76\u8005\u63d0\u51fa\u4e86LTLZinc\u57fa\u51c6\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u65f6\u5e8f\u63a8\u7406\u548c\u6301\u7eed\u5b66\u4e60\u4efb\u52a1\uff0c\u4ee5\u8bc4\u4f30\u795e\u7ecf\u7b26\u53f7AI\u65b9\u6cd5\u5728\u65f6\u95f4\u7ef4\u5ea6\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u53d1\u73b0\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u795e\u7ecf\u7b26\u53f7AI\u65b9\u6cd5\u5927\u591a\u53ea\u5e94\u7528\u4e8e\u9759\u6001\u573a\u666f\uff0c\u7f3a\u4e4f\u5728\u65f6\u95f4\u7ef4\u5ea6\u4e0a\u8fdb\u884c\u63a8\u7406\u7684\u80fd\u529b\uff0c\u800c\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u65f6\u5e8f\u63a8\u7406\u662f\u4e00\u4e2a\u5f88\u5c11\u88ab\u63a2\u7d22\u4f46\u5177\u6709\u6311\u6218\u6027\u7684\u91cd\u8981\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u4e86LTLZinc\u57fa\u51c6\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u7ebf\u6027\u65f6\u5e8f\u903b\u8f91\u89c4\u8303\u7ed3\u5408MiniZinc\u7ea6\u675f\u548c\u4efb\u610f\u56fe\u50cf\u5206\u7c7b\u6570\u636e\u96c6\uff0c\u751f\u6210\u8868\u8fbe\u6027\u4e30\u5bcc\u7684\u65f6\u5e8f\u63a8\u7406\u548c\u6301\u7eed\u5b66\u4e60\u4efb\u52a1\uff0c\u5e76\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u6807\u6ce8\u652f\u6301\u591a\u79cd\u795e\u7ecf\u548c\u795e\u7ecf\u7b26\u53f7\u8bad\u7ec3\u8bbe\u7f6e\u3002", "result": "\u5728LTLZinc\u751f\u6210\u7684\u516d\u4e2a\u795e\u7ecf\u7b26\u53f7\u5e8f\u5217\u5206\u7c7b\u4efb\u52a1\u548c\u56db\u4e2a\u7c7b\u6301\u7eed\u5b66\u4e60\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u65f6\u5e8f\u5b66\u4e60\u548c\u63a8\u7406\u5177\u6709\u6311\u6218\u6027\uff0c\u5e76\u7a81\u51fa\u4e86\u5f53\u524d\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "conclusion": "LTLZinc\u6846\u67b6\u6210\u529f\u63ed\u793a\u4e86\u73b0\u6709\u795e\u7ecf\u7b26\u53f7AI\u548c\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\u5728\u65f6\u5e8f\u63a8\u7406\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u4e3a\u795e\u7ecf\u7b26\u53f7\u548c\u6301\u7eed\u5b66\u4e60\u793e\u533a\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u57fa\u51c6\u5de5\u5177\uff0c\u6709\u671b\u63a8\u52a8\u7edf\u4e00\u65f6\u5e8f\u5b66\u4e60\u548c\u63a8\u7406\u6846\u67b6\u7684\u7814\u7a76\u53d1\u5c55\u3002"}}
{"id": "2507.16988", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.16988", "abs": "https://arxiv.org/abs/2507.16988", "authors": ["Maaz Qureshi", "Mohammad Omid Bagheri", "Abdelrahman Elbadrawy", "William Melek", "George Shaker"], "title": "RAPTAR: Radar Radiation Pattern Acquisition through Automated Collaborative Robotics", "comment": "8 Pages, IEEE Journal", "summary": "Accurate characterization of modern on-chip antennas remains challenging, as\ncurrent probe-station techniques offer limited angular coverage, rely on\nbespoke hardware, and require frequent manual alignment. This research\nintroduces RAPTAR (Radiation Pattern Acquisition through Robotic Automation), a\nportable, state-of-the-art, and autonomous system based on collaborative\nrobotics. RAPTAR enables 3D radiation-pattern measurement of integrated radar\nmodules without dedicated anechoic facilities. The system is designed to\naddress the challenges of testing radar modules mounted in diverse real-world\nconfigurations, including vehicles, UAVs, AR/VR headsets, and biomedical\ndevices, where traditional measurement setups are impractical. A\n7-degree-of-freedom Franka cobot holds the receiver probe and performs\ncollision-free manipulation across a hemispherical spatial domain, guided by\nreal-time motion planning and calibration accuracy with RMS error below 0.9 mm.\nThe system achieves an angular resolution upto 2.5 degree and integrates\nseamlessly with RF instrumentation for near- and far-field power measurements.\nExperimental scans of a 60 GHz radar module show a mean absolute error of less\nthan 2 dB compared to full-wave electromagnetic simulations ground truth.\nBenchmarking against baseline method demonstrates 36.5% lower mean absolute\nerror, highlighting RAPTAR accuracy and repeatability.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86RAPTAR\u7cfb\u7edf\uff0c\u4e00\u4e2a\u57fa\u4e8e\u534f\u4f5c\u673a\u5668\u4eba\u7684\u4fbf\u643a\u5f0f\u81ea\u4e3b\u7cfb\u7edf\uff0c\u7528\u4e8e\u6d4b\u91cf\u96c6\u6210\u96f7\u8fbe\u6a21\u5757\u76843D\u8f90\u5c04\u65b9\u5411\u56fe\uff0c\u65e0\u9700\u4e13\u95e8\u7684\u7535\u6ce2\u6697\u5ba4\u8bbe\u65bd", "motivation": "\u73b0\u6709\u63a2\u9488\u53f0\u6280\u672f\u5728\u7247\u4e0a\u5929\u7ebf\u8868\u5f81\u65b9\u9762\u5b58\u5728\u89d2\u5ea6\u8986\u76d6\u6709\u9650\u3001\u4f9d\u8d56\u5b9a\u5236\u786c\u4ef6\u3001\u9700\u8981\u9891\u7e41\u624b\u52a8\u5bf9\u51c6\u7b49\u6311\u6218\uff0c\u4e14\u4f20\u7edf\u6d4b\u91cf\u8bbe\u7f6e\u5728\u8f66\u8f86\u3001\u65e0\u4eba\u673a\u3001AR/VR\u5934\u663e\u7b49\u771f\u5b9e\u5e94\u7528\u573a\u666f\u4e2d\u4e0d\u5b9e\u7528", "method": "\u91c7\u75287\u81ea\u7531\u5ea6Franka\u534f\u4f5c\u673a\u5668\u4eba\u6301\u6709\u63a5\u6536\u63a2\u9488\uff0c\u5728\u534a\u7403\u7a7a\u95f4\u57df\u5185\u8fdb\u884c\u65e0\u78b0\u649e\u64cd\u4f5c\uff0c\u7ed3\u5408\u5b9e\u65f6\u8fd0\u52a8\u89c4\u5212\u548c\u6821\u51c6\u6280\u672f\uff0c\u4e0e\u5c04\u9891\u4eea\u5668\u96c6\u6210\u8fdb\u884c\u8fd1\u573a\u548c\u8fdc\u573a\u529f\u7387\u6d4b\u91cf", "result": "\u7cfb\u7edf\u6821\u51c6\u7cbe\u5ea6RMS\u8bef\u5dee\u4f4e\u4e8e0.9\u6beb\u7c73\uff0c\u89d2\u5ea6\u5206\u8fa8\u7387\u53ef\u8fbe2.5\u5ea6\uff0c60GHz\u96f7\u8fbe\u6a21\u5757\u5b9e\u9a8c\u626b\u63cf\u4e0e\u5168\u6ce2\u7535\u78c1\u4eff\u771f\u771f\u503c\u5bf9\u6bd4\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u5c0f\u4e8e2dB\uff0c\u76f8\u6bd4\u57fa\u51c6\u65b9\u6cd5\u964d\u4f4e36.5%\u7684\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee", "conclusion": "RAPTAR\u7cfb\u7edf\u6210\u529f\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u3001\u53ef\u91cd\u590d\u7684\u96c6\u6210\u96f7\u8fbe\u6a21\u57573D\u8f90\u5c04\u65b9\u5411\u56fe\u6d4b\u91cf\uff0c\u4e3a\u591a\u6837\u5316\u5b9e\u9645\u5e94\u7528\u573a\u666f\u4e2d\u7684\u96f7\u8fbe\u6a21\u5757\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848"}}
{"id": "2507.17487", "categories": ["cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2507.17487", "abs": "https://arxiv.org/abs/2507.17487", "authors": ["Lorenzo Marconi", "Flavia Ricci", "Riccardo Rosati"], "title": "CQE under Epistemic Dependencies: Algorithms and Experiments (extended version)", "comment": "Extended version of paper accepted at the 24th International Semantic\n  Web Conference (ISWC 2025)", "summary": "We investigate Controlled Query Evaluation (CQE) over ontologies, where\ninformation disclosure is regulated by epistemic dependencies (EDs), a family\nof logical rules recently proposed for the CQE framework. In particular, we\ncombine EDs with the notion of optimal GA censors, i.e. maximal sets of ground\natoms that are entailed by the ontology and can be safely revealed. We focus on\nanswering Boolean unions of conjunctive queries (BUCQs) with respect to the\nintersection of all optimal GA censors - an approach that has been shown in\nother contexts to ensure strong security guarantees with favorable\ncomputational behavior. First, we characterize the security of this\nintersection-based approach and identify a class of EDs (namely, full EDs) for\nwhich it remains safe. Then, for a subclass of EDs and for DL-Lite_R\nontologies, we show that answering BUCQs in the above CQE semantics is in AC^0\nin data complexity by presenting a suitable, detailed first-order rewriting\nalgorithm. Finally, we report on experiments conducted in two different\nevaluation scenarios, showing the practical feasibility of our rewriting\nfunction.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u672c\u4f53\u4e0a\u7684\u53d7\u63a7\u67e5\u8be2\u8bc4\u4f30(CQE)\uff0c\u901a\u8fc7\u7ed3\u5408\u8ba4\u77e5\u4f9d\u8d56(EDs)\u548c\u6700\u4f18GA\u68c0\u67e5\u5668\u7684\u4ea4\u96c6\u6765\u56de\u7b54\u5e03\u5c14\u8fde\u63a5\u67e5\u8be2\u8054\u5408(BUCQs)\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728DL-Lite_R\u672c\u4f53\u4e0b\u5177\u6709AC^0\u6570\u636e\u590d\u6742\u5ea6\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5b9e\u7528\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u53d7\u63a7\u67e5\u8be2\u8bc4\u4f30\u6846\u67b6\u9700\u8981\u5728\u4fdd\u8bc1\u4fe1\u606f\u5b89\u5168\u7684\u540c\u65f6\u63d0\u4f9b\u6709\u6548\u7684\u67e5\u8be2\u56de\u7b54\u673a\u5236\u3002\u8ba4\u77e5\u4f9d\u8d56\u4f5c\u4e3a\u65b0\u63d0\u51fa\u7684\u903b\u8f91\u89c4\u5219\u65cf\uff0c\u9700\u8981\u4e0e\u6700\u4f18GA\u68c0\u67e5\u5668\u7ed3\u5408\u6765\u5b9e\u73b0\u65e2\u5b89\u5168\u53c8\u9ad8\u6548\u7684\u67e5\u8be2\u5904\u7406\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u5e03\u5c14\u8fde\u63a5\u67e5\u8be2\u8054\u5408\u7684\u5904\u7406\u3002", "method": "\u5c06\u8ba4\u77e5\u4f9d\u8d56(EDs)\u4e0e\u6700\u4f18GA\u68c0\u67e5\u5668\u7684\u6982\u5ff5\u7ed3\u5408\uff0c\u91cd\u70b9\u7814\u7a76\u57fa\u4e8e\u6240\u6709\u6700\u4f18GA\u68c0\u67e5\u5668\u4ea4\u96c6\u7684\u65b9\u6cd5\u6765\u56de\u7b54\u5e03\u5c14\u8fde\u63a5\u67e5\u8be2\u8054\u5408(BUCQs)\u3002\u9488\u5bf9EDs\u7684\u5b50\u7c7b\u548cDL-Lite_R\u672c\u4f53\uff0c\u8bbe\u8ba1\u4e86\u8be6\u7ec6\u7684\u4e00\u9636\u91cd\u5199\u7b97\u6cd5\u6765\u5b9e\u73b0\u67e5\u8be2\u56de\u7b54\u3002", "result": "1) \u523b\u753b\u4e86\u57fa\u4e8e\u4ea4\u96c6\u65b9\u6cd5\u7684\u5b89\u5168\u6027\uff0c\u8bc6\u522b\u51fa\u5b8c\u6574EDs\u7c7b\u522b\u4ecd\u7136\u4fdd\u6301\u5b89\u5168\uff1b2) \u8bc1\u660e\u4e86\u5728EDs\u5b50\u7c7b\u548cDL-Lite_R\u672c\u4f53\u4e0b\uff0cBUCQs\u67e5\u8be2\u56de\u7b54\u7684\u6570\u636e\u590d\u6742\u5ea6\u4e3aAC^0\uff1b3) \u901a\u8fc7\u4e24\u79cd\u4e0d\u540c\u8bc4\u4f30\u573a\u666f\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u91cd\u5199\u51fd\u6570\u7684\u5b9e\u9645\u53ef\u884c\u6027\u3002", "conclusion": "\u57fa\u4e8e\u6700\u4f18GA\u68c0\u67e5\u5668\u4ea4\u96c6\u7684\u53d7\u63a7\u67e5\u8be2\u8bc4\u4f30\u65b9\u6cd5\u80fd\u591f\u4e3a\u8ba4\u77e5\u4f9d\u8d56\u6846\u67b6\u63d0\u4f9b\u5f3a\u5b89\u5168\u4fdd\u8bc1\u548c\u826f\u597d\u7684\u8ba1\u7b97\u6027\u80fd\u3002\u5bf9\u4e8e\u7279\u5b9a\u7684EDs\u5b50\u7c7b\u548cDL-Lite_R\u672c\u4f53\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u4f18\u5f02\u7684\u7406\u8bba\u590d\u6742\u5ea6\u548c\u5b9e\u9645\u53ef\u884c\u6027\u3002"}}
{"id": "2507.17055", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.17055", "abs": "https://arxiv.org/abs/2507.17055", "authors": ["Jannis B\u00e4hler", "Diego Paez-Granados", "Jorge Pe\u00f1a-Queralta"], "title": "Shared Control of Holonomic Wheelchairs through Reinforcement Learning", "comment": null, "summary": "Smart electric wheelchairs can improve user experience by supporting the\ndriver with shared control. State-of-the-art work showed the potential of\nshared control in improving safety in navigation for non-holonomic robots.\nHowever, for holonomic systems, current approaches often lead to unintuitive\nbehavior for the user and fail to utilize the full potential of omnidirectional\ndriving. Therefore, we propose a reinforcement learning-based method, which\ntakes a 2D user input and outputs a 3D motion while ensuring user comfort and\nreducing cognitive load on the driver. Our approach is trained in Isaac Gym and\ntested in simulation in Gazebo. We compare different RL agent architectures and\nreward functions based on metrics considering cognitive load and user comfort.\nWe show that our method ensures collision-free navigation while smartly\norienting the wheelchair and showing better or competitive smoothness compared\nto a previous non-learning-based method. We further perform a sim-to-real\ntransfer and demonstrate, to the best of our knowledge, the first real-world\nimplementation of RL-based shared control for an omnidirectional mobility\nplatform.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u667a\u80fd\u7535\u52a8\u8f6e\u6905\u5171\u4eab\u63a7\u5236\u65b9\u6cd5\uff0c\u80fd\u591f\u5c06\u7528\u6237\u76842D\u8f93\u5165\u8f6c\u6362\u4e3a3D\u8fd0\u52a8\uff0c\u5728\u786e\u4fdd\u5b89\u5168\u5bfc\u822a\u7684\u540c\u65f6\u63d0\u5347\u7528\u6237\u8212\u9002\u5ea6\u5e76\u964d\u4f4e\u8ba4\u77e5\u8d1f\u62c5", "motivation": "\u73b0\u6709\u7684\u5168\u5411\u8f6e\u6905\u5171\u4eab\u63a7\u5236\u65b9\u6cd5\u5f80\u5f80\u5bfc\u81f4\u7528\u6237\u4f53\u9a8c\u4e0d\u76f4\u89c2\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u5168\u5411\u9a71\u52a8\u7684\u6f5c\u529b\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u66f4\u667a\u80fd\u7684\u63a7\u5236\u65b9\u6cd5\u6765\u6539\u5584\u7528\u6237\u4f53\u9a8c", "method": "\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5728Isaac Gym\u4e2d\u8bad\u7ec3\u667a\u80fd\u4f53\uff0c\u5c06\u7528\u6237\u76842D\u8f93\u5165\u6620\u5c04\u4e3a3D\u8fd0\u52a8\u8f93\u51fa\uff0c\u6bd4\u8f83\u4e0d\u540c\u7684\u5f3a\u5316\u5b66\u4e60\u67b6\u6784\u548c\u5956\u52b1\u51fd\u6570\uff0c\u5e76\u5728Gazebo\u4eff\u771f\u73af\u5883\u4e2d\u6d4b\u8bd5", "result": "\u5b9e\u73b0\u4e86\u65e0\u78b0\u649e\u5bfc\u822a\uff0c\u667a\u80fd\u8c03\u6574\u8f6e\u6905\u671d\u5411\uff0c\u5728\u5e73\u6ed1\u6027\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u6216\u7b49\u540c\u4e8e\u4f20\u7edf\u975e\u5b66\u4e60\u65b9\u6cd5\uff0c\u6210\u529f\u8fdb\u884c\u4e86\u4eff\u771f\u5230\u771f\u5b9e\u73af\u5883\u7684\u8fc1\u79fb\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86\u5168\u5411\u79fb\u52a8\u5e73\u53f0\u7684\u5f3a\u5316\u5b66\u4e60\u5171\u4eab\u63a7\u5236", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u63d0\u5347\u4e86\u5168\u5411\u7535\u52a8\u8f6e\u6905\u7684\u7528\u6237\u4f53\u9a8c\uff0c\u964d\u4f4e\u4e86\u8ba4\u77e5\u8d1f\u62c5\uff0c\u5e76\u5b9e\u73b0\u4e86\u9996\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u5168\u5411\u79fb\u52a8\u5e73\u53f0\u5171\u4eab\u63a7\u5236\u7684\u771f\u5b9e\u4e16\u754c\u5e94\u7528"}}
{"id": "2507.17493", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.17493", "abs": "https://arxiv.org/abs/2507.17493", "authors": ["Alexander Beiser", "Markus Hecher", "Stefan Woltran"], "title": "Automated Hybrid Grounding Using Structural and Data-Driven Heuristics", "comment": null, "summary": "The grounding bottleneck poses one of the key challenges that hinders the\nwidespread adoption of Answer Set Programming in industry. Hybrid Grounding is\na step in alleviating the bottleneck by combining the strength of standard\nbottom-up grounding with recently proposed techniques where rule bodies are\ndecoupled during grounding. However, it has remained unclear when hybrid\ngrounding shall use body-decoupled grounding and when to use standard bottom-up\ngrounding. In this paper, we address this issue by developing automated hybrid\ngrounding: we introduce a splitting algorithm based on data-structural\nheuristics that detects when to use body-decoupled grounding and when standard\ngrounding is beneficial. We base our heuristics on the structure of rules and\nan estimation procedure that incorporates the data of the instance. The\nexperiments conducted on our prototypical implementation demonstrate promising\nresults, which show an improvement on hard-to-ground scenarios, whereas on\nhard-to-solve instances we approach state-of-the-art performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u81ea\u52a8\u5316\u6df7\u5408\u63a5\u5730\u7b97\u6cd5\uff0c\u901a\u8fc7\u6570\u636e\u7ed3\u6784\u542f\u53d1\u5f0f\u65b9\u6cd5\u89e3\u51b3\u7b54\u6848\u96c6\u7f16\u7a0b\u4e2d\u63a5\u5730\u74f6\u9888\u95ee\u9898\uff0c\u81ea\u52a8\u51b3\u5b9a\u4f55\u65f6\u4f7f\u7528\u89e3\u8026\u63a5\u5730\u548c\u6807\u51c6\u63a5\u5730", "motivation": "\u7b54\u6848\u96c6\u7f16\u7a0b\u5728\u5de5\u4e1a\u5e94\u7528\u4e2d\u9762\u4e34\u63a5\u5730\u74f6\u9888\u95ee\u9898\uff0c\u73b0\u6709\u7684\u6df7\u5408\u63a5\u5730\u65b9\u6cd5\u7f3a\u4e4f\u81ea\u52a8\u5316\u51b3\u7b56\u673a\u5236\u6765\u786e\u5b9a\u4f55\u65f6\u4f7f\u7528\u89c4\u5219\u4f53\u89e3\u8026\u63a5\u5730\u548c\u4f55\u65f6\u4f7f\u7528\u6807\u51c6\u81ea\u5e95\u5411\u4e0a\u63a5\u5730", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8e\u6570\u636e\u7ed3\u6784\u542f\u53d1\u5f0f\u7684\u5206\u5272\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u80fd\u68c0\u6d4b\u4f55\u65f6\u4f7f\u7528\u89c4\u5219\u4f53\u89e3\u8026\u63a5\u5730\u548c\u4f55\u65f6\u4f7f\u7528\u6807\u51c6\u63a5\u5730\u3002\u542f\u53d1\u5f0f\u65b9\u6cd5\u57fa\u4e8e\u89c4\u5219\u7ed3\u6784\u548c\u5305\u542b\u5b9e\u4f8b\u6570\u636e\u7684\u4f30\u8ba1\u7a0b\u5e8f", "result": "\u5728\u539f\u578b\u5b9e\u73b0\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\u4e86\u6709\u524d\u666f\u7684\u7ed3\u679c\uff0c\u5728\u96be\u4ee5\u63a5\u5730\u7684\u573a\u666f\u4e0a\u663e\u793a\u4e86\u6539\u8fdb\uff0c\u800c\u5728\u96be\u4ee5\u6c42\u89e3\u7684\u5b9e\u4f8b\u4e0a\u63a5\u8fd1\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd", "conclusion": "\u81ea\u52a8\u5316\u6df7\u5408\u63a5\u5730\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u7f13\u89e3\u7b54\u6848\u96c6\u7f16\u7a0b\u4e2d\u7684\u63a5\u5730\u74f6\u9888\u95ee\u9898\uff0c\u901a\u8fc7\u667a\u80fd\u9009\u62e9\u63a5\u5730\u7b56\u7565\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u83b7\u5f97\u6027\u80fd\u63d0\u5347"}}
{"id": "2507.17085", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.17085", "abs": "https://arxiv.org/abs/2507.17085", "authors": ["Jayadeep Jacob", "Wenzheng Zhang", "Houston Warren", "Paulo Borges", "Tirthankar Bandyopadhyay", "Fabio Ramos"], "title": "Deformable Cluster Manipulation via Whole-Arm Policy Learning", "comment": null, "summary": "Manipulating clusters of deformable objects presents a substantial challenge\nwith widespread applicability, but requires contact-rich whole-arm\ninteractions. A potential solution must address the limited capacity for\nrealistic model synthesis, high uncertainty in perception, and the lack of\nefficient spatial abstractions, among others. We propose a novel framework for\nlearning model-free policies integrating two modalities: 3D point clouds and\nproprioceptive touch indicators, emphasising manipulation with full body\ncontact awareness, going beyond traditional end-effector modes. Our\nreinforcement learning framework leverages a distributional state\nrepresentation, aided by kernel mean embeddings, to achieve improved training\nefficiency and real-time inference. Furthermore, we propose a novel\ncontext-agnostic occlusion heuristic to clear deformables from a target region\nfor exposure tasks. We deploy the framework in a power line clearance scenario\nand observe that the agent generates creative strategies leveraging multiple\narm links for de-occlusion. Finally, we perform zero-shot sim-to-real policy\ntransfer, allowing the arm to clear real branches with unknown occlusion\npatterns, unseen topology, and uncertain dynamics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u54083D\u70b9\u4e91\u548c\u672c\u4f53\u611f\u89c9\u89e6\u89c9\u6307\u793a\u5668\u7684\u65e0\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u64cd\u4f5c\u53ef\u53d8\u5f62\u7269\u4f53\u96c6\u7fa4\uff0c\u7279\u522b\u662f\u5728\u7535\u529b\u7ebf\u6e05\u7406\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u7b56\u7565\u8f6c\u79fb\u3002", "motivation": "\u64cd\u4f5c\u53ef\u53d8\u5f62\u7269\u4f53\u96c6\u7fa4\u662f\u4e00\u4e2a\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u7684\u91cd\u5927\u6311\u6218\uff0c\u9700\u8981\u63a5\u89e6\u4e30\u5bcc\u7684\u5168\u81c2\u4ea4\u4e92\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u73b0\u5b9e\u6a21\u578b\u5408\u6210\u80fd\u529b\u6709\u9650\u3001\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\u9ad8\u3001\u7f3a\u4e4f\u9ad8\u6548\u7684\u7a7a\u95f4\u62bd\u8c61\u7b49\u65b9\u9762\u5b58\u5728\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u6765\u5904\u7406\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5b66\u4e60\u65e0\u6a21\u578b\u7b56\u7565\u7684\u65b0\u6846\u67b6\uff0c\u6574\u5408\u4e24\u79cd\u6a21\u6001\uff1a3D\u70b9\u4e91\u548c\u672c\u4f53\u611f\u89c9\u89e6\u89c9\u6307\u793a\u5668\uff0c\u5f3a\u8c03\u5177\u6709\u5168\u8eab\u63a5\u89e6\u611f\u77e5\u7684\u64cd\u4f5c\uff0c\u8d85\u8d8a\u4f20\u7edf\u7684\u672b\u7aef\u6267\u884c\u5668\u6a21\u5f0f\u3002\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u5229\u7528\u5206\u5e03\u5f0f\u72b6\u6001\u8868\u793a\uff0c\u901a\u8fc7\u6838\u5747\u503c\u5d4c\u5165\u8f85\u52a9\uff0c\u5b9e\u73b0\u66f4\u597d\u7684\u8bad\u7ec3\u6548\u7387\u548c\u5b9e\u65f6\u63a8\u7406\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4e0a\u4e0b\u6587\u65e0\u5173\u906e\u6321\u542f\u53d1\u5f0f\u65b9\u6cd5\u6765\u6e05\u7406\u76ee\u6807\u533a\u57df\u7684\u53ef\u53d8\u5f62\u7269\u4f53\u3002", "result": "\u5728\u7535\u529b\u7ebf\u6e05\u7406\u573a\u666f\u4e2d\u90e8\u7f72\u8be5\u6846\u67b6\uff0c\u89c2\u5bdf\u5230\u667a\u80fd\u4f53\u751f\u6210\u4e86\u5229\u7528\u591a\u4e2a\u624b\u81c2\u94fe\u8282\u8fdb\u884c\u53bb\u906e\u6321\u7684\u521b\u9020\u6027\u7b56\u7565\u3002\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u7b56\u7565\u8f6c\u79fb\uff0c\u4f7f\u673a\u68b0\u81c2\u80fd\u591f\u6e05\u7406\u5177\u6709\u672a\u77e5\u906e\u6321\u6a21\u5f0f\u3001\u672a\u89c1\u62d3\u6251\u7ed3\u6784\u548c\u4e0d\u786e\u5b9a\u52a8\u529b\u5b66\u7684\u771f\u5b9e\u6811\u679d\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u53ef\u53d8\u5f62\u7269\u4f53\u96c6\u7fa4\u64cd\u4f5c\u7684\u5173\u952e\u6311\u6218\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u611f\u77e5\u548c\u5206\u5e03\u5f0f\u72b6\u6001\u8868\u793a\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u5e76\u5728\u7535\u529b\u7ebf\u6e05\u7406\u4efb\u52a1\u4e2d\u5c55\u793a\u4e86\u51fa\u8272\u7684\u96f6\u6837\u672c\u8fc1\u79fb\u80fd\u529b\uff0c\u4e3a\u63a5\u89e6\u4e30\u5bcc\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.17512", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.17512", "abs": "https://arxiv.org/abs/2507.17512", "authors": ["Yu Li", "Zhuoshi Pan", "Honglin Lin", "Mengyuan Sun", "Conghui He", "Lijun Wu"], "title": "Can One Domain Help Others? A Data-Centric Study on Multi-Domain Reasoning via Reinforcement Learning", "comment": "27 pages, 24 figures", "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a\npowerful paradigm for enhancing the reasoning capabilities of LLMs. Existing\nresearch has predominantly concentrated on isolated reasoning domains such as\nmathematical problem-solving, coding tasks, or logical reasoning. However, real\nworld reasoning scenarios inherently demand an integrated application of\nmultiple cognitive skills. Despite this, the interplay among these reasoning\nskills under reinforcement learning remains poorly understood. To bridge this\ngap, we present a systematic investigation of multi-domain reasoning within the\nRLVR framework, explicitly focusing on three primary domains: mathematical\nreasoning, code generation, and logical puzzle solving. We conduct a\ncomprehensive study comprising four key components: (1) Leveraging the GRPO\nalgorithm and the Qwen-2.5-7B model family, our study thoroughly evaluates the\nmodels' in-domain improvements and cross-domain generalization capabilities\nwhen trained on single-domain datasets. (2) Additionally, we examine the\nintricate interactions including mutual enhancements and conflicts that emerge\nduring combined cross-domain training. (3) To further understand the influence\nof SFT on RL, we also analyze and compare performance differences between base\nand instruct models under identical RL configurations. (4) Furthermore, we\ndelve into critical RL training details, systematically exploring the impacts\nof curriculum learning strategies, variations in reward design, and\nlanguage-specific factors. Through extensive experiments, our results offer\nsignificant insights into the dynamics governing domain interactions, revealing\nkey factors influencing both specialized and generalizable reasoning\nperformance. These findings provide valuable guidance for optimizing RL\nmethodologies to foster comprehensive, multi-domain reasoning capabilities in\nLLMs.", "AI": {"tldr": "\u672c\u7814\u7a76\u7cfb\u7edf\u6027\u5730\u63a2\u8ba8\u4e86\u53ef\u9a8c\u8bc1\u5956\u52b1\u5f3a\u5316\u5b66\u4e60(RLVR)\u6846\u67b6\u4e0b\u7684\u591a\u9886\u57df\u63a8\u7406\u80fd\u529b\uff0c\u91cd\u70b9\u5173\u6ce8\u6570\u5b66\u63a8\u7406\u3001\u4ee3\u7801\u751f\u6210\u548c\u903b\u8f91\u8c1c\u9898\u6c42\u89e3\u4e09\u4e2a\u6838\u5fc3\u9886\u57df\uff0c\u901a\u8fc7\u5168\u9762\u7684\u5b9e\u9a8c\u5206\u6790\u63ed\u793a\u4e86\u9886\u57df\u95f4\u76f8\u4e92\u4f5c\u7528\u7684\u5173\u952e\u56e0\u7d20\u548c\u4f18\u5316\u7b56\u7565\u3002", "motivation": "\u73b0\u6709RLVR\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u5355\u4e00\u63a8\u7406\u9886\u57df\uff08\u5982\u6570\u5b66\u95ee\u9898\u3001\u7f16\u7a0b\u6216\u903b\u8f91\u63a8\u7406\uff09\uff0c\u4f46\u73b0\u5b9e\u573a\u666f\u9700\u8981\u591a\u79cd\u8ba4\u77e5\u6280\u80fd\u7684\u7efc\u5408\u5e94\u7528\u3002\u7136\u800c\uff0c\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u4e0b\u591a\u79cd\u63a8\u7406\u6280\u80fd\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u673a\u5236\u4ecd\u7136\u7f3a\u4e4f\u6df1\u5165\u7406\u89e3\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u7814\u7a76\u6765\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u91c7\u7528GRPO\u7b97\u6cd5\u548cQwen-2.5-7B\u6a21\u578b\u7cfb\u5217\u8fdb\u884c\u56db\u4e2a\u5173\u952e\u5b9e\u9a8c\uff1a(1)\u8bc4\u4f30\u5355\u9886\u57df\u8bad\u7ec3\u7684\u57df\u5185\u6539\u8fdb\u548c\u8de8\u57df\u6cdb\u5316\u80fd\u529b\uff1b(2)\u5206\u6790\u8de8\u57df\u8054\u5408\u8bad\u7ec3\u4e2d\u7684\u76f8\u4e92\u589e\u5f3a\u548c\u51b2\u7a81\uff1b(3)\u6bd4\u8f83\u57fa\u7840\u6a21\u578b\u548c\u6307\u4ee4\u6a21\u578b\u5728\u76f8\u540c\u5f3a\u5316\u5b66\u4e60\u914d\u7f6e\u4e0b\u7684\u6027\u80fd\u5dee\u5f02\uff1b(4)\u6df1\u5165\u7814\u7a76\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\u3001\u5956\u52b1\u8bbe\u8ba1\u53d8\u5316\u548c\u8bed\u8a00\u7279\u5b9a\u56e0\u7d20\u7b49\u5173\u952e\u8bad\u7ec3\u7ec6\u8282\u7684\u5f71\u54cd\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u7814\u7a76\u63ed\u793a\u4e86\u652f\u914d\u9886\u57df\u76f8\u4e92\u4f5c\u7528\u7684\u52a8\u529b\u5b66\u673a\u5236\uff0c\u8bc6\u522b\u51fa\u5f71\u54cd\u4e13\u4e1a\u5316\u548c\u6cdb\u5316\u63a8\u7406\u6027\u80fd\u7684\u5173\u952e\u56e0\u7d20\u3002\u5b9e\u9a8c\u7ed3\u679c\u63d0\u4f9b\u4e86\u5173\u4e8e\u591a\u9886\u57df\u63a8\u7406\u80fd\u529b\u53d1\u5c55\u7684\u91cd\u8981\u89c1\u89e3\uff0c\u5305\u62ec\u9886\u57df\u95f4\u7684\u76f8\u4e92\u4fc3\u8fdb\u548c\u51b2\u7a81\u6a21\u5f0f\u3002", "conclusion": "\u7814\u7a76\u4e3a\u4f18\u5316\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u63d0\u4f9b\u4e86\u5b9d\u8d35\u6307\u5bfc\uff0c\u4ee5\u57f9\u517b\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7efc\u5408\u6027\u591a\u9886\u57df\u63a8\u7406\u80fd\u529b\u3002\u8fd9\u4e9b\u53d1\u73b0\u6709\u52a9\u4e8e\u7406\u89e3\u5982\u4f55\u5728RLVR\u6846\u67b6\u4e0b\u6709\u6548\u6574\u5408\u4e0d\u540c\u8ba4\u77e5\u6280\u80fd\uff0c\u4e3a\u6784\u5efa\u66f4\u5f3a\u5927\u7684\u591a\u9886\u57df\u63a8\u7406\u7cfb\u7edf\u5960\u5b9a\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2507.17130", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.17130", "abs": "https://arxiv.org/abs/2507.17130", "authors": ["Seokhwan Jeong", "Hogyun Kim", "Younggun Cho"], "title": "MARSCalib: Multi-robot, Automatic, Robust, Spherical Target-based Extrinsic Calibration in Field and Extraterrestrial Environments", "comment": "8 pages, 9 figures", "summary": "This paper presents a novel spherical target-based LiDAR-camera extrinsic\ncalibration method designed for outdoor environments with multi-robot systems,\nconsidering both target and sensor corruption. The method extracts the 2D\nellipse center from the image and the 3D sphere center from the pointcloud,\nwhich are then paired to compute the transformation matrix. Specifically, the\nimage is first decomposed using the Segment Anything Model (SAM). Then, a novel\nalgorithm extracts an ellipse from a potentially corrupted sphere, and the\nextracted center of ellipse is corrected for errors caused by the perspective\nprojection model. For the LiDAR pointcloud, points on the sphere tend to be\nhighly noisy due to the absence of flat regions. To accurately extract the\nsphere from these noisy measurements, we apply a hierarchical weighted sum to\nthe accumulated pointcloud. Through experiments, we demonstrated that the\nsphere can be robustly detected even under both types of corruption,\noutperforming other targets. We evaluated our method using three different\ntypes of LiDARs (spinning, solid-state, and non-repetitive) with cameras\npositioned in three different locations. Furthermore, we validated the\nrobustness of our method to target corruption by experimenting with spheres\nsubjected to various types of degradation. These experiments were conducted in\nboth a planetary test and a field environment. Our code is available at\nhttps://github.com/sparolab/MARSCalib.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7403\u5f62\u76ee\u6807\u7684LiDAR-\u76f8\u673a\u5916\u53c2\u6807\u5b9a\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u6237\u5916\u73af\u5883\uff0c\u80fd\u591f\u5904\u7406\u76ee\u6807\u548c\u4f20\u611f\u5668\u635f\u574f\u60c5\u51b5", "motivation": "\u73b0\u6709\u7684LiDAR-\u76f8\u673a\u6807\u5b9a\u65b9\u6cd5\u5728\u6237\u5916\u73af\u5883\u548c\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u9762\u4e34\u6311\u6218\uff0c\u7279\u522b\u662f\u5f53\u76ee\u6807\u7269\u4f53\u548c\u4f20\u611f\u5668\u51fa\u73b0\u635f\u574f\u65f6\uff0c\u9700\u8981\u4e00\u79cd\u9c81\u68d2\u7684\u6807\u5b9a\u65b9\u6cd5\u6765\u5e94\u5bf9\u8fd9\u4e9b\u5b9e\u9645\u5e94\u7528\u573a\u666f", "method": "\u4f7f\u7528\u7403\u5f62\u76ee\u6807\u8fdb\u884c\u6807\u5b9a\uff0c\u4ece\u56fe\u50cf\u4e2d\u63d0\u53d62D\u692d\u5706\u4e2d\u5fc3\uff0c\u4ece\u70b9\u4e91\u4e2d\u63d0\u53d63D\u7403\u5fc3\uff0c\u7136\u540e\u914d\u5bf9\u8ba1\u7b97\u53d8\u6362\u77e9\u9635\u3002\u56fe\u50cf\u5904\u7406\u91c7\u7528SAM\u6a21\u578b\u5206\u89e3\uff0c\u8bbe\u8ba1\u65b0\u7b97\u6cd5\u4ece\u635f\u574f\u7403\u4f53\u4e2d\u63d0\u53d6\u692d\u5706\u5e76\u6821\u6b63\u900f\u89c6\u6295\u5f71\u8bef\u5dee\uff1b\u70b9\u4e91\u5904\u7406\u91c7\u7528\u5206\u5c42\u52a0\u6743\u6c42\u548c\u5904\u7406\u566a\u58f0\u70b9\u4e91\u4ee5\u51c6\u786e\u63d0\u53d6\u7403\u4f53", "result": "\u5728\u591a\u79cdLiDAR\u7c7b\u578b\uff08\u65cb\u8f6c\u5f0f\u3001\u56fa\u6001\u5f0f\u3001\u975e\u91cd\u590d\u5f0f\uff09\u548c\u4e0d\u540c\u76f8\u673a\u4f4d\u7f6e\u4e0b\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u7403\u5f62\u76ee\u6807\u5728\u4e24\u79cd\u635f\u574f\u60c5\u51b5\u4e0b\u90fd\u80fd\u88ab\u9c81\u68d2\u68c0\u6d4b\uff0c\u6027\u80fd\u4f18\u4e8e\u5176\u4ed6\u76ee\u6807\u3002\u5728\u884c\u661f\u6d4b\u8bd5\u548c\u91ce\u5916\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u5bf9\u76ee\u6807\u635f\u574f\u7684\u9c81\u68d2\u6027", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8e\u7403\u5f62\u76ee\u6807\u7684LiDAR-\u76f8\u673a\u5916\u53c2\u6807\u5b9a\u65b9\u6cd5\u80fd\u591f\u5728\u76ee\u6807\u548c\u4f20\u611f\u5668\u635f\u574f\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u9c81\u68d2\u6807\u5b9a\uff0c\u9002\u7528\u4e8e\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u6237\u5916\u73af\u5883\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2507.17514", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17514", "abs": "https://arxiv.org/abs/2507.17514", "authors": ["Athanasios Davvetas", "Xenia Ziouvelou", "Ypatia Dami", "Alexis Kaponis", "Konstantina Giouvanopoulou", "Michael Papademas"], "title": "TAI Scan Tool: A RAG-Based Tool With Minimalistic Input for Trustworthy AI Self-Assessment", "comment": "9 pages, 1 figure, 4 tables", "summary": "This paper introduces the TAI Scan Tool, a RAG-based TAI self-assessment tool\nwith minimalistic input. The current version of the tool supports the legal TAI\nassessment, with a particular emphasis on facilitating compliance with the AI\nAct. It involves a two-step approach with a pre-screening and an assessment\nphase. The assessment output of the system includes insight regarding the\nrisk-level of the AI system according to the AI Act, while at the same time\nretrieving relevant articles to aid with compliance and notify on their\nobligations. Our qualitative evaluation using use-case scenarios yields\npromising results, correctly predicting risk levels while retrieving relevant\narticles across three distinct semantic groups. Furthermore, interpretation of\nresults shows that the tool's reasoning relies on comparison with the setting\nof high-risk systems, a behaviour attributed to their deployment requiring\ncareful consideration, and therefore frequently presented within the AI Act.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86TAI\u626b\u63cf\u5de5\u5177\uff0c\u8fd9\u662f\u4e00\u4e2a\u57fa\u4e8eRAG\u7684AI\u7cfb\u7edf\u81ea\u8bc4\u4f30\u5de5\u5177\uff0c\u4e13\u95e8\u7528\u4e8e\u534f\u52a9\u9075\u5b88AI\u6cd5\u6848\uff0c\u901a\u8fc7\u9884\u7b5b\u9009\u548c\u8bc4\u4f30\u4e24\u4e2a\u9636\u6bb5\u6765\u786e\u5b9aAI\u7cfb\u7edf\u7684\u98ce\u9669\u7b49\u7ea7\u5e76\u63d0\u4f9b\u76f8\u5173\u5408\u89c4\u6307\u5bfc\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u6709\u6548\u7684AI\u7cfb\u7edf\u5408\u89c4\u8bc4\u4f30\u5de5\u5177\u6765\u5e2e\u52a9\u7ec4\u7ec7\u9075\u5b88AI\u6cd5\u6848\u7684\u8981\u6c42\uff0c\u7279\u522b\u662f\u5728\u786e\u5b9aAI\u7cfb\u7edf\u98ce\u9669\u7b49\u7ea7\u548c\u7406\u89e3\u76f8\u5173\u6cd5\u5f8b\u4e49\u52a1\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u4ee5\u6700\u5c11\u8f93\u5165\u63d0\u4f9b\u51c6\u786e\u8bc4\u4f30\u7684\u81ea\u52a8\u5316\u5de5\u5177\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u7684\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a(1)\u9884\u7b5b\u9009\u9636\u6bb5\u8fdb\u884c\u521d\u6b65\u8bc4\u4f30\uff1b(2)\u8be6\u7ec6\u8bc4\u4f30\u9636\u6bb5\u786e\u5b9aAI\u7cfb\u7edf\u7684\u98ce\u9669\u7b49\u7ea7\u3002\u7cfb\u7edf\u901a\u8fc7\u6bd4\u8f83\u9ad8\u98ce\u9669\u7cfb\u7edf\u7684\u8bbe\u7f6e\u6765\u8fdb\u884c\u63a8\u7406\uff0c\u5e76\u68c0\u7d22\u76f8\u5173\u7684AI\u6cd5\u6848\u6761\u6b3e\u6765\u63d0\u4f9b\u5408\u89c4\u6307\u5bfc\u3002", "result": "\u5b9a\u6027\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\u8be5\u5de5\u5177\u5728\u4e09\u4e2a\u4e0d\u540c\u8bed\u4e49\u7ec4\u7684\u7528\u4f8b\u573a\u666f\u4e2d\u90fd\u80fd\u6b63\u786e\u9884\u6d4b\u98ce\u9669\u7b49\u7ea7\uff0c\u540c\u65f6\u6210\u529f\u68c0\u7d22\u5230\u76f8\u5173\u6761\u6b3e\u3002\u7ed3\u679c\u89e3\u91ca\u8868\u660e\u5de5\u5177\u7684\u63a8\u7406\u4e3b\u8981\u4f9d\u8d56\u4e8e\u4e0e\u9ad8\u98ce\u9669\u7cfb\u7edf\u8bbe\u7f6e\u7684\u6bd4\u8f83\uff0c\u8fd9\u79cd\u884c\u4e3a\u5f52\u56e0\u4e8e\u6b64\u7c7b\u7cfb\u7edf\u7684\u90e8\u7f72\u9700\u8981\u4ed4\u7ec6\u8003\u8651\u3002", "conclusion": "TAI\u626b\u63cf\u5de5\u5177\u6210\u529f\u5b9e\u73b0\u4e86AI\u7cfb\u7edf\u7684\u81ea\u52a8\u5316\u98ce\u9669\u8bc4\u4f30\u548c\u5408\u89c4\u6307\u5bfc\uff0c\u80fd\u591f\u51c6\u786e\u8bc6\u522b\u98ce\u9669\u7b49\u7ea7\u5e76\u63d0\u4f9b\u76f8\u5173\u7684AI\u6cd5\u6848\u6761\u6b3e\uff0c\u4e3a\u7ec4\u7ec7\u9075\u5b88AI\u6cd5\u6848\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6280\u672f\u652f\u6301\uff0c\u9a8c\u8bc1\u4e86\u57fa\u4e8eRAG\u65b9\u6cd5\u5728\u6cd5\u5f8b\u5408\u89c4\u8bc4\u4f30\u9886\u57df\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2507.17132", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.17132", "abs": "https://arxiv.org/abs/2507.17132", "authors": ["Xiao Liu", "Xianlong Yang", "Weijun Wang", "Wei Feng"], "title": "Dynamic Modeling and Dimensional Optimization of Legged Mechanisms for Construction Robot", "comment": null, "summary": "With the rapid development of the construction industry, issues such as harsh\nworking environments, high-intensity and high-risk tasks, and labor shortages\nhave become increasingly prominent. This drives higher demands for construction\nrobots in terms of low energy consumption, high mobility, and high load\ncapacity. This paper focuses on the design and optimization of leg structures\nfor construction robots, aiming to improve their dynamic performance, reduce\nenergy consumption, and enhance load-bearing capabilities. Firstly, based on\nthe leg configuration of ants in nature, we design a structure for the robot's\nleg. Secondly, we propose a novel structural optimization method. Using the\nLagrangian approach, a dynamic model of the leg was established. Combining the\ndynamic model with the leg's motion trajectory, we formulated multiple dynamic\nevaluation metrics and conducted a comprehensive optimization study on the\ngeometric parameters of each leg segment. The results show that the optimized\nleg structure reduces peak joint torques and energy consumption by over 20%.\nFinally, dynamic simulation experiments were conducted using ADAMS. The results\ndemonstrate a significant reduction in the driving power of each joint after\noptimization, validating the effectiveness and rationality of the proposed\nstrategy. This study provides a theoretical foundation and technical support\nfor the design of heavy-load, high-performance construction robots.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u5efa\u7b51\u673a\u5668\u4eba\u817f\u90e8\u7ed3\u6784\u8fdb\u884c\u8bbe\u8ba1\u548c\u4f18\u5316\uff0c\u57fa\u4e8e\u8682\u8681\u817f\u90e8\u6784\u578b\u8bbe\u8ba1\u673a\u5668\u4eba\u817f\u90e8\u7ed3\u6784\uff0c\u63d0\u51fa\u65b0\u578b\u7ed3\u6784\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u62c9\u683c\u6717\u65e5\u65b9\u6cd5\u5efa\u7acb\u52a8\u529b\u5b66\u6a21\u578b\u5e76\u4f18\u5316\u51e0\u4f55\u53c2\u6570\uff0c\u5b9e\u73b0\u5cf0\u503c\u5173\u8282\u626d\u77e9\u548c\u80fd\u8017\u964d\u4f4e\u8d85\u8fc720%\uff0c\u4e3a\u91cd\u8f7d\u9ad8\u6027\u80fd\u5efa\u7b51\u673a\u5668\u4eba\u8bbe\u8ba1\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u3002", "motivation": "\u5efa\u7b51\u884c\u4e1a\u5feb\u901f\u53d1\u5c55\u5e26\u6765\u7684\u6076\u52a3\u5de5\u4f5c\u73af\u5883\u3001\u9ad8\u5f3a\u5ea6\u9ad8\u98ce\u9669\u4efb\u52a1\u548c\u52b3\u52a8\u529b\u77ed\u7f3a\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\uff0c\u8fd9\u63a8\u52a8\u4e86\u5bf9\u4f4e\u80fd\u8017\u3001\u9ad8\u673a\u52a8\u6027\u548c\u9ad8\u8f7d\u8377\u80fd\u529b\u5efa\u7b51\u673a\u5668\u4eba\u7684\u66f4\u9ad8\u9700\u6c42\u3002", "method": "\u57fa\u4e8e\u81ea\u7136\u754c\u8682\u8681\u7684\u817f\u90e8\u6784\u578b\u8bbe\u8ba1\u673a\u5668\u4eba\u817f\u90e8\u7ed3\u6784\uff1b\u63d0\u51fa\u65b0\u578b\u7ed3\u6784\u4f18\u5316\u65b9\u6cd5\uff1b\u4f7f\u7528\u62c9\u683c\u6717\u65e5\u65b9\u6cd5\u5efa\u7acb\u817f\u90e8\u52a8\u529b\u5b66\u6a21\u578b\uff1b\u7ed3\u5408\u52a8\u529b\u5b66\u6a21\u578b\u548c\u817f\u90e8\u8fd0\u52a8\u8f68\u8ff9\uff0c\u5236\u5b9a\u591a\u4e2a\u52a8\u6001\u8bc4\u4f30\u6307\u6807\uff1b\u5bf9\u5404\u817f\u6bb5\u51e0\u4f55\u53c2\u6570\u8fdb\u884c\u7efc\u5408\u4f18\u5316\u7814\u7a76\uff1b\u4f7f\u7528ADAMS\u8fdb\u884c\u52a8\u6001\u4eff\u771f\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u4f18\u5316\u540e\u7684\u817f\u90e8\u7ed3\u6784\u4f7f\u5cf0\u503c\u5173\u8282\u626d\u77e9\u548c\u80fd\u8017\u964d\u4f4e\u8d85\u8fc720%\uff1bADAMS\u52a8\u6001\u4eff\u771f\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u4f18\u5316\u540e\u5404\u5173\u8282\u9a71\u52a8\u529f\u7387\u663e\u8457\u964d\u4f4e\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u7b56\u7565\u7684\u6709\u6548\u6027\u548c\u5408\u7406\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u91cd\u8f7d\u9ad8\u6027\u80fd\u5efa\u7b51\u673a\u5668\u4eba\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u6280\u672f\u652f\u6301\uff0c\u6240\u63d0\u51fa\u7684\u4eff\u751f\u8bbe\u8ba1\u548c\u4f18\u5316\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u6539\u5584\u5efa\u7b51\u673a\u5668\u4eba\u7684\u52a8\u6001\u6027\u80fd\u3001\u964d\u4f4e\u80fd\u8017\u5e76\u589e\u5f3a\u627f\u8f7d\u80fd\u529b\u3002"}}
{"id": "2507.17539", "categories": ["cs.AI", "cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.17539", "abs": "https://arxiv.org/abs/2507.17539", "authors": ["Xinyao Liu", "Diping Song"], "title": "Constructing Ophthalmic MLLM for Positioning-diagnosis Collaboration Through Clinical Cognitive Chain Reasoning", "comment": null, "summary": "Multimodal large language models (MLLMs) demonstrate significant potential in\nthe field of medical diagnosis. However, they face critical challenges in\nspecialized domains such as ophthalmology, particularly the fragmentation of\nannotation granularity and inconsistencies in clinical reasoning logic, which\nhinder precise cross-modal understanding. This paper introduces FundusExpert,\nan ophthalmology-specific MLLM with integrated positioning-diagnosis reasoning\ncapabilities, along with FundusGen, a dataset constructed through the\nintelligent Fundus-Engine system. Fundus-Engine automates localization and\nleverages MLLM-based semantic expansion to integrate global disease\nclassification, local object detection, and fine-grained feature analysis\nwithin a single fundus image. Additionally, by constructing a clinically\naligned cognitive chain, it guides the model to generate interpretable\nreasoning paths. FundusExpert, fine-tuned with instruction data from FundusGen,\nachieves the best performance in ophthalmic question-answering tasks,\nsurpassing the average accuracy of the 40B MedRegA by 26.6%. It also excels in\nzero-shot report generation tasks, achieving a clinical consistency of 77.0%,\nsignificantly outperforming GPT-4o's 47.6%. Furthermore, we reveal a scaling\nlaw between data quality and model capability ($L \\propto N^{0.068}$),\ndemonstrating that the cognitive alignment annotations in FundusGen enhance\ndata utilization efficiency. By integrating region-level localization with\ndiagnostic reasoning chains, our work develops a scalable, clinically-aligned\nMLLM and explores a pathway toward bridging the visual-language gap in specific\nMLLMs. Our project can be found at https://github.com/MeteorElf/FundusExpert.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86FundusExpert\uff0c\u4e00\u4e2a\u4e13\u95e8\u7528\u4e8e\u773c\u79d1\u8bca\u65ad\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7FundusGen\u6570\u636e\u96c6\u548cFundus-Engine\u7cfb\u7edf\u5b9e\u73b0\u4e86\u5b9a\u4f4d-\u8bca\u65ad\u63a8\u7406\u80fd\u529b\u7684\u6574\u5408\uff0c\u5728\u773c\u79d1\u95ee\u7b54\u548c\u62a5\u544a\u751f\u6210\u4efb\u52a1\u4e2d\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u8bca\u65ad\u9886\u57df\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u5728\u773c\u79d1\u7b49\u4e13\u4e1a\u9886\u57df\u9762\u4e34\u6ce8\u91ca\u7c92\u5ea6\u788e\u7247\u5316\u548c\u4e34\u5e8a\u63a8\u7406\u903b\u8f91\u4e0d\u4e00\u81f4\u7684\u5173\u952e\u6311\u6218\uff0c\u8fd9\u4e9b\u95ee\u9898\u963b\u788d\u4e86\u7cbe\u786e\u7684\u8de8\u6a21\u6001\u7406\u89e3\u80fd\u529b\u3002", "method": "\u63d0\u51faFundusExpert\u773c\u79d1\u4e13\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u548cFundusGen\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u667a\u80fdFundus-Engine\u7cfb\u7edf\u81ea\u52a8\u5316\u5b9a\u4f4d\u5e76\u5229\u7528\u57fa\u4e8eMLLM\u7684\u8bed\u4e49\u6269\u5c55\uff0c\u5728\u5355\u5f20\u773c\u5e95\u56fe\u50cf\u4e2d\u6574\u5408\u5168\u5c40\u75be\u75c5\u5206\u7c7b\u3001\u5c40\u90e8\u76ee\u6807\u68c0\u6d4b\u548c\u7ec6\u7c92\u5ea6\u7279\u5f81\u5206\u6790\u3002\u901a\u8fc7\u6784\u5efa\u4e34\u5e8a\u5bf9\u9f50\u7684\u8ba4\u77e5\u94fe\u6765\u6307\u5bfc\u6a21\u578b\u751f\u6210\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u8def\u5f84\u3002", "result": "FundusExpert\u5728\u773c\u79d1\u95ee\u7b54\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u5e73\u5747\u51c6\u786e\u7387\u6bd440B MedRegA\u9ad826.6%\uff1b\u5728\u96f6\u6837\u672c\u62a5\u544a\u751f\u6210\u4efb\u52a1\u4e2d\u8fbe\u523077.0%\u7684\u4e34\u5e8a\u4e00\u81f4\u6027\uff0c\u663e\u8457\u8d85\u8d8aGPT-4o\u768447.6%\uff1b\u53d1\u73b0\u6570\u636e\u8d28\u91cf\u4e0e\u6a21\u578b\u80fd\u529b\u4e4b\u95f4\u7684\u7f29\u653e\u5b9a\u5f8b\uff08L \u221d N^0.068\uff09\u3002", "conclusion": "\u901a\u8fc7\u6574\u5408\u533a\u57df\u7ea7\u5b9a\u4f4d\u4e0e\u8bca\u65ad\u63a8\u7406\u94fe\uff0c\u8be5\u5de5\u4f5c\u5f00\u53d1\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u3001\u4e34\u5e8a\u5bf9\u9f50\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u63a2\u7d22\u4e86\u7f29\u5c0f\u7279\u5b9a\u9886\u57df\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u89c6\u89c9-\u8bed\u8a00\u5dee\u8ddd\u7684\u8def\u5f84\uff0c\u4e3a\u533b\u7597AI\u5728\u4e13\u4e1a\u9886\u57df\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.17136", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.17136", "abs": "https://arxiv.org/abs/2507.17136", "authors": ["Xiao Liu", "Yunxiao Cheng", "Weijun Wang", "Tianlun Huang", "Wei Feng"], "title": "Dynamic Parameter Identification of a Curtain Wall Installation Robotic Arm", "comment": null, "summary": "In the construction industry, traditional methods fail to meet the modern\ndemands for efficiency and quality. The curtain wall installation is a critical\ncomponent of construction projects. We design a hydraulically driven robotic\narm for curtain wall installation and a dynamic parameter identification\nmethod. We establish a Denavit-Hartenberg (D-H) model based on measured robotic\narm structural parameters and integrate hydraulic cylinder dynamics to\nconstruct a composite parametric system driven by a Stribeck friction model. By\ndesigning high-signal-to-noise ratio displacement excitation signals for\nhydraulic cylinders and combining Fourier series to construct optimal\nexcitation trajectories that satisfy joint constraints, this method effectively\nexcites the characteristics of each parameter in the minimal parameter set of\nthe dynamic model of the robotic arm. On this basis, a hierarchical progressive\nparameter identification strategy is proposed: least squares estimation is\nemployed to separately identify and jointly calibrate the dynamic parameters of\nboth the hydraulic cylinder and the robotic arm, yielding Stribeck model curves\nfor each joint. Experimental validation on a robotic arm platform demonstrates\nresidual standard deviations below 0.4 Nm between theoretical and measured\njoint torques, confirming high-precision dynamic parameter identification for\nthe hydraulic-driven curtain wall installation robotic arm. This significantly\ncontributes to enhancing the intelligence level of curtain wall installation\noperations.", "AI": {"tldr": "\u672c\u6587\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u6db2\u538b\u9a71\u52a8\u7684\u5e55\u5899\u5b89\u88c5\u673a\u5668\u4eba\u81c2\uff0c\u5e76\u63d0\u51fa\u4e86\u5206\u5c42\u9012\u8fdb\u7684\u52a8\u6001\u53c2\u6570\u8bc6\u522b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5efa\u7acbD-H\u6a21\u578b\u548cStribeck\u6469\u64e6\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u52a8\u6001\u53c2\u6570\u8bc6\u522b\uff0c\u7406\u8bba\u4e0e\u5b9e\u6d4b\u5173\u8282\u626d\u77e9\u7684\u6b8b\u5dee\u6807\u51c6\u5dee\u4f4e\u4e8e0.4 Nm\u3002", "motivation": "\u4f20\u7edf\u5efa\u7b51\u65bd\u5de5\u65b9\u6cd5\u65e0\u6cd5\u6ee1\u8db3\u73b0\u4ee3\u5bf9\u6548\u7387\u548c\u8d28\u91cf\u7684\u9700\u6c42\uff0c\u5e55\u5899\u5b89\u88c5\u4f5c\u4e3a\u5efa\u7b51\u9879\u76ee\u7684\u5173\u952e\u7ec4\u6210\u90e8\u5206\uff0c\u9700\u8981\u63d0\u9ad8\u667a\u80fd\u5316\u6c34\u5e73\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u52a8\u6001\u53c2\u6570\u8bc6\u522b\u65b9\u9762\u5b58\u5728\u7cbe\u5ea6\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u5efa\u7acb\u57fa\u4e8e\u5b9e\u6d4b\u673a\u5668\u4eba\u81c2\u7ed3\u6784\u53c2\u6570\u7684D-H\u6a21\u578b\uff0c\u96c6\u6210\u6db2\u538b\u7f38\u52a8\u529b\u5b66\u6784\u5efaStribeck\u6469\u64e6\u6a21\u578b\u9a71\u52a8\u7684\u590d\u5408\u53c2\u6570\u7cfb\u7edf\uff1b\u8bbe\u8ba1\u9ad8\u4fe1\u566a\u6bd4\u7684\u6db2\u538b\u7f38\u4f4d\u79fb\u6fc0\u52b1\u4fe1\u53f7\uff0c\u7ed3\u5408\u5085\u91cc\u53f6\u7ea7\u6570\u6784\u5efa\u6ee1\u8db3\u5173\u8282\u7ea6\u675f\u7684\u6700\u4f18\u6fc0\u52b1\u8f68\u8ff9\uff1b\u63d0\u51fa\u5206\u5c42\u9012\u8fdb\u53c2\u6570\u8bc6\u522b\u7b56\u7565\uff0c\u91c7\u7528\u6700\u5c0f\u4e8c\u4e58\u4f30\u8ba1\u5206\u522b\u8bc6\u522b\u548c\u8054\u5408\u6807\u5b9a\u6db2\u538b\u7f38\u548c\u673a\u5668\u4eba\u81c2\u7684\u52a8\u6001\u53c2\u6570\u3002", "result": "\u5728\u673a\u5668\u4eba\u81c2\u5e73\u53f0\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u663e\u793a\uff0c\u7406\u8bba\u4e0e\u5b9e\u6d4b\u5173\u8282\u626d\u77e9\u4e4b\u95f4\u7684\u6b8b\u5dee\u6807\u51c6\u5dee\u4f4e\u4e8e0.4 Nm\uff0c\u5b9e\u73b0\u4e86\u6db2\u538b\u9a71\u52a8\u5e55\u5899\u5b89\u88c5\u673a\u5668\u4eba\u81c2\u7684\u9ad8\u7cbe\u5ea6\u52a8\u6001\u53c2\u6570\u8bc6\u522b\uff0c\u83b7\u5f97\u4e86\u5404\u5173\u8282\u7684Stribeck\u6a21\u578b\u66f2\u7ebf\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u5b9e\u73b0\u4e86\u6db2\u538b\u9a71\u52a8\u5e55\u5899\u5b89\u88c5\u673a\u5668\u4eba\u81c2\u7684\u9ad8\u7cbe\u5ea6\u52a8\u6001\u53c2\u6570\u8bc6\u522b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5e55\u5899\u5b89\u88c5\u4f5c\u4e1a\u7684\u667a\u80fd\u5316\u6c34\u5e73\uff0c\u4e3a\u5efa\u7b51\u884c\u4e1a\u7684\u81ea\u52a8\u5316\u548c\u667a\u80fd\u5316\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u6280\u672f\u652f\u6491\u3002"}}
{"id": "2507.17680", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.17680", "abs": "https://arxiv.org/abs/2507.17680", "authors": ["Yongchao Zeng", "Calum Brown", "Ioannis Kyriakou", "Ronja Hotz", "Mark Rounsevell"], "title": "Simulating multiple human perspectives in socio-ecological systems using large language models", "comment": null, "summary": "Understanding socio-ecological systems requires insights from diverse\nstakeholder perspectives, which are often hard to access. To enable\nalternative, simulation-based exploration of different stakeholder\nperspectives, we develop the HoPeS (Human-Oriented Perspective Shifting)\nmodelling framework. HoPeS employs agents powered by large language models\n(LLMs) to represent various stakeholders; users can step into the agent roles\nto experience perspectival differences. A simulation protocol serves as a\n\"scaffold\" to streamline multiple perspective-taking simulations, supporting\nusers in reflecting on, transitioning between, and integrating across\nperspectives. A prototype system is developed to demonstrate HoPeS in the\ncontext of institutional dynamics and land use change, enabling both\nnarrative-driven and numerical experiments. In an illustrative experiment, a\nuser successively adopts the perspectives of a system observer and a researcher\n- a role that analyses data from the embedded land use model to inform\nevidence-based decision-making for other LLM agents representing various\ninstitutions. Despite the user's effort to recommend technically sound\npolicies, discrepancies persist between the policy recommendation and\nimplementation due to stakeholders' competing advocacies, mirroring real-world\nmisalignment between researcher and policymaker perspectives. The user's\nreflection highlights the subjective feelings of frustration and disappointment\nas a researcher, especially due to the challenge of maintaining political\nneutrality while attempting to gain political influence. Despite this, the user\nexhibits high motivation to experiment with alternative narrative framing\nstrategies, suggesting the system's potential in exploring different\nperspectives. Further system and protocol refinement are likely to enable new\nforms of interdisciplinary collaboration in socio-ecological simulations.", "AI": {"tldr": "\u7814\u7a76\u5f00\u53d1\u4e86HoPeS\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u4ee3\u7406\u6765\u6a21\u62df\u4e0d\u540c\u5229\u76ca\u76f8\u5173\u8005\u7684\u89c6\u89d2\uff0c\u5e2e\u52a9\u7528\u6237\u901a\u8fc7\u89d2\u8272\u626e\u6f14\u4f53\u9a8c\u548c\u7406\u89e3\u793e\u4f1a\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u591a\u5143\u89c2\u70b9\u5dee\u5f02", "motivation": "\u793e\u4f1a\u751f\u6001\u7cfb\u7edf\u7684\u7406\u89e3\u9700\u8981\u6765\u81ea\u4e0d\u540c\u5229\u76ca\u76f8\u5173\u8005\u7684\u591a\u5143\u89c6\u89d2\u6d1e\u5bdf\uff0c\u4f46\u8fd9\u4e9b\u89c6\u89d2\u5f80\u5f80\u96be\u4ee5\u83b7\u53d6\u3002\u4f20\u7edf\u65b9\u6cd5\u5728\u63a2\u7d22\u548c\u6574\u5408\u4e0d\u540cstakeholder\u89c2\u70b9\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u57fa\u4e8e\u4eff\u771f\u7684\u66ff\u4ee3\u65b9\u6cd5\u6765\u63a2\u7d22\u4e0d\u540c\u7684\u5229\u76ca\u76f8\u5173\u8005\u89c6\u89d2", "method": "\u5f00\u53d1HoPeS\uff08\u9762\u5411\u4eba\u7c7b\u7684\u89c6\u89d2\u8f6c\u6362\uff09\u5efa\u6a21\u6846\u67b6\uff0c\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u4ee3\u7406\u6765\u4ee3\u8868\u5404\u79cd\u5229\u76ca\u76f8\u5173\u8005\uff1b\u7528\u6237\u53ef\u4ee5\u626e\u6f14\u4ee3\u7406\u89d2\u8272\u6765\u4f53\u9a8c\u89c6\u89d2\u5dee\u5f02\uff1b\u91c7\u7528\u4eff\u771f\u534f\u8bae\u4f5c\u4e3a\"\u811a\u624b\u67b6\"\u6765\u7b80\u5316\u591a\u89c6\u89d2\u4eff\u771f\u8fc7\u7a0b\uff0c\u652f\u6301\u7528\u6237\u53cd\u601d\u3001\u8f6c\u6362\u548c\u6574\u5408\u4e0d\u540c\u89c6\u89d2\uff1b\u6784\u5efa\u539f\u578b\u7cfb\u7edf\u5728\u5236\u5ea6\u52a8\u6001\u548c\u571f\u5730\u5229\u7528\u53d8\u5316\u80cc\u666f\u4e0b\u6f14\u793aHoPeS", "result": "\u5728\u8bf4\u660e\u6027\u5b9e\u9a8c\u4e2d\uff0c\u7528\u6237\u4f9d\u6b21\u91c7\u7528\u7cfb\u7edf\u89c2\u5bdf\u8005\u548c\u7814\u7a76\u8005\u7684\u89c6\u89d2\uff0c\u5c3d\u7ba1\u7528\u6237\u52aa\u529b\u63a8\u8350\u6280\u672f\u4e0a\u5408\u7406\u7684\u653f\u7b56\uff0c\u4f46\u7531\u4e8e\u5229\u76ca\u76f8\u5173\u8005\u7684\u7ade\u4e89\u6027\u5021\u5bfc\uff0c\u653f\u7b56\u5efa\u8bae\u4e0e\u5b9e\u65bd\u4e4b\u95f4\u4ecd\u5b58\u5728\u5dee\u5f02\uff0c\u53cd\u6620\u4e86\u7814\u7a76\u8005\u4e0e\u653f\u7b56\u5236\u5b9a\u8005\u89c6\u89d2\u4e4b\u95f4\u7684\u73b0\u5b9e\u9519\u4f4d\u3002\u7528\u6237\u4f53\u9a8c\u5230\u4e86\u4f5c\u4e3a\u7814\u7a76\u8005\u7684\u4e3b\u89c2\u632b\u6298\u611f\u548c\u5931\u671b\u611f\uff0c\u7279\u522b\u662f\u5728\u8bd5\u56fe\u83b7\u5f97\u653f\u6cbb\u5f71\u54cd\u529b\u7684\u540c\u65f6\u4fdd\u6301\u653f\u6cbb\u4e2d\u7acb\u7684\u6311\u6218", "conclusion": "\u5c3d\u7ba1\u5b58\u5728\u632b\u6298\uff0c\u7528\u6237\u4ecd\u8868\u73b0\u51fa\u5c1d\u8bd5\u66ff\u4ee3\u53d9\u4e8b\u6846\u67b6\u7b56\u7565\u7684\u9ad8\u5ea6\u52a8\u673a\uff0c\u8868\u660e\u8be5\u7cfb\u7edf\u5728\u63a2\u7d22\u4e0d\u540c\u89c6\u89d2\u65b9\u9762\u7684\u6f5c\u529b\u3002\u8fdb\u4e00\u6b65\u7684\u7cfb\u7edf\u548c\u534f\u8bae\u5b8c\u5584\u53ef\u80fd\u4f1a\u5728\u793e\u4f1a\u751f\u6001\u4eff\u771f\u4e2d\u5b9e\u73b0\u65b0\u5f62\u5f0f\u7684\u8de8\u5b66\u79d1\u5408\u4f5c"}}
{"id": "2507.17140", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.17140", "abs": "https://arxiv.org/abs/2507.17140", "authors": ["Xiao Liu", "Yunxiao Cheng", "Weijun Wang", "Tianlun Huang", "Zhiyong Wang", "Wei Feng"], "title": "Multi-Objective Trajectory Planning for a Robotic Arm in Curtain Wall Installation", "comment": null, "summary": "In the context of labor shortages and rising costs, construction robots are\nregarded as the key to revolutionizing traditional construction methods and\nimproving efficiency and quality in the construction industry. In order to\nensure that construction robots can perform tasks efficiently and accurately in\ncomplex construction environments, traditional single-objective trajectory\noptimization methods are difficult to meet the complex requirements of the\nchanging construction environment. Therefore, we propose a multi-objective\ntrajectory optimization for the robotic arm used in the curtain wall\ninstallation. First, we design a robotic arm for curtain wall installation,\nintegrating serial, parallel, and folding arm elements, while considering its\nphysical properties and motion characteristics. In addition, this paper\nproposes an NSGA-III-FO algorithm (NSGA-III with Focused Operator, NSGA-III-FO)\nthat incorporates a focus operator screening mechanism to accelerate the\nconvergence of the algorithm towards the Pareto front, thereby effectively\nbalancing the multi-objective constraints of construction robots. The proposed\nalgorithm is tested against NSGA-III, MOEA/D, and MSOPS-II in ten consecutive\ntrials on the DTLZ3 and WFG3 test functions, showing significantly better\nconvergence efficiency than the other algorithms. Finally, we conduct two sets\nof experiments on the designed robotic arm platform, which confirm the\nefficiency and practicality of the NSGA-III-FO algorithm in solving\nmulti-objective trajectory planning problems for curtain wall installation\ntasks.", "AI": {"tldr": "\u9488\u5bf9\u5efa\u7b51\u884c\u4e1a\u52b3\u52a8\u529b\u77ed\u7f3a\u548c\u6210\u672c\u4e0a\u5347\u95ee\u9898\uff0c\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5e55\u5899\u5b89\u88c5\u7684\u673a\u68b0\u81c2\u591a\u76ee\u6807\u8f68\u8ff9\u4f18\u5316\u65b9\u6cd5\uff0c\u8bbe\u8ba1\u4e86\u96c6\u6210\u4e32\u8054\u3001\u5e76\u8054\u548c\u6298\u53e0\u81c2\u5143\u7d20\u7684\u673a\u68b0\u81c2\uff0c\u5e76\u5f00\u53d1\u4e86NSGA-III-FO\u7b97\u6cd5\u6765\u5e73\u8861\u591a\u76ee\u6807\u7ea6\u675f\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002", "motivation": "\u5efa\u7b51\u884c\u4e1a\u9762\u4e34\u52b3\u52a8\u529b\u77ed\u7f3a\u548c\u6210\u672c\u4e0a\u5347\u7684\u6311\u6218\uff0c\u4f20\u7edf\u5355\u76ee\u6807\u8f68\u8ff9\u4f18\u5316\u65b9\u6cd5\u96be\u4ee5\u6ee1\u8db3\u590d\u6742\u5efa\u7b51\u73af\u5883\u4e2d\u65bd\u5de5\u673a\u5668\u4eba\u7684\u591a\u6837\u5316\u9700\u6c42\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5728\u590d\u6742\u73af\u5883\u4e2d\u9ad8\u6548\u51c6\u786e\u6267\u884c\u4efb\u52a1\u7684\u5efa\u7b51\u673a\u5668\u4eba\u591a\u76ee\u6807\u8f68\u8ff9\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u8bbe\u8ba1\u4e86\u96c6\u6210\u4e32\u8054\u3001\u5e76\u8054\u548c\u6298\u53e0\u81c2\u5143\u7d20\u7684\u5e55\u5899\u5b89\u88c5\u673a\u68b0\u81c2\uff0c\u8003\u8651\u5176\u7269\u7406\u7279\u6027\u548c\u8fd0\u52a8\u7279\u5f81\uff1b\u63d0\u51fa\u4e86NSGA-III-FO\u7b97\u6cd5\uff08\u5e26\u6709\u805a\u7126\u7b97\u5b50\u7684NSGA-III\uff09\uff0c\u878d\u5408\u805a\u7126\u7b97\u5b50\u7b5b\u9009\u673a\u5236\u6765\u52a0\u901f\u7b97\u6cd5\u5411\u5e15\u7d2f\u6258\u524d\u6cbf\u6536\u655b\uff0c\u6709\u6548\u5e73\u8861\u5efa\u7b51\u673a\u5668\u4eba\u7684\u591a\u76ee\u6807\u7ea6\u675f\u3002", "result": "\u5728DTLZ3\u548cWFG3\u6d4b\u8bd5\u51fd\u6570\u4e0a\u8fdb\u884c\u7684\u5341\u6b21\u8fde\u7eed\u8bd5\u9a8c\u4e2d\uff0cNSGA-III-FO\u7b97\u6cd5\u76f8\u6bd4NSGA-III\u3001MOEA/D\u548cMSOPS-II\u7b97\u6cd5\u8868\u73b0\u51fa\u660e\u663e\u66f4\u597d\u7684\u6536\u655b\u6548\u7387\uff1b\u5728\u8bbe\u8ba1\u7684\u673a\u68b0\u81c2\u5e73\u53f0\u4e0a\u8fdb\u884c\u7684\u4e24\u7ec4\u5b9e\u9a8c\u8bc1\u5b9e\u4e86\u8be5\u7b97\u6cd5\u5728\u89e3\u51b3\u5e55\u5899\u5b89\u88c5\u4efb\u52a1\u591a\u76ee\u6807\u8f68\u8ff9\u89c4\u5212\u95ee\u9898\u65b9\u9762\u7684\u6548\u7387\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "\u7814\u7a76\u6210\u529f\u5f00\u53d1\u4e86\u7528\u4e8e\u5e55\u5899\u5b89\u88c5\u7684\u5efa\u7b51\u673a\u5668\u4eba\u591a\u76ee\u6807\u8f68\u8ff9\u4f18\u5316\u65b9\u6cd5\uff0cNSGA-III-FO\u7b97\u6cd5\u5728\u7406\u8bba\u6d4b\u8bd5\u548c\u5b9e\u9645\u5e94\u7528\u4e2d\u90fd\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u6027\u80fd\uff0c\u4e3a\u5efa\u7b51\u884c\u4e1a\u81ea\u52a8\u5316\u548c\u6548\u7387\u63d0\u5347\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u6280\u672f\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.17695", "categories": ["cs.AI", "cs.NI"], "pdf": "https://arxiv.org/pdf/2507.17695", "abs": "https://arxiv.org/abs/2507.17695", "authors": ["Ilias Chatzistefanidis", "Navid Nikaein"], "title": "Symbiotic Agents: A Novel Paradigm for Trustworthy AGI-driven Networks", "comment": "Submitted to Computer Networks AI for 6G", "summary": "Large Language Model (LLM)-based autonomous agents are expected to play a\nvital role in the evolution of 6G networks, by empowering real-time\ndecision-making related to management and service provisioning to end-users.\nThis shift facilitates the transition from a specialized intelligence approach,\nwhere artificial intelligence (AI) algorithms handle isolated tasks, to\nartificial general intelligence (AGI)-driven networks, where agents possess\nbroader reasoning capabilities and can manage diverse network functions. In\nthis paper, we introduce a novel agentic paradigm that combines LLMs with\nreal-time optimization algorithms towards Trustworthy AI, defined as symbiotic\nagents. Optimizers at the LLM's input-level provide bounded uncertainty\nsteering for numerically precise tasks, whereas output-level optimizers\nsupervised by the LLM enable adaptive real-time control. We design and\nimplement two novel agent types including: (i) Radio Access Network optimizers,\nand (ii) multi-agent negotiators for Service-Level Agreements (SLAs). We\nfurther propose an end-to-end architecture for AGI networks and evaluate it on\na 5G testbed capturing channel fluctuations from moving vehicles. Results show\nthat symbiotic agents reduce decision errors fivefold compared to standalone\nLLM-based agents, while smaller language models (SLM) achieve similar accuracy\nwith a 99.9% reduction in GPU resource overhead and in near-real-time loops of\n82 ms. A multi-agent demonstration for collaborative RAN on the real-world\ntestbed highlights significant flexibility in service-level agreement and\nresource allocation, reducing RAN over-utilization by approximately 44%.\nDrawing on our findings and open-source implementations, we introduce the\nsymbiotic paradigm as the foundation for next-generation, AGI-driven\nnetworks-systems designed to remain adaptable, efficient, and trustworthy even\nas LLMs advance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u548c\u5b9e\u65f6\u4f18\u5316\u7b97\u6cd5\u7684\u5171\u751f\u667a\u80fd\u4f53\u8303\u5f0f\uff0c\u7528\u4e8e\u6784\u5efa\u53ef\u4fe1\u8d56\u76846G\u7f51\u7edc\u81ea\u4e3b\u7ba1\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7\u8f93\u5165\u7ea7\u548c\u8f93\u51fa\u7ea7\u4f18\u5316\u5668\u5b9e\u73b0\u7cbe\u786e\u63a7\u5236\u548c\u5b9e\u65f6\u9002\u5e94\u3002", "motivation": "6G\u7f51\u7edc\u9700\u8981\u4ece\u4e13\u95e8\u5316AI\u5411\u901a\u7528\u4eba\u5de5\u667a\u80fd(AGI)\u9a71\u52a8\u7684\u7f51\u7edc\u8f6c\u53d8\uff0c\u73b0\u6709\u7684LLM\u667a\u80fd\u4f53\u5728\u7f51\u7edc\u7ba1\u7406\u4e2d\u5b58\u5728\u51b3\u7b56\u8bef\u5dee\u548c\u5b9e\u65f6\u6027\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u53ef\u4fe1\u8d56\u3001\u9ad8\u6548\u7684\u667a\u80fd\u4f53\u67b6\u6784\u6765\u5b9e\u73b0\u5b9e\u65f6\u7f51\u7edc\u7ba1\u7406\u548c\u670d\u52a1\u4f9b\u5e94\u3002", "method": "\u8bbe\u8ba1\u4e86\u5171\u751f\u667a\u80fd\u4f53\u8303\u5f0f\uff0c\u7ed3\u5408LLM\u4e0e\u5b9e\u65f6\u4f18\u5316\u7b97\u6cd5\uff1a(1)\u8f93\u5165\u7ea7\u4f18\u5316\u5668\u4e3a\u6570\u503c\u7cbe\u786e\u4efb\u52a1\u63d0\u4f9b\u6709\u754c\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\uff1b(2)\u8f93\u51fa\u7ea7\u4f18\u5316\u5668\u5728LLM\u76d1\u7763\u4e0b\u5b9e\u73b0\u81ea\u9002\u5e94\u5b9e\u65f6\u63a7\u5236\u3002\u5b9e\u73b0\u4e86\u4e24\u79cd\u65b0\u578b\u667a\u80fd\u4f53\uff1a\u65e0\u7ebf\u63a5\u5165\u7f51\u4f18\u5316\u5668\u548c\u670d\u52a1\u7b49\u7ea7\u534f\u8bae\u591a\u667a\u80fd\u4f53\u534f\u5546\u5668\uff0c\u5e76\u63d0\u51fa\u4e86\u7aef\u5230\u7aefAGI\u7f51\u7edc\u67b6\u6784\u3002", "result": "\u57285G\u6d4b\u8bd5\u5e73\u53f0\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5171\u751f\u667a\u80fd\u4f53\u6bd4\u72ec\u7acbLLM\u667a\u80fd\u4f53\u51cf\u5c11\u4e865\u500d\u7684\u51b3\u7b56\u9519\u8bef\uff1b\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u5728GPU\u8d44\u6e90\u5f00\u9500\u51cf\u5c1199.9%\u7684\u60c5\u51b5\u4e0b\u8fbe\u5230\u76f8\u4f3c\u7cbe\u5ea6\uff0c\u5b9e\u73b082\u6beb\u79d2\u7684\u8fd1\u5b9e\u65f6\u5faa\u73af\uff1b\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u5728\u771f\u5b9e\u6d4b\u8bd5\u5e73\u53f0\u4e0a\u5c06RAN\u8fc7\u5ea6\u5229\u7528\u7387\u964d\u4f4e\u7ea644%\u3002", "conclusion": "\u5171\u751f\u667a\u80fd\u4f53\u8303\u5f0f\u4e3a\u4e0b\u4e00\u4ee3AGI\u9a71\u52a8\u7684\u7f51\u7edc\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5728LLM\u4e0d\u65ad\u53d1\u5c55\u7684\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u4e86\u7cfb\u7edf\u7684\u9002\u5e94\u6027\u3001\u6548\u7387\u548c\u53ef\u4fe1\u8d56\u6027\uff0c\u4e3a6G\u7f51\u7edc\u7684\u667a\u80fd\u5316\u7ba1\u7406\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.17141", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17141", "abs": "https://arxiv.org/abs/2507.17141", "authors": ["Guang Gao", "Jianan Wang", "Jinbo Zuo", "Junnan Jiang", "Jingfan Zhang", "Xianwen Zeng", "Yuejiang Zhu", "Lianyang Ma", "Ke Chen", "Minhua Sheng", "Ruirui Zhang", "Zhaohui An"], "title": "Towards Human-level Intelligence via Human-like Whole-Body Manipulation", "comment": null, "summary": "Building general-purpose intelligent robots has long been a fundamental goal\nof robotics. A promising approach is to mirror the evolutionary trajectory of\nhumans: learning through continuous interaction with the environment, with\nearly progress driven by the imitation of human behaviors. Achieving this goal\npresents three core challenges: (1) designing safe robotic hardware with\nhuman-level physical capabilities; (2) developing an intuitive and scalable\nwhole-body teleoperation interface for data collection; and (3) creating\nalgorithms capable of learning whole-body visuomotor policies from human\ndemonstrations. To address these challenges in a unified framework, we propose\nAstribot Suite, a robot learning suite for whole-body manipulation aimed at\ngeneral daily tasks across diverse environments. We demonstrate the\neffectiveness of our system on a wide range of activities that require\nwhole-body coordination, extensive reachability, human-level dexterity, and\nagility. Our results show that Astribot's cohesive integration of embodiment,\nteleoperation interface, and learning pipeline marks a significant step towards\nreal-world, general-purpose whole-body robotic manipulation, laying the\ngroundwork for the next generation of intelligent robots.", "AI": {"tldr": "Astribot Suite\u662f\u4e00\u4e2a\u673a\u5668\u4eba\u5b66\u4e60\u5957\u4ef6\uff0c\u901a\u8fc7\u5b89\u5168\u7684\u673a\u5668\u4eba\u786c\u4ef6\u3001\u76f4\u89c2\u7684\u5168\u8eab\u9065\u64cd\u4f5c\u63a5\u53e3\u548c\u4ece\u4eba\u7c7b\u6f14\u793a\u4e2d\u5b66\u4e60\u7684\u7b97\u6cd5\uff0c\u5b9e\u73b0\u4e86\u901a\u7528\u65e5\u5e38\u4efb\u52a1\u7684\u5168\u8eab\u64cd\u4f5c\u80fd\u529b", "motivation": "\u6784\u5efa\u901a\u7528\u667a\u80fd\u673a\u5668\u4eba\u662f\u673a\u5668\u4eba\u5b66\u7684\u57fa\u672c\u76ee\u6807\uff0c\u9700\u8981\u901a\u8fc7\u6a21\u4eff\u4eba\u7c7b\u8fdb\u5316\u8f68\u8ff9\u6765\u5b9e\u73b0\u2014\u2014\u901a\u8fc7\u4e0e\u73af\u5883\u7684\u6301\u7eed\u4ea4\u4e92\u5b66\u4e60\uff0c\u65e9\u671f\u8fdb\u5c55\u7531\u6a21\u4eff\u4eba\u7c7b\u884c\u4e3a\u9a71\u52a8\u3002\u8fd9\u9700\u8981\u89e3\u51b3\u4e09\u4e2a\u6838\u5fc3\u6311\u6218\uff1a\u8bbe\u8ba1\u5b89\u5168\u7684\u4eba\u7c7b\u7ea7\u522b\u7269\u7406\u80fd\u529b\u786c\u4ef6\u3001\u5f00\u53d1\u76f4\u89c2\u53ef\u6269\u5c55\u7684\u5168\u8eab\u9065\u64cd\u4f5c\u63a5\u53e3\u3001\u521b\u5efa\u80fd\u4ece\u4eba\u7c7b\u6f14\u793a\u4e2d\u5b66\u4e60\u5168\u8eab\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u7684\u7b97\u6cd5", "method": "\u63d0\u51faAstribot Suite\u7edf\u4e00\u6846\u67b6\uff0c\u6574\u5408\u4e86\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a(1)\u5177\u6709\u4eba\u7c7b\u7ea7\u522b\u7269\u7406\u80fd\u529b\u7684\u5b89\u5168\u673a\u5668\u4eba\u786c\u4ef6\u8bbe\u8ba1\uff1b(2)\u7528\u4e8e\u6570\u636e\u6536\u96c6\u7684\u76f4\u89c2\u4e14\u53ef\u6269\u5c55\u7684\u5168\u8eab\u9065\u64cd\u4f5c\u63a5\u53e3\uff1b(3)\u80fd\u591f\u4ece\u4eba\u7c7b\u6f14\u793a\u4e2d\u5b66\u4e60\u5168\u8eab\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u7684\u7b97\u6cd5", "result": "\u7cfb\u7edf\u5728\u9700\u8981\u5168\u8eab\u534f\u8c03\u3001\u5e7f\u6cdb\u53ef\u8fbe\u6027\u3001\u4eba\u7c7b\u7ea7\u522b\u7075\u5de7\u6027\u548c\u654f\u6377\u6027\u7684\u5404\u79cd\u6d3b\u52a8\u4e2d\u5c55\u73b0\u4e86\u6709\u6548\u6027\uff0c\u6210\u529f\u5b8c\u6210\u4e86\u8de8\u591a\u6837\u5316\u73af\u5883\u7684\u901a\u7528\u65e5\u5e38\u4efb\u52a1", "conclusion": "Astribot\u5728\u673a\u5668\u4eba\u672c\u4f53\u3001\u9065\u64cd\u4f5c\u63a5\u53e3\u548c\u5b66\u4e60\u7ba1\u9053\u65b9\u9762\u7684\u6574\u5408\u6807\u5fd7\u7740\u5411\u771f\u5b9e\u4e16\u754c\u901a\u7528\u5168\u8eab\u673a\u5668\u4eba\u64cd\u4f5c\u8fc8\u51fa\u7684\u91cd\u8981\u4e00\u6b65\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u667a\u80fd\u673a\u5668\u4eba\u5960\u5b9a\u4e86\u57fa\u7840"}}
{"id": "2507.17699", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17699", "abs": "https://arxiv.org/abs/2507.17699", "authors": ["Zhao Song", "Song Yue", "Jiahao Zhang"], "title": "Thinking Isn't an Illusion: Overcoming the Limitations of Reasoning Models via Tool Augmentations", "comment": null, "summary": "Large Reasoning Models (LRMs) have become a central focus in today's large\nlanguage model (LLM) research, where models are designed to output a\nstep-by-step thinking process before arriving at a final answer to handle\ncomplex reasoning tasks. Despite their promise, recent empirical studies (e.g.,\n[Shojaee et al., 2025] from Apple) suggest that this thinking process may not\nactually enhance reasoning ability, where LLMs without explicit reasoning\nactually outperform LRMs on tasks with low or high complexity. In this work, we\nrevisit these findings and investigate whether the limitations of LRMs persist\nwhen tool augmentations are introduced. We incorporate two types of tools,\nPython interpreters and scratchpads, and evaluate three representative LLMs and\ntheir LRM counterparts on Apple's benchmark reasoning puzzles. Our results show\nthat, with proper tool use, LRMs consistently outperform their non-reasoning\ncounterparts across all levels of task complexity. These findings challenge the\nrecent narrative that reasoning is an illusion and highlight the potential of\ntool-augmented LRMs for solving complex problems.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5728\u5f15\u5165\u5de5\u5177\u589e\u5f3a\uff08Python\u89e3\u91ca\u5668\u548c\u8349\u7a3f\u672c\uff09\u540e\uff0c\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u5728\u5404\u79cd\u590d\u6742\u5ea6\u4efb\u52a1\u4e0a\u90fd\u80fd\u6301\u7eed\u8d85\u8d8a\u975e\u63a8\u7406\u6a21\u578b\uff0c\u6311\u6218\u4e86\"\u63a8\u7406\u662f\u5e7b\u89c9\"\u7684\u89c2\u70b9", "motivation": "\u9488\u5bf9\u6700\u8fd1\u7814\u7a76\u58f0\u79f0\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u9010\u6b65\u601d\u8003\u8fc7\u7a0b\u5e76\u4e0d\u80fd\u771f\u6b63\u589e\u5f3a\u63a8\u7406\u80fd\u529b\uff0c\u751a\u81f3\u5728\u67d0\u4e9b\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u5982\u666e\u901aLLM\u7684\u95ee\u9898\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7a76\u5f53\u5f15\u5165\u5de5\u5177\u589e\u5f3a\u65f6\uff0cLRM\u7684\u5c40\u9650\u6027\u662f\u5426\u4ecd\u7136\u5b58\u5728", "method": "\u5728\u4e09\u4e2a\u4ee3\u8868\u6027LLM\u53ca\u5176LRM\u5bf9\u5e94\u7248\u672c\u4e0a\u5f15\u5165\u4e24\u79cd\u5de5\u5177\u589e\u5f3a\uff1aPython\u89e3\u91ca\u5668\u548c\u8349\u7a3f\u672c\uff0c\u5e76\u5728Apple\u57fa\u51c6\u63a8\u7406\u8c1c\u9898\u4e0a\u8fdb\u884c\u8bc4\u4f30\u5bf9\u6bd4", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u901a\u8fc7\u9002\u5f53\u7684\u5de5\u5177\u4f7f\u7528\uff0cLRM\u5728\u6240\u6709\u4efb\u52a1\u590d\u6742\u5ea6\u7ea7\u522b\u4e0a\u90fd\u6301\u7eed\u8d85\u8d8a\u5176\u975e\u63a8\u7406\u5bf9\u5e94\u6a21\u578b", "conclusion": "\u7814\u7a76\u7ed3\u679c\u6311\u6218\u4e86\"\u63a8\u7406\u662f\u5e7b\u89c9\"\u7684\u6700\u65b0\u89c2\u70b9\uff0c\u7a81\u51fa\u4e86\u5de5\u5177\u589e\u5f3a\u578bLRM\u5728\u89e3\u51b3\u590d\u6742\u95ee\u9898\u65b9\u9762\u7684\u6f5c\u529b"}}
{"id": "2507.17144", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.17144", "abs": "https://arxiv.org/abs/2507.17144", "authors": ["Kazuki Numazato", "Keiichiro Kan", "Masaki Kitagawa", "Yunong Li", "Johannes Kubel", "Moju Zhao"], "title": "Falconry-like palm landing by a flapping-wing drone based on the human gesture interaction and distance-aware flight planning", "comment": "8 pages, 14 figures", "summary": "Flapping-wing drones have attracted significant attention due to their\nbiomimetic flight. They are considered more human-friendly due to their\ncharacteristics such as low noise and flexible wings, making them suitable for\nhuman-drone interactions. However, few studies have explored the practical\ninteraction between humans and flapping-wing drones. On establishing a physical\ninteraction system with flapping-wing drones, we can acquire inspirations from\nfalconers who guide birds of prey to land on their arms. This interaction\ninterprets the human body as a dynamic landing platform, which can be utilized\nin various scenarios such as crowded or spatially constrained environments.\nThus, in this study, we propose a falconry-like interaction system in which a\nflapping-wing drone performs a palm landing motion on a human hand. To achieve\na safe approach toward humans, we design a trajectory planning method that\nconsiders both physical and psychological factors of the human safety such as\nthe drone's velocity and distance from the user. We use a commercial flapping\nplatform with our implemented motion planning and conduct experiments to\nevaluate the palm landing performance and safety. The results demonstrate that\nour approach enables safe and smooth hand landing interactions. To the best of\nour knowledge, it is the first time to achieve a contact-based interaction\nbetween flapping-wing drones and humans.", "AI": {"tldr": "\u672c\u7814\u7a76\u9996\u6b21\u5b9e\u73b0\u4e86\u6251\u7ffc\u65e0\u4eba\u673a\u4e0e\u4eba\u7c7b\u7684\u63a5\u89e6\u5f0f\u4ea4\u4e92\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u4eff\u9e70\u730e\u4eba\u7684\u4ea4\u4e92\u7cfb\u7edf\uff0c\u4f7f\u6251\u7ffc\u65e0\u4eba\u673a\u80fd\u591f\u5b89\u5168\u5730\u964d\u843d\u5728\u4eba\u7c7b\u624b\u638c\u4e0a\u3002", "motivation": "\u6251\u7ffc\u65e0\u4eba\u673a\u5177\u6709\u4f4e\u566a\u97f3\u548c\u67d4\u6027\u7ffc\u7b49\u4eba\u7c7b\u53cb\u597d\u7279\u6027\uff0c\u9002\u5408\u4eba\u673a\u4ea4\u4e92\uff0c\u4f46\u7f3a\u4e4f\u5b9e\u9645\u4ea4\u4e92\u7814\u7a76\u3002\u53d7\u9e70\u730e\u4eba\u5f15\u5bfc\u731b\u79bd\u964d\u843d\u5728\u624b\u81c2\u4e0a\u7684\u542f\u53d1\uff0c\u5c06\u4eba\u4f53\u89c6\u4e3a\u52a8\u6001\u7740\u9646\u5e73\u53f0\uff0c\u53ef\u5e94\u7528\u4e8e\u62e5\u6324\u6216\u7a7a\u95f4\u53d7\u9650\u7684\u73af\u5883\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u4eff\u9e70\u730e\u4eba\u7684\u4ea4\u4e92\u7cfb\u7edf\uff0c\u5f00\u53d1\u4e86\u8003\u8651\u4eba\u7c7b\u5b89\u5168\u7269\u7406\u548c\u5fc3\u7406\u56e0\u7d20\u7684\u8f68\u8ff9\u89c4\u5212\u65b9\u6cd5\uff0c\u5305\u62ec\u65e0\u4eba\u673a\u901f\u5ea6\u548c\u4e0e\u7528\u6237\u8ddd\u79bb\u7b49\u53c2\u6570\u3002\u4f7f\u7528\u5546\u7528\u6251\u7ffc\u5e73\u53f0\u5b9e\u73b0\u8fd0\u52a8\u89c4\u5212\u5e76\u8fdb\u884c\u5b9e\u9a8c\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u5b89\u5168\u3001\u5e73\u6ed1\u7684\u624b\u638c\u7740\u9646\u4ea4\u4e92\u3002\u6210\u529f\u9a8c\u8bc1\u4e86\u6251\u7ffc\u65e0\u4eba\u673a\u5728\u4eba\u7c7b\u624b\u638c\u4e0a\u7684\u7740\u9646\u6027\u80fd\u548c\u5b89\u5168\u6027\u3002", "conclusion": "\u9996\u6b21\u5b9e\u73b0\u4e86\u6251\u7ffc\u65e0\u4eba\u673a\u4e0e\u4eba\u7c7b\u7684\u63a5\u89e6\u5f0f\u4ea4\u4e92\uff0c\u8bc1\u660e\u4e86\u4eff\u9e70\u730e\u4eba\u4ea4\u4e92\u7cfb\u7edf\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u4eba\u673a\u4ea4\u4e92\u9886\u57df\u5f00\u8f9f\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2507.17730", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17730", "abs": "https://arxiv.org/abs/2507.17730", "authors": ["Zhe Chen", "Daniel Harabor", "Ryan Hechnenberger", "Nathan R. Sturtevant"], "title": "Online Submission and Evaluation System Design for Competition Operations", "comment": "This work was presented at the Workshop on the International Planning\n  Competition (WIPC 2024)", "summary": "Research communities have developed benchmark datasets across domains to\ncompare the performance of algorithms and techniques However, tracking the\nprogress in these research areas is not easy, as publications appear in\ndifferent venues at the same time, and many of them claim to represent the\nstate-of-the-art. To address this, research communities often organise periodic\ncompetitions to evaluate the performance of various algorithms and techniques,\nthereby tracking advancements in the field. However, these competitions pose a\nsignificant operational burden. The organisers must manage and evaluate a large\nvolume of submissions. Furthermore, participants typically develop their\nsolutions in diverse environments, leading to compatibility issues during the\nevaluation of their submissions. This paper presents an online competition\nsystem that automates the submission and evaluation process for a competition.\nThe competition system allows organisers to manage large numbers of submissions\nefficiently, utilising isolated environments to evaluate submissions. This\nsystem has already been used successfully for several competitions, including\nthe Grid-Based Pathfinding Competition and the League of Robot Runners\ncompetition.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5728\u7ebf\u7ade\u8d5b\u7cfb\u7edf\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u5904\u7406\u7ade\u8d5b\u7684\u63d0\u4ea4\u548c\u8bc4\u4f30\u8fc7\u7a0b\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5b66\u672f\u7ade\u8d5b\u4e2d\u7ec4\u7ec7\u8005\u7ba1\u7406\u5927\u91cf\u63d0\u4ea4\u4ee5\u53ca\u53c2\u4e0e\u8005\u73af\u5883\u517c\u5bb9\u6027\u95ee\u9898", "motivation": "\u7814\u7a76\u793e\u533a\u867d\u7136\u5f00\u53d1\u4e86\u57fa\u51c6\u6570\u636e\u96c6\u6765\u6bd4\u8f83\u7b97\u6cd5\u6027\u80fd\uff0c\u4f46\u8ddf\u8e2a\u7814\u7a76\u8fdb\u5c55\u56f0\u96be\uff0c\u56e0\u4e3a\u8bba\u6587\u53d1\u8868\u5728\u4e0d\u540c\u573a\u6240\u4e14\u90fd\u58f0\u79f0\u4ee3\u8868\u6700\u5148\u8fdb\u6c34\u5e73\u3002\u4f20\u7edf\u7684\u5b9a\u671f\u7ade\u8d5b\u867d\u7136\u80fd\u8bc4\u4f30\u7b97\u6cd5\u6027\u80fd\uff0c\u4f46\u7ed9\u7ec4\u7ec7\u8005\u5e26\u6765\u5de8\u5927\u8fd0\u8425\u8d1f\u62c5\uff0c\u9700\u8981\u7ba1\u7406\u548c\u8bc4\u4f30\u5927\u91cf\u63d0\u4ea4\uff0c\u4e14\u53c2\u4e0e\u8005\u5728\u4e0d\u540c\u73af\u5883\u4e2d\u5f00\u53d1\u89e3\u51b3\u65b9\u6848\u5bfc\u81f4\u8bc4\u4f30\u65f6\u51fa\u73b0\u517c\u5bb9\u6027\u95ee\u9898", "method": "\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e86\u4e00\u4e2a\u5728\u7ebf\u7ade\u8d5b\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u80fd\u591f\u81ea\u52a8\u5316\u7ade\u8d5b\u7684\u63d0\u4ea4\u548c\u8bc4\u4f30\u8fc7\u7a0b\u3002\u7cfb\u7edf\u5141\u8bb8\u7ec4\u7ec7\u8005\u9ad8\u6548\u7ba1\u7406\u5927\u91cf\u63d0\u4ea4\uff0c\u5e76\u5229\u7528\u9694\u79bb\u73af\u5883\u6765\u8bc4\u4f30\u63d0\u4ea4\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ece\u800c\u89e3\u51b3\u517c\u5bb9\u6027\u95ee\u9898", "result": "\u8be5\u7cfb\u7edf\u5df2\u7ecf\u6210\u529f\u5e94\u7528\u4e8e\u591a\u4e2a\u7ade\u8d5b\uff0c\u5305\u62ec\u57fa\u4e8e\u7f51\u683c\u7684\u8def\u5f84\u89c4\u5212\u7ade\u8d5b\uff08Grid-Based Pathfinding Competition\uff09\u548c\u673a\u5668\u4eba\u8dd1\u6b65\u8005\u8054\u76df\u7ade\u8d5b\uff08League of Robot Runners competition\uff09", "conclusion": "\u5728\u7ebf\u7ade\u8d5b\u7cfb\u7edf\u6210\u529f\u89e3\u51b3\u4e86\u4f20\u7edf\u5b66\u672f\u7ade\u8d5b\u4e2d\u7684\u8fd0\u8425\u8d1f\u62c5\u548c\u6280\u672f\u517c\u5bb9\u6027\u95ee\u9898\uff0c\u4e3a\u7814\u7a76\u793e\u533a\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u7684\u7b97\u6cd5\u6027\u80fd\u8bc4\u4f30\u548c\u8fdb\u5c55\u8ddf\u8e2a\u5e73\u53f0\uff0c\u5df2\u5728\u5b9e\u9645\u7ade\u8d5b\u4e2d\u5f97\u5230\u9a8c\u8bc1"}}
{"id": "2507.17152", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.17152", "abs": "https://arxiv.org/abs/2507.17152", "authors": ["Fangze Lin", "Ying He", "Fei Yu", "Hong Zhang"], "title": "JAM: Keypoint-Guided Joint Prediction after Classification-Aware Marginal Proposal for Multi-Agent Interaction", "comment": "IROS 2025 Accepted", "summary": "Predicting the future motion of road participants is a critical task in\nautonomous driving. In this work, we address the challenge of low-quality\ngeneration of low-probability modes in multi-agent joint prediction. To tackle\nthis issue, we propose a two-stage multi-agent interactive prediction framework\nnamed \\textit{keypoint-guided joint prediction after classification-aware\nmarginal proposal} (JAM). The first stage is modeled as a marginal prediction\nprocess, which classifies queries by trajectory type to encourage the model to\nlearn all categories of trajectories, providing comprehensive mode information\nfor the joint prediction module. The second stage is modeled as a joint\nprediction process, which takes the scene context and the marginal proposals\nfrom the first stage as inputs to learn the final joint distribution. We\nexplicitly introduce key waypoints to guide the joint prediction module in\nbetter capturing and leveraging the critical information from the initial\npredicted trajectories. We conduct extensive experiments on the real-world\nWaymo Open Motion Dataset interactive prediction benchmark. The results show\nthat our approach achieves competitive performance. In particular, in the\nframework comparison experiments, the proposed JAM outperforms other prediction\nframeworks and achieves state-of-the-art performance in interactive trajectory\nprediction. The code is available at https://github.com/LinFunster/JAM to\nfacilitate future research.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86JAM\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u9884\u6d4b\uff08\u8fb9\u9645\u9884\u6d4b+\u8054\u5408\u9884\u6d4b\uff09\u548c\u5173\u952e\u70b9\u5f15\u5bfc\u6765\u6539\u5584\u81ea\u52a8\u9a7e\u9a76\u4e2d\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u8f68\u8ff9\u9884\u6d4b\u7684\u4f4e\u6982\u7387\u6a21\u5f0f\u751f\u6210\u8d28\u91cf\u95ee\u9898", "motivation": "\u73b0\u6709\u7684\u591a\u667a\u80fd\u4f53\u8054\u5408\u9884\u6d4b\u65b9\u6cd5\u5728\u751f\u6210\u4f4e\u6982\u7387\u6a21\u5f0f\u65f6\u8d28\u91cf\u8f83\u5dee\uff0c\u96be\u4ee5\u51c6\u786e\u9884\u6d4b\u9053\u8def\u53c2\u4e0e\u8005\u7684\u672a\u6765\u8fd0\u52a8\u8f68\u8ff9\uff0c\u8fd9\u5bf9\u81ea\u52a8\u9a7e\u9a76\u7684\u5b89\u5168\u6027\u6784\u6210\u6311\u6218", "method": "\u63d0\u51faJAM\uff08keypoint-guided joint prediction after classification-aware marginal proposal\uff09\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u8fdb\u884c\u8fb9\u9645\u9884\u6d4b\uff0c\u901a\u8fc7\u8f68\u8ff9\u7c7b\u578b\u5206\u7c7b\u9f13\u52b1\u6a21\u578b\u5b66\u4e60\u6240\u6709\u7c7b\u522b\u8f68\u8ff9\uff1b\u7b2c\u4e8c\u9636\u6bb5\u8fdb\u884c\u8054\u5408\u9884\u6d4b\uff0c\u5229\u7528\u573a\u666f\u4e0a\u4e0b\u6587\u548c\u8fb9\u9645\u63d0\u8bae\u5b66\u4e60\u6700\u7ec8\u8054\u5408\u5206\u5e03\uff0c\u5e76\u5f15\u5165\u5173\u952e\u8def\u5f84\u70b9\u6307\u5bfc\u8054\u5408\u9884\u6d4b\u6a21\u5757", "result": "\u5728Waymo Open Motion\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0cJAM\u65b9\u6cd5\u53d6\u5f97\u4e86\u7ade\u4e89\u6027\u80fd\uff0c\u5728\u6846\u67b6\u6bd4\u8f83\u5b9e\u9a8c\u4e2d\u8d85\u8d8a\u4e86\u5176\u4ed6\u9884\u6d4b\u6846\u67b6\uff0c\u5728\u4ea4\u4e92\u8f68\u8ff9\u9884\u6d4b\u4efb\u52a1\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd", "conclusion": "JAM\u6846\u67b6\u901a\u8fc7\u4e24\u9636\u6bb5\u8bbe\u8ba1\u548c\u5173\u952e\u70b9\u5f15\u5bfc\u6709\u6548\u89e3\u51b3\u4e86\u591a\u667a\u80fd\u4f53\u8054\u5408\u9884\u6d4b\u4e2d\u4f4e\u6982\u7387\u6a21\u5f0f\u751f\u6210\u8d28\u91cf\u5dee\u7684\u95ee\u9898\uff0c\u5728\u4ea4\u4e92\u8f68\u8ff9\u9884\u6d4b\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u6700\u4f18\u6027\u80fd\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u8fd0\u52a8\u9884\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2507.17163", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.17163", "abs": "https://arxiv.org/abs/2507.17163", "authors": ["Botao Lin", "Shuang Song", "Jiaole Wang"], "title": "Reconfigurable Tendon-Driven Robots: Eliminating Inter-segmental Coupling via Independently Lockable Joints", "comment": null, "summary": "With a slender redundant body, the tendon-driven robot (TDR) has a large\nworkspace and great maneuverability while working in complex environments. TDR\ncomprises multiple independently controlled robot segments, each with a set of\ndriving tendons. While increasing the number of robot segments enhances\ndexterity and expands the workspace, this structural expansion also introduces\nintensified inter-segmental coupling. Therefore, achieving precise TDR control\nrequires more complex models and additional motors. This paper presents a\nreconfigurable tendon-driven robot (RTR) equipped with innovative lockable\njoints. Each joint's state (locked/free) can be individually controlled through\na pair of antagonistic tendons, and its structure eliminates the need for a\ncontinuous power supply to maintain the state. Operators can selectively\nactuate the targeted robot segments, and this scheme fundamentally eliminates\nthe inter-segmental coupling, thereby avoiding the requirement for complex\ncoordinated control between segments. The workspace of RTR has been simulated\nand compared with traditional TDRs' workspace, and RTR's advantages are further\nrevealed. The kinematics and statics models of the RTR have been derived and\nvalidation experiments have been conducted. Demonstrations have been performed\nusing a seven-joint RTR prototype to show its reconfigurability and moving\nability in complex environments with an actuator pack comprising only six\nmotors.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u91cd\u6784\u62c9\u7ef3\u9a71\u52a8\u673a\u5668\u4eba\uff08RTR\uff09\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u53ef\u9501\u5b9a\u5173\u8282\u8bbe\u8ba1\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u62c9\u7ef3\u9a71\u52a8\u673a\u5668\u4eba\u7684\u6bb5\u95f4\u8026\u5408\u95ee\u9898\uff0c\u4ec5\u75286\u4e2a\u7535\u673a\u5b9e\u73b0\u4e867\u5173\u8282\u673a\u5668\u4eba\u7684\u590d\u6742\u73af\u5883\u64cd\u4f5c\u3002", "motivation": "\u4f20\u7edf\u62c9\u7ef3\u9a71\u52a8\u673a\u5668\u4eba\u867d\u7136\u5177\u6709\u5927\u5de5\u4f5c\u7a7a\u95f4\u548c\u826f\u597d\u673a\u52a8\u6027\uff0c\u4f46\u589e\u52a0\u673a\u5668\u4eba\u6bb5\u6570\u4f1a\u5bfc\u81f4\u6bb5\u95f4\u8026\u5408\u52a0\u5267\uff0c\u9700\u8981\u66f4\u590d\u6742\u7684\u6a21\u578b\u548c\u66f4\u591a\u7535\u673a\u6765\u5b9e\u73b0\u7cbe\u786e\u63a7\u5236\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u8bbe\u8ba1\u4e86\u914d\u5907\u521b\u65b0\u53ef\u9501\u5b9a\u5173\u8282\u7684\u53ef\u91cd\u6784\u62c9\u7ef3\u9a71\u52a8\u673a\u5668\u4eba\u3002\u6bcf\u4e2a\u5173\u8282\u7684\u72b6\u6001\uff08\u9501\u5b9a/\u81ea\u7531\uff09\u53ef\u901a\u8fc7\u4e00\u5bf9\u62ee\u6297\u62c9\u7ef3\u72ec\u7acb\u63a7\u5236\uff0c\u7ed3\u6784\u8bbe\u8ba1\u65e0\u9700\u6301\u7eed\u4f9b\u7535\u7ef4\u6301\u72b6\u6001\u3002\u64cd\u4f5c\u5458\u53ef\u9009\u62e9\u6027\u9a71\u52a8\u76ee\u6807\u673a\u5668\u4eba\u6bb5\uff0c\u4ece\u6839\u672c\u4e0a\u6d88\u9664\u6bb5\u95f4\u8026\u5408\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u6bd4\u8f83\u4e86RTR\u4e0e\u4f20\u7edfTDR\u7684\u5de5\u4f5c\u7a7a\u95f4\uff0c\u9a8c\u8bc1\u4e86RTR\u7684\u4f18\u52bf\u3002\u5efa\u7acb\u4e86RTR\u7684\u8fd0\u52a8\u5b66\u548c\u9759\u529b\u5b66\u6a21\u578b\u5e76\u8fdb\u884c\u4e86\u9a8c\u8bc1\u5b9e\u9a8c\u3002\u4f7f\u75287\u5173\u8282RTR\u539f\u578b\u8fdb\u884c\u4e86\u6f14\u793a\uff0c\u8bc1\u660e\u5176\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u53ef\u91cd\u6784\u6027\u548c\u8fd0\u52a8\u80fd\u529b\uff0c\u4ec5\u97006\u4e2a\u7535\u673a\u7684\u9a71\u52a8\u5305\u3002", "conclusion": "\u53ef\u91cd\u6784\u62c9\u7ef3\u9a71\u52a8\u673a\u5668\u4eba\u901a\u8fc7\u53ef\u9501\u5b9a\u5173\u8282\u8bbe\u8ba1\u6210\u529f\u89e3\u51b3\u4e86\u4f20\u7edfTDR\u7684\u6bb5\u95f4\u8026\u5408\u95ee\u9898\uff0c\u907f\u514d\u4e86\u590d\u6742\u7684\u6bb5\u95f4\u534f\u8c03\u63a7\u5236\u9700\u6c42\uff0c\u5728\u4fdd\u6301\u5927\u5de5\u4f5c\u7a7a\u95f4\u548c\u673a\u52a8\u6027\u7684\u540c\u65f6\u663e\u8457\u7b80\u5316\u4e86\u63a7\u5236\u7cfb\u7edf\uff0c\u4e3a\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.17210", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.17210", "abs": "https://arxiv.org/abs/2507.17210", "authors": ["Chunran Zheng", "Fu Zhang"], "title": "FAST-Calib: LiDAR-Camera Extrinsic Calibration in One Second", "comment": null, "summary": "This paper proposes FAST-Calib, a fast and user-friendly LiDAR-camera\nextrinsic calibration tool based on a custom-made 3D target. FAST-Calib\nsupports both mechanical and solid-state LiDARs by leveraging an efficient and\nreliable edge extraction algorithm that is agnostic to LiDAR scan patterns. It\nalso compensates for edge dilation artifacts caused by LiDAR spot spread\nthrough ellipse fitting, and supports joint optimization across multiple\nscenes. We validate FAST-Calib on three LiDAR models (Ouster, Avia, and\nMid360), each paired with a wide-angle camera. Experimental results demonstrate\nsuperior accuracy and robustness compared to existing methods. With\npoint-to-point registration errors consistently below 6.5mm and total\nprocessing time under 0.7s, FAST-Calib provides an efficient, accurate, and\ntarget-based automatic calibration pipeline. We have open-sourced our code and\ndataset on GitHub to benefit the robotics community.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86FAST-Calib\uff0c\u4e00\u79cd\u57fa\u4e8e\u5b9a\u52363D\u6807\u5b9a\u677f\u7684\u5feb\u901f\u7528\u6237\u53cb\u597d\u7684\u6fc0\u5149\u96f7\u8fbe-\u76f8\u673a\u5916\u53c2\u6807\u5b9a\u5de5\u5177\uff0c\u652f\u6301\u673a\u68b0\u5f0f\u548c\u56fa\u6001\u6fc0\u5149\u96f7\u8fbe\uff0c\u5177\u6709\u9ad8\u7cbe\u5ea6\u548c\u5feb\u901f\u5904\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u6fc0\u5149\u96f7\u8fbe-\u76f8\u673a\u5916\u53c2\u6807\u5b9a\u65b9\u6cd5\u5b58\u5728\u7cbe\u5ea6\u4e0d\u8db3\u3001\u5904\u7406\u901f\u5ea6\u6162\u3001\u5bf9\u4e0d\u540c\u7c7b\u578b\u6fc0\u5149\u96f7\u8fbe\u9002\u5e94\u6027\u5dee\u7b49\u95ee\u9898\uff0c\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u5feb\u901f\u3001\u51c6\u786e\u3001\u7528\u6237\u53cb\u597d\u7684\u81ea\u52a8\u5316\u6807\u5b9a\u5de5\u5177\u3002", "method": "\u63d0\u51faFAST-Calib\u6807\u5b9a\u65b9\u6cd5\uff0c\u57fa\u4e8e\u5b9a\u52363D\u6807\u5b9a\u677f\uff0c\u91c7\u7528\u4e0e\u6fc0\u5149\u96f7\u8fbe\u626b\u63cf\u6a21\u5f0f\u65e0\u5173\u7684\u9ad8\u6548\u53ef\u9760\u8fb9\u7f18\u63d0\u53d6\u7b97\u6cd5\uff0c\u901a\u8fc7\u692d\u5706\u62df\u5408\u8865\u507f\u6fc0\u5149\u96f7\u8fbe\u5149\u6591\u6269\u6563\u5f15\u8d77\u7684\u8fb9\u7f18\u81a8\u80c0\u4f2a\u5f71\uff0c\u652f\u6301\u591a\u573a\u666f\u8054\u5408\u4f18\u5316\u3002", "result": "\u5728\u4e09\u79cd\u6fc0\u5149\u96f7\u8fbe\u6a21\u578b(Ouster\u3001Avia\u3001Mid360)\u4e0a\u9a8c\u8bc1\uff0c\u70b9\u5230\u70b9\u914d\u51c6\u8bef\u5dee\u59cb\u7ec8\u4f4e\u4e8e6.5mm\uff0c\u603b\u5904\u7406\u65f6\u95f4\u5c11\u4e8e0.7\u79d2\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5c55\u73b0\u51fa\u66f4\u4f18\u7684\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "FAST-Calib\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u3001\u51c6\u786e\u4e14\u57fa\u4e8e\u6807\u5b9a\u677f\u7684\u81ea\u52a8\u5316\u6807\u5b9a\u6d41\u7a0b\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u5f00\u6e90\uff0c\u4e3a\u673a\u5668\u4eba\u793e\u533a\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u6fc0\u5149\u96f7\u8fbe-\u76f8\u673a\u6807\u5b9a\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.17253", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.17253", "abs": "https://arxiv.org/abs/2507.17253", "authors": ["Maharshi Shastri", "Ujjval Shrivastav"], "title": "Optimizing Delivery Logistics: Enhancing Speed and Safety with Drone Technology", "comment": null, "summary": "The increasing demand for fast and cost effective last mile delivery\nsolutions has catalyzed significant advancements in drone based logistics. This\nresearch describes the development of an AI integrated drone delivery system,\nfocusing on route optimization, object detection, secure package handling, and\nreal time tracking. The proposed system leverages YOLOv4 Tiny for object\ndetection, the NEO 6M GPS module for navigation, and the A7670 SIM module for\nreal time communication. A comparative analysis of lightweight AI models and\nhardware components is conducted to determine the optimal configuration for\nreal time UAV based delivery. Key challenges including battery efficiency,\nregulatory compliance, and security considerations are addressed through the\nintegration of machine learning techniques, IoT devices, and encryption\nprotocols. Preliminary studies demonstrate improvement in delivery time\ncompared to conventional ground based logistics, along with high accuracy\nrecipient authentication through facial recognition. The study also discusses\nethical implications and societal acceptance of drone deliveries, ensuring\ncompliance with FAA, EASA and DGCA regulatory standards. Note: This paper\npresents the architecture, design, and preliminary simulation results of the\nproposed system. Experimental results, simulation benchmarks, and deployment\nstatistics are currently being acquired. A comprehensive analysis will be\nincluded in the extended version of this work.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2aAI\u96c6\u6210\u7684\u65e0\u4eba\u673a\u914d\u9001\u7cfb\u7edf\uff0c\u91c7\u7528YOLOv4 Tiny\u8fdb\u884c\u76ee\u6807\u68c0\u6d4b\uff0c\u96c6\u6210GPS\u5bfc\u822a\u548c\u5b9e\u65f6\u901a\u4fe1\u6a21\u5757\uff0c\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u6280\u672f\u4f18\u5316\u8def\u7ebf\u5e76\u63d0\u9ad8\u914d\u9001\u6548\u7387\uff0c\u540c\u65f6\u89e3\u51b3\u4e86\u7535\u6c60\u6548\u7387\u3001\u76d1\u7ba1\u5408\u89c4\u548c\u5b89\u5168\u7b49\u5173\u952e\u6311\u6218\u3002", "motivation": "\u968f\u7740\u5bf9\u5feb\u901f\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u6700\u540e\u4e00\u516c\u91cc\u914d\u9001\u89e3\u51b3\u65b9\u6848\u9700\u6c42\u4e0d\u65ad\u589e\u957f\uff0c\u9700\u8981\u5f00\u53d1\u5148\u8fdb\u7684\u65e0\u4eba\u673a\u7269\u6d41\u7cfb\u7edf\u6765\u6ee1\u8db3\u73b0\u4ee3\u914d\u9001\u8981\u6c42\uff0c\u63d0\u9ad8\u914d\u9001\u6548\u7387\u5e76\u964d\u4f4e\u6210\u672c\u3002", "method": "\u91c7\u7528YOLOv4 Tiny\u8f7b\u91cf\u7ea7\u6a21\u578b\u8fdb\u884c\u76ee\u6807\u68c0\u6d4b\uff0c\u4f7f\u7528NEO 6M GPS\u6a21\u5757\u8fdb\u884c\u5bfc\u822a\u5b9a\u4f4d\uff0c\u96c6\u6210A7670 SIM\u6a21\u5757\u5b9e\u73b0\u5b9e\u65f6\u901a\u4fe1\uff0c\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u6280\u672f\u8fdb\u884c\u8def\u7ebf\u4f18\u5316\uff0c\u7ed3\u5408IoT\u8bbe\u5907\u548c\u52a0\u5bc6\u534f\u8bae\u89e3\u51b3\u5b89\u5168\u95ee\u9898\uff0c\u5e76\u5b9e\u73b0\u9762\u90e8\u8bc6\u522b\u8fdb\u884c\u6536\u4ef6\u4eba\u8eab\u4efd\u9a8c\u8bc1\u3002", "result": "\u521d\u6b65\u7814\u7a76\u663e\u793a\u76f8\u6bd4\u4f20\u7edf\u5730\u9762\u7269\u6d41\u914d\u9001\u65f6\u95f4\u6709\u6240\u6539\u5584\uff0c\u901a\u8fc7\u9762\u90e8\u8bc6\u522b\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u6536\u4ef6\u4eba\u8eab\u4efd\u9a8c\u8bc1\uff0c\u7cfb\u7edf\u67b6\u6784\u8bbe\u8ba1\u5b8c\u6210\u5e76\u8fdb\u884c\u4e86\u521d\u6b65\u4eff\u771f\uff0c\u76ee\u524d\u6b63\u5728\u83b7\u53d6\u5b9e\u9a8c\u7ed3\u679c\u3001\u4eff\u771f\u57fa\u51c6\u548c\u90e8\u7f72\u7edf\u8ba1\u6570\u636e\u3002", "conclusion": "\u6210\u529f\u8bbe\u8ba1\u4e86AI\u96c6\u6210\u65e0\u4eba\u673a\u914d\u9001\u7cfb\u7edf\u7684\u67b6\u6784\uff0c\u89e3\u51b3\u4e86\u7535\u6c60\u6548\u7387\u3001\u76d1\u7ba1\u5408\u89c4\u7b49\u5173\u952e\u6280\u672f\u6311\u6218\uff0c\u7cfb\u7edf\u5728\u914d\u9001\u6548\u7387\u548c\u5b89\u5168\u6027\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u7b26\u5408FAA\u3001EASA\u548cDGCA\u7b49\u76d1\u7ba1\u6807\u51c6\uff0c\u4f46\u9700\u8981\u8fdb\u4e00\u6b65\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u548c\u5168\u9762\u5206\u6790\u4ee5\u5b8c\u5584\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2507.17445", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.17445", "abs": "https://arxiv.org/abs/2507.17445", "authors": ["Haichuan Li", "Changda Tian", "Panos Trahanias", "Tomi Westerlund"], "title": "IndoorBEV: Joint Detection and Footprint Completion of Objects via Mask-based Prediction in Indoor Scenarios for Bird's-Eye View Perception", "comment": null, "summary": "Detecting diverse objects within complex indoor 3D point clouds presents\nsignificant challenges for robotic perception, particularly with varied object\nshapes, clutter, and the co-existence of static and dynamic elements where\ntraditional bounding box methods falter. To address these limitations, we\npropose IndoorBEV, a novel mask-based Bird's-Eye View (BEV) method for indoor\nmobile robots.\n  In a BEV method, a 3D scene is projected into a 2D BEV grid which handles\nnaturally occlusions and provides a consistent top-down view aiding to\ndistinguish static obstacles from dynamic agents. The obtained 2D BEV results\nis directly usable to downstream robotic tasks like navigation, motion\nprediction, and planning. Our architecture utilizes an axis compact encoder and\na window-based backbone to extract rich spatial features from this BEV map. A\nquery-based decoder head then employs learned object queries to concurrently\npredict object classes and instance masks in the BEV space. This mask-centric\nformulation effectively captures the footprint of both static and dynamic\nobjects regardless of their shape, offering a robust alternative to bounding\nbox regression. We demonstrate the effectiveness of IndoorBEV on a custom\nindoor dataset featuring diverse object classes including static objects\n  and dynamic elements like robots and miscellaneous items, showcasing its\npotential for robust indoor scene understanding.", "AI": {"tldr": "IndoorBEV\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u63a9\u7801\u7684\u9e1f\u77b0\u89c6\u56fe\u65b9\u6cd5\uff0c\u7528\u4e8e\u5ba4\u5185\u79fb\u52a8\u673a\u5668\u4eba\u76843D\u70b9\u4e91\u76ee\u6807\u68c0\u6d4b\uff0c\u901a\u8fc7\u5c063D\u573a\u666f\u6295\u5f71\u52302D BEV\u7f51\u683c\u6765\u5904\u7406\u906e\u6321\u95ee\u9898\uff0c\u5e76\u4f7f\u7528\u67e5\u8be2\u89e3\u7801\u5668\u9884\u6d4b\u76ee\u6807\u7c7b\u522b\u548c\u5b9e\u4f8b\u63a9\u7801\uff0c\u6709\u6548\u6355\u83b7\u9759\u6001\u548c\u52a8\u6001\u7269\u4f53\u7684\u8db3\u8ff9\u3002", "motivation": "\u4f20\u7edf\u8fb9\u754c\u6846\u65b9\u6cd5\u5728\u590d\u6742\u5ba4\u51853D\u70b9\u4e91\u73af\u5883\u4e2d\u68c0\u6d4b\u591a\u6837\u5316\u76ee\u6807\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u4e0d\u540c\u7269\u4f53\u5f62\u72b6\u3001\u6742\u4e71\u73af\u5883\u4ee5\u53ca\u9759\u6001\u548c\u52a8\u6001\u5143\u7d20\u5171\u5b58\u7684\u573a\u666f\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9c81\u68d2\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u63d0\u51faIndoorBEV\u65b9\u6cd5\uff0c\u5c063D\u573a\u666f\u6295\u5f71\u52302D\u9e1f\u77b0\u89c6\u56fe\u7f51\u683c\u4e2d\uff0c\u4f7f\u7528\u8f74\u7d27\u51d1\u7f16\u7801\u5668\u548c\u57fa\u4e8e\u7a97\u53e3\u7684\u4e3b\u5e72\u7f51\u7edc\u4eceBEV\u5730\u56fe\u4e2d\u63d0\u53d6\u4e30\u5bcc\u7684\u7a7a\u95f4\u7279\u5f81\uff0c\u7136\u540e\u901a\u8fc7\u57fa\u4e8e\u67e5\u8be2\u7684\u89e3\u7801\u5668\u5934\u4f7f\u7528\u5b66\u4e60\u7684\u76ee\u6807\u67e5\u8be2\u540c\u65f6\u9884\u6d4bBEV\u7a7a\u95f4\u4e2d\u7684\u76ee\u6807\u7c7b\u522b\u548c\u5b9e\u4f8b\u63a9\u7801\u3002", "result": "\u5728\u5305\u542b\u591a\u6837\u5316\u76ee\u6807\u7c7b\u522b\uff08\u5305\u62ec\u9759\u6001\u7269\u4f53\u548c\u673a\u5668\u4eba\u7b49\u52a8\u6001\u5143\u7d20\uff09\u7684\u81ea\u5b9a\u4e49\u5ba4\u5185\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86IndoorBEV\u7684\u6709\u6548\u6027\uff0c\u5c55\u793a\u4e86\u5176\u5728\u9c81\u68d2\u5ba4\u5185\u573a\u666f\u7406\u89e3\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u751f\u6210\u76842D BEV\u7ed3\u679c\u53ef\u76f4\u63a5\u7528\u4e8e\u5bfc\u822a\u3001\u8fd0\u52a8\u9884\u6d4b\u548c\u89c4\u5212\u7b49\u4e0b\u6e38\u673a\u5668\u4eba\u4efb\u52a1\u3002", "conclusion": "IndoorBEV\u901a\u8fc7\u57fa\u4e8e\u63a9\u7801\u7684\u9e1f\u77b0\u89c6\u56fe\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5ba4\u5185\u590d\u6742\u73af\u5883\u4e2d\u7684\u76ee\u6807\u68c0\u6d4b\u95ee\u9898\uff0c\u5176\u63a9\u7801\u4e2d\u5fc3\u7684\u8868\u8ff0\u65b9\u5f0f\u80fd\u591f\u6709\u6548\u6355\u83b7\u4e0d\u540c\u5f62\u72b6\u7684\u9759\u6001\u548c\u52a8\u6001\u7269\u4f53\u8db3\u8ff9\uff0c\u4e3a\u8fb9\u754c\u6846\u56de\u5f52\u63d0\u4f9b\u4e86\u9c81\u68d2\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5728\u5ba4\u5185\u573a\u666f\u7406\u89e3\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u826f\u597d\u7684\u6027\u80fd\u3002"}}
{"id": "2507.17275", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.17275", "abs": "https://arxiv.org/abs/2507.17275", "authors": ["Po-Yen Wu", "Cheng-Yu Kuo", "Yuki Kadokawa", "Takamitsu Matsubara"], "title": "Prolonging Tool Life: Learning Skillful Use of General-purpose Tools through Lifespan-guided Reinforcement Learning", "comment": "Under review", "summary": "In inaccessible environments with uncertain task demands, robots often rely\non general-purpose tools that lack predefined usage strategies. These tools are\nnot tailored for particular operations, making their longevity highly sensitive\nto how they are used. This creates a fundamental challenge: how can a robot\nlearn a tool-use policy that both completes the task and prolongs the tool's\nlifespan? In this work, we address this challenge by introducing a\nreinforcement learning (RL) framework that incorporates tool lifespan as a\nfactor during policy optimization. Our framework leverages Finite Element\nAnalysis (FEA) and Miner's Rule to estimate Remaining Useful Life (RUL) based\non accumulated stress, and integrates the RUL into the RL reward to guide\npolicy learning toward lifespan-guided behavior. To handle the fact that RUL\ncan only be estimated after task execution, we introduce an Adaptive Reward\nNormalization (ARN) mechanism that dynamically adjusts reward scaling based on\nestimated RULs, ensuring stable learning signals. We validate our method across\nsimulated and real-world tool use tasks, including Object-Moving and\nDoor-Opening with multiple general-purpose tools. The learned policies\nconsistently prolong tool lifespan (up to 8.01x in simulation) and transfer\neffectively to real-world settings, demonstrating the practical value of\nlearning lifespan-guided tool use strategies.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u5de5\u5177\u5bff\u547d\u4f5c\u4e3a\u7b56\u7565\u4f18\u5316\u56e0\u7d20\uff0c\u8bad\u7ec3\u673a\u5668\u4eba\u5b66\u4e60\u65e2\u80fd\u5b8c\u6210\u4efb\u52a1\u53c8\u80fd\u5ef6\u957f\u5de5\u5177\u4f7f\u7528\u5bff\u547d\u7684\u7b56\u7565\uff0c\u5728\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u6548\u679c\u3002", "motivation": "\u5728\u4e0d\u53ef\u8fbe\u73af\u5883\u4e2d\uff0c\u673a\u5668\u4eba\u9700\u8981\u4f7f\u7528\u901a\u7528\u5de5\u5177\u5b8c\u6210\u4efb\u52a1\uff0c\u4f46\u8fd9\u4e9b\u5de5\u5177\u7f3a\u4e4f\u9884\u5b9a\u4e49\u7684\u4f7f\u7528\u7b56\u7565\uff0c\u4e14\u5bff\u547d\u5bf9\u4f7f\u7528\u65b9\u5f0f\u9ad8\u5ea6\u654f\u611f\u3002\u5982\u4f55\u8ba9\u673a\u5668\u4eba\u5b66\u4f1a\u65e2\u80fd\u5b8c\u6210\u4efb\u52a1\u53c8\u80fd\u5ef6\u957f\u5de5\u5177\u5bff\u547d\u7684\u4f7f\u7528\u7b56\u7565\u662f\u4e00\u4e2a\u57fa\u672c\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5c06\u5de5\u5177\u5bff\u547d\u7eb3\u5165\u7b56\u7565\u4f18\u5316\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u3002\u4f7f\u7528\u6709\u9650\u5143\u5206\u6790(FEA)\u548c\u77ff\u5de5\u6cd5\u5219\u57fa\u4e8e\u7d2f\u79ef\u5e94\u529b\u4f30\u7b97\u5269\u4f59\u4f7f\u7528\u5bff\u547d(RUL)\uff0c\u5c06RUL\u6574\u5408\u5230RL\u5956\u52b1\u4e2d\u6307\u5bfc\u7b56\u7565\u5b66\u4e60\u3002\u5f15\u5165\u81ea\u9002\u5e94\u5956\u52b1\u5f52\u4e00\u5316(ARN)\u673a\u5236\uff0c\u6839\u636e\u4f30\u8ba1\u7684RUL\u52a8\u6001\u8c03\u6574\u5956\u52b1\u7f29\u653e\uff0c\u786e\u4fdd\u5b66\u4e60\u4fe1\u53f7\u7a33\u5b9a\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u7684\u5de5\u5177\u4f7f\u7528\u4efb\u52a1(\u5305\u62ec\u7269\u4f53\u79fb\u52a8\u548c\u5f00\u95e8\u4efb\u52a1)\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u5b66\u4e60\u5230\u7684\u7b56\u7565\u80fd\u591f\u6301\u7eed\u5ef6\u957f\u5de5\u5177\u5bff\u547d(\u4eff\u771f\u4e2d\u6700\u9ad8\u8fbe8.01\u500d)\uff0c\u5e76\u80fd\u6709\u6548\u8fc1\u79fb\u5230\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u5728\u4f7f\u7528\u901a\u7528\u5de5\u5177\u65f6\u5982\u4f55\u5e73\u8861\u4efb\u52a1\u5b8c\u6210\u548c\u5de5\u5177\u5bff\u547d\u5ef6\u957f\u7684\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u5b66\u4e60\u5bff\u547d\u5bfc\u5411\u5de5\u5177\u4f7f\u7528\u7b56\u7565\u7684\u5b9e\u7528\u4ef7\u503c\uff0c\u4e3a\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5de5\u5177\u4f7f\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.17294", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.17294", "abs": "https://arxiv.org/abs/2507.17294", "authors": ["Jianxin Bi", "Kevin Yuchen Ma", "Ce Hao", "Mike Zheng Shou", "Harold Soh"], "title": "VLA-Touch: Enhancing Vision-Language-Action Models with Dual-Level Tactile Feedback", "comment": "19 pages, 5 figures", "summary": "Tactile feedback is generally recognized to be crucial for effective\ninteraction with the physical world. However, state-of-the-art\nVision-Language-Action (VLA) models lack the ability to interpret and use\ntactile signals, limiting their effectiveness in contact-rich tasks.\nIncorporating tactile feedback into these systems is challenging due to the\nabsence of large multi-modal datasets. We present VLA-Touch, an approach that\nenhances generalist robot policies with tactile sensing \\emph{without\nfine-tuning} the base VLA. Our method introduces two key innovations: (1) a\npipeline that leverages a pretrained tactile-language model that provides\nsemantic tactile feedback for high-level task planning, and (2) a\ndiffusion-based controller that refines VLA-generated actions with tactile\nsignals for contact-rich manipulation. Through real-world experiments, we\ndemonstrate that our dual-level integration of tactile feedback improves task\nplanning efficiency while enhancing execution precision. Code is open-sourced\nat \\href{https://github.com/jxbi1010/VLA-Touch}{this URL}.", "AI": {"tldr": "VLA-Touch\u662f\u4e00\u79cd\u5728\u4e0d\u5fae\u8c03\u57fa\u7840VLA\u6a21\u578b\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u53cc\u5c42\u6b21\u89e6\u89c9\u53cd\u9988\u96c6\u6210\u6765\u589e\u5f3a\u901a\u7528\u673a\u5668\u4eba\u7b56\u7565\u7684\u65b9\u6cd5\uff0c\u5305\u62ec\u7528\u4e8e\u9ad8\u7ea7\u4efb\u52a1\u89c4\u5212\u7684\u89e6\u89c9-\u8bed\u8a00\u6a21\u578b\u548c\u7528\u4e8e\u63a5\u89e6\u4e30\u5bcc\u64cd\u4f5c\u7684\u57fa\u4e8e\u6269\u6563\u7684\u63a7\u5236\u5668\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c(VLA)\u6a21\u578b\u7f3a\u4e4f\u89e3\u91ca\u548c\u4f7f\u7528\u89e6\u89c9\u4fe1\u53f7\u7684\u80fd\u529b\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5728\u63a5\u89e6\u4e30\u5bcc\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002\u7531\u4e8e\u7f3a\u5c11\u5927\u578b\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u5c06\u89e6\u89c9\u53cd\u9988\u6574\u5408\u5230\u8fd9\u4e9b\u7cfb\u7edf\u4e2d\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u63d0\u51faVLA-Touch\u65b9\u6cd5\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u521b\u65b0\uff1a(1)\u5229\u7528\u9884\u8bad\u7ec3\u89e6\u89c9-\u8bed\u8a00\u6a21\u578b\u4e3a\u9ad8\u7ea7\u4efb\u52a1\u89c4\u5212\u63d0\u4f9b\u8bed\u4e49\u89e6\u89c9\u53cd\u9988\u7684\u7ba1\u9053\uff1b(2)\u57fa\u4e8e\u6269\u6563\u7684\u63a7\u5236\u5668\uff0c\u7528\u89e6\u89c9\u4fe1\u53f7\u7ec6\u5316VLA\u751f\u6210\u7684\u52a8\u4f5c\u4ee5\u8fdb\u884c\u63a5\u89e6\u4e30\u5bcc\u7684\u64cd\u4f5c\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u5fae\u8c03\u57fa\u7840VLA\u6a21\u578b\u3002", "result": "\u901a\u8fc7\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u8bc1\u660e\uff0c\u53cc\u5c42\u6b21\u89e6\u89c9\u53cd\u9988\u96c6\u6210\u63d0\u9ad8\u4e86\u4efb\u52a1\u89c4\u5212\u6548\u7387\uff0c\u540c\u65f6\u589e\u5f3a\u4e86\u6267\u884c\u7cbe\u5ea6\u3002\u4ee3\u7801\u5df2\u5f00\u6e90\u3002", "conclusion": "VLA-Touch\u6210\u529f\u5b9e\u73b0\u4e86\u5728\u4e0d\u4fee\u6539\u57fa\u7840VLA\u6a21\u578b\u7684\u524d\u63d0\u4e0b\u6574\u5408\u89e6\u89c9\u53cd\u9988\uff0c\u901a\u8fc7\u9ad8\u7ea7\u89c4\u5212\u548c\u4f4e\u7ea7\u63a7\u5236\u7684\u53cc\u5c42\u6b21\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5728\u63a5\u89e6\u4e30\u5bcc\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2507.17317", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.17317", "abs": "https://arxiv.org/abs/2507.17317", "authors": ["Miguel Escudero-Jim\u00e9nez", "No\u00e9 P\u00e9rez-Higueras", "Andr\u00e9s Mart\u00ednez-Silva", "Fernando Caballero", "Luis Merino"], "title": "HuNavSim 2.0", "comment": "Preprint submitted to the 8th Iberian Robotics Conference (ROBOT\n  2025)", "summary": "This work presents a new iteration of the Human Navigation Simulator\n(HuNavSim), a novel open-source tool for the simulation of different\nhuman-agent navigation behaviors in scenarios with mobile robots. The tool,\nprogrammed under the ROS 2 framework, can be used together with different\nwell-known robotics simulators such as Gazebo or NVidia Isaac Sim. The main\ngoal is to facilitate the development and evaluation of human-aware robot\nnavigation systems in simulation. In this new version, several features have\nbeen improved and new ones added, such as the extended set of actions and\nconditions that can be combined in Behavior Trees to compound complex and\nrealistic human behaviors.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4eba\u7c7b\u5bfc\u822a\u4eff\u771f\u5668(HuNavSim)\u7684\u65b0\u7248\u672c\uff0c\u8fd9\u662f\u4e00\u4e2a\u5f00\u6e90\u5de5\u5177\uff0c\u7528\u4e8e\u5728\u79fb\u52a8\u673a\u5668\u4eba\u573a\u666f\u4e2d\u4eff\u771f\u4e0d\u540c\u7684\u4eba\u673a\u5bfc\u822a\u884c\u4e3a\uff0c\u57fa\u4e8eROS 2\u6846\u67b6\u5f00\u53d1\uff0c\u53ef\u4e0eGazebo\u6216NVidia Isaac Sim\u7b49\u673a\u5668\u4eba\u4eff\u771f\u5668\u914d\u5408\u4f7f\u7528\u3002", "motivation": "\u4e3a\u4e86\u4fc3\u8fdb\u4eba\u673a\u611f\u77e5\u673a\u5668\u4eba\u5bfc\u822a\u7cfb\u7edf\u5728\u4eff\u771f\u73af\u5883\u4e2d\u7684\u5f00\u53d1\u548c\u8bc4\u4f30\uff0c\u9700\u8981\u4e00\u4e2a\u80fd\u591f\u6a21\u62df\u590d\u6742\u771f\u5b9e\u4eba\u7c7b\u884c\u4e3a\u7684\u4eff\u771f\u5de5\u5177\u3002", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8eROS 2\u6846\u67b6\u7684\u4eba\u7c7b\u5bfc\u822a\u4eff\u771f\u5668\uff0c\u53ef\u4e0e\u591a\u79cd\u77e5\u540d\u673a\u5668\u4eba\u4eff\u771f\u5668\uff08\u5982Gazebo\u548cNVidia Isaac Sim\uff09\u96c6\u6210\u4f7f\u7528\uff0c\u901a\u8fc7\u884c\u4e3a\u6811\u7ec4\u5408\u6269\u5c55\u7684\u52a8\u4f5c\u548c\u6761\u4ef6\u96c6\u6765\u6784\u5efa\u590d\u6742\u4e14\u771f\u5b9e\u7684\u4eba\u7c7b\u884c\u4e3a\u3002", "result": "\u65b0\u7248\u672c\u6539\u8fdb\u4e86\u591a\u9879\u529f\u80fd\u5e76\u6dfb\u52a0\u4e86\u65b0\u7279\u6027\uff0c\u7279\u522b\u662f\u6269\u5c55\u4e86\u53ef\u5728\u884c\u4e3a\u6811\u4e2d\u7ec4\u5408\u7684\u52a8\u4f5c\u548c\u6761\u4ef6\u96c6\uff0c\u80fd\u591f\u6784\u5efa\u66f4\u52a0\u590d\u6742\u548c\u771f\u5b9e\u7684\u4eba\u7c7b\u884c\u4e3a\u6a21\u5f0f\u3002", "conclusion": "HuNavSim\u65b0\u7248\u672c\u4e3a\u4eba\u673a\u611f\u77e5\u673a\u5668\u4eba\u5bfc\u822a\u7cfb\u7edf\u7684\u4eff\u771f\u5f00\u53d1\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u5de5\u5177\u652f\u6301\uff0c\u901a\u8fc7\u6539\u8fdb\u7684\u884c\u4e3a\u6811\u7cfb\u7edf\u80fd\u591f\u66f4\u597d\u5730\u6a21\u62df\u771f\u5b9e\u7684\u4eba\u7c7b\u5bfc\u822a\u884c\u4e3a\u3002"}}
{"id": "2507.17338", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.17338", "abs": "https://arxiv.org/abs/2507.17338", "authors": ["Corrado Pezzato", "Ozan \u00c7atal", "Toon Van de Maele", "Riddhi J. Pitliya", "Tim Verbelen"], "title": "Mobile Manipulation with Active Inference for Long-Horizon Rearrangement Tasks", "comment": null, "summary": "Despite growing interest in active inference for robotic control, its\napplication to complex, long-horizon tasks remains untested. We address this\ngap by introducing a fully hierarchical active inference architecture for\ngoal-directed behavior in realistic robotic settings. Our model combines a\nhigh-level active inference model that selects among discrete skills realized\nvia a whole-body active inference controller. This unified approach enables\nflexible skill composition, online adaptability, and recovery from task\nfailures without requiring offline training. Evaluated on the Habitat Benchmark\nfor mobile manipulation, our method outperforms state-of-the-art baselines\nacross the three long-horizon tasks, demonstrating for the first time that\nactive inference can scale to the complexity of modern robotics benchmarks.", "AI": {"tldr": "\u7814\u7a76\u56e2\u961f\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u4e3b\u52a8\u63a8\u7406\u67b6\u6784\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u590d\u6742\u957f\u671f\u4efb\u52a1\u63a7\u5236\uff0c\u5728Habitat\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e86\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\uff0c\u9996\u6b21\u8bc1\u660e\u4e3b\u52a8\u63a8\u7406\u53ef\u4ee5\u6269\u5c55\u5230\u73b0\u4ee3\u673a\u5668\u4eba\u57fa\u51c6\u7684\u590d\u6742\u6027\u3002", "motivation": "\u5c3d\u7ba1\u4e3b\u52a8\u63a8\u7406\u5728\u673a\u5668\u4eba\u63a7\u5236\u9886\u57df\u5f15\u8d77\u4e86\u8d8a\u6765\u8d8a\u591a\u7684\u5173\u6ce8\uff0c\u4f46\u5176\u5728\u590d\u6742\u3001\u957f\u671f\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4ecd\u672a\u5f97\u5230\u6d4b\u8bd5\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u5b8c\u5168\u5206\u5c42\u7684\u4e3b\u52a8\u63a8\u7406\u67b6\u6784\uff0c\u7ed3\u5408\u9ad8\u7ea7\u4e3b\u52a8\u63a8\u7406\u6a21\u578b\u6765\u9009\u62e9\u79bb\u6563\u6280\u80fd\uff0c\u5e76\u901a\u8fc7\u5168\u8eab\u4e3b\u52a8\u63a8\u7406\u63a7\u5236\u5668\u5b9e\u73b0\u8fd9\u4e9b\u6280\u80fd\uff0c\u5f62\u6210\u7edf\u4e00\u7684\u65b9\u6cd5\u6846\u67b6\u3002", "result": "\u5728Habitat\u79fb\u52a8\u64cd\u4f5c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u4e09\u4e2a\u957f\u671f\u4efb\u52a1\u4e0a\u90fd\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u7075\u6d3b\u7684\u6280\u80fd\u7ec4\u5408\u3001\u5728\u7ebf\u9002\u5e94\u6027\u548c\u4efb\u52a1\u5931\u8d25\u6062\u590d\u80fd\u529b\u3002", "conclusion": "\u9996\u6b21\u8bc1\u660e\u4e86\u4e3b\u52a8\u63a8\u7406\u53ef\u4ee5\u6269\u5c55\u5230\u73b0\u4ee3\u673a\u5668\u4eba\u57fa\u51c6\u7684\u590d\u6742\u6027\uff0c\u4e3a\u76ee\u6807\u5bfc\u5411\u884c\u4e3a\u5728\u73b0\u5b9e\u673a\u5668\u4eba\u73af\u5883\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u4e14\u65e0\u9700\u79bb\u7ebf\u8bad\u7ec3\u3002"}}
{"id": "2507.17376", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.17376", "abs": "https://arxiv.org/abs/2507.17376", "authors": ["Tianshu Ruan", "Aniketh Ramesh", "Rustam Stolkin", "Manolis Chiou"], "title": "An Exploratory Study on Human-Robot Interaction using Semantics-based Situational Awareness", "comment": null, "summary": "In this paper, we investigate the impact of high-level semantics (evaluation\nof the environment) on Human-Robot Teams (HRT) and Human-Robot Interaction\n(HRI) in the context of mobile robot deployments. Although semantics has been\nwidely researched in AI, how high-level semantics can benefit the HRT paradigm\nis underexplored, often fuzzy, and intractable. We applied a semantics-based\nframework that could reveal different indicators of the environment (i.e. how\nmuch semantic information exists) in a mock-up disaster response mission. In\nsuch missions, semantics are crucial as the HRT should handle complex\nsituations and respond quickly with correct decisions, where humans might have\na high workload and stress. Especially when human operators need to shift their\nattention between robots and other tasks, they will struggle to build\nSituational Awareness (SA) quickly. The experiment suggests that the presented\nsemantics: 1) alleviate the perceived workload of human operators; 2) increase\nthe operator's trust in the SA; and 3) help to reduce the reaction time in\nswitching the level of autonomy when needed. Additionally, we find that\nparticipants with higher trust in the system are encouraged by high-level\nsemantics to use teleoperation mode more.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u9ad8\u7ea7\u8bed\u4e49\u4fe1\u606f\u5bf9\u4eba\u673a\u56e2\u961f\u534f\u4f5c\u548c\u4eba\u673a\u4ea4\u4e92\u7684\u5f71\u54cd\uff0c\u901a\u8fc7\u707e\u96be\u54cd\u5e94\u4efb\u52a1\u5b9e\u9a8c\u53d1\u73b0\uff0c\u8bed\u4e49\u4fe1\u606f\u80fd\u591f\u51cf\u8f7b\u64cd\u4f5c\u5458\u5de5\u4f5c\u8d1f\u8377\u3001\u63d0\u9ad8\u60c5\u5883\u611f\u77e5\u4fe1\u4efb\u5ea6\u5e76\u7f29\u77ed\u81ea\u4e3b\u6027\u5207\u6362\u53cd\u5e94\u65f6\u95f4\u3002", "motivation": "\u5728\u79fb\u52a8\u673a\u5668\u4eba\u90e8\u7f72\u7684\u4eba\u673a\u56e2\u961f\u534f\u4f5c\u4e2d\uff0c\u9ad8\u7ea7\u8bed\u4e49\u4fe1\u606f\u7684\u4f5c\u7528\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u7279\u522b\u662f\u5728\u707e\u96be\u54cd\u5e94\u7b49\u590d\u6742\u4efb\u52a1\u4e2d\uff0c\u64cd\u4f5c\u5458\u9762\u4e34\u9ad8\u5de5\u4f5c\u8d1f\u8377\u548c\u538b\u529b\uff0c\u9700\u8981\u5728\u673a\u5668\u4eba\u548c\u5176\u4ed6\u4efb\u52a1\u95f4\u5feb\u901f\u5207\u6362\u6ce8\u610f\u529b\uff0c\u96be\u4ee5\u5feb\u901f\u5efa\u7acb\u60c5\u5883\u611f\u77e5\u80fd\u529b\u3002", "method": "\u91c7\u7528\u57fa\u4e8e\u8bed\u4e49\u7684\u6846\u67b6\uff0c\u5728\u6a21\u62df\u707e\u96be\u54cd\u5e94\u4efb\u52a1\u4e2d\u8bc4\u4f30\u73af\u5883\u7684\u9ad8\u7ea7\u8bed\u4e49\u4fe1\u606f\uff08\u5373\u73af\u5883\u4e2d\u5b58\u5728\u591a\u5c11\u8bed\u4e49\u4fe1\u606f\uff09\u3002\u901a\u8fc7\u5b9e\u9a8c\u6d4b\u8bd5\u8bed\u4e49\u4fe1\u606f\u5bf9\u4eba\u673a\u56e2\u961f\u534f\u4f5c\u6548\u679c\u7684\u5f71\u54cd\uff0c\u5305\u62ec\u5de5\u4f5c\u8d1f\u8377\u3001\u60c5\u5883\u611f\u77e5\u4fe1\u4efb\u5ea6\u548c\u81ea\u4e3b\u6027\u5207\u6362\u53cd\u5e94\u65f6\u95f4\u7b49\u6307\u6807\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u9ad8\u7ea7\u8bed\u4e49\u4fe1\u606f\u80fd\u591f\uff1a1\uff09\u51cf\u8f7b\u4eba\u7c7b\u64cd\u4f5c\u5458\u7684\u611f\u77e5\u5de5\u4f5c\u8d1f\u8377\uff1b2\uff09\u63d0\u9ad8\u64cd\u4f5c\u5458\u5bf9\u60c5\u5883\u611f\u77e5\u7684\u4fe1\u4efb\u5ea6\uff1b3\uff09\u5e2e\u52a9\u7f29\u77ed\u5728\u9700\u8981\u65f6\u5207\u6362\u81ea\u4e3b\u6027\u7ea7\u522b\u7684\u53cd\u5e94\u65f6\u95f4\u3002\u6b64\u5916\uff0c\u5bf9\u7cfb\u7edf\u4fe1\u4efb\u5ea6\u8f83\u9ad8\u7684\u53c2\u4e0e\u8005\u66f4\u503e\u5411\u4e8e\u5728\u9ad8\u7ea7\u8bed\u4e49\u4fe1\u606f\u7684\u9f13\u52b1\u4e0b\u4f7f\u7528\u8fdc\u7a0b\u64cd\u4f5c\u6a21\u5f0f\u3002", "conclusion": "\u9ad8\u7ea7\u8bed\u4e49\u4fe1\u606f\u5728\u4eba\u673a\u56e2\u961f\u534f\u4f5c\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u80fd\u591f\u6709\u6548\u6539\u5584\u4eba\u673a\u4ea4\u4e92\u6548\u679c\u3002\u8bed\u4e49\u6846\u67b6\u4e3a\u590d\u6742\u73af\u5883\u4e0b\u7684\u4eba\u673a\u534f\u4f5c\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u5feb\u901f\u51b3\u7b56\u548c\u9ad8\u6548\u534f\u4f5c\u7684\u707e\u96be\u54cd\u5e94\u7b49\u573a\u666f\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2507.17379", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.17379", "abs": "https://arxiv.org/abs/2507.17379", "authors": ["Shen Tan", "Dong Zhou", "Xiangyu Shao", "Junqiao Wang", "Guanghui Sun"], "title": "Language-Conditioned Open-Vocabulary Mobile Manipulation with Pretrained Models", "comment": "IJCAI 2025", "summary": "Open-vocabulary mobile manipulation (OVMM) that involves the handling of\nnovel and unseen objects across different workspaces remains a significant\nchallenge for real-world robotic applications. In this paper, we propose a\nnovel Language-conditioned Open-Vocabulary Mobile Manipulation framework, named\nLOVMM, incorporating the large language model (LLM) and vision-language model\n(VLM) to tackle various mobile manipulation tasks in household environments.\nOur approach is capable of solving various OVMM tasks with free-form natural\nlanguage instructions (e.g. \"toss the food boxes on the office room desk to the\ntrash bin in the corner\", and \"pack the bottles from the bed to the box in the\nguestroom\"). Extensive experiments simulated in complex household environments\nshow strong zero-shot generalization and multi-task learning abilities of\nLOVMM. Moreover, our approach can also generalize to multiple tabletop\nmanipulation tasks and achieve better success rates compared to other\nstate-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86LOVMM\u6846\u67b6\uff0c\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u5b9e\u73b0\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u7684\u5f00\u653e\u8bcd\u6c47\u79fb\u52a8\u64cd\u4f5c\uff0c\u80fd\u591f\u5904\u7406\u5bb6\u5ead\u73af\u5883\u4e2d\u7684\u5404\u79cd\u65b0\u9896\u7269\u4f53\u64cd\u4f5c\u4efb\u52a1", "motivation": "\u5f00\u653e\u8bcd\u6c47\u79fb\u52a8\u64cd\u4f5c(OVMM)\u9700\u8981\u5904\u7406\u4e0d\u540c\u5de5\u4f5c\u7a7a\u95f4\u4e2d\u7684\u65b0\u9896\u548c\u672a\u89c1\u8fc7\u7684\u7269\u4f53\uff0c\u8fd9\u5bf9\u771f\u5b9e\u4e16\u754c\u7684\u673a\u5668\u4eba\u5e94\u7528\u6765\u8bf4\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218", "method": "\u63d0\u51faLOVMM\u6846\u67b6\uff0c\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b(LLM)\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b(VLM)\u6765\u5904\u7406\u5bb6\u5ead\u73af\u5883\u4e2d\u7684\u5404\u79cd\u79fb\u52a8\u64cd\u4f5c\u4efb\u52a1\uff0c\u80fd\u591f\u6267\u884c\u81ea\u7531\u5f62\u5f0f\u7684\u81ea\u7136\u8bed\u8a00\u6307\u4ee4", "result": "\u5728\u590d\u6742\u5bb6\u5ead\u73af\u5883\u7684\u4eff\u771f\u5b9e\u9a8c\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u548c\u591a\u4efb\u52a1\u5b66\u4e60\u80fd\u529b\uff0c\u5728\u591a\u4e2a\u684c\u9762\u64cd\u4f5c\u4efb\u52a1\u4e2d\u4e5f\u80fd\u6cdb\u5316\u5e76\u83b7\u5f97\u6bd4\u5176\u4ed6\u6700\u5148\u8fdb\u65b9\u6cd5\u66f4\u597d\u7684\u6210\u529f\u7387", "conclusion": "LOVMM\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u5f00\u653e\u8bcd\u6c47\u79fb\u52a8\u64cd\u4f5c\u7684\u6311\u6218\uff0c\u901a\u8fc7\u7ed3\u5408LLM\u548cVLM\u5b9e\u73b0\u4e86\u5bf9\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u7684\u7406\u89e3\u548c\u6267\u884c\uff0c\u5728\u5bb6\u5ead\u73af\u5883\u548c\u684c\u9762\u64cd\u4f5c\u4efb\u52a1\u4e2d\u90fd\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u6027\u80fd"}}
{"id": "2507.17383", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.17383", "abs": "https://arxiv.org/abs/2507.17383", "authors": ["Thomas P Zollo", "Richard Zemel"], "title": "Confidence Calibration in Vision-Language-Action Models", "comment": "34 pages, 19 figures", "summary": "Trustworthy robot behavior requires not only high levels of task success but\nalso that the robot can reliably quantify how likely it is to succeed. To this\nend, we present the first systematic study of confidence calibration in\nvision-language-action (VLA) foundation models, which map visual observations\nand natural-language instructions to low-level robot motor commands. We begin\nwith extensive benchmarking to understand the critical relationship between\ntask success and calibration error across multiple datasets and VLA variants,\nfinding that task performance and calibration are not in tension. Next, we\nintroduce prompt ensembles for VLAs, a lightweight, Bayesian-inspired algorithm\nthat averages confidence across paraphrased instructions and consistently\nimproves calibration. We further analyze calibration over the task time\nhorizon, showing that confidence is often most reliable after making some\nprogress, suggesting natural points for risk-aware intervention. Finally, we\nreveal differential miscalibration across action dimensions and propose\naction-wise Platt scaling, a method to recalibrate each action dimension\nindependently to produce better confidence estimates. Our aim in this study is\nto begin to develop the tools and conceptual understanding necessary to render\nVLAs both highly performant and highly trustworthy via reliable uncertainty\nquantification.", "AI": {"tldr": "\u672c\u7814\u7a76\u9996\u6b21\u7cfb\u7edf\u6027\u5730\u7814\u7a76\u4e86\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c(VLA)\u57fa\u7840\u6a21\u578b\u4e2d\u7684\u7f6e\u4fe1\u5ea6\u6821\u51c6\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u63d0\u793a\u96c6\u6210\u548c\u52a8\u4f5c\u7ef4\u5ea6Platt\u6807\u5b9a\u7b49\u65b9\u6cd5\u6765\u63d0\u9ad8\u673a\u5668\u4eba\u884c\u4e3a\u7684\u53ef\u4fe1\u5ea6\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u80fd\u529b\u3002", "motivation": "\u53ef\u4fe1\u8d56\u7684\u673a\u5668\u4eba\u884c\u4e3a\u4e0d\u4ec5\u9700\u8981\u9ad8\u4efb\u52a1\u6210\u529f\u7387\uff0c\u8fd8\u9700\u8981\u673a\u5668\u4eba\u80fd\u591f\u53ef\u9760\u5730\u91cf\u5316\u5176\u6210\u529f\u6982\u7387\u3002\u73b0\u6709\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u57fa\u7840\u6a21\u578b\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7684\u7f6e\u4fe1\u5ea6\u6821\u51c6\u7814\u7a76\uff0c\u8fd9\u9650\u5236\u4e86\u673a\u5668\u4eba\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53ef\u4fe1\u5ea6\u3002", "method": "1) \u5bf9\u591a\u4e2a\u6570\u636e\u96c6\u548cVLA\u53d8\u4f53\u8fdb\u884c\u5e7f\u6cdb\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5206\u6790\u4efb\u52a1\u6210\u529f\u7387\u4e0e\u6821\u51c6\u8bef\u5dee\u7684\u5173\u7cfb\uff1b2) \u63d0\u51fa\u63d0\u793a\u96c6\u6210\u7b97\u6cd5\uff0c\u901a\u8fc7\u5bf9\u91ca\u4e49\u6307\u4ee4\u7684\u7f6e\u4fe1\u5ea6\u6c42\u5e73\u5747\u6765\u6539\u5584\u6821\u51c6\uff1b3) \u5206\u6790\u4efb\u52a1\u65f6\u95f4\u8303\u56f4\u5185\u7684\u6821\u51c6\u60c5\u51b5\uff1b4) \u63d0\u51fa\u52a8\u4f5c\u7ef4\u5ea6\u7684Platt\u6807\u5b9a\u65b9\u6cd5\uff0c\u72ec\u7acb\u91cd\u65b0\u6821\u51c6\u6bcf\u4e2a\u52a8\u4f5c\u7ef4\u5ea6\u3002", "result": "\u53d1\u73b0\u4efb\u52a1\u6027\u80fd\u4e0e\u6821\u51c6\u5e76\u4e0d\u51b2\u7a81\uff1b\u63d0\u793a\u96c6\u6210\u7b97\u6cd5\u6301\u7eed\u6539\u5584\u4e86\u6821\u51c6\u6548\u679c\uff1b\u7f6e\u4fe1\u5ea6\u5728\u53d6\u5f97\u4e00\u5b9a\u8fdb\u5c55\u540e\u5f80\u5f80\u6700\u53ef\u9760\uff0c\u4e3a\u98ce\u9669\u611f\u77e5\u5e72\u9884\u63d0\u4f9b\u4e86\u81ea\u7136\u8282\u70b9\uff1b\u4e0d\u540c\u52a8\u4f5c\u7ef4\u5ea6\u5b58\u5728\u5dee\u5f02\u6027\u8bef\u6821\u51c6\u73b0\u8c61\u3002", "conclusion": "\u901a\u8fc7\u5f00\u53d1\u5fc5\u8981\u7684\u5de5\u5177\u548c\u6982\u5ff5\u7406\u89e3\uff0c\u7814\u7a76\u4e3a\u4f7fVLA\u6a21\u578b\u65e2\u9ad8\u6027\u80fd\u53c8\u9ad8\u53ef\u4fe1\u8d56\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u901a\u8fc7\u53ef\u9760\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u5b9e\u73b0\u4e86\u8fd9\u4e00\u76ee\u6807\u3002\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u673a\u5668\u4eba\u884c\u4e3a\u7684\u7f6e\u4fe1\u5ea6\u6821\u51c6\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.17401", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.17401", "abs": "https://arxiv.org/abs/2507.17401", "authors": ["Rachel Ringe", "Mihai Pomarlan", "Nikolaos Tsiogkas", "Stefano De Giorgis", "Maria Hedblom", "Rainer Malaka"], "title": "The Wilhelm Tell Dataset of Affordance Demonstrations", "comment": "\\c{opyright} 2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "Affordances - i.e. possibilities for action that an environment or objects in\nit provide - are important for robots operating in human environments to\nperceive. Existing approaches train such capabilities on annotated static\nimages or shapes. This work presents a novel dataset for affordance learning of\ncommon household tasks. Unlike previous approaches, our dataset consists of\nvideo sequences demonstrating the tasks from first- and third-person\nperspectives, along with metadata about the affordances that are manifested in\nthe task, and is aimed towards training perception systems to recognize\naffordance manifestations. The demonstrations were collected from several\nparticipants and in total record about seven hours of human activity. The\nvariety of task performances also allows studying preparatory maneuvers that\npeople may perform for a task, such as how they arrange their task space, which\nis also relevant for collaborative service robots.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u89c6\u9891\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bad\u7ec3\u673a\u5668\u4eba\u5728\u5bb6\u5ead\u73af\u5883\u4e2d\u8bc6\u522b\u7269\u4f53\u7684\u53ef\u4f9b\u6027\uff08affordances\uff09\uff0c\u6570\u636e\u96c6\u5305\u542b\u7b2c\u4e00\u4eba\u79f0\u548c\u7b2c\u4e09\u4eba\u79f0\u89c6\u89d2\u7684\u4efb\u52a1\u6f14\u793a\u89c6\u9891\uff0c\u603b\u8ba1\u7ea67\u5c0f\u65f6\u7684\u4eba\u7c7b\u6d3b\u52a8\u8bb0\u5f55\u3002", "motivation": "\u73b0\u6709\u7684\u53ef\u4f9b\u6027\u5b66\u4e60\u65b9\u6cd5\u4e3b\u8981\u57fa\u4e8e\u9759\u6001\u56fe\u50cf\u6216\u5f62\u72b6\u7684\u6807\u6ce8\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u7f3a\u4e4f\u52a8\u6001\u7684\u3001\u771f\u5b9e\u7684\u4efb\u52a1\u6f14\u793a\u6570\u636e\u3002\u673a\u5668\u4eba\u5728\u4eba\u7c7b\u73af\u5883\u4e2d\u64cd\u4f5c\u9700\u8981\u80fd\u591f\u611f\u77e5\u73af\u5883\u548c\u7269\u4f53\u63d0\u4f9b\u7684\u884c\u52a8\u53ef\u80fd\u6027\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u597d\u7684\u6570\u636e\u96c6\u6765\u8bad\u7ec3\u8fd9\u79cd\u611f\u77e5\u80fd\u529b\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u5e38\u89c1\u5bb6\u5ead\u4efb\u52a1\u7684\u89c6\u9891\u5e8f\u5217\u6570\u636e\u96c6\uff0c\u4ece\u7b2c\u4e00\u4eba\u79f0\u548c\u7b2c\u4e09\u4eba\u79f0\u89c6\u89d2\u8bb0\u5f55\u4efb\u52a1\u6f14\u793a\uff0c\u5e76\u63d0\u4f9b\u5173\u4e8e\u4efb\u52a1\u4e2d\u4f53\u73b0\u7684\u53ef\u4f9b\u6027\u7684\u5143\u6570\u636e\u3002\u6570\u636e\u6536\u96c6\u6765\u81ea\u591a\u4e2a\u53c2\u4e0e\u8005\uff0c\u65e8\u5728\u8bad\u7ec3\u611f\u77e5\u7cfb\u7edf\u8bc6\u522b\u53ef\u4f9b\u6027\u7684\u8868\u73b0\u5f62\u5f0f\u3002", "result": "\u6210\u529f\u6536\u96c6\u4e86\u7ea67\u5c0f\u65f6\u7684\u4eba\u7c7b\u6d3b\u52a8\u8bb0\u5f55\uff0c\u6570\u636e\u96c6\u6db5\u76d6\u4e86\u591a\u79cd\u4efb\u52a1\u6267\u884c\u65b9\u5f0f\uff0c\u8fd8\u80fd\u591f\u7814\u7a76\u4eba\u4eec\u4e3a\u5b8c\u6210\u4efb\u52a1\u800c\u8fdb\u884c\u7684\u51c6\u5907\u6027\u64cd\u4f5c\uff0c\u5982\u4efb\u52a1\u7a7a\u95f4\u7684\u5b89\u6392\u7b49\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u4e3a\u673a\u5668\u4eba\u53ef\u4f9b\u6027\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u8d44\u6e90\uff0c\u7279\u522b\u9002\u7528\u4e8e\u8bad\u7ec3\u80fd\u591f\u8bc6\u522b\u53ef\u4f9b\u6027\u8868\u73b0\u7684\u611f\u77e5\u7cfb\u7edf\uff0c\u540c\u65f6\u4e3a\u534f\u4f5c\u670d\u52a1\u673a\u5668\u4eba\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u6570\u636e\uff0c\u5305\u62ec\u4eba\u7c7b\u4efb\u52a1\u51c6\u5907\u884c\u4e3a\u7684\u5206\u6790\u3002"}}
{"id": "2507.17519", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.17519", "abs": "https://arxiv.org/abs/2507.17519", "authors": ["Kostas Karakontis", "Thanos Petsanis", "Athanasios Ch. Kapoutsis", "Pavlos Ch. Kapoutsis", "Elias B. Kosmatopoulos"], "title": "Terrain-Aware Adaptation for Two-Dimensional UAV Path Planners", "comment": null, "summary": "Multi-UAV Coverage Path Planning (mCPP) algorithms in popular commercial\nsoftware typically treat a Region of Interest (RoI) only as a 2D plane,\nignoring important3D structure characteristics. This leads to incomplete\n3Dreconstructions, especially around occluded or vertical surfaces. In this\npaper, we propose a modular algorithm that can extend commercial\ntwo-dimensional path planners to facilitate terrain-aware planning by adjusting\naltitude and camera orientations. To demonstrate it, we extend the well-known\nDARP (Divide Areas for Optimal Multi-Robot Coverage Path Planning) algorithm\nand produce DARP-3D. We present simulation results in multiple 3D environments\nand a real-world flight test using DJI hardware. Compared to baseline, our\napproach consistently captures improved 3D reconstructions, particularly in\nareas with significant vertical features. An open-source implementation of the\nalgorithm is available here:https://github.com/konskara/TerraPlan", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u5757\u5316\u7b97\u6cd5\uff0c\u5c06\u5546\u4e1a\u4e8c\u7ef4\u8def\u5f84\u89c4\u5212\u5668\u6269\u5c55\u4e3a\u5730\u5f62\u611f\u77e5\u7684\u4e09\u7ef4\u89c4\u5212\u5668\uff0c\u901a\u8fc7\u8c03\u6574\u9ad8\u5ea6\u548c\u76f8\u673a\u65b9\u5411\u6765\u6539\u5584\u65e0\u4eba\u673a\u96c6\u7fa4\u7684\u8986\u76d6\u8def\u5f84\u89c4\u5212\uff0c\u7279\u522b\u662f\u5728\u5782\u76f4\u8868\u9762\u548c\u906e\u6321\u533a\u57df\u76843D\u91cd\u5efa\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u5546\u4e1a\u8f6f\u4ef6\u4e2d\u7684\u591a\u65e0\u4eba\u673a\u8986\u76d6\u8def\u5f84\u89c4\u5212\u7b97\u6cd5\u901a\u5e38\u53ea\u5c06\u611f\u5174\u8da3\u533a\u57df\u89c6\u4e3a2D\u5e73\u9762\uff0c\u5ffd\u7565\u4e86\u91cd\u8981\u76843D\u7ed3\u6784\u7279\u5f81\uff0c\u8fd9\u5bfc\u81f43D\u91cd\u5efa\u4e0d\u5b8c\u6574\uff0c\u7279\u522b\u662f\u5728\u906e\u6321\u6216\u5782\u76f4\u8868\u9762\u5468\u56f4\u5b58\u5728\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u5757\u5316\u7b97\u6cd5\uff0c\u53ef\u4ee5\u6269\u5c55\u5546\u4e1a\u4e8c\u7ef4\u8def\u5f84\u89c4\u5212\u5668\u4ee5\u5b9e\u73b0\u5730\u5f62\u611f\u77e5\u89c4\u5212\uff0c\u901a\u8fc7\u8c03\u6574\u9ad8\u5ea6\u548c\u76f8\u673a\u65b9\u5411\u6765\u4f18\u5316\u8def\u5f84\u3002\u4f5c\u4e3a\u6f14\u793a\uff0c\u5c06\u8457\u540d\u7684DARP\u7b97\u6cd5\u6269\u5c55\u4e3aDARP-3D\u7b97\u6cd5\u3002", "result": "\u5728\u591a\u4e2a3D\u73af\u5883\u4e2d\u8fdb\u884c\u4e86\u4eff\u771f\u6d4b\u8bd5\uff0c\u5e76\u4f7f\u7528\u5927\u7586\u786c\u4ef6\u8fdb\u884c\u4e86\u771f\u5b9e\u4e16\u754c\u7684\u98de\u884c\u6d4b\u8bd5\u3002\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u57283D\u91cd\u5efa\u65b9\u9762\u8868\u73b0\u66f4\u597d\uff0c\u7279\u522b\u662f\u5728\u5177\u6709\u663e\u8457\u5782\u76f4\u7279\u5f81\u7684\u533a\u57df\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6a21\u5757\u5316\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u5730\u5c062D\u8def\u5f84\u89c4\u5212\u6269\u5c55\u52303D\u5730\u5f62\u611f\u77e5\u89c4\u5212\uff0c\u663e\u8457\u6539\u5584\u4e86\u591a\u65e0\u4eba\u673a\u7cfb\u7edf\u5728\u590d\u67423D\u73af\u5883\u4e2d\u7684\u8986\u76d6\u6548\u679c\u548c\u91cd\u5efa\u8d28\u91cf\uff0c\u5e76\u63d0\u4f9b\u4e86\u5f00\u6e90\u5b9e\u73b0\u3002"}}
{"id": "2507.17520", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.17520", "abs": "https://arxiv.org/abs/2507.17520", "authors": ["Shuai Yang", "Hao Li", "Yilun Chen", "Bin Wang", "Yang Tian", "Tai Wang", "Hanqing Wang", "Feng Zhao", "Yiyi Liao", "Jiangmiao Pang"], "title": "InstructVLA: Vision-Language-Action Instruction Tuning from Understanding to Manipulation", "comment": "38 pages", "summary": "To operate effectively in the real world, robots must integrate multimodal\nreasoning with precise action generation. However, existing\nvision-language-action (VLA) models often sacrifice one for the other, narrow\ntheir abilities to task-specific manipulation data, and suffer catastrophic\nforgetting of pre-trained vision-language capabilities. To bridge this gap, we\nintroduce InstructVLA, an end-to-end VLA model that preserves the flexible\nreasoning of large vision-language models (VLMs) while delivering leading\nmanipulation performance. InstructVLA introduces a novel training paradigm,\nVision-Language-Action Instruction Tuning (VLA-IT), which employs multimodal\ntraining with mixture-of-experts adaptation to jointly optimize textual\nreasoning and action generation on both standard VLM corpora and a curated\n650K-sample VLA-IT dataset. On in-domain SimplerEnv tasks, InstructVLA achieves\n30.5% improvement over SpatialVLA. To evaluate generalization, we introduce\nSimplerEnv-Instruct, an 80-task benchmark requiring closed-loop control and\nhigh-level instruction understanding, where it outperforms a fine-tuned OpenVLA\nby 92% and an action expert aided by GPT-4o by 29%. Additionally, InstructVLA\nsurpasses baseline VLMs on multimodal tasks and exhibits inference-time scaling\nby leveraging textual reasoning to boost manipulation performance in both\nsimulated and real-world settings. These results demonstrate InstructVLA's\npotential for bridging intuitive and steerable human-robot interaction with\nefficient policy learning.", "AI": {"tldr": "InstructVLA\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u901a\u8fc7\u65b0\u9896\u7684VLA-IT\u8bad\u7ec3\u8303\u5f0f\uff0c\u5728\u4fdd\u6301\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7075\u6d3b\u63a8\u7406\u80fd\u529b\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u9886\u5148\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u6027\u80fd\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c(VLA)\u6a21\u578b\u5f80\u5f80\u5728\u591a\u6a21\u6001\u63a8\u7406\u548c\u7cbe\u786e\u52a8\u4f5c\u751f\u6210\u4e4b\u95f4\u505a\u51fa\u727a\u7272\uff0c\u80fd\u529b\u5c40\u9650\u4e8e\u7279\u5b9a\u4efb\u52a1\u7684\u64cd\u4f5c\u6570\u636e\uff0c\u5e76\u4e14\u4f1a\u906d\u53d7\u9884\u8bad\u7ec3\u89c6\u89c9\u8bed\u8a00\u80fd\u529b\u7684\u707e\u96be\u6027\u9057\u5fd8\u3002\u9700\u8981\u5f00\u53d1\u4e00\u4e2a\u80fd\u591f\u540c\u65f6\u4fdd\u6301\u7075\u6d3b\u63a8\u7406\u548c\u5b9e\u73b0\u9ad8\u6548\u64cd\u4f5c\u6027\u80fd\u7684\u7edf\u4e00\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u4e86\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6307\u4ee4\u8c03\u4f18(VLA-IT)\u8bad\u7ec3\u8303\u5f0f\uff0c\u91c7\u7528\u6df7\u5408\u4e13\u5bb6\u9002\u5e94\u7684\u591a\u6a21\u6001\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5728\u6807\u51c6VLM\u8bed\u6599\u5e93\u548c\u7cbe\u5fc3\u7b56\u5212\u768465\u4e07\u6837\u672cVLA-IT\u6570\u636e\u96c6\u4e0a\u8054\u5408\u4f18\u5316\u6587\u672c\u63a8\u7406\u548c\u52a8\u4f5c\u751f\u6210\u80fd\u529b\u3002", "result": "\u5728\u57df\u5185SimplerEnv\u4efb\u52a1\u4e0a\u6bd4SpatialVLA\u63d0\u534730.5%\uff1b\u5728\u65b0\u5f15\u5165\u7684SimplerEnv-Instruct 80\u4efb\u52a1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6bd4\u5fae\u8c03\u7684OpenVLA\u8d85\u51fa92%\uff0c\u6bd4GPT-4o\u8f85\u52a9\u7684\u52a8\u4f5c\u4e13\u5bb6\u8d85\u51fa29%\uff1b\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e0a\u8d85\u8d8a\u57fa\u7ebfVLM\uff0c\u5e76\u5c55\u73b0\u4e86\u63a8\u7406\u65f6\u7f29\u653e\u80fd\u529b\u3002", "conclusion": "InstructVLA\u6210\u529f\u5f25\u5408\u4e86\u76f4\u89c2\u53ef\u63a7\u7684\u4eba\u673a\u4ea4\u4e92\u4e0e\u9ad8\u6548\u7b56\u7565\u5b66\u4e60\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u673a\u5668\u4eba\u9886\u57df\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u96c6\u6210\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.17531", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.17531", "abs": "https://arxiv.org/abs/2507.17531", "authors": ["Abdel-Raouf Dannaoui", "Johann Laconte", "Christophe Debain", "Francois Pomerleau", "Paul Checchin"], "title": "When and Where Localization Fails: An Analysis of the Iterative Closest Point in Evolving Environment", "comment": "7 pages, 7 figures, proceedings in European Conference on Mobile\n  Robots (ECMR) 2025", "summary": "Robust relocalization in dynamic outdoor environments remains a key challenge\nfor autonomous systems relying on 3D lidar. While long-term localization has\nbeen widely studied, short-term environmental changes, occurring over days or\nweeks, remain underexplored despite their practical significance. To address\nthis gap, we present a highresolution, short-term multi-temporal dataset\ncollected weekly from February to April 2025 across natural and semi-urban\nsettings. Each session includes high-density point cloud maps, 360 deg\npanoramic images, and trajectory data. Projected lidar scans, derived from the\npoint cloud maps and modeled with sensor-accurate occlusions, are used to\nevaluate alignment accuracy against the ground truth using two Iterative\nClosest Point (ICP) variants: Point-to-Point and Point-to-Plane. Results show\nthat Point-to-Plane offers significantly more stable and accurate registration,\nparticularly in areas with sparse features or dense vegetation. This study\nprovides a structured dataset for evaluating short-term localization\nrobustness, a reproducible framework for analyzing scan-to-map alignment under\nnoise, and a comparative evaluation of ICP performance in evolving outdoor\nenvironments. Our analysis underscores how local geometry and environmental\nvariability affect localization success, offering insights for designing more\nresilient robotic systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u9ad8\u5206\u8fa8\u7387\u77ed\u671f\u591a\u65f6\u5e8f\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u81ea\u4e3b\u7cfb\u7edf\u5728\u52a8\u6001\u6237\u5916\u73af\u5883\u4e2d\u7684\u91cd\u5b9a\u4f4d\u6027\u80fd\uff0c\u6bd4\u8f83\u4e86\u4e24\u79cdICP\u7b97\u6cd5\u5728\u4e0d\u540c\u73af\u5883\u6761\u4ef6\u4e0b\u7684\u8868\u73b0\u3002", "motivation": "\u52a8\u6001\u6237\u5916\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u91cd\u5b9a\u4f4d\u4ecd\u662f\u4f9d\u8d563D\u6fc0\u5149\u96f7\u8fbe\u7684\u81ea\u4e3b\u7cfb\u7edf\u9762\u4e34\u7684\u5173\u952e\u6311\u6218\u3002\u867d\u7136\u957f\u671f\u5b9a\u4f4d\u5df2\u88ab\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u53d1\u751f\u5728\u6570\u5929\u6216\u6570\u5468\u5185\u7684\u77ed\u671f\u73af\u5883\u53d8\u5316\u5c3d\u7ba1\u5177\u6709\u5b9e\u9645\u610f\u4e49\uff0c\u5374\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u4ece2025\u5e742\u6708\u52304\u6708\u6bcf\u5468\u6536\u96c6\u7684\u9ad8\u5206\u8fa8\u7387\u77ed\u671f\u591a\u65f6\u5e8f\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u81ea\u7136\u548c\u534a\u57ce\u5e02\u73af\u5883\u3002\u6bcf\u4e2a\u4f1a\u8bdd\u5305\u62ec\u9ad8\u5bc6\u5ea6\u70b9\u4e91\u5730\u56fe\u3001360\u5ea6\u5168\u666f\u56fe\u50cf\u548c\u8f68\u8ff9\u6570\u636e\u3002\u4f7f\u7528\u4ece\u70b9\u4e91\u5730\u56fe\u5bfc\u51fa\u7684\u6295\u5f71\u6fc0\u5149\u96f7\u8fbe\u626b\u63cf\uff0c\u901a\u8fc7\u4f20\u611f\u5668\u7cbe\u786e\u906e\u6321\u5efa\u6a21\uff0c\u8bc4\u4f30\u4e24\u79cd\u8fed\u4ee3\u6700\u8fd1\u70b9(ICP)\u7b97\u6cd5\u53d8\u4f53\u7684\u5bf9\u9f50\u7cbe\u5ea6\u3002", "result": "Point-to-Plane ICP\u7b97\u6cd5\u6bd4Point-to-Point ICP\u63d0\u4f9b\u4e86\u663e\u8457\u66f4\u7a33\u5b9a\u548c\u51c6\u786e\u7684\u914d\u51c6\uff0c\u7279\u522b\u662f\u5728\u7a00\u758f\u7279\u5f81\u6216\u5bc6\u96c6\u690d\u88ab\u533a\u57df\u3002\u7814\u7a76\u63d0\u4f9b\u4e86\u7528\u4e8e\u8bc4\u4f30\u77ed\u671f\u5b9a\u4f4d\u9c81\u68d2\u6027\u7684\u7ed3\u6784\u5316\u6570\u636e\u96c6\uff0c\u4ee5\u53ca\u5728\u566a\u58f0\u6761\u4ef6\u4e0b\u5206\u6790\u626b\u63cf\u5230\u5730\u56fe\u5bf9\u9f50\u7684\u53ef\u91cd\u73b0\u6846\u67b6\u3002", "conclusion": "\u5206\u6790\u5f3a\u8c03\u4e86\u5c40\u90e8\u51e0\u4f55\u548c\u73af\u5883\u53d8\u5f02\u6027\u5982\u4f55\u5f71\u54cd\u5b9a\u4f4d\u6210\u529f\u7387\uff0c\u4e3a\u8bbe\u8ba1\u66f4\u5177\u5f39\u6027\u7684\u673a\u5668\u4eba\u7cfb\u7edf\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002\u8be5\u7814\u7a76\u4e3a\u8bc4\u4f30\u4e0d\u65ad\u53d8\u5316\u7684\u6237\u5916\u73af\u5883\u4e2dICP\u6027\u80fd\u63d0\u4f9b\u4e86\u6bd4\u8f83\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2507.17561", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.17561", "abs": "https://arxiv.org/abs/2507.17561", "authors": ["Lorenzo Vianello", "Matthew Short", "Julia Manczurowsky", "Emek Bar\u0131\u015f K\u00fc\u00e7\u00fcktabak", "Francesco Di Tommaso", "Alessia Noccaro", "Laura Bandini", "Shoshana Clark", "Alaina Fiorenza", "Francesca Lunardini", "Alberto Canton", "Marta Gandolla", "Alessandra L. G. Pedrocchi", "Emilia Ambrosini", "Manuel Murie-Fernandez", "Carmen B. Roman", "Jesus Tornero", "Natacha Leon", "Andrew Sawers", "Jim Patton", "Domenico Formica", "Nevio Luigi Tagliamonte", "Georg Rauter", "Kilian Baur", "Fabian Just", "Christopher J. Hasson", "Vesna D. Novak", "Jose L. Pons"], "title": "Robot-mediated physical Human-Human Interaction in Neurorehabilitation: a position paper", "comment": null, "summary": "Neurorehabilitation conventionally relies on the interaction between a\npatient and a physical therapist. Robotic systems can improve and enrich the\nphysical feedback provided to patients after neurological injury, but they\nunder-utilize the adaptability and clinical expertise of trained therapists. In\nthis position paper, we advocate for a novel approach that integrates the\ntherapist's clinical expertise and nuanced decision-making with the strength,\naccuracy, and repeatability of robotics: Robot-mediated physical Human-Human\nInteraction. This framework, which enables two individuals to physically\ninteract through robotic devices, has been studied across diverse research\ngroups and has recently emerged as a promising link between conventional manual\ntherapy and rehabilitation robotics, harmonizing the strengths of both\napproaches. This paper presents the rationale of a multidisciplinary\nteam-including engineers, doctors, and physical therapists-for conducting\nresearch that utilizes: a unified taxonomy to describe robot-mediated\nrehabilitation, a framework of interaction based on social psychology, and a\ntechnological approach that makes robotic systems seamless facilitators of\nnatural human-human interaction.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u795e\u7ecf\u5eb7\u590d\u65b9\u6cd5\u2014\u2014\u673a\u5668\u4eba\u4ecb\u5bfc\u7684\u4eba-\u4eba\u7269\u7406\u4ea4\u4e92\uff0c\u5c06\u6cbb\u7597\u5e08\u7684\u4e34\u5e8a\u4e13\u4e1a\u77e5\u8bc6\u4e0e\u673a\u5668\u4eba\u7684\u7cbe\u786e\u6027\u548c\u91cd\u590d\u6027\u76f8\u7ed3\u5408\uff0c\u4f5c\u4e3a\u4f20\u7edf\u624b\u6cd5\u6cbb\u7597\u548c\u5eb7\u590d\u673a\u5668\u4eba\u4e4b\u95f4\u7684\u6865\u6881\u3002", "motivation": "\u4f20\u7edf\u795e\u7ecf\u5eb7\u590d\u4f9d\u8d56\u60a3\u8005\u4e0e\u7269\u7406\u6cbb\u7597\u5e08\u7684\u76f4\u63a5\u4ea4\u4e92\uff0c\u800c\u73b0\u6709\u673a\u5668\u4eba\u7cfb\u7edf\u867d\u80fd\u6539\u5584\u7269\u7406\u53cd\u9988\uff0c\u4f46\u672a\u80fd\u5145\u5206\u5229\u7528\u8bad\u7ec3\u6709\u7d20\u7684\u6cbb\u7597\u5e08\u7684\u9002\u5e94\u6027\u548c\u4e34\u5e8a\u4e13\u4e1a\u77e5\u8bc6\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u5c06\u6cbb\u7597\u5e08\u7684\u4e34\u5e8a\u4e13\u957f\u4e0e\u673a\u5668\u4eba\u7684\u4f18\u52bf\u76f8\u7ed3\u5408\u3002", "method": "\u63d0\u51fa\u673a\u5668\u4eba\u4ecb\u5bfc\u7684\u4eba-\u4eba\u7269\u7406\u4ea4\u4e92\u6846\u67b6\uff0c\u4f7f\u4e24\u4e2a\u4e2a\u4f53\u80fd\u591f\u901a\u8fc7\u673a\u5668\u4eba\u8bbe\u5907\u8fdb\u884c\u7269\u7406\u4ea4\u4e92\u3002\u8be5\u65b9\u6cd5\u5305\u62ec\uff1a\u7edf\u4e00\u7684\u5206\u7c7b\u6cd5\u6765\u63cf\u8ff0\u673a\u5668\u4eba\u4ecb\u5bfc\u7684\u5eb7\u590d\u3001\u57fa\u4e8e\u793e\u4f1a\u5fc3\u7406\u5b66\u7684\u4ea4\u4e92\u6846\u67b6\uff0c\u4ee5\u53ca\u4f7f\u673a\u5668\u4eba\u7cfb\u7edf\u6210\u4e3a\u81ea\u7136\u4eba-\u4eba\u4ea4\u4e92\u65e0\u7f1d\u4fc3\u8fdb\u8005\u7684\u6280\u672f\u65b9\u6cd5\u3002", "result": "\u8be5\u6846\u67b6\u5df2\u5728\u4e0d\u540c\u7814\u7a76\u56e2\u961f\u4e2d\u5f97\u5230\u7814\u7a76\uff0c\u5e76\u4f5c\u4e3a\u4f20\u7edf\u624b\u6cd5\u6cbb\u7597\u548c\u5eb7\u590d\u673a\u5668\u4eba\u4e4b\u95f4\u7684\u6709\u524d\u9014\u7684\u8fde\u63a5\u65b9\u5f0f\u51fa\u73b0\uff0c\u534f\u8c03\u4e86\u4e24\u79cd\u65b9\u6cd5\u7684\u4f18\u52bf\u3002\u591a\u5b66\u79d1\u56e2\u961f\uff08\u5305\u62ec\u5de5\u7a0b\u5e08\u3001\u533b\u751f\u548c\u7269\u7406\u6cbb\u7597\u5e08\uff09\u4e3a\u6b64\u7814\u7a76\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002", "conclusion": "\u673a\u5668\u4eba\u4ecb\u5bfc\u7684\u4eba-\u4eba\u7269\u7406\u4ea4\u4e92\u4e3a\u795e\u7ecf\u5eb7\u590d\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u8303\u5f0f\uff0c\u80fd\u591f\u6574\u5408\u6cbb\u7597\u5e08\u7684\u4e34\u5e8a\u4e13\u4e1a\u77e5\u8bc6\u548c\u7ec6\u81f4\u51b3\u7b56\u80fd\u529b\u4e0e\u673a\u5668\u4eba\u7684\u529b\u91cf\u3001\u51c6\u786e\u6027\u548c\u91cd\u590d\u6027\uff0c\u4ee3\u8868\u4e86\u5eb7\u590d\u6280\u672f\u53d1\u5c55\u7684\u91cd\u8981\u65b9\u5411\u3002"}}
{"id": "2507.17572", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.17572", "abs": "https://arxiv.org/abs/2507.17572", "authors": ["Antoine Groudiev", "Fabian Schramm", "\u00c9lo\u00efse Berthier", "Justin Carpentier", "Frederike D\u00fcmbgen"], "title": "KernelSOS for Global Sampling-Based Optimal Control and Estimation via Semidefinite Programming", "comment": null, "summary": "Global optimization has gained attraction over the past decades, thanks to\nthe development of both theoretical foundations and efficient numerical\nroutines to cope with optimization problems of various complexities. Among\nrecent methods, Kernel Sum of Squares (KernelSOS) appears as a powerful\nframework, leveraging the potential of sum of squares methods from the\npolynomial optimization community with the expressivity of kernel methods\nwidely used in machine learning. This paper applies the kernel sum of squares\nframework for solving control and estimation problems, which exhibit poor local\nminima. We demonstrate that KernelSOS performs well on a selection of problems\nfrom both domains. In particular, we show that KernelSOS is competitive with\nother sum of squares approaches on estimation problems, while being applicable\nto non-polynomial and non-parametric formulations. The sample-based nature of\nKernelSOS allows us to apply it to trajectory optimization problems with an\nintegrated simulator treated as a black box, both as a standalone method and as\na powerful initialization method for local solvers, facilitating the discovery\nof better solutions.", "AI": {"tldr": "\u672c\u6587\u5c06\u6838\u5e73\u65b9\u548c(KernelSOS)\u6846\u67b6\u5e94\u7528\u4e8e\u63a7\u5236\u548c\u4f30\u8ba1\u95ee\u9898\u7684\u5168\u5c40\u4f18\u5316\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5904\u7406\u5177\u6709\u4e0d\u826f\u5c40\u90e8\u6700\u5c0f\u503c\u95ee\u9898\u4e0a\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u5728\u8f68\u8ff9\u4f18\u5316\u4e2d\u53ef\u4f5c\u4e3a\u72ec\u7acb\u65b9\u6cd5\u6216\u5c40\u90e8\u6c42\u89e3\u5668\u7684\u521d\u59cb\u5316\u65b9\u6cd5\u4f7f\u7528\u3002", "motivation": "\u63a7\u5236\u548c\u4f30\u8ba1\u95ee\u9898\u7ecf\u5e38\u9047\u5230\u4e0d\u826f\u5c40\u90e8\u6700\u5c0f\u503c\u7684\u56f0\u6270\uff0c\u4f20\u7edf\u4f18\u5316\u65b9\u6cd5\u5bb9\u6613\u9677\u5165\u5c40\u90e8\u6700\u4f18\u89e3\u3002\u867d\u7136\u5168\u5c40\u4f18\u5316\u7406\u8bba\u548c\u6570\u503c\u65b9\u6cd5\u5df2\u6709\u53d1\u5c55\uff0c\u4f46\u9700\u8981\u4e00\u4e2a\u80fd\u591f\u7ed3\u5408\u591a\u9879\u5f0f\u4f18\u5316\u4e2d\u5e73\u65b9\u548c\u65b9\u6cd5\u7684\u7406\u8bba\u57fa\u7840\u4e0e\u673a\u5668\u5b66\u4e60\u4e2d\u6838\u65b9\u6cd5\u8868\u8fbe\u80fd\u529b\u7684\u5f3a\u5927\u6846\u67b6\u6765\u89e3\u51b3\u8fd9\u7c7b\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6838\u5e73\u65b9\u548c(KernelSOS)\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u4e2a\u7ed3\u5408\u4e86\u591a\u9879\u5f0f\u4f18\u5316\u793e\u533a\u5e73\u65b9\u548c\u65b9\u6cd5\u6f5c\u529b\u4e0e\u673a\u5668\u5b66\u4e60\u4e2d\u5e7f\u6cdb\u4f7f\u7528\u7684\u6838\u65b9\u6cd5\u8868\u8fbe\u80fd\u529b\u7684\u4f18\u5316\u6846\u67b6\u3002\u8be5\u65b9\u6cd5\u57fa\u4e8e\u6837\u672c\uff0c\u53ef\u4ee5\u5e94\u7528\u4e8e\u975e\u591a\u9879\u5f0f\u548c\u975e\u53c2\u6570\u5316\u7684\u95ee\u9898\u8868\u8ff0\uff0c\u5e76\u80fd\u591f\u5904\u7406\u5c06\u96c6\u6210\u6a21\u62df\u5668\u89c6\u4e3a\u9ed1\u76d2\u7684\u8f68\u8ff9\u4f18\u5316\u95ee\u9898\u3002", "result": "KernelSOS\u5728\u63a7\u5236\u548c\u4f30\u8ba1\u9886\u57df\u7684\u591a\u4e2a\u95ee\u9898\u4e0a\u8868\u73b0\u826f\u597d\u3002\u5728\u4f30\u8ba1\u95ee\u9898\u4e0a\uff0c\u8be5\u65b9\u6cd5\u4e0e\u5176\u4ed6\u5e73\u65b9\u548c\u65b9\u6cd5\u5177\u6709\u7ade\u4e89\u529b\uff0c\u540c\u65f6\u9002\u7528\u4e8e\u975e\u591a\u9879\u5f0f\u548c\u975e\u53c2\u6570\u5316\u95ee\u9898\u3002\u5728\u8f68\u8ff9\u4f18\u5316\u95ee\u9898\u4e2d\uff0cKernelSOS\u65e2\u53ef\u4ee5\u4f5c\u4e3a\u72ec\u7acb\u65b9\u6cd5\u4f7f\u7528\uff0c\u4e5f\u53ef\u4ee5\u4f5c\u4e3a\u5c40\u90e8\u6c42\u89e3\u5668\u7684\u5f3a\u5927\u521d\u59cb\u5316\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u53d1\u73b0\u66f4\u597d\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u6838\u5e73\u65b9\u548c\u6846\u67b6\u4e3a\u89e3\u51b3\u63a7\u5236\u548c\u4f30\u8ba1\u4e2d\u7684\u5168\u5c40\u4f18\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u5de5\u5177\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u5177\u6709\u4e0d\u826f\u5c40\u90e8\u6700\u5c0f\u503c\u7684\u590d\u6742\u4f18\u5316\u95ee\u9898\u65f6\u3002\u8be5\u65b9\u6cd5\u7684\u6837\u672c\u5316\u7279\u6027\u548c\u5bf9\u975e\u591a\u9879\u5f0f\u95ee\u9898\u7684\u9002\u7528\u6027\u4f7f\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5177\u6709\u5e7f\u9614\u524d\u666f\uff0c\u65e2\u53ef\u72ec\u7acb\u4f7f\u7528\u4e5f\u53ef\u4e0e\u73b0\u6709\u5c40\u90e8\u4f18\u5316\u65b9\u6cd5\u7ed3\u5408\u4f7f\u7528\u4ee5\u83b7\u5f97\u66f4\u597d\u7684\u4f18\u5316\u7ed3\u679c\u3002"}}
{"id": "2507.17649", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.17649", "abs": "https://arxiv.org/abs/2507.17649", "authors": ["J. D. Clark", "P. Ellison"], "title": "Event Detection for Active Lower Limb Prosthesis", "comment": null, "summary": "Accurate event detection is key to the successful design of semi-passive and\npowered prosthetics. Kinematically, the natural knee is complex, with\ntranslation and rotation components that have a substantial impact on gait\ncharacteristics. When simplified to a pin joint, some of this behaviour is\nlost. This study investigates the role of cruciate ligament stretch in event\ndetection. A bicondylar knee design was used, constrained by analogues of the\nanterior and posterior cruciate ligaments. This offers the ability to\ncharacterize knee kinematics by the stretch of the ligaments. The ligament\nstretch was recorded using LVDTs parallel to the ligaments of the Russell knee\non a bent knee crutch. Which was used to capture data on a treadmill at 3\nspeeds. This study finds speed dependence within the stretch of the cruciate\nligaments, prominently around 5\\% and 80\\% of the gait cycle for the posterior\nand anterior. The cycle profile remains consistent with speed; therefore, other\nstatic events such as the turning point feature at around 90\\% and 95\\% of the\ncycle, for the posterior and anterior, respectively, could be used as a\npredictive precursor for initial contact. Likewise at 90\\% and 95\\%, another\npair of turning points that in this case could be used to predict foot flat.\nThis concludes that the use of a bicondylar knee design could improve the\ndetection of events during the gait cycle, and therefore could increase the\naccuracy of subsequent controllers for powered prosthetics.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22\u4e86\u5728\u5047\u80a2\u819d\u5173\u8282\u8bbe\u8ba1\u4e2d\u4f7f\u7528\u53cc\u9ac1\u819d\u5173\u8282\u548c\u5341\u5b57\u97e7\u5e26\u62c9\u4f38\u6765\u6539\u5584\u6b65\u6001\u4e8b\u4ef6\u68c0\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u53d1\u73b0\u97e7\u5e26\u62c9\u4f38\u6a21\u5f0f\u53ef\u4ee5\u4f5c\u4e3a\u9884\u6d4b\u6b65\u6001\u5173\u952e\u65f6\u523b\u7684\u6709\u6548\u6307\u6807\u3002", "motivation": "\u51c6\u786e\u7684\u4e8b\u4ef6\u68c0\u6d4b\u662f\u534a\u88ab\u52a8\u548c\u52a8\u529b\u5047\u80a2\u6210\u529f\u8bbe\u8ba1\u7684\u5173\u952e\u3002\u4f20\u7edf\u7684\u9500\u9489\u5173\u8282\u7b80\u5316\u8bbe\u8ba1\u4f1a\u4e22\u5931\u819d\u5173\u8282\u7684\u590d\u6742\u8fd0\u52a8\u5b66\u884c\u4e3a\uff08\u5305\u62ec\u5e73\u79fb\u548c\u65cb\u8f6c\uff09\uff0c\u5f71\u54cd\u6b65\u6001\u7279\u5f81\u3002\u56e0\u6b64\u9700\u8981\u7814\u7a76\u5341\u5b57\u97e7\u5e26\u62c9\u4f38\u5728\u4e8b\u4ef6\u68c0\u6d4b\u4e2d\u7684\u4f5c\u7528\u3002", "method": "\u4f7f\u7528\u53cc\u9ac1\u819d\u5173\u8282\u8bbe\u8ba1\uff0c\u7531\u524d\u540e\u5341\u5b57\u97e7\u5e26\u7c7b\u4f3c\u7269\u7ea6\u675f\u3002\u901a\u8fc7\u4e0e\u97e7\u5e26\u5e73\u884c\u7684LVDT\u4f20\u611f\u5668\u8bb0\u5f55Russell\u819d\u5173\u8282\u7684\u97e7\u5e26\u62c9\u4f38\u60c5\u51b5\u3002\u5728\u8dd1\u6b65\u673a\u4e0a\u4ee53\u79cd\u4e0d\u540c\u901f\u5ea6\u91c7\u96c6\u6570\u636e\uff0c\u901a\u8fc7\u97e7\u5e26\u62c9\u4f38\u6765\u8868\u5f81\u819d\u5173\u8282\u8fd0\u52a8\u5b66\u7279\u6027\u3002", "result": "\u53d1\u73b0\u5341\u5b57\u97e7\u5e26\u62c9\u4f38\u5b58\u5728\u901f\u5ea6\u4f9d\u8d56\u6027\uff0c\u4e3b\u8981\u51fa\u73b0\u5728\u6b65\u6001\u5468\u671f\u76845%\u548c80%\uff08\u540e\u5341\u5b57\u97e7\u5e26\u548c\u524d\u5341\u5b57\u97e7\u5e26\uff09\u3002\u5faa\u73af\u8f6e\u5ed3\u968f\u901f\u5ea6\u4fdd\u6301\u4e00\u81f4\uff0c\u572890%\u548c95%\u5904\u7684\u8f6c\u6298\u70b9\u7279\u5f81\u53ef\u4f5c\u4e3a\u521d\u59cb\u63a5\u89e6\u7684\u9884\u6d4b\u524d\u5146\uff0c\u540c\u6837\u7684\u8f6c\u6298\u70b9\u53ef\u7528\u4e8e\u9884\u6d4b\u8db3\u5e95\u5e73\u653e\u3002", "conclusion": "\u53cc\u9ac1\u819d\u5173\u8282\u8bbe\u8ba1\u7684\u4f7f\u7528\u53ef\u4ee5\u6539\u5584\u6b65\u6001\u5468\u671f\u4e2d\u4e8b\u4ef6\u7684\u68c0\u6d4b\uff0c\u4ece\u800c\u63d0\u9ad8\u52a8\u529b\u5047\u80a2\u540e\u7eed\u63a7\u5236\u5668\u7684\u51c6\u786e\u6027\u3002\u97e7\u5e26\u62c9\u4f38\u6a21\u5f0f\u4e3a\u5047\u80a2\u63a7\u5236\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u751f\u7269\u529b\u5b66\u53cd\u9988\u673a\u5236\u3002"}}
{"id": "2507.17679", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.17679", "abs": "https://arxiv.org/abs/2507.17679", "authors": ["Theodoros Tavoulareas", "Marzia Cescon"], "title": "Safety Assurance for Quadrotor Kinodynamic Motion Planning", "comment": "Accepted for publication at 2025 Modeling, Estimation and Control\n  Conference (MECC)", "summary": "Autonomous drones have gained considerable attention for applications in\nreal-world scenarios, such as search and rescue, inspection, and delivery. As\ntheir use becomes ever more pervasive in civilian applications, failure to\nensure safe operation can lead to physical damage to the system, environmental\npollution, and even loss of human life. Recent work has demonstrated that\nmotion planning techniques effectively generate a collision-free trajectory\nduring navigation. However, these methods, while creating the motion plans, do\nnot inherently consider the safe operational region of the system, leading to\npotential safety constraints violation during deployment. In this paper, we\npropose a method that leverages run time safety assurance in a kinodynamic\nmotion planning scheme to satisfy the system's operational constraints. First,\nwe use a sampling-based geometric planner to determine a high-level\ncollision-free path within a user-defined space. Second, we design a low-level\nsafety assurance filter to provide safety guarantees to the control input of a\nLinear Quadratic Regulator (LQR) designed with the purpose of trajectory\ntracking. We demonstrate our proposed approach in a restricted 3D simulation\nenvironment using a model of the Crazyflie 2.0 drone.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8fd0\u884c\u65f6\u5b89\u5168\u4fdd\u969c\u7684\u65e0\u4eba\u673a\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\uff0c\u901a\u8fc7\u91c7\u6837\u51e0\u4f55\u89c4\u5212\u5668\u751f\u6210\u65e0\u78b0\u649e\u8def\u5f84\uff0c\u5e76\u8bbe\u8ba1\u4f4e\u5c42\u5b89\u5168\u4fdd\u969c\u6ee4\u6ce2\u5668\u786e\u4fdd\u63a7\u5236\u8f93\u5165\u6ee1\u8db3\u7cfb\u7edf\u8fd0\u884c\u7ea6\u675f\uff0c\u5728Crazyflie 2.0\u65e0\u4eba\u673a\u4eff\u771f\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u8fd0\u52a8\u89c4\u5212\u6280\u672f\u867d\u80fd\u751f\u6210\u65e0\u78b0\u649e\u8f68\u8ff9\uff0c\u4f46\u5728\u89c4\u5212\u8fc7\u7a0b\u4e2d\u672a\u8003\u8651\u7cfb\u7edf\u7684\u5b89\u5168\u8fd0\u884c\u533a\u57df\uff0c\u53ef\u80fd\u5bfc\u81f4\u90e8\u7f72\u65f6\u8fdd\u53cd\u5b89\u5168\u7ea6\u675f\uff0c\u8fdb\u800c\u9020\u6210\u7cfb\u7edf\u635f\u574f\u3001\u73af\u5883\u6c61\u67d3\u751a\u81f3\u4eba\u5458\u4f24\u4ea1\u3002\u968f\u7740\u81ea\u4e3b\u65e0\u4eba\u673a\u5728\u641c\u6551\u3001\u68c0\u67e5\u3001\u914d\u9001\u7b49\u6c11\u7528\u573a\u666f\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u786e\u4fdd\u5b89\u5168\u8fd0\u884c\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u5c42\u8fd0\u52a8\u89c4\u5212\u67b6\u6784\uff1a1\uff09\u9ad8\u5c42\u4f7f\u7528\u57fa\u4e8e\u91c7\u6837\u7684\u51e0\u4f55\u89c4\u5212\u5668\u5728\u7528\u6237\u5b9a\u4e49\u7a7a\u95f4\u5185\u786e\u5b9a\u65e0\u78b0\u649e\u8def\u5f84\uff1b2\uff09\u4f4e\u5c42\u8bbe\u8ba1\u5b89\u5168\u4fdd\u969c\u6ee4\u6ce2\u5668\uff0c\u4e3a\u7528\u4e8e\u8f68\u8ff9\u8ddf\u8e2a\u7684\u7ebf\u6027\u4e8c\u6b21\u8c03\u8282\u5668(LQR)\u63a7\u5236\u8f93\u5165\u63d0\u4f9b\u5b89\u5168\u4fdd\u8bc1\uff0c\u786e\u4fdd\u6ee1\u8db3\u7cfb\u7edf\u8fd0\u884c\u7ea6\u675f\u3002\u8be5\u65b9\u6cd5\u5c06\u8fd0\u884c\u65f6\u5b89\u5168\u4fdd\u969c\u878d\u5165\u5230\u52a8\u529b\u5b66\u8fd0\u52a8\u89c4\u5212\u65b9\u6848\u4e2d\u3002", "result": "\u5728\u9650\u5236\u60273D\u4eff\u771f\u73af\u5883\u4e2d\u4f7f\u7528Crazyflie 2.0\u65e0\u4eba\u673a\u6a21\u578b\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u4fdd\u8bc1\u65e0\u78b0\u649e\u5bfc\u822a\u7684\u540c\u65f6\uff0c\u6ee1\u8db3\u7cfb\u7edf\u7684\u5b89\u5168\u8fd0\u884c\u7ea6\u675f\u3002", "conclusion": "\u901a\u8fc7\u5c06\u8fd0\u884c\u65f6\u5b89\u5168\u4fdd\u969c\u673a\u5236\u96c6\u6210\u5230\u8fd0\u52a8\u89c4\u5212\u6846\u67b6\u4e2d\uff0c\u8be5\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u4f20\u7edf\u8fd0\u52a8\u89c4\u5212\u6280\u672f\u5ffd\u89c6\u7cfb\u7edf\u5b89\u5168\u8fd0\u884c\u533a\u57df\u7684\u95ee\u9898\u3002\u53cc\u5c42\u67b6\u6784\u8bbe\u8ba1\u65e2\u4fdd\u8bc1\u4e86\u8def\u5f84\u7684\u65e0\u78b0\u649e\u6027\uff0c\u53c8\u786e\u4fdd\u4e86\u63a7\u5236\u8f93\u5165\u6ee1\u8db3\u5b89\u5168\u7ea6\u675f\uff0c\u4e3a\u81ea\u4e3b\u65e0\u4eba\u673a\u7684\u5b89\u5168\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.17727", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.17727", "abs": "https://arxiv.org/abs/2507.17727", "authors": ["Robel Mamo", "Taeyeong Choi"], "title": "CA-Cut: Crop-Aligned Cutout for Data Augmentation to Learn More Robust Under-Canopy Navigation", "comment": "Accepted for publication at the 12th European Conference on Mobile\n  Robots (ECMR 2025)", "summary": "State-of-the-art visual under-canopy navigation methods are designed with\ndeep learning-based perception models to distinguish traversable space from\ncrop rows. While these models have demonstrated successful performance, they\nrequire large amounts of training data to ensure reliability in real-world\nfield deployment. However, data collection is costly, demanding significant\nhuman resources for in-field sampling and annotation. To address this\nchallenge, various data augmentation techniques are commonly employed during\nmodel training, such as color jittering, Gaussian blur, and horizontal flip, to\ndiversify training data and enhance model robustness. In this paper, we\nhypothesize that utilizing only these augmentation techniques may lead to\nsuboptimal performance, particularly in complex under-canopy environments with\nfrequent occlusions, debris, and non-uniform spacing of crops. Instead, we\npropose a novel augmentation method, so-called Crop-Aligned Cutout (CA-Cut)\nwhich masks random regions out in input images that are spatially distributed\naround crop rows on the sides to encourage trained models to capture high-level\ncontextual features even when fine-grained information is obstructed. Our\nextensive experiments with a public cornfield dataset demonstrate that\nmasking-based augmentations are effective for simulating occlusions and\nsignificantly improving robustness in semantic keypoint predictions for visual\nnavigation. In particular, we show that biasing the mask distribution toward\ncrop rows in CA-Cut is critical for enhancing both prediction accuracy and\ngeneralizability across diverse environments achieving up to a 36.9% reduction\nin prediction error. In addition, we conduct ablation studies to determine the\nnumber of masks, the size of each mask, and the spatial distribution of masks\nto maximize overall performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCrop-Aligned Cutout (CA-Cut)\u7684\u65b0\u578b\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u4f5c\u7269\u884c\u5468\u56f4\u7a7a\u95f4\u5206\u5e03\u5730\u906e\u853d\u968f\u673a\u533a\u57df\u6765\u6539\u5584\u519c\u4e1a\u89c6\u89c9\u5bfc\u822a\u6a21\u578b\u7684\u9c81\u68d2\u6027\uff0c\u5728\u7389\u7c73\u7530\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u8fbe36.9%\u7684\u9884\u6d4b\u8bef\u5dee\u964d\u4f4e\u3002", "motivation": "\u73b0\u6709\u7684\u519c\u4e1a\u89c6\u89c9\u5bfc\u822a\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u8bad\u7ec3\u6570\u636e\uff0c\u800c\u6570\u636e\u6536\u96c6\u6210\u672c\u9ad8\u6602\u3002\u4f20\u7edf\u7684\u6570\u636e\u589e\u5f3a\u6280\u672f\uff08\u5982\u989c\u8272\u6296\u52a8\u3001\u9ad8\u65af\u6a21\u7cca\u7b49\uff09\u5728\u590d\u6742\u7684\u4f5c\u7269\u8986\u76d6\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u7279\u522b\u662f\u5728\u9762\u4e34\u9891\u7e41\u906e\u6321\u3001\u6742\u7269\u548c\u4f5c\u7269\u95f4\u8ddd\u4e0d\u5747\u5300\u7684\u60c5\u51b5\u4e0b\uff0c\u53ef\u80fd\u5bfc\u81f4\u6027\u80fd\u6b21\u4f18\u3002", "method": "\u63d0\u51fa\u4e86Crop-Aligned Cutout (CA-Cut)\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u8f93\u5165\u56fe\u50cf\u4e2d\u6cbf\u4f5c\u7269\u884c\u4e24\u4fa7\u7a7a\u95f4\u5206\u5e03\u5730\u906e\u853d\u968f\u673a\u533a\u57df\uff0c\u8feb\u4f7f\u8bad\u7ec3\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u4fe1\u606f\u88ab\u906e\u6321\u65f6\u4ecd\u80fd\u6355\u83b7\u9ad8\u7ea7\u4e0a\u4e0b\u6587\u7279\u5f81\u3002\u901a\u8fc7\u6d88\u878d\u7814\u7a76\u786e\u5b9a\u4e86\u906e\u7f69\u6570\u91cf\u3001\u6bcf\u4e2a\u906e\u7f69\u7684\u5927\u5c0f\u4ee5\u53ca\u906e\u7f69\u7684\u7a7a\u95f4\u5206\u5e03\u7b49\u5173\u952e\u53c2\u6570\u3002", "result": "\u5728\u516c\u5f00\u7389\u7c73\u7530\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u57fa\u4e8e\u906e\u7f69\u7684\u589e\u5f3a\u65b9\u6cd5\u80fd\u6709\u6548\u6a21\u62df\u906e\u6321\u60c5\u51b5\uff0c\u663e\u8457\u63d0\u9ad8\u89c6\u89c9\u5bfc\u822a\u4e2d\u8bed\u4e49\u5173\u952e\u70b9\u9884\u6d4b\u7684\u9c81\u68d2\u6027\u3002CA-Cut\u65b9\u6cd5\u5728\u9884\u6d4b\u51c6\u786e\u6027\u548c\u8de8\u73af\u5883\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u90fd\u6709\u663e\u8457\u63d0\u5347\uff0c\u9884\u6d4b\u8bef\u5dee\u6700\u591a\u964d\u4f4e\u4e8636.9%\u3002", "conclusion": "\u5c06\u906e\u7f69\u5206\u5e03\u504f\u5411\u4f5c\u7269\u884c\u7684CA-Cut\u65b9\u6cd5\u5bf9\u4e8e\u589e\u5f3a\u9884\u6d4b\u51c6\u786e\u6027\u548c\u8de8\u4e0d\u540c\u73af\u5883\u7684\u6cdb\u5316\u80fd\u529b\u81f3\u5173\u91cd\u8981\u3002\u8fd9\u79cd\u65b0\u9896\u7684\u6570\u636e\u589e\u5f3a\u7b56\u7565\u4e3a\u519c\u4e1a\u89c6\u89c9\u5bfc\u822a\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u590d\u6742\u7684\u4f5c\u7269\u8986\u76d6\u73af\u5883\u65f6\u8868\u73b0\u51fa\u8272\u3002"}}
