<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 21]
- [cs.AI](#cs.AI) [Total: 20]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [Hard-Stop Synthesis for Multi-DOF Compliant Mechanisms](https://arxiv.org/abs/2507.13455)
*Dean Chen,Armin Pomeroy,Brandon T. Peterson,Will Flanagan,He Kai Lim,Alexandra Stavrakis,Nelson F. SooHoo,Jonathan B. Hopkins,Tyler R. Clites*

Main category: cs.RO

TL;DR: 提出了一种系统化的设计方法，通过集成多自由度运动限制的紧凑硬止挡表面，为柔性机构提供过载保护，确保其在弹性范围内工作。


<details>
  <summary>Details</summary>
Motivation: 柔性机构在精密应用中潜力巨大，但其疲劳和机械故障的脆弱性阻碍了实际应用。复杂和不确定的负载环境尤其需要硬止挡来防止屈服和屈曲。

Method: 引入理论和实践框架，优化接触表面几何形状，以最大化机构的多自由度工作空间，同时确保其保持在弹性范围内。

Result: 通过数值和实验验证，该方法在骨科植入物的笼式铰链机构中提供了可靠的疲劳、屈服和屈曲保护。

Conclusion: 为在不确定负载下工作的柔性系统提供了精密硬止挡设计的基础，推动了柔性机构在实际系统中的应用。

Abstract: Compliant mechanisms have significant potential in precision applications due
to their ability to guide motion without contact. However, an inherent
vulnerability to fatigue and mechanical failure has hindered the translation of
compliant mechanisms to real-world applications. This is particularly
challenging in service environments where loading is complex and uncertain, and
the cost of failure is high. In such cases, mechanical hard stops are critical
to prevent yielding and buckling. Conventional hard-stop designs, which rely on
stacking single-DOF limits, must be overly restrictive in multi-DOF space to
guarantee safety in the presence of unknown loads. In this study, we present a
systematic design synthesis method to guarantee overload protection in
compliant mechanisms by integrating coupled multi-DOF motion limits within a
single pair of compact hard-stop surfaces. Specifically, we introduce a
theoretical and practical framework for optimizing the contact surface geometry
to maximize the mechanisms multi-DOF working space while still ensuring that
the mechanism remains within its elastic regime. We apply this synthesis method
to a case study of a caged-hinge mechanism for orthopaedic implants, and
provide numerical and experimental validation that the derived design offers
reliable protection against fatigue, yielding, and buckling. This work
establishes a foundation for precision hard-stop design in compliant systems
operating under uncertain loads, which is a crucial step toward enabling the
application of compliant mechanisms in real-world systems.

</details>


### [2] [ERR@HRI 2.0 Challenge: Multimodal Detection of Errors and Failures in Human-Robot Conversations](https://arxiv.org/abs/2507.13468)
*Shiye Cao,Maia Stiber,Amama Mahmood,Maria Teresa Parreira,Wendy Ju,Micol Spitale,Hatice Gunes,Chien-Ming Huang*

Main category: cs.RO

TL;DR: 论文探讨了LLM驱动的对话机器人在人机交互中的错误检测问题，并提出了一个多模态数据集（ERR@HRI 2.0 Challenge）用于模型测试。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM使对话机器人更动态，但其仍易出错（如误解用户意图或中断对话），需检测和解决这些错误以维持用户信任。

Method: 通过提供包含16小时人机交互的多模态数据集（含面部、语音和头部动作特征），鼓励研究者开发机器学习模型检测机器人错误。

Result: 数据集标注了机器人错误和用户意图修正，模型将基于检测准确率和误报率等指标评估。

Conclusion: 该挑战是改进人机交互中错误检测的关键一步，通过社交信号分析提升机器人性能。

Abstract: The integration of large language models (LLMs) into conversational robots
has made human-robot conversations more dynamic. Yet, LLM-powered
conversational robots remain prone to errors, e.g., misunderstanding user
intent, prematurely interrupting users, or failing to respond altogether.
Detecting and addressing these failures is critical for preventing
conversational breakdowns, avoiding task disruptions, and sustaining user
trust. To tackle this problem, the ERR@HRI 2.0 Challenge provides a multimodal
dataset of LLM-powered conversational robot failures during human-robot
conversations and encourages researchers to benchmark machine learning models
designed to detect robot failures. The dataset includes 16 hours of dyadic
human-robot interactions, incorporating facial, speech, and head movement
features. Each interaction is annotated with the presence or absence of robot
errors from the system perspective, and perceived user intention to correct for
a mismatch between robot behavior and user expectation. Participants are
invited to form teams and develop machine learning models that detect these
failures using multimodal data. Submissions will be evaluated using various
performance metrics, including detection accuracy and false positive rate. This
challenge represents another key step toward improving failure detection in
human-robot interaction through social signal analysis.

</details>


### [3] [SCOPE for Hexapod Gait Generation](https://arxiv.org/abs/2507.13539)
*Jim O'Connor,Jay B. Nash,Derin Gezgin,Gary B. Parker*

Main category: cs.RO

TL;DR: SCOPE方法通过离散余弦变换（DCT）压缩输入数据，显著提升六足机器人步态学习效率。


<details>
  <summary>Details</summary>
Motivation: 随着输入空间复杂度增加，进化算法性能急剧下降，需解决高维输入问题。

Method: 利用DCT提取输入矩阵特征系数，截断系数矩阵以降低维度，保留高能量特征。

Result: SCOPE将输入数据从2700压缩至54，提升算法效率20%。

Conclusion: SCOPE能显著压缩输入数据，提升控制器性能，具有广泛应用潜力。

Abstract: Evolutionary methods have previously been shown to be an effective learning
method for walking gaits on hexapod robots. However, the ability of these
algorithms to evolve an effective policy rapidly degrades as the input space
becomes more complex. This degradation is due to the exponential growth of the
solution space, resulting from an increasing parameter count to handle a more
complex input. In order to address this challenge, we introduce Sparse Cosine
Optimized Policy Evolution (SCOPE). SCOPE utilizes the Discrete Cosine
Transform (DCT) to learn directly from the feature coefficients of an input
matrix. By truncating the coefficient matrix returned by the DCT, we can reduce
the dimensionality of an input while retaining the highest energy features of
the original input. We demonstrate the effectiveness of this method by using
SCOPE to learn the gait of a hexapod robot. The hexapod controller is given a
matrix input containing time-series information of previous poses, which are
then transformed to gait parameters by an evolved policy. In this task, the
addition of SCOPE to a reference algorithm achieves a 20% increase in efficacy.
SCOPE achieves this result by reducing the total input size of the time-series
pose data from 2700 to 54, a 98% decrease. Additionally, SCOPE is capable of
compressing an input to any output shape, provided that each output dimension
is no greater than the corresponding input dimension. This paper demonstrates
that SCOPE is capable of significantly compressing the size of an input to an
evolved controller, resulting in a statistically significant gain in efficacy.

</details>


### [4] [Improving Low-Cost Teleoperation: Augmenting GELLO with Force](https://arxiv.org/abs/2507.13602)
*Shivakanth Sujit,Luca Nunziante,Dan Ogawa Lillrank,Rousslan Fernand Julien Dossa,Kai Arulkumaran*

Main category: cs.RO

TL;DR: 扩展了低成本GELLO遥操作系统，增加了力反馈和力信息数据收集，验证了力信息对任务成功率的提升。


<details>
  <summary>Details</summary>
Motivation: 提升遥操作系统的用户体验和任务性能，通过力反馈和力信息增强模仿学习模型的训练效果。

Method: 在GELLO系统中实现力反馈，并将力信息加入数据收集和模仿学习模型训练；通过用户研究和任务性能对比验证改进。

Result: 有机器人经验的用户偏好新控制器，力信息显著提高了多数任务的成功率。

Conclusion: 力反馈和力信息的加入有效提升了遥操作系统的性能和用户体验。

Abstract: In this work we extend the low-cost GELLO teleoperation system, initially
designed for joint position control, with additional force information. Our
first extension is to implement force feedback, allowing users to feel
resistance when interacting with the environment. Our second extension is to
add force information into the data collection process and training of
imitation learning models. We validate our additions by implementing these on a
GELLO system with a Franka Panda arm as the follower robot, performing a user
study, and comparing the performance of policies trained with and without force
information on a range of simulated and real dexterous manipulation tasks.
Qualitatively, users with robotics experience preferred our controller, and the
addition of force inputs improved task success on the majority of tasks.

</details>


### [5] [Improved particle swarm optimization algorithm: multi-target trajectory optimization for swarm drones](https://arxiv.org/abs/2507.13647)
*Minze Li,Wei Zhao,Ran Chen,Mingqiang Wei*

Main category: cs.RO

TL;DR: 提出了一种改进的PSO方法（PE-PSO），用于无人机在动态环境中的实时轨迹规划，通过持久探索机制和熵参数调整提升性能，并结合B样条曲线和多智能体框架实现高效协同。


<details>
  <summary>Details</summary>
Motivation: 传统PSO方法在实时场景中存在过早收敛和延迟问题，无法满足无人机在动态环境中的快速响应需求。

Method: 引入持久探索机制和熵参数调整策略，结合B样条曲线建模轨迹，并开发多智能体框架（结合GA任务分配和分布式PE-PSO）。

Result: 仿真表明，PE-PSO在轨迹质量、能效、避障和计算时间上优于传统PSO和其他群智能方法。

Conclusion: PE-PSO适用于复杂环境下的实时多无人机协同操作，具有高效性和可扩展性。

Abstract: Real-time trajectory planning for unmanned aerial vehicles (UAVs) in dynamic
environments remains a key challenge due to high computational demands and the
need for fast, adaptive responses. Traditional Particle Swarm Optimization
(PSO) methods, while effective for offline planning, often struggle with
premature convergence and latency in real-time scenarios. To overcome these
limitations, we propose PE-PSO, an enhanced PSO-based online trajectory
planner. The method introduces a persistent exploration mechanism to preserve
swarm diversity and an entropy-based parameter adjustment strategy to
dynamically adapt optimization behavior. UAV trajectories are modeled using
B-spline curves, which ensure path smoothness while reducing optimization
complexity. To extend this capability to UAV swarms, we develop a multi-agent
framework that combines genetic algorithm (GA)-based task allocation with
distributed PE-PSO, supporting scalable and coordinated trajectory generation.
The distributed architecture allows for parallel computation and decentralized
control, enabling effective cooperation among agents while maintaining
real-time performance. Comprehensive simulations demonstrate that the proposed
framework outperforms conventional PSO and other swarm-based planners across
several metrics, including trajectory quality, energy efficiency, obstacle
avoidance, and computation time. These results confirm the effectiveness and
applicability of PE-PSO in real-time multi-UAV operations under complex
environmental conditions.

</details>


### [6] [Safe Robotic Capsule Cleaning with Integrated Transpupillary and Intraocular Optical Coherence Tomography](https://arxiv.org/abs/2507.13650)
*Yu-Ting Lai,Yasamin Foroutani,Aya Barzelay,Tsu-Chin Tsao*

Main category: cs.RO

TL;DR: 论文提出了一种机器人系统，用于白内障手术后囊膜清洁，通过集成光学相干断层扫描探头实现实时可视化与精确操作。


<details>
  <summary>Details</summary>
Motivation: 白内障手术后残留晶状体材料的增殖是视力丧失的常见并发症，传统手术需要高精度的可视化与操作。

Method: 开发了机器人系统，集成光学相干断层扫描探头，实现囊膜实时成像与工具-组织距离反馈，并通过机器人精度完成囊膜映射。

Result: 在眼模型实验中验证了囊膜映射策略的准确性，并在离体猪眼中成功实施清洁操作，未造成组织损伤。

Conclusion: 该系统为白内障术后囊膜清洁提供了一种高精度、安全的解决方案。

Abstract: Secondary cataract is one of the most common complications of vision loss due
to the proliferation of residual lens materials that naturally grow on the lens
capsule after cataract surgery. A potential treatment is capsule cleaning, a
surgical procedure that requires enhanced visualization of the entire capsule
and tool manipulation on the thin membrane. This article presents a robotic
system capable of performing the capsule cleaning procedure by integrating a
standard transpupillary and an intraocular optical coherence tomography probe
on a surgical instrument for equatorial capsule visualization and real-time
tool-to-tissue distance feedback. Using robot precision, the developed system
enables complete capsule mapping in the pupillary and equatorial regions with
in-situ calibration of refractive index and fiber offset, which are still
current challenges in obtaining an accurate capsule model. To demonstrate
effectiveness, the capsule mapping strategy was validated through five
experimental trials on an eye phantom that showed reduced root-mean-square
errors in the constructed capsule model, while the cleaning strategy was
performed in three ex-vivo pig eyes without tissue damage.

</details>


### [7] [A Study of Teleoperation Methods in a Simulated Virtual Eye Surgery Environment](https://arxiv.org/abs/2507.13654)
*Haoran Wang,Yasamin Foroutani,Matthew Nepo,Mercedes Rodriguez,Ji Ma,Jean-Pierre Hubschman,Tsu-Chin Tsao,Jacob Rosen*

Main category: cs.RO

TL;DR: 研究比较了不同缩放因子下Inside和Outside Control模式在模拟玻璃体视网膜手术中的表现，发现高缩放因子（20或30）的Inside Control模式表现最佳。


<details>
  <summary>Details</summary>
Motivation: 优化控制模式和缩放因子以提高机器人辅助眼内手术的效率和准确性，降低风险。

Method: 使用IRISS远程手术系统，将模拟显微镜视图投影到VR头显，由5名经验丰富的玻璃体视网膜外科医生和5名无手术经验的工程师执行任务。

Result: 高缩放因子的Inside Control模式表现最佳，但最优缩放因子因任务和复杂度而异。

Conclusion: 优化控制方法和缩放因子可提升手术效率和准确性，减少未来机器人辅助眼内手术的风险。

Abstract: This paper examines the performance of Inside and Outside Control modes at
various scaling factors in a simulated vitreoretinal surgical setting. The
IRISS teleoperated surgical system's console (cockpit) was adapted to project a
simulated microscope view of an intraocular setup to a virtual reality (VR)
headset. Five experienced vitreoretinal surgeons and five engineers with no
surgical experience used the system to perform tasks common to vitreoretinal
surgery. Experimental results indicate that Inside Control methods at higher
scaling factors (20 or 30) achieved the best performance overall, though the
optimal scaling factor may vary by task and complexity. Optimizing control
methods and scaling factors could lead to improvements in surgical efficiency
and accuracy, as well as minimize risks in future robotic-assisted intraocular
procedures.

</details>


### [8] [Iteratively Learning Muscle Memory for Legged Robots to Master Adaptive and High Precision Locomotion](https://arxiv.org/abs/2507.13662)
*Jing Cheng,Yasser G. Alqaham,Zhenyu Gan,Amit K. Sanyal*

Main category: cs.RO

TL;DR: 提出了一种结合迭代学习控制（ILC）和生物启发的扭矩库（TL）的可扩展自适应控制框架，用于提高腿式机器人在复杂环境中的运动性能。


<details>
  <summary>Details</summary>
Motivation: 解决腿式机器人在未建模动力学和外部干扰下的精确轨迹跟踪问题，同时提升其在非周期性任务中的泛化能力。

Method: 整合物理模型和实时学习，通过扭矩库存储控制配置文件，实现快速适应速度和地形变化。

Result: 在双足机器人Cassie和四足机器人A1上验证，关节跟踪误差减少85%，控制更新速率提升30倍。

Conclusion: ILC与扭矩库结合是一种高效且实用的腿式运动控制解决方案，适用于非结构化动态环境。

Abstract: This paper presents a scalable and adaptive control framework for legged
robots that integrates Iterative Learning Control (ILC) with a biologically
inspired torque library (TL), analogous to muscle memory. The proposed method
addresses key challenges in robotic locomotion, including accurate trajectory
tracking under unmodeled dynamics and external disturbances. By leveraging the
repetitive nature of periodic gaits and extending ILC to nonperiodic tasks, the
framework enhances accuracy and generalization across diverse locomotion
scenarios. The control architecture is data-enabled, combining a physics-based
model derived from hybrid-system trajectory optimization with real-time
learning to compensate for model uncertainties and external disturbances. A
central contribution is the development of a generalized TL that stores learned
control profiles and enables rapid adaptation to changes in speed, terrain, and
gravitational conditions-eliminating the need for repeated learning and
significantly reducing online computation. The approach is validated on the
bipedal robot Cassie and the quadrupedal robot A1 through extensive simulations
and hardware experiments. Results demonstrate that the proposed framework
reduces joint tracking errors by up to 85% within a few seconds and enables
reliable execution of both periodic and nonperiodic gaits, including slope
traversal and terrain adaptation. Compared to state-of-the-art whole-body
controllers, the learned skills eliminate the need for online computation
during execution and achieve control update rates exceeding 30x those of
existing methods. These findings highlight the effectiveness of integrating ILC
with torque memory as a highly data-efficient and practical solution for legged
locomotion in unstructured and dynamic environments.

</details>


### [9] [SaWa-ML: Structure-Aware Pose Correction and Weight Adaptation-Based Robust Multi-Robot Localization](https://arxiv.org/abs/2507.13702)
*Junho Choi,Kihwan Ryoo,Jeewon Kim,Taeyun Kim,Eungchang Lee,Myeongwoo Jeong,Kevin Christiansen Marsim,Hyungtae Lim,Hyun Myung*

Main category: cs.RO

TL;DR: 提出了一种名为SaWa-ML的新型视觉-惯性-距离多机器人定位方法，通过几何结构感知的位姿校正和权重自适应，显著减少长期漂移误差。


<details>
  <summary>Details</summary>
Motivation: 现有方法未充分考虑机器人个体里程计估计和距离测量的特性，且易受里程计精度影响，导致长期漂移误差不可避免。

Method: 利用UWB传感器数据估计机器人间相对位置并校正位姿，同时设计自适应权重以优化传感器数据和视觉-惯性里程计估计。

Result: 在真实实验中验证了性能提升，优于现有先进算法。

Conclusion: SaWa-ML通过结构感知和权重自适应，有效解决了多机器人定位中的长期漂移问题。

Abstract: Multi-robot localization is a crucial task for implementing multi-robot
systems. Numerous researchers have proposed optimization-based multi-robot
localization methods that use camera, IMU, and UWB sensors. Nevertheless,
characteristics of individual robot odometry estimates and distance
measurements between robots used in the optimization are not sufficiently
considered. In addition, previous researches were heavily influenced by the
odometry accuracy that is estimated from individual robots. Consequently,
long-term drift error caused by error accumulation is potentially inevitable.
In this paper, we propose a novel visual-inertial-range-based multi-robot
localization method, named SaWa-ML, which enables geometric structure-aware
pose correction and weight adaptation-based robust multi-robot localization.
Our contributions are twofold: (i) we leverage UWB sensor data, whose range
error does not accumulate over time, to first estimate the relative positions
between robots and then correct the positions of each robot, thus reducing
long-term drift errors, (ii) we design adaptive weights for robot pose
correction by considering the characteristics of the sensor data and
visual-inertial odometry estimates. The proposed method has been validated in
real-world experiments, showing a substantial performance increase compared
with state-of-the-art algorithms.

</details>


### [10] [AGENTS-LLM: Augmentative GENeration of Challenging Traffic Scenarios with an Agentic LLM Framework](https://arxiv.org/abs/2507.13729)
*Yu Yao,Salil Bhatnagar,Markus Mazzola,Vasileios Belagiannis,Igor Gilitschenski,Luigi Palmieri,Simon Razniewski,Marcel Hallgarten*

Main category: cs.RO

TL;DR: 论文提出了一种基于LLM-agent的框架，通过自然语言描述增强真实交通场景，解决了现有方法在生成高质量测试场景时的局限性。


<details>
  <summary>Details</summary>
Motivation: 测试自动驾驶规划器时，罕见但关键的场景难以捕捉，现有方法依赖大量数据或专家手动操作，无法满足规模化需求。

Method: 采用基于LLM-agent的设计，通过自然语言描述生成增强场景，实现对输出的细粒度控制。

Result: 人类专家评估表明，该框架能准确遵循用户意图，生成与手动创建场景质量相当的增强场景。

Conclusion: 该框架为自动驾驶系统评估提供了一种高效、可控的场景生成方法。

Abstract: Rare, yet critical, scenarios pose a significant challenge in testing and
evaluating autonomous driving planners. Relying solely on real-world driving
scenes requires collecting massive datasets to capture these scenarios. While
automatic generation of traffic scenarios appears promising, data-driven models
require extensive training data and often lack fine-grained control over the
output. Moreover, generating novel scenarios from scratch can introduce a
distributional shift from the original training scenes which undermines the
validity of evaluations especially for learning-based planners. To sidestep
this, recent work proposes to generate challenging scenarios by augmenting
original scenarios from the test set. However, this involves the manual
augmentation of scenarios by domain experts. An approach that is unable to meet
the demands for scale in the evaluation of self-driving systems. Therefore,
this paper introduces a novel LLM-agent based framework for augmenting
real-world traffic scenarios using natural language descriptions, addressing
the limitations of existing methods. A key innovation is the use of an agentic
design, enabling fine-grained control over the output and maintaining high
performance even with smaller, cost-effective LLMs. Extensive human expert
evaluation demonstrates our framework's ability to accurately adhere to user
intent, generating high quality augmented scenarios comparable to those created
manually.

</details>


### [11] [Design Analysis of an Innovative Parallel Robot for Minimally Invasive Pancreatic Surgery](https://arxiv.org/abs/2507.13787)
*Doina Pisla,Alexandru Pusca,Andrei Caprariu,Adrian Pisla,Bogdan Gherman,Calin Vaida,Damien Chablat*

Main category: cs.RO

TL;DR: 本文提出两种4自由度并行机器人架构（ATHENA-1和ATHENA-2），用于胰腺微创手术辅助，通过FEM模拟和定量工作空间分析评估其性能，最终选择符合设计标准的架构用于实验模型开发。


<details>
  <summary>Details</summary>
Motivation: 设计适用于胰腺微创手术辅助的高性能并行机器人架构，以满足手术需求。

Method: 提出两种4自由度并行架构，进行FEM模拟评估刚度，并通过定量工作空间分析验证实用性。

Result: 通过模拟和分析确定了一种架构具有更高刚度和更好的工作空间适应性。

Conclusion: 选择符合设计标准的架构用于后续实验模型开发，以支持胰腺微创手术机器人技术。

Abstract: This paper focuses on the design of a parallel robot designed for robotic
assisted minimally invasive pancreatic surgery. Two alternative architectures,
called ATHENA-1 and ATHENA-2, each with 4 degrees of freedom (DOF) are
proposed. Their kinematic schemes are presented, and the conceptual 3D CAD
models are illustrated. Based on these, two Finite Element Method (FEM)
simulations were performed to determine which architecture has the higher
stiffness. A workspace quantitative analysis is performed to further assess the
usability of the two proposed parallel architectures related to the medical
tasks. The obtained results are used to select the architecture which fit the
required design criteria and will be used to develop the experimental model of
the surgical robot.

</details>


### [12] [Safety Certification in the Latent space using Control Barrier Functions and World Models](https://arxiv.org/abs/2507.13871)
*Mehul Anand,Shishir Kolathaya*

Main category: cs.RO

TL;DR: 提出了一种半监督框架，利用世界模型的潜在空间中的控制屏障证书（CBCs）来合成安全的视觉运动策略。


<details>
  <summary>Details</summary>
Motivation: 从视觉数据合成安全控制器通常需要大量标记安全关键数据，这在现实场景中不切实际。

Method: 结合神经屏障函数和安全控制器的联合学习，利用现代视觉变换器的预测能力进行潜在动力学建模。

Result: 通过有限标记数据实现了安全的视觉运动策略合成。

Conclusion: 该方法为数据高效的安全控制提供了新途径。

Abstract: Synthesising safe controllers from visual data typically requires extensive
supervised labelling of safety-critical data, which is often impractical in
real-world settings. Recent advances in world models enable reliable prediction
in latent spaces, opening new avenues for scalable and data-efficient safe
control. In this work, we introduce a semi-supervised framework that leverages
control barrier certificates (CBCs) learned in the latent space of a world
model to synthesise safe visuomotor policies. Our approach jointly learns a
neural barrier function and a safe controller using limited labelled data,
while exploiting the predictive power of modern vision transformers for latent
dynamics modelling.

</details>


### [13] [AeroThrow: An Autonomous Aerial Throwing System for Precise Payload Delivery](https://arxiv.org/abs/2507.13903)
*Ziliang Li,Hongming Chen,Yiyang Lin,Biyu Ye,Ximin Lyu*

Main category: cs.RO

TL;DR: 本文提出了一种基于空中机械臂（AM）的自主空投系统，通过引入额外的自由度主动补偿无人机跟踪误差，并结合非线性模型预测控制（NMPC）提高空投精度。


<details>
  <summary>Details</summary>
Motivation: 自主空中系统在复杂环境中的运输和投递任务面临控制模式切换和系统延迟等挑战，需解决这些问题以提高任务性能。

Method: 采用空中机械臂增加自由度，结合平滑约束和NMPC框架，设计分层扰动补偿策略以优化轨迹生成和控制。

Result: 仿真和实验表明，该系统在空投任务中表现出更高的敏捷性和精度。

Conclusion: 提出的方法有效解决了空投任务中的控制问题，提升了系统性能。

Abstract: Autonomous aerial systems play an increasingly vital role in a wide range of
applications, particularly for transport and delivery tasks in complex
environments. In airdrop missions, these platforms face the dual challenges of
abrupt control mode switching and inherent system delays along with control
errors. To address these issues, this paper presents an autonomous airdrop
system based on an aerial manipulator (AM). The introduction of additional
actuated degrees of freedom enables active compensation for UAV tracking
errors. By imposing smooth and continuous constraints on the parabolic landing
point, the proposed approach generates aerial throwing trajectories that are
less sensitive to the timing of payload release. A hierarchical disturbance
compensation strategy is incorporated into the Nonlinear Model Predictive
Control (NMPC) framework to mitigate the effects of sudden changes in system
parameters, while the predictive capabilities of NMPC are further exploited to
improve the precision of aerial throwing. Both simulation and real-world
experimental results demonstrate that the proposed system achieves greater
agility and precision in airdrop missions.

</details>


### [14] [NeHMO: Neural Hamilton-Jacobi Reachability Learning for Decentralized Safe Multi-Agent Motion Planning](https://arxiv.org/abs/2507.13940)
*Qingyi Chen,Ahmed H. Qureshi*

Main category: cs.RO

TL;DR: 论文提出了一种基于神经汉密尔顿-雅可比可达性学习（HJR）的去中心化多智能体运动规划方法，解决了现有方法在可扩展性和实时性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有去中心化算法依赖行为预测或通信，而集中式方法难以扩展和实时决策，因此需要一种新方法。

Method: 采用神经HJR建模处理高维配置空间，并结合去中心化轨迹优化框架实现实时规划。

Result: 方法在12维双臂设置等复杂场景中表现出色，优于现有技术。

Conclusion: 该方法具有可扩展性和数据高效性，适用于高维复杂任务。

Abstract: Safe Multi-Agent Motion Planning (MAMP) is a significant challenge in
robotics. Despite substantial advancements, existing methods often face a
dilemma. Decentralized algorithms typically rely on predicting the behavior of
other agents, sharing contracts, or maintaining communication for safety, while
centralized approaches struggle with scalability and real-time decision-making.
To address these challenges, we introduce Neural Hamilton-Jacobi Reachability
Learning (HJR) for Decentralized Multi-Agent Motion Planning. Our method
provides scalable neural HJR modeling to tackle high-dimensional configuration
spaces and capture worst-case collision and safety constraints between agents.
We further propose a decentralized trajectory optimization framework that
incorporates the learned HJR solutions to solve MAMP tasks in real-time. We
demonstrate that our method is both scalable and data-efficient, enabling the
solution of MAMP problems in higher-dimensional scenarios with complex
collision constraints. Our approach generalizes across various dynamical
systems, including a 12-dimensional dual-arm setup, and outperforms a range of
state-of-the-art techniques in successfully addressing challenging MAMP tasks.
Video demonstrations are available at https://youtu.be/IZiePX0p1Mc.

</details>


### [15] [A Minimalist Controller for Autonomously Self-Aggregating Robotic Swarms: Enabling Compact Formations in Multitasking Scenarios](https://arxiv.org/abs/2507.13969)
*Maria Eduarda Silva de Macedo,Ana Paula Chiarelli de Souza,Roberto Silvio Ubertino Rosso Jr.,Yuri Kaszubowski Lopes*

Main category: cs.RO

TL;DR: 本文提出了一种多任务自聚集方法，使同质机器人仅依赖视线传感器形成紧凑集群，解决了现有方法中集群动态相互影响的问题。


<details>
  <summary>Details</summary>
Motivation: 现有多任务自聚集方法存在集群动态相互干扰、形成不紧凑的圆形结构或缺乏完全自主性的问题。

Method: 使用同质机器人，仅依赖视线传感器实现多任务自聚集，形成紧凑集群。

Result: 通过模拟试验验证了方法的可扩展性，集群紧凑性得到提升，同时保持了其他研究中集群机器人的比例。

Conclusion: 该方法在多任务自聚集场景中表现出色，解决了紧凑性和自主性问题。

Abstract: The deployment of simple emergent behaviors in swarm robotics has been
well-rehearsed in the literature. A recent study has shown how self-aggregation
is possible in a multitask approach -- where multiple self-aggregation task
instances occur concurrently in the same environment. The multitask approach
poses new challenges, in special, how the dynamic of each group impacts the
performance of others. So far, the multitask self-aggregation of groups of
robots suffers from generating a circular formation -- that is not fully
compact -- or is not fully autonomous. In this paper, we present a multitask
self-aggregation where groups of homogeneous robots sort themselves into
different compact clusters, relying solely on a line-of-sight sensor. Our
multitask self-aggregation behavior was able to scale well and achieve a
compact formation. We report scalability results from a series of simulation
trials with different configurations in the number of groups and the number of
robots per group. We were able to improve the multitask self-aggregation
behavior performance in terms of the compactness of the clusters, keeping the
proportion of clustered robots found in other studies.

</details>


### [16] [A segmented robot grasping perception neural network for edge AI](https://arxiv.org/abs/2507.13970)
*Casper Bröcheler,Thomas Vroom,Derrick Timmermans,Alan van den Akker,Guangzhi Tang,Charalampos S. Kouzinopoulos,Rico Möckel*

Main category: cs.RO

TL;DR: 本文提出了一种基于热图引导的6自由度抓取检测框架，并在GAP9 RISC-V芯片上实现，通过硬件优化技术实现低功耗实时抓取。


<details>
  <summary>Details</summary>
Motivation: 机器人抓取需要精确的感知与控制，而深度神经网络在抓取合成中表现优异。在边缘设备上部署这些模型可以实现低延迟、低功耗的实时抓取。

Method: 采用热图引导的抓取检测框架，结合输入降维、模型分区和量化等硬件优化技术，在GAP9 RISC-V芯片上实现。

Result: 在GraspNet-1Billion基准测试中验证了全芯片推理的可行性，展示了低功耗微控制器在实时自主操作中的潜力。

Conclusion: 该框架为资源受限环境下的实时机器人抓取提供了可行解决方案。

Abstract: Robotic grasping, the ability of robots to reliably secure and manipulate
objects of varying shapes, sizes and orientations, is a complex task that
requires precise perception and control. Deep neural networks have shown
remarkable success in grasp synthesis by learning rich and abstract
representations of objects. When deployed at the edge, these models can enable
low-latency, low-power inference, making real-time grasping feasible in
resource-constrained environments. This work implements Heatmap-Guided Grasp
Detection, an end-to-end framework for the detection of 6-Dof grasp poses, on
the GAP9 RISC-V System-on-Chip. The model is optimised using hardware-aware
techniques, including input dimensionality reduction, model partitioning, and
quantisation. Experimental evaluation on the GraspNet-1Billion benchmark
validates the feasibility of fully on-chip inference, highlighting the
potential of low-power MCUs for real-time, autonomous manipulation.

</details>


### [17] [A multi-strategy improved snake optimizer for three-dimensional UAV path planning and engineering problems](https://arxiv.org/abs/2507.14043)
*Genliang Li,Yaxin Cui,Jinyu Su*

Main category: cs.RO

TL;DR: 提出了一种多策略改进的蛇优化算法（MISO），通过自适应随机扰动、Levy飞行策略和精英领导结合布朗运动的位置更新策略，解决了原蛇优化算法（SO）收敛慢和易陷入局部最优的问题，并在测试函数和无人机路径规划中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 蛇优化算法（SO）存在收敛速度慢和易陷入局部最优的缺点，限制了其应用效果。因此，需要改进算法以提升其性能。

Method: 1. 基于正弦函数的自适应随机扰动策略；2. 基于尺度因子和领导者的自适应Levy飞行策略；3. 结合精英领导和布朗运动的位置更新策略。

Result: 在CEC2017和CEC2022测试函数及无人机3D路径规划中，MISO表现优于11种流行算法，具有更高的解质量和稳定性。

Conclusion: MISO在理论和实际应用中均表现出色，展示了其在复杂优化问题中的强大潜力。

Abstract: Metaheuristic algorithms have gained widespread application across various
fields owing to their ability to generate diverse solutions. One such algorithm
is the Snake Optimizer (SO), a progressive optimization approach. However, SO
suffers from the issues of slow convergence speed and susceptibility to local
optima. In light of these shortcomings, we propose a novel Multi-strategy
Improved Snake Optimizer (MISO). Firstly, we propose a new adaptive random
disturbance strategy based on sine function to alleviate the risk of getting
trapped in a local optimum. Secondly, we introduce adaptive Levy flight
strategy based on scale factor and leader and endow the male snake leader with
flight capability, which makes it easier for the algorithm to leap out of the
local optimum and find the global optimum. More importantly, we put forward a
position update strategy combining elite leadership and Brownian motion,
effectively accelerating the convergence speed while ensuring precision.
Finally, to demonstrate the performance of MISO, we utilize 30 CEC2017 test
functions and the CEC2022 test suite, comparing it with 11 popular algorithms
across different dimensions to validate its effectiveness. Moreover, Unmanned
Aerial Vehicle (UAV) has been widely used in various fields due to its
advantages of low cost, high mobility and easy operation. However, the UAV path
planning problem is crucial for flight safety and efficiency, and there are
still challenges in establishing and optimizing the path model. Therefore, we
apply MISO to the UAV 3D path planning problem as well as 6 engineering design
problems to assess its feasibility in practical applications. The experimental
results demonstrate that MISO exceeds other competitive algorithms in terms of
solution quality and stability, establishing its strong potential for
application.

</details>


### [18] [EdgeVLA: Efficient Vision-Language-Action Models](https://arxiv.org/abs/2507.14049)
*Paweł Budzianowski,Wesley Maa,Matthew Freed,Jingxiang Mo,Winston Hsiao,Aaron Xie,Tomasz Młoduchowski,Viraj Tipnis,Benjamin Bolte*

Main category: cs.RO

TL;DR: EVLA是一种新型方法，显著提升了视觉-语言-动作（VLA）模型的推理速度，适用于边缘设备，同时保持模型的表现力。


<details>
  <summary>Details</summary>
Motivation: 解决在资源受限的移动操作系统中部署大规模视觉语言模型（VLMs）的挑战。

Method: 1）消除自回归需求，实现7倍推理加速；2）利用小型语言模型（SLMs）提高计算效率。

Result: EVLA在推理速度和内存效率上显著提升，同时保持与OpenVLA相当的训练性能。

Conclusion: EVLA为边缘设备上的实时VLA模型部署提供了可行方案，并开源代码以促进研究。

Abstract: Vision-Language Models (VLMs) have emerged as a promising approach to address
the data scarcity challenge in robotics, enabling the development of
generalizable visuomotor control policies. While models like OpenVLA showcase
the potential of this paradigm, deploying large-scale VLMs on
resource-constrained mobile manipulation systems remains a significant hurdle.
This paper introduces Edge VLA (EVLA), a novel approach designed to
significantly enhance the inference speed of Vision-Language-Action (VLA)
models. EVLA maintains the representational power of these models while
enabling real-time performance on edge devices. We achieve this through two key
innovations: 1) Eliminating the autoregressive requirement for end-effector
position prediction, leading to a 7x speedup in inference, and 2) Leveraging
the efficiency of Small Language Models (SLMs), demonstrating comparable
training performance to larger models with significantly reduced computational
demands. Our early results demonstrate that EVLA achieves comparable training
characteristics to OpenVLA while offering substantial gains in inference speed
and memory efficiency. We release our model checkpoints and training
\href{https://github.com/kscalelabs/evla }{codebase} to foster further
research.

</details>


### [19] [Design of a Modular Mobile Inspection and Maintenance Robot for an Orbital Servicing Hub](https://arxiv.org/abs/2507.14059)
*Tianyuan Wang,Mark A Post,Mathieu Deremetz*

Main category: cs.RO

TL;DR: STARFAB项目开发了一种移动检查模块（MIM），用于轨道自动化仓库的自主检查和维护。


<details>
  <summary>Details</summary>
Motivation: 为支持‘新太空’商业生态中的空间硬件组件组装和再利用，需要自主机器人进行监控和维护。

Method: MIM采用标准接口设计，可被行走机械臂携带，配备多种传感器和工具，支持模块化扩展。

Result: MIM具备高分辨率相机、3D轮廓仪、热成像传感器等功能，并支持维护工具的使用。

Conclusion: MIM为轨道自主检查和维护提供了可行方案，目前仍在测试中。

Abstract: The use of autonomous robots in space is an essential part of the "New Space"
commercial ecosystem of assembly and re-use of space hardware components in
Earth orbit and beyond. The STARFAB project aims to create a ground
demonstration of an orbital automated warehouse as a hub for sustainable
commercial operations and servicing. A critical part of this fully-autonomous
robotic facility will be the capability to monitor, inspect, and assess the
condition of both the components stored in the warehouse, and the STARFAB
facility itself. This paper introduces ongoing work on the STARFAB Mobile
Inspection Module (MIM). The MIM uses Standard Interconnects (SI) so that it
can be carried by Walking Manipulators (WM) as an independently-mobile robot,
and multiple MIMs can be stored and retrieved as needed for operations on
STARFAB. The MIM carries high-resolution cameras, a 3D profilometer, and a
thermal imaging sensor, with the capability to add other modular sensors. A
grasping tool and torque wrench are stored within the modular body for use by
an attached WM for maintenance operations. Implementation and testing is still
ongoing at the time of writing. This paper details the concept of operations
for the MIM as an on-orbit autonomous inspection and maintenance system, the
mechanical and electronic design of the MIM, and the sensors package used for
non-destructive testing.

</details>


### [20] [MorphIt: Flexible Spherical Approximation of Robot Morphology for Representation-driven Adaptation](https://arxiv.org/abs/2507.14061)
*Nataliya Nechyporenko,Yutong Zhang,Sean Campbell,Alessandro Roncone*

Main category: cs.RO

TL;DR: MorphIt是一种新型算法，通过球形基元近似机器人形态，平衡几何精度与计算效率，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前机器人系统将物理形态视为固定约束，无法适应多样化任务需求，限制了计算效率和精度。

Method: MorphIt采用基于梯度的自动优化框架，可调参数控制物理保真度与计算成本的权衡。

Result: 实验表明，MorphIt在网格近似、碰撞检测、接触模拟和狭窄空间导航中表现优于基线方法。

Conclusion: 动态适应几何表示使机器人能将物理形态作为主动资源，提升在复杂环境中的操作能力。

Abstract: What if a robot could rethink its own morphological representation to better
meet the demands of diverse tasks? Most robotic systems today treat their
physical form as a fixed constraint rather than an adaptive resource, forcing
the same rigid geometric representation to serve applications with vastly
different computational and precision requirements. We introduce MorphIt, a
novel algorithm for approximating robot morphology using spherical primitives
that balances geometric accuracy with computational efficiency. Unlike existing
approaches that rely on either labor-intensive manual specification or
inflexible computational methods, MorphIt implements an automatic
gradient-based optimization framework with tunable parameters that provides
explicit control over the physical fidelity versus computational cost tradeoff.
Quantitative evaluations demonstrate that MorphIt outperforms baseline
approaches (Variational Sphere Set Approximation and Adaptive Medial-Axis
Approximation) across multiple metrics, achieving better mesh approximation
with fewer spheres and reduced computational overhead. Our experiments show
enhanced robot capabilities in collision detection accuracy, contact-rich
interaction simulation, and navigation through confined spaces. By dynamically
adapting geometric representations to task requirements, robots can now exploit
their physical embodiment as an active resource rather than an inflexible
parameter, opening new frontiers for manipulation in environments where
physical form must continuously balance precision with computational
tractability.

</details>


### [21] [Context-Aware Behavior Learning with Heuristic Motion Memory for Underwater Manipulation](https://arxiv.org/abs/2507.14099)
*Markus Buchholz,Ignacio Carlucho,Michele Grimaldi,Maria Koskinopoulou,Yvan R. Petillot*

Main category: cs.RO

TL;DR: 提出了一种自适应启发式运动规划框架，结合启发式运动空间和贝叶斯网络，优化水下自主操作的运动规划。


<details>
  <summary>Details</summary>
Motivation: 现有运动规划方法难以有效利用先验运动经验并适应水下环境中的实时不确定性。

Method: 采用启发式运动空间（HMS）和贝叶斯网络，结合概率路线图（PRM）算法，最小化复合成本函数。

Result: 框架显著减少搜索空间，提升计算性能，并通过贝叶斯网络动态更新不确定性估计，优化路径成功率。

Conclusion: 该方法显著提升水下自主机器人的运动规划能力，适应动态海洋环境挑战。

Abstract: Autonomous motion planning is critical for efficient and safe underwater
manipulation in dynamic marine environments. Current motion planning methods
often fail to effectively utilize prior motion experiences and adapt to
real-time uncertainties inherent in underwater settings. In this paper, we
introduce an Adaptive Heuristic Motion Planner framework that integrates a
Heuristic Motion Space (HMS) with Bayesian Networks to enhance motion planning
for autonomous underwater manipulation. Our approach employs the Probabilistic
Roadmap (PRM) algorithm within HMS to optimize paths by minimizing a composite
cost function that accounts for distance, uncertainty, energy consumption, and
execution time. By leveraging HMS, our framework significantly reduces the
search space, thereby boosting computational performance and enabling real-time
planning capabilities. Bayesian Networks are utilized to dynamically update
uncertainty estimates based on real-time sensor data and environmental
conditions, thereby refining the joint probability of path success. Through
extensive simulations and real-world test scenarios, we showcase the advantages
of our method in terms of enhanced performance and robustness. This
probabilistic approach significantly advances the capability of autonomous
underwater robots, ensuring optimized motion planning in the face of dynamic
marine challenges.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [22] [GraphTrafficGPT: Enhancing Traffic Management Through Graph-Based AI Agent Coordination](https://arxiv.org/abs/2507.13511)
*Nabil Abdelaziz Ferhat Taleb,Abdolazim Rezaei,Raj Atulkumar Patel,Mehdi Sookhak*

Main category: cs.AI

TL;DR: GraphTrafficGPT是一种基于图的架构，通过并行执行和动态资源分配优化LLM驱动的交通管理任务，显著降低令牌消耗和响应延迟。


<details>
  <summary>Details</summary>
Motivation: 现有链式系统（如TrafficGPT）因顺序执行任务、高令牌使用和扩展性差，难以应对复杂交通场景。

Method: 提出GraphTrafficGPT，将任务及其依赖关系建模为有向图，通过Brain Agent分解查询并协调多个专业代理。

Result: 实验显示，令牌消耗减少50.2%，响应延迟降低19.0%，多查询执行效率提升23.0%。

Conclusion: GraphTrafficGPT显著提升了LLM在交通管理中的效率和扩展性。

Abstract: Large Language Models (LLMs) offer significant promise for intelligent
traffic management; however, current chain-based systems like TrafficGPT are
hindered by sequential task execution, high token usage, and poor scalability,
making them inefficient for complex, real-world scenarios. To address these
limitations, we propose GraphTrafficGPT, a novel graph-based architecture,
which fundamentally redesigns the task coordination process for LLM-driven
traffic applications. GraphTrafficGPT represents tasks and their dependencies
as nodes and edges in a directed graph, enabling efficient parallel execution
and dynamic resource allocation. The main idea behind the proposed model is a
Brain Agent that decomposes user queries, constructs optimized dependency
graphs, and coordinates a network of specialized agents for data retrieval,
analysis, visualization, and simulation. By introducing advanced context-aware
token management and supporting concurrent multi-query processing, the proposed
architecture handles interdependent tasks typical of modern urban mobility
environments. Experimental results demonstrate that GraphTrafficGPT reduces
token consumption by 50.2% and average response latency by 19.0% compared to
TrafficGPT, while supporting simultaneous multi-query execution with up to
23.0% improvement in efficiency.

</details>


### [23] [PrefPalette: Personalized Preference Modeling with Latent Attributes](https://arxiv.org/abs/2507.13541)
*Shuyue Stella Li,Melanie Sclar,Hunter Lang,Ansong Ni,Jacqueline He,Puxin Xu,Andrew Cohen,Chan Young Park,Yulia Tsvetkov,Asli Celikyilmaz*

Main category: cs.AI

TL;DR: PrefPalette通过分解偏好属性并基于社区价值观预测偏好，优于GPT-4o 46.6%，同时提供可解释的社区特征分析。


<details>
  <summary>Details</summary>
Motivation: 当前偏好模型将人类判断视为黑箱，缺乏对偏好背后原因的理解，PrefPalette旨在解决这一问题。

Method: 结合多属性决策理论，通过合成反事实属性和基于注意力的偏好建模，捕捉社区动态权重。

Result: 在Reddit的45个社区中，PrefPalette预测准确率比GPT-4o高46.6%，并揭示社区特定偏好特征。

Conclusion: PrefPalette不仅提升预测性能，还提供透明、可解释的偏好分析，为个性化AI系统奠定基础。

Abstract: Personalizing AI systems requires understanding not just what users prefer,
but the reasons that underlie those preferences - yet current preference models
typically treat human judgment as a black box. We introduce PrefPalette, a
framework that decomposes preferences into attribute dimensions and tailors its
preference prediction to distinct social community values in a
human-interpretable manner. PrefPalette operationalizes a cognitive science
principle known as multi-attribute decision making in two ways: (1) a scalable
counterfactual attribute synthesis step that involves generating synthetic
training data to isolate for individual attribute effects (e.g., formality,
humor, cultural values), and (2) attention-based preference modeling that
learns how different social communities dynamically weight these attributes.
This approach moves beyond aggregate preference modeling to capture the diverse
evaluation frameworks that drive human judgment. When evaluated on 45 social
communities from the online platform Reddit, PrefPalette outperforms GPT-4o by
46.6% in average prediction accuracy. Beyond raw predictive improvements,
PrefPalette also shed light on intuitive, community-specific profiles:
scholarly communities prioritize verbosity and stimulation, conflict-oriented
communities value sarcasm and directness, and support-based communities
emphasize empathy. By modeling the attribute-mediated structure of human
judgment, PrefPalette delivers both superior preference modeling and
transparent, interpretable insights, and serves as a first step toward more
trustworthy, value-aware personalized applications.

</details>


### [24] [GOFAI meets Generative AI: Development of Expert Systems by means of Large Language Models](https://arxiv.org/abs/2507.13550)
*Eduardo C. Garrido-Merchán,Cristina Puente*

Main category: cs.AI

TL;DR: 提出了一种结合大语言模型（LLMs）与符号系统的新方法，通过限制领域和结构化提示生成可验证的Prolog知识表示，提升专家系统的可靠性。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在生成信息时可能出现幻觉或错误的问题，开发可控、透明的专家系统。

Method: 采用结构化提示提取方法，生成Prolog符号知识表示，并由人类专家验证和修正。

Result: 实验表明，该方法在事实准确性和语义连贯性上表现优异，结合了LLMs的召回能力和符号系统的精确性。

Conclusion: 该方法为敏感领域提供了可靠、可解释的AI应用基础，兼具可扩展性和可靠性。

Abstract: The development of large language models (LLMs) has successfully transformed
knowledge-based systems such as open domain question nswering, which can
automatically produce vast amounts of seemingly coherent information. Yet,
those models have several disadvantages like hallucinations or confident
generation of incorrect or unverifiable facts. In this paper, we introduce a
new approach to the development of expert systems using LLMs in a controlled
and transparent way. By limiting the domain and employing a well-structured
prompt-based extraction approach, we produce a symbolic representation of
knowledge in Prolog, which can be validated and corrected by human experts.
This approach also guarantees interpretability, scalability and reliability of
the developed expert systems. Via quantitative and qualitative experiments with
Claude Sonnet 3.7 and GPT-4.1, we show strong adherence to facts and semantic
coherence on our generated knowledge bases. We present a transparent hybrid
solution that combines the recall capacity of LLMs with the precision of
symbolic systems, thereby laying the foundation for dependable AI applications
in sensitive domains.

</details>


### [25] [Why Isn't Relational Learning Taking Over the World?](https://arxiv.org/abs/2507.13558)
*David Poole*

Main category: cs.AI

TL;DR: 论文探讨了AI应关注实体及其关系建模，而非仅关注像素和文字，并分析了关系学习未普及的原因及改进方向。


<details>
  <summary>Details</summary>
Motivation: 当前AI主要建模像素和文字，但世界由实体及其关系构成，关系学习应成为核心。

Method: 分析关系学习在数据中的实际应用，探讨其受限原因。

Result: 关系学习仅在少数受限场景中成功，需进一步改进以普及。

Conclusion: 关系学习需突破限制，才能实现其潜力并成为AI主流。

Abstract: AI seems to be taking over the world with systems that model pixels, words,
and phonemes. The world is arguably made up, not of pixels, words, and phonemes
but of entities (objects, things, including events) with properties and
relations among them. Surely we should model these, not the perception or
description of them. You might suspect that concentrating on modeling words and
pixels is because all of the (valuable) data in the world is in terms of text
and images. If you look into almost any company you will find their most
valuable data is in spreadsheets, databases and other relational formats. These
are not the form that are studied in introductory machine learning, but are
full of product numbers, student numbers, transaction numbers and other
identifiers that can't be interpreted naively as numbers. The field that
studies this sort of data has various names including relational learning,
statistical relational AI, and many others. This paper explains why relational
learning is not taking over the world -- except in a few cases with restricted
relations -- and what needs to be done to bring it to it's rightful prominence.

</details>


### [26] [BifrostRAG: Bridging Dual Knowledge Graphs for Multi-Hop Question Answering in Construction Safety](https://arxiv.org/abs/2507.13625)
*Yuxin Zhang,Xi Wang,Mo Hu,Zhenyu Zhang*

Main category: cs.AI

TL;DR: BifrostRAG是一种双图RAG系统，通过实体网络图和文档导航图结合语言关系和文档结构，显著提升多跳问答性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统RAG系统在处理复杂法规文本和多跳查询时的局限性。

Method: 采用双图架构（实体网络图和文档导航图），结合图遍历和向量语义搜索的混合检索机制。

Result: 在多跳问答数据集上达到92.8%精确率、85.5%召回率和87.3% F1分数，显著优于单模态RAG基线。

Conclusion: BifrostRAG为复杂技术文档的检索提供了可迁移的解决方案，适用于知识密集型工程领域。

Abstract: Information retrieval and question answering from safety regulations are
essential for automated construction compliance checking but are hindered by
the linguistic and structural complexity of regulatory text. Many
compliance-related queries are multi-hop, requiring synthesis of information
across interlinked clauses. This poses a challenge for traditional
retrieval-augmented generation (RAG) systems. To overcome this, we introduce
BifrostRAG: a dual-graph RAG-integrated system that explicitly models both
linguistic relationships (via an Entity Network Graph) and document structure
(via a Document Navigator Graph). This architecture powers a hybrid retrieval
mechanism that combines graph traversal with vector-based semantic search,
enabling large language models to reason over both the meaning and the
structure of the text. Evaluation on a multi-hop question dataset shows that
BifrostRAG achieves 92.8 percent precision, 85.5 percent recall, and an F1
score of 87.3 percent. These results significantly outperform vector-only and
graph-only RAG baselines that represent current leading approaches. Error
analysis further highlights the comparative advantages of our hybrid method
over single-modality RAGs. These findings establish BifrostRAG as a robust
knowledge engine for LLM-driven compliance checking. Its dual-graph, hybrid
retrieval mechanism offers a transferable blueprint for navigating complex
technical documents across knowledge-intensive engineering domains.

</details>


### [27] [Buggy rule diagnosis for combined steps through final answer evaluation in stepwise tasks](https://arxiv.org/abs/2507.13651)
*Gerben van der Hoek,Johan Jeuring,Rogier Bos*

Main category: cs.AI

TL;DR: 论文探讨了基于最终答案的自动错误诊断方法，用于解决学生在分步任务中合并步骤导致的组合爆炸问题。


<details>
  <summary>Details</summary>
Motivation: 学生在分步任务中合并步骤会导致可能的路径组合爆炸，增加错误诊断难度。通过最终答案诊断可以缓解这一问题。

Method: 设计了一种基于最终答案的错误诊断服务，并应用于二次方程求解的学生数据集。

Result: 结果显示，该方法能诊断29.4%的未诊断步骤，且与教师诊断结果的一致性达97%。

Conclusion: 该方法为错误诊断提供了新思路，值得进一步探索。

Abstract: Many intelligent tutoring systems can support a student in solving a stepwise
task. When a student combines several steps in one step, the number of possible
paths connecting consecutive inputs may be very large. This combinatorial
explosion makes error diagnosis hard. Using a final answer to diagnose a
combination of steps can mitigate the combinatorial explosion, because there
are generally fewer possible (erroneous) final answers than (erroneous)
solution paths. An intermediate input for a task can be diagnosed by
automatically completing it according to the task solution strategy and
diagnosing this solution. This study explores the potential of automated error
diagnosis based on a final answer. We investigate the design of a service that
provides a buggy rule diagnosis when a student combines several steps. To
validate the approach, we apply the service to an existing dataset (n=1939) of
unique student steps when solving quadratic equations, which could not be
diagnosed by a buggy rule service that tries to connect consecutive inputs with
a single rule. Results show that final answer evaluation can diagnose 29,4% of
these steps. Moreover, a comparison of the generated diagnoses with teacher
diagnoses on a subset (n=115) shows that the diagnoses align in 97% of the
cases. These results can be considered a basis for further exploration of the
approach.

</details>


### [28] [Combining model tracing and constraint-based modeling for multistep strategy diagnoses](https://arxiv.org/abs/2507.13652)
*Gerben van der Hoek,Johan Jeuring,Rogier Bos*

Main category: cs.AI

TL;DR: 提出了一种结合模型追踪和约束建模的方法，用于诊断学生在多步任务中的输入，并在实际数据中验证其与教师诊断的一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法（模型追踪和约束建模）各有局限性，无法全面诊断学生输入，尤其是在学生将多步合并为一步时。

Method: 通过将约束定义为学生输入与策略步骤的共同属性，设计了一个支持多步策略诊断的系统。

Result: 在2136个学生解二次方程的步骤中，系统诊断与教师编码的140个学生步骤完全一致。

Conclusion: 该方法有效结合了两种范式，能够准确诊断学生的多步输入，具有实际应用潜力。

Abstract: Model tracing and constraint-based modeling are two approaches to diagnose
student input in stepwise tasks. Model tracing supports identifying consecutive
problem-solving steps taken by a student, whereas constraint-based modeling
supports student input diagnosis even when several steps are combined into one
step. We propose an approach that merges both paradigms. By defining
constraints as properties that a student input has in common with a step of a
strategy, it is possible to provide a diagnosis when a student deviates from a
strategy even when the student combines several steps. In this study we explore
the design of a system for multistep strategy diagnoses, and evaluate these
diagnoses. As a proof of concept, we generate diagnoses for an existing dataset
containing steps students take when solving quadratic equations (n=2136). To
compare with human diagnoses, two teachers coded a random sample of deviations
(n=70) and applications of the strategy (n=70). Results show that that the
system diagnosis aligned with the teacher coding in all of the 140 student
steps.

</details>


### [29] [DailyLLM: Context-Aware Activity Log Generation Using Multi-Modal Sensors and LLMs](https://arxiv.org/abs/2507.13737)
*Ye Tian,Xiaoyuan Ren,Zihao Wang,Onat Gungor,Xiaofan Yu,Tajana Rosing*

Main category: cs.AI

TL;DR: DailyLLM是一种基于轻量级LLM的框架，通过整合位置、运动、环境和生理四个维度的上下文信息，显著提升了活动日志生成的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有活动日志生成方法在准确性、效率和语义丰富性方面存在不足，需要一种更高效且全面的解决方案。

Method: DailyLLM采用轻量级LLM框架，结合结构化提示和高效特征提取，实现高级活动理解。

Result: 实验表明，DailyLLM在BERTScore精度上比70B参数的SOTA基线提升17%，推理速度快10倍。

Conclusion: DailyLLM为活动日志生成提供了一种高效、轻量且全面的解决方案，适用于智能手机和智能手表等设备。

Abstract: Rich and context-aware activity logs facilitate user behavior analysis and
health monitoring, making them a key research focus in ubiquitous computing.
The remarkable semantic understanding and generation capabilities of Large
Language Models (LLMs) have recently created new opportunities for activity log
generation. However, existing methods continue to exhibit notable limitations
in terms of accuracy, efficiency, and semantic richness. To address these
challenges, we propose DailyLLM. To the best of our knowledge, this is the
first log generation and summarization system that comprehensively integrates
contextual activity information across four dimensions: location, motion,
environment, and physiology, using only sensors commonly available on
smartphones and smartwatches. To achieve this, DailyLLM introduces a
lightweight LLM-based framework that integrates structured prompting with
efficient feature extraction to enable high-level activity understanding.
Extensive experiments demonstrate that DailyLLM outperforms state-of-the-art
(SOTA) log generation methods and can be efficiently deployed on personal
computers and Raspberry Pi. Utilizing only a 1.5B-parameter LLM model, DailyLLM
achieves a 17% improvement in log generation BERTScore precision compared to
the 70B-parameter SOTA baseline, while delivering nearly 10x faster inference
speed.

</details>


### [30] [OntView: What you See is What you Meant](https://arxiv.org/abs/2507.13759)
*Carlos Bobed,Carlota Quintana,Eduardo Mena,Jorge Bobed,Fernando Bobillo*

Main category: cs.AI

TL;DR: OntView是一个新型本体可视化工具，通过直观界面展示本体概念及其关系，支持GCI可视化，并提供简化视图功能以避免信息过载。


<details>
  <summary>Details</summary>
Motivation: 现有本体可视化工具无法有效展示复杂本体结构，限制了用户对依赖关系和属性的理解。

Method: OntView基于DL推理器，采用“所见即所意”范式，支持GCI可视化，并提供三种简化视图方式：概念重要性评估、TBox元素聚焦和动态分支隐藏。

Result: OntView成功实现了直观且非冗余的本体可视化，并通过开源方式发布。

Conclusion: OntView解决了现有工具在复杂本体可视化上的不足，提升了用户理解能力。

Abstract: In the field of knowledge management and computer science, ontologies provide
a structured framework for modeling domain-specific knowledge by defining
concepts and their relationships. However, the lack of tools that provide
effective visualization is still a significant challenge. While numerous
ontology editors and viewers exist, most of them fail to graphically represent
ontology structures in a meaningful and non-overwhelming way, limiting users'
ability to comprehend dependencies and properties within large ontological
frameworks.
  In this paper, we present OntView, an ontology viewer that is designed to
provide users with an intuitive visual representation of ontology concepts and
their formal definitions through a user-friendly interface. Building on the use
of a DL reasoner, OntView follows a "What you see is what you meant" paradigm,
showing the actual inferred knowledge. One key aspect for this is its ability
to visualize General Concept Inclusions (GCI), a feature absent in existing
visualization tools. Moreover, to avoid a possible information overload,
OntView also offers different ways to show a simplified view of the ontology
by: 1) creating ontology summaries by assessing the importance of the concepts
(according to different available algorithms), 2) focusing the visualization on
the existing TBox elements between two given classes and 3) allowing to
hide/show different branches in a dynamic way without losing the semantics.
OntView has been released with an open-source license for the whole community.

</details>


### [31] [From Extraction to Synthesis: Entangled Heuristics for Agent-Augmented Strategic Reasoning](https://arxiv.org/abs/2507.13768)
*Renato Ghisellini,Remo Pareschi,Marco Pedroni,Giovanni Battista Raggi*

Main category: cs.AI

TL;DR: 提出了一种结合启发式提取、语义激活和组合合成的混合架构，用于增强代理的战略推理能力。


<details>
  <summary>Details</summary>
Motivation: 传统决策引擎通常选择最佳规则，而该研究旨在通过语义交互建模和修辞框架，将冲突的启发式融合为连贯且上下文敏感的叙述。

Method: 结合经典军事理论和现代企业战略，通过语义相互依赖的过程激活和组合多个启发式，灵感来源于量子认知研究。

Result: 通过Meta与FTC的案例研究展示了该框架，并通过语义指标进行了初步验证。

Conclusion: 讨论了动态干扰调谐等局限性和扩展方向。

Abstract: We present a hybrid architecture for agent-augmented strategic reasoning,
combining heuristic extraction, semantic activation, and compositional
synthesis. Drawing on sources ranging from classical military theory to
contemporary corporate strategy, our model activates and composes multiple
heuristics through a process of semantic interdependence inspired by research
in quantum cognition. Unlike traditional decision engines that select the best
rule, our system fuses conflicting heuristics into coherent and
context-sensitive narratives, guided by semantic interaction modeling and
rhetorical framing. We demonstrate the framework via a Meta vs. FTC case study,
with preliminary validation through semantic metrics. Limitations and
extensions (e.g., dynamic interference tuning) are discussed.

</details>


### [32] [When Speed meets Accuracy: an Efficient and Effective Graph Model for Temporal Link Prediction](https://arxiv.org/abs/2507.13825)
*Haoyang Li,Yuming Xu,Yiming Li,Hanmo Liu,Darian Li,Chen Jason Zhang,Lei Chen,Qing Li*

Main category: cs.AI

TL;DR: EAGLE是一个轻量级框架，整合短期时间新近性和长期全局结构模式，显著提升动态图时间链路预测的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有T-GNNs因计算开销大而面临可扩展性和效率问题，需一种更高效的方法。

Method: EAGLE结合时间感知模块（聚合最近邻信息）和结构感知模块（利用PageRank捕获全局节点影响），并通过自适应权重机制平衡两者。

Result: 在七个真实世界动态图上，EAGLE在性能和效率上均优于现有T-GNNs，速度提升超50倍。

Conclusion: EAGLE通过简化架构和动态调整机制，实现了高效且有效的时间链路预测。

Abstract: Temporal link prediction in dynamic graphs is a critical task with
applications in diverse domains such as social networks, recommendation
systems, and e-commerce platforms. While existing Temporal Graph Neural
Networks (T-GNNs) have achieved notable success by leveraging complex
architectures to model temporal and structural dependencies, they often suffer
from scalability and efficiency challenges due to high computational overhead.
In this paper, we propose EAGLE, a lightweight framework that integrates
short-term temporal recency and long-term global structural patterns. EAGLE
consists of a time-aware module that aggregates information from a node's most
recent neighbors to reflect its immediate preferences, and a structure-aware
module that leverages temporal personalized PageRank to capture the influence
of globally important nodes. To balance these attributes, EAGLE employs an
adaptive weighting mechanism to dynamically adjust their contributions based on
data characteristics. Also, EAGLE eliminates the need for complex multi-hop
message passing or memory-intensive mechanisms, enabling significant
improvements in efficiency. Extensive experiments on seven real-world temporal
graphs demonstrate that EAGLE consistently achieves superior performance
against state-of-the-art T-GNNs in both effectiveness and efficiency,
delivering more than a 50x speedup over effective transformer-based T-GNNs.

</details>


### [33] [Causal Knowledge Transfer for Multi-Agent Reinforcement Learning in Dynamic Environments](https://arxiv.org/abs/2507.13846)
*Kathrin Korte,Christian Medeiros Adriano,Sona Ghahremani,Holger Giese*

Main category: cs.AI

TL;DR: 论文提出了一种因果知识转移框架，帮助多智能体强化学习（MARL）在非静态环境中共享紧凑的因果表示，实现零样本适应。


<details>
  <summary>Details</summary>
Motivation: 传统MARL知识转移方法在非静态环境中难以泛化，智能体需高成本重新训练。本文旨在通过因果知识转移解决这一问题。

Method: 通过建模碰撞为因果干预，生成恢复动作宏（macro），并在智能体间在线转移，实现零样本适应。

Result: 实验表明，该方法能填补随机探索与完全重新训练策略间约一半的差距，且效果受环境复杂度和智能体目标异质性影响。

Conclusion: 因果知识转移框架为MARL在非静态环境中的适应提供了高效解决方案。

Abstract: [Context] Multi-agent reinforcement learning (MARL) has achieved notable
success in environments where agents must learn coordinated behaviors. However,
transferring knowledge across agents remains challenging in non-stationary
environments with changing goals. [Problem] Traditional knowledge transfer
methods in MARL struggle to generalize, and agents often require costly
retraining to adapt. [Approach] This paper introduces a causal knowledge
transfer framework that enables RL agents to learn and share compact causal
representations of paths within a non-stationary environment. As the
environment changes (new obstacles), agents' collisions require adaptive
recovery strategies. We model each collision as a causal intervention
instantiated as a sequence of recovery actions (a macro) whose effect
corresponds to a causal knowledge of how to circumvent the obstacle while
increasing the chances of achieving the agent's goal (maximizing cumulative
reward). This recovery action macro is transferred online from a second agent
and is applied in a zero-shot fashion, i.e., without retraining, just by
querying a lookup model with local context information (collisions). [Results]
Our findings reveal two key insights: (1) agents with heterogeneous goals were
able to bridge about half of the gap between random exploration and a fully
retrained policy when adapting to new environments, and (2) the impact of
causal knowledge transfer depends on the interplay between environment
complexity and agents' heterogeneous goals.

</details>


### [34] [Large Language Models as Innovators: A Framework to Leverage Latent Space Exploration for Novelty Discovery](https://arxiv.org/abs/2507.13874)
*Mateusz Bystroński,Mikołaj Hołysz,Grzegorz Piotrowski,Nitesh V. Chawla,Tomasz Kajdanowicz*

Main category: cs.AI

TL;DR: 提出了一种模型无关的潜在空间创意框架，通过导航连续嵌入空间实现可控、可扩展的创造力，无需手工规则且易于适应不同领域。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在生成新颖且相关的内容方面存在局限，通常依赖训练数据的模式复制，缺乏创造性发散能力。现有解决方案（如领域特定启发式方法）脆弱且难以泛化。

Method: 提出了一种模型无关的潜在空间创意框架，通过导航连续嵌入空间实现可控创造力，无需手工规则，适应性强。

Result: 初步结果显示该框架具有潜力，可作为通用的人类-AI协作共创工具。

Conclusion: 该框架为AI创意生成提供了一种灵活且可扩展的解决方案，有望推动人类-AI协作的发展。

Abstract: Innovative idea generation remains a core challenge in AI, as large language
models (LLMs) often struggle to produce outputs that are both novel and
relevant. Despite their fluency, LLMs tend to replicate patterns seen during
training, limiting their ability to diverge creatively without extensive prompt
engineering. Prior work has addressed this through domain-specific heuristics
and structured prompting pipelines, but such solutions are brittle and
difficult to generalize. In this paper, we propose a model-agnostic
latent-space ideation framework that enables controlled, scalable creativity by
navigating the continuous embedding space of ideas. Unlike prior methods, our
framework requires no handcrafted rules and adapts easily to different domains,
input formats, and creative tasks. This paper introduces an early-stage
prototype of our method, outlining the conceptual framework and preliminary
results highlighting its potential as a general-purpose co-ideator for human-AI
collaboration.

</details>


### [35] [Cross-modal Causal Intervention for Alzheimer's Disease Prediction](https://arxiv.org/abs/2507.13956)
*Yutao Jin,Haowen Xiao,Jielei Chu,Fengmao Lv,Yuxiao Li,Tianrui Li*

Main category: cs.AI

TL;DR: 提出了一种名为ADPC的新型视觉语言因果干预框架，用于辅助诊断阿尔茨海默病（AD），通过消除混杂因素提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 早期识别轻度认知障碍（MCI）和AD对延缓痴呆进展至关重要，但现有方法因数据选择偏差和变量间复杂关系而受限。

Method: 结合MRI、fMRI图像和LLM生成的文本数据，通过因果干预消除混杂因素，分类CN/MCI/AD。

Result: 实验表明ADPC在分类性能上达到SOTA水平。

Conclusion: 研究展示了因果推理与多模态学习结合在神经疾病诊断中的潜力。

Abstract: Mild Cognitive Impairment (MCI) serves as a prodromal stage of Alzheimer's
Disease (AD), where early identification and intervention can effectively slow
the progression to dementia. However, diagnosing AD remains a significant
challenge in neurology due to the confounders caused mainly by the selection
bias of multimodal data and the complex relationships between variables. To
address these issues, we propose a novel visual-language causal intervention
framework named Alzheimer's Disease Prediction with Cross-modal Causal
Intervention (ADPC) for diagnostic assistance. Our ADPC employs large language
model (LLM) to summarize clinical data under strict templates, maintaining
structured text outputs even with incomplete or unevenly distributed datasets.
The ADPC model utilizes Magnetic Resonance Imaging (MRI), functional MRI (fMRI)
images and textual data generated by LLM to classify participants into
Cognitively Normal (CN), MCI, and AD categories. Because of the presence of
confounders, such as neuroimaging artifacts and age-related biomarkers,
non-causal models are likely to capture spurious input-output correlations,
generating less reliable results. Our framework implicitly eliminates
confounders through causal intervention. Experimental results demonstrate the
outstanding performance of our method in distinguishing CN/MCI/AD cases,
achieving state-of-the-art (SOTA) metrics across most evaluation metrics. The
study showcases the potential of integrating causal reasoning with multi-modal
learning for neurological disease diagnosis.

</details>


### [36] [Towards Constraint Temporal Answer Set Programming](https://arxiv.org/abs/2507.13958)
*Pedro Cabalar,Martín Diéguez,François Olivier,Torsten Schaub,Igor Stéphan*

Main category: cs.AI

TL;DR: 论文提出了一种新颖的时态和约束扩展方法，用于解决动态系统中高精度时态和数值推理的挑战，结合了Here-and-There逻辑的线性时间版本和约束逻辑。


<details>
  <summary>Details</summary>
Motivation: 动态系统中高精度时态和数值推理对逻辑方法（如ASP）具有挑战性，需要新的扩展来支持非单调时态推理和约束处理。

Method: 结合线性时间的Here-and-There逻辑（支持非单调时态推理）和带约束的Here-and-There逻辑（支持数值约束），提出了一种新的逻辑框架。

Result: 建立了一个专为ASP设计的非单调时态推理与约束处理的逻辑框架，适用于高精度动态系统建模。

Conclusion: 该工作为ASP范式下处理复杂动态系统提供了基础逻辑框架，填补了非单调时态推理与约束结合的空白。

Abstract: Reasoning about dynamic systems with a fine-grained temporal and numeric
resolution presents significant challenges for logic-based approaches like
Answer Set Programming (ASP). To address this, we introduce and elaborate upon
a novel temporal and constraint-based extension of the logic of Here-and-There
and its nonmonotonic equilibrium extension, representing, to the best of our
knowledge, the first approach to nonmonotonic temporal reasoning with
constraints specifically tailored for ASP. This expressive system is achieved
by a synergistic combination of two foundational ASP extensions: the
linear-time logic of Here-and-There, providing robust nonmonotonic temporal
reasoning capabilities, and the logic of Here-and-There with constraints,
enabling the direct integration and manipulation of numeric constraints, among
others. This work establishes the foundational logical framework for tackling
complex dynamic systems with high resolution within the ASP paradigm.

</details>


### [37] [KROMA: Ontology Matching with Knowledge Retrieval and Large Language Models](https://arxiv.org/abs/2507.14032)
*Lam Nguyen,Erika Barcelos,Roger French,Yinghui Wu*

Main category: cs.AI

TL;DR: KROMA是一个新颖的OM框架，利用LLMs和RAG动态增强语义上下文，通过双相似度匹配和轻量级本体优化提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有OM系统依赖手工规则或专用模型，适应性有限，KROMA旨在通过LLMs和知识检索提升语义互操作性。

Method: 结合双相似度概念匹配和轻量级本体优化，减少LLMs调用开销，动态丰富语义上下文。

Result: 在多个基准数据集上表现优异，优于传统OM系统和前沿LLM方法，同时保持低通信开销。

Conclusion: KROMA证明了知识检索、提示增强和本体优化在大规模OM中的可行性和优势。

Abstract: Ontology Matching (OM) is a cornerstone task of semantic interoperability,
yet existing systems often rely on handcrafted rules or specialized models with
limited adaptability. We present KROMA, a novel OM framework that harnesses
Large Language Models (LLMs) within a Retrieval-Augmented Generation (RAG)
pipeline to dynamically enrich the semantic context of OM tasks with
structural, lexical, and definitional knowledge. To optimize both performance
and efficiency, KROMA integrates a bisimilarity-based concept matching and a
lightweight ontology refinement step, which prune candidate concepts and
substantially reduce the communication overhead from invoking LLMs. Through
experiments on multiple benchmark datasets, we show that integrating knowledge
retrieval with context-augmented LLMs significantly enhances ontology matching,
outperforming both classic OM systems and cutting-edge LLM-based approaches
while keeping communication overhead comparable. Our study highlights the
feasibility and benefit of the proposed optimization techniques (targeted
knowledge retrieval, prompt enrichment, and ontology refinement) for ontology
matching at scale.

</details>


### [38] [Glucose-ML: A collection of longitudinal diabetes datasets for development of robust AI solutions](https://arxiv.org/abs/2507.14077)
*Temiloluwa Prioleau,Baiying Lu,Yanjun Cui*

Main category: cs.AI

TL;DR: Glucose-ML是一个包含10个公开糖尿病数据集的集合，旨在加速透明、可重复和稳健的AI解决方案开发。


<details>
  <summary>Details</summary>
Motivation: 解决高质量糖尿病数据集获取困难的问题，促进AI在糖尿病管理中的应用。

Method: 收集并分析10个公开数据集，提供比较分析和血糖预测案例研究。

Result: 同一算法在不同数据集上的预测结果差异显著，研究结果为开发稳健AI解决方案提供了建议。

Conclusion: Glucose-ML数据集和代码的公开将支持糖尿病领域的AI研究和创新。

Abstract: Artificial intelligence (AI) algorithms are a critical part of
state-of-the-art digital health technology for diabetes management. Yet, access
to large high-quality datasets is creating barriers that impede development of
robust AI solutions. To accelerate development of transparent, reproducible,
and robust AI solutions, we present Glucose-ML, a collection of 10 publicly
available diabetes datasets, released within the last 7 years (i.e., 2018 -
2025). The Glucose-ML collection comprises over 300,000 days of continuous
glucose monitor (CGM) data with a total of 38 million glucose samples collected
from 2500+ people across 4 countries. Participants include persons living with
type 1 diabetes, type 2 diabetes, prediabetes, and no diabetes. To support
researchers and innovators with using this rich collection of diabetes
datasets, we present a comparative analysis to guide algorithm developers with
data selection. Additionally, we conduct a case study for the task of blood
glucose prediction - one of the most common AI tasks within the field. Through
this case study, we provide a benchmark for short-term blood glucose prediction
across all 10 publicly available diabetes datasets within the Glucose-ML
collection. We show that the same algorithm can have significantly different
prediction results when developed/evaluated with different datasets. Findings
from this study are then used to inform recommendations for developing robust
AI solutions within the diabetes or broader health domain. We provide direct
links to each longitudinal diabetes dataset in the Glucose-ML collection and
openly provide our code.

</details>


### [39] [Generative AI-Driven High-Fidelity Human Motion Simulation](https://arxiv.org/abs/2507.14097)
*Hari Iyer,Neel Macwan,Atharva Jitendra Hude,Heejin Jeong,Shenghan Guo*

Main category: cs.AI

TL;DR: 本文提出了一种基于生成式AI的人体运动模拟方法（G-AI-HMS），通过结合文本到文本和文本到运动模型，提高了工业任务中运动模拟的逼真度。


<details>
  <summary>Details</summary>
Motivation: 现有的人体运动模拟方法在运动逼真度上表现不佳，影响了对工人行为、安全和生产效率的评估。

Method: G-AI-HMS利用大型语言模型将任务描述转化为运动感知语言，并通过计算机视觉验证AI生成的运动与真实人类动作的相似性。

Result: 在八项任务的案例研究中，AI生成的运动在多数场景中表现出更低的误差，显著减少了关节误差和时间错位。

Conclusion: G-AI-HMS显著提升了运动模拟的质量，为工业任务中的行为评估提供了更可靠的工具。

Abstract: Human motion simulation (HMS) supports cost-effective evaluation of worker
behavior, safety, and productivity in industrial tasks. However, existing
methods often suffer from low motion fidelity. This study introduces
Generative-AI-Enabled HMS (G-AI-HMS), which integrates text-to-text and
text-to-motion models to enhance simulation quality for physical tasks.
G-AI-HMS tackles two key challenges: (1) translating task descriptions into
motion-aware language using Large Language Models aligned with MotionGPT's
training vocabulary, and (2) validating AI-enhanced motions against real human
movements using computer vision. Posture estimation algorithms are applied to
real-time videos to extract joint landmarks, and motion similarity metrics are
used to compare them with AI-enhanced sequences. In a case study involving
eight tasks, the AI-enhanced motions showed lower error than human created
descriptions in most scenarios, performing better in six tasks based on spatial
accuracy, four tasks based on alignment after pose normalization, and seven
tasks based on overall temporal similarity. Statistical analysis showed that
AI-enhanced prompts significantly (p $<$ 0.0001) reduced joint error and
temporal misalignment while retaining comparable posture accuracy.

</details>


### [40] [Automated Interpretation of Non-Destructive Evaluation Contour Maps Using Large Language Models for Bridge Condition Assessment](https://arxiv.org/abs/2507.14107)
*Viraj Nishesh Darji,Callie C. Liao,Duoduo Liao*

Main category: cs.AI

TL;DR: 该研究探讨了利用大型语言模型（LLMs）自动解释无损评估（NDE）轮廓图的能力，以提高桥梁维护效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 桥梁维护和安全至关重要，但NDE数据解释耗时且需专业知识，LLMs为自动化分析提供了新途径。

Method: 研究设计了特定提示词，测试多种LLMs对五种NDE轮廓图的解释能力，评估其描述细节、缺陷识别和推荐能力。

Result: 九种模型中四种表现更优，尤其是ChatGPT-4和Claude 3.5 Sonnet，能生成更有效的桥梁状况总结。

Conclusion: LLMs可显著提升桥梁维护效率与准确性，为基础设施管理提供创新支持。

Abstract: Bridge maintenance and safety are essential for transportation authorities,
and Non-Destructive Evaluation (NDE) techniques are critical to assessing
structural integrity. However, interpreting NDE data can be time-consuming and
requires expertise, potentially delaying decision-making. Recent advancements
in Large Language Models (LLMs) offer new ways to automate and improve this
analysis. This pilot study introduces a holistic assessment of LLM capabilities
for interpreting NDE contour maps and demonstrates the effectiveness of LLMs in
providing detailed bridge condition analyses. It establishes a framework for
integrating LLMs into bridge inspection workflows, indicating that LLM-assisted
analysis can enhance efficiency without compromising accuracy. In this study,
several LLMs are explored with prompts specifically designed to enhance the
quality of image descriptions, which are applied to interpret five different
NDE contour maps obtained through technologies for assessing bridge conditions.
Each LLM model is evaluated based on its ability to produce detailed
descriptions, identify defects, provide actionable recommendations, and
demonstrate overall accuracy. The research indicates that four of the nine
models provide better image descriptions, effectively covering a wide range of
topics related to the bridge's condition. The outputs from these four models
are summarized using five different LLMs to form a comprehensive overview of
the bridge. Notably, LLMs ChatGPT-4 and Claude 3.5 Sonnet generate more
effective summaries. The findings suggest that LLMs have the potential to
significantly improve efficiency and accuracy. This pilot study presents an
innovative approach that leverages LLMs for image captioning in parallel and
summarization, enabling faster decision-making in bridge maintenance and
enhancing infrastructure management and safety assessments.

</details>


### [41] [CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement Learning](https://arxiv.org/abs/2507.14111)
*Xiaoya Li,Xiaofei Sun,Albert Wang,Jiwei Li,Chris Shum*

Main category: cs.AI

TL;DR: CUDA-L1是一种基于强化学习的自动化CUDA优化框架，显著提升CUDA内核性能，并展示出色的跨GPU架构移植性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的快速发展，GPU计算资源需求激增，亟需自动化CUDA优化策略。现有模型在CUDA优化上成功率低，因此提出CUDA-L1。

Method: 采用强化学习框架，通过速度提升奖励信号训练模型，无需人工干预或领域知识。

Result: 在NVIDIA A100上平均加速17.7倍，峰值达449倍；在其他GPU架构上也表现优异。模型还能发现优化技巧、识别性能瓶颈。

Conclusion: CUDA-L1证明强化学习可有效优化CUDA性能，为自动化GPU优化开辟新途径，有望缓解GPU资源压力。

Abstract: The exponential growth in demand for GPU computing resources, driven by the
rapid advancement of Large Language Models, has created an urgent need for
automated CUDA optimization strategies. While recent advances in LLMs show
promise for code generation, current SOTA models (e.g. R1, o1) achieve low
success rates in improving CUDA speed. In this paper, we introduce CUDA-L1, an
automated reinforcement learning framework for CUDA optimization.
  CUDA-L1 achieves performance improvements on the CUDA optimization task:
trained on NVIDIA A100, it delivers an average speedup of x17.7 across all 250
CUDA kernels of KernelBench, with peak speedups reaching x449. Furthermore, the
model also demonstrates excellent portability across GPU architectures,
achieving average speedups of x17.8 on H100, x19.0 on RTX 3090, x16.5 on L40,
x14.7 on H800, and x13.9 on H20 despite being optimized specifically for A100.
Beyond these benchmark results, CUDA-L1 demonstrates several remarkable
properties: 1) Discovers a variety of CUDA optimization techniques and learns
to combine them strategically to achieve optimal performance; 2) Uncovers
fundamental principles of CUDA optimization; 3) Identifies non-obvious
performance bottlenecks and rejects seemingly beneficial optimizations that
harm performance.
  The capabilities of CUDA-L1 demonstrate that reinforcement learning can
transform an initially poor-performing LLM into an effective CUDA optimizer
through speedup-based reward signals alone, without human expertise or domain
knowledge. More importantly, the trained RL model extend the acquired reasoning
abilities to new kernels. This paradigm opens possibilities for automated
optimization of CUDA operations, and holds promise to substantially promote GPU
efficiency and alleviate the rising pressure on GPU computing resources.

</details>
