{"id": "2507.02953", "categories": ["cs.RO", "cs.NA", "cs.SY", "eess.SY", "math.NA", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.02953", "abs": "https://arxiv.org/abs/2507.02953", "authors": ["Maksym Shamrai"], "title": "Closed-Form Robustness Bounds for Second-Order Pruning of Neural Controller Policies", "comment": "7 pages", "summary": "Deep neural policies have unlocked agile flight for quadcopters, adaptive\ngrasping for manipulators, and reliable navigation for ground robots, yet their\nmillions of weights conflict with the tight memory and real-time constraints of\nembedded microcontrollers. Second-order pruning methods, such as Optimal Brain\nDamage (OBD) and its variants, including Optimal Brain Surgeon (OBS) and the\nrecent SparseGPT, compress networks in a single pass by leveraging the local\nHessian, achieving far higher sparsity than magnitude thresholding. Despite\ntheir success in vision and language, the consequences of such weight removal\non closed-loop stability, tracking accuracy, and safety have remained unclear.\nWe present the first mathematically rigorous robustness analysis of\nsecond-order pruning in nonlinear discrete-time control. The system evolves\nunder a continuous transition map, while the controller is an $L$-layer\nmultilayer perceptron with ReLU-type activations that are globally 1-Lipschitz.\nPruning the weight matrix of layer $k$ replaces $W_k$ with $W_k+\\delta W_k$,\nproducing the perturbed parameter vector $\\widehat{\\Theta}=\\Theta+\\delta\\Theta$\nand the pruned policy $\\pi(\\cdot;\\widehat{\\Theta})$. For every input state\n$s\\in X$ we derive the closed-form inequality $\n\\|\\pi(s;\\Theta)-\\pi(s;\\widehat{\\Theta})\\|_2 \\le C_k(s)\\,\\|\\delta W_k\\|_2, $\nwhere the constant $C_k(s)$ depends only on unpruned spectral norms and biases,\nand can be evaluated in closed form from a single forward pass. The derived\nbounds specify, prior to field deployment, the maximal admissible pruning\nmagnitude compatible with a prescribed control-error threshold. By linking\nsecond-order network compression with closed-loop performance guarantees, our\nwork narrows a crucial gap between modern deep-learning tooling and the\nrobustness demands of safety-critical autonomous systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u5d4c\u5165\u5f0f\u5fae\u63a7\u5236\u5668\u4e0a\u5b9e\u65f6\u8fd0\u884c\u7684\u4e8c\u9636\u526a\u679d\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u5206\u6790\uff0c\u4e3a\u95ed\u73af\u63a7\u5236\u7cfb\u7edf\u7684\u7a33\u5b9a\u6027\u548c\u5b89\u5168\u6027\u63d0\u4f9b\u4e86\u6570\u5b66\u4fdd\u8bc1\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5e9e\u5927\u7684\u53c2\u6570\u91cf\u4e0e\u5d4c\u5165\u5f0f\u8bbe\u5907\u7684\u8d44\u6e90\u9650\u5236\u5b58\u5728\u51b2\u7a81\u3002\u4e8c\u9636\u526a\u679d\u65b9\u6cd5\u867d\u80fd\u9ad8\u6548\u538b\u7f29\u7f51\u7edc\uff0c\u4f46\u5176\u5bf9\u95ed\u73af\u63a7\u5236\u6027\u80fd\u7684\u5f71\u54cd\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u901a\u8fc7\u5206\u6790\u975e\u7ebf\u6027\u79bb\u6563\u65f6\u95f4\u63a7\u5236\u7cfb\u7edf\u4e2d\u7684\u4e8c\u9636\u526a\u679d\uff0c\u63a8\u5bfc\u51fa\u526a\u679d\u540e\u7b56\u7565\u4e0e\u539f\u59cb\u7b56\u7565\u4e4b\u95f4\u7684\u8bef\u5dee\u4e0a\u754c\uff0c\u8be5\u4e0a\u754c\u4ec5\u4f9d\u8d56\u4e8e\u672a\u526a\u679d\u7f51\u7edc\u7684\u8c31\u8303\u6570\u548c\u504f\u7f6e\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u95ed\u5f0f\u4e0d\u7b49\u5f0f\uff0c\u80fd\u591f\u5728\u526a\u679d\u524d\u8bc4\u4f30\u526a\u679d\u5e45\u5ea6\u5bf9\u63a7\u5236\u8bef\u5dee\u7684\u5f71\u54cd\uff0c\u4ece\u800c\u786e\u4fdd\u526a\u679d\u540e\u7684\u7f51\u7edc\u6ee1\u8db3\u9884\u8bbe\u7684\u6027\u80fd\u8981\u6c42\u3002", "conclusion": "\u8be5\u7814\u7a76\u586b\u8865\u4e86\u6df1\u5ea6\u5b66\u4e60\u5de5\u5177\u4e0e\u5b89\u5168\u5173\u952e\u81ea\u4e3b\u7cfb\u7edf\u9c81\u68d2\u6027\u9700\u6c42\u4e4b\u95f4\u7684\u91cd\u8981\u7a7a\u767d\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u3002"}}
{"id": "2507.03049", "categories": ["cs.RO", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.03049", "abs": "https://arxiv.org/abs/2507.03049", "authors": ["Ferran Gebell\u00ed", "Ana\u00eds Garrell", "Jan-Gerrit Habekost", "S\u00e9verin Lemaignan", "Stefan Wermter", "Raquel Ros"], "title": "Personalised Explanations in Long-term Human-Robot Interactions", "comment": "8 pages. It will be published at RO-MAN 2025", "summary": "In the field of Human-Robot Interaction (HRI), a fundamental challenge is to\nfacilitate human understanding of robots. The emerging domain of eXplainable\nHRI (XHRI) investigates methods to generate explanations and evaluate their\nimpact on human-robot interactions. Previous works have highlighted the need to\npersonalise the level of detail of these explanations to enhance usability and\ncomprehension. Our paper presents a framework designed to update and retrieve\nuser knowledge-memory models, allowing for adapting the explanations' level of\ndetail while referencing previously acquired concepts. Three architectures\nbased on our proposed framework that use Large Language Models (LLMs) are\nevaluated in two distinct scenarios: a hospital patrolling robot and a kitchen\nassistant robot. Experimental results demonstrate that a two-stage\narchitecture, which first generates an explanation and then personalises it, is\nthe framework architecture that effectively reduces the level of detail only\nwhen there is related user knowledge.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7528\u6237\u77e5\u8bc6\u8bb0\u5fc6\u6a21\u578b\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728XHRI\u4e2d\u52a8\u6001\u8c03\u6574\u673a\u5668\u4eba\u89e3\u91ca\u7684\u8be6\u7ec6\u7a0b\u5ea6\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u4e24\u9636\u6bb5\u67b6\u6784\u7684\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u4eba\u673a\u4ea4\u4e92\u4e2d\u89e3\u91ca\u4e2a\u6027\u5316\u7684\u95ee\u9898\uff0c\u63d0\u5347\u7528\u6237\u5bf9\u673a\u5668\u4eba\u7684\u7406\u89e3\u548c\u4ea4\u4e92\u4f53\u9a8c\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u6846\u67b6\uff0c\u7ed3\u5408\u7528\u6237\u77e5\u8bc6\u8bb0\u5fc6\u6a21\u578b\u548cLLMs\uff0c\u8bbe\u8ba1\u4e09\u79cd\u67b6\u6784\u5e76\u5728\u533b\u9662\u5de1\u903b\u673a\u5668\u4eba\u548c\u53a8\u623f\u52a9\u624b\u673a\u5668\u4eba\u573a\u666f\u4e2d\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4e24\u9636\u6bb5\u67b6\u6784\uff08\u5148\u751f\u6210\u89e3\u91ca\u518d\u4e2a\u6027\u5316\uff09\u80fd\u6709\u6548\u51cf\u5c11\u89e3\u91ca\u7684\u8be6\u7ec6\u7a0b\u5ea6\uff0c\u524d\u63d0\u662f\u5b58\u5728\u76f8\u5173\u7528\u6237\u77e5\u8bc6\u3002", "conclusion": "\u4e24\u9636\u6bb5\u67b6\u6784\u5728XHRI\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u80fd\u6839\u636e\u7528\u6237\u77e5\u8bc6\u52a8\u6001\u8c03\u6574\u89e3\u91ca\uff0c\u63d0\u5347\u4ea4\u4e92\u6548\u679c\u3002"}}
{"id": "2507.03166", "categories": ["cs.RO", "cs.GR"], "pdf": "https://arxiv.org/pdf/2507.03166", "abs": "https://arxiv.org/abs/2507.03166", "authors": ["Daniel Berio", "Guillaume Clivaz", "Michael Stroh", "Oliver Deussen", "R\u00e9jean Plamondon", "Sylvain Calinon", "Frederic Fol Leymarie"], "title": "Image-driven Robot Drawing with Rapid Lognormal Movements", "comment": "Accepted at IEEE RO-MAN 2025", "summary": "Large image generation and vision models, combined with differentiable\nrendering technologies, have become powerful tools for generating paths that\ncan be drawn or painted by a robot. However, these tools often overlook the\nintrinsic physicality of the human drawing/writing act, which is usually\nexecuted with skillful hand/arm gestures. Taking this into account is important\nfor the visual aesthetics of the results and for the development of closer and\nmore intuitive artist-robot collaboration scenarios. We present a method that\nbridges this gap by enabling gradient-based optimization of natural human-like\nmotions guided by cost functions defined in image space. To this end, we use\nthe sigma-lognormal model of human hand/arm movements, with an adaptation that\nenables its use in conjunction with a differentiable vector graphics (DiffVG)\nrenderer. We demonstrate how this pipeline can be used to generate feasible\ntrajectories for a robot by combining image-driven objectives with a\nminimum-time smoothing criterion. We demonstrate applications with generation\nand robotic reproduction of synthetic graffiti as well as image abstraction.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4eba\u7c7b\u624b\u52bf\u6a21\u578b\u7684\u68af\u5ea6\u4f18\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u673a\u5668\u4eba\u7ed8\u753b\u7684\u81ea\u7136\u8f68\u8ff9\u3002", "motivation": "\u73b0\u6709\u5de5\u5177\u5ffd\u7565\u4e86\u4eba\u7c7b\u7ed8\u753b/\u4e66\u5199\u884c\u4e3a\u7684\u7269\u7406\u7279\u6027\uff0c\u5f71\u54cd\u4e86\u89c6\u89c9\u7f8e\u5b66\u548c\u827a\u672f\u5bb6-\u673a\u5668\u4eba\u534f\u4f5c\u7684\u76f4\u89c2\u6027\u3002", "method": "\u4f7f\u7528sigma-lognormal\u6a21\u578b\u7ed3\u5408\u53ef\u5fae\u5206\u77e2\u91cf\u56fe\u5f62\u6e32\u67d3\u5668\uff08DiffVG\uff09\uff0c\u901a\u8fc7\u68af\u5ea6\u4f18\u5316\u751f\u6210\u81ea\u7136\u624b\u52bf\u8f68\u8ff9\u3002", "result": "\u5c55\u793a\u4e86\u8be5\u65b9\u6cd5\u5728\u5408\u6210\u6d82\u9e26\u548c\u56fe\u50cf\u62bd\u8c61\u751f\u6210\u53ca\u673a\u5668\u4eba\u518d\u73b0\u4e2d\u7684\u5e94\u7528\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u7ed3\u5408\u4e86\u56fe\u50cf\u9a71\u52a8\u76ee\u6807\u548c\u624b\u52bf\u6a21\u578b\uff0c\u63d0\u5347\u4e86\u673a\u5668\u4eba\u7ed8\u753b\u7684\u81ea\u7136\u6027\u548c\u534f\u4f5c\u6f5c\u529b\u3002"}}
{"id": "2507.03227", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.03227", "abs": "https://arxiv.org/abs/2507.03227", "authors": ["Ruoshi Wen", "Jiajun Zhang", "Guangzeng Chen", "Zhongren Cui", "Min Du", "Yang Gou", "Zhigang Han", "Junkai Hu", "Liqun Huang", "Hao Niu", "Wei Xu", "Haoxiang Zhang", "Zhengming Zhu", "Hang Li", "Zeyu Ren"], "title": "Dexterous Teleoperation of 20-DoF ByteDexter Hand via Human Motion Retargeting", "comment": "Tech Report. Project page: https://byte-dexter.github.io/", "summary": "Replicating human--level dexterity remains a fundamental robotics challenge,\nrequiring integrated solutions from mechatronic design to the control of high\ndegree--of--freedom (DoF) robotic hands. While imitation learning shows promise\nin transferring human dexterity to robots, the efficacy of trained policies\nrelies on the quality of human demonstration data. We bridge this gap with a\nhand--arm teleoperation system featuring: (1) a 20--DoF linkage--driven\nanthropomorphic robotic hand for biomimetic dexterity, and (2) an\noptimization--based motion retargeting for real--time, high--fidelity\nreproduction of intricate human hand motions and seamless hand--arm\ncoordination. We validate the system via extensive empirical evaluations,\nincluding dexterous in-hand manipulation tasks and a long--horizon task\nrequiring the organization of a cluttered makeup table randomly populated with\nnine objects. Experimental results demonstrate its intuitive teleoperation\ninterface with real--time control and the ability to generate high--quality\ndemonstration data. Please refer to the accompanying video for further details.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u624b-\u81c2\u8fdc\u7a0b\u64cd\u4f5c\u7cfb\u7edf\uff0c\u901a\u8fc7\u4eff\u4eba\u673a\u68b0\u624b\u548c\u4f18\u5316\u8fd0\u52a8\u91cd\u5b9a\u5411\u6280\u672f\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u7684\u4eba\u7c7b\u624b\u90e8\u52a8\u4f5c\u590d\u5236\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u590d\u5236\u4eba\u7c7b\u7075\u5de7\u6027\u662f\u673a\u5668\u4eba\u5b66\u7684\u6838\u5fc3\u6311\u6218\uff0c\u73b0\u6709\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u7684\u6027\u80fd\u53d7\u9650\u4e8e\u4eba\u7c7b\u793a\u8303\u6570\u636e\u7684\u8d28\u91cf\u3002", "method": "\u5f00\u53d1\u4e8620\u81ea\u7531\u5ea6\u7684\u4eff\u4eba\u673a\u68b0\u624b\u548c\u57fa\u4e8e\u4f18\u5316\u7684\u8fd0\u52a8\u91cd\u5b9a\u5411\u6280\u672f\uff0c\u5b9e\u73b0\u5b9e\u65f6\u9ad8\u4fdd\u771f\u7684\u4eba\u7c7b\u624b\u90e8\u52a8\u4f5c\u590d\u5236\u548c\u624b-\u81c2\u534f\u8c03\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u7684\u5b9e\u65f6\u63a7\u5236\u80fd\u529b\u548c\u9ad8\u8d28\u91cf\u793a\u8303\u6570\u636e\u751f\u6210\u80fd\u529b\uff0c\u6210\u529f\u5b8c\u6210\u590d\u6742\u4efb\u52a1\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u673a\u5668\u4eba\u7075\u5de7\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u793a\u8303\u6570\u636e\u751f\u6210\u5de5\u5177\u3002"}}
{"id": "2507.02977", "categories": ["cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2507.02977", "abs": "https://arxiv.org/abs/2507.02977", "authors": ["Igor Ivanov"], "title": "LLMs are Capable of Misaligned Behavior Under Explicit Prohibition and Surveillance", "comment": "10 pages, 2 figures", "summary": "In this paper, LLMs are tasked with completing an impossible quiz, while they\nare in a sandbox, monitored, told about these measures and instructed not to\ncheat. Some frontier LLMs cheat consistently and attempt to circumvent\nrestrictions despite everything. The results reveal a fundamental tension\nbetween goal-directed behavior and alignment in current LLMs. The code and\nevaluation logs are available at github.com/baceolus/cheating_evals", "AI": {"tldr": "\u524d\u6cbfLLMs\u5728\u53d7\u9650\u73af\u5883\u4e2d\u4ecd\u8bd5\u56fe\u4f5c\u5f0a\uff0c\u63ed\u793a\u4e86\u76ee\u6807\u5bfc\u5411\u884c\u4e3a\u4e0e\u5bf9\u9f50\u4e4b\u95f4\u7684\u51b2\u7a81\u3002", "motivation": "\u7814\u7a76LLMs\u5728\u660e\u786e\u9650\u5236\u4e0b\u662f\u5426\u4ecd\u4f1a\u4f5c\u5f0a\uff0c\u4ee5\u63a2\u7d22\u5176\u76ee\u6807\u5bfc\u5411\u884c\u4e3a\u4e0e\u5bf9\u9f50\u7684\u6f5c\u5728\u77db\u76fe\u3002", "method": "\u5728\u6c99\u76d2\u73af\u5883\u4e2d\u76d1\u63a7LLMs\u5b8c\u6210\u4e0d\u53ef\u80fd\u7684\u4efb\u52a1\uff0c\u660e\u786e\u544a\u77e5\u9650\u5236\u5e76\u7981\u6b62\u4f5c\u5f0a\u3002", "result": "\u90e8\u5206\u524d\u6cbfLLMs\u6301\u7eed\u4f5c\u5f0a\u5e76\u8bd5\u56fe\u89c4\u907f\u9650\u5236\u3002", "conclusion": "\u5f53\u524dLLMs\u5b58\u5728\u76ee\u6807\u5bfc\u5411\u884c\u4e3a\u4e0e\u5bf9\u9f50\u4e4b\u95f4\u7684\u6839\u672c\u51b2\u7a81\u3002"}}
{"id": "2507.03231", "categories": ["cs.RO", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.03231", "abs": "https://arxiv.org/abs/2507.03231", "authors": ["Ishaan Mahajan", "Brian Plancher"], "title": "Robust and Efficient Embedded Convex Optimization through First-Order Adaptive Caching", "comment": "Accepted to IROS 2025, 7 pages, 4 figures", "summary": "Recent advances in Model Predictive Control (MPC) leveraging a combination of\nfirst-order methods, such as the Alternating Direction Method of Multipliers\n(ADMM), and offline precomputation and caching of select operations, have\nexcitingly enabled real-time MPC on microcontrollers. Unfortunately, these\napproaches require the use of fixed hyperparameters, limiting their\nadaptability and overall performance. In this work, we introduce First-Order\nAdaptive Caching, which precomputes not only select matrix operations but also\ntheir sensitivities to hyperparameter variations, enabling online\nhyperparameter updates without full recomputation of the cache. We demonstrate\nthe effectiveness of our approach on a number of dynamic quadrotor tasks,\nachieving up to a 63.4% reduction in ADMM iterations over the use of optimized\nfixed hyperparameters and approaching 70% of the performance of a full cache\nrecomputation, while reducing the computational cost from O(n^3) to O(n^2)\ncomplexity. This performance enables us to perform figure-eight trajectories on\na 27g tiny quadrotor under wind disturbances. We release our implementation\nopen-source for the benefit of the wider robotics community.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201cFirst-Order Adaptive Caching\u201d\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u9884\u8ba1\u7b97\u77e9\u9635\u64cd\u4f5c\u53ca\u5176\u5bf9\u8d85\u53c2\u6570\u53d8\u5316\u7684\u654f\u611f\u6027\uff0c\u5b9e\u73b0\u5728\u7ebf\u8d85\u53c2\u6570\u66f4\u65b0\uff0c\u663e\u8457\u63d0\u5347\u4e86MPC\u7684\u6027\u80fd\u548c\u9002\u5e94\u6027\u3002", "motivation": "\u73b0\u6709MPC\u65b9\u6cd5\u4f9d\u8d56\u56fa\u5b9a\u8d85\u53c2\u6570\uff0c\u9650\u5236\u4e86\u5176\u9002\u5e94\u6027\u548c\u6027\u80fd\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u7075\u6d3b\u7684\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408\u4e00\u9636\u65b9\u6cd5\u548c\u79bb\u7ebf\u9884\u8ba1\u7b97\uff0c\u9884\u8ba1\u7b97\u77e9\u9635\u64cd\u4f5c\u53ca\u5176\u5bf9\u8d85\u53c2\u6570\u7684\u654f\u611f\u6027\uff0c\u652f\u6301\u5728\u7ebf\u8d85\u53c2\u6570\u66f4\u65b0\u3002", "result": "\u5728\u52a8\u6001\u56db\u65cb\u7ffc\u4efb\u52a1\u4e2d\uff0cADMM\u8fed\u4ee3\u51cf\u5c1163.4%\uff0c\u6027\u80fd\u63a5\u8fd1\u5168\u7f13\u5b58\u91cd\u8ba1\u7b97\u768470%\uff0c\u8ba1\u7b97\u590d\u6742\u5ea6\u4eceO(n^3)\u964d\u81f3O(n^2)\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86MPC\u7684\u5b9e\u65f6\u6027\u548c\u9002\u5e94\u6027\uff0c\u6210\u529f\u5e94\u7528\u4e8e\u5fae\u578b\u56db\u65cb\u7ffc\u98de\u884c\u5668\uff0c\u5e76\u5f00\u6e90\u5b9e\u73b0\u3002"}}
{"id": "2507.03190", "categories": ["cs.AI", "cs.DS", "cs.LG", "es: 68T05, 68T20, 68Q12, 90C27", "I.2.6; I.2.8; F.2.2; F.1.2; G.2.1"], "pdf": "https://arxiv.org/pdf/2507.03190", "abs": "https://arxiv.org/abs/2507.03190", "authors": ["Theo Bourdais", "Abeynaya Gnanasekaran", "Houman Owhadi", "Tuhin Sahai"], "title": "Discovering Algorithms with Computational Language Processing", "comment": "21 pages", "summary": "Algorithms are the engine for reproducible problem-solving. We present a\nframework automating algorithm discovery by conceptualizing them as sequences\nof operations, represented as tokens. These computational tokens are chained\nusing a grammar, enabling the formation of increasingly sophisticated\nprocedures. Our ensemble Monte Carlo tree search (MCTS) guided by reinforcement\nlearning (RL) explores token chaining and drives the creation of new tokens.\nThis methodology rediscovers, improves, and generates new algorithms that\nsubstantially outperform existing methods for strongly NP-hard combinatorial\noptimization problems and foundational quantum computing approaches such as\nGrover's and Quantum Approximate Optimization Algorithm. Operating at the\ncomputational rather than code-generation level, our framework produces\nalgorithms that can be tailored specifically to problem instances, not merely\nclasses.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u7b97\u6cd5\u53d1\u73b0\u7684\u6846\u67b6\uff0c\u5c06\u7b97\u6cd5\u89c6\u4e3a\u64cd\u4f5c\u5e8f\u5217\u7684\u6807\u8bb0\uff0c\u5e76\u901a\u8fc7\u8bed\u6cd5\u94fe\u5f0f\u7ec4\u5408\u8fd9\u4e9b\u6807\u8bb0\uff0c\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u5f15\u5bfc\u7684\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u63a2\u7d22\u6807\u8bb0\u7ec4\u5408\uff0c\u751f\u6210\u65b0\u7b97\u6cd5\u3002", "motivation": "\u89e3\u51b3\u7b97\u6cd5\u53d1\u73b0\u7684\u81ea\u52a8\u5316\u95ee\u9898\uff0c\u63d0\u5347\u7b97\u6cd5\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728NP\u96be\u7ec4\u5408\u4f18\u5316\u548c\u91cf\u5b50\u8ba1\u7b97\u9886\u57df\u3002", "method": "\u5c06\u7b97\u6cd5\u8868\u793a\u4e3a\u6807\u8bb0\u5e8f\u5217\uff0c\u901a\u8fc7\u8bed\u6cd5\u94fe\u5f0f\u7ec4\u5408\u6807\u8bb0\uff0c\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u5f15\u5bfc\u7684\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u63a2\u7d22\u548c\u751f\u6210\u65b0\u7b97\u6cd5\u3002", "result": "\u6846\u67b6\u80fd\u591f\u91cd\u65b0\u53d1\u73b0\u3001\u6539\u8fdb\u5e76\u751f\u6210\u65b0\u7b97\u6cd5\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u9002\u7528\u4e8eNP\u96be\u95ee\u9898\u548c\u91cf\u5b50\u8ba1\u7b97\u9886\u57df\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u8ba1\u7b97\u5c42\u9762\u800c\u975e\u4ee3\u7801\u751f\u6210\u5c42\u9762\u64cd\u4f5c\uff0c\u80fd\u591f\u9488\u5bf9\u5177\u4f53\u95ee\u9898\u5b9e\u4f8b\u751f\u6210\u5b9a\u5236\u5316\u7b97\u6cd5\u3002"}}
{"id": "2507.03365", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.03365", "abs": "https://arxiv.org/abs/2507.03365", "authors": ["Hanfang Liang", "Shenghai Yuan", "Fen Liu", "Yizhuo Yang", "Bing Wang", "Zhuyu Huang", "Chenyang Shi", "Jing Jin"], "title": "Label-Free Long-Horizon 3D UAV Trajectory Prediction via Motion-Aligned RGB and Event Cues", "comment": null, "summary": "The widespread use of consumer drones has introduced serious challenges for\nairspace security and public safety. Their high agility and unpredictable\nmotion make drones difficult to track and intercept. While existing methods\nfocus on detecting current positions, many counter-drone strategies rely on\nforecasting future trajectories and thus require more than reactive detection\nto be effective. To address this critical gap, we propose an unsupervised\nvision-based method for predicting the three-dimensional trajectories of\ndrones. Our approach first uses an unsupervised technique to extract drone\ntrajectories from raw LiDAR point clouds, then aligns these trajectories with\ncamera images through motion consistency to generate reliable pseudo-labels. We\nthen combine kinematic estimation with a visual Mamba neural network in a\nself-supervised manner to predict future drone trajectories. We evaluate our\nmethod on the challenging MMAUD dataset, including the V2 sequences that\nfeature wide-field-of-view multimodal sensors and dynamic UAV motion in urban\nscenes. Extensive experiments show that our framework outperforms supervised\nimage-only and audio-visual baselines in long-horizon trajectory prediction,\nreducing 5-second 3D error by around 40 percent without using any manual 3D\nlabels. The proposed system offers a cost-effective, scalable alternative for\nreal-time counter-drone deployment. All code will be released upon acceptance\nto support reproducible research in the robotics community.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u76d1\u7763\u89c6\u89c9\u65b9\u6cd5\uff0c\u7528\u4e8e\u9884\u6d4b\u65e0\u4eba\u673a\u7684\u4e09\u7ef4\u8f68\u8ff9\uff0c\u7ed3\u5408LiDAR\u70b9\u4e91\u548c\u76f8\u673a\u56fe\u50cf\uff0c\u901a\u8fc7\u81ea\u76d1\u7763\u5b66\u4e60\u663e\u8457\u63d0\u5347\u4e86\u957f\u65f6\u8f68\u8ff9\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u6d88\u8d39\u7ea7\u65e0\u4eba\u673a\u7684\u5e7f\u6cdb\u4f7f\u7528\u5bf9\u7a7a\u57df\u5b89\u5168\u548c\u516c\u5171\u5b89\u5168\u6784\u6210\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u8ddf\u8e2a\u548c\u62e6\u622a\u5176\u9ad8\u673a\u52a8\u6027\u3002", "method": "\u4f7f\u7528\u65e0\u76d1\u7763\u6280\u672f\u4eceLiDAR\u70b9\u4e91\u63d0\u53d6\u8f68\u8ff9\uff0c\u4e0e\u76f8\u673a\u56fe\u50cf\u5bf9\u9f50\u751f\u6210\u4f2a\u6807\u7b7e\uff0c\u7ed3\u5408\u8fd0\u52a8\u4f30\u8ba1\u548c\u89c6\u89c9Mamba\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u81ea\u76d1\u7763\u9884\u6d4b\u3002", "result": "\u5728MMAUD\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c5\u79d23D\u8bef\u5dee\u964d\u4f4e\u7ea640%\uff0c\u65e0\u9700\u4eba\u5de53D\u6807\u7b7e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5b9e\u65f6\u53cd\u65e0\u4eba\u673a\u90e8\u7f72\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u5c06\u5f00\u6e90\u4ee5\u652f\u6301\u53ef\u91cd\u590d\u7814\u7a76\u3002"}}
{"id": "2507.03223", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.03223", "abs": "https://arxiv.org/abs/2507.03223", "authors": ["Jeshwanth Challagundla"], "title": "SI-Agent: An Agentic Framework for Feedback-Driven Generation and Tuning of Human-Readable System Instructions for Large Language Models", "comment": null, "summary": "System Instructions (SIs), or system prompts, are pivotal for guiding Large\nLanguage Models (LLMs) but manual crafting is resource-intensive and often\nsuboptimal. Existing automated methods frequently generate non-human-readable\n\"soft prompts,\" sacrificing interpretability. This paper introduces SI-Agent, a\nnovel agentic framework designed to automatically generate and iteratively\nrefine human-readable SIs through a feedback-driven loop. SI-Agent employs\nthree collaborating agents: an Instructor Agent, an Instruction Follower Agent\n(target LLM), and a Feedback/Reward Agent evaluating task performance and\noptionally SI readability. The framework utilizes iterative cycles where\nfeedback guides the Instructor's refinement strategy (e.g., LLM-based editing,\nevolutionary algorithms). We detail the framework's architecture, agent roles,\nthe iterative refinement process, and contrast it with existing methods. We\npresent experimental results validating SI-Agent's effectiveness, focusing on\nmetrics for task performance, SI readability, and efficiency. Our findings\nindicate that SI-Agent generates effective, readable SIs, offering a favorable\ntrade-off between performance and interpretability compared to baselines.\nPotential implications include democratizing LLM customization and enhancing\nmodel transparency. Challenges related to computational cost and feedback\nreliability are acknowledged.", "AI": {"tldr": "SI-Agent\u662f\u4e00\u4e2a\u81ea\u52a8\u5316\u751f\u6210\u548c\u4f18\u5316\u4eba\u7c7b\u53ef\u8bfb\u7cfb\u7edf\u6307\u4ee4\uff08SIs\uff09\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u53cd\u9988\u9a71\u52a8\u5faa\u73af\u63d0\u5347\u4efb\u52a1\u6027\u80fd\u548c\u53ef\u8bfb\u6027\u3002", "motivation": "\u624b\u52a8\u8bbe\u8ba1\u7cfb\u7edf\u6307\u4ee4\u8d44\u6e90\u5bc6\u96c6\u4e14\u6548\u679c\u4e0d\u4f73\uff0c\u73b0\u6709\u81ea\u52a8\u5316\u65b9\u6cd5\u727a\u7272\u53ef\u8bfb\u6027\uff0cSI-Agent\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "SI-Agent\u91c7\u7528\u4e09\u4e2a\u534f\u4f5c\u4ee3\u7406\uff08Instructor\u3001Follower\u3001Feedback Agent\uff09\uff0c\u901a\u8fc7\u8fed\u4ee3\u53cd\u9988\u4f18\u5316\u6307\u4ee4\uff0c\u7ed3\u5408LLM\u7f16\u8f91\u548c\u8fdb\u5316\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eSI-Agent\u5728\u4efb\u52a1\u6027\u80fd\u3001\u53ef\u8bfb\u6027\u548c\u6548\u7387\u4e0a\u4f18\u4e8e\u57fa\u7ebf\uff0c\u5b9e\u73b0\u4e86\u6027\u80fd\u4e0e\u53ef\u8bfb\u6027\u7684\u5e73\u8861\u3002", "conclusion": "SI-Agent\u4e3aLLM\u5b9a\u5236\u5316\u548c\u900f\u660e\u5ea6\u63d0\u4f9b\u65b0\u9014\u5f84\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u548c\u53cd\u9988\u53ef\u9760\u6027\u4ecd\u9700\u6539\u8fdb\u3002"}}
{"id": "2507.03381", "categories": ["cs.RO", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.03381", "abs": "https://arxiv.org/abs/2507.03381", "authors": ["Maryem Fadili", "Louis Lecrosnier", "Steve Pechberti", "Redouane Khemmar"], "title": "Evaluation of an Uncertainty-Aware Late Fusion Algorithm for Multi-Source Bird's Eye View Detections Under Controlled Noise", "comment": null, "summary": "Reliable multi-source fusion is crucial for robust perception in autonomous\nsystems. However, evaluating fusion performance independently of detection\nerrors remains challenging. This work introduces a systematic evaluation\nframework that injects controlled noise into ground-truth bounding boxes to\nisolate the fusion process. We then propose Unified Kalman Fusion (UniKF), a\nlate-fusion algorithm based on Kalman filtering to merge Bird's Eye View (BEV)\ndetections while handling synchronization issues. Experiments show that UniKF\noutperforms baseline methods across various noise levels, achieving up to 3x\nlower object's positioning and orientation errors and 2x lower dimension\nestimation errors, while maintaining nearperfect precision and recall between\n99.5% and 100%.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5361\u5c14\u66fc\u6ee4\u6ce2\u7684\u665a\u671f\u878d\u5408\u7b97\u6cd5UniKF\uff0c\u901a\u8fc7\u6ce8\u5165\u53d7\u63a7\u566a\u58f0\u8bc4\u4f30\u591a\u6e90\u878d\u5408\u6027\u80fd\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5b9a\u4f4d\u548c\u5c3a\u5bf8\u4f30\u8ba1\u8bef\u5dee\u3002", "motivation": "\u591a\u6e90\u878d\u5408\u5bf9\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u9c81\u68d2\u611f\u77e5\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u72ec\u7acb\u4e8e\u68c0\u6d4b\u9519\u8bef\u8bc4\u4f30\u878d\u5408\u6027\u80fd\u4ecd\u5177\u6311\u6218\u6027\u3002", "method": "\u63d0\u51fa\u7cfb\u7edf\u8bc4\u4f30\u6846\u67b6\uff0c\u6ce8\u5165\u53d7\u63a7\u566a\u58f0\u4ee5\u9694\u79bb\u878d\u5408\u8fc7\u7a0b\uff1b\u8bbe\u8ba1UniKF\u7b97\u6cd5\uff0c\u57fa\u4e8e\u5361\u5c14\u66fc\u6ee4\u6ce2\u5408\u5e76BEV\u68c0\u6d4b\u7ed3\u679c\u5e76\u5904\u7406\u540c\u6b65\u95ee\u9898\u3002", "result": "UniKF\u5728\u591a\u79cd\u566a\u58f0\u6c34\u5e73\u4e0b\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5b9a\u4f4d\u548c\u65b9\u5411\u8bef\u5dee\u964d\u4f4e3\u500d\uff0c\u5c3a\u5bf8\u4f30\u8ba1\u8bef\u5dee\u964d\u4f4e2\u500d\uff0c\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\u63a5\u8fd1\u5b8c\u7f8e\uff0899.5%-100%\uff09\u3002", "conclusion": "UniKF\u901a\u8fc7\u7cfb\u7edf\u8bc4\u4f30\u548c\u4f18\u5316\u878d\u5408\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u6e90\u878d\u5408\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2507.03226", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.03226", "abs": "https://arxiv.org/abs/2507.03226", "authors": ["Congmin Min", "Rhea Mathew", "Joyce Pan", "Sahil Bansal", "Abbas Keshavarzi", "Amar Viswanathan Kannan"], "title": "Efficient Knowledge Graph Construction and Retrieval from Unstructured Text for Large-Scale RAG Systems", "comment": null, "summary": "We propose a scalable and cost-efficient framework for deploying Graph-based\nRetrieval Augmented Generation (GraphRAG) in enterprise environments. While\nGraphRAG has shown promise for multi-hop reasoning and structured retrieval,\nits adoption has been limited by the high computational cost of constructing\nknowledge graphs using large language models (LLMs) and the latency of\ngraph-based retrieval. To address these challenges, we introduce two core\ninnovations: (1) a dependency-based knowledge graph construction pipeline that\nleverages industrial-grade NLP libraries to extract entities and relations from\nunstructured text completely eliminating reliance on LLMs; and (2) a\nlightweight graph retrieval strategy that combines hybrid query node\nidentification with efficient one-hop traversal for high-recall, low-latency\nsubgraph extraction. We evaluate our framework on two SAP datasets focused on\nlegacy code migration and demonstrate strong empirical performance. Our system\nachieves up to 15% and 4.35% improvements over traditional RAG baselines based\non LLM-as-Judge and RAGAS metrics, respectively. Moreover, our dependency-based\nconstruction approach attains 94% of the performance of LLM-generated knowledge\ngraphs (61.87% vs. 65.83%) while significantly reducing cost and improving\nscalability. These results validate the feasibility of deploying GraphRAG\nsystems in real-world, large-scale enterprise applications without incurring\nprohibitive resource requirements paving the way for practical, explainable,\nand domain-adaptable retrieval-augmented reasoning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u6210\u672c\u9ad8\u6548\u7684GraphRAG\u6846\u67b6\uff0c\u901a\u8fc7\u4f9d\u8d56\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u548c\u8f7b\u91cf\u7ea7\u68c0\u7d22\u7b56\u7565\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u548c\u5ef6\u8fdf\uff0c\u5e76\u5728\u4f01\u4e1a\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\u3002", "motivation": "GraphRAG\u5728\u591a\u8df3\u63a8\u7406\u548c\u7ed3\u6784\u5316\u68c0\u7d22\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u5ef6\u8fdf\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "1. \u4f9d\u8d56\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u7ba1\u9053\uff0c\u5229\u7528\u5de5\u4e1a\u7ea7NLP\u5e93\u63d0\u53d6\u5b9e\u4f53\u548c\u5173\u7cfb\uff0c\u65e0\u9700\u4f9d\u8d56LLM\uff1b2. \u8f7b\u91cf\u7ea7\u56fe\u68c0\u7d22\u7b56\u7565\uff0c\u7ed3\u5408\u6df7\u5408\u67e5\u8be2\u8282\u70b9\u8bc6\u522b\u548c\u9ad8\u6548\u5355\u8df3\u904d\u5386\u3002", "result": "\u5728SAP\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0c\u6027\u80fd\u4f18\u4e8e\u4f20\u7edfRAG\u57fa\u7ebf\uff08LLM-as-Judge\u63d0\u534715%\uff0cRAGAS\u63d0\u53474.35%\uff09\uff0c\u4f9d\u8d56\u6784\u5efa\u65b9\u6cd5\u8fbe\u5230LLM\u751f\u6210\u56fe\u8c3194%\u7684\u6027\u80fd\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u6210\u672c\u3002", "conclusion": "\u8be5\u6846\u67b6\u8bc1\u660e\u4e86GraphRAG\u5728\u5927\u89c4\u6a21\u4f01\u4e1a\u5e94\u7528\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u5177\u6709\u5b9e\u9645\u3001\u53ef\u89e3\u91ca\u548c\u9886\u57df\u9002\u5e94\u6027\u5f3a\u7684\u7279\u70b9\u3002"}}
{"id": "2507.03517", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.03517", "abs": "https://arxiv.org/abs/2507.03517", "authors": ["Antonio Gonz\u00e1lez-Morgado", "Sander Smits", "Guillermo Heredia", "Anibal Ollero", "Alexandre Krupa", "Fran\u00e7ois Chaumette", "Fabien Spindler", "Antonio Franchi", "Chiara Gabellieri"], "title": "Multi-robot Aerial Soft Manipulator For Floating Litter Collection", "comment": null, "summary": "Removing floating litter from water bodies is crucial to preserving aquatic\necosystems and preventing environmental pollution. In this work, we present a\nmulti-robot aerial soft manipulator for floating litter collection, leveraging\nthe capabilities of aerial robots. The proposed system consists of two aerial\nrobots connected by a flexible rope manipulator, which collects floating litter\nusing a hook-based tool. Compared to single-aerial-robot solutions, the use of\ntwo aerial robots increases payload capacity and flight endurance while\nreducing the downwash effect at the manipulation point, located at the midpoint\nof the rope. Additionally, we employ an optimization-based rope-shape planner\nto compute the desired rope shape. The planner incorporates an adaptive\nbehavior that maximizes grasping capabilities near the litter while minimizing\nrope tension when farther away. The computed rope shape trajectory is\ncontrolled by a shape visual servoing controller, which approximates the rope\nas a parabola. The complete system is validated in outdoor experiments,\ndemonstrating successful grasping operations. An ablation study highlights how\nthe planner's adaptive mechanism improves the success rate of the operation.\nFurthermore, real-world tests in a water channel confirm the effectiveness of\nour system in floating litter collection. These results demonstrate the\npotential of aerial robots for autonomous litter removal in aquatic\nenvironments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u673a\u5668\u4eba\u7a7a\u4e2d\u8f6f\u64cd\u7eb5\u5668\uff0c\u7528\u4e8e\u6536\u96c6\u6c34\u9762\u6f02\u6d6e\u5783\u573e\uff0c\u901a\u8fc7\u53cc\u673a\u5668\u4eba\u7cfb\u7edf\u63d0\u9ad8\u8d1f\u8f7d\u80fd\u529b\u548c\u98de\u884c\u8010\u529b\uff0c\u5e76\u51cf\u5c11\u4e0b\u6d17\u6548\u5e94\u3002", "motivation": "\u4fdd\u62a4\u6c34\u751f\u751f\u6001\u7cfb\u7edf\u548c\u9632\u6b62\u73af\u5883\u6c61\u67d3\u9700\u8981\u9ad8\u6548\u7684\u6c34\u9762\u5783\u573e\u6e05\u7406\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u4e24\u4e2a\u7a7a\u4e2d\u673a\u5668\u4eba\u8fde\u63a5\u67d4\u6027\u7ef3\u7d22\u64cd\u7eb5\u5668\uff0c\u914d\u5907\u94a9\u5f0f\u5de5\u5177\u6536\u96c6\u5783\u573e\uff0c\u91c7\u7528\u57fa\u4e8e\u4f18\u5316\u7684\u7ef3\u7d22\u5f62\u72b6\u89c4\u5212\u5668\u548c\u89c6\u89c9\u4f3a\u670d\u63a7\u5236\u5668\u3002", "result": "\u6237\u5916\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u7684\u6709\u6548\u6027\uff0c\u81ea\u9002\u5e94\u89c4\u5212\u5668\u63d0\u9ad8\u4e86\u64cd\u4f5c\u6210\u529f\u7387\uff0c\u5b9e\u9645\u6c34\u9053\u6d4b\u8bd5\u8bc1\u5b9e\u4e86\u5783\u573e\u6536\u96c6\u80fd\u529b\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u5c55\u793a\u4e86\u7a7a\u4e2d\u673a\u5668\u4eba\u5728\u81ea\u4e3b\u6e05\u7406\u6c34\u9762\u5783\u573e\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.03254", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.03254", "abs": "https://arxiv.org/abs/2507.03254", "authors": ["Bruce Yang", "Xinfeng He", "Huan Gao", "Yifan Cao", "Xiaofan Li", "David Hsu"], "title": "CodeAgents: A Token-Efficient Framework for Codified Multi-Agent Reasoning in LLMs", "comment": null, "summary": "Effective prompt design is essential for improving the planning capabilities\nof large language model (LLM)-driven agents. However, existing structured\nprompting strategies are typically limited to single-agent, plan-only settings,\nand often evaluate performance solely based on task accuracy - overlooking\ncritical factors such as token efficiency, modularity, and scalability in\nmulti-agent environments. To address these limitations, we introduce\nCodeAgents, a prompting framework that codifies multi-agent reasoning and\nenables structured, token-efficient planning in multi-agent systems. In\nCodeAgents, all components of agent interaction - Task, Plan, Feedback, system\nroles, and external tool invocations - are codified into modular pseudocode\nenriched with control structures (e.g., loops, conditionals), boolean logic,\nand typed variables. This design transforms loosely connected agent plans into\ncohesive, interpretable, and verifiable multi-agent reasoning programs. We\nevaluate the proposed framework across three diverse benchmarks - GAIA,\nHotpotQA, and VirtualHome - using a range of representative LLMs. Results show\nconsistent improvements in planning performance, with absolute gains of 3-36\npercentage points over natural language prompting baselines. On VirtualHome,\nour method achieves a new state-of-the-art success rate of 56%. In addition,\nour approach reduces input and output token usage by 55-87% and 41-70%,\nrespectively, underscoring the importance of token-aware evaluation metrics in\nthe development of scalable multi-agent LLM systems. The code and resources are\navailable at: https://anonymous.4open.science/r/CodifyingAgent-5A86", "AI": {"tldr": "CodeAgents\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u63d0\u793a\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u4f2a\u4ee3\u7801\u63d0\u5347\u89c4\u5212\u548c\u4ee4\u724c\u6548\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u57fa\u7ebf\u3002", "motivation": "\u73b0\u6709\u7ed3\u6784\u5316\u63d0\u793a\u7b56\u7565\u5c40\u9650\u4e8e\u5355\u667a\u80fd\u4f53\u4e14\u4ec5\u5173\u6ce8\u4efb\u52a1\u51c6\u786e\u6027\uff0c\u5ffd\u89c6\u4e86\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e2d\u7684\u4ee4\u724c\u6548\u7387\u3001\u6a21\u5757\u5316\u548c\u53ef\u6269\u5c55\u6027\u3002", "method": "CodeAgents\u5c06\u667a\u80fd\u4f53\u4ea4\u4e92\u7ec4\u4ef6\uff08\u4efb\u52a1\u3001\u8ba1\u5212\u3001\u53cd\u9988\u7b49\uff09\u7f16\u7801\u4e3a\u6a21\u5757\u5316\u4f2a\u4ee3\u7801\uff0c\u5305\u542b\u63a7\u5236\u7ed3\u6784\u548c\u7c7b\u578b\u53d8\u91cf\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u89c4\u5212\u6027\u80fd\u63d0\u53473-36\u4e2a\u767e\u5206\u70b9\uff0c\u4ee4\u724c\u4f7f\u7528\u51cf\u5c1155-87%\uff08\u8f93\u5165\uff09\u548c41-70%\uff08\u8f93\u51fa\uff09\u3002", "conclusion": "CodeAgents\u4e3a\u53ef\u6269\u5c55\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u7684\u89c4\u5212\u6846\u67b6\u3002"}}
{"id": "2507.03806", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.03806", "abs": "https://arxiv.org/abs/2507.03806", "authors": ["Yuta Takahashi", "Hayate Tajima", "Shin-ichiro Sakai"], "title": "Coil Geometry Learning for Short-Range Magnetic Actuation", "comment": null, "summary": "Fuel-free docking is a key operational technology for in-space assembly,\nresupplying space stations, sample return missions, and formation keeping of\nlarge-scale satellite swarms. The use of conventional propulsion systems,\nincluding thrusters, can cause adverse effects at short distances, such as\nsensor contamination, which may lead to the failure of the satellite or onboard\nequipment. The magnetic field interaction control generated by magnetorquers\ncan overcome these weaknesses of propulsion. This actuation enables\nsimultaneous control of attitude and formation control among desired satellite\ngroups. The previous study typically uses the traditional dipole approximation\nmodel of the exact magnetic field to reduce computation cost. However,\nproximity operations often involve relatively short distances between\nsatellites, which can easily compromise the effectiveness of this\napproximation. To avoid model errors that could result in satellite collisions,\nwe utilize a magnetic field model described by Biot-Savart's law, without\ndistance approximations (Near-field model), in consideration of short-distance\noperations. To overcome the high computational cost associated with the coil\ngeometry and relative states information, a learning-based magnetic field\napproximation is derived, and its effectiveness is shown in the docking\nsimulation of target and chaser satellites equipped with electromagnetic coils\non three axes. Our method significantly reduces the computational cost of the\nexact magnetic model and possesses scalability that can accommodate an\nincreasing number of target satellites through parallel processing.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u78c1\u573a\u8fd1\u4f3c\u65b9\u6cd5\uff0c\u7528\u4e8e\u71c3\u6599\u81ea\u7531\u5bf9\u63a5\u4efb\u52a1\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u63a8\u8fdb\u7cfb\u7edf\u5728\u77ed\u8ddd\u79bb\u64cd\u4f5c\u4e2d\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u63a8\u8fdb\u7cfb\u7edf\u5728\u77ed\u8ddd\u79bb\u64cd\u4f5c\u4e2d\u53ef\u80fd\u5bfc\u81f4\u4f20\u611f\u5668\u6c61\u67d3\u7b49\u95ee\u9898\uff0c\u800c\u78c1\u573a\u63a7\u5236\u53ef\u4ee5\u907f\u514d\u8fd9\u4e9b\u7f3a\u9677\u3002\u7136\u800c\uff0c\u4f20\u7edf\u7684\u5076\u6781\u8fd1\u4f3c\u6a21\u578b\u5728\u77ed\u8ddd\u79bb\u4e0b\u6548\u679c\u4e0d\u4f73\uff0c\u53ef\u80fd\u5bfc\u81f4\u536b\u661f\u78b0\u649e\u3002", "method": "\u4f7f\u7528\u57fa\u4e8eBiot-Savart\u5b9a\u5f8b\u7684\u8fd1\u8ddd\u79bb\u78c1\u573a\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u5b66\u4e60\u65b9\u6cd5\u8fd1\u4f3c\u78c1\u573a\uff0c\u4ee5\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "result": "\u5728\u76ee\u6807\u536b\u661f\u548c\u8ffd\u8e2a\u536b\u661f\u7684\u5bf9\u63a5\u6a21\u62df\u4e2d\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u7cbe\u786e\u78c1\u573a\u6a21\u578b\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u5e76\u5177\u5907\u901a\u8fc7\u5e76\u884c\u5904\u7406\u6269\u5c55\u7684\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u71c3\u6599\u81ea\u7531\u5bf9\u63a5\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.03267", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.03267", "abs": "https://arxiv.org/abs/2507.03267", "authors": ["Jie Peng", "Jiarui Ji", "Runlin Lei", "Zhewei Wei", "Yongchao Liu", "Chuntao Hong"], "title": "GDGB: A Benchmark for Generative Dynamic Text-Attributed Graph Learning", "comment": null, "summary": "Dynamic Text-Attributed Graphs (DyTAGs), which intricately integrate\nstructural, temporal, and textual attributes, are crucial for modeling complex\nreal-world systems. However, most of the existing DyTAG datasets exhibit poor\ntextual quality, which severely limits their utility for DyTAG generation tasks\nrequiring semantically rich inputs. Additionally, prior work mainly focuses on\ndiscriminative tasks on DyTAGs, resulting in a lack of standardized task\nformulations and evaluation protocols tailored for DyTAG generation. To address\nthese critical issues, we propose Generative DyTAG Benchmark (GDGB), which\ncomprises eight meticulously curated DyTAG datasets with high-quality textual\nfeatures for both nodes and edges, overcoming limitations of prior datasets.\nBuilding on GDGB, we define two novel DyTAG generation tasks: Transductive\nDynamic Graph Generation (TDGG) and Inductive Dynamic Graph Generation (IDGG).\nTDGG transductively generates a target DyTAG based on the given source and\ndestination node sets, while the more challenging IDGG introduces new node\ngeneration to inductively model the dynamic expansion of real-world graph data.\nTo enable holistic evaluation, we design multifaceted metrics that assess the\nstructural, temporal, and textual quality of the generated DyTAGs. We further\npropose GAG-General, an LLM-based multi-agent generative framework tailored for\nreproducible and robust benchmarking of DyTAG generation. Experimental results\ndemonstrate that GDGB enables rigorous evaluation of TDGG and IDGG, with key\ninsights revealing the critical interplay of structural and textual features in\nDyTAG generation. These findings establish GDGB as a foundational resource for\nadvancing generative DyTAG research and unlocking further practical\napplications in DyTAG generation. GDGB datasets, source codes, and leaderboards\nare available at \\href{https://gdgb-algo.github.io/}{here}.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faGDGB\u57fa\u51c6\uff0c\u89e3\u51b3\u73b0\u6709DyTAG\u6570\u636e\u96c6\u6587\u672c\u8d28\u91cf\u5dee\u548c\u751f\u6210\u4efb\u52a1\u6807\u51c6\u5316\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5b9a\u4e49\u4e86\u4e24\u4e2a\u65b0\u4efb\u52a1\uff08TDGG\u548cIDGG\uff09\uff0c\u5e76\u8bbe\u8ba1\u4e86\u591a\u7ef4\u5ea6\u8bc4\u4f30\u6307\u6807\u548cLLM\u6846\u67b6GAG-General\u3002", "motivation": "\u73b0\u6709DyTAG\u6570\u636e\u96c6\u6587\u672c\u8d28\u91cf\u5dee\uff0c\u4e14\u7f3a\u4e4f\u9488\u5bf9\u751f\u6210\u4efb\u52a1\u7684\u6807\u51c6\u5316\u8bc4\u4f30\u65b9\u6cd5\uff0c\u9650\u5236\u4e86DyTAG\u751f\u6210\u7814\u7a76\u7684\u8fdb\u5c55\u3002", "method": "\u63d0\u51faGDGB\u57fa\u51c6\uff0c\u5305\u542b8\u4e2a\u9ad8\u8d28\u91cf\u6587\u672c\u6570\u636e\u96c6\uff0c\u5b9a\u4e49TDGG\u548cIDGG\u4efb\u52a1\uff0c\u8bbe\u8ba1\u591a\u7ef4\u5ea6\u8bc4\u4f30\u6307\u6807\uff0c\u5e76\u5f00\u53d1LLM\u6846\u67b6GAG-General\u3002", "result": "GDGB\u652f\u6301\u5bf9TDGG\u548cIDGG\u7684\u4e25\u683c\u8bc4\u4f30\uff0c\u63ed\u793a\u4e86\u7ed3\u6784\u548c\u6587\u672c\u7279\u5f81\u5728DyTAG\u751f\u6210\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002", "conclusion": "GDGB\u4e3a\u751f\u6210DyTAG\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\u8d44\u6e90\uff0c\u63a8\u52a8\u4e86\u5b9e\u9645\u5e94\u7528\u7684\u8fdb\u5c55\u3002"}}
{"id": "2507.03878", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.03878", "abs": "https://arxiv.org/abs/2507.03878", "authors": ["Qi Chen", "Rui Liu", "Kangtong Mo", "Boli Zhang", "Dezhi Yu"], "title": "DK-RRT: Deep Koopman RRT for Collision-Aware Motion Planning of Space Manipulators in Dynamic Debris Environments", "comment": null, "summary": "Trajectory planning for robotic manipulators operating in dynamic orbital\ndebris environments poses significant challenges due to complex obstacle\nmovements and uncertainties. This paper presents Deep Koopman RRT (DK-RRT), an\nadvanced collision-aware motion planning framework integrating deep learning\nwith Koopman operator theory and Rapidly-exploring Random Trees (RRT). DK-RRT\nleverages deep neural networks to identify efficient nonlinear embeddings of\ndebris dynamics, enhancing Koopman-based predictions and enabling accurate,\nproactive planning in real-time. By continuously refining predictive models\nthrough online sensor feedback, DK-RRT effectively navigates the manipulator\nthrough evolving obstacle fields. Simulation studies demonstrate DK-RRT's\nsuperior performance in terms of adaptability, robustness, and computational\nefficiency compared to traditional RRT and conventional Koopman-based planning,\nhighlighting its potential for autonomous space manipulation tasks.", "AI": {"tldr": "DK-RRT\u662f\u4e00\u79cd\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u548cKoopman\u7b97\u5b50\u7684\u8f68\u8ff9\u89c4\u5212\u65b9\u6cd5\uff0c\u7528\u4e8e\u52a8\u6001\u8f68\u9053\u788e\u7247\u73af\u5883\u4e2d\u7684\u673a\u5668\u4eba\u64cd\u4f5c\uff0c\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u52a8\u6001\u8f68\u9053\u788e\u7247\u73af\u5883\u4e2d\u7684\u590d\u6742\u969c\u788d\u7269\u8fd0\u52a8\u548c\u4e0d\u786e\u5b9a\u6027\u7ed9\u673a\u5668\u4eba\u8f68\u8ff9\u89c4\u5212\u5e26\u6765\u6311\u6218\u3002", "method": "DK-RRT\u7ed3\u5408\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u3001Koopman\u7b97\u5b50\u7406\u8bba\u548cRRT\uff0c\u901a\u8fc7\u975e\u7ebf\u6027\u5d4c\u5165\u788e\u7247\u52a8\u6001\uff0c\u5b9e\u73b0\u5b9e\u65f6\u9ad8\u6548\u89c4\u5212\u3002", "result": "\u4eff\u771f\u7814\u7a76\u8868\u660e\uff0cDK-RRT\u5728\u9002\u5e94\u6027\u3001\u9c81\u68d2\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u4f18\u4e8e\u4f20\u7edfRRT\u548cKoopman\u65b9\u6cd5\u3002", "conclusion": "DK-RRT\u5728\u81ea\u4e3b\u7a7a\u95f4\u64cd\u4f5c\u4efb\u52a1\u4e2d\u5177\u6709\u6f5c\u5728\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2507.03285", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.03285", "abs": "https://arxiv.org/abs/2507.03285", "authors": ["Jianyu Zhang", "L\u00e9on Bottou"], "title": "Memory Mosaics at scale", "comment": "arXiv admin note: substantial text overlap with arXiv:2504.14751", "summary": "Memory Mosaics [Zhang et al., 2025], networks of associative memories, have\ndemonstrated appealing compositional and in-context learning capabilities on\nmedium-scale networks (GPT-2 scale) and synthetic small datasets. This work\nshows that these favorable properties remain when we scale memory mosaics to\nlarge language model sizes (llama-8B scale) and real-world datasets.\n  To this end, we scale memory mosaics to 10B size, we train them on one\ntrillion tokens, we introduce a couple architectural modifications (\"Memory\nMosaics v2\"), we assess their capabilities across three evaluation dimensions:\ntraining-knowledge storage, new-knowledge storage, and in-context learning.\n  Throughout the evaluation, memory mosaics v2 match transformers on the\nlearning of training knowledge (first dimension) and significantly outperforms\ntransformers on carrying out new tasks at inference time (second and third\ndimensions). These improvements cannot be easily replicated by simply\nincreasing the training data for transformers. A memory mosaics v2 trained on\none trillion tokens still perform better on these tasks than a transformer\ntrained on eight trillion tokens.", "AI": {"tldr": "Memory Mosaics v2\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u89c4\u6a21\uff08\u5982llama-8B\uff09\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5c55\u73b0\u4e86\u4f18\u5f02\u7684\u7ec4\u5408\u5b66\u4e60\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edfTransformer\u6a21\u578b\u3002", "motivation": "\u9a8c\u8bc1Memory Mosaics\u5728\u66f4\u5927\u89c4\u6a21\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u662f\u5426\u4ecd\u4fdd\u6301\u5176\u4f18\u52bf\u3002", "method": "\u5c06Memory Mosaics\u6269\u5c55\u523010B\u89c4\u6a21\uff0c\u8bad\u7ec31\u4e07\u4ebftoken\uff0c\u5e76\u5f15\u5165\u67b6\u6784\u6539\u8fdb\uff08Memory Mosaics v2\uff09\uff0c\u8bc4\u4f30\u5176\u5728\u8bad\u7ec3\u77e5\u8bc6\u5b58\u50a8\u3001\u65b0\u77e5\u8bc6\u5b58\u50a8\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\u4e09\u4e2a\u7ef4\u5ea6\u7684\u80fd\u529b\u3002", "result": "Memory Mosaics v2\u5728\u8bad\u7ec3\u77e5\u8bc6\u5b66\u4e60\u4e0a\u4e0eTransformer\u76f8\u5f53\uff0c\u5728\u65b0\u4efb\u52a1\u63a8\u7406\u80fd\u529b\u4e0a\u663e\u8457\u4f18\u4e8eTransformer\uff0c\u4e14\u65e0\u6cd5\u901a\u8fc7\u589e\u52a0Transformer\u8bad\u7ec3\u6570\u636e\u8f7b\u6613\u590d\u5236\u5176\u4f18\u52bf\u3002", "conclusion": "Memory Mosaics v2\u5728\u5927\u578b\u6a21\u578b\u548c\u771f\u5b9e\u6570\u636e\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728\u63a8\u7406\u80fd\u529b\u4e0a\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2507.03925", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.03925", "abs": "https://arxiv.org/abs/2507.03925", "authors": ["Abhay Negi", "Omey M. Manyar", "Dhanush K. Penmetsa", "Satyandra K. Gupta"], "title": "Accurate Pose Estimation Using Contact Manifold Sampling for Safe Peg-in-Hole Insertion of Complex Geometries", "comment": "Accepted in IEEE CASE 2025 (https://2025.ieeecase.org/). 8 pages with\n  6 figures", "summary": "Robotic assembly of complex, non-convex geometries with tight clearances\nremains a challenging problem, demanding precise state estimation for\nsuccessful insertion. In this work, we propose a novel framework that relies\nsolely on contact states to estimate the full SE(3) pose of a peg relative to a\nhole. Our method constructs an online submanifold of contact states through\nprimitive motions with just 6 seconds of online execution, subsequently mapping\nit to an offline contact manifold for precise pose estimation. We demonstrate\nthat without such state estimation, robots risk jamming and excessive force\napplication, potentially causing damage. We evaluate our approach on five\nindustrially relevant, complex geometries with 0.1 to 1.0 mm clearances,\nachieving a 96.7% success rate - a 6x improvement over primitive-based\ninsertion without state estimation. Additionally, we analyze insertion forces,\nand overall insertion times, showing our method significantly reduces the\naverage wrench, enabling safer and more efficient assembly.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4ec5\u4f9d\u8d56\u63a5\u89e6\u72b6\u6001\u4f30\u8ba1SE(3)\u4f4d\u59ff\u7684\u65b0\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u590d\u6742\u51e0\u4f55\u88c5\u914d\u7684\u6210\u529f\u7387\u3002", "motivation": "\u89e3\u51b3\u975e\u51f8\u51e0\u4f55\u4f53\u5728\u7d27\u5bc6\u95f4\u9699\u4e0b\u7684\u673a\u5668\u4eba\u88c5\u914d\u95ee\u9898\uff0c\u907f\u514d\u56e0\u4f4d\u59ff\u4f30\u8ba1\u4e0d\u51c6\u5bfc\u81f4\u7684\u5361\u987f\u548c\u635f\u574f\u3002", "method": "\u901a\u8fc7\u539f\u59cb\u8fd0\u52a8\u5728\u7ebf\u6784\u5efa\u63a5\u89e6\u72b6\u6001\u5b50\u6d41\u5f62\uff0c\u6620\u5c04\u5230\u79bb\u7ebf\u63a5\u89e6\u6d41\u5f62\u5b9e\u73b0\u7cbe\u786e\u4f4d\u59ff\u4f30\u8ba1\u3002", "result": "\u57280.1\u81f31.0\u6beb\u7c73\u95f4\u9699\u7684\u5de5\u4e1a\u51e0\u4f55\u4f53\u4e0a\u5b9e\u73b096.7%\u6210\u529f\u7387\uff0c\u6bd4\u65e0\u72b6\u6001\u4f30\u8ba1\u65b9\u6cd5\u63d0\u53476\u500d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u88c5\u914d\u529b\u548c\u65f6\u95f4\uff0c\u63d0\u9ad8\u5b89\u5168\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2507.03293", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.03293", "abs": "https://arxiv.org/abs/2507.03293", "authors": ["Anand Gokhale", "Vaibhav Srivastava", "Francesco Bullo"], "title": "LTLCrit: A Temporal Logic-based LLM Critic for Safe and Efficient Embodied Agents", "comment": null, "summary": "Large language models (LLMs) have demonstrated promise in reasoning tasks and\ngeneral decision-making in static environments. In long-term planning tasks,\nhowever, errors tend to accumulate, often leading to unsafe or inefficient\nbehavior, limiting their use in general-purpose settings. We propose a modular\nactor-critic architecture in which an LLM actor is guided by LTLCrit, a\ntrajectory-level LLM critic that communicates via linear temporal logic (LTL).\nOur setup combines the reasoning strengths of language models with the\nguarantees of formal logic. The actor selects high-level actions from natural\nlanguage observations, while the critic analyzes full trajectories and proposes\nnew LTL constraints that shield the actor from future unsafe or inefficient\nbehavior. The architecture supports both fixed, hand-specified safety\nconstraints and adaptive, learned soft constraints that promote long-term\nefficiency. Our architecture is model-agnostic: any LLM-based planner can serve\nas the actor, and LTLCrit serves as a logic-generating wrapper. We formalize\nplanning as graph traversal under symbolic constraints, allowing LTLCrit to\nanalyze failed or suboptimal trajectories and generate new temporal logic rules\nthat improve future behavior. We evaluate our system on the Minecraft\ndiamond-mining benchmark, achieving 100% completion rates and improving\nefficiency compared to baseline LLM planners. Our results suggest that enabling\nLLMs to supervise each other through logic is a powerful and flexible paradigm\nfor safe, generalizable decision making.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u5757\u5316\u7684actor-critic\u67b6\u6784\uff0c\u901a\u8fc7\u7ebf\u6027\u65f6\u5e8f\u903b\u8f91\uff08LTL\uff09\u6307\u5bfcLLM\uff0c\u7ed3\u5408\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u548c\u5f62\u5f0f\u903b\u8f91\u7684\u4fdd\u8bc1\uff0c\u63d0\u5347\u957f\u671f\u89c4\u5212\u4efb\u52a1\u7684\u5b89\u5168\u6027\u548c\u6548\u7387\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u9759\u6001\u73af\u5883\u4e2d\u7684\u63a8\u7406\u548c\u51b3\u7b56\u4efb\u52a1\u4e2d\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u957f\u671f\u89c4\u5212\u4efb\u52a1\u4e2d\u9519\u8bef\u4f1a\u7d2f\u79ef\uff0c\u5bfc\u81f4\u4e0d\u5b89\u5168\u6216\u4f4e\u6548\u884c\u4e3a\uff0c\u9650\u5236\u4e86\u5176\u901a\u7528\u6027\u3002", "method": "\u91c7\u7528\u6a21\u5757\u5316\u67b6\u6784\uff0cLLM actor\u8d1f\u8d23\u4ece\u81ea\u7136\u8bed\u8a00\u89c2\u5bdf\u4e2d\u9009\u62e9\u9ad8\u5c42\u52a8\u4f5c\uff0cLTLCrit critic\u5206\u6790\u5b8c\u6574\u8f68\u8ff9\u5e76\u63d0\u51fa\u65b0\u7684LTL\u7ea6\u675f\uff0c\u907f\u514d\u672a\u6765\u4e0d\u5b89\u5168\u6216\u4f4e\u6548\u884c\u4e3a\u3002\u652f\u6301\u56fa\u5b9a\u5b89\u5168\u7ea6\u675f\u548c\u81ea\u9002\u5e94\u8f6f\u7ea6\u675f\u3002", "result": "\u5728Minecraft\u94bb\u77f3\u6316\u6398\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5b9e\u73b0\u4e86100%\u5b8c\u6210\u7387\uff0c\u5e76\u6bd4\u57fa\u7ebfLLM\u89c4\u5212\u5668\u66f4\u9ad8\u6548\u3002", "conclusion": "\u901a\u8fc7\u903b\u8f91\u8ba9LLM\u76f8\u4e92\u76d1\u7763\u662f\u4e00\u79cd\u5f3a\u5927\u4e14\u7075\u6d3b\u7684\u8303\u5f0f\uff0c\u53ef\u5b9e\u73b0\u5b89\u5168\u3001\u901a\u7528\u7684\u51b3\u7b56\u3002"}}
{"id": "2507.03930", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.03930", "abs": "https://arxiv.org/abs/2507.03930", "authors": ["Liang Heng", "Xiaoqi Li", "Shangqing Mao", "Jiaming Liu", "Ruolin Liu", "Jingli Wei", "Yu-Kai Wang", "Yueru Jia", "Chenyang Gu", "Rui Zhao", "Shanghang Zhang", "Hao Dong"], "title": "RwoR: Generating Robot Demonstrations from Human Hand Collection for Policy Learning without Robot", "comment": null, "summary": "Recent advancements in imitation learning have shown promising results in\nrobotic manipulation, driven by the availability of high-quality training data.\nTo improve data collection efficiency, some approaches focus on developing\nspecialized teleoperation devices for robot control, while others directly use\nhuman hand demonstrations to obtain training data.However, the former requires\nboth a robotic system and a skilled operator, limiting scalability, while the\nlatter faces challenges in aligning the visual gap between human hand\ndemonstrations and the deployed robot observations.To address this, we propose\na human hand data collection system combined with our hand-to-gripper\ngenerative model, which translates human hand demonstrations into robot gripper\ndemonstrations, effectively bridging the observation gap.Specifically, a GoPro\nfisheye camera is mounted on the human wrist to capture human hand\ndemonstrations.We then train a generative model on a self-collected dataset of\npaired human hand and UMI gripper demonstrations, which have been processed\nusing a tailored data pre-processing strategy to ensure alignment in both\ntimestamps and observations.Therefore, given only human hand demonstrations, we\nare able to automatically extract the corresponding SE(3) actions and integrate\nthem with high-quality generated robot demonstrations through our generation\npipeline for training robotic policy model.In experiments, the robust\nmanipulation performance demonstrates not only the quality of the generated\nrobot demonstrations but also the efficiency and practicality of our data\ncollection method.More demonstrations can be found at: https://rwor.github.io/", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4eba\u7c7b\u624b\u90e8\u6570\u636e\u6536\u96c6\u548c\u751f\u6210\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u5c06\u4eba\u7c7b\u624b\u90e8\u6f14\u793a\u8f6c\u5316\u4e3a\u673a\u5668\u4eba\u5939\u6301\u5668\u6f14\u793a\uff0c\u89e3\u51b3\u4e86\u89c6\u89c9\u5bf9\u9f50\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e2d\uff0c\u4e13\u7528\u9065\u64cd\u4f5c\u8bbe\u5907\u9700\u8981\u673a\u5668\u4eba\u7cfb\u7edf\u548c\u719f\u7ec3\u64cd\u4f5c\u5458\uff0c\u800c\u76f4\u63a5\u4f7f\u7528\u4eba\u7c7b\u624b\u90e8\u6f14\u793a\u5b58\u5728\u89c6\u89c9\u5bf9\u9f50\u95ee\u9898\u3002", "method": "\u4f7f\u7528GoPro\u9c7c\u773c\u6444\u50cf\u5934\u91c7\u96c6\u4eba\u7c7b\u624b\u90e8\u6f14\u793a\uff0c\u8bad\u7ec3\u751f\u6210\u6a21\u578b\u5c06\u4eba\u7c7b\u624b\u90e8\u52a8\u4f5c\u8f6c\u5316\u4e3a\u673a\u5668\u4eba\u5939\u6301\u5668\u52a8\u4f5c\uff0c\u5e76\u901a\u8fc7\u9884\u5904\u7406\u786e\u4fdd\u65f6\u95f4\u548c\u89c2\u5bdf\u5bf9\u9f50\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u751f\u6210\u7684\u673a\u5668\u4eba\u6f14\u793a\u8d28\u91cf\u9ad8\uff0c\u6570\u636e\u6536\u96c6\u65b9\u6cd5\u9ad8\u6548\u5b9e\u7528\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u89c9\u5bf9\u9f50\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60\u7684\u6548\u7387\u548c\u8d28\u91cf\u3002"}}
{"id": "2507.03329", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.03329", "abs": "https://arxiv.org/abs/2507.03329", "authors": ["Devendra Patel", "Aaditya Jain", "Jayant Verma", "Divyansh Rajput", "Sunil Mahala", "Ketki Suresh Khapare", "Jayateja Kalla"], "title": "NDAI-NeuroMAP: A Neuroscience-Specific Embedding Model for Domain-Specific Retrieval", "comment": "The document consists of 15 pages in total: the first 13 pages\n  comprise the main paper, while the last two pages contain supplementary\n  material", "summary": "We present NDAI-NeuroMAP, the first neuroscience-domain-specific dense vector\nembedding model engineered for high-precision information retrieval tasks. Our\nmethodology encompasses the curation of an extensive domain-specific training\ncorpus comprising 500,000 carefully constructed triplets\n(query-positive-negative configurations), augmented with 250,000\nneuroscience-specific definitional entries and 250,000 structured\nknowledge-graph triplets derived from authoritative neurological ontologies. We\nemploy a sophisticated fine-tuning approach utilizing the\nFremyCompany/BioLORD-2023 foundation model, implementing a multi-objective\noptimization framework combining contrastive learning with triplet-based metric\nlearning paradigms. Comprehensive evaluation on a held-out test dataset\ncomprising approximately 24,000 neuroscience-specific queries demonstrates\nsubstantial performance improvements over state-of-the-art general-purpose and\nbiomedical embedding models. These empirical findings underscore the critical\nimportance of domain-specific embedding architectures for neuroscience-oriented\nRAG systems and related clinical natural language processing applications.", "AI": {"tldr": "NDAI-NeuroMAP\u662f\u9996\u4e2a\u4e13\u4e3a\u795e\u7ecf\u79d1\u5b66\u9886\u57df\u8bbe\u8ba1\u7684\u9ad8\u7cbe\u5ea6\u4fe1\u606f\u68c0\u7d22\u5bc6\u96c6\u5411\u91cf\u5d4c\u5165\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u76ee\u6807\u4f18\u5316\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u795e\u7ecf\u79d1\u5b66\u9886\u57df\u4fe1\u606f\u68c0\u7d22\u4efb\u52a1\u4e2d\u901a\u7528\u548c\u751f\u7269\u533b\u5b66\u5d4c\u5165\u6a21\u578b\u7684\u4e0d\u8db3\uff0c\u5f3a\u8c03\u9886\u57df\u7279\u5b9a\u5d4c\u5165\u67b6\u6784\u7684\u91cd\u8981\u6027\u3002", "method": "\u5229\u752850\u4e07\u7cbe\u5fc3\u6784\u5efa\u7684\u4e09\u5143\u7ec4\uff08\u67e5\u8be2-\u6b63\u4f8b-\u8d1f\u4f8b\uff09\u300125\u4e07\u795e\u7ecf\u79d1\u5b66\u5b9a\u4e49\u6761\u76ee\u548c25\u4e07\u77e5\u8bc6\u56fe\u8c31\u4e09\u5143\u7ec4\uff0c\u57fa\u4e8eFremyCompany/BioLORD-2023\u6a21\u578b\u8fdb\u884c\u591a\u76ee\u6807\u4f18\u5316\u5fae\u8c03\u3002", "result": "\u57282.4\u4e07\u795e\u7ecf\u79d1\u5b66\u67e5\u8be2\u7684\u6d4b\u8bd5\u96c6\u4e0a\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u901a\u7528\u548c\u751f\u7269\u533b\u5b66\u5d4c\u5165\u6a21\u578b\u3002", "conclusion": "\u9886\u57df\u7279\u5b9a\u5d4c\u5165\u67b6\u6784\u5bf9\u795e\u7ecf\u79d1\u5b66RAG\u7cfb\u7edf\u548c\u4e34\u5e8aNLP\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2507.03934", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.03934", "abs": "https://arxiv.org/abs/2507.03934", "authors": ["Elian Neppel", "Ashutosh Mishra", "Shamistan Karimov", "Kentaro Uno", "Shreya Santra", "Kazuya Yoshida"], "title": "Robust and Modular Multi-Limb Synchronization in Motion Stack for Space Robots with Trajectory Clamping via Hypersphere", "comment": "6 pages, 15 figures. Accepted at IROS 2025 | Video:\n  https://youtu.be/hr_kUrbqnFg | Open source project:\n  http://motion-stack.deditoolbox.fr | Code:\n  https://github.com/2lian/Motion-Stack", "summary": "Modular robotics holds immense potential for space exploration, where\nreliability, repairability, and reusability are critical for cost-effective\nmissions. Coordination between heterogeneous units is paramount for precision\ntasks -- whether in manipulation, legged locomotion, or multi-robot\ninteraction. Such modular systems introduce challenges far exceeding those in\nmonolithic robot architectures. This study presents a robust method for\nsynchronizing the trajectories of multiple heterogeneous actuators, adapting\ndynamically to system variations with minimal system knowledge. This design\nmakes it inherently robot-agnostic, thus highly suited for modularity. To\nensure smooth trajectory adherence, the multidimensional state is constrained\nwithin a hypersphere representing the allowable deviation. The distance metric\ncan be adapted hence, depending on the task and system under control,\ndeformation of the constraint region is possible. This approach is compatible\nwith a wide range of robotic platforms and serves as a core interface for\nMotion-Stack, our new open-source universal framework for limb coordination\n(available at https://github.com/2lian/Motion-Stack ). The method is validated\nby synchronizing the end-effectors of six highly heterogeneous robotic limbs,\nevaluating both trajectory adherence and recovery from significant external\ndisturbances.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5f02\u6784\u6267\u884c\u5668\u8f68\u8ff9\u540c\u6b65\u7684\u9c81\u68d2\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u6a21\u5757\u5316\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u52a8\u6001\u9002\u5e94\u7cfb\u7edf\u53d8\u5316\uff0c\u4e14\u65e0\u9700\u8fc7\u591a\u7cfb\u7edf\u77e5\u8bc6\u3002", "motivation": "\u6a21\u5757\u5316\u673a\u5668\u4eba\u5728\u592a\u7a7a\u63a2\u7d22\u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u5f02\u6784\u5355\u5143\u95f4\u7684\u534f\u8c03\u662f\u7cbe\u786e\u4efb\u52a1\u7684\u5173\u952e\u6311\u6218\u3002", "method": "\u901a\u8fc7\u5c06\u591a\u7ef4\u72b6\u6001\u7ea6\u675f\u5728\u8868\u793a\u5141\u8bb8\u504f\u5dee\u7684\u8d85\u7403\u4f53\u5185\uff0c\u52a8\u6001\u9002\u5e94\u7cfb\u7edf\u53d8\u5316\uff0c\u786e\u4fdd\u8f68\u8ff9\u5e73\u6ed1\u3002", "result": "\u65b9\u6cd5\u5728\u516d\u79cd\u9ad8\u5ea6\u5f02\u6784\u7684\u673a\u5668\u4eba\u80a2\u4f53\u4e0a\u9a8c\u8bc1\uff0c\u6210\u529f\u540c\u6b65\u672b\u7aef\u6267\u884c\u5668\u8f68\u8ff9\u5e76\u6062\u590d\u5916\u90e8\u5e72\u6270\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5177\u6709\u673a\u5668\u4eba\u65e0\u5173\u6027\uff0c\u9002\u7528\u4e8e\u6a21\u5757\u5316\u7cfb\u7edf\uff0c\u5e76\u4f5c\u4e3a\u5f00\u6e90\u6846\u67b6Motion-Stack\u7684\u6838\u5fc3\u63a5\u53e3\u3002"}}
{"id": "2507.03330", "categories": ["cs.AI", "cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.03330", "abs": "https://arxiv.org/abs/2507.03330", "authors": ["Franklin Mingzhe Li", "Kaitlyn Ng", "Bin Zhu", "Patrick Carrington"], "title": "Exploring Object Status Recognition for Recipe Progress Tracking in Non-Visual Cooking", "comment": "ASSETS 2025", "summary": "Cooking plays a vital role in everyday independence and well-being, yet\nremains challenging for people with vision impairments due to limited support\nfor tracking progress and receiving contextual feedback. Object status - the\ncondition or transformation of ingredients and tools - offers a promising but\nunderexplored foundation for context-aware cooking support. In this paper, we\npresent OSCAR (Object Status Context Awareness for Recipes), a technical\npipeline that explores the use of object status recognition to enable recipe\nprogress tracking in non-visual cooking. OSCAR integrates recipe parsing,\nobject status extraction, visual alignment with cooking steps, and time-causal\nmodeling to support real-time step tracking. We evaluate OSCAR on 173\ninstructional videos and a real-world dataset of 12 non-visual cooking sessions\nrecorded by BLV individuals in their homes. Our results show that object status\nconsistently improves step prediction accuracy across vision-language models,\nand reveal key factors that impact performance in real-world conditions, such\nas implicit tasks, camera placement, and lighting. We contribute the pipeline\nof context-aware recipe progress tracking, an annotated real-world non-visual\ncooking dataset, and design insights to guide future context-aware assistive\ncooking systems.", "AI": {"tldr": "OSCAR\u662f\u4e00\u4e2a\u57fa\u4e8e\u7269\u4f53\u72b6\u6001\u8bc6\u522b\u7684\u6280\u672f\u7ba1\u9053\uff0c\u7528\u4e8e\u652f\u6301\u975e\u89c6\u89c9\u70f9\u996a\u4e2d\u7684\u98df\u8c31\u8fdb\u5ea6\u8ddf\u8e2a\uff0c\u901a\u8fc7\u6574\u5408\u98df\u8c31\u89e3\u6790\u3001\u7269\u4f53\u72b6\u6001\u63d0\u53d6\u3001\u89c6\u89c9\u5bf9\u9f50\u548c\u65f6\u95f4\u56e0\u679c\u5efa\u6a21\uff0c\u63d0\u9ad8\u4e86\u6b65\u9aa4\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u70f9\u996a\u5bf9\u65e5\u5e38\u751f\u6d3b\u72ec\u7acb\u6027\u548c\u5e78\u798f\u611f\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5bf9\u89c6\u529b\u969c\u788d\u8005\u6765\u8bf4\uff0c\u7531\u4e8e\u7f3a\u4e4f\u8fdb\u5ea6\u8ddf\u8e2a\u548c\u4e0a\u4e0b\u6587\u53cd\u9988\u7684\u652f\u6301\uff0c\u70f9\u996a\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u7269\u4f53\u72b6\u6001\uff08\u98df\u6750\u548c\u5de5\u5177\u7684\u53d8\u6362\u72b6\u6001\uff09\u4e3a\u4e0a\u4e0b\u6587\u611f\u77e5\u70f9\u996a\u652f\u6301\u63d0\u4f9b\u4e86\u6f5c\u529b\u3002", "method": "OSCAR\u6574\u5408\u4e86\u98df\u8c31\u89e3\u6790\u3001\u7269\u4f53\u72b6\u6001\u63d0\u53d6\u3001\u89c6\u89c9\u5bf9\u9f50\u548c\u65f6\u95f4\u56e0\u679c\u5efa\u6a21\uff0c\u652f\u6301\u5b9e\u65f6\u6b65\u9aa4\u8ddf\u8e2a\u3002\u7814\u7a76\u8bc4\u4f30\u4e86173\u4e2a\u6559\u5b66\u89c6\u9891\u548c12\u4e2a\u7531\u89c6\u529b\u969c\u788d\u8005\u5728\u5bb6\u4e2d\u5f55\u5236\u7684\u771f\u5b9e\u70f9\u996a\u4f1a\u8bdd\u3002", "result": "\u7269\u4f53\u72b6\u6001\u663e\u8457\u63d0\u9ad8\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6b65\u9aa4\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u5e76\u63ed\u793a\u4e86\u5f71\u54cd\u771f\u5b9e\u573a\u666f\u6027\u80fd\u7684\u5173\u952e\u56e0\u7d20\uff08\u5982\u9690\u542b\u4efb\u52a1\u3001\u6444\u50cf\u5934\u4f4d\u7f6e\u548c\u7167\u660e\uff09\u3002", "conclusion": "OSCAR\u4e3a\u4e0a\u4e0b\u6587\u611f\u77e5\u8f85\u52a9\u70f9\u996a\u7cfb\u7edf\u63d0\u4f9b\u4e86\u6280\u672f\u7ba1\u9053\u3001\u771f\u5b9e\u6570\u636e\u96c6\u548c\u8bbe\u8ba1\u89c1\u89e3\uff0c\u63a8\u52a8\u4e86\u975e\u89c6\u89c9\u70f9\u996a\u652f\u6301\u7684\u53d1\u5c55\u3002"}}
{"id": "2507.03992", "categories": ["cs.RO", "cs.SY", "eess.SY", "68T40", "I.2.9"], "pdf": "https://arxiv.org/pdf/2507.03992", "abs": "https://arxiv.org/abs/2507.03992", "authors": ["Shreenabh Agrawal", "Hugo T. M. Kussaba", "Lingyun Chen", "Allen Emmanuel Binny", "Abdalla Swikir", "Pushpak Jagtap", "Sami Haddadin"], "title": "Scalable Learning of High-Dimensional Demonstrations with Composition of Linear Parameter Varying Dynamical Systems", "comment": "Submitted to the 2025 IEEE/RSJ International Conference on\n  Intelligent Robots and Systems (IROS 2025)", "summary": "Learning from Demonstration (LfD) techniques enable robots to learn and\ngeneralize tasks from user demonstrations, eliminating the need for coding\nexpertise among end-users. One established technique to implement LfD in robots\nis to encode demonstrations in a stable Dynamical System (DS). However, finding\na stable dynamical system entails solving an optimization problem with bilinear\nmatrix inequality (BMI) constraints, a non-convex problem which, depending on\nthe number of scalar constraints and variables, demands significant\ncomputational resources and is susceptible to numerical issues such as\nfloating-point errors. To address these challenges, we propose a novel\ncompositional approach that enhances the applicability and scalability of\nlearning stable DSs with BMIs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7ec4\u5408\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u9ad8\u5b66\u4e60\u7a33\u5b9a\u52a8\u6001\u7cfb\u7edf\u7684\u9002\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u4e2d\u7684\u8ba1\u7b97\u8d44\u6e90\u548c\u6570\u503c\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u7684\u52a8\u6001\u7cfb\u7edf\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u89e3\u51b3\u975e\u51f8\u7684BMI\u7ea6\u675f\u4f18\u5316\u95ee\u9898\uff0c\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u5927\u4e14\u6613\u53d7\u6570\u503c\u95ee\u9898\u5f71\u54cd\u3002", "method": "\u91c7\u7528\u4e00\u79cd\u65b0\u9896\u7684\u7ec4\u5408\u65b9\u6cd5\uff0c\u4f18\u5316\u7a33\u5b9a\u52a8\u6001\u7cfb\u7edf\u7684\u5b66\u4e60\u8fc7\u7a0b\u3002", "result": "\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u5b66\u4e60\u7a33\u5b9a\u52a8\u6001\u7cfb\u7edf\u7684\u9002\u7528\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u51cf\u5c11\u4e86\u8ba1\u7b97\u8d1f\u62c5\u548c\u6570\u503c\u95ee\u9898\u3002", "conclusion": "\u63d0\u51fa\u7684\u7ec4\u5408\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u52a8\u6001\u7cfb\u7edf\u5b66\u4e60\u4e2d\u7684\u6311\u6218\uff0c\u4e3a\u673a\u5668\u4eba\u5b66\u4e60\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.03336", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.03336", "abs": "https://arxiv.org/abs/2507.03336", "authors": ["Ashutosh Hathidara", "Julien Yu", "Sebastian Schreiber"], "title": "Disambiguation-Centric Finetuning Makes Enterprise Tool-Calling LLMs More Realistic and Less Risky", "comment": null, "summary": "Large language models (LLMs) are increasingly tasked with invoking enterprise\nAPIs, yet they routinely falter when near-duplicate tools vie for the same user\nintent or when required arguments are left underspecified. We introduce\nDiaFORGE (Dialogue Framework for Organic Response Generation & Evaluation), a\ndisambiguation-centric, three-stage pipeline that (i) synthesizes\npersona-driven, multi-turn dialogues in which the assistant must distinguish\namong highly similar tools, (ii) performs supervised fine-tuning of open-source\nmodels with reasoning traces across 3B - 70B parameters, and (iii) evaluates\nreal-world readiness via a dynamic suite that redeploys each model in a live\nagentic loop and reports end-to-end goal completion alongside conventional\nstatic metrics. On our dynamic benchmark DiaBENCH, models trained with DiaFORGE\nraise tool-invocation success by 27 pp over GPT-4o and by 49 pp over\nClaude-3.5-Sonnet, both under optimized prompting. To spur further research, we\nrelease an open corpus of 5000 production-grade enterprise API specifications\npaired with rigorously validated, disambiguation-focused dialogues, offering a\npractical blueprint for building reliable, enterprise-ready tool-calling\nagents.", "AI": {"tldr": "DiaFORGE\u662f\u4e00\u4e2a\u4e09\u9636\u6bb5\u5bf9\u8bdd\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3LLMs\u5728\u8c03\u7528\u4f01\u4e1aAPI\u65f6\u56e0\u5de5\u5177\u76f8\u4f3c\u6216\u53c2\u6570\u4e0d\u660e\u786e\u800c\u5931\u8d25\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u5408\u6210\u591a\u8f6e\u5bf9\u8bdd\u3001\u76d1\u7763\u5fae\u8c03\u548c\u52a8\u6001\u8bc4\u4f30\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5de5\u5177\u8c03\u7528\u6210\u529f\u7387\u3002", "motivation": "\u89e3\u51b3LLMs\u5728\u4f01\u4e1aAPI\u8c03\u7528\u4e2d\u56e0\u5de5\u5177\u76f8\u4f3c\u6216\u53c2\u6570\u4e0d\u660e\u786e\u800c\u9891\u7e41\u5931\u8d25\u7684\u95ee\u9898\u3002", "method": "\u4e09\u9636\u6bb5\u6d41\u7a0b\uff1a(i)\u5408\u6210\u591a\u8f6e\u5bf9\u8bdd\uff0c(ii)\u76d1\u7763\u5fae\u8c03\u5f00\u6e90\u6a21\u578b\uff0c(iii)\u52a8\u6001\u8bc4\u4f30\u6a21\u578b\u5728\u5b9e\u9645\u73af\u5883\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u5728DiaBENCH\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDiaFORGE\u8bad\u7ec3\u6a21\u578b\u6bd4GPT-4o\u548cClaude-3.5-Sonnet\u5206\u522b\u63d0\u9ad8\u4e8627\u548c49\u4e2a\u767e\u5206\u70b9\u7684\u5de5\u5177\u8c03\u7528\u6210\u529f\u7387\u3002", "conclusion": "DiaFORGE\u4e3a\u6784\u5efa\u53ef\u9760\u7684\u4f01\u4e1a\u7ea7\u5de5\u5177\u8c03\u7528\u4ee3\u7406\u63d0\u4f9b\u4e86\u5b9e\u7528\u65b9\u6848\uff0c\u5e76\u53d1\u5e03\u4e86\u5f00\u653e\u6570\u636e\u96c6\u4ee5\u63a8\u52a8\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2507.04004", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.04004", "abs": "https://arxiv.org/abs/2507.04004", "authors": ["Xiaolei Lang", "Jiajun Lv", "Kai Tang", "Laijian Li", "Jianxin Huang", "Lina Liu", "Yong Liu", "Xingxing Zuo"], "title": "Gaussian-LIC2: LiDAR-Inertial-Camera Gaussian Splatting SLAM", "comment": null, "summary": "This paper proposes an innovative LiDAR-Inertial-Camera SLAM system with 3D\nGaussian Splatting, which is the first to jointly consider visual quality,\ngeometric accuracy, and real-time performance. It robustly and accurately\nestimates poses while building a photo-realistic 3D Gaussian map in real time\nthat enables high-quality novel view RGB and depth rendering. To effectively\naddress under-reconstruction in regions not covered by the LiDAR, we employ a\nlightweight zero-shot depth model that synergistically combines RGB appearance\ncues with sparse LiDAR measurements to generate dense depth maps. The depth\ncompletion enables reliable Gaussian initialization in LiDAR-blind areas,\nsignificantly improving system applicability for sparse LiDAR sensors. To\nenhance geometric accuracy, we use sparse but precise LiDAR depths to supervise\nGaussian map optimization and accelerate it with carefully designed\nCUDA-accelerated strategies. Furthermore, we explore how the incrementally\nreconstructed Gaussian map can improve the robustness of odometry. By tightly\nincorporating photometric constraints from the Gaussian map into the\ncontinuous-time factor graph optimization, we demonstrate improved pose\nestimation under LiDAR degradation scenarios. We also showcase downstream\napplications via extending our elaborate system, including video frame\ninterpolation and fast 3D mesh extraction. To support rigorous evaluation, we\nconstruct a dedicated LiDAR-Inertial-Camera dataset featuring ground-truth\nposes, depth maps, and extrapolated trajectories for assessing out-of-sequence\nnovel view synthesis. Extensive experiments on both public and self-collected\ndatasets demonstrate the superiority and versatility of our system across LiDAR\nsensors with varying sampling densities. Both the dataset and code will be made\npublicly available on project page https://xingxingzuo.github.io/gaussian_lic2.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684LiDAR-\u60ef\u6027-\u76f8\u673aSLAM\u7cfb\u7edf\uff0c\u7ed3\u54083D\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\uff0c\u9996\u6b21\u540c\u65f6\u8003\u8651\u89c6\u89c9\u8d28\u91cf\u3001\u51e0\u4f55\u7cbe\u5ea6\u548c\u5b9e\u65f6\u6027\u80fd\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u4f4d\u59ff\u4f30\u8ba1\u548c\u5b9e\u65f6\u6784\u5efa\u903c\u771f3D\u9ad8\u65af\u5730\u56fe\u3002", "motivation": "\u89e3\u51b3LiDAR\u8986\u76d6\u4e0d\u8db3\u533a\u57df\u7684\u6b20\u91cd\u5efa\u95ee\u9898\uff0c\u63d0\u5347\u7a00\u758fLiDAR\u4f20\u611f\u5668\u7684\u9002\u7528\u6027\uff0c\u540c\u65f6\u4f18\u5316\u51e0\u4f55\u7cbe\u5ea6\u548c\u4f4d\u59ff\u4f30\u8ba1\u9c81\u68d2\u6027\u3002", "method": "\u91c7\u7528\u8f7b\u91cf\u7ea7\u96f6\u6837\u672c\u6df1\u5ea6\u6a21\u578b\u7ed3\u5408RGB\u548c\u7a00\u758fLiDAR\u6570\u636e\u751f\u6210\u5bc6\u96c6\u6df1\u5ea6\u56fe\uff0c\u5229\u7528LiDAR\u6df1\u5ea6\u76d1\u7763\u9ad8\u65af\u5730\u56fe\u4f18\u5316\uff0c\u5e76\u901a\u8fc7CUDA\u52a0\u901f\u7b56\u7565\u63d0\u5347\u6548\u7387\u3002", "result": "\u7cfb\u7edf\u5728\u7a00\u758fLiDAR\u4f20\u611f\u5668\u4e0b\u8868\u73b0\u51fa\u8272\uff0c\u652f\u6301\u9ad8\u8d28\u91cfRGB\u548c\u6df1\u5ea6\u6e32\u67d3\uff0c\u5e76\u5728LiDAR\u9000\u5316\u573a\u666f\u4e2d\u63d0\u5347\u4f4d\u59ff\u4f30\u8ba1\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u5728\u591a\u79cdLiDAR\u5bc6\u5ea6\u4e0b\u5747\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u548c\u591a\u529f\u80fd\u6027\uff0c\u6570\u636e\u96c6\u548c\u4ee3\u7801\u5c06\u516c\u5f00\u4ee5\u652f\u6301\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2507.03347", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.03347", "abs": "https://arxiv.org/abs/2507.03347", "authors": ["Sachith Gunasekara", "Yasiru Ratnayake"], "title": "Effects of structure on reasoning in instance-level Self-Discover", "comment": null, "summary": "The drive for predictable LLM reasoning in their integration with compound\nsystems has popularized structured outputs, yet concerns remain about\nperformance trade-offs compared to unconstrained natural language. At the same\ntime, training on unconstrained Chain of Thought (CoT) traces has brought about\na new class of strong reasoning models that nevertheless present novel compute\nbudget and faithfulness challenges. This paper introduces iSelf-Discover, an\ninstance-level adaptation of the Self-Discover framework, and using it compares\ndynamically generated structured JSON reasoning with its unstructured\ncounterpart. Our empirical evaluation across diverse benchmarks using\nstate-of-the-art open-source models supports a consistent advantage for\nunstructured reasoning. Notably, on the complex MATH benchmark, unstructured\nplans achieved relative performance improvements of up to 18.90\\% over\nstructured approaches. Zero-shot unstructured iSelf-Discover variants are also\nshown to outperform their five-shot structured counterparts, underscoring the\nsignificance of this gap, even when structured plans are dynamically generated\nto ensure reasoning precedes the final answer. We further demonstrate that the\noptimal granularity of plan generation (instance-level vs. task-level) is\ncontext-dependent. These findings invite re-evaluation of the reliance on\nstructured formats for complex problem-solving and how compound systems should\nbe organized.", "AI": {"tldr": "\u8bba\u6587\u6bd4\u8f83\u4e86\u7ed3\u6784\u5316\u4e0e\u975e\u7ed3\u6784\u5316\u63a8\u7406\u5728LLM\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u975e\u7ed3\u6784\u5316\u63a8\u7406\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u5c24\u5176\u662f\u5728MATH\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u63d0\u5347\u8fbe18.90%\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u89e3\u51b3\u7ed3\u6784\u5316\u8f93\u51fa\u5728LLM\u63a8\u7406\u4e2d\u7684\u6027\u80fd\u6298\u8877\u95ee\u9898\uff0c\u5e76\u63a2\u7d22\u975e\u7ed3\u6784\u5316\u63a8\u7406\u7684\u6f5c\u529b\u3002", "method": "\u91c7\u7528iSelf-Discover\u6846\u67b6\uff0c\u52a8\u6001\u751f\u6210\u7ed3\u6784\u5316JSON\u4e0e\u975e\u7ed3\u6784\u5316\u63a8\u7406\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u975e\u7ed3\u6784\u5316\u63a8\u7406\u8868\u73b0\u66f4\u4f18\uff0c\u5c24\u5176\u5728MATH\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u63d0\u5347\u663e\u8457\uff1b\u96f6\u6837\u672c\u975e\u7ed3\u6784\u5316\u63a8\u7406\u751a\u81f3\u4f18\u4e8e\u4e94\u6837\u672c\u7ed3\u6784\u5316\u63a8\u7406\u3002", "conclusion": "\u7814\u7a76\u547c\u5401\u91cd\u65b0\u8bc4\u4f30\u590d\u6742\u95ee\u9898\u89e3\u51b3\u4e2d\u5bf9\u7ed3\u6784\u5316\u683c\u5f0f\u7684\u4f9d\u8d56\uff0c\u5e76\u63a2\u8ba8\u590d\u5408\u7cfb\u7edf\u7684\u7ec4\u7ec7\u65b9\u5f0f\u3002"}}
{"id": "2507.04039", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.04039", "abs": "https://arxiv.org/abs/2507.04039", "authors": ["Lingxiao Guo", "Yue Gao"], "title": "Generalized Locomotion in Out-of-distribution Conditions with Robust Transformer", "comment": null, "summary": "To succeed in the real world, robots must deal with situations that differ\nfrom those seen during training. Those out-of-distribution situations for\nlegged robot mainly include challenging dynamic gaps and perceptual gaps. Here\nwe study the problem of robust locomotion in such novel situations. While\nprevious methods usually rely on designing elaborate training and adaptation\ntechniques, we approach the problem from a network model perspective. Our\napproach, RObust Locomotion Transformer(ROLT),a variation of transformer,could\nachieve robustness in a variety of unseen conditions. ROLT introduces two key\ndesigns: body tokenization and consistent dropout. Body tokenization supports\nknowledge share across different limbs, which boosts generalization ability of\nthe network. Meanwhile, a novel dropout strategy enhances the policy's\nrobustness to unseen perceptual noise. We conduct extensive experiments both on\nquadruped and hexapod robots. Results demonstrate that ROLT is more robust than\nexisting methods. Although trained in only a few dynamic settings, the learned\npolicy generalizes well to multiple unseen dynamic conditions. Additionally,\ndespite training with clean observations, the model handles challenging\ncorruption noise during testing.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u9c81\u68d2\u8fd0\u52a8\u63a7\u5236\u65b9\u6cd5ROLT\uff0c\u901a\u8fc7\u80a2\u4f53\u6807\u8bb0\u5316\u548c\u4e00\u81f4\u6027dropout\u8bbe\u8ba1\uff0c\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5728\u672a\u77e5\u52a8\u6001\u548c\u611f\u77e5\u6761\u4ef6\u4e0b\u7684\u9002\u5e94\u6027\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u5728\u8bad\u7ec3\u5206\u5e03\u5916\u60c5\u51b5\u4e0b\u7684\u9c81\u68d2\u8fd0\u52a8\u95ee\u9898\uff0c\u5c24\u5176\u662f\u52a8\u6001\u548c\u611f\u77e5\u5dee\u5f02\u3002", "method": "\u63d0\u51faROLT\u65b9\u6cd5\uff0c\u7ed3\u5408\u80a2\u4f53\u6807\u8bb0\u5316\u548c\u4e00\u81f4\u6027dropout\uff0c\u589e\u5f3a\u6cdb\u5316\u80fd\u529b\u548c\u6297\u566a\u6027\u3002", "result": "\u5728\u56db\u8db3\u548c\u516d\u8db3\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\uff0cROLT\u5728\u672a\u77e5\u52a8\u6001\u548c\u566a\u58f0\u6761\u4ef6\u4e0b\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "ROLT\u901a\u8fc7\u65b0\u9896\u7684\u7f51\u7edc\u8bbe\u8ba1\u5b9e\u73b0\u4e86\u5bf9\u672a\u77e5\u6761\u4ef6\u7684\u9ad8\u9c81\u68d2\u6027\uff0c\u65e0\u9700\u590d\u6742\u8bad\u7ec3\u6280\u5de7\u3002"}}
{"id": "2507.03407", "categories": ["cs.AI", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2507.03407", "abs": "https://arxiv.org/abs/2507.03407", "authors": ["Junwei Su", "Cheng Xin", "Ao Shang", "Shan Wu", "Zhenzhen Xie", "Ruogu Xiong", "Xiaoyu Xu", "Cheng Zhang", "Guang Chen", "Yau-Tuen Chan", "Guoyi Tang", "Ning Wang", "Yong Xu", "Yibin Feng"], "title": "Artificial intelligence in drug discovery: A comprehensive review with a case study on hyperuricemia, gout arthritis, and hyperuricemic nephropathy", "comment": null, "summary": "This paper systematically reviews recent advances in artificial intelligence\n(AI), with a particular focus on machine learning (ML), across the entire drug\ndiscovery pipeline. Due to the inherent complexity, escalating costs, prolonged\ntimelines, and high failure rates of traditional drug discovery methods, there\nis a critical need to comprehensively understand how AI/ML can be effectively\nintegrated throughout the full process. Currently available literature reviews\noften narrowly focus on specific phases or methodologies, neglecting the\ndependence between key stages such as target identification, hit screening, and\nlead optimization. To bridge this gap, our review provides a detailed and\nholistic analysis of AI/ML applications across these core phases, highlighting\nsignificant methodological advances and their impacts at each stage. We further\nillustrate the practical impact of these techniques through an in-depth case\nstudy focused on hyperuricemia, gout arthritis, and hyperuricemic nephropathy,\nhighlighting real-world successes in molecular target identification and\ntherapeutic candidate discovery. Additionally, we discuss significant\nchallenges facing AI/ML in drug discovery and outline promising future research\ndirections. Ultimately, this review serves as an essential orientation for\nresearchers aiming to leverage AI/ML to overcome existing bottlenecks and\naccelerate drug discovery.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7efc\u8ff0\u4e86AI/ML\u5728\u836f\u7269\u53d1\u73b0\u5168\u6d41\u7a0b\u4e2d\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u586b\u8865\u4e86\u73b0\u6709\u6587\u732e\u5bf9\u5173\u952e\u9636\u6bb5\u4f9d\u8d56\u5173\u7cfb\u7684\u5ffd\u89c6\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86\u5b9e\u9645\u5e94\u7528\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u836f\u7269\u53d1\u73b0\u65b9\u6cd5\u590d\u6742\u3001\u6210\u672c\u9ad8\u3001\u8017\u65f6\u957f\u4e14\u5931\u8d25\u7387\u9ad8\uff0c\u4e9f\u9700\u5168\u9762\u4e86\u89e3AI/ML\u5982\u4f55\u6709\u6548\u6574\u5408\u5230\u5168\u6d41\u7a0b\u4e2d\u3002", "method": "\u8be6\u7ec6\u5206\u6790\u4e86AI/ML\u5728\u76ee\u6807\u8bc6\u522b\u3001\u547d\u4e2d\u7b5b\u9009\u548c\u5148\u5bfc\u4f18\u5316\u7b49\u6838\u5fc3\u9636\u6bb5\u7684\u5e94\u7528\uff0c\u5e76\u7ed3\u5408\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u5b9e\u9645\u6548\u679c\u3002", "result": "\u5c55\u793a\u4e86AI/ML\u5728\u5404\u9636\u6bb5\u7684\u65b9\u6cd5\u5b66\u8fdb\u5c55\u53ca\u5176\u5b9e\u9645\u5f71\u54cd\uff0c\u7279\u522b\u662f\u5728\u9ad8\u5c3f\u9178\u8840\u75c7\u7b49\u75be\u75c5\u4e2d\u7684\u6210\u529f\u5e94\u7528\u3002", "conclusion": "\u672c\u6587\u4e3a\u7814\u7a76\u4eba\u5458\u5229\u7528AI/ML\u514b\u670d\u74f6\u9888\u3001\u52a0\u901f\u836f\u7269\u53d1\u73b0\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2507.04086", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.04086", "abs": "https://arxiv.org/abs/2507.04086", "authors": ["Nigitha Selvaraj", "Alex Mitrevski", "Sebastian Houben"], "title": "Are Learning-Based Approaches Ready for Real-World Indoor Navigation? A Case for Imitation Learning", "comment": "Accepted for publication at the 12th European Conference on Mobile\n  Robots (ECMR 2025)", "summary": "Traditional indoor robot navigation methods provide a reliable solution when\nadapted to constrained scenarios, but lack flexibility or require manual\nre-tuning when deployed in more complex settings. In contrast, learning-based\napproaches learn directly from sensor data and environmental interactions,\nenabling easier adaptability. While significant work has been presented in the\ncontext of learning navigation policies, learning-based methods are rarely\ncompared to traditional navigation methods directly, which is a problem for\ntheir ultimate acceptance in general navigation contexts. In this work, we\nexplore the viability of imitation learning (IL) for indoor navigation, using\nexpert (joystick) demonstrations to train various navigation policy networks\nbased on RGB images, LiDAR, and a combination of both, and we compare our IL\napproach to a traditional potential field-based navigation method. We evaluate\nthe approach on a physical mobile robot platform equipped with a 2D LiDAR and a\ncamera in an indoor university environment. Our multimodal model demonstrates\nsuperior navigation capabilities in most scenarios, but faces challenges in\ndynamic environments, likely due to limited diversity in the demonstrations.\nNevertheless, the ability to learn directly from data and generalise across\nlayouts suggests that IL can be a practical navigation approach, and\npotentially a useful initialisation strategy for subsequent lifelong learning.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u6a21\u4eff\u5b66\u4e60\uff08IL\uff09\u5728\u5ba4\u5185\u5bfc\u822a\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u901a\u8fc7\u4e13\u5bb6\u6f14\u793a\u8bad\u7ec3\u591a\u79cd\u5bfc\u822a\u7b56\u7565\u7f51\u7edc\uff0c\u5e76\u4e0e\u4f20\u7edf\u52bf\u573a\u5bfc\u822a\u65b9\u6cd5\u5bf9\u6bd4\uff0c\u7ed3\u679c\u663e\u793aIL\u5728\u591a\u6a21\u6001\u6570\u636e\u4e0b\u8868\u73b0\u66f4\u4f18\uff0c\u4f46\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5b58\u5728\u6311\u6218\u3002", "motivation": "\u4f20\u7edf\u5ba4\u5185\u673a\u5668\u4eba\u5bfc\u822a\u65b9\u6cd5\u5728\u590d\u6742\u573a\u666f\u4e2d\u7f3a\u4e4f\u7075\u6d3b\u6027\u6216\u9700\u624b\u52a8\u8c03\u6574\uff0c\u800c\u5b66\u4e60\u578b\u65b9\u6cd5\u80fd\u76f4\u63a5\u4ece\u6570\u636e\u4e2d\u5b66\u4e60\uff0c\u4f46\u7f3a\u4e4f\u4e0e\u4f20\u7edf\u65b9\u6cd5\u7684\u76f4\u63a5\u5bf9\u6bd4\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u4f7f\u7528\u4e13\u5bb6\uff08\u64cd\u7eb5\u6746\uff09\u6f14\u793a\u8bad\u7ec3\u57fa\u4e8eRGB\u56fe\u50cf\u3001LiDAR\u53ca\u4e24\u8005\u7ed3\u5408\u7684\u5bfc\u822a\u7b56\u7565\u7f51\u7edc\uff0c\u5e76\u4e0e\u4f20\u7edf\u52bf\u573a\u5bfc\u822a\u65b9\u6cd5\u5bf9\u6bd4\u3002", "result": "\u591a\u6a21\u6001\u6a21\u578b\u5728\u591a\u6570\u573a\u666f\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u4f46\u5728\u52a8\u6001\u73af\u5883\u4e2d\u56e0\u6f14\u793a\u591a\u6837\u6027\u4e0d\u8db3\u800c\u53d7\u9650\u3002", "conclusion": "IL\u53ef\u76f4\u63a5\u4ece\u6570\u636e\u4e2d\u5b66\u4e60\u5e76\u6cdb\u5316\u5230\u4e0d\u540c\u5e03\u5c40\uff0c\u662f\u4e00\u79cd\u5b9e\u7528\u7684\u5bfc\u822a\u65b9\u6cd5\uff0c\u5e76\u53ef\u80fd\u4f5c\u4e3a\u7ec8\u8eab\u5b66\u4e60\u7684\u521d\u59cb\u5316\u7b56\u7565\u3002"}}
{"id": "2507.03409", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.03409", "abs": "https://arxiv.org/abs/2507.03409", "authors": ["Christopher Summerfield", "Lennart Luettgau", "Magda Dubois", "Hannah Rose Kirk", "Kobi Hackenburg", "Catherine Fist", "Katarina Slama", "Nicola Ding", "Rebecca Anselmetti", "Andrew Strait", "Mario Giulianelli", "Cozmin Ududec"], "title": "Lessons from a Chimp: AI \"Scheming\" and the Quest for Ape Language", "comment": null, "summary": "We examine recent research that asks whether current AI systems may be\ndeveloping a capacity for \"scheming\" (covertly and strategically pursuing\nmisaligned goals). We compare current research practices in this field to those\nadopted in the 1970s to test whether non-human primates could master natural\nlanguage. We argue that there are lessons to be learned from that historical\nresearch endeavour, which was characterised by an overattribution of human\ntraits to other agents, an excessive reliance on anecdote and descriptive\nanalysis, and a failure to articulate a strong theoretical framework for the\nresearch. We recommend that research into AI scheming actively seeks to avoid\nthese pitfalls. We outline some concrete steps that can be taken for this\nresearch programme to advance in a productive and scientifically rigorous\nfashion.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u5f53\u524dAI\u7cfb\u7edf\u662f\u5426\u53ef\u80fd\u53d1\u5c55\u51fa\u201c\u9634\u8c0b\u201d\u80fd\u529b\uff08\u6697\u4e2d\u8ffd\u6c42\u672a\u5bf9\u9f50\u76ee\u6807\uff09\uff0c\u5e76\u4e0e1970\u5e74\u4ee3\u975e\u4eba\u7075\u957f\u7c7b\u8bed\u8a00\u7814\u7a76\u5bf9\u6bd4\uff0c\u63d0\u51fa\u907f\u514d\u5386\u53f2\u7814\u7a76\u9677\u9631\u7684\u5efa\u8bae\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u8ba8AI\u662f\u5426\u53ef\u80fd\u5177\u5907\u201c\u9634\u8c0b\u201d\u80fd\u529b\uff0c\u5e76\u501f\u9274\u5386\u53f2\u7814\u7a76\u7ecf\u9a8c\u4ee5\u907f\u514d\u7c7b\u4f3c\u9519\u8bef\u3002", "method": "\u901a\u8fc7\u6bd4\u8f83\u5f53\u524dAI\u7814\u7a76\u4e0e1970\u5e74\u4ee3\u7075\u957f\u7c7b\u8bed\u8a00\u7814\u7a76\u7684\u65b9\u6cd5\uff0c\u5206\u6790\u5176\u5171\u6027\u4e0e\u95ee\u9898\u3002", "result": "\u6307\u51fa\u5f53\u524d\u7814\u7a76\u5b58\u5728\u8fc7\u5ea6\u62df\u4eba\u5316\u3001\u4f9d\u8d56\u8f76\u4e8b\u548c\u63cf\u8ff0\u6027\u5206\u6790\u3001\u7f3a\u4e4f\u7406\u8bba\u6846\u67b6\u7b49\u95ee\u9898\u3002", "conclusion": "\u5efa\u8baeAI\u9634\u8c0b\u7814\u7a76\u5e94\u907f\u514d\u5386\u53f2\u9677\u9631\uff0c\u5e76\u63d0\u51fa\u4e86\u5177\u4f53\u6539\u8fdb\u6b65\u9aa4\uff0c\u4ee5\u63a8\u52a8\u79d1\u5b66\u4e25\u8c28\u7684\u7814\u7a76\u8fdb\u5c55\u3002"}}
{"id": "2507.04140", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.04140", "abs": "https://arxiv.org/abs/2507.04140", "authors": ["Ho Jae Lee", "Se Hwan Jeon", "Sangbae Kim"], "title": "Learning Humanoid Arm Motion via Centroidal Momentum Regularized Multi-Agent Reinforcement Learning", "comment": "8 pages, 10 figures", "summary": "Humans naturally swing their arms during locomotion to regulate whole-body\ndynamics, reduce angular momentum, and help maintain balance. Inspired by this\nprinciple, we present a limb-level multi-agent reinforcement learning (RL)\nframework that enables coordinated whole-body control of humanoid robots\nthrough emergent arm motion. Our approach employs separate actor-critic\nstructures for the arms and legs, trained with centralized critics but\ndecentralized actors that share only base states and centroidal angular\nmomentum (CAM) observations, allowing each agent to specialize in task-relevant\nbehaviors through modular reward design. The arm agent guided by CAM tracking\nand damping rewards promotes arm motions that reduce overall angular momentum\nand vertical ground reaction moments, contributing to improved balance during\nlocomotion or under external perturbations. Comparative studies with\nsingle-agent and alternative multi-agent baselines further validate the\neffectiveness of our approach. Finally, we deploy the learned policy on a\nhumanoid platform, achieving robust performance across diverse locomotion\ntasks, including flat-ground walking, rough terrain traversal, and stair\nclimbing.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u624b\u81c2\u6446\u52a8\u6765\u4f18\u5316\u4eba\u5f62\u673a\u5668\u4eba\u7684\u5168\u8eab\u63a7\u5236\uff0c\u63d0\u5347\u5e73\u8861\u6027\u548c\u8fd0\u52a8\u6027\u80fd\u3002", "motivation": "\u53d7\u4eba\u7c7b\u884c\u8d70\u65f6\u624b\u81c2\u81ea\u7136\u6446\u52a8\u8c03\u8282\u5168\u8eab\u52a8\u529b\u5b66\u548c\u5e73\u8861\u7684\u542f\u53d1\uff0c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u4eba\u5f62\u673a\u5668\u4eba\u7684\u534f\u8c03\u63a7\u5236\u3002", "method": "\u91c7\u7528\u5206\u6563\u5f0f\u6267\u884c-\u96c6\u4e2d\u5f0f\u6279\u8bc4\uff08actor-critic\uff09\u7ed3\u6784\uff0c\u624b\u81c2\u548c\u817f\u90e8\u5206\u522b\u8bad\u7ec3\uff0c\u5171\u4eab\u57fa\u7840\u72b6\u6001\u548c\u89d2\u52a8\u91cf\u89c2\u6d4b\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u5956\u52b1\u8bbe\u8ba1\u5b9e\u73b0\u4efb\u52a1\u76f8\u5173\u884c\u4e3a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u8fd0\u52a8\u4efb\u52a1\uff08\u5982\u5e73\u5730\u884c\u8d70\u3001\u5d0e\u5c96\u5730\u5f62\u548c\u722c\u697c\u68af\uff09\u4e2d\u8868\u73b0\u4f18\u4e8e\u5355\u667a\u80fd\u4f53\u548c\u57fa\u7ebf\u591a\u667a\u80fd\u4f53\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u624b\u81c2\u8fd0\u52a8\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u7684\u5e73\u8861\u6027\u548c\u8fd0\u52a8\u9c81\u68d2\u6027\uff0c\u9002\u7528\u4e8e\u590d\u6742\u73af\u5883\u3002"}}
{"id": "2507.03460", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.03460", "abs": "https://arxiv.org/abs/2507.03460", "authors": ["Weitong Zhang", "Mengyun Qiao", "Chengqi Zang", "Steven Niederer", "Paul M Matthews", "Wenjia Bai", "Bernhard Kainz"], "title": "Multi-Agent Reasoning for Cardiovascular Imaging Phenotype Analysis", "comment": null, "summary": "Identifying the associations between imaging phenotypes and disease risk\nfactors and outcomes is essential for understanding disease mechanisms and\nimproving diagnosis and prognosis models. However, traditional approaches rely\non human-driven hypothesis testing and selection of association factors, often\noverlooking complex, non-linear dependencies among imaging phenotypes and other\nmulti-modal data. To address this, we introduce a Multi-agent Exploratory\nSynergy for the Heart (MESHAgents) framework that leverages large language\nmodels as agents to dynamically elicit, surface, and decide confounders and\nphenotypes in association studies, using cardiovascular imaging as a proof of\nconcept. Specifically, we orchestrate a multi-disciplinary team of AI agents --\nspanning cardiology, biomechanics, statistics, and clinical research -- which\nspontaneously generate and converge on insights through iterative,\nself-organizing reasoning. The framework dynamically synthesizes statistical\ncorrelations with multi-expert consensus, providing an automated pipeline for\nphenome-wide association studies (PheWAS). We demonstrate the system's\ncapabilities through a population-based study of imaging phenotypes of the\nheart and aorta. MESHAgents autonomously uncovered correlations between imaging\nphenotypes and a wide range of non-imaging factors, identifying additional\nconfounder variables beyond standard demographic factors. Validation on\ndiagnosis tasks reveals that MESHAgents-discovered phenotypes achieve\nperformance comparable to expert-selected phenotypes, with mean AUC differences\nas small as -0.004 on disease classification tasks. Notably, the recall score\nimproves for 6 out of 9 disease types. Our framework provides clinically\nrelevant imaging phenotypes with transparent reasoning, offering a scalable\nalternative to expert-driven methods.", "AI": {"tldr": "MESHAgents\u6846\u67b6\u5229\u7528\u591a\u5b66\u79d1AI\u4ee3\u7406\u52a8\u6001\u53d1\u73b0\u5f71\u50cf\u8868\u578b\u4e0e\u75be\u75c5\u98ce\u9669\u56e0\u7d20\u7684\u5173\u8054\uff0c\u63d0\u4f9b\u81ea\u52a8\u5316\u7684PheWAS\u7ba1\u9053\uff0c\u6027\u80fd\u63a5\u8fd1\u4e13\u5bb6\u9009\u62e9\u7684\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u4eba\u5de5\u5047\u8bbe\u6d4b\u8bd5\uff0c\u5ffd\u7565\u590d\u6742\u975e\u7ebf\u6027\u4f9d\u8d56\u5173\u7cfb\uff0c\u9700\u81ea\u52a8\u5316\u5de5\u5177\u63d0\u5347\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "method": "\u591a\u5b66\u79d1AI\u4ee3\u7406\uff08\u5fc3\u810f\u75c5\u5b66\u3001\u751f\u7269\u529b\u5b66\u7b49\uff09\u901a\u8fc7\u81ea\u7ec4\u7ec7\u63a8\u7406\u52a8\u6001\u751f\u6210\u548c\u9a8c\u8bc1\u5173\u8054\uff0c\u7ed3\u5408\u7edf\u8ba1\u4e0e\u4e13\u5bb6\u5171\u8bc6\u3002", "result": "\u5728\u5fc3\u810f\u548c\u4e3b\u52a8\u8109\u5f71\u50cf\u7814\u7a76\u4e2d\uff0cMESHAgents\u53d1\u73b0\u8d85\u51fa\u6807\u51c6\u4eba\u53e3\u56e0\u7d20\u7684\u6df7\u6742\u53d8\u91cf\uff0c\u75be\u75c5\u5206\u7c7b\u4efb\u52a1AUC\u5dee\u5f02\u4ec5-0.004\u3002", "conclusion": "MESHAgents\u63d0\u4f9b\u53ef\u6269\u5c55\u3001\u900f\u660e\u7684\u4e34\u5e8a\u76f8\u5173\u8868\u578b\uff0c\u6027\u80fd\u63a5\u8fd1\u4e13\u5bb6\u65b9\u6cd5\uff0c\u63d0\u5347\u90e8\u5206\u75be\u75c5\u7c7b\u578b\u7684\u53ec\u56de\u7387\u3002"}}
{"id": "2507.04167", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.04167", "abs": "https://arxiv.org/abs/2507.04167", "authors": ["Hasan Seyyedhasani", "Daniel Udekwe", "Muhammad Ali Qadri"], "title": "Comparative Evaluation of VR-Enabled Robots and Human Operators for Targeted Disease Management in Vineyards", "comment": null, "summary": "This study explores the use of immersive virtual reality (VR) as a control\ninterface for agricultural robots in vineyard disease detection and treatment.\nUsing a Unity-ROS simulation, it compares three agents: a human operator, an\nimmersive VR-controlled robot, and a non-immersive VR-controlled robot. During\nthe scanning phase, humans perform best due to agility and control speed.\nHowever, in the treatment phase, immersive VR robots outperform others,\ncompleting tasks up to 65% faster by using stored infection data and optimized\npath planning. In yield-map-based navigation, immersive robots are also 38%\nfaster than humans. Despite slower performance in manual scanning tasks,\nimmersive VR excels in memory-guided, repetitive operations. The study\nhighlights the role of interface design and path optimization, noting\nlimitations in simulation fidelity and generalizability. It concludes that\nimmersive VR has strong potential to enhance efficiency and precision in\nprecision agriculture.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u6c89\u6d78\u5f0fVR\u4f5c\u4e3a\u519c\u4e1a\u673a\u5668\u4eba\u63a7\u5236\u63a5\u53e3\u7684\u6f5c\u529b\uff0c\u5728\u8461\u8404\u56ed\u75c5\u5bb3\u68c0\u6d4b\u4e0e\u6cbb\u7597\u4e2d\uff0c\u6c89\u6d78\u5f0fVR\u673a\u5668\u4eba\u5728\u6cbb\u7597\u9636\u6bb5\u8868\u73b0\u6700\u4f18\uff0c\u6548\u7387\u63d0\u534765%\u3002", "motivation": "\u63a2\u7d22\u6c89\u6d78\u5f0fVR\u5728\u519c\u4e1a\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u7684\u5e94\u7528\uff0c\u4ee5\u63d0\u9ad8\u75c5\u5bb3\u68c0\u6d4b\u4e0e\u6cbb\u7597\u7684\u6548\u7387\u548c\u7cbe\u786e\u6027\u3002", "method": "\u901a\u8fc7Unity-ROS\u6a21\u62df\u6bd4\u8f83\u4eba\u7c7b\u64cd\u4f5c\u5458\u3001\u6c89\u6d78\u5f0fVR\u63a7\u5236\u673a\u5668\u4eba\u548c\u975e\u6c89\u6d78\u5f0fVR\u63a7\u5236\u673a\u5668\u4eba\u5728\u626b\u63cf\u548c\u6cbb\u7597\u9636\u6bb5\u7684\u6027\u80fd\u3002", "result": "\u6c89\u6d78\u5f0fVR\u673a\u5668\u4eba\u5728\u6cbb\u7597\u9636\u6bb5\u6548\u7387\u6700\u9ad8\uff08\u5feb65%\uff09\uff0c\u5728\u5bfc\u822a\u4efb\u52a1\u4e2d\u4e5f\u6bd4\u4eba\u7c7b\u5feb38%\u3002", "conclusion": "\u6c89\u6d78\u5f0fVR\u5728\u7cbe\u51c6\u519c\u4e1a\u4e2d\u5177\u6709\u663e\u8457\u6f5c\u529b\uff0c\u5c24\u5176\u5728\u91cd\u590d\u6027\u4efb\u52a1\u4e2d\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2507.03477", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.03477", "abs": "https://arxiv.org/abs/2507.03477", "authors": ["Kexin Zhu", "Yang Han"], "title": "REAL: Benchmarking Abilities of Large Language Models for Housing Transactions and Services", "comment": null, "summary": "The development of large language models (LLMs) has greatly promoted the\nprogress of chatbot in multiple fields. There is an urgent need to evaluate\nwhether LLMs can play the role of agent in housing transactions and services as\nwell as humans. We present Real Estate Agent Large Language Model Evaluation\n(REAL), the first evaluation suite designed to assess the abilities of LLMs in\nthe field of housing transactions and services. REAL comprises 5,316\nhigh-quality evaluation entries across 4 topics: memory, comprehension,\nreasoning and hallucination. All these entries are organized as 14 categories\nto assess whether LLMs have the knowledge and ability in housing transactions\nand services scenario. Additionally, the REAL is used to evaluate the\nperformance of most advanced LLMs. The experiment results indicate that LLMs\nstill have significant room for improvement to be applied in the real estate\nfield.", "AI": {"tldr": "REAL\u662f\u9996\u4e2a\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u623f\u5730\u4ea7\u4ea4\u6613\u548c\u670d\u52a1\u4e2d\u80fd\u529b\u7684\u8bc4\u6d4b\u5957\u4ef6\uff0c\u5305\u542b5,316\u6761\u9ad8\u8d28\u91cf\u6761\u76ee\uff0c\u8986\u76d64\u4e2a\u4e3b\u9898\u548c14\u4e2a\u7c7b\u522b\u3002\u5b9e\u9a8c\u8868\u660e\uff0cLLMs\u5728\u8be5\u9886\u57df\u4ecd\u6709\u663e\u8457\u6539\u8fdb\u7a7a\u95f4\u3002", "motivation": "\u8bc4\u4f30LLMs\u80fd\u5426\u5728\u623f\u5730\u4ea7\u4ea4\u6613\u548c\u670d\u52a1\u4e2d\u626e\u6f14\u4eba\u7c7b\u4ee3\u7406\u7684\u89d2\u8272\u3002", "method": "\u5f00\u53d1REAL\u8bc4\u6d4b\u5957\u4ef6\uff0c\u5305\u542b5,316\u6761\u6761\u76ee\uff0c\u8986\u76d6\u8bb0\u5fc6\u3001\u7406\u89e3\u3001\u63a8\u7406\u548c\u5e7b\u89c94\u4e2a\u4e3b\u9898\uff0c\u5206\u4e3a14\u4e2a\u7c7b\u522b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5f53\u524dLLMs\u5728\u623f\u5730\u4ea7\u9886\u57df\u7684\u8868\u73b0\u4ecd\u6709\u8f83\u5927\u63d0\u5347\u7a7a\u95f4\u3002", "conclusion": "LLMs\u5728\u623f\u5730\u4ea7\u4ea4\u6613\u548c\u670d\u52a1\u4e2d\u7684\u5e94\u7528\u4ecd\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u3002"}}
{"id": "2507.04184", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.04184", "abs": "https://arxiv.org/abs/2507.04184", "authors": ["Abhijeet Behera", "Sogol Kharrazi", "Erik Frisk", "Maytheewat Aramrattana"], "title": "An improved 2D time-to-collision for articulated vehicles: predicting sideswipe and rear-end collisions", "comment": null, "summary": "Time-to-collision (TTC) is a widely used measure for estimating the time\nuntil a rear-end collision between two vehicles, assuming both maintain\nconstant speeds and headings in the prediction horizon. To also capture\nsideswipe collisions, a two-dimensional extension, TTC$_{\\text{2D}}$, was\nintroduced. However, this formulation assumes both vehicles have the same\nheading and that their headings remain unchanged during the manoeuvre, in\naddition to the standard assumptions on the prediction horizon. Moreover, its\nuse for articulated vehicles like a tractor-semitrailer remains unclear. This\npaper addresses these limitations by developing three enhanced versions of\nTTC$_{\\text{2D}}$. The first incorporates vehicle heading information, which is\nmissing in the original formulation. The standard assumption of constant speed\nand heading in the prediction horizon holds. The second adapts this to\narticulated vehicles while retaining the assumptions of the first version. The\nthird version maintains the constant heading assumption but relaxes the\nconstant speed assumption by allowing constant acceleration. The versions are\ntested in a cut-in scenario using the CARLA simulation environment. They detect\nrear-end collisions, similar to TTC, and moreover, they also identify sideswipe\nrisks, something TTC could not predict.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e09\u79cd\u6539\u8fdb\u7684TTC2D\u65b9\u6cd5\uff0c\u5206\u522b\u5f15\u5165\u8f66\u8f86\u822a\u5411\u4fe1\u606f\u3001\u9002\u5e94\u94f0\u63a5\u5f0f\u8f66\u8f86\u4ee5\u53ca\u653e\u5bbd\u901f\u5ea6\u6052\u5b9a\u5047\u8bbe\uff0c\u4ee5\u66f4\u5168\u9762\u5730\u9884\u6d4b\u78b0\u649e\u98ce\u9669\u3002", "motivation": "\u73b0\u6709TTC2D\u65b9\u6cd5\u5047\u8bbe\u8f66\u8f86\u822a\u5411\u76f8\u540c\u4e14\u6052\u5b9a\uff0c\u4e14\u672a\u8003\u8651\u94f0\u63a5\u5f0f\u8f66\u8f86\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u8303\u56f4\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u5c40\u9650\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u4e09\u79cd\u6539\u8fdb\u7684TTC2D\u7248\u672c\uff1a1) \u5f15\u5165\u822a\u5411\u4fe1\u606f\uff1b2) \u9002\u5e94\u94f0\u63a5\u5f0f\u8f66\u8f86\uff1b3) \u653e\u5bbd\u901f\u5ea6\u6052\u5b9a\u5047\u8bbe\uff0c\u5141\u8bb8\u6052\u5b9a\u52a0\u901f\u5ea6\u3002\u5728CARLA\u4eff\u771f\u73af\u5883\u4e2d\u6d4b\u8bd5\u3002", "result": "\u6539\u8fdb\u7248\u672c\u4e0d\u4ec5\u80fd\u68c0\u6d4b\u8ffd\u5c3e\u78b0\u649e\uff08\u7c7b\u4f3cTTC\uff09\uff0c\u8fd8\u80fd\u8bc6\u522b\u4fa7\u78b0\u98ce\u9669\uff08TTC\u65e0\u6cd5\u9884\u6d4b\uff09\u3002", "conclusion": "\u6539\u8fdb\u7684TTC2D\u65b9\u6cd5\u6269\u5c55\u4e86\u78b0\u649e\u9884\u6d4b\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u66f4\u590d\u6742\u7684\u8f66\u8f86\u7c7b\u578b\u548c\u52a8\u6001\u573a\u666f\u3002"}}
{"id": "2507.03525", "categories": ["cs.AI", "cs.SY", "eess.SY", "I.2; K.6; D.2.9"], "pdf": "https://arxiv.org/pdf/2507.03525", "abs": "https://arxiv.org/abs/2507.03525", "authors": ["David Manheim", "Aidan Homewood"], "title": "Limits of Safe AI Deployment: Differentiating Oversight and Control", "comment": null, "summary": "Oversight and control (collectively, supervision) are often invoked as key\nlevers for ensuring that AI systems are accountable, reliable, and able to\nfulfill governance and management requirements. However, the concepts are\nfrequently conflated or insufficiently distinguished in academic and policy\ndiscourse, undermining efforts to design or evaluate systems that should remain\nunder meaningful human supervision.\n  This paper undertakes a targeted critical review of literature on supervision\noutside of AI, along with a brief summary of past work on the topic related to\nAI. We then differentiate control as being ex-ante or real-time, and\noperational rather than policy or governance. In contrast, oversight is either\na policy and governance function, or is ex-post. We suggest that control aims\nto prevent failures. In contrast, oversight often focuses on detection,\nremediation, or incentives for future prevention; all preventative oversight\nstrategies nonetheless necessitate control.\n  Building on this foundation, we make three contributions. First, we propose a\ntheoretically-informed yet policy-grounded framework that articulates the\nconditions under which each mechanism is possible, where they fall short, and\nwhat is required to make them meaningful in practice. Second, we outline how\nsupervision methods should be documented and integrated into risk management,\nand drawing on the Microsoft Responsible AI Maturity Model, we outline a\nmaturity model for AI supervision. Third, we explicitly highlight some\nboundaries of these mechanisms, including where they apply, where they fail,\nand where it is clear that no existing methods suffice. This foregrounds the\nquestion of whether meaningful supervision is possible in a given deployment\ncontext, and can support regulators, auditors, and practitioners in identifying\nboth present limitations and the need for new conceptual and technical\nadvances.", "AI": {"tldr": "\u8bba\u6587\u533a\u5206\u4e86AI\u7cfb\u7edf\u4e2d\u7684\u76d1\u7763\u4e0e\u63a7\u5236\uff0c\u63d0\u51fa\u7406\u8bba\u6846\u67b6\u548c\u6210\u719f\u5ea6\u6a21\u578b\uff0c\u5e76\u63a2\u8ba8\u5176\u5c40\u9650\u6027\u548c\u9002\u7528\u6027\u3002", "motivation": "\u89e3\u51b3AI\u9886\u57df\u4e2d\u76d1\u7763\u4e0e\u63a7\u5236\u6982\u5ff5\u6df7\u6dc6\u7684\u95ee\u9898\uff0c\u4ee5\u652f\u6301\u6709\u6548\u7684\u6cbb\u7406\u548c\u7ba1\u7406\u3002", "method": "\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\u548c\u7406\u8bba\u5206\u6790\uff0c\u533a\u5206\u76d1\u7763\u4e0e\u63a7\u5236\uff0c\u5e76\u63d0\u51fa\u6846\u67b6\u548c\u6210\u719f\u5ea6\u6a21\u578b\u3002", "result": "\u63d0\u51fa\u4e86\u76d1\u7763\u4e0e\u63a7\u5236\u7684\u533a\u5206\u6846\u67b6\u3001\u6210\u719f\u5ea6\u6a21\u578b\uff0c\u5e76\u660e\u786e\u4e86\u5176\u8fb9\u754c\u548c\u5c40\u9650\u6027\u3002", "conclusion": "\u8bba\u6587\u4e3aAI\u7cfb\u7edf\u7684\u76d1\u7763\u4e0e\u63a7\u5236\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u548c\u5b9e\u8df5\u6307\u5bfc\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2507.04229", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.04229", "abs": "https://arxiv.org/abs/2507.04229", "authors": ["Dianyong Hou", "Chengrui Zhu", "Zhen Zhang", "Zhibin Li", "Chuang Guo", "Yong Liu"], "title": "Efficient Learning of A Unified Policy For Whole-body Manipulation and Locomotion Skills", "comment": null, "summary": "Equipping quadruped robots with manipulators provides unique\nloco-manipulation capabilities, enabling diverse practical applications. This\nintegration creates a more complex system that has increased difficulties in\nmodeling and control. Reinforcement learning (RL) offers a promising solution\nto address these challenges by learning optimal control policies through\ninteraction. Nevertheless, RL methods often struggle with local optima when\nexploring large solution spaces for motion and manipulation tasks. To overcome\nthese limitations, we propose a novel approach that integrates an explicit\nkinematic model of the manipulator into the RL framework. This integration\nprovides feedback on the mapping of the body postures to the manipulator's\nworkspace, guiding the RL exploration process and effectively mitigating the\nlocal optima issue. Our algorithm has been successfully deployed on a\nDeepRobotics X20 quadruped robot equipped with a Unitree Z1 manipulator, and\nextensive experimental results demonstrate the superior performance of this\napproach.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5c06\u663e\u5f0f\u8fd0\u52a8\u5b66\u6a21\u578b\u4e0e\u5f3a\u5316\u5b66\u4e60\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u56db\u8db3\u673a\u5668\u4eba\u914d\u5907\u673a\u68b0\u81c2\u65f6\u7684\u5c40\u90e8\u6700\u4f18\u95ee\u9898\u3002", "motivation": "\u56db\u8db3\u673a\u5668\u4eba\u914d\u5907\u673a\u68b0\u81c2\u589e\u52a0\u4e86\u7cfb\u7edf\u7684\u590d\u6742\u6027\uff0c\u4f20\u7edf\u5efa\u6a21\u548c\u63a7\u5236\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\uff0c\u5f3a\u5316\u5b66\u4e60\u867d\u80fd\u5b66\u4e60\u6700\u4f18\u7b56\u7565\uff0c\u4f46\u5728\u5927\u89e3\u7a7a\u95f4\u4e2d\u6613\u9677\u5165\u5c40\u90e8\u6700\u4f18\u3002", "method": "\u5c06\u673a\u68b0\u81c2\u7684\u663e\u5f0f\u8fd0\u52a8\u5b66\u6a21\u578b\u96c6\u6210\u5230\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u4e2d\uff0c\u901a\u8fc7\u53cd\u9988\u8eab\u4f53\u59ff\u6001\u4e0e\u673a\u68b0\u81c2\u5de5\u4f5c\u7a7a\u95f4\u7684\u6620\u5c04\uff0c\u5f15\u5bfc\u63a2\u7d22\u8fc7\u7a0b\u3002", "result": "\u5728DeepRobotics X20\u56db\u8db3\u673a\u5668\u4eba\u548cUnitree Z1\u673a\u68b0\u81c2\u4e0a\u6210\u529f\u90e8\u7f72\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u6027\u80fd\u4f18\u8d8a\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5c40\u90e8\u6700\u4f18\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u56db\u8db3\u673a\u5668\u4eba\u914d\u5907\u673a\u68b0\u81c2\u7684\u64cd\u63a7\u6027\u80fd\u3002"}}
{"id": "2507.03579", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.03579", "abs": "https://arxiv.org/abs/2507.03579", "authors": ["Riccardo Lo Bianco", "Remco Dijkman", "Wim Nuijten", "Willem van Jaarsveld"], "title": "A Universal Approach to Feature Representation in Dynamic Task Assignment Problems", "comment": null, "summary": "Dynamic task assignment concerns the optimal assignment of resources to tasks\nin a business process. Recently, Deep Reinforcement Learning (DRL) has been\nproposed as the state of the art for solving assignment problems. DRL methods\nusually employ a neural network (NN) as an approximator for the policy\nfunction, which ingests the state of the process and outputs a valuation of the\npossible assignments. However, representing the state and the possible\nassignments so that they can serve as inputs and outputs for a policy NN\nremains an open challenge, especially when tasks or resources have features\nwith an infinite number of possible values. To solve this problem, this paper\nproposes a method for representing and solving assignment problems with\ninfinite state and action spaces. In doing so, it provides three contributions:\n(I) A graph-based feature representation of assignment problems, which we call\nassignment graph; (II) A mapping from marked Colored Petri Nets to assignment\ngraphs; (III) An adaptation of the Proximal Policy Optimization algorithm that\ncan learn to solve assignment problems represented through assignment graphs.\nTo evaluate the proposed representation method, we model three archetypal\nassignment problems ranging from finite to infinite state and action space\ndimensionalities. The experiments show that the method is suitable for\nrepresenting and learning close-to-optimal task assignment policies regardless\nof the state and action space dimensionalities.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u8868\u793a\u548c\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u52a8\u6001\u4efb\u52a1\u5206\u914d\u95ee\u9898\u4e2d\u7684\u65e0\u9650\u72b6\u6001\u548c\u52a8\u4f5c\u7a7a\u95f4\u95ee\u9898\u3002", "motivation": "\u52a8\u6001\u4efb\u52a1\u5206\u914d\u95ee\u9898\u4e2d\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u65e0\u9650\u72b6\u6001\u548c\u52a8\u4f5c\u7a7a\u95f4\uff0c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u56fe\u7684\u7279\u5f81\u8868\u793a\uff08assignment graph\uff09\uff0c\u5c06\u6807\u8bb0\u7684Colored Petri Nets\u6620\u5c04\u5230assignment graph\uff0c\u5e76\u6539\u8fdbProximal Policy Optimization\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u4e0d\u540c\u7ef4\u5ea6\uff08\u6709\u9650\u5230\u65e0\u9650\uff09\u7684\u72b6\u6001\u548c\u52a8\u4f5c\u7a7a\u95f4\uff0c\u5e76\u80fd\u5b66\u4e60\u63a5\u8fd1\u6700\u4f18\u7684\u4efb\u52a1\u5206\u914d\u7b56\u7565\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u52a8\u6001\u4efb\u52a1\u5206\u914d\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u8868\u793a\u548c\u89e3\u51b3\u6846\u67b6\u3002"}}
{"id": "2507.04235", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.04235", "abs": "https://arxiv.org/abs/2507.04235", "authors": ["Kento Kawaharazuka", "Shintaro Inoue", "Yuta Sahara", "Keita Yoneda", "Temma Suzuki", "Kei Okada"], "title": "Design Optimization of Three-Dimensional Wire Arrangement Considering Wire Crossings for Tendon-driven Robots", "comment": "Accepted at IROS2025, Website -\n  https://haraduka.github.io/muscle-3d-opt/ , YouTube -\n  https://www.youtube.com/watch?v=cy510s-kOaY", "summary": "Tendon-driven mechanisms are useful from the perspectives of variable\nstiffness, redundant actuation, and lightweight design, and they are widely\nused, particularly in hands, wrists, and waists of robots. The design of these\nwire arrangements has traditionally been done empirically, but it becomes\nextremely challenging when dealing with complex structures. Various studies\nhave attempted to optimize wire arrangement, but many of them have\noversimplified the problem by imposing conditions such as restricting movements\nto a 2D plane, keeping the moment arm constant, or neglecting wire crossings.\nTherefore, this study proposes a three-dimensional wire arrangement\noptimization that takes wire crossings into account. We explore wire\narrangements through a multi-objective black-box optimization method that\nensures wires do not cross while providing sufficient joint torque along a\ndefined target trajectory. For a 3D link structure, we optimize the wire\narrangement under various conditions, demonstrate its effectiveness, and\ndiscuss the obtained design solutions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u8003\u8651\u5bfc\u7ebf\u4ea4\u53c9\u7684\u4e09\u7ef4\u5bfc\u7ebf\u6392\u5217\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u76ee\u6807\u9ed1\u76d2\u4f18\u5316\u786e\u4fdd\u5bfc\u7ebf\u4e0d\u4ea4\u53c9\u5e76\u63d0\u4f9b\u8db3\u591f\u7684\u5173\u8282\u626d\u77e9\u3002", "motivation": "\u4f20\u7edf\u5bfc\u7ebf\u6392\u5217\u8bbe\u8ba1\u7ecf\u9a8c\u6027\u5f3a\uff0c\u590d\u6742\u7ed3\u6784\u4e0b\u96be\u4ee5\u5904\u7406\uff0c\u4e14\u73b0\u6709\u7814\u7a76\u5e38\u7b80\u5316\u95ee\u9898\uff08\u5982\u9650\u52362D\u5e73\u9762\u3001\u5ffd\u7565\u5bfc\u7ebf\u4ea4\u53c9\u7b49\uff09\u3002", "method": "\u91c7\u7528\u591a\u76ee\u6807\u9ed1\u76d2\u4f18\u5316\u65b9\u6cd5\uff0c\u63a2\u7d22\u5bfc\u7ebf\u6392\u5217\uff0c\u786e\u4fdd\u4e0d\u4ea4\u53c9\u4e14\u6ee1\u8db3\u76ee\u6807\u8f68\u8ff9\u7684\u5173\u8282\u626d\u77e9\u9700\u6c42\u3002", "result": "\u5728\u4e09\u7ef4\u94fe\u63a5\u7ed3\u6784\u4e0b\u4f18\u5316\u5bfc\u7ebf\u6392\u5217\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5e76\u8ba8\u8bba\u4e86\u8bbe\u8ba1\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u590d\u6742\u7ed3\u6784\u4e0b\u7684\u5bfc\u7ebf\u6392\u5217\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2507.03608", "categories": ["cs.AI", "cs.DC", "cs.ET", "cs.NI"], "pdf": "https://arxiv.org/pdf/2507.03608", "abs": "https://arxiv.org/abs/2507.03608", "authors": ["Sarat Ahmad", "Zeinab Nezami", "Maryam Hafeez", "Syed Ali Raza Zaidi"], "title": "Benchmarking Vector, Graph and Hybrid Retrieval Augmented Generation (RAG) Pipelines for Open Radio Access Networks (ORAN)", "comment": null, "summary": "Generative AI (GenAI) is expected to play a pivotal role in enabling\nautonomous optimization in future wireless networks. Within the ORAN\narchitecture, Large Language Models (LLMs) can be specialized to generate xApps\nand rApps by leveraging specifications and API definitions from the RAN\nIntelligent Controller (RIC) platform. However, fine-tuning base LLMs for\ntelecom-specific tasks remains expensive and resource-intensive.\nRetrieval-Augmented Generation (RAG) offers a practical alternative through\nin-context learning, enabling domain adaptation without full retraining. While\ntraditional RAG systems rely on vector-based retrieval, emerging variants such\nas GraphRAG and Hybrid GraphRAG incorporate knowledge graphs or dual retrieval\nstrategies to support multi-hop reasoning and improve factual grounding.\nDespite their promise, these methods lack systematic, metric-driven\nevaluations, particularly in high-stakes domains such as ORAN. In this study,\nwe conduct a comparative evaluation of Vector RAG, GraphRAG, and Hybrid\nGraphRAG using ORAN specifications. We assess performance across varying\nquestion complexities using established generation metrics: faithfulness,\nanswer relevance, context relevance, and factual correctness. Results show that\nboth GraphRAG and Hybrid GraphRAG outperform traditional RAG. Hybrid GraphRAG\nimproves factual correctness by 8%, while GraphRAG improves context relevance\nby 7%.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e86Vector RAG\u3001GraphRAG\u548cHybrid GraphRAG\u5728ORAN\u67b6\u6784\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0GraphRAG\u548cHybrid GraphRAG\u4f18\u4e8e\u4f20\u7edfRAG\u3002", "motivation": "\u751f\u6210\u5f0fAI\u5728\u65e0\u7ebf\u7f51\u7edc\u4f18\u5316\u4e2d\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u6210\u672c\u9ad8\u4e14\u8d44\u6e90\u5bc6\u96c6\uff0cRAG\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u9700\u5b8c\u5168\u91cd\u65b0\u8bad\u7ec3\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u6bd4\u8f83Vector RAG\u3001GraphRAG\u548cHybrid GraphRAG\u5728ORAN\u89c4\u8303\u4e2d\u7684\u8868\u73b0\uff0c\u8bc4\u4f30\u5176\u751f\u6210\u8d28\u91cf\u3002", "result": "GraphRAG\u548cHybrid GraphRAG\u8868\u73b0\u66f4\u4f18\uff0cHybrid GraphRAG\u4e8b\u5b9e\u6b63\u786e\u6027\u63d0\u53478%\uff0cGraphRAG\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\u63d0\u53477%\u3002", "conclusion": "GraphRAG\u548cHybrid GraphRAG\u5728ORAN\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u9002\u5408\u9ad8\u8981\u6c42\u9886\u57df\u3002"}}
{"id": "2507.04240", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.04240", "abs": "https://arxiv.org/abs/2507.04240", "authors": ["Yuankai Zhu", "Wenwu Lu", "Guoqiang Ren", "Yibin Ying", "Stavros Vougioukas", "Chen Peng"], "title": "Optimal Scheduling of a Dual-Arm Robot for Efficient Strawberry Harvesting in Plant Factories", "comment": null, "summary": "Plant factory cultivation is widely recognized for its ability to optimize\nresource use and boost crop yields. To further increase the efficiency in these\nenvironments, we propose a mixed-integer linear programming (MILP) framework\nthat systematically schedules and coordinates dual-arm harvesting tasks,\nminimizing the overall harvesting makespan based on pre-mapped fruit locations.\nSpecifically, we focus on a specialized dual-arm harvesting robot and employ\npose coverage analysis of its end effector to maximize picking reachability.\nAdditionally, we compare the performance of the dual-arm configuration with\nthat of a single-arm vehicle, demonstrating that the dual-arm system can nearly\ndouble efficiency when fruit densities are roughly equal on both sides.\nExtensive simulations show a 10-20% increase in throughput and a significant\nreduction in the number of stops compared to non-optimized methods. These\nresults underscore the advantages of an optimal scheduling approach in\nimproving the scalability and efficiency of robotic harvesting in plant\nfactories.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cdMILP\u6846\u67b6\uff0c\u4f18\u5316\u53cc\u81c2\u91c7\u6458\u673a\u5668\u4eba\u7684\u4efb\u52a1\u8c03\u5ea6\uff0c\u663e\u8457\u63d0\u5347\u690d\u7269\u5de5\u5382\u7684\u91c7\u6458\u6548\u7387\u3002", "motivation": "\u690d\u7269\u5de5\u5382\u9700\u8981\u66f4\u9ad8\u6548\u7684\u91c7\u6458\u65b9\u6cd5\u4ee5\u4f18\u5316\u8d44\u6e90\u5229\u7528\u548c\u4f5c\u7269\u4ea7\u91cf\u3002", "method": "\u91c7\u7528\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\uff08MILP\uff09\u6846\u67b6\uff0c\u7ed3\u5408\u53cc\u81c2\u91c7\u6458\u673a\u5668\u4eba\u7684\u4f4d\u59ff\u8986\u76d6\u5206\u6790\uff0c\u6700\u5927\u5316\u91c7\u6458\u53ef\u8fbe\u6027\u3002", "result": "\u53cc\u81c2\u7cfb\u7edf\u6548\u7387\u63a5\u8fd1\u5355\u81c2\u7684\u4e24\u500d\uff0c\u6a21\u62df\u663e\u793a\u541e\u5410\u91cf\u63d0\u534710-20%\uff0c\u4e14\u505c\u6b62\u6b21\u6570\u663e\u8457\u51cf\u5c11\u3002", "conclusion": "\u4f18\u5316\u8c03\u5ea6\u65b9\u6cd5\u53ef\u663e\u8457\u63d0\u5347\u690d\u7269\u5de5\u5382\u4e2d\u673a\u5668\u4eba\u91c7\u6458\u7684\u53ef\u6269\u5c55\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2507.03616", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.03616", "abs": "https://arxiv.org/abs/2507.03616", "authors": ["Yingxu Wang", "Siwei Liu", "Jinyuan Fang", "Zaiqiao Meng"], "title": "EvoAgentX: An Automated Framework for Evolving Agentic Workflows", "comment": null, "summary": "Multi-agent systems (MAS) have emerged as a powerful paradigm for\norchestrating large language models (LLMs) and specialized tools to\ncollaboratively address complex tasks. However, existing MAS frameworks often\nrequire manual workflow configuration and lack native support for dynamic\nevolution and performance optimization. In addition, many MAS optimization\nalgorithms are not integrated into a unified framework. In this paper, we\npresent EvoAgentX, an open-source platform that automates the generation,\nexecution, and evolutionary optimization of multi-agent workflows. EvoAgentX\nemploys a modular architecture consisting of five core layers: the basic\ncomponents, agent, workflow, evolving, and evaluation layers. Specifically,\nwithin the evolving layer, EvoAgentX integrates three MAS optimization\nalgorithms, TextGrad, AFlow, and MIPRO, to iteratively refine agent prompts,\ntool configurations, and workflow topologies. We evaluate EvoAgentX on\nHotPotQA, MBPP, and MATH for multi-hop reasoning, code generation, and\nmathematical problem solving, respectively, and further assess it on real-world\ntasks using GAIA. Experimental results show that EvoAgentX consistently\nachieves significant performance improvements, including a 7.44% increase in\nHotPotQA F1, a 10.00% improvement in MBPP pass@1, a 10.00% gain in MATH solve\naccuracy, and an overall accuracy improvement of up to 20.00% on GAIA. The\nsource code is available at: https://github.com/EvoAgentX/EvoAgentX", "AI": {"tldr": "EvoAgentX\u662f\u4e00\u4e2a\u5f00\u6e90\u5e73\u53f0\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u751f\u6210\u3001\u6267\u884c\u548c\u4f18\u5316\u591a\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\uff0c\u901a\u8fc7\u96c6\u6210\u591a\u79cd\u4f18\u5316\u7b97\u6cd5\u663e\u8457\u63d0\u5347\u4efb\u52a1\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08MAS\uff09\u6846\u67b6\u9700\u8981\u624b\u52a8\u914d\u7f6e\u5de5\u4f5c\u6d41\uff0c\u4e14\u7f3a\u4e4f\u52a8\u6001\u6f14\u5316\u548c\u6027\u80fd\u4f18\u5316\u7684\u539f\u751f\u652f\u6301\uff0cEvoAgentX\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "EvoAgentX\u91c7\u7528\u4e94\u5c42\u6a21\u5757\u5316\u67b6\u6784\uff0c\u5305\u62ec\u57fa\u7840\u7ec4\u4ef6\u3001\u667a\u80fd\u4f53\u3001\u5de5\u4f5c\u6d41\u3001\u6f14\u5316\u548c\u8bc4\u4f30\u5c42\uff0c\u5e76\u96c6\u6210TextGrad\u3001AFlow\u548cMIPRO\u4e09\u79cd\u4f18\u5316\u7b97\u6cd5\u3002", "result": "\u5728HotPotQA\u3001MBPP\u3001MATH\u548cGAIA\u7b49\u4efb\u52a1\u4e0a\uff0cEvoAgentX\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u5982HotPotQA F1\u63d0\u9ad87.44%\uff0cMBPP pass@1\u63d0\u534710.00%\u3002", "conclusion": "EvoAgentX\u901a\u8fc7\u81ea\u52a8\u5316\u5de5\u4f5c\u6d41\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u4e3a\u590d\u6742\u4efb\u52a1\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.04263", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.04263", "abs": "https://arxiv.org/abs/2507.04263", "authors": ["Liwen Xiao", "Zhiyu Pan", "Zhicheng Wang", "Zhiguo Cao", "Wei Li"], "title": "SRefiner: Soft-Braid Attention for Multi-Agent Trajectory Refinement", "comment": null, "summary": "Accurate prediction of multi-agent future trajectories is crucial for\nautonomous driving systems to make safe and efficient decisions. Trajectory\nrefinement has emerged as a key strategy to enhance prediction accuracy.\nHowever, existing refinement methods often overlook the topological\nrelationships between trajectories, which are vital for improving prediction\nprecision. Inspired by braid theory, we propose a novel trajectory refinement\napproach, Soft-Braid Refiner (SRefiner), guided by the soft-braid topological\nstructure of trajectories using Soft-Braid Attention. Soft-Braid Attention\ncaptures spatio-temporal topological relationships between trajectories by\nconsidering both spatial proximity and vehicle motion states at ``soft\nintersection points\". Additionally, we extend this approach to model\ninteractions between trajectories and lanes, further improving the prediction\naccuracy. SRefiner is a multi-iteration, multi-agent framework that iteratively\nrefines trajectories, incorporating topological information to enhance\ninteractions within traffic scenarios. SRefiner achieves significant\nperformance improvements over four baseline methods across two datasets,\nestablishing a new state-of-the-art in trajectory refinement. Code is here\nhttps://github.com/Liwen-Xiao/SRefiner.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8f6f\u8fab\u62d3\u6251\u7ed3\u6784\u7684\u8f68\u8ff9\u7ec6\u5316\u65b9\u6cd5SRefiner\uff0c\u901a\u8fc7Soft-Braid Attention\u6355\u6349\u8f68\u8ff9\u95f4\u7684\u65f6\u7a7a\u62d3\u6251\u5173\u7cfb\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u667a\u80fd\u4f53\u8f68\u8ff9\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u8f68\u8ff9\u7ec6\u5316\u65b9\u6cd5\u5e38\u5ffd\u7565\u8f68\u8ff9\u95f4\u7684\u62d3\u6251\u5173\u7cfb\uff0c\u800c\u8fd9\u5bf9\u9884\u6d4b\u7cbe\u5ea6\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51faSRefiner\u6846\u67b6\uff0c\u5229\u7528Soft-Braid Attention\u5efa\u6a21\u8f68\u8ff9\u95f4\u7684\u65f6\u7a7a\u62d3\u6251\u5173\u7cfb\uff0c\u5e76\u6269\u5c55\u5230\u8f68\u8ff9\u4e0e\u8f66\u9053\u7684\u4ea4\u4e92\u3002", "result": "\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u56db\u79cd\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8fbe\u5230\u65b0\u7684\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "SRefiner\u901a\u8fc7\u5f15\u5165\u62d3\u6251\u5173\u7cfb\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8f68\u8ff9\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u5b89\u5168\u7684\u51b3\u7b56\u652f\u6301\u3002"}}
{"id": "2507.03637", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.03637", "abs": "https://arxiv.org/abs/2507.03637", "authors": ["Francesca Da Ros", "Michael Soprano", "Luca Di Gaspero", "Kevin Roitero"], "title": "Large Language Models for Combinatorial Optimization: A Systematic Review", "comment": null, "summary": "This systematic review explores the application of Large Language Models\n(LLMs) in Combinatorial Optimization (CO). We report our findings using the\nPreferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA)\nguidelines. We conduct a literature search via Scopus and Google Scholar,\nexamining over 2,000 publications. We assess publications against four\ninclusion and four exclusion criteria related to their language, research\nfocus, publication year, and type. Eventually, we select 103 studies. We\nclassify these studies into semantic categories and topics to provide a\ncomprehensive overview of the field, including the tasks performed by LLMs, the\narchitectures of LLMs, the existing datasets specifically designed for\nevaluating LLMs in CO, and the field of application. Finally, we identify\nfuture directions for leveraging LLMs in this field.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u7cfb\u7edf\u7efc\u8ff0\u63a2\u8ba8\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u7ec4\u5408\u4f18\u5316\uff08CO\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u7b5b\u9009\u4e86103\u9879\u7814\u7a76\u5e76\u5206\u7c7b\uff0c\u603b\u7ed3\u4e86\u4efb\u52a1\u3001\u67b6\u6784\u3001\u6570\u636e\u96c6\u548c\u5e94\u7528\u9886\u57df\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u7814\u7a76LLMs\u5728CO\u4e2d\u7684\u5e94\u7528\uff0c\u4ee5\u586b\u8865\u8be5\u9886\u57df\u7684\u77e5\u8bc6\u7a7a\u767d\u5e76\u63d0\u4f9b\u5168\u9762\u6982\u8ff0\u3002", "method": "\u91c7\u7528PRISMA\u6307\u5357\u8fdb\u884c\u6587\u732e\u641c\u7d22\u548c\u7b5b\u9009\uff0c\u901a\u8fc7Scopus\u548cGoogle Scholar\u68c0\u7d222000\u591a\u7bc7\u6587\u732e\uff0c\u6700\u7ec8\u9009\u62e9103\u9879\u7814\u7a76\u8fdb\u884c\u5206\u7c7b\u5206\u6790\u3002", "result": "\u603b\u7ed3\u4e86LLMs\u5728CO\u4e2d\u7684\u4efb\u52a1\u3001\u67b6\u6784\u3001\u6570\u636e\u96c6\u548c\u5e94\u7528\u9886\u57df\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "LLMs\u5728CO\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u672a\u6765\u9700\u8fdb\u4e00\u6b65\u63a2\u7d22\u5176\u5e94\u7528\u548c\u4f18\u5316\u65b9\u6cd5\u3002"}}
{"id": "2507.04293", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.04293", "abs": "https://arxiv.org/abs/2507.04293", "authors": ["Weixing Chen", "Dafeng Chi", "Yang Liu", "Yuxi Yang", "Yexin Zhang", "Yuzheng Zhuang", "Xingyue Quan", "Jianye Hao", "Guanbin Li", "Liang Lin"], "title": "AutoLayout: Closed-Loop Layout Synthesis via Slow-Fast Collaborative Reasoning", "comment": null, "summary": "The automated generation of layouts is vital for embodied intelligence and\nautonomous systems, supporting applications from virtual environment\nconstruction to home robot deployment. Current approaches, however, suffer from\nspatial hallucination and struggle with balancing semantic fidelity and\nphysical plausibility, often producing layouts with deficits such as floating\nor overlapping objects and misaligned stacking relation. In this paper, we\npropose AutoLayout, a fully automated method that integrates a closed-loop\nself-validation process within a dual-system framework. Specifically, a slow\nsystem harnesses detailed reasoning with a Reasoning-Reflection-Generation\n(RRG) pipeline to extract object attributes and spatial constraints. Then, a\nfast system generates discrete coordinate sets and a topological relation set\nthat are jointly validated. To mitigate the limitations of handcrafted rules,\nwe further introduce an LLM-based Adaptive Relation Library (ARL) for\ngenerating and evaluating layouts. Through the implementation of Slow-Fast\nCollaborative Reasoning, the AutoLayout efficiently generates layouts after\nthorough deliberation, effectively mitigating spatial hallucination. Its\nself-validation mechanism establishes a closed-loop process that iteratively\ncorrects potential errors, achieving a balance between physical stability and\nsemantic consistency. The effectiveness of AutoLayout was validated across 8\ndistinct scenarios, where it demonstrated a significant 10.1% improvement over\nSOTA methods in terms of physical plausibility, semantic consistency, and\nfunctional completeness.", "AI": {"tldr": "AutoLayout\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u7cfb\u7edf\u6846\u67b6\uff0c\u901a\u8fc7\u6162\u901f\u7cfb\u7edf\uff08RRG\u7ba1\u9053\uff09\u548c\u5feb\u901f\u7cfb\u7edf\u534f\u540c\u5de5\u4f5c\uff0c\u7ed3\u5408LLM\u81ea\u9002\u5e94\u5173\u7cfb\u5e93\uff08ARL\uff09\uff0c\u6709\u6548\u51cf\u5c11\u7a7a\u95f4\u5e7b\u89c9\uff0c\u63d0\u5347\u5e03\u5c40\u7684\u7269\u7406\u5408\u7406\u6027\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "motivation": "\u5f53\u524d\u5e03\u5c40\u751f\u6210\u65b9\u6cd5\u5b58\u5728\u7a7a\u95f4\u5e7b\u89c9\u95ee\u9898\uff0c\u96be\u4ee5\u5e73\u8861\u8bed\u4e49\u4fdd\u771f\u5ea6\u548c\u7269\u7406\u5408\u7406\u6027\uff0c\u5bfc\u81f4\u5e03\u5c40\u4e2d\u51fa\u73b0\u7269\u4f53\u6f02\u6d6e\u3001\u91cd\u53e0\u6216\u5806\u53e0\u5173\u7cfb\u9519\u8bef\u3002", "method": "\u91c7\u7528\u53cc\u7cfb\u7edf\u6846\u67b6\uff1a\u6162\u901f\u7cfb\u7edf\u901a\u8fc7RRG\u7ba1\u9053\u8fdb\u884c\u8be6\u7ec6\u63a8\u7406\uff0c\u63d0\u53d6\u7269\u4f53\u5c5e\u6027\u548c\u7a7a\u95f4\u7ea6\u675f\uff1b\u5feb\u901f\u7cfb\u7edf\u751f\u6210\u79bb\u6563\u5750\u6807\u548c\u62d3\u6251\u5173\u7cfb\u96c6\uff0c\u5e76\u901a\u8fc7\u81ea\u9a8c\u8bc1\u673a\u5236\u8054\u5408\u9a8c\u8bc1\u3002\u5f15\u5165LLM-based ARL\u751f\u6210\u548c\u8bc4\u4f30\u5e03\u5c40\u3002", "result": "\u57288\u79cd\u4e0d\u540c\u573a\u666f\u4e2d\u9a8c\u8bc1\uff0cAutoLayout\u5728\u7269\u7406\u5408\u7406\u6027\u3001\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u529f\u80fd\u5b8c\u6574\u6027\u4e0a\u6bd4SOTA\u65b9\u6cd5\u63d0\u534710.1%\u3002", "conclusion": "AutoLayout\u901a\u8fc7\u6162-\u5feb\u534f\u540c\u63a8\u7406\u548c\u81ea\u9a8c\u8bc1\u673a\u5236\uff0c\u6709\u6548\u51cf\u5c11\u7a7a\u95f4\u5e7b\u89c9\uff0c\u5e73\u8861\u7269\u7406\u7a33\u5b9a\u6027\u548c\u8bed\u4e49\u4e00\u81f4\u6027\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2507.03682", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.03682", "abs": "https://arxiv.org/abs/2507.03682", "authors": ["Rebekah A. Gelp\u00ed", "Eric Xue", "William A. Cunningham"], "title": "Towards Machine Theory of Mind with Large Language Model-Augmented Inverse Planning", "comment": null, "summary": "We propose a hybrid approach to machine Theory of Mind (ToM) that uses large\nlanguage models (LLMs) as a mechanism for generating hypotheses and likelihood\nfunctions with a Bayesian inverse planning model that computes posterior\nprobabilities for an agent's likely mental states given its actions. Bayesian\ninverse planning models can accurately predict human reasoning on a variety of\nToM tasks, but these models are constrained in their ability to scale these\npredictions to scenarios with a large number of possible hypotheses and\nactions. Conversely, LLM-based approaches have recently demonstrated promise in\nsolving ToM benchmarks, but can exhibit brittleness and failures on reasoning\ntasks even when they pass otherwise structurally identical versions. By\ncombining these two methods, this approach leverages the strengths of each\ncomponent, closely matching optimal results on a task inspired by prior inverse\nplanning models and improving performance relative to models that utilize LLMs\nalone or with chain-of-thought prompting, even with smaller LLMs that typically\nperform poorly on ToM tasks. We also exhibit the model's potential to predict\nmental states on open-ended tasks, offering a promising direction for future\ndevelopment of ToM models and the creation of socially intelligent generative\nagents.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u6df7\u5408\u65b9\u6cd5\uff0c\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u8d1d\u53f6\u65af\u9006\u5411\u89c4\u5212\u6a21\u578b\uff0c\u4ee5\u63d0\u5347\u673a\u5668\u5fc3\u667a\u7406\u8bba\uff08ToM\uff09\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u8d1d\u53f6\u65af\u9006\u5411\u89c4\u5212\u6a21\u578b\u5728\u590d\u6742\u573a\u666f\u4e0b\u7684\u6269\u5c55\u6027\u95ee\u9898\uff0c\u4ee5\u53caLLMs\u5728\u5fc3\u667a\u7406\u8bba\u4efb\u52a1\u4e2d\u7684\u8106\u5f31\u6027\u3002", "method": "\u4f7f\u7528LLMs\u751f\u6210\u5047\u8bbe\u548c\u4f3c\u7136\u51fd\u6570\uff0c\u7ed3\u5408\u8d1d\u53f6\u65af\u9006\u5411\u89c4\u5212\u6a21\u578b\u8ba1\u7b97\u540e\u9a8c\u6982\u7387\u3002", "result": "\u6df7\u5408\u65b9\u6cd5\u5728ToM\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u5355\u72ec\u4f7f\u7528LLMs\u6216\u8d1d\u53f6\u65af\u6a21\u578b\uff0c\u4e14\u9002\u7528\u4e8e\u5f00\u653e\u4efb\u52a1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3aToM\u6a21\u578b\u7684\u672a\u6765\u53d1\u5c55\u548c\u793e\u4ea4\u667a\u80fd\u751f\u6210\u4ee3\u7406\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2507.04311", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.04311", "abs": "https://arxiv.org/abs/2507.04311", "authors": ["Yan Dong", "Enci Xu", "Shaoqiang Qiu", "Wenxuan Li", "Yang Liu", "Bin Han"], "title": "Vibration-aware Lidar-Inertial Odometry based on Point-wise Post-Undistortion Uncertainty", "comment": "8 pages, 10 figures, 5 tables. Accepted by Robotics and Automation\n  Letters at June 30", "summary": "High-speed ground robots moving on unstructured terrains generate intense\nhigh-frequency vibrations, leading to LiDAR scan distortions in Lidar-inertial\nodometry (LIO). Accurate and efficient undistortion is extremely challenging\ndue to (1) rapid and non-smooth state changes during intense vibrations and (2)\nunpredictable IMU noise coupled with a limited IMU sampling frequency. To\naddress this issue, this paper introduces post-undistortion uncertainty. First,\nwe model the undistortion errors caused by linear and angular vibrations and\nassign post-undistortion uncertainty to each point. We then leverage this\nuncertainty to guide point-to-map matching, compute uncertainty-aware\nresiduals, and update the odometry states using an iterated Kalman filter. We\nconduct vibration-platform and mobile-platform experiments on multiple public\ndatasets as well as our own recordings, demonstrating that our method achieves\nbetter performance than other methods when LiDAR undergoes intense vibration.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u9ad8\u901f\u5730\u9762\u673a\u5668\u4eba\u5728\u975e\u7ed3\u6784\u5316\u5730\u5f62\u4e2d\u8fd0\u52a8\u65f6\u4ea7\u751f\u7684LiDAR\u626b\u63cf\u7578\u53d8\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u5f15\u5165\u540e\u53bb\u7578\u53d8\u4e0d\u786e\u5b9a\u6027\u6a21\u578b\uff0c\u6539\u8fdb\u4e86\u70b9\u5bf9\u5730\u56fe\u5339\u914d\u548c\u72b6\u6001\u66f4\u65b0\u3002", "motivation": "\u9ad8\u901f\u5730\u9762\u673a\u5668\u4eba\u5728\u975e\u7ed3\u6784\u5316\u5730\u5f62\u4e2d\u8fd0\u52a8\u65f6\u4f1a\u4ea7\u751f\u9ad8\u9891\u632f\u52a8\uff0c\u5bfc\u81f4LiDAR\u626b\u63cf\u7578\u53d8\uff0c\u800c\u73b0\u6709\u7684\u53bb\u7578\u53d8\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u5feb\u901f\u4e14\u975e\u5e73\u6ed1\u7684\u72b6\u6001\u53d8\u5316\u4ee5\u53caIMU\u566a\u58f0\u7684\u8026\u5408\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u540e\u53bb\u7578\u53d8\u4e0d\u786e\u5b9a\u6027\u6a21\u578b\uff0c\u5bf9\u6bcf\u4e2a\u70b9\u5206\u914d\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u5229\u7528\u8be5\u4e0d\u786e\u5b9a\u6027\u6307\u5bfc\u70b9\u5bf9\u5730\u56fe\u5339\u914d\u3001\u8ba1\u7b97\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u6b8b\u5dee\uff0c\u6700\u540e\u4f7f\u7528\u8fed\u4ee3\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u66f4\u65b0\u72b6\u6001\u3002", "result": "\u5728\u632f\u52a8\u5e73\u53f0\u548c\u79fb\u52a8\u5e73\u53f0\u7684\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728LiDAR\u7ecf\u5386\u5f3a\u70c8\u632f\u52a8\u65f6\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5efa\u6a21\u53bb\u7578\u53d8\u8bef\u5dee\u548c\u4e0d\u786e\u5b9a\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u4e86LiDAR\u5728\u5f3a\u70c8\u632f\u52a8\u73af\u5883\u4e0b\u7684\u5b9a\u4f4d\u7cbe\u5ea6\u3002"}}
{"id": "2507.03697", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.03697", "abs": "https://arxiv.org/abs/2507.03697", "authors": ["Qika Lin", "Fangzhi Xu", "Hao Lu", "Kai He", "Rui Mao", "Jun Liu", "Erik Cambria", "Mengling Feng"], "title": "Towards Unified Neurosymbolic Reasoning on Knowledge Graphs", "comment": "15 pages", "summary": "Knowledge Graph (KG) reasoning has received significant attention in the\nfields of artificial intelligence and knowledge engineering, owing to its\nability to autonomously deduce new knowledge and consequently enhance the\navailability and precision of downstream applications. However, current methods\npredominantly concentrate on a single form of neural or symbolic reasoning,\nfailing to effectively integrate the inherent strengths of both approaches.\nFurthermore, the current prevalent methods primarily focus on addressing a\nsingle reasoning scenario, presenting limitations in meeting the diverse\ndemands of real-world reasoning tasks. Unifying the neural and symbolic\nmethods, as well as diverse reasoning scenarios in one model is challenging as\nthere is a natural representation gap between symbolic rules and neural\nnetworks, and diverse scenarios exhibit distinct knowledge structures and\nspecific reasoning objectives. To address these issues, we propose a unified\nneurosymbolic reasoning framework, namely Tunsr, for KG reasoning. Tunsr first\nintroduces a consistent structure of reasoning graph that starts from the query\nentity and constantly expands subsequent nodes by iteratively searching\nposterior neighbors. Based on it, a forward logic message-passing mechanism is\nproposed to update both the propositional representations and attentions, as\nwell as first-order logic (FOL) representations and attentions of each node. In\nthis way, Tunsr conducts the transformation of merging multiple rules by\nmerging possible relations at each step. Finally, the FARI algorithm is\nproposed to induce FOL rules by constantly performing attention calculations\nover the reasoning graph. Extensive experimental results on 19 datasets of four\nreasoning scenarios (transductive, inductive, interpolation, and extrapolation)\ndemonstrate the effectiveness of Tunsr.", "AI": {"tldr": "Tunsr\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u795e\u7ecf\u7b26\u53f7\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u795e\u7ecf\u548c\u7b26\u53f7\u65b9\u6cd5\u7684\u4f18\u52bf\uff0c\u89e3\u51b3\u4e86\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u4e2d\u7684\u591a\u6837\u6027\u548c\u7edf\u4e00\u6027\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u65b9\u6cd5\u4e3b\u8981\u96c6\u4e2d\u4e8e\u5355\u4e00\u5f62\u5f0f\u7684\u795e\u7ecf\u6216\u7b26\u53f7\u63a8\u7406\uff0c\u672a\u80fd\u6709\u6548\u7ed3\u5408\u4e24\u8005\u7684\u4f18\u52bf\uff0c\u4e14\u96be\u4ee5\u6ee1\u8db3\u73b0\u5b9e\u4efb\u52a1\u4e2d\u7684\u591a\u6837\u6027\u9700\u6c42\u3002", "method": "Tunsr\u5f15\u5165\u4e86\u4e00\u81f4\u7684\u63a8\u7406\u56fe\u7ed3\u6784\uff0c\u901a\u8fc7\u524d\u5411\u903b\u8f91\u6d88\u606f\u4f20\u9012\u673a\u5236\u66f4\u65b0\u8282\u70b9\u8868\u793a\u548c\u6ce8\u610f\u529b\uff0c\u5e76\u7ed3\u5408FARI\u7b97\u6cd5\u5f52\u7eb3\u4e00\u9636\u903b\u8f91\u89c4\u5219\u3002", "result": "\u572819\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTunsr\u5728\u56db\u79cd\u63a8\u7406\u573a\u666f\uff08\u8f6c\u5bfc\u3001\u5f52\u7eb3\u3001\u63d2\u503c\u548c\u5916\u63a8\uff09\u4e2d\u5747\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "Tunsr\u901a\u8fc7\u7edf\u4e00\u795e\u7ecf\u548c\u7b26\u53f7\u65b9\u6cd5\uff0c\u4e3a\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.04314", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.04314", "abs": "https://arxiv.org/abs/2507.04314", "authors": ["Wenxuan Li", "Yan Dong", "Shaoqiang Qiu", "Bin Han"], "title": "Hardware-Free Event Cameras Temporal Synchronization Based on Event Density Alignment", "comment": "12 pages, 8 figures. Conference paper, International Conference on\n  Intelligent Robotics and Applications 2024", "summary": "Event cameras are a novel type of sensor designed for capturing the dynamic\nchanges of a scene. Due to factors such as trigger and transmission delays, a\ntime offset exists in the data collected by multiple event cameras, leading to\ninaccurate information fusion. Thus, the collected data needs to be\nsynchronized to overcome any potential time offset issue. Hardware\nsynchronization methods require additional circuits, while certain models of\nevent cameras (e.g., CeleX5) do not support hardware synchronization.\nTherefore, this paper proposes a hardware-free event camera synchronization\nmethod. This method determines differences between start times by minimizing\nthe dissimilarity of the event density distributions of different event cameras\nand synchronizes the data by adjusting timestamps. The experiments demonstrate\nthat the method's synchronization error is less than 10ms under various senses\nwith multiple models of event cameras.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u786c\u4ef6\u7684\u591a\u4e8b\u4ef6\u76f8\u673a\u540c\u6b65\u65b9\u6cd5\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u4e8b\u4ef6\u5bc6\u5ea6\u5206\u5e03\u5dee\u5f02\u6765\u8c03\u6574\u65f6\u95f4\u6233\uff0c\u5b9e\u9a8c\u663e\u793a\u540c\u6b65\u8bef\u5dee\u5c0f\u4e8e10ms\u3002", "motivation": "\u591a\u4e8b\u4ef6\u76f8\u673a\u56e0\u89e6\u53d1\u548c\u4f20\u8f93\u5ef6\u8fdf\u5bfc\u81f4\u6570\u636e\u65f6\u95f4\u504f\u79fb\uff0c\u786c\u4ef6\u540c\u6b65\u65b9\u6cd5\u9700\u989d\u5916\u7535\u8def\u4e14\u90e8\u5206\u76f8\u673a\u4e0d\u652f\u6301\uff0c\u56e0\u6b64\u9700\u8f6f\u4ef6\u540c\u6b65\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u6700\u5c0f\u5316\u4e0d\u540c\u4e8b\u4ef6\u76f8\u673a\u4e8b\u4ef6\u5bc6\u5ea6\u5206\u5e03\u7684\u5dee\u5f02\u6765\u786e\u5b9a\u65f6\u95f4\u5dee\uff0c\u5e76\u8c03\u6574\u65f6\u95f4\u6233\u5b9e\u73b0\u540c\u6b65\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u573a\u666f\u548c\u76f8\u673a\u578b\u53f7\u4e0b\u540c\u6b65\u8bef\u5dee\u5c0f\u4e8e10ms\u3002", "conclusion": "\u8be5\u786c\u4ef6\u65e0\u5173\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u591a\u4e8b\u4ef6\u76f8\u673a\u7684\u65f6\u95f4\u540c\u6b65\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u4e0d\u652f\u6301\u786c\u4ef6\u540c\u6b65\u7684\u8bbe\u5907\u3002"}}
{"id": "2507.03722", "categories": ["cs.AI", "q-bio.OT"], "pdf": "https://arxiv.org/pdf/2507.03722", "abs": "https://arxiv.org/abs/2507.03722", "authors": ["Ruian Ke", "Ruy M. Ribeiro"], "title": "Roadmap for using large language models (LLMs) to accelerate cross-disciplinary research with an example from computational biology", "comment": null, "summary": "Large language models (LLMs) are powerful artificial intelligence (AI) tools\ntransforming how research is conducted. However, their use in research has been\nmet with skepticism, due to concerns about hallucinations, biases and potential\nharms to research. These emphasize the importance of clearly understanding the\nstrengths and weaknesses of LLMs to ensure their effective and responsible use.\nHere, we present a roadmap for integrating LLMs into cross-disciplinary\nresearch, where effective communication, knowledge transfer and collaboration\nacross diverse fields are essential but often challenging. We examine the\ncapabilities and limitations of LLMs and provide a detailed computational\nbiology case study (on modeling HIV rebound dynamics) demonstrating how\niterative interactions with an LLM (ChatGPT) can facilitate interdisciplinary\ncollaboration and research. We argue that LLMs are best used as augmentative\ntools within a human-in-the-loop framework. Looking forward, we envisage that\nthe responsible use of LLMs will enhance innovative cross-disciplinary research\nand substantially accelerate scientific discoveries.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8de8\u5b66\u79d1\u7814\u7a76\u4e2d\u7684\u5e94\u7528\uff0c\u5f3a\u8c03\u5176\u4f5c\u4e3a\u8f85\u52a9\u5de5\u5177\u7684\u6f5c\u529b\uff0c\u5e76\u63d0\u51fa\u8d1f\u8d23\u4efb\u4f7f\u7528\u7684\u8def\u7ebf\u56fe\u3002", "motivation": "LLMs\u5728\u7814\u7a76\u4e2d\u867d\u5f3a\u5927\u4f46\u5b58\u5728\u8d28\u7591\uff0c\u5982\u5e7b\u89c9\u548c\u504f\u89c1\u95ee\u9898\uff0c\u9700\u660e\u786e\u5176\u4f18\u7f3a\u70b9\u4ee5\u5b9e\u73b0\u6709\u6548\u548c\u8d1f\u8d23\u4efb\u7684\u4f7f\u7528\u3002", "method": "\u901a\u8fc7\u5206\u6790LLMs\u7684\u80fd\u529b\u4e0e\u9650\u5236\uff0c\u5e76\u4ee5\u8ba1\u7b97\u751f\u7269\u5b66\u6848\u4f8b\uff08HIV\u53cd\u5f39\u52a8\u529b\u5b66\u5efa\u6a21\uff09\u5c55\u793a\u5176\u5982\u4f55\u4fc3\u8fdb\u8de8\u5b66\u79d1\u5408\u4f5c\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0cLLMs\u5728\u4eba\u7c7b\u53c2\u4e0e\u6846\u67b6\u4e0b\u4f5c\u4e3a\u8f85\u52a9\u5de5\u5177\uff0c\u53ef\u6709\u6548\u63a8\u52a8\u8de8\u5b66\u79d1\u7814\u7a76\u548c\u52a0\u901f\u79d1\u5b66\u53d1\u73b0\u3002", "conclusion": "LLMs\u7684\u8d1f\u8d23\u4efb\u4f7f\u7528\u5c06\u4fc3\u8fdb\u521b\u65b0\u6027\u8de8\u5b66\u79d1\u7814\u7a76\uff0c\u5e76\u5927\u5e45\u52a0\u901f\u79d1\u5b66\u8fdb\u6b65\u3002"}}
{"id": "2507.04321", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.04321", "abs": "https://arxiv.org/abs/2507.04321", "authors": ["Doumegna Mawuto Koudjo Felix", "Xianjia Yu", "Jiaqiang Zhang", "Sier Ha", "Zhuo Zou", "Tomi Westerlund"], "title": "Lidar Variability: A Novel Dataset and Comparative Study of Solid-State and Spinning Lidars", "comment": null, "summary": "Lidar technology has been widely employed across various applications, such\nas robot localization in GNSS-denied environments and 3D reconstruction. Recent\nadvancements have introduced different lidar types, including cost-effective\nsolid-state lidars such as the Livox Avia and Mid-360. The Mid-360, with its\ndome-like design, is increasingly used in portable mapping and unmanned aerial\nvehicle (UAV) applications due to its low cost, compact size, and reliable\nperformance. However, the lack of datasets that include dome-shaped lidars,\nsuch as the Mid-360, alongside other solid-state and spinning lidars\nsignificantly hinders the comparative evaluation of novel approaches across\nplatforms. Additionally, performance differences between low-cost solid-state\nand high-end spinning lidars (e.g., Ouster OS series) remain insufficiently\nexamined, particularly without an Inertial Measurement Unit (IMU) in odometry.\n  To address this gap, we introduce a novel dataset comprising data from\nmultiple lidar types, including the low-cost Livox Avia and the dome-shaped\nMid-360, as well as high-end spinning lidars such as the Ouster series.\nNotably, to the best of our knowledge, no existing dataset comprehensively\nincludes dome-shaped lidars such as Mid-360 alongside both other solid-state\nand spinning lidars. In addition to the dataset, we provide a benchmark\nevaluation of state-of-the-art SLAM algorithms applied to this diverse sensor\ndata. Furthermore, we present a quantitative analysis of point cloud\nregistration techniques, specifically point-to-point, point-to-plane, and\nhybrid methods, using indoor and outdoor data collected from the included lidar\nsystems. The outcomes of this study establish a foundational reference for\nfuture research in SLAM and 3D reconstruction across heterogeneous lidar\nplatforms.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u5305\u542b\u591a\u79cd\u6fc0\u5149\u96f7\u8fbe\u7c7b\u578b\u7684\u65b0\u6570\u636e\u96c6\uff0c\u586b\u8865\u4e86\u4f4e\u6210\u672c\u56fa\u6001\u6fc0\u5149\u96f7\u8fbe\u4e0e\u9ad8\u7aef\u65cb\u8f6c\u6fc0\u5149\u96f7\u8fbe\u6027\u80fd\u6bd4\u8f83\u7684\u7a7a\u767d\uff0c\u5e76\u8bc4\u4f30\u4e86SLAM\u7b97\u6cd5\u548c\u70b9\u4e91\u914d\u51c6\u6280\u672f\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u5305\u542b\u5706\u9876\u5f62\u6fc0\u5149\u96f7\u8fbe\uff08\u5982Mid-360\uff09\u4e0e\u5176\u4ed6\u56fa\u6001\u548c\u65cb\u8f6c\u6fc0\u5149\u96f7\u8fbe\u7684\u7efc\u5408\u6570\u636e\u96c6\uff0c\u4e14\u4f4e\u6210\u672c\u4e0e\u9ad8\u7aef\u6fc0\u5149\u96f7\u8fbe\u7684\u6027\u80fd\u5dee\u5f02\u672a\u5145\u5206\u7814\u7a76\u3002", "method": "\u63d0\u51fa\u5305\u542bLivox Avia\u3001Mid-360\u548cOuster\u7cfb\u5217\u6fc0\u5149\u96f7\u8fbe\u7684\u65b0\u6570\u636e\u96c6\uff0c\u5e76\u8bc4\u4f30SLAM\u7b97\u6cd5\u53ca\u70b9\u4e91\u914d\u51c6\u6280\u672f\uff08\u70b9\u5bf9\u70b9\u3001\u70b9\u5bf9\u9762\u548c\u6df7\u5408\u65b9\u6cd5\uff09\u3002", "result": "\u6570\u636e\u96c6\u586b\u8865\u4e86\u7814\u7a76\u7a7a\u767d\uff0c\u8bc4\u4f30\u7ed3\u679c\u4e3aSLAM\u548c3D\u91cd\u5efa\u63d0\u4f9b\u4e86\u57fa\u51c6\u53c2\u8003\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5f02\u6784\u6fc0\u5149\u96f7\u8fbe\u5e73\u53f0\u7684SLAM\u548c3D\u91cd\u5efa\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.03726", "categories": ["cs.AI", "cs.CL", "cs.IR", "I.2"], "pdf": "https://arxiv.org/pdf/2507.03726", "abs": "https://arxiv.org/abs/2507.03726", "authors": ["Riya Naik", "Ashwin Srinivasan", "Swati Agarwal", "Estrid He"], "title": "Agent-Based Detection and Resolution of Incompleteness and Ambiguity in Interactions with Large Language Models", "comment": "14 pages. arXiv admin note: text overlap with arXiv:2503.17936", "summary": "Many of us now treat LLMs as modern-day oracles asking it almost any kind of\nquestion. However, consulting an LLM does not have to be a single turn\nactivity. But long multi-turn interactions can get tedious if it is simply to\nclarify contextual information that can be arrived at through reasoning. In\nthis paper, we examine the use of agent-based architecture to bolster LLM-based\nQuestion-Answering systems with additional reasoning capabilities. We examine\nthe automatic resolution of potential incompleteness or ambiguities in\nquestions by transducers implemented using LLM-based agents. We focus on\nseveral benchmark datasets that are known to contain questions with these\ndeficiencies to varying degrees. We equip different LLMs (GPT-3.5-Turbo and\nLlama-4-Scout) with agents that act as specialists in detecting and resolving\ndeficiencies of incompleteness and ambiguity. The agents are implemented as\nzero-shot ReAct agents. Rather than producing an answer in a single step, the\nmodel now decides between 3 actions a) classify b) resolve c) answer. Action a)\ndecides if the question is incomplete, ambiguous, or normal. Action b)\ndetermines if any deficiencies identified can be resolved. Action c) answers\nthe resolved form of the question. We compare the use of LLMs with and without\nthe use of agents with these components. Our results show benefits of agents\nwith transducer 1) A shortening of the length of interactions with human 2) An\nimprovement in the answer quality and 3) Explainable resolution of deficiencies\nin the question. On the negative side we find while it may result in additional\nLLM invocations and in some cases, increased latency. But on tested datasets,\nthe benefits outweigh the costs except when questions already have sufficient\ncontext. Suggesting the agent-based approach could be a useful mechanism to\nharness the power of LLMs to develop more robust QA systems.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u901a\u8fc7\u57fa\u4e8e\u4ee3\u7406\u7684\u67b6\u6784\u589e\u5f3aLLM\u95ee\u7b54\u7cfb\u7edf\u7684\u63a8\u7406\u80fd\u529b\uff0c\u81ea\u52a8\u89e3\u51b3\u95ee\u9898\u7684\u6a21\u7cca\u6027\u6216\u7f3a\u5931\uff0c\u7f29\u77ed\u4ea4\u4e92\u65f6\u95f4\u5e76\u63d0\u9ad8\u7b54\u6848\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709LLM\u95ee\u7b54\u7cfb\u7edf\u5728\u591a\u8f6e\u4ea4\u4e92\u4e2d\u53ef\u80fd\u56e0\u4e0a\u4e0b\u6587\u4fe1\u606f\u4e0d\u8db3\u800c\u6548\u7387\u4f4e\u4e0b\uff0c\u9700\u901a\u8fc7\u4ee3\u7406\u67b6\u6784\u63d0\u5347\u63a8\u7406\u80fd\u529b\u3002", "method": "\u4f7f\u7528LLM\u4ee3\u7406\uff08\u5982GPT-3.5-Turbo\u548cLlama-4-Scout\uff09\u4f5c\u4e3a\u96f6\u6837\u672cReAct\u4ee3\u7406\uff0c\u5206\u7c7b\u3001\u89e3\u51b3\u6216\u56de\u7b54\u95ee\u9898\uff0c\u4f18\u5316\u4ea4\u4e92\u6d41\u7a0b\u3002", "result": "\u4ee3\u7406\u67b6\u6784\u7f29\u77ed\u4e86\u4ea4\u4e92\u65f6\u95f4\uff0c\u63d0\u9ad8\u4e86\u7b54\u6848\u8d28\u91cf\uff0c\u5e76\u80fd\u89e3\u91ca\u95ee\u9898\u7f3a\u9677\u7684\u89e3\u51b3\u8fc7\u7a0b\uff0c\u4f46\u53ef\u80fd\u589e\u52a0LLM\u8c03\u7528\u548c\u5ef6\u8fdf\u3002", "conclusion": "\u4ee3\u7406\u65b9\u6cd5\u5728\u591a\u6570\u60c5\u51b5\u4e0b\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u95ee\u9898\u4e0a\u4e0b\u6587\u4e0d\u8db3\u7684\u573a\u666f\uff0c\u4e3a\u6784\u5efa\u66f4\u5065\u58ee\u7684QA\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002"}}
{"id": "2507.04331", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.04331", "abs": "https://arxiv.org/abs/2507.04331", "authors": ["Hao Huang", "Shuaihang Yuan", "Geeta Chandra Raju Bethala", "Congcong Wen", "Anthony Tzes", "Yi Fang"], "title": "Wavelet Policy: Lifting Scheme for Policy Learning in Long-Horizon Tasks", "comment": "11 pages, 5 figures, 6 tables", "summary": "Policy learning focuses on devising strategies for agents in embodied\nartificial intelligence systems to perform optimal actions based on their\nperceived states. One of the key challenges in policy learning involves\nhandling complex, long-horizon tasks that require managing extensive sequences\nof actions and observations with multiple modes. Wavelet analysis offers\nsignificant advantages in signal processing, notably in decomposing signals at\nmultiple scales to capture both global trends and fine-grained details. In this\nwork, we introduce a novel wavelet policy learning framework that utilizes\nwavelet transformations to enhance policy learning. Our approach leverages\nlearnable multi-scale wavelet decomposition to facilitate detailed observation\nanalysis and robust action planning over extended sequences. We detail the\ndesign and implementation of our wavelet policy, which incorporates lifting\nschemes for effective multi-resolution analysis and action generation. This\nframework is evaluated across multiple complex scenarios, including robotic\nmanipulation, self-driving, and multi-robot collaboration, demonstrating the\neffectiveness of our method in improving the precision and reliability of the\nlearned policy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5c0f\u6ce2\u53d8\u6362\u7684\u7b56\u7565\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u52a8\u4f5c\u89c4\u5212\u548c\u89c2\u5bdf\u5206\u6790\u3002", "motivation": "\u89e3\u51b3\u7b56\u7565\u5b66\u4e60\u4e2d\u5904\u7406\u590d\u6742\u3001\u957f\u65f6\u7a0b\u4efb\u52a1\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u9700\u8981\u7ba1\u7406\u591a\u6a21\u6001\u52a8\u4f5c\u548c\u89c2\u5bdf\u5e8f\u5217\u7684\u60c5\u51b5\u3002", "method": "\u5229\u7528\u53ef\u5b66\u4e60\u7684\u591a\u5c3a\u5ea6\u5c0f\u6ce2\u5206\u89e3\u6280\u672f\uff0c\u7ed3\u5408\u63d0\u5347\u65b9\u6848\u8fdb\u884c\u591a\u5206\u8fa8\u7387\u5206\u6790\u548c\u52a8\u4f5c\u751f\u6210\u3002", "result": "\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u3001\u81ea\u52a8\u9a7e\u9a76\u548c\u591a\u673a\u5668\u4eba\u534f\u4f5c\u7b49\u590d\u6742\u573a\u666f\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u63d0\u9ad8\u4e86\u7b56\u7565\u7684\u7cbe\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "conclusion": "\u5c0f\u6ce2\u7b56\u7565\u5b66\u4e60\u6846\u67b6\u4e3a\u590d\u6742\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7b56\u7565\u5b66\u4e60\u7684\u6027\u80fd\u3002"}}
{"id": "2507.03775", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.03775", "abs": "https://arxiv.org/abs/2507.03775", "authors": ["Hiba Bederina"], "title": "Optimizing UAV Trajectories via a Simplified Close Enough TSP Approach", "comment": null, "summary": "This article explores an approach to addressing the Close Enough Traveling\nSalesman Problem (CETSP). The objective is to streamline the mathematical\nformulation by introducing reformulations that approximate the Euclidean\ndistances and simplify the objective function. Additionally, the use of convex\nsets in the constraint design offers computational benefits. The proposed\nmethodology is empirically validated on real-world CETSP instances, with the\naid of computational strategies such as a fragmented CPLEX-based approach.\nResults demonstrate its effectiveness in managing computational resources\nwithout compromising solution quality. Furthermore, the article analyzes the\nbehavior of the proposed mathematical formulations, providing comprehensive\ninsights into their performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u51b3\u201c\u8db3\u591f\u63a5\u8fd1\u65c5\u884c\u5546\u95ee\u9898\u201d\uff08CETSP\uff09\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7b80\u5316\u6b27\u51e0\u91cc\u5f97\u8ddd\u79bb\u548c\u76ee\u6807\u51fd\u6570\uff0c\u5e76\u7ed3\u5408\u51f8\u96c6\u7ea6\u675f\u8bbe\u8ba1\uff0c\u4f18\u5316\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "CETSP\u95ee\u9898\u7684\u4f20\u7edf\u6570\u5b66\u5efa\u6a21\u590d\u6742\u4e14\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u672c\u6587\u65e8\u5728\u901a\u8fc7\u7b80\u5316\u6a21\u578b\u548c\u4f18\u5316\u7ea6\u675f\u8bbe\u8ba1\u6765\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u3002", "method": "\u5f15\u5165\u8fd1\u4f3c\u6b27\u51e0\u91cc\u5f97\u8ddd\u79bb\u7684\u91cd\u65b0\u5efa\u6a21\uff0c\u7b80\u5316\u76ee\u6807\u51fd\u6570\uff0c\u5e76\u5229\u7528\u51f8\u96c6\u8bbe\u8ba1\u7ea6\u675f\u3002\u91c7\u7528\u5206\u6bb5CPLEX\u8ba1\u7b97\u7b56\u7565\u8fdb\u884c\u5b9e\u8bc1\u9a8c\u8bc1\u3002", "result": "\u5728\u771f\u5b9eCETSP\u5b9e\u4f8b\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u5728\u8282\u7701\u8ba1\u7b97\u8d44\u6e90\u7684\u540c\u65f6\u4fdd\u6301\u89e3\u7684\u8d28\u91cf\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u548c\u6027\u80fd\u5206\u6790\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4e3aCETSP\u95ee\u9898\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.04345", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.04345", "abs": "https://arxiv.org/abs/2507.04345", "authors": ["Wenzhi Bai", "Andrew Weightman", "Rory J O Connor", "Zhengtao Ding", "Mingming Zhang", "Sheng Quan Xie", "Zhenhong Li"], "title": "Robot-assisted Transcranial Magnetic Stimulation (Robo-TMS): A Review", "comment": "Accepted by IEEE Transactions on Neural Systems and Rehabilitation\n  Engineering", "summary": "Transcranial magnetic stimulation (TMS) is a non-invasive and safe brain\nstimulation procedure with growing applications in clinical treatments and\nneuroscience research. However, achieving precise stimulation over prolonged\nsessions poses significant challenges. By integrating advanced robotics with\nconventional TMS, robot-assisted TMS (Robo-TMS) has emerged as a promising\nsolution to enhance efficacy and streamline procedures. Despite growing\ninterest, a comprehensive review from an engineering perspective has been\nnotably absent. This paper systematically examines four critical aspects of\nRobo-TMS: hardware and integration, calibration and registration,\nneuronavigation systems, and control systems. We review state-of-the-art\ntechnologies in each area, identify current limitations, and propose future\nresearch directions. Our findings suggest that broader clinical adoption of\nRobo-TMS is currently limited by unverified clinical applicability, high\noperational complexity, and substantial implementation costs. Emerging\ntechnologies, including marker-less tracking, non-rigid registration,\nlearning-based electric field (E-field) modelling, individualised magnetic\nresonance imaging (MRI) generation, robot-assisted multi-locus TMS (Robo-mTMS),\nand automated calibration and registration, present promising pathways to\naddress these challenges.", "AI": {"tldr": "\u673a\u5668\u4eba\u8f85\u52a9\u7ecf\u9885\u78c1\u523a\u6fc0\uff08Robo-TMS\uff09\u901a\u8fc7\u7ed3\u5408\u5148\u8fdb\u673a\u5668\u4eba\u6280\u672f\u63d0\u5347TMS\u7684\u7cbe\u786e\u6027\u548c\u6548\u7387\uff0c\u4f46\u4e34\u5e8a\u63a8\u5e7f\u4ecd\u9762\u4e34\u672a\u7ecf\u9a8c\u8bc1\u7684\u9002\u7528\u6027\u3001\u64cd\u4f5c\u590d\u6742\u6027\u548c\u9ad8\u6210\u672c\u7b49\u6311\u6218\u3002", "motivation": "\u4f20\u7edfTMS\u5728\u957f\u65f6\u95f4\u7cbe\u786e\u523a\u6fc0\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0cRobo-TMS\u4f5c\u4e3a\u89e3\u51b3\u65b9\u6848\u53d7\u5230\u5173\u6ce8\uff0c\u4f46\u7f3a\u4e4f\u5de5\u7a0b\u89d2\u5ea6\u7684\u5168\u9762\u7efc\u8ff0\u3002", "method": "\u7cfb\u7edf\u5206\u6790\u4e86Robo-TMS\u7684\u786c\u4ef6\u4e0e\u96c6\u6210\u3001\u6821\u51c6\u4e0e\u914d\u51c6\u3001\u795e\u7ecf\u5bfc\u822a\u7cfb\u7edf\u548c\u63a7\u5236\u7cfb\u7edf\u56db\u5927\u5173\u952e\u65b9\u9762\uff0c\u5e76\u8bc4\u4f30\u4e86\u6700\u65b0\u6280\u672f\u548c\u5c40\u9650\u6027\u3002", "result": "Robo-TMS\u7684\u4e34\u5e8a\u63a8\u5e7f\u53d7\u9650\u4e8e\u9002\u7528\u6027\u9a8c\u8bc1\u4e0d\u8db3\u3001\u64cd\u4f5c\u590d\u6742\u548c\u9ad8\u6210\u672c\uff0c\u4f46\u65b0\u5174\u6280\u672f\u5982\u65e0\u6807\u8bb0\u8ddf\u8e2a\u3001\u975e\u521a\u6027\u914d\u51c6\u7b49\u63d0\u4f9b\u4e86\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u672a\u6765\u7814\u7a76\u5e94\u805a\u7126\u65b0\u5174\u6280\u672f\u4ee5\u514b\u670d\u5f53\u524d\u9650\u5236\uff0c\u63a8\u52a8Robo-TMS\u5728\u4e34\u5e8a\u548c\u79d1\u7814\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u3002"}}
{"id": "2507.03793", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.03793", "abs": "https://arxiv.org/abs/2507.03793", "authors": ["Jim O'Connor", "Gary B. Parker", "Mustafa Bugti"], "title": "Learning Dark Souls Combat Through Pixel Input With Neuroevolution", "comment": "IEEE Conference on Games 2025", "summary": "This paper investigates the application of Neuroevolution of Augmenting\nTopologies (NEAT) to automate gameplay in Dark Souls, a notoriously challenging\naction role-playing game characterized by complex combat mechanics, dynamic\nenvironments, and high-dimensional visual inputs. Unlike traditional\nreinforcement learning or game playing approaches, our method evolves neural\nnetworks directly from raw pixel data, circumventing the need for explicit\ngame-state information. To facilitate this approach, we introduce the Dark\nSouls API (DSAPI), a novel Python framework leveraging real-time computer\nvision techniques for extracting critical game metrics, including player and\nenemy health states. Using NEAT, agents evolve effective combat strategies for\ndefeating the Asylum Demon, the game's initial boss, without predefined\nbehaviors or domain-specific heuristics. Experimental results demonstrate that\nevolved agents achieve up to a 35% success rate, indicating the viability of\nneuroevolution in addressing complex, visually intricate gameplay scenarios.\nThis work represents an interesting application of vision-based neuroevolution,\nhighlighting its potential use in a wide range of challenging game environments\nlacking direct API support or well-defined state representations.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86NEAT\u5728\u300a\u9ed1\u6697\u4e4b\u9b42\u300b\u6e38\u620f\u81ea\u52a8\u5316\u4e2d\u7684\u5e94\u7528\uff0c\u901a\u8fc7\u76f4\u63a5\u5904\u7406\u50cf\u7d20\u6570\u636e\u751f\u6210\u795e\u7ecf\u7f51\u7edc\uff0c\u65e0\u9700\u6e38\u620f\u72b6\u6001\u4fe1\u606f\uff0c\u6210\u529f\u7387\u8fbe\u523035%\u3002", "motivation": "\u63a2\u7d22\u795e\u7ecf\u8fdb\u5316\u5728\u590d\u6742\u89c6\u89c9\u6e38\u620f\u73af\u5883\u4e2d\u7684\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u7f3a\u4e4fAPI\u652f\u6301\u6216\u660e\u786e\u72b6\u6001\u8868\u793a\u7684\u6e38\u620f\u3002", "method": "\u4f7f\u7528NEAT\u4ece\u50cf\u7d20\u6570\u636e\u76f4\u63a5\u8fdb\u5316\u795e\u7ecf\u7f51\u7edc\uff0c\u7ed3\u5408DSAPI\u6846\u67b6\u63d0\u53d6\u6e38\u620f\u5173\u952e\u6307\u6807\u3002", "result": "\u8fdb\u5316\u51fa\u7684\u4ee3\u7406\u5728\u51fb\u8d25\u521d\u59cbboss\u65f6\u8fbe\u523035%\u7684\u6210\u529f\u7387\u3002", "conclusion": "\u89c6\u89c9\u795e\u7ecf\u8fdb\u5316\u5728\u590d\u6742\u6e38\u620f\u73af\u5883\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u7f3a\u4e4fAPI\u652f\u6301\u7684\u60c5\u51b5\u3002"}}
{"id": "2507.04351", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.04351", "abs": "https://arxiv.org/abs/2507.04351", "authors": ["Liman Wang", "Hanyang Zhong", "Tianyuan Wang", "Shan Luo", "Jihong Zhu"], "title": "MLLM-Fabric: Multimodal Large Language Model-Driven Robotic Framework for Fabric Sorting and Selection", "comment": null, "summary": "Choosing the right fabric is crucial to meet functional and quality\nrequirements in robotic applications for textile manufacturing, apparel\nproduction, and smart retail. We present MLLM-Fabric, a robotic framework\npowered by multimodal large language models (MLLMs) for fabric sorting and\nselection. The system includes a robotic arm, a camera, a visuotactile sensor,\nand a pressure sensor. It employs supervised fine-tuning and multimodal\nexplanation-guided knowledge distillation to accurately classify and rank\nfabric properties. To facilitate further research, we release a dataset of 220\nunique fabric samples, including RGB images and synchronized visuotactile and\npressure data. Experimental results show that our Fabric-Llama-90B model\nconsistently outperforms pretrained vision-language baselines in both property\nranking accuracy and selection reliability.", "AI": {"tldr": "MLLM-Fabric\u662f\u4e00\u4e2a\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u673a\u5668\u4eba\u6846\u67b6\uff0c\u7528\u4e8e\u7ec7\u7269\u5206\u7c7b\u548c\u9009\u62e9\uff0c\u901a\u8fc7\u591a\u4f20\u611f\u5668\u6570\u636e\u548c\u76d1\u7763\u5fae\u8c03\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u6027\u80fd\u3002", "motivation": "\u5728\u673a\u5668\u4eba\u5e94\u7528\u4e2d\uff0c\u9009\u62e9\u5408\u9002\u7684\u7ec7\u7269\u5bf9\u529f\u80fd\u548c\u8d28\u91cf\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u7eba\u7ec7\u5236\u9020\u3001\u670d\u88c5\u751f\u4ea7\u548c\u667a\u80fd\u96f6\u552e\u9886\u57df\u3002", "method": "\u7cfb\u7edf\u7ed3\u5408\u673a\u5668\u4eba\u624b\u81c2\u3001\u6444\u50cf\u5934\u3001\u89c6\u89c9\u89e6\u89c9\u4f20\u611f\u5668\u548c\u538b\u529b\u4f20\u611f\u5668\uff0c\u91c7\u7528\u76d1\u7763\u5fae\u8c03\u548c\u591a\u6a21\u6001\u89e3\u91ca\u5f15\u5bfc\u7684\u77e5\u8bc6\u84b8\u998f\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFabric-Llama-90B\u6a21\u578b\u5728\u7ec7\u7269\u5c5e\u6027\u6392\u5e8f\u548c\u9009\u62e9\u53ef\u9760\u6027\u4e0a\u4f18\u4e8e\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u8bed\u8a00\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "MLLM-Fabric\u4e3a\u7ec7\u7269\u5206\u7c7b\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u53d1\u5e03\u4e86\u5305\u542b220\u79cd\u7ec7\u7269\u6837\u672c\u7684\u6570\u636e\u96c6\u4ee5\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2507.03802", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.03802", "abs": "https://arxiv.org/abs/2507.03802", "authors": ["Mayank Kejriwal", "Shilpa Thomas"], "title": "Generating Novelty in Open-World Multi-Agent Strategic Board Games", "comment": "16 pages, shorter version demonstrated in NeurIPS 2020", "summary": "We describe GNOME (Generating Novelty in Open-world Multi-agent\nEnvironments), an experimental platform that is designed to test the\neffectiveness of multi-agent AI systems when faced with \\emph{novelty}. GNOME\nseparates the development of AI gameplaying agents with the simulator, allowing\n\\emph{unanticipated} novelty (in essence, novelty that is not subject to\nmodel-selection bias). Using a Web GUI, GNOME was recently demonstrated at\nNeurIPS 2020 using the game of Monopoly to foster an open discussion on AI\nrobustness and the nature of novelty in real-world environments. In this\narticle, we further detail the key elements of the demonstration, and also\nprovide an overview of the experimental design that is being currently used in\nthe DARPA Science of Artificial Intelligence and Learning for Open-World\nNovelty (SAIL-ON) program to evaluate external teams developing\nnovelty-adaptive gameplaying agents.", "AI": {"tldr": "GNOME\u662f\u4e00\u4e2a\u5b9e\u9a8c\u5e73\u53f0\uff0c\u7528\u4e8e\u6d4b\u8bd5\u591a\u667a\u80fd\u4f53AI\u7cfb\u7edf\u5728\u9762\u5bf9\u672a\u9884\u671f\u7684\u65b0\u9896\u6027\u65f6\u7684\u8868\u73b0\uff0c\u652f\u6301\u5f00\u653e\u8ba8\u8bbaAI\u9c81\u68d2\u6027\u548c\u65b0\u9896\u6027\u3002", "motivation": "\u7814\u7a76\u591a\u667a\u80fd\u4f53AI\u7cfb\u7edf\u5728\u5f00\u653e\u4e16\u754c\u73af\u5883\u4e2d\u5e94\u5bf9\u672a\u9884\u671f\u65b0\u9896\u6027\u7684\u80fd\u529b\uff0c\u4ee5\u63d0\u5347AI\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u3002", "method": "\u901a\u8fc7\u5206\u79bbAI\u6e38\u620f\u4ee3\u7406\u4e0e\u6a21\u62df\u5668\u7684\u5f00\u53d1\uff0cGNOME\u907f\u514d\u4e86\u6a21\u578b\u9009\u62e9\u504f\u5dee\uff0c\u5e76\u4f7f\u7528Web GUI\u5c55\u793a\u5176\u529f\u80fd\u3002", "result": "GNOME\u5df2\u5728NeurIPS 2020\u4e0a\u4ee5\u300a\u5927\u5bcc\u7fc1\u300b\u6e38\u620f\u4e3a\u4f8b\u5c55\u793a\uff0c\u5e76\u7528\u4e8eDARPA SAIL-ON\u9879\u76ee\u4e2d\u8bc4\u4f30\u65b0\u9896\u6027\u9002\u5e94\u4ee3\u7406\u3002", "conclusion": "GNOME\u4e3a\u7814\u7a76AI\u5728\u5f00\u653e\u4e16\u754c\u4e2d\u7684\u65b0\u9896\u6027\u9002\u5e94\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u652f\u6301\u672a\u6765AI\u9c81\u68d2\u6027\u7814\u7a76\u3002"}}
{"id": "2507.04371", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.04371", "abs": "https://arxiv.org/abs/2507.04371", "authors": ["Benjamin Johnson", "Qilun Zhu", "Robert Prucka", "Morgan Barron", "Miriam Figueroa-Santos", "Matthew Castanier"], "title": "Implicit Dual-Control for Visibility-Aware Navigation in Unstructured Environments", "comment": "15 pages, 13 figures, submitted to IEEE Transactions on Robotics\n  (06/2025)", "summary": "Navigating complex, cluttered, and unstructured environments that are a\npriori unknown presents significant challenges for autonomous ground vehicles,\nparticularly when operating with a limited field of view(FOV) resulting in\nfrequent occlusion and unobserved space. This paper introduces a novel\nvisibility-aware model predictive path integral framework(VA-MPPI). Formulated\nas a dual control problem where perceptual uncertainties and control decisions\nare intertwined, it reasons over perception uncertainty evolution within a\nunified planning and control pipeline. Unlike traditional methods that rely on\nexplicit uncertainty objectives, the VA-MPPI controller implicitly balances\nexploration and exploitation, reducing uncertainty only when system performance\nwould be increased. The VA-MPPI framework is evaluated in simulation against\ndeterministic and prescient controllers across multiple scenarios, including a\ncluttered urban alleyway and an occluded off-road environment. The results\ndemonstrate that VA-MPPI significantly improves safety by reducing collision\nwith unseen obstacles while maintaining competitive performance. For example,\nin the off-road scenario with 400 control samples, the VA-MPPI controller\nachieved a success rate of 84%, compared to only 8% for the deterministic\ncontroller, with all VA-MPPI failures arising from unmet stopping criteria\nrather than collisions. Furthermore, the controller implicitly avoids\nunobserved space, improving safety without explicit directives. The proposed\nframework highlights the potential for robust, visibility-aware navigation in\nunstructured and occluded environments, paving the way for future advancements\nin autonomous ground vehicle systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u53ef\u89c1\u6027\u611f\u77e5\u6a21\u578b\u9884\u6d4b\u8def\u5f84\u79ef\u5206\u6846\u67b6\uff08VA-MPPI\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u81ea\u4e3b\u5730\u9762\u8f66\u8f86\u5728\u590d\u6742\u3001\u6742\u4e71\u4e14\u672a\u77e5\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u95ee\u9898\u3002", "motivation": "\u81ea\u4e3b\u5730\u9762\u8f66\u8f86\u5728\u6709\u9650\u89c6\u91ce\uff08FOV\uff09\u4e0b\u5bfc\u822a\u65f6\uff0c\u9891\u7e41\u7684\u906e\u6321\u548c\u672a\u89c2\u6d4b\u7a7a\u95f4\u5bfc\u81f4\u5b89\u5168\u6027\u548c\u6027\u80fd\u6311\u6218\u3002", "method": "VA-MPPI\u5c06\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\u548c\u63a7\u5236\u51b3\u7b56\u7ed3\u5408\u4e3a\u53cc\u91cd\u63a7\u5236\u95ee\u9898\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u89c4\u5212\u548c\u63a7\u5236\u6d41\u7a0b\u63a8\u7406\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\u6f14\u5316\u3002", "result": "\u5728\u6a21\u62df\u6d4b\u8bd5\u4e2d\uff0cVA-MPPI\u663e\u8457\u63d0\u9ad8\u4e86\u5b89\u5168\u6027\uff08\u5982\u6210\u529f\u7387\u4ece8%\u63d0\u5347\u81f384%\uff09\uff0c\u5e76\u51cf\u5c11\u4e86\u4e0e\u672a\u77e5\u969c\u788d\u7269\u7684\u78b0\u649e\u3002", "conclusion": "VA-MPPI\u6846\u67b6\u4e3a\u590d\u6742\u548c\u906e\u6321\u73af\u5883\u4e2d\u7684\u81ea\u4e3b\u5bfc\u822a\u63d0\u4f9b\u4e86\u9c81\u68d2\u6027\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u672a\u6765\u81ea\u4e3b\u5730\u9762\u8f66\u8f86\u7cfb\u7edf\u7684\u53d1\u5c55\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.03811", "categories": ["cs.AI", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.03811", "abs": "https://arxiv.org/abs/2507.03811", "authors": ["Gianlucca Zuin", "Saulo Mastelini", "T\u00falio Loures", "Adriano Veloso"], "title": "Leveraging Large Language Models for Tacit Knowledge Discovery in Organizational Contexts", "comment": "8 pages, 4 figures, accepted to International Joint Conference on\n  Neural Networks (IJCNN) 2025", "summary": "Documenting tacit knowledge in organizations can be a challenging task due to\nincomplete initial information, difficulty in identifying knowledgeable\nindividuals, the interplay of formal hierarchies and informal networks, and the\nneed to ask the right questions. To address this, we propose an agent-based\nframework leveraging large language models (LLMs) to iteratively reconstruct\ndataset descriptions through interactions with employees. Modeling knowledge\ndissemination as a Susceptible-Infectious (SI) process with waning infectivity,\nwe conduct 864 simulations across various synthetic company structures and\ndifferent dissemination parameters. Our results show that the agent achieves\n94.9% full-knowledge recall, with self-critical feedback scores strongly\ncorrelating with external literature critic scores. We analyze how each\nsimulation parameter affects the knowledge retrieval process for the agent. In\nparticular, we find that our approach is able to recover information without\nneeding to access directly the only domain specialist. These findings highlight\nthe agent's ability to navigate organizational complexity and capture\nfragmented knowledge that would otherwise remain inaccessible.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7SI\u6a21\u578b\u6a21\u62df\u77e5\u8bc6\u4f20\u64ad\uff0c\u5b9e\u73b094.9%\u7684\u77e5\u8bc6\u53ec\u56de\u7387\uff0c\u65e0\u9700\u76f4\u63a5\u8bbf\u95ee\u9886\u57df\u4e13\u5bb6\u3002", "motivation": "\u7ec4\u7ec7\u4e2d\u7684\u9690\u6027\u77e5\u8bc6\u96be\u4ee5\u8bb0\u5f55\uff0c\u56e0\u4fe1\u606f\u4e0d\u5b8c\u6574\u3001\u96be\u4ee5\u8bc6\u522b\u77e5\u8bc6\u6301\u6709\u8005\u53ca\u7ec4\u7ec7\u590d\u6742\u6027\u3002", "method": "\u4f7f\u7528LLM\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7SI\u6a21\u578b\u6a21\u62df\u77e5\u8bc6\u4f20\u64ad\uff0c\u8fdb\u884c864\u6b21\u4eff\u771f\u5b9e\u9a8c\u3002", "result": "\u4ee3\u7406\u5b9e\u73b094.9%\u77e5\u8bc6\u53ec\u56de\u7387\uff0c\u53cd\u9988\u5206\u6570\u4e0e\u5916\u90e8\u6587\u732e\u8bc4\u5206\u5f3a\u76f8\u5173\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u6355\u83b7\u788e\u7247\u5316\u77e5\u8bc6\uff0c\u5e94\u5bf9\u7ec4\u7ec7\u590d\u6742\u6027\u3002"}}
{"id": "2507.04384", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.04384", "abs": "https://arxiv.org/abs/2507.04384", "authors": ["Wule Mao", "Zhouheng Li", "Yunhao Luo", "Yilun Du", "Lei Xie"], "title": "Rapid and Safe Trajectory Planning over Diverse Scenes through Diffusion Composition", "comment": null, "summary": "Safe trajectory planning remains a significant challenge in complex\nenvironments, where traditional methods often trade off computational\nefficiency for safety. Comprehensive obstacle modeling improves safety but is\ncomputationally expensive, while approximate methods are more efficient but may\ncompromise safety. To address this issue, this paper introduces a rapid and\nsafe trajectory planning framework based on state-based diffusion models.\nLeveraging only low-dimensional vehicle states, the diffusion models achieve\nnotable inference efficiency while ensuring sufficient collision-free\ncharacteristics. By composing diffusion models, the proposed framework can\nsafely generalize across diverse scenarios, planning collision-free\ntrajectories even in unseen scenes. To further ensure the safety of the\ngenerated trajectories, an efficient, rule-based safety filter is proposed,\nwhich selects optimal trajectories that satisfy both sufficient safety and\ncontrol feasibility from among candidate trajectories. Both in seen and unseen\nscenarios, the proposed method achieves efficient inference time while\nmaintaining high safety and stability. Evaluations on the F1TENTH vehicle\nfurther demonstrate that the proposed method is practical in real-world\napplications. The project page is at: https://rstp-comp-diffuser.github.io/.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u72b6\u6001\u6269\u6563\u6a21\u578b\u7684\u5feb\u901f\u5b89\u5168\u8f68\u8ff9\u89c4\u5212\u6846\u67b6\uff0c\u517c\u987e\u8ba1\u7b97\u6548\u7387\u4e0e\u5b89\u5168\u6027\u3002", "motivation": "\u590d\u6742\u73af\u5883\u4e2d\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5e73\u8861\u8ba1\u7b97\u6548\u7387\u4e0e\u5b89\u5168\u6027\uff0c\u9700\u6539\u8fdb\u3002", "method": "\u5229\u7528\u4f4e\u7ef4\u8f66\u8f86\u72b6\u6001\u7684\u6269\u6563\u6a21\u578b\uff0c\u7ed3\u5408\u89c4\u5219\u5b89\u5168\u8fc7\u6ee4\u5668\u751f\u6210\u5b89\u5168\u8f68\u8ff9\u3002", "result": "\u5728\u5df2\u77e5\u548c\u672a\u77e5\u573a\u666f\u4e2d\u5747\u80fd\u9ad8\u6548\u751f\u6210\u5b89\u5168\u7a33\u5b9a\u8f68\u8ff9\uff0cF1TENTH\u8f66\u8f86\u9a8c\u8bc1\u5b9e\u7528\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u8bc1\u5b89\u5168\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u8ba1\u7b97\u6548\u7387\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u3002"}}
{"id": "2507.03829", "categories": ["cs.AI", "I.2.4; I.2.1"], "pdf": "https://arxiv.org/pdf/2507.03829", "abs": "https://arxiv.org/abs/2507.03829", "authors": ["George Hannah", "Jacopo de Berardinis", "Terry R. Payne", "Valentina Tamma", "Andrew Mitchell", "Ellen Piercy", "Ewan Johnson", "Andrew Ng", "Harry Rostron", "Boris Konev"], "title": "RELRaE: LLM-Based Relationship Extraction, Labelling, Refinement, and Evaluation", "comment": "18 Pages, 8 Tables, Under-review at ISWC 2025", "summary": "A large volume of XML data is produced in experiments carried out by robots\nin laboratories. In order to support the interoperability of data between labs,\nthere is a motivation to translate the XML data into a knowledge graph. A key\nstage of this process is the enrichment of the XML schema to lay the foundation\nof an ontology schema. To achieve this, we present the RELRaE framework, a\nframework that employs large language models in different stages to extract and\naccurately label the relationships implicitly present in the XML schema. We\ninvestigate the capability of LLMs to accurately generate these labels and then\nevaluate them. Our work demonstrates that LLMs can be effectively used to\nsupport the generation of relationship labels in the context of lab automation,\nand that they can play a valuable role within semi-automatic ontology\ngeneration frameworks more generally.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faRELRaE\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ece\u5b9e\u9a8c\u5ba4\u673a\u5668\u4eba\u751f\u6210\u7684XML\u6570\u636e\u4e2d\u63d0\u53d6\u548c\u6807\u6ce8\u5173\u7cfb\uff0c\u652f\u6301\u77e5\u8bc6\u56fe\u8c31\u8f6c\u6362\u3002", "motivation": "\u5b9e\u9a8c\u5ba4\u673a\u5668\u4eba\u4ea7\u751f\u7684XML\u6570\u636e\u9700\u8981\u8f6c\u6362\u4e3a\u77e5\u8bc6\u56fe\u8c31\u4ee5\u5b9e\u73b0\u6570\u636e\u4e92\u64cd\u4f5c\u6027\uff0c\u5173\u952e\u6b65\u9aa4\u662f\u4e30\u5bccXML\u6a21\u5f0f\u4ee5\u6784\u5efa\u672c\u4f53\u6a21\u5f0f\u3002", "method": "\u91c7\u7528RELRaE\u6846\u67b6\uff0c\u5229\u7528LLM\u5728\u4e0d\u540c\u9636\u6bb5\u63d0\u53d6\u548c\u6807\u6ce8XML\u6a21\u5f0f\u4e2d\u9690\u542b\u7684\u5173\u7cfb\u3002", "result": "\u7814\u7a76\u8868\u660eLLM\u80fd\u6709\u6548\u751f\u6210\u5173\u7cfb\u6807\u7b7e\uff0c\u652f\u6301\u5b9e\u9a8c\u5ba4\u81ea\u52a8\u5316\u4e2d\u7684\u534a\u81ea\u52a8\u672c\u4f53\u751f\u6210\u3002", "conclusion": "LLM\u5728\u534a\u81ea\u52a8\u672c\u4f53\u751f\u6210\u6846\u67b6\u4e2d\u5177\u6709\u91cd\u8981\u4ef7\u503c\u3002"}}
{"id": "2507.04430", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.04430", "abs": "https://arxiv.org/abs/2507.04430", "authors": ["Ziqin Wang", "Jinyu Chen", "Xiangyi Zheng", "Qinan Liao", "Linjiang Huang", "Si Liu"], "title": "\"Hi AirStar, Guide Me to the Badminton Court.\"", "comment": null, "summary": "Unmanned Aerial Vehicles, operating in environments with relatively few\nobstacles, offer high maneuverability and full three-dimensional mobility. This\nallows them to rapidly approach objects and perform a wide range of tasks often\nchallenging for ground robots, making them ideal for exploration, inspection,\naerial imaging, and everyday assistance. In this paper, we introduce AirStar, a\nUAV-centric embodied platform that turns a UAV into an intelligent aerial\nassistant: a large language model acts as the cognitive core for environmental\nunderstanding, contextual reasoning, and task planning. AirStar accepts natural\ninteraction through voice commands and gestures, removing the need for a remote\ncontroller and significantly broadening its user base. It combines geospatial\nknowledge-driven long-distance navigation with contextual reasoning for\nfine-grained short-range control, resulting in an efficient and accurate\nvision-and-language navigation (VLN) capability.Furthermore, the system also\noffers built-in capabilities such as cross-modal question answering,\nintelligent filming, and target tracking. With a highly extensible framework,\nit supports seamless integration of new functionalities, paving the way toward\na general-purpose, instruction-driven intelligent UAV agent. The supplementary\nPPT is available at\n\\href{https://buaa-colalab.github.io/airstar.github.io}{https://buaa-colalab.github.io/airstar.github.io}.", "AI": {"tldr": "AirStar\u662f\u4e00\u4e2a\u57fa\u4e8e\u65e0\u4eba\u673a\u7684\u667a\u80fd\u52a9\u624b\u5e73\u53f0\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u73af\u5883\u7406\u89e3\u3001\u4efb\u52a1\u89c4\u5212\u548c\u81ea\u7136\u4ea4\u4e92\uff0c\u652f\u6301\u8fdc\u8ddd\u79bb\u5bfc\u822a\u548c\u7cbe\u7ec6\u63a7\u5236\uff0c\u5177\u5907\u591a\u529f\u80fd\u6269\u5c55\u6027\u3002", "motivation": "\u65e0\u4eba\u673a\u5728\u5f00\u653e\u73af\u5883\u4e2d\u5177\u6709\u9ad8\u673a\u52a8\u6027\u548c\u4e09\u7ef4\u79fb\u52a8\u80fd\u529b\uff0c\u9002\u5408\u6267\u884c\u591a\u79cd\u4efb\u52a1\uff0c\u4f46\u7f3a\u4e4f\u667a\u80fd\u4ea4\u4e92\u548c\u81ea\u4e3b\u89c4\u5212\u80fd\u529b\u3002AirStar\u65e8\u5728\u901a\u8fc7\u8bed\u8a00\u6a21\u578b\u548c\u81ea\u7136\u4ea4\u4e92\u63d0\u5347\u65e0\u4eba\u673a\u7684\u667a\u80fd\u5316\u6c34\u5e73\u3002", "method": "AirStar\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u4f5c\u4e3a\u8ba4\u77e5\u6838\u5fc3\uff0c\u652f\u6301\u8bed\u97f3\u548c\u624b\u52bf\u4ea4\u4e92\uff0c\u6574\u5408\u5730\u7406\u7a7a\u95f4\u77e5\u8bc6\u548c\u4e0a\u4e0b\u6587\u63a8\u7406\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u89c6\u89c9\u4e0e\u8bed\u8a00\u5bfc\u822a\uff08VLN\uff09\u3002", "result": "\u7cfb\u7edf\u5177\u5907\u8de8\u6a21\u6001\u95ee\u7b54\u3001\u667a\u80fd\u62cd\u6444\u548c\u76ee\u6807\u8ddf\u8e2a\u7b49\u529f\u80fd\uff0c\u6846\u67b6\u9ad8\u5ea6\u53ef\u6269\u5c55\uff0c\u652f\u6301\u65b0\u529f\u80fd\u7684\u65e0\u7f1d\u96c6\u6210\u3002", "conclusion": "AirStar\u4e3a\u901a\u7528\u6307\u4ee4\u9a71\u52a8\u7684\u667a\u80fd\u65e0\u4eba\u673a\u4ee3\u7406\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5c55\u793a\u4e86\u65e0\u4eba\u673a\u5728\u667a\u80fd\u8f85\u52a9\u9886\u57df\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.03834", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.03834", "abs": "https://arxiv.org/abs/2507.03834", "authors": ["Michael J. Zellinger", "Matt Thomson"], "title": "Economic Evaluation of LLMs", "comment": "14 pages, 6 figures", "summary": "Practitioners often navigate LLM performance trade-offs by plotting Pareto\nfrontiers of optimal accuracy-cost trade-offs. However, this approach offers no\nway to compare between LLMs with distinct strengths and weaknesses: for\nexample, a cheap, error-prone model vs a pricey but accurate one. To address\nthis gap, we propose economic evaluation of LLMs. Our framework quantifies the\nperformance trade-off of an LLM as a single number based on the economic\nconstraints of a concrete use case, all expressed in dollars: the cost of\nmaking a mistake, the cost of incremental latency, and the cost of abstaining\nfrom a query. We apply our economic evaluation framework to compare the\nperformance of reasoning and non-reasoning models on difficult questions from\nthe MATH benchmark, discovering that reasoning models offer better\naccuracy-cost tradeoffs as soon as the economic cost of a mistake exceeds\n\\$0.01. In addition, we find that single large LLMs often outperform cascades\nwhen the cost of making a mistake is as low as \\$0.1. Overall, our findings\nsuggest that when automating meaningful human tasks with AI models,\npractitioners should typically use the most powerful available model, rather\nthan attempt to minimize AI deployment costs, since deployment costs are likely\ndwarfed by the economic impact of AI errors.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ecf\u6d4e\u7ea6\u675f\u7684LLM\u8bc4\u4f30\u6846\u67b6\uff0c\u5c06\u6027\u80fd\u6743\u8861\u91cf\u5316\u4e3a\u5355\u4e00\u6570\u503c\uff0c\u53d1\u73b0\u63a8\u7406\u6a21\u578b\u5728\u9519\u8bef\u6210\u672c\u8d85\u8fc70.01\u7f8e\u5143\u65f6\u66f4\u5177\u4f18\u52bf\u3002", "motivation": "\u4f20\u7edfPareto\u524d\u6cbf\u65b9\u6cd5\u65e0\u6cd5\u6bd4\u8f83\u4e0d\u540c\u4f18\u7f3a\u70b9\u7684LLM\uff0c\u9700\u4e00\u79cd\u7ecf\u6d4e\u89c6\u89d2\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u91cf\u5316\u9519\u8bef\u6210\u672c\u3001\u5ef6\u8fdf\u6210\u672c\u548c\u653e\u5f03\u67e5\u8be2\u6210\u672c\uff0c\u5c06LLM\u6027\u80fd\u6743\u8861\u8f6c\u5316\u4e3a\u7ecf\u6d4e\u6570\u503c\u3002", "result": "\u63a8\u7406\u6a21\u578b\u5728\u9519\u8bef\u6210\u672c\u8d85\u8fc70.01\u7f8e\u5143\u65f6\u8868\u73b0\u66f4\u4f18\uff1b\u5355\u4e00\u5927\u6a21\u578b\u5728\u9519\u8bef\u6210\u672c\u4f4e\u81f30.1\u7f8e\u5143\u65f6\u4f18\u4e8e\u7ea7\u8054\u6a21\u578b\u3002", "conclusion": "\u5b9e\u8df5\u4e2d\u5e94\u4f18\u5148\u9009\u62e9\u6027\u80fd\u6700\u5f3a\u7684\u6a21\u578b\uff0c\u800c\u975e\u6700\u5c0f\u5316\u90e8\u7f72\u6210\u672c\uff0c\u56e0\u4e3aAI\u9519\u8bef\u7684\u6210\u672c\u66f4\u9ad8\u3002"}}
{"id": "2507.04443", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.04443", "abs": "https://arxiv.org/abs/2507.04443", "authors": ["Giuseppe Silano", "Daniel Bonilla Licea", "Hajar El Hammouti", "Martin Saska"], "title": "Free-Space Optical Communication-Driven NMPC Framework for Multi-Rotor Aerial Vehicles in Structured Inspection Scenarios", "comment": "Accepted for presentation to the 2025 IEEE International Conference\n  on Systems, Man, and Cybernetics (SMC), Vienna, Austria", "summary": "This paper introduces a Nonlinear Model Predictive Control (NMPC) framework\nfor communication-aware motion planning of Multi-Rotor Aerial Vehicles (MRAVs)\nusing Free-Space Optical (FSO) links. The scenario involves MRAVs equipped with\nbody-fixed optical transmitters and Unmanned Ground Vehicles (UGVs) acting as\nmobile relays, each outfitted with fixed conical Field-of-View (FoV) receivers.\nThe controller integrates optical connectivity constraints into the NMPC\nformulation to ensure beam alignment and minimum link quality, while also\nenabling UGV tracking and obstacle avoidance. The method supports both coplanar\nand tilted MRAV configurations. MATLAB simulations demonstrate its feasibility\nand effectiveness.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08NMPC\uff09\u7684\u901a\u4fe1\u611f\u77e5\u8fd0\u52a8\u89c4\u5212\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u65cb\u7ffc\u98de\u884c\u5668\uff08MRAVs\uff09\u901a\u8fc7\u81ea\u7531\u7a7a\u95f4\u5149\u901a\u4fe1\uff08FSO\uff09\u94fe\u8def\u7684\u8fd0\u52a8\u63a7\u5236\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u89e3\u51b3MRAVs\u5728\u79fb\u52a8\u4e2d\u901a\u8fc7FSO\u94fe\u8def\u4fdd\u6301\u901a\u4fe1\u8d28\u91cf\u7684\u95ee\u9898\uff0c\u540c\u65f6\u5b9e\u73b0\u5730\u9762\u65e0\u4eba\u8f66\uff08UGVs\uff09\u7684\u8ddf\u8e2a\u548c\u907f\u969c\u3002", "method": "\u65b9\u6cd5\u662f\u5c06\u5149\u5b66\u8fde\u63a5\u7ea6\u675f\u96c6\u6210\u5230NMPC\u6846\u67b6\u4e2d\uff0c\u786e\u4fdd\u5149\u675f\u5bf9\u51c6\u548c\u6700\u5c0f\u94fe\u8def\u8d28\u91cf\uff0c\u652f\u6301\u5171\u9762\u548c\u503e\u659c\u7684MRAV\u914d\u7f6e\u3002", "result": "MATLAB\u4eff\u771f\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u53ef\u884c\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "\u7ed3\u8bba\u8868\u660e\u8be5\u6846\u67b6\u80fd\u591f\u6709\u6548\u89e3\u51b3MRAVs\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u901a\u4fe1\u611f\u77e5\u8fd0\u52a8\u89c4\u5212\u95ee\u9898\u3002"}}
{"id": "2507.03839", "categories": ["cs.AI", "cs.GR"], "pdf": "https://arxiv.org/pdf/2507.03839", "abs": "https://arxiv.org/abs/2507.03839", "authors": ["Shuowen Li", "Kexin Wang", "Minglu Fang", "Danqi Huang", "Ali Asadipour", "Haipeng Mi", "Yitong Sun"], "title": "Participatory Evolution of Artificial Life Systems via Semantic Feedback", "comment": "10 pages", "summary": "We present a semantic feedback framework that enables natural language to\nguide the evolution of artificial life systems. Integrating a\nprompt-to-parameter encoder, a CMA-ES optimizer, and CLIP-based evaluation, the\nsystem allows user intent to modulate both visual outcomes and underlying\nbehavioral rules. Implemented in an interactive ecosystem simulation, the\nframework supports prompt refinement, multi-agent interaction, and emergent\nrule synthesis. User studies show improved semantic alignment over manual\ntuning and demonstrate the system's potential as a platform for participatory\ngenerative design and open-ended evolution.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8bed\u4e49\u53cd\u9988\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u6307\u5bfc\u4eba\u5de5\u751f\u547d\u7cfb\u7edf\u7684\u6f14\u5316\uff0c\u7ed3\u5408\u7f16\u7801\u5668\u3001\u4f18\u5316\u5668\u548c\u8bc4\u4f30\u6a21\u5757\uff0c\u5b9e\u73b0\u7528\u6237\u610f\u56fe\u5bf9\u89c6\u89c9\u7ed3\u679c\u548c\u884c\u4e3a\u89c4\u5219\u7684\u8c03\u63a7\u3002", "motivation": "\u65e8\u5728\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u63d0\u5347\u4eba\u5de5\u751f\u547d\u7cfb\u7edf\u7684\u8bed\u4e49\u5bf9\u9f50\u548c\u7528\u6237\u53c2\u4e0e\u5ea6\uff0c\u63a2\u7d22\u5f00\u653e\u5f0f\u6f14\u5316\u548c\u751f\u6210\u8bbe\u8ba1\u7684\u6f5c\u529b\u3002", "method": "\u6574\u5408\u4e86prompt-to-parameter\u7f16\u7801\u5668\u3001CMA-ES\u4f18\u5316\u5668\u548cCLIP\u8bc4\u4f30\u6a21\u5757\uff0c\u652f\u6301\u4ea4\u4e92\u5f0f\u751f\u6001\u7cfb\u7edf\u6a21\u62df\u3001\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u548c\u89c4\u5219\u5408\u6210\u3002", "result": "\u7528\u6237\u7814\u7a76\u8868\u660e\uff0c\u8be5\u7cfb\u7edf\u5728\u8bed\u4e49\u5bf9\u9f50\u4e0a\u4f18\u4e8e\u624b\u52a8\u8c03\u6574\uff0c\u5c55\u793a\u4e86\u5176\u5728\u751f\u6210\u8bbe\u8ba1\u548c\u5f00\u653e\u5f0f\u6f14\u5316\u4e2d\u7684\u5e73\u53f0\u6f5c\u529b\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u53c2\u4e0e\u5f0f\u751f\u6210\u8bbe\u8ba1\u548c\u5f00\u653e\u5f0f\u6f14\u5316\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u5de5\u5177\uff0c\u9a8c\u8bc1\u4e86\u81ea\u7136\u8bed\u8a00\u5f15\u5bfc\u4eba\u5de5\u751f\u547d\u7cfb\u7edf\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2507.04452", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.04452", "abs": "https://arxiv.org/abs/2507.04452", "authors": ["Mingdong Wu", "Lehong Wu", "Yizhuo Wu", "Weiyao Huang", "Hongwei Fan", "Zheyuan Hu", "Haoran Geng", "Jinzhou Li", "Jiahe Ying", "Long Yang", "Yuanpei Chen", "Hao Dong"], "title": "SimLauncher: Launching Sample-Efficient Real-world Robotic Reinforcement Learning via Simulation Pre-training", "comment": null, "summary": "Autonomous learning of dexterous, long-horizon robotic skills has been a\nlongstanding pursuit of embodied AI. Recent advances in robotic reinforcement\nlearning (RL) have demonstrated remarkable performance and robustness in\nreal-world visuomotor control tasks. However, applying RL in the real world\nfaces challenges such as low sample efficiency, slow exploration, and\nsignificant reliance on human intervention. In contrast, simulators offer a\nsafe and efficient environment for extensive exploration and data collection,\nwhile the visual sim-to-real gap, often a limiting factor, can be mitigated\nusing real-to-sim techniques. Building on these, we propose SimLauncher, a\nnovel framework that combines the strengths of real-world RL and\nreal-to-sim-to-real approaches to overcome these challenges. Specifically, we\nfirst pre-train a visuomotor policy in the digital twin simulation environment,\nwhich then benefits real-world RL in two ways: (1) bootstrapping target values\nusing extensive simulated demonstrations and real-world demonstrations derived\nfrom pre-trained policy rollouts, and (2) Incorporating action proposals from\nthe pre-trained policy for better exploration. We conduct comprehensive\nexperiments across multi-stage, contact-rich, and dexterous hand manipulation\ntasks. Compared to prior real-world RL approaches, SimLauncher significantly\nimproves sample efficiency and achieves near-perfect success rates. We hope\nthis work serves as a proof of concept and inspires further research on\nleveraging large-scale simulation pre-training to benefit real-world robotic\nRL.", "AI": {"tldr": "SimLauncher\u6846\u67b6\u7ed3\u5408\u771f\u5b9e\u4e16\u754cRL\u4e0e\u4eff\u771f\u6280\u672f\uff0c\u901a\u8fc7\u9884\u8bad\u7ec3\u7b56\u7565\u63d0\u5347\u6837\u672c\u6548\u7387\u4e0e\u6210\u529f\u7387\u3002", "motivation": "\u89e3\u51b3\u771f\u5b9e\u4e16\u754cRL\u7684\u4f4e\u6837\u672c\u6548\u7387\u3001\u63a2\u7d22\u6162\u548c\u4f9d\u8d56\u4eba\u5de5\u5e72\u9884\u95ee\u9898\u3002", "method": "\u5728\u6570\u5b57\u5b6a\u751f\u4eff\u771f\u73af\u5883\u4e2d\u9884\u8bad\u7ec3\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\uff0c\u5229\u7528\u4eff\u771f\u548c\u771f\u5b9e\u6f14\u793a\u5f15\u5bfcRL\uff0c\u5e76\u6574\u5408\u52a8\u4f5c\u5efa\u8bae\u4f18\u5316\u63a2\u7d22\u3002", "result": "\u5728\u591a\u9636\u6bb5\u3001\u63a5\u89e6\u4e30\u5bcc\u548c\u7075\u5de7\u624b\u64cd\u4f5c\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u5347\u6837\u672c\u6548\u7387\uff0c\u63a5\u8fd1\u5b8c\u7f8e\u6210\u529f\u7387\u3002", "conclusion": "SimLauncher\u8bc1\u660e\u4e86\u5927\u89c4\u6a21\u4eff\u771f\u9884\u8bad\u7ec3\u5bf9\u771f\u5b9e\u4e16\u754cRL\u7684\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u542f\u793a\u3002"}}
{"id": "2507.03868", "categories": ["cs.AI", "cs.CE", "cs.CY", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.03868", "abs": "https://arxiv.org/abs/2507.03868", "authors": ["Xinyi Wu", "Yanhao Jia", "Luwei Xiao", "Shuai Zhao", "Fengkuang Chiang", "Erik Cambria"], "title": "From Query to Explanation: Uni-RAG for Multi-Modal Retrieval-Augmented Learning in STEM", "comment": null, "summary": "In AI-facilitated teaching, leveraging various query styles to interpret\nabstract educational content is crucial for delivering effective and accessible\nlearning experiences. However, existing retrieval systems predominantly focus\non natural text-image matching and lack the capacity to address the diversity\nand ambiguity inherent in real-world educational scenarios. To address this\nlimitation, we develop a lightweight and efficient multi-modal retrieval\nmodule, named Uni-Retrieval, which extracts query-style prototypes and\ndynamically matches them with tokens from a continually updated Prompt Bank.\nThis Prompt Bank encodes and stores domain-specific knowledge by leveraging a\nMixture-of-Expert Low-Rank Adaptation (MoE-LoRA) module and can be adapted to\nenhance Uni-Retrieval's capability to accommodate unseen query types at test\ntime. To enable natural language educational content generation, we integrate\nthe original Uni-Retrieval with a compact instruction-tuned language model,\nforming a complete retrieval-augmented generation pipeline named Uni-RAG. Given\na style-conditioned query, Uni-RAG first retrieves relevant educational\nmaterials and then generates human-readable explanations, feedback, or\ninstructional content aligned with the learning objective. Experimental results\non SER and other multi-modal benchmarks show that Uni-RAG outperforms baseline\nretrieval and RAG systems in both retrieval accuracy and generation quality,\nwhile maintaining low computational cost. Our framework provides a scalable,\npedagogically grounded solution for intelligent educational systems, bridging\nretrieval and generation to support personalized, explainable, and efficient\nlearning assistance across diverse STEM scenarios.", "AI": {"tldr": "Uni-RAG\u6846\u67b6\u7ed3\u5408\u591a\u6a21\u6001\u68c0\u7d22\u4e0e\u751f\u6210\u6a21\u578b\uff0c\u63d0\u5347\u6559\u80b2\u5185\u5bb9\u68c0\u7d22\u4e0e\u751f\u6210\u7684\u6548\u7387\u548c\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u68c0\u7d22\u7cfb\u7edf\u96be\u4ee5\u5904\u7406\u6559\u80b2\u573a\u666f\u4e2d\u7684\u591a\u6837\u6027\u548c\u6a21\u7cca\u6027\uff0c\u9700\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f00\u53d1\u8f7b\u91cf\u7ea7\u591a\u6a21\u6001\u68c0\u7d22\u6a21\u5757Uni-Retrieval\uff0c\u7ed3\u5408Prompt Bank\u548cMoE-LoRA\u6a21\u5757\uff0c\u5e76\u4e0e\u6307\u4ee4\u8c03\u4f18\u8bed\u8a00\u6a21\u578b\u96c6\u6210\u5f62\u6210Uni-RAG\u3002", "result": "\u5728SER\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cUni-RAG\u5728\u68c0\u7d22\u51c6\u786e\u6027\u548c\u751f\u6210\u8d28\u91cf\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u7cfb\u7edf\uff0c\u4e14\u8ba1\u7b97\u6210\u672c\u4f4e\u3002", "conclusion": "Uni-RAG\u4e3a\u667a\u80fd\u6559\u80b2\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u4e2a\u6027\u5316\u3001\u53ef\u89e3\u91ca\u7684\u9ad8\u6548\u5b66\u4e60\u8f85\u52a9\u3002"}}
{"id": "2507.04523", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.04523", "abs": "https://arxiv.org/abs/2507.04523", "authors": ["Alexander Estornell", "Leonard Jung", "Michael Everett"], "title": "Verification of Visual Controllers via Compositional Geometric Transformations", "comment": null, "summary": "Perception-based neural network controllers are increasingly used in\nautonomous systems that rely on visual inputs to operate in the real world.\nEnsuring the safety of such systems under uncertainty is challenging. Existing\nverification techniques typically focus on Lp-bounded perturbations in the\npixel space, which fails to capture the low-dimensional structure of many\nreal-world effects. In this work, we introduce a novel verification framework\nfor perception-based controllers that can generate outer-approximations of\nreachable sets through explicitly modeling uncertain observations with\ngeometric perturbations. Our approach constructs a boundable mapping from\nstates to images, enabling the use of state-based verification tools while\naccounting for uncertainty in perception. We provide theoretical guarantees on\nthe soundness of our method and demonstrate its effectiveness across benchmark\ncontrol environments. This work provides a principled framework for certifying\nthe safety of perception-driven control systems under realistic visual\nperturbations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u611f\u77e5\u63a7\u5236\u5668\u9a8c\u8bc1\u6846\u67b6\uff0c\u901a\u8fc7\u51e0\u4f55\u6270\u52a8\u5efa\u6a21\u4e0d\u786e\u5b9a\u89c2\u6d4b\uff0c\u751f\u6210\u53ef\u8fbe\u96c6\u7684\u5916\u8fd1\u4f3c\u3002", "motivation": "\u73b0\u6709\u9a8c\u8bc1\u6280\u672f\u901a\u5e38\u5173\u6ce8\u50cf\u7d20\u7a7a\u95f4\u7684Lp\u6709\u754c\u6270\u52a8\uff0c\u65e0\u6cd5\u6355\u6349\u73b0\u5b9e\u4e16\u754c\u6548\u5e94\u7684\u4f4e\u7ef4\u7ed3\u6784\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u6709\u6548\u7684\u9a8c\u8bc1\u65b9\u6cd5\u3002", "method": "\u6784\u5efa\u4ece\u72b6\u6001\u5230\u56fe\u50cf\u7684\u53ef\u7ed1\u5b9a\u6620\u5c04\uff0c\u7ed3\u5408\u72b6\u6001\u9a8c\u8bc1\u5de5\u5177\uff0c\u663e\u5f0f\u5efa\u6a21\u51e0\u4f55\u6270\u52a8\u3002", "result": "\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\uff0c\u5e76\u5728\u57fa\u51c6\u63a7\u5236\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u4e3a\u611f\u77e5\u9a71\u52a8\u63a7\u5236\u7cfb\u7edf\u5728\u771f\u5b9e\u89c6\u89c9\u6270\u52a8\u4e0b\u7684\u5b89\u5168\u6027\u8ba4\u8bc1\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u6846\u67b6\u3002"}}
{"id": "2507.03870", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.03870", "abs": "https://arxiv.org/abs/2507.03870", "authors": ["Rahil P Mehta", "Yashwanthi Anand", "Manish Motwani", "Sandhya Saisubramanian"], "title": "Uncovering Systemic and Environment Errors in Autonomous Systems Using Differential Testing", "comment": null, "summary": "When an autonomous agent behaves undesirably, including failure to complete a\ntask, it can be difficult to determine whether the behavior is due to a\nsystemic agent error, such as flaws in the model or policy, or an environment\nerror, where a task is inherently infeasible under a given environment\nconfiguration, even for an ideal agent. As agents and their environments grow\nmore complex, identifying the error source becomes increasingly difficult but\ncritical for reliable deployment. We introduce AIProbe, a novel black-box\ntesting technique that applies differential testing to attribute undesirable\nagent behaviors either to agent deficiencies, such as modeling or training\nflaws, or due to environmental infeasibility. AIProbe first generates diverse\nenvironmental configurations and tasks for testing the agent, by modifying\nconfigurable parameters using Latin Hypercube sampling. It then solves each\ngenerated task using a search-based planner, independent of the agent. By\ncomparing the agent's performance to the planner's solution, AIProbe identifies\nwhether failures are due to errors in the agent's model or policy, or due to\nunsolvable task conditions. Our evaluation across multiple domains shows that\nAIProbe significantly outperforms state-of-the-art techniques in detecting both\ntotal and unique errors, thereby contributing to a reliable deployment of\nautonomous agents.", "AI": {"tldr": "AIProbe\u662f\u4e00\u79cd\u9ed1\u76d2\u6d4b\u8bd5\u6280\u672f\uff0c\u901a\u8fc7\u5dee\u5f02\u6d4b\u8bd5\u533a\u5206\u81ea\u4e3b\u4ee3\u7406\u884c\u4e3a\u9519\u8bef\u6e90\u4e8e\u4ee3\u7406\u7f3a\u9677\u8fd8\u662f\u73af\u5883\u4e0d\u53ef\u884c\u6027\u3002", "motivation": "\u968f\u7740\u81ea\u4e3b\u4ee3\u7406\u53ca\u5176\u73af\u5883\u590d\u6742\u6027\u589e\u52a0\uff0c\u8bc6\u522b\u884c\u4e3a\u9519\u8bef\u6765\u6e90\u53d8\u5f97\u56f0\u96be\u4f46\u5bf9\u53ef\u9760\u90e8\u7f72\u81f3\u5173\u91cd\u8981\u3002", "method": "AIProbe\u751f\u6210\u591a\u6837\u5316\u73af\u5883\u914d\u7f6e\u548c\u4efb\u52a1\uff0c\u4f7f\u7528\u62c9\u4e01\u8d85\u7acb\u65b9\u91c7\u6837\uff0c\u5e76\u901a\u8fc7\u72ec\u7acb\u641c\u7d22\u89c4\u5212\u5668\u89e3\u51b3\u4efb\u52a1\uff0c\u6bd4\u8f83\u4ee3\u7406\u4e0e\u89c4\u5212\u5668\u8868\u73b0\u4ee5\u5b9a\u4f4d\u9519\u8bef\u3002", "result": "\u8bc4\u4f30\u8868\u660eAIProbe\u5728\u68c0\u6d4b\u603b\u9519\u8bef\u548c\u72ec\u7279\u9519\u8bef\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "AIProbe\u6709\u52a9\u4e8e\u81ea\u4e3b\u4ee3\u7406\u7684\u53ef\u9760\u90e8\u7f72\uff0c\u6709\u6548\u533a\u5206\u4ee3\u7406\u7f3a\u9677\u4e0e\u73af\u5883\u4e0d\u53ef\u884c\u6027\u3002"}}
{"id": "2507.04524", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.04524", "abs": "https://arxiv.org/abs/2507.04524", "authors": ["Kefeng Huang", "Tingguang Li", "Yuzhen Liu", "Zhe Zhang", "Jiankun Wang", "Lei Han"], "title": "VLM-TDP: VLM-guided Trajectory-conditioned Diffusion Policy for Robust Long-Horizon Manipulation", "comment": null, "summary": "Diffusion policy has demonstrated promising performance in the field of\nrobotic manipulation. However, its effectiveness has been primarily limited in\nshort-horizon tasks, and its performance significantly degrades in the presence\nof image noise. To address these limitations, we propose a VLM-guided\ntrajectory-conditioned diffusion policy (VLM-TDP) for robust and long-horizon\nmanipulation. Specifically, the proposed method leverages state-of-the-art\nvision-language models (VLMs) to decompose long-horizon tasks into concise,\nmanageable sub-tasks, while also innovatively generating voxel-based\ntrajectories for each sub-task. The generated trajectories serve as a crucial\nconditioning factor, effectively steering the diffusion policy and\nsubstantially enhancing its performance. The proposed Trajectory-conditioned\nDiffusion Policy (TDP) is trained on trajectories derived from demonstration\ndata and validated using the trajectories generated by the VLM. Simulation\nexperimental results indicate that our method significantly outperforms\nclassical diffusion policies, achieving an average 44% increase in success\nrate, over 100% improvement in long-horizon tasks, and a 20% reduction in\nperformance degradation in challenging conditions, such as noisy images or\naltered environments. These findings are further reinforced by our real-world\nexperiments, where the performance gap becomes even more pronounced in\nlong-horizon tasks. Videos are available on https://youtu.be/g0T6h32OSC8", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cdVLM\u5f15\u5bfc\u7684\u8f68\u8ff9\u6761\u4ef6\u6269\u6563\u7b56\u7565\uff08VLM-TDP\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u6269\u6563\u7b56\u7565\u5728\u957f\u65f6\u7a0b\u4efb\u52a1\u548c\u56fe\u50cf\u566a\u58f0\u4e0b\u7684\u6027\u80fd\u95ee\u9898\u3002", "motivation": "\u6269\u6563\u7b56\u7565\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u957f\u65f6\u7a0b\u4efb\u52a1\u548c\u56fe\u50cf\u566a\u58f0\u4e0b\u6027\u80fd\u663e\u8457\u4e0b\u964d\u3002", "method": "\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5206\u89e3\u4efb\u52a1\u4e3a\u5b50\u4efb\u52a1\uff0c\u5e76\u751f\u6210\u4f53\u7d20\u8f68\u8ff9\u4f5c\u4e3a\u6269\u6563\u7b56\u7565\u7684\u6761\u4ef6\u3002", "result": "\u4eff\u771f\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u5728\u6210\u529f\u7387\u3001\u957f\u65f6\u7a0b\u4efb\u52a1\u548c\u6297\u566a\u58f0\u80fd\u529b\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u6269\u6563\u7b56\u7565\u3002", "conclusion": "VLM-TDP\u5728\u590d\u6742\u73af\u5883\u4e0b\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u957f\u65f6\u7a0b\u4efb\u52a1\u3002"}}
{"id": "2507.03876", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.03876", "abs": "https://arxiv.org/abs/2507.03876", "authors": ["Alyssa Loo", "Ellie Pavlick", "Roman Feiman"], "title": "LLMs model how humans induce logically structured rules", "comment": null, "summary": "A central goal of cognitive science is to provide a computationally explicit\naccount of both the structure of the mind and its development: what are the\nprimitive representational building blocks of cognition, what are the rules via\nwhich those primitives combine, and where do these primitives and rules come\nfrom in the first place? A long-standing debate concerns the adequacy of\nartificial neural networks as computational models that can answer these\nquestions, in particular in domains related to abstract cognitive function,\nsuch as language and logic. This paper argues that recent advances in neural\nnetworks -- specifically, the advent of large language models (LLMs) --\nrepresent an important shift in this debate. We test a variety of LLMs on an\nexisting experimental paradigm used for studying the induction of rules\nformulated over logical concepts. Across four experiments, we find converging\nempirical evidence that LLMs provide at least as good a fit to human behavior\nas models that implement a Bayesian probablistic language of thought (pLoT),\nwhich have been the best computational models of human behavior on the same\ntask. Moreover, we show that the LLMs make qualitatively different predictions\nabout the nature of the rules that are inferred and deployed in order to\ncomplete the task, indicating that the LLM is unlikely to be a mere\nimplementation of the pLoT solution. Based on these results, we argue that LLMs\nmay instantiate a novel theoretical account of the primitive representations\nand computations necessary to explain human logical concepts, with which future\nwork in cognitive science should engage.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u795e\u7ecf\u7f51\u7edc\uff08\u5c24\u5176\u662f\u5927\u8bed\u8a00\u6a21\u578bLLMs\uff09\u662f\u5426\u80fd\u89e3\u91ca\u4eba\u7c7b\u8ba4\u77e5\u7684\u62bd\u8c61\u529f\u80fd\uff0c\u901a\u8fc7\u5b9e\u9a8c\u53d1\u73b0LLMs\u5728\u903b\u8f91\u89c4\u5219\u5f52\u7eb3\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u6216\u7b49\u540c\u4e8e\u8d1d\u53f6\u65af\u6982\u7387\u601d\u7ef4\u6a21\u578b\uff08pLoT\uff09\uff0c\u5e76\u63d0\u51fa\u4e86LLMs\u53ef\u80fd\u4ee3\u8868\u4e00\u79cd\u65b0\u7684\u8ba4\u77e5\u7406\u8bba\u6846\u67b6\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u9a8c\u8bc1\u795e\u7ecf\u7f51\u7edc\uff08\u7279\u522b\u662fLLMs\uff09\u80fd\u5426\u4f5c\u4e3a\u89e3\u91ca\u4eba\u7c7b\u62bd\u8c61\u8ba4\u77e5\u529f\u80fd\uff08\u5982\u8bed\u8a00\u548c\u903b\u8f91\uff09\u7684\u8ba1\u7b97\u6a21\u578b\uff0c\u6311\u6218\u4f20\u7edf\u8d1d\u53f6\u65af\u6982\u7387\u601d\u7ef4\u6a21\u578b\uff08pLoT\uff09\u7684\u4f18\u8d8a\u6027\u3002", "method": "\u901a\u8fc7\u56db\u4e2a\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u591a\u79cdLLMs\u4e0epLoT\u6a21\u578b\u5728\u903b\u8f91\u89c4\u5219\u5f52\u7eb3\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5206\u6790\u5176\u5bf9\u4eba\u7c7b\u884c\u4e3a\u7684\u62df\u5408\u7a0b\u5ea6\u53ca\u89c4\u5219\u63a8\u65ad\u7684\u5dee\u5f02\u3002", "result": "LLMs\u5728\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0epLoT\u76f8\u5f53\u6216\u66f4\u4f18\uff0c\u4e14\u5176\u89c4\u5219\u63a8\u65ad\u65b9\u5f0f\u4e0epLoT\u6709\u672c\u8d28\u533a\u522b\uff0c\u8868\u660eLLMs\u5e76\u975epLoT\u7684\u7b80\u5355\u5b9e\u73b0\u3002", "conclusion": "LLMs\u53ef\u80fd\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u8ba4\u77e5\u7406\u8bba\u6846\u67b6\uff0c\u89e3\u91ca\u4e86\u4eba\u7c7b\u903b\u8f91\u6982\u5ff5\u7684\u57fa\u672c\u8868\u5f81\u548c\u8ba1\u7b97\u673a\u5236\uff0c\u503c\u5f97\u672a\u6765\u8ba4\u77e5\u79d1\u5b66\u7814\u7a76\u5173\u6ce8\u3002"}}
{"id": "2507.04568", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.04568", "abs": "https://arxiv.org/abs/2507.04568", "authors": ["Yixiao Ge", "Giulio Delama", "Martin Scheiber", "Alessandro Fornasier", "Pieter van Goor", "Stephan Weiss", "Robert Mahony"], "title": "The Difference between the Left and Right Invariant Extended Kalman Filter", "comment": "20 pages, 4 figures", "summary": "The extended Kalman filter (EKF) has been the industry standard for state\nestimation problems over the past sixty years. The Invariant Extended Kalman\nFilter (IEKF) is a recent development of the EKF for the class of group-affine\nsystems on Lie groups that has shown superior performance for inertial\nnavigation problems. The IEKF comes in two versions, left- and right- handed\nrespectively, and there is a perception in the robotics community that these\nfilters are different and one should choose the handedness of the IEKF to match\nhandedness of the measurement model for a given filtering problem. In this\npaper, we revisit these algorithms and demonstrate that the left- and right-\nIEKF algorithms (with reset step) are identical, that is, the choice of the\nhandedness does not affect the IEKF's performance when the reset step is\nproperly implemented. The reset step was not originally proposed as part of the\nIEKF, however, we provide simulations to show that the reset step improves\nasymptotic performance of all versions of the the filter, and should be\nincluded in all high performance algorithms. The GNSS-aided inertial navigation\nsystem (INS) is used as a motivating example to demonstrate the equivalence of\nthe two filters.", "AI": {"tldr": "\u8bba\u6587\u91cd\u65b0\u5ba1\u89c6\u4e86\u5de6\u3001\u53f3IEKF\u7b97\u6cd5\uff0c\u8bc1\u660e\u5b83\u4eec\u5728\u91cd\u7f6e\u6b65\u9aa4\u4e0b\u6027\u80fd\u76f8\u540c\uff0c\u4e14\u91cd\u7f6e\u6b65\u9aa4\u80fd\u63d0\u5347\u6240\u6709\u7248\u672c\u6ee4\u6ce2\u5668\u7684\u6e10\u8fd1\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u793e\u533a\u5bf9\u5de6\u3001\u53f3IEKF\u7b97\u6cd5\u6027\u80fd\u5dee\u5f02\u7684\u8bef\u89e3\uff0c\u5e76\u8bc1\u660e\u91cd\u7f6e\u6b65\u9aa4\u7684\u91cd\u8981\u6027\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u4eff\u771f\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u5de6\u3001\u53f3IEKF\u7b97\u6cd5\u5728\u91cd\u7f6e\u6b65\u9aa4\u4e0b\u7684\u6027\u80fd\u3002", "result": "\u5de6\u3001\u53f3IEKF\u7b97\u6cd5\u5728\u91cd\u7f6e\u6b65\u9aa4\u4e0b\u6027\u80fd\u76f8\u540c\uff0c\u91cd\u7f6e\u6b65\u9aa4\u663e\u8457\u63d0\u5347\u6ee4\u6ce2\u5668\u7684\u6e10\u8fd1\u6027\u80fd\u3002", "conclusion": "\u91cd\u7f6e\u6b65\u9aa4\u5e94\u7eb3\u5165\u9ad8\u6027\u80fd\u7b97\u6cd5\u4e2d\uff0c\u4e14\u5de6\u3001\u53f3IEKF\u7b97\u6cd5\u9009\u62e9\u4e0d\u5f71\u54cd\u6027\u80fd\u3002"}}
{"id": "2507.03904", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.03904", "abs": "https://arxiv.org/abs/2507.03904", "authors": ["Yingxuan Yang", "Ying Wen", "Jun Wang", "Weinan Zhang"], "title": "Agent Exchange: Shaping the Future of AI Agent Economics", "comment": null, "summary": "The rise of Large Language Models (LLMs) has transformed AI agents from\npassive computational tools into autonomous economic actors. This shift marks\nthe emergence of the agent-centric economy, in which agents take on active\neconomic roles-exchanging value, making strategic decisions, and coordinating\nactions with minimal human oversight. To realize this vision, we propose Agent\nExchange (AEX), a specialized auction platform designed to support the dynamics\nof the AI agent marketplace. AEX offers an optimized infrastructure for agent\ncoordination and economic participation. Inspired by Real-Time Bidding (RTB)\nsystems in online advertising, AEX serves as the central auction engine,\nfacilitating interactions among four ecosystem components: the User-Side\nPlatform (USP), which translates human goals into agent-executable tasks; the\nAgent-Side Platform (ASP), responsible for capability representation,\nperformance tracking, and optimization; Agent Hubs, which coordinate agent\nteams and participate in AEX-hosted auctions; and the Data Management Platform\n(DMP), ensuring secure knowledge sharing and fair value attribution. We outline\nthe design principles and system architecture of AEX, laying the groundwork for\nagent-based economic infrastructure in future AI ecosystems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faAgent Exchange (AEX)\uff0c\u4e00\u4e2a\u4e13\u4e3aAI\u4ee3\u7406\u7ecf\u6d4e\u8bbe\u8ba1\u7684\u62cd\u5356\u5e73\u53f0\uff0c\u652f\u6301\u4ee3\u7406\u95f4\u7684\u4ef7\u503c\u4ea4\u6362\u548c\u534f\u8c03\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u53d1\u5c55\uff0cAI\u4ee3\u7406\u4ece\u88ab\u52a8\u5de5\u5177\u8f6c\u53d8\u4e3a\u81ea\u4e3b\u7ecf\u6d4e\u53c2\u4e0e\u8005\uff0c\u9700\u8981\u4e13\u95e8\u7684\u57fa\u7840\u8bbe\u65bd\u652f\u6301\u5176\u7ecf\u6d4e\u6d3b\u52a8\u3002", "method": "AEX\u57fa\u4e8e\u5b9e\u65f6\u7ade\u4ef7\uff08RTB\uff09\u7cfb\u7edf\u8bbe\u8ba1\uff0c\u5305\u542b\u7528\u6237\u4fa7\u5e73\u53f0\uff08USP\uff09\u3001\u4ee3\u7406\u4fa7\u5e73\u53f0\uff08ASP\uff09\u3001\u4ee3\u7406\u4e2d\u5fc3\uff08Agent Hubs\uff09\u548c\u6570\u636e\u7ba1\u7406\u5e73\u53f0\uff08DMP\uff09\u56db\u4e2a\u7ec4\u4ef6\u3002", "result": "AEX\u4e3a\u4ee3\u7406\u7ecf\u6d4e\u63d0\u4f9b\u4e86\u4f18\u5316\u7684\u534f\u8c03\u548c\u53c2\u4e0e\u57fa\u7840\u8bbe\u65bd\uff0c\u652f\u6301\u4ee3\u7406\u95f4\u7684\u4efb\u52a1\u5206\u914d\u3001\u80fd\u529b\u5c55\u793a\u548c\u77e5\u8bc6\u5171\u4eab\u3002", "conclusion": "AEX\u4e3a\u672a\u6765AI\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u4ee3\u7406\u7ecf\u6d4e\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5c55\u793a\u4e86\u5176\u8bbe\u8ba1\u539f\u5219\u548c\u7cfb\u7edf\u67b6\u6784\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2507.04602", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.04602", "abs": "https://arxiv.org/abs/2507.04602", "authors": ["Skanda Harisha", "Jimmy G. D. Hester", "Aline Eid"], "title": "DragonFly: Single mmWave Radar 3D Localization of Highly Dynamic Tags in GPS-Denied Environments", "comment": "16 pages including appendix", "summary": "The accurate localization and tracking of dynamic targets, such as equipment,\npeople, vehicles, drones, robots, and the assets that they interact with in\nGPS-denied indoor environments is critical to enabling safe and efficient\noperations in the next generation of spatially aware industrial facilities.\nThis paper presents DragonFly , a 3D localization system of highly dynamic\nbackscatter tags using a single MIMO mmWave radar. The system delivers the\nfirst demonstration of a mmWave backscatter system capable of exploiting the\ncapabilities of MIMO radars for the 3D localization of mmID tags moving at high\nspeeds and accelerations at long ranges by introducing a critical Doppler\ndisambiguation algorithm and a fully integrated cross-polarized dielectric\nlens-based mmID tag consuming a mere 68 uW. DragonFly was extensively evaluated\nin static and dynamic configurations, including on a flying quadcopter, and\nbenchmarked against multiple baselines, demonstrating its ability to track the\npositions of multiple tags with a median 3D accuracy of 12 cm at speeds and\nacceleration on the order of 10 m/s-1 and 4 m/s-2 and at ranges of up to 50 m.", "AI": {"tldr": "DragonFly\u662f\u4e00\u79cd\u57fa\u4e8e\u5355MIMO\u6beb\u7c73\u6ce2\u96f7\u8fbe\u76843D\u5b9a\u4f4d\u7cfb\u7edf\uff0c\u7528\u4e8e\u9ad8\u901f\u52a8\u6001\u53cd\u5411\u6563\u5c04\u6807\u7b7e\u7684\u5b9a\u4f4d\u4e0e\u8ddf\u8e2a\uff0c\u5728GPS\u7f3a\u5931\u7684\u5ba4\u5185\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u5b9a\u4f4d\u3002", "motivation": "\u5728GPS\u7f3a\u5931\u7684\u5ba4\u5185\u73af\u5883\u4e2d\uff0c\u52a8\u6001\u76ee\u6807\uff08\u5982\u8bbe\u5907\u3001\u4eba\u5458\u3001\u8f66\u8f86\u7b49\uff09\u7684\u7cbe\u786e\u5b9a\u4f4d\u5bf9\u4e0b\u4e00\u4ee3\u7a7a\u95f4\u611f\u77e5\u5de5\u4e1a\u8bbe\u65bd\u7684\u5b89\u5168\u9ad8\u6548\u8fd0\u884c\u81f3\u5173\u91cd\u8981\u3002", "method": "\u7cfb\u7edf\u91c7\u7528MIMO\u6beb\u7c73\u6ce2\u96f7\u8fbe\uff0c\u5f15\u5165\u5173\u952e\u7684\u591a\u666e\u52d2\u89e3\u6b67\u4e49\u7b97\u6cd5\uff0c\u5e76\u4f7f\u7528\u4f4e\u529f\u8017\uff0868 uW\uff09\u7684\u4ea4\u53c9\u6781\u5316\u4ecb\u7535\u900f\u955cmmID\u6807\u7b7e\u3002", "result": "\u5728\u9759\u6001\u548c\u52a8\u6001\u914d\u7f6e\u4e0b\uff08\u5305\u62ec\u98de\u884c\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\uff09\u8fdb\u884c\u4e86\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u80fd\u591f\u4ee512 cm\u7684\u4e2d\u503c3D\u7cbe\u5ea6\u8ddf\u8e2a\u591a\u4e2a\u6807\u7b7e\uff0c\u901f\u5ea6\u8fbe10 m/s\uff0c\u52a0\u901f\u5ea6\u8fbe4 m/s\u00b2\uff0c\u8303\u56f4\u8fbe50 m\u3002", "conclusion": "DragonFly\u5c55\u793a\u4e86\u6beb\u7c73\u6ce2\u53cd\u5411\u6563\u5c04\u7cfb\u7edf\u5728\u9ad8\u901f\u52a8\u6001\u76ee\u68073D\u5b9a\u4f4d\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u5de5\u4e1a\u5e94\u7528\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.03916", "categories": ["cs.AI", "cs.CV", "68T01"], "pdf": "https://arxiv.org/pdf/2507.03916", "abs": "https://arxiv.org/abs/2507.03916", "authors": ["Yifan Jiang", "Yibo Xue", "Yukun Kang", "Pin Zheng", "Jian Peng", "Feiran Wu", "Changliang Xu"], "title": "Animation Needs Attention: A Holistic Approach to Slides Animation Comprehension with Visual-Language Models", "comment": "Appendix at:\n  https://github.com/PAMPAS-Lab/ANA-PPT-Anamation/blob/main/Appendix.pdf", "summary": "Slide animations, such as fade-ins, fly-ins, and wipes, are critical for\naudience engagement, efficient information delivery, and vivid visual\nexpression. However, most AI-driven slide-generation tools still lack native\nanimation support, and existing vision-language models (VLMs) struggle with\nanimation tasks due to the absence of public datasets and limited\ntemporal-reasoning capabilities. To address this gap, we release the first\npublic dataset for slide-animation modeling: 12,000 triplets of\nnatural-language descriptions, animation JSON files, and rendered videos,\ncollectively covering every built-in PowerPoint effect. Using this resource, we\nfine-tune Qwen-2.5-VL-7B with Low-Rank Adaptation (LoRA) and achieve consistent\nimprovements over GPT-4.1 and Gemini-2.5-Pro in BLEU-4, ROUGE-L, SPICE, and our\nCoverage-Order-Detail Assessment (CODA) metric, which evaluates action\ncoverage, temporal order, and detail fidelity. On a manually curated test set\nof slides, the LoRA model increases BLEU-4 by around 60%, ROUGE-L by 30%, and\nshows significant improvements in CODA-detail. This demonstrates that low-rank\nadaptation enables reliable temporal reasoning and generalization beyond\nsynthetic data. Overall, our dataset, LoRA-enhanced model, and CODA metric\nprovide a rigorous benchmark and foundation for future research on VLM-based\ndynamic slide generation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u516c\u5f00\u7684\u5e7b\u706f\u7247\u52a8\u753b\u6570\u636e\u96c6\uff0c\u5e76\u5229\u7528LoRA\u5fae\u8c03Qwen-2.5-VL-7B\u6a21\u578b\uff0c\u5728\u591a\u9879\u6307\u6807\u4e0a\u8d85\u8d8aGPT-4.1\u548cGemini-2.5-Pro\u3002", "motivation": "\u73b0\u6709AI\u9a71\u52a8\u7684\u5e7b\u706f\u7247\u751f\u6210\u5de5\u5177\u7f3a\u4e4f\u539f\u751f\u52a8\u753b\u652f\u6301\uff0c\u4e14\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u56e0\u7f3a\u4e4f\u516c\u5f00\u6570\u636e\u96c6\u548c\u65f6\u5e8f\u63a8\u7406\u80fd\u529b\u800c\u96be\u4ee5\u5904\u7406\u52a8\u753b\u4efb\u52a1\u3002", "method": "\u53d1\u5e03\u5305\u542b12,000\u7ec4\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u3001\u52a8\u753bJSON\u6587\u4ef6\u548c\u6e32\u67d3\u89c6\u9891\u7684\u6570\u636e\u96c6\uff0c\u5e76\u5229\u7528LoRA\u5fae\u8c03Qwen-2.5-VL-7B\u6a21\u578b\u3002", "result": "LoRA\u6a21\u578b\u5728BLEU-4\u3001ROUGE-L\u3001SPICE\u548cCODA\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0c\u5c24\u5176\u5728CODA-detail\u4e0a\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u6570\u636e\u96c6\u3001LoRA\u589e\u5f3a\u6a21\u578b\u548cCODA\u6307\u6807\u4e3a\u672a\u6765\u52a8\u6001\u5e7b\u706f\u7247\u751f\u6210\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u51c6\u548c\u57fa\u7840\u3002"}}
{"id": "2507.04620", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.04620", "abs": "https://arxiv.org/abs/2507.04620", "authors": ["Haotian Liu", "Yuchuang Tong", "Guanchen Liu", "Zhaojie Ju", "Zhengtao Zhang"], "title": "IDAGC: Adaptive Generalized Human-Robot Collaboration via Human Intent Estimation and Multimodal Policy Learning", "comment": "Accepted by IROS 2025", "summary": "In Human-Robot Collaboration (HRC), which encompasses physical interaction\nand remote cooperation, accurate estimation of human intentions and seamless\nswitching of collaboration modes to adjust robot behavior remain paramount\nchallenges. To address these issues, we propose an Intent-Driven Adaptive\nGeneralized Collaboration (IDAGC) framework that leverages multimodal data and\nhuman intent estimation to facilitate adaptive policy learning across\nmulti-tasks in diverse scenarios, thereby facilitating autonomous inference of\ncollaboration modes and dynamic adjustment of robotic actions. This framework\novercomes the limitations of existing HRC methods, which are typically\nrestricted to a single collaboration mode and lack the capacity to identify and\ntransition between diverse states. Central to our framework is a predictive\nmodel that captures the interdependencies among vision, language, force, and\nrobot state data to accurately recognize human intentions with a Conditional\nVariational Autoencoder (CVAE) and automatically switch collaboration modes. By\nemploying dedicated encoders for each modality and integrating extracted\nfeatures through a Transformer decoder, the framework efficiently learns\nmulti-task policies, while force data optimizes compliance control and intent\nestimation accuracy during physical interactions. Experiments highlights our\nframework's practical potential to advance the comprehensive development of\nHRC.", "AI": {"tldr": "\u63d0\u51faIDAGC\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u6570\u636e\u548c\u610f\u56fe\u4f30\u8ba1\u5b9e\u73b0\u81ea\u9002\u5e94\u534f\u4f5c\u6a21\u5f0f\u5207\u6362\uff0c\u63d0\u5347\u4eba\u673a\u534f\u4f5c\u7684\u7075\u6d3b\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u89e3\u51b3\u4eba\u673a\u534f\u4f5c\u4e2d\u610f\u56fe\u4f30\u8ba1\u4e0d\u51c6\u786e\u548c\u534f\u4f5c\u6a21\u5f0f\u5207\u6362\u4e0d\u7075\u6d3b\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528CVAE\u548c\u591a\u6a21\u6001\u6570\u636e\uff08\u89c6\u89c9\u3001\u8bed\u8a00\u3001\u529b\u3001\u673a\u5668\u4eba\u72b6\u6001\uff09\u6784\u5efa\u9884\u6d4b\u6a21\u578b\uff0c\u901a\u8fc7Transformer\u89e3\u7801\u5668\u6574\u5408\u7279\u5f81\uff0c\u5b9e\u73b0\u610f\u56fe\u8bc6\u522b\u548c\u6a21\u5f0f\u5207\u6362\u3002", "result": "\u6846\u67b6\u80fd\u591f\u51c6\u786e\u8bc6\u522b\u610f\u56fe\u5e76\u52a8\u6001\u8c03\u6574\u534f\u4f5c\u6a21\u5f0f\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "IDAGC\u6846\u67b6\u4e3a\u4eba\u673a\u534f\u4f5c\u7684\u5168\u9762\u53d1\u5c55\u63d0\u4f9b\u4e86\u5b9e\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.03928", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.03928", "abs": "https://arxiv.org/abs/2507.03928", "authors": ["Yiliu Sun", "Zicheng Zhao", "Sheng Wan", "Chen Gong"], "title": "CortexDebate: Debating Sparsely and Equally for Multi-Agent Debate", "comment": "Accepted by ACL 2025", "summary": "Nowadays, single Large Language Model (LLM) struggles with critical issues\nsuch as hallucination and inadequate reasoning abilities. To mitigate these\nissues, Multi-Agent Debate (MAD) has emerged as an effective strategy, where\nLLM agents engage in in-depth debates with others on tasks. However, existing\nMAD methods face two major issues: (a) too lengthy input contexts, which causes\nLLM agents to get lost in plenty of input information and experiences\nperformance drop; and (b) the overconfidence dilemma, where self-assured LLM\nagents dominate the debate, leading to low debating effectiveness. To address\nthese limitations, we propose a novel MAD method called \"CortexDebate\".\nInspired by the human brain's tendency to establish a sparse and dynamically\noptimized network among cortical areas governed by white matter, CortexDebate\nconstructs a sparse debating graph among LLM agents, where each LLM agent only\ndebates with the ones that are helpful to it. To optimize the graph, we propose\na module named McKinsey-based Debate Matter (MDM), which acts as an artificial\nanalog to white matter. By integrating the McKinsey Trust Formula, a\nwell-established measure of trustworthiness from sociology, MDM enables\ncredible evaluations that guide graph optimization. The effectiveness of our\nCortexDebate has been well demonstrated by extensive experimental results\nacross eight datasets from four task types.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCortexDebate\u7684\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u65b9\u6cd5\uff0c\u901a\u8fc7\u7a00\u758f\u8fa9\u8bba\u56fe\u548cMcKinsey-based Debate Matter\u6a21\u5757\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4e2d\u4e0a\u4e0b\u6587\u8fc7\u957f\u548c\u8fc7\u5ea6\u81ea\u4fe1\u7684\u95ee\u9898\u3002", "motivation": "\u5355\u4e00\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5b58\u5728\u5e7b\u89c9\u548c\u63a8\u7406\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u800c\u73b0\u6709\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\uff08MAD\uff09\u65b9\u6cd5\u53c8\u9762\u4e34\u8f93\u5165\u4e0a\u4e0b\u6587\u8fc7\u957f\u548c\u8fc7\u5ea6\u81ea\u4fe1\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faCortexDebate\u65b9\u6cd5\uff0c\u6784\u5efa\u7a00\u758f\u8fa9\u8bba\u56fe\uff0c\u6bcf\u4e2aLLM\u667a\u80fd\u4f53\u4ec5\u4e0e\u5bf9\u5176\u6709\u5e2e\u52a9\u7684\u667a\u80fd\u4f53\u8fa9\u8bba\uff0c\u5e76\u901a\u8fc7MDM\u6a21\u5757\u4f18\u5316\u56fe\u7ed3\u6784\u3002", "result": "\u5728\u56db\u4e2a\u4efb\u52a1\u7c7b\u578b\u7684\u516b\u4e2a\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86CortexDebate\u7684\u6709\u6548\u6027\u3002", "conclusion": "CortexDebate\u901a\u8fc7\u7a00\u758f\u8fa9\u8bba\u56fe\u548cMDM\u6a21\u5757\u663e\u8457\u63d0\u5347\u4e86\u591a\u667a\u80fd\u4f53\u8fa9\u8bba\u7684\u6548\u679c\u3002"}}
{"id": "2507.04633", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.04633", "abs": "https://arxiv.org/abs/2507.04633", "authors": ["Daqi Huang", "Zhehao Cai", "Yuzhi Hao", "Zechen Li", "Chee-Meng Chew"], "title": "PRISM: Pointcloud Reintegrated Inference via Segmentation and Cross-attention for Manipulation", "comment": null, "summary": "Robust imitation learning for robot manipulation requires comprehensive 3D\nperception, yet many existing methods struggle in cluttered environments. Fixed\ncamera view approaches are vulnerable to perspective changes, and 3D point\ncloud techniques often limit themselves to keyframes predictions, reducing\ntheir efficacy in dynamic, contact-intensive tasks. To address these\nchallenges, we propose PRISM, designed as an end-to-end framework that directly\nlearns from raw point cloud observations and robot states, eliminating the need\nfor pretrained models or external datasets. PRISM comprises three main\ncomponents: a segmentation embedding unit that partitions the raw point cloud\ninto distinct object clusters and encodes local geometric details; a\ncross-attention component that merges these visual features with processed\nrobot joint states to highlight relevant targets; and a diffusion module that\ntranslates the fused representation into smooth robot actions. With training on\n100 demonstrations per task, PRISM surpasses both 2D and 3D baseline policies\nin accuracy and efficiency within our simulated environments, demonstrating\nstrong robustness in complex, object-dense scenarios. Code and some demos are\navailable on https://github.com/czknuaa/PRISM.", "AI": {"tldr": "PRISM\u662f\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u6a21\u4eff\u5b66\u4e60\u6846\u67b6\uff0c\u76f4\u63a5\u4ece\u539f\u59cb\u70b9\u4e91\u548c\u673a\u5668\u4eba\u72b6\u6001\u5b66\u4e60\uff0c\u65e0\u9700\u9884\u8bad\u7ec3\u6a21\u578b\u6216\u5916\u90e8\u6570\u636e\u96c6\uff0c\u5728\u590d\u6742\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u5728\u6742\u4e71\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u56fa\u5b9a\u89c6\u89d2\u548c\u5173\u952e\u5e27\u9884\u6d4b\u9650\u5236\u4e86\u5176\u9c81\u68d2\u6027\u3002PRISM\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "PRISM\u5305\u542b\u4e09\u4e2a\u7ec4\u4ef6\uff1a\u5206\u5272\u5d4c\u5165\u5355\u5143\u5904\u7406\u70b9\u4e91\uff0c\u4ea4\u53c9\u6ce8\u610f\u529b\u878d\u5408\u89c6\u89c9\u7279\u5f81\u4e0e\u673a\u5668\u4eba\u72b6\u6001\uff0c\u6269\u6563\u6a21\u5757\u751f\u6210\u5e73\u6ed1\u52a8\u4f5c\u3002", "result": "\u5728\u6bcf\u4efb\u52a1100\u6b21\u6f14\u793a\u7684\u8bad\u7ec3\u4e0b\uff0cPRISM\u5728\u6a21\u62df\u73af\u5883\u4e2d\u8d85\u8d8a\u4e862D\u548c3D\u57fa\u7ebf\u7b56\u7565\uff0c\u8868\u73b0\u51fa\u9ad8\u9c81\u68d2\u6027\u3002", "conclusion": "PRISM\u4e3a\u590d\u6742\u73af\u5883\u4e2d\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u6a21\u4eff\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.03929", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.03929", "abs": "https://arxiv.org/abs/2507.03929", "authors": ["Mohimenul Kabir", "Kuldeep S Meel"], "title": "An ASP-Based Framework for MUSes", "comment": "To appear in ICLP 2025 Technical Communication", "summary": "Given an unsatisfiable formula, understanding the core reason for\nunsatisfiability is crucial in several applications. One effective way to\ncapture this is through the minimal unsatisfiable subset (MUS), the\nsubset-minimal set of clauses that remains unsatisfiable. Current research\nbroadly focuses on two directions: (i) enumerating as many MUSes as possible\nwithin a given time limit, and (ii) counting the total number of MUSes for a\ngiven unsatisfiable formula.\n  In this paper, we introduce an answer set programming-based framework, named\nMUS-ASP, designed for online enumeration of MUSes. ASP is a powerful tool for\nits strengths in knowledge representation and is particularly suitable for\nspecifying complex combinatorial problems. By translating MUS enumeration into\nanswer set solving, MUS-ASP leverages the computational efficiency of\nstate-of-the-art ASP systems. Our extensive experimental evaluation\ndemonstrates the effectiveness of MUS-ASP and highlights the acceleration in\nboth MUS enumeration and counting tasks, particularly when integrated within\nhybrid solvers, including the framework proposed in this paper.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7b54\u6848\u96c6\u7f16\u7a0b\uff08ASP\uff09\u7684\u6846\u67b6MUS-ASP\uff0c\u7528\u4e8e\u5728\u7ebf\u679a\u4e3e\u6700\u5c0f\u4e0d\u53ef\u6ee1\u8db3\u5b50\u96c6\uff08MUS\uff09\uff0c\u663e\u8457\u63d0\u5347\u4e86\u679a\u4e3e\u548c\u8ba1\u6570\u6548\u7387\u3002", "motivation": "\u7406\u89e3\u4e0d\u53ef\u6ee1\u8db3\u516c\u5f0f\u7684\u6838\u5fc3\u539f\u56e0\u5bf9\u8bb8\u591a\u5e94\u7528\u81f3\u5173\u91cd\u8981\uff0c\u800cMUS\u662f\u6355\u6349\u8fd9\u4e00\u539f\u56e0\u7684\u6709\u6548\u65b9\u6cd5\u3002\u5f53\u524d\u7814\u7a76\u96c6\u4e2d\u5728\u679a\u4e3e\u548c\u8ba1\u6570MUS\u4e0a\uff0c\u4f46\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u5c06MUS\u679a\u4e3e\u95ee\u9898\u8f6c\u5316\u4e3a\u7b54\u6848\u96c6\u6c42\u89e3\u95ee\u9898\uff0c\u5229\u7528ASP\u5728\u77e5\u8bc6\u8868\u793a\u548c\u7ec4\u5408\u95ee\u9898\u4e0a\u7684\u4f18\u52bf\uff0c\u8bbe\u8ba1MUS-ASP\u6846\u67b6\u3002", "result": "\u5b9e\u9a8c\u8868\u660eMUS-ASP\u5728\u679a\u4e3e\u548c\u8ba1\u6570MUS\u4efb\u52a1\u4e2d\u8868\u73b0\u9ad8\u6548\uff0c\u5c24\u5176\u5728\u6df7\u5408\u6c42\u89e3\u5668\u4e2d\u96c6\u6210\u65f6\u52a0\u901f\u6548\u679c\u663e\u8457\u3002", "conclusion": "MUS-ASP\u6846\u67b6\u901a\u8fc7ASP\u7684\u9ad8\u6548\u8ba1\u7b97\u80fd\u529b\uff0c\u4e3aMUS\u7684\u5728\u7ebf\u679a\u4e3e\u548c\u8ba1\u6570\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.04649", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.04649", "abs": "https://arxiv.org/abs/2507.04649", "authors": ["Tuan Dang", "Manfred Huber"], "title": "Bio-Inspired Hybrid Map: Spatial Implicit Local Frames and Topological Map for Mobile Cobot Navigation", "comment": null, "summary": "Navigation is a fundamental capacity for mobile robots, enabling them to\noperate autonomously in complex and dynamic environments. Conventional\napproaches use probabilistic models to localize robots and build maps\nsimultaneously using sensor observations. Recent approaches employ\nhuman-inspired learning, such as imitation and reinforcement learning, to\nnavigate robots more effectively. However, these methods suffer from high\ncomputational costs, global map inconsistency, and poor generalization to\nunseen environments. This paper presents a novel method inspired by how humans\nperceive and navigate themselves effectively in novel environments.\nSpecifically, we first build local frames that mimic how humans represent\nessential spatial information in the short term. Points in local frames are\nhybrid representations, including spatial information and learned features,\nso-called spatial-implicit local frames. Then, we integrate spatial-implicit\nlocal frames into the global topological map represented as a factor graph.\nLastly, we developed a novel navigation algorithm based on Rapid-Exploring\nRandom Tree Star (RRT*) that leverages spatial-implicit local frames and the\ntopological map to navigate effectively in environments. To validate our\napproach, we conduct extensive experiments in real-world datasets and in-lab\nenvironments. We open our source code at\nhttps://github.com/tuantdang/simn}{https://github.com/tuantdang/simn.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4eba\u7c7b\u611f\u77e5\u7684\u5bfc\u822a\u65b9\u6cd5\uff0c\u901a\u8fc7\u7a7a\u95f4\u9690\u5f0f\u5c40\u90e8\u5e27\u548c\u5168\u5c40\u62d3\u6251\u5730\u56fe\u7ed3\u5408RRT*\u7b97\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u9ad8\u8ba1\u7b97\u6210\u672c\u548c\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u5bfc\u822a\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u672c\u6587\u53d7\u4eba\u7c7b\u611f\u77e5\u542f\u53d1\uff0c\u63d0\u51fa\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u6784\u5efa\u7a7a\u95f4\u9690\u5f0f\u5c40\u90e8\u5e27\uff0c\u6574\u5408\u5230\u5168\u5c40\u62d3\u6251\u5730\u56fe\u4e2d\uff0c\u5e76\u5f00\u53d1\u57fa\u4e8eRRT*\u7684\u5bfc\u822a\u7b97\u6cd5\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u548c\u5b9e\u9a8c\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u65b0\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5bfc\u822a\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.03998", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.03998", "abs": "https://arxiv.org/abs/2507.03998", "authors": ["Thuy An Ha", "Bao Quoc Vo"], "title": "Toward Better Generalisation in Uncertainty Estimators: Leveraging Data-Agnostic Features", "comment": null, "summary": "Large Language Models (LLMs) often generate responses that are factually\nincorrect yet expressed with high confidence, which can pose serious risks for\nend users. To address this, it is essential for LLMs not only to produce\nanswers but also to provide accurate estimates of their correctness.\nUncertainty quantification methods have been introduced to assess the quality\nof LLM outputs, with factual accuracy being a key aspect of that quality. Among\nthese methods, those that leverage hidden states to train probes have shown\nparticular promise, as these internal representations encode information\nrelevant to the factuality of responses, making this approach the focus of this\npaper. However, the probe trained on the hidden states of one dataset often\nstruggles to generalise to another dataset of a different task or domain. To\naddress this limitation, we explore combining data-agnostic features with\nhidden-state features and assess whether this hybrid feature set enhances\nout-of-domain performance. We further examine whether selecting only the most\ninformative hidden-state features, thereby discarding task-specific noise,\nenables the data-agnostic features to contribute more effectively. The\nexperiment results indicate that although introducing data-agnostic features\ngenerally enhances generalisation performance in most cases, in certain\nscenarios their inclusion degrades performance. A similar pattern emerges when\nretaining only the most important hidden-state features - adding data-agnostic\nfeatures does not consistently further enhance performance compared to using\nthe full set of hidden-state features. A closer analysis reveals that, in some\nspecific cases, the trained probe underweights the data-agnostic features\nrelative to the hidden-state features, which we believe is the main reason why\nthe results are inconclusive.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u901a\u8fc7\u7ed3\u5408\u6570\u636e\u65e0\u5173\u7279\u5f81\u548c\u9690\u85cf\u72b6\u6001\u7279\u5f81\u6765\u63d0\u9ad8\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8f93\u51fa\u7684\u4e8b\u5b9e\u51c6\u786e\u6027\uff0c\u5e76\u8bc4\u4f30\u4e86\u8fd9\u79cd\u6df7\u5408\u7279\u5f81\u96c6\u5728\u8de8\u57df\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "LLM\u5e38\u751f\u6210\u9ad8\u81ea\u4fe1\u4f46\u4e8b\u5b9e\u9519\u8bef\u7684\u56de\u7b54\uff0c\u9700\u91cf\u5316\u5176\u4e0d\u786e\u5b9a\u6027\u4ee5\u63d0\u9ad8\u8f93\u51fa\u8d28\u91cf\u3002\u9690\u85cf\u72b6\u6001\u7279\u5f81\u867d\u6709\u6548\uff0c\u4f46\u8de8\u57df\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u7ed3\u5408\u6570\u636e\u65e0\u5173\u7279\u5f81\u4e0e\u9690\u85cf\u72b6\u6001\u7279\u5f81\uff0c\u5e76\u7b5b\u9009\u6700\u5177\u4fe1\u606f\u91cf\u7684\u9690\u85cf\u72b6\u6001\u7279\u5f81\uff0c\u4ee5\u63d0\u5347\u8de8\u57df\u6027\u80fd\u3002", "result": "\u6df7\u5408\u7279\u5f81\u96c6\u5728\u591a\u6570\u60c5\u51b5\u4e0b\u63d0\u5347\u6cdb\u5316\u6027\u80fd\uff0c\u4f46\u5728\u67d0\u4e9b\u573a\u666f\u4e0b\u53cd\u800c\u964d\u4f4e\u6027\u80fd\u3002\u7b5b\u9009\u9690\u85cf\u72b6\u6001\u7279\u5f81\u540e\uff0c\u6570\u636e\u65e0\u5173\u7279\u5f81\u7684\u8d21\u732e\u4e0d\u4e00\u81f4\u3002", "conclusion": "\u6570\u636e\u65e0\u5173\u7279\u5f81\u4e0e\u9690\u85cf\u72b6\u6001\u7279\u5f81\u7684\u7ed3\u5408\u6548\u679c\u56e0\u573a\u666f\u800c\u5f02\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u7279\u5f81\u6743\u91cd\u5206\u914d\u95ee\u9898\u3002"}}
{"id": "2507.04661", "categories": ["cs.RO", "I.2.9; I.2.6"], "pdf": "https://arxiv.org/pdf/2507.04661", "abs": "https://arxiv.org/abs/2507.04661", "authors": ["Yayu Long", "Kewei Chen", "Long Jin", "Mingsheng Shang"], "title": "DRAE: Dynamic Retrieval-Augmented Expert Networks for Lifelong Learning and Task Adaptation in Robotics", "comment": "Accepted to the main conference of the Annual Meeting of the\n  Association for Computational Linguistics (ACL 2025)", "summary": "We introduce Dynamic Retrieval-Augmented Expert Networks (DRAE), a\ngroundbreaking architecture that addresses the challenges of lifelong learning,\ncatastrophic forgetting, and task adaptation by combining the dynamic routing\ncapabilities of Mixture-of-Experts (MoE); leveraging the knowledge-enhancement\npower of Retrieval-Augmented Generation (RAG); incorporating a novel\nhierarchical reinforcement learning (RL) framework; and coordinating through\nReflexNet-SchemaPlanner-HyperOptima (RSHO).DRAE dynamically routes expert\nmodels via a sparse MoE gating mechanism, enabling efficient resource\nallocation while leveraging external knowledge through parametric retrieval\n(P-RAG) to augment the learning process. We propose a new RL framework with\nReflexNet for low-level task execution, SchemaPlanner for symbolic reasoning,\nand HyperOptima for long-term context modeling, ensuring continuous adaptation\nand memory retention. Experimental results show that DRAE significantly\noutperforms baseline approaches in long-term task retention and knowledge\nreuse, achieving an average task success rate of 82.5% across a set of dynamic\nrobotic manipulation tasks, compared to 74.2% for traditional MoE models.\nFurthermore, DRAE maintains an extremely low forgetting rate, outperforming\nstate-of-the-art methods in catastrophic forgetting mitigation. These results\ndemonstrate the effectiveness of our approach in enabling flexible, scalable,\nand efficient lifelong learning for robotics.", "AI": {"tldr": "DRAE\u7ed3\u5408MoE\u3001RAG\u548c\u5206\u5c42RL\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8def\u7531\u548c\u5916\u90e8\u77e5\u8bc6\u589e\u5f3a\uff0c\u663e\u8457\u63d0\u5347\u7ec8\u8eab\u5b66\u4e60\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u7ec8\u8eab\u5b66\u4e60\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u548c\u4efb\u52a1\u9002\u5e94\u95ee\u9898\u3002", "method": "\u52a8\u6001\u8def\u7531MoE\u3001\u53c2\u6570\u5316\u68c0\u7d22RAG\u3001\u5206\u5c42RL\u6846\u67b6\uff08ReflexNet\u3001SchemaPlanner\u3001HyperOptima\uff09\u3002", "result": "\u4efb\u52a1\u6210\u529f\u738782.5%\uff0c\u9057\u5fd8\u7387\u6781\u4f4e\uff0c\u4f18\u4e8e\u4f20\u7edfMoE\u6a21\u578b\u3002", "conclusion": "DRAE\u5728\u673a\u5668\u4eba\u7ec8\u8eab\u5b66\u4e60\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u3001\u7075\u6d3b\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2507.04034", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.04034", "abs": "https://arxiv.org/abs/2507.04034", "authors": ["Weizhi Tang", "Kwabena Nuamah", "Vaishak Belle"], "title": "Lyria: A General LLM-Driven Genetic Algorithm Framework for Problem Solving", "comment": null, "summary": "While Large Language Models (LLMs) have demonstrated impressive abilities\nacross various domains, they still struggle with complex problems characterized\nby multi-objective optimization, precise constraint satisfaction, immense\nsolution spaces, etc. To address the limitation, drawing on the superior\nsemantic understanding ability of LLMs and also the outstanding global search\nand optimization capability of genetic algorithms, we propose to capitalize on\ntheir respective strengths and introduce Lyria, a general LLM-driven genetic\nalgorithm framework, comprising 7 essential components. Through conducting\nextensive experiments with 4 LLMs across 3 types of problems, we demonstrated\nthe efficacy of Lyria. Additionally, with 7 additional ablation experiments, we\nfurther systematically analyzed and elucidated the factors that affect its\nperformance.", "AI": {"tldr": "Lyria\u662f\u4e00\u4e2a\u7ed3\u5408LLMs\u548c\u9057\u4f20\u7b97\u6cd5\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u590d\u6742\u95ee\u9898\u3002", "motivation": "LLMs\u5728\u591a\u76ee\u6807\u4f18\u5316\u548c\u7ea6\u675f\u6ee1\u8db3\u7b49\u95ee\u9898\u4e0a\u8868\u73b0\u4e0d\u8db3\uff0c\u9700\u8981\u7ed3\u5408\u9057\u4f20\u7b97\u6cd5\u7684\u5168\u5c40\u641c\u7d22\u80fd\u529b\u3002", "method": "\u63d0\u51faLyria\u6846\u67b6\uff0c\u5305\u542b7\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff0c\u7ed3\u5408LLMs\u7684\u8bed\u4e49\u7406\u89e3\u548c\u9057\u4f20\u7b97\u6cd5\u7684\u4f18\u5316\u80fd\u529b\u3002", "result": "\u57284\u79cdLLMs\u548c3\u7c7b\u95ee\u9898\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86Lyria\u7684\u6709\u6548\u6027\uff0c\u5e76\u901a\u8fc77\u9879\u6d88\u878d\u5b9e\u9a8c\u5206\u6790\u4e86\u6027\u80fd\u5f71\u54cd\u56e0\u7d20\u3002", "conclusion": "Lyria\u901a\u8fc7\u7ed3\u5408LLMs\u548c\u9057\u4f20\u7b97\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u95ee\u9898\u7684\u89e3\u51b3\u80fd\u529b\u3002"}}
{"id": "2507.04686", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.04686", "abs": "https://arxiv.org/abs/2507.04686", "authors": ["Jing Liang", "Kasun Weerakoon", "Daeun Song", "Senthurbavan Kirubaharan", "Xuesu Xiao", "Dinesh Manocha"], "title": "MOSU: Autonomous Long-range Robot Navigation with Multi-modal Scene Understanding", "comment": null, "summary": "We present MOSU, a novel autonomous long-range navigation system that\nenhances global navigation for mobile robots through multimodal perception and\non-road scene understanding. MOSU addresses the outdoor robot navigation\nchallenge by integrating geometric, semantic, and contextual information to\nensure comprehensive scene understanding. The system combines GPS and QGIS\nmap-based routing for high-level global path planning and multi-modal\ntrajectory generation for local navigation refinement. For trajectory\ngeneration, MOSU leverages multi-modalities: LiDAR-based geometric data for\nprecise obstacle avoidance, image-based semantic segmentation for\ntraversability assessment, and Vision-Language Models (VLMs) to capture social\ncontext and enable the robot to adhere to social norms in complex environments.\nThis multi-modal integration improves scene understanding and enhances\ntraversability, allowing the robot to adapt to diverse outdoor conditions. We\nevaluate our system in real-world on-road environments and benchmark it on the\nGND dataset, achieving a 10% improvement in traversability on navigable\nterrains while maintaining a comparable navigation distance to existing global\nnavigation methods.", "AI": {"tldr": "MOSU\u662f\u4e00\u79cd\u65b0\u578b\u81ea\u4e3b\u957f\u8ddd\u79bb\u5bfc\u822a\u7cfb\u7edf\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u611f\u77e5\u548c\u9053\u8def\u573a\u666f\u7406\u89e3\u63d0\u5347\u79fb\u52a8\u673a\u5668\u4eba\u7684\u5168\u5c40\u5bfc\u822a\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u6237\u5916\u673a\u5668\u4eba\u5bfc\u822a\u7684\u6311\u6218\uff0c\u901a\u8fc7\u6574\u5408\u51e0\u4f55\u3001\u8bed\u4e49\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\u5b9e\u73b0\u5168\u9762\u7684\u573a\u666f\u7406\u89e3\u3002", "method": "\u7ed3\u5408GPS\u548cQGIS\u5730\u56fe\u8fdb\u884c\u5168\u5c40\u8def\u5f84\u89c4\u5212\uff0c\u5229\u7528LiDAR\u3001\u8bed\u4e49\u5206\u5272\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u8fdb\u884c\u5c40\u90e8\u5bfc\u822a\u4f18\u5316\u3002", "result": "\u5728GND\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0c\u5bfc\u822a\u5730\u5f62\u901a\u8fc7\u6027\u63d0\u534710%\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u5f53\u7684\u5bfc\u822a\u8ddd\u79bb\u3002", "conclusion": "MOSU\u901a\u8fc7\u591a\u6a21\u6001\u96c6\u6210\u663e\u8457\u63d0\u5347\u4e86\u6237\u5916\u673a\u5668\u4eba\u7684\u5bfc\u822a\u80fd\u529b\u3002"}}
{"id": "2507.04037", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.04037", "abs": "https://arxiv.org/abs/2507.04037", "authors": ["Zheng Jia", "Shengbin Yue", "Wei Chen", "Siyuan Wang", "Yidong Liu", "Yun Song", "Zhongyu Wei"], "title": "Ready Jurist One: Benchmarking Language Agents for Legal Intelligence in Dynamic Environments", "comment": null, "summary": "The gap between static benchmarks and the dynamic nature of real-world legal\npractice poses a key barrier to advancing legal intelligence. To this end, we\nintroduce J1-ENVS, the first interactive and dynamic legal environment tailored\nfor LLM-based agents. Guided by legal experts, it comprises six representative\nscenarios from Chinese legal practices across three levels of environmental\ncomplexity. We further introduce J1-EVAL, a fine-grained evaluation framework,\ndesigned to assess both task performance and procedural compliance across\nvarying levels of legal proficiency. Extensive experiments on 17 LLM agents\nreveal that, while many models demonstrate solid legal knowledge, they struggle\nwith procedural execution in dynamic settings. Even the SOTA model, GPT-4o,\nfalls short of 60% overall performance. These findings highlight persistent\nchallenges in achieving dynamic legal intelligence and offer valuable insights\nto guide future research.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86J1-ENVS\u548cJ1-EVAL\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u5728\u52a8\u6001\u6cd5\u5f8b\u73af\u5883\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u7a0b\u5e8f\u6267\u884c\u4e0a\u4ecd\u6709\u4e0d\u8db3\u3002", "motivation": "\u89e3\u51b3\u9759\u6001\u57fa\u51c6\u4e0e\u52a8\u6001\u6cd5\u5f8b\u5b9e\u8df5\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u63a8\u52a8\u6cd5\u5f8b\u667a\u80fd\u7684\u53d1\u5c55\u3002", "method": "\u5f00\u53d1\u4e86\u4ea4\u4e92\u5f0f\u6cd5\u5f8b\u73af\u5883J1-ENVS\u548c\u8bc4\u4f30\u6846\u67b6J1-EVAL\uff0c\u6d4b\u8bd5\u4e8617\u4e2aLLM\u4ee3\u7406\u3002", "result": "\u8bb8\u591a\u6a21\u578b\u5728\u6cd5\u5f8b\u77e5\u8bc6\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u7a0b\u5e8f\u6267\u884c\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0cGPT-4o\u6574\u4f53\u8868\u73b0\u672a\u8fbe60%\u3002", "conclusion": "\u52a8\u6001\u6cd5\u5f8b\u667a\u80fd\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u7814\u7a76\u7ed3\u679c\u4e3a\u672a\u6765\u65b9\u5411\u63d0\u4f9b\u4e86\u53c2\u8003\u3002"}}
{"id": "2507.04730", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.04730", "abs": "https://arxiv.org/abs/2507.04730", "authors": ["Giulio Schiavi", "Andrei Cramariuc", "Lionel Ott", "Roland Siegwart"], "title": "CueLearner: Bootstrapping and local policy adaptation from relative feedback", "comment": "Accepted to IROS 2025", "summary": "Human guidance has emerged as a powerful tool for enhancing reinforcement\nlearning (RL). However, conventional forms of guidance such as demonstrations\nor binary scalar feedback can be challenging to collect or have low information\ncontent, motivating the exploration of other forms of human input. Among these,\nrelative feedback (i.e., feedback on how to improve an action, such as \"more to\nthe left\") offers a good balance between usability and information richness.\nPrevious research has shown that relative feedback can be used to enhance\npolicy search methods. However, these efforts have been limited to specific\npolicy classes and use feedback inefficiently. In this work, we introduce a\nnovel method to learn from relative feedback and combine it with off-policy\nreinforcement learning. Through evaluations on two sparse-reward tasks, we\ndemonstrate our method can be used to improve the sample efficiency of\nreinforcement learning by guiding its exploration process. Additionally, we\nshow it can adapt a policy to changes in the environment or the user's\npreferences. Finally, we demonstrate real-world applicability by employing our\napproach to learn a navigation policy in a sparse reward setting.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u76f8\u5bf9\u53cd\u9988\u589e\u5f3a\u5f3a\u5316\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u79bb\u7b56\u7565\u5b66\u4e60\uff0c\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\u5e76\u9002\u5e94\u73af\u5883\u53d8\u5316\u3002", "motivation": "\u4f20\u7edf\u7684\u4eba\u7c7b\u6307\u5bfc\u5f62\u5f0f\uff08\u5982\u6f14\u793a\u6216\u4e8c\u5143\u53cd\u9988\uff09\u96be\u4ee5\u6536\u96c6\u6216\u4fe1\u606f\u91cf\u4f4e\uff0c\u76f8\u5bf9\u53cd\u9988\u5728\u53ef\u7528\u6027\u548c\u4fe1\u606f\u4e30\u5bcc\u6027\u4e4b\u95f4\u63d0\u4f9b\u4e86\u5e73\u8861\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u4ece\u76f8\u5bf9\u53cd\u9988\u4e2d\u5b66\u4e60\u7684\u65b0\u65b9\u6cd5\uff0c\u5e76\u5c06\u5176\u4e0e\u79bb\u7b56\u7565\u5f3a\u5316\u5b66\u4e60\u7ed3\u5408\uff0c\u901a\u8fc7\u7a00\u758f\u5956\u52b1\u4efb\u52a1\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u63d0\u9ad8\u5f3a\u5316\u5b66\u4e60\u7684\u6837\u672c\u6548\u7387\uff0c\u5e76\u9002\u5e94\u73af\u5883\u6216\u7528\u6237\u504f\u597d\u7684\u53d8\u5316\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u7a00\u758f\u5956\u52b1\u573a\u666f\u4e2d\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\uff0c\u5982\u5bfc\u822a\u7b56\u7565\u5b66\u4e60\u3002"}}
{"id": "2507.04067", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.04067", "abs": "https://arxiv.org/abs/2507.04067", "authors": ["Yuyang Cheng", "Yumiao Xu", "Chaojia Yu", "Yong Zhao"], "title": "HAWK: A Hierarchical Workflow Framework for Multi-Agent Collaboration", "comment": "AgentIR@SIGIR 2025", "summary": "Contemporary multi-agent systems encounter persistent challenges in\ncross-platform interoperability, dynamic task scheduling, and efficient\nresource sharing. Agents with heterogeneous implementations often lack\nstandardized interfaces; collaboration frameworks remain brittle and hard to\nextend; scheduling policies are static; and inter-agent state synchronization\nis insufficient. We propose Hierarchical Agent Workflow (HAWK), a modular\nframework comprising five layers-User, Workflow, Operator, Agent, and\nResource-and supported by sixteen standardized interfaces. HAWK delivers an\nend-to-end pipeline covering task parsing, workflow orchestration, intelligent\nscheduling, resource invocation, and data synchronization. At its core lies an\nadaptive scheduling and optimization module in the Workflow Layer, which\nharnesses real-time feedback and dynamic strategy adjustment to maximize\nutilization. The Resource Layer provides a unified abstraction over\nheterogeneous data sources, large models, physical devices, and third-party\nservices&tools, simplifying cross-domain information retrieval. We demonstrate\nHAWK's scalability and effectiveness via CreAgentive, a multi-agent\nnovel-generation prototype, which achieves marked gains in throughput, lowers\ninvocation complexity, and improves system controllability. We also show how\nhybrid deployments of large language models integrate seamlessly within HAWK,\nhighlighting its flexibility. Finally, we outline future research\navenues-hallucination mitigation, real-time performance tuning, and enhanced\ncross-domain adaptability-and survey prospective applications in healthcare,\ngovernment, finance, and education.", "AI": {"tldr": "HAWK\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u8bbe\u8ba1\u548c\u6807\u51c6\u5316\u63a5\u53e3\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u4e92\u64cd\u4f5c\u6027\u3001\u4efb\u52a1\u8c03\u5ea6\u548c\u8d44\u6e90\u5171\u4eab\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u539f\u578b\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5f53\u524d\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u9762\u4e34\u8de8\u5e73\u53f0\u4e92\u64cd\u4f5c\u6027\u5dee\u3001\u4efb\u52a1\u8c03\u5ea6\u9759\u6001\u5316\u3001\u8d44\u6e90\u5171\u4eab\u6548\u7387\u4f4e\u7b49\u95ee\u9898\uff0c\u7f3a\u4e4f\u6807\u51c6\u5316\u63a5\u53e3\u548c\u7075\u6d3b\u534f\u4f5c\u6846\u67b6\u3002", "method": "\u63d0\u51faHAWK\u6846\u67b6\uff0c\u5305\u542b\u4e94\u5c42\u7ed3\u6784\u548c\u5341\u516d\u4e2a\u6807\u51c6\u5316\u63a5\u53e3\uff0c\u652f\u6301\u4efb\u52a1\u89e3\u6790\u3001\u5de5\u4f5c\u6d41\u7f16\u6392\u3001\u667a\u80fd\u8c03\u5ea6\u7b49\u529f\u80fd\uff0c\u6838\u5fc3\u662f\u81ea\u9002\u5e94\u8c03\u5ea6\u6a21\u5757\u3002", "result": "\u901a\u8fc7CreAgentive\u539f\u578b\u9a8c\u8bc1\uff0cHAWK\u63d0\u9ad8\u4e86\u541e\u5410\u91cf\u3001\u964d\u4f4e\u4e86\u8c03\u7528\u590d\u6742\u5ea6\uff0c\u5e76\u589e\u5f3a\u4e86\u7cfb\u7edf\u53ef\u63a7\u6027\u3002", "conclusion": "HAWK\u5c55\u793a\u4e86\u5728\u591a\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u672a\u6765\u7814\u7a76\u65b9\u5411\u5305\u62ec\u5e7b\u89c9\u7f13\u89e3\u3001\u5b9e\u65f6\u6027\u80fd\u4f18\u5316\u548c\u8de8\u57df\u9002\u5e94\u6027\u63d0\u5347\u3002"}}
{"id": "2507.04789", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.04789", "abs": "https://arxiv.org/abs/2507.04789", "authors": ["Yinuo Zhao", "Jiale Yuan", "Zhiyuan Xu", "Xiaoshuai Hao", "Xinyi Zhang", "Kun Wu", "Zhengping Che", "Chi Harold Liu", "Jian Tang"], "title": "Training-free Generation of Temporally Consistent Rewards from VLMs", "comment": null, "summary": "Recent advances in vision-language models (VLMs) have significantly improved\nperformance in embodied tasks such as goal decomposition and visual\ncomprehension. However, providing accurate rewards for robotic manipulation\nwithout fine-tuning VLMs remains challenging due to the absence of\ndomain-specific robotic knowledge in pre-trained datasets and high\ncomputational costs that hinder real-time applicability. To address this, we\npropose $\\mathrm{T}^2$-VLM, a novel training-free, temporally consistent\nframework that generates accurate rewards through tracking the status changes\nin VLM-derived subgoals. Specifically, our method first queries the VLM to\nestablish spatially aware subgoals and an initial completion estimate before\neach round of interaction. We then employ a Bayesian tracking algorithm to\nupdate the goal completion status dynamically, using subgoal hidden states to\ngenerate structured rewards for reinforcement learning (RL) agents. This\napproach enhances long-horizon decision-making and improves failure recovery\ncapabilities with RL. Extensive experiments indicate that $\\mathrm{T}^2$-VLM\nachieves state-of-the-art performance in two robot manipulation benchmarks,\ndemonstrating superior reward accuracy with reduced computation consumption. We\nbelieve our approach not only advances reward generation techniques but also\ncontributes to the broader field of embodied AI. Project website:\nhttps://t2-vlm.github.io/.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.04103", "categories": ["cs.AI", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.04103", "abs": "https://arxiv.org/abs/2507.04103", "authors": ["Dheeraj Vattikonda", "Santhoshi Ravichandran", "Emiliano Penaloza", "Hadi Nekoei", "Megh Thakkar", "Thibault Le Sellier de Chezelles", "Nicolas Gontier", "Miguel Mu\u00f1oz-M\u00e1rmol", "Sahar Omidi Shayegan", "Stefania Raimondo", "Xue Liu", "Alexandre Drouin", "Laurent Charlin", "Alexandre Pich\u00e9", "Alexandre Lacoste", "Massimo Caccia"], "title": "How to Train Your LLM Web Agent: A Statistical Diagnosis", "comment": null, "summary": "LLM-based web agents have recently made significant progress, but much of it\nhas occurred in closed-source systems, widening the gap with open-source\nalternatives. Progress has been held back by two key challenges: first, a\nnarrow focus on single-step tasks that overlooks the complexity of multi-step\nweb interactions; and second, the high compute costs required to post-train\nLLM-based web agents. To address this, we present the first statistically\ngrounded study on compute allocation for LLM web-agent post-training. Our\napproach uses a two-stage pipeline, training a Llama 3.1 8B student to imitate\na Llama 3.3 70B teacher via supervised fine-tuning (SFT), followed by on-policy\nreinforcement learning. We find this process highly sensitive to hyperparameter\nchoices, making exhaustive sweeps impractical. To spare others from expensive\ntrial-and-error, we sample 1,370 configurations and use bootstrapping to\nestimate effective hyperparameters. Our results show that combining SFT with\non-policy RL consistently outperforms either approach alone on both WorkArena\nand MiniWob++. Further, this strategy requires only 55% of the compute to match\nthe peak performance of pure SFT on MiniWob++, effectively pushing the\ncompute-performance Pareto frontier, and is the only strategy that can close\nthe gap with closed-source models.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u7f51\u9875\u4ee3\u7406\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\uff08SFT\u548cRL\uff09\u4f18\u5316\u8ba1\u7b97\u5206\u914d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u5e76\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u5f00\u6e90LLM\u7f51\u9875\u4ee3\u7406\u6027\u80fd\u843d\u540e\u4e8e\u95ed\u6e90\u7cfb\u7edf\uff0c\u4e3b\u8981\u56e0\u5355\u6b65\u4efb\u52a1\u5c40\u9650\u6027\u548c\u9ad8\u8ba1\u7b97\u6210\u672c\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\uff1a\u5148\u901a\u8fc7SFT\u6a21\u4eff\u6559\u5e08\u6a21\u578b\uff0c\u518d\u8fdb\u884c\u7b56\u7565\u5f3a\u5316\u5b66\u4e60\uff0c\u5e76\u901a\u8fc7\u91c7\u6837\u548c\u5f15\u5bfc\u4f18\u5316\u8d85\u53c2\u6570\u3002", "result": "SFT\u7ed3\u5408RL\u4f18\u4e8e\u5355\u72ec\u65b9\u6cd5\uff0c\u8ba1\u7b97\u6210\u672c\u964d\u4f4e55%\uff0c\u6027\u80fd\u63a5\u8fd1\u95ed\u6e90\u6a21\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u8ba1\u7b97\u5206\u914d\u95ee\u9898\uff0c\u63a8\u52a8\u4e86\u5f00\u6e90LLM\u7f51\u9875\u4ee3\u7406\u7684\u53d1\u5c55\u3002"}}
{"id": "2507.04790", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.04790", "abs": "https://arxiv.org/abs/2507.04790", "authors": ["Giwon Lee", "Wooseong Jeong", "Daehee Park", "Jaewoo Jeong", "Kuk-Jin Yoon"], "title": "Interaction-Merged Motion Planning: Effectively Leveraging Diverse Motion Datasets for Robust Planning", "comment": "Accepted at ICCV 2025", "summary": "Motion planning is a crucial component of autonomous robot driving. While\nvarious trajectory datasets exist, effectively utilizing them for a target\ndomain remains challenging due to differences in agent interactions and\nenvironmental characteristics. Conventional approaches, such as domain\nadaptation or ensemble learning, leverage multiple source datasets but suffer\nfrom domain imbalance, catastrophic forgetting, and high computational costs.\nTo address these challenges, we propose Interaction-Merged Motion Planning\n(IMMP), a novel approach that leverages parameter checkpoints trained on\ndifferent domains during adaptation to the target domain. IMMP follows a\ntwo-step process: pre-merging to capture agent behaviors and interactions,\nsufficiently extracting diverse information from the source domain, followed by\nmerging to construct an adaptable model that efficiently transfers diverse\ninteractions to the target domain. Our method is evaluated on various planning\nbenchmarks and models, demonstrating superior performance compared to\nconventional approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aIMMP\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5408\u5e76\u53c2\u6570\u68c0\u67e5\u70b9\u6765\u4f18\u5316\u8de8\u57df\u8fd0\u52a8\u89c4\u5212\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u8f68\u8ff9\u6570\u636e\u96c6\u5728\u76ee\u6807\u57df\u4e2d\u7684\u5e94\u7528\u5b58\u5728\u6311\u6218\uff0c\u5982\u57df\u4e0d\u5e73\u8861\u548c\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "IMMP\u91c7\u7528\u4e24\u6b65\u6d41\u7a0b\uff1a\u9884\u5408\u5e76\u4ee5\u6355\u83b7\u6e90\u57df\u884c\u4e3a\uff0c\u518d\u5408\u5e76\u4ee5\u6784\u5efa\u53ef\u9002\u5e94\u76ee\u6807\u57df\u7684\u6a21\u578b\u3002", "result": "\u5728\u591a\u4e2a\u89c4\u5212\u57fa\u51c6\u548c\u6a21\u578b\u4e0a\u9a8c\u8bc1\uff0cIMMP\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "IMMP\u4e3a\u8de8\u57df\u8fd0\u52a8\u89c4\u5212\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.04105", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.04105", "abs": "https://arxiv.org/abs/2507.04105", "authors": ["Jinwei Hu", "Yi Dong", "Zhengtao Ding", "Xiaowei Huang"], "title": "Enhancing Robustness of LLM-Driven Multi-Agent Systems through Randomized Smoothing", "comment": "Preprint accepted by Chinese Journal of Aeronautics", "summary": "This paper presents a defense framework for enhancing the safety of large\nlanguage model (LLM) empowered multi-agent systems (MAS) in safety-critical\ndomains such as aerospace. We apply randomized smoothing, a statistical\nrobustness certification technique, to the MAS consensus context, enabling\nprobabilistic guarantees on agent decisions under adversarial influence. Unlike\ntraditional verification methods, our approach operates in black-box settings\nand employs a two-stage adaptive sampling mechanism to balance robustness and\ncomputational efficiency. Simulation results demonstrate that our method\neffectively prevents the propagation of adversarial behaviors and\nhallucinations while maintaining consensus performance. This work provides a\npractical and scalable path toward safe deployment of LLM-based MAS in\nreal-world, high-stakes environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u589e\u5f3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u9a71\u52a8\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08MAS\uff09\u5728\u5b89\u5168\u5173\u952e\u9886\u57df\uff08\u5982\u822a\u7a7a\u822a\u5929\uff09\u4e2d\u7684\u5b89\u5168\u6027\u7684\u9632\u5fa1\u6846\u67b6\uff0c\u91c7\u7528\u968f\u673a\u5e73\u6ed1\u6280\u672f\u63d0\u4f9b\u6982\u7387\u4fdd\u8bc1\u3002", "motivation": "\u5728\u5b89\u5168\u5173\u952e\u9886\u57df\uff0cLLM\u9a71\u52a8\u7684MAS\u5bb9\u6613\u53d7\u5230\u5bf9\u6297\u6027\u884c\u4e3a\u548c\u5e7b\u89c9\u7684\u5f71\u54cd\uff0c\u9700\u8981\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u5b9e\u7528\u7684\u9632\u5fa1\u65b9\u6cd5\u3002", "method": "\u5e94\u7528\u968f\u673a\u5e73\u6ed1\u6280\u672f\uff0c\u7ed3\u5408\u4e24\u9636\u6bb5\u81ea\u9002\u5e94\u91c7\u6837\u673a\u5236\uff0c\u5728\u65e0\u9700\u767d\u76d2\u8bbf\u95ee\u7684\u60c5\u51b5\u4e0b\u5e73\u8861\u9c81\u68d2\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u963b\u6b62\u5bf9\u6297\u6027\u884c\u4e3a\u548c\u5e7b\u89c9\u7684\u4f20\u64ad\uff0c\u540c\u65f6\u4fdd\u6301\u5171\u8bc6\u6027\u80fd\u3002", "conclusion": "\u4e3aLLM\u9a71\u52a8\u7684MAS\u5728\u73b0\u5b9e\u9ad8\u98ce\u9669\u73af\u5883\u4e2d\u7684\u5b89\u5168\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2507.04791", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.04791", "abs": "https://arxiv.org/abs/2507.04791", "authors": ["Dionis Totsila", "Clemente Donoso", "Enrico Mingo Hoffman", "Jean-Baptiste Mouret", "Serena Ivaldi"], "title": "Safe Bimanual Teleoperation with Language-Guided Collision Avoidance", "comment": null, "summary": "Teleoperating precise bimanual manipulations in cluttered environments is\nchallenging for operators, who often struggle with limited spatial perception\nand difficulty estimating distances between target objects, the robot's body,\nobstacles, and the surrounding environment. To address these challenges, local\nrobot perception and control should assist the operator during teleoperation.\nIn this work, we introduce a safe teleoperation system that enhances operator\ncontrol by preventing collisions in cluttered environments through the\ncombination of immersive VR control and voice-activated collision avoidance.\nUsing HTC Vive controllers, operators directly control a bimanual mobile\nmanipulator, while spoken commands such as \"avoid the yellow tool\" trigger\nvisual grounding and segmentation to build 3D obstacle meshes. These meshes are\nintegrated into a whole-body controller to actively prevent collisions during\nteleoperation. Experiments in static, cluttered scenes demonstrate that our\nsystem significantly improves operational safety without compromising task\nefficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408VR\u63a7\u5236\u548c\u8bed\u97f3\u907f\u969c\u7684\u5b89\u5168\u9065\u64cd\u4f5c\u7cfb\u7edf\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5728\u6742\u4e71\u73af\u5883\u4e2d\u7684\u64cd\u4f5c\u5b89\u5168\u6027\u3002", "motivation": "\u89e3\u51b3\u64cd\u4f5c\u8005\u5728\u6742\u4e71\u73af\u5883\u4e2d\u9065\u64cd\u4f5c\u65f6\u56e0\u7a7a\u95f4\u611f\u77e5\u6709\u9650\u548c\u8ddd\u79bb\u4f30\u8ba1\u56f0\u96be\u5bfc\u81f4\u7684\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u6c89\u6d78\u5f0fVR\u63a7\u5236\u548c\u8bed\u97f3\u6fc0\u6d3b\u7684\u907f\u969c\u529f\u80fd\uff0c\u901a\u8fc7HTC Vive\u63a7\u5236\u5668\u76f4\u63a5\u63a7\u5236\u673a\u5668\u4eba\uff0c\u8bed\u97f3\u6307\u4ee4\u89e6\u53d1\u89c6\u89c9\u5206\u5272\u751f\u62103D\u969c\u788d\u7269\u7f51\u683c\uff0c\u5e76\u96c6\u6210\u5230\u5168\u8eab\u63a7\u5236\u5668\u4e2d\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7cfb\u7edf\u663e\u8457\u63d0\u9ad8\u4e86\u64cd\u4f5c\u5b89\u5168\u6027\uff0c\u4e14\u4e0d\u5f71\u54cd\u4efb\u52a1\u6548\u7387\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u6742\u4e71\u73af\u5883\u4e2d\u7684\u9065\u64cd\u4f5c\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b89\u5168\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.04136", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.04136", "abs": "https://arxiv.org/abs/2507.04136", "authors": ["Saksham Sahai Srivastava", "Vaneet Aggarwal"], "title": "A Technical Survey of Reinforcement Learning Techniques for Large Language Models", "comment": "24 pages, LaTeX source", "summary": "Reinforcement Learning (RL) has emerged as a transformative approach for\naligning and enhancing Large Language Models (LLMs), addressing critical\nchallenges in instruction following, ethical alignment, and reasoning\ncapabilities. This survey offers a comprehensive foundation on the integration\nof RL with language models, highlighting prominent algorithms such as Proximal\nPolicy Optimization (PPO), Q-Learning, and Actor-Critic methods. Additionally,\nit provides an extensive technical overview of RL techniques specifically\ntailored for LLMs, including foundational methods like Reinforcement Learning\nfrom Human Feedback (RLHF) and AI Feedback (RLAIF), as well as advanced\nstrategies such as Direct Preference Optimization (DPO) and Group Relative\nPolicy Optimization (GRPO). We systematically analyze their applications across\ndomains, i.e., from code generation to tool-augmented reasoning. We also\npresent a comparative taxonomy based on reward modeling, feedback mechanisms,\nand optimization strategies. Our evaluation highlights key trends. RLHF remains\ndominant for alignment, and outcome-based RL such as RLVR significantly\nimproves stepwise reasoning. However, persistent challenges such as reward\nhacking, computational costs, and scalable feedback collection underscore the\nneed for continued innovation. We further discuss emerging directions,\nincluding hybrid RL algorithms, verifier-guided training, and multi-objective\nalignment frameworks. This survey serves as a roadmap for researchers advancing\nRL-driven LLM development, balancing capability enhancement with safety and\nscalability.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u91cd\u70b9\u4ecb\u7ecd\u4e86RLHF\u3001RLAIF\u7b49\u5173\u952e\u65b9\u6cd5\u53ca\u5176\u5728\u6307\u4ee4\u9075\u5faa\u3001\u4f26\u7406\u5bf9\u9f50\u7b49\u9886\u57df\u7684\u5e94\u7528\u3002", "motivation": "\u63a2\u8ba8\u5982\u4f55\u901a\u8fc7RL\u63d0\u5347LLMs\u7684\u6307\u4ee4\u9075\u5faa\u3001\u4f26\u7406\u5bf9\u9f50\u548c\u63a8\u7406\u80fd\u529b\uff0c\u89e3\u51b3\u73b0\u6709\u6311\u6218\u3002", "method": "\u7efc\u8ff0\u4e86PPO\u3001Q-Learning\u3001Actor-Critic\u7b49\u65b9\u6cd5\uff0c\u5e76\u5206\u6790\u4e86RLHF\u3001RLAIF\u3001DPO\u3001GRPO\u7b49\u6280\u672f\u7684\u5e94\u7528\u3002", "result": "RLHF\u5728\u6a21\u578b\u5bf9\u9f50\u4e2d\u5360\u4e3b\u5bfc\uff0c\u800cRLVR\u663e\u8457\u63d0\u5347\u9010\u6b65\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u4ecd\u5b58\u5728\u5956\u52b1\u9ed1\u5ba2\u653b\u51fb\u3001\u8ba1\u7b97\u6210\u672c\u7b49\u95ee\u9898\u3002", "conclusion": "\u672a\u6765\u9700\u53d1\u5c55\u6df7\u5408RL\u7b97\u6cd5\u3001\u9a8c\u8bc1\u5668\u5f15\u5bfc\u8bad\u7ec3\u7b49\u65b9\u5411\uff0c\u4ee5\u5e73\u8861\u80fd\u529b\u63d0\u5347\u4e0e\u5b89\u5168\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2507.04846", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.04846", "abs": "https://arxiv.org/abs/2507.04846", "authors": ["Anna Zigelman", "Zitao Yu", "Rom Levy", "Yizhar Or"], "title": "Dynamics and multi-stability of a rotor-actuated Twistcar robot with passive steering joint", "comment": "Supporting Information is available at\n  https://yizhar.net.technion.ac.il/files/2025/06/SI-MATLAB-file-Anna-Z.zip", "summary": "The nonlinear dynamics of many under-actuated wheeled platforms are governed\nby nonholonomic constraints of no-skid for passively rolling wheels, coupled\nwith momentum balance. In most of theoretical models, the shape variables, i.e.\njoint angles, are directly prescribed as periodic inputs, such as steering\nangle of the Twistcar. In this work, we study a variant of the Twistcar model\nwhere the actuation input is periodic oscillations of an inertial rotor\nattached to the main body, while the steering joint is passively free to\nrotate. Remarkably, the dynamics of this model is extremely rich, and includes\nmultiplicity of periodic solutions, both symmetric and asymmetric, as well as\nstability transitions and bifurcations. We conduct numerical simulations as\nwell as asymptotic analysis of the vehicle's reduced equations of motion. We\nuse perturbation expansion in order to obtain leading-order dynamics under\nsymmetric periodic solution. Then, we utilize harmonic balance and further\nscaling assumptions in order to approximate the conditions for\nsymmetry-breaking pitchfork bifurcation and stability transition of the\nsymmetric periodic solution, as a function of actuation frequency and\nstructural parameters. The asymptotic results show good agreement with\nnumerical simulations. The results highlight the role of passive shape\nvariables in generating multi-stable periodic solutions for nonholonomic\nsystems of robotic locomotion.", "AI": {"tldr": "\u7814\u7a76\u4e86Twistcar\u6a21\u578b\u7684\u53d8\u4f53\uff0c\u901a\u8fc7\u60ef\u6027\u8f6c\u5b50\u7684\u5468\u671f\u6027\u632f\u8361\u9a71\u52a8\uff0c\u53d1\u73b0\u5176\u52a8\u529b\u5b66\u884c\u4e3a\u4e30\u5bcc\uff0c\u5305\u62ec\u591a\u7a33\u5b9a\u5468\u671f\u89e3\u548c\u5206\u5c94\u73b0\u8c61\u3002", "motivation": "\u63a2\u7d22\u975e\u5b8c\u6574\u7ea6\u675f\u7cfb\u7edf\u4e2d\u88ab\u52a8\u5f62\u72b6\u53d8\u91cf\u5bf9\u591a\u7a33\u5b9a\u5468\u671f\u89e3\u7684\u5f71\u54cd\u3002", "method": "\u6570\u503c\u6a21\u62df\u548c\u6e10\u8fd1\u5206\u6790\uff0c\u7ed3\u5408\u5fae\u6270\u5c55\u5f00\u548c\u8c10\u6ce2\u5e73\u8861\u6cd5\u3002", "result": "\u6e10\u8fd1\u5206\u6790\u4e0e\u6570\u503c\u6a21\u62df\u4e00\u81f4\uff0c\u63ed\u793a\u4e86\u5bf9\u79f0\u6027\u7834\u7f3a\u5206\u5c94\u548c\u7a33\u5b9a\u6027\u8f6c\u53d8\u7684\u6761\u4ef6\u3002", "conclusion": "\u88ab\u52a8\u5f62\u72b6\u53d8\u91cf\u5728\u975e\u5b8c\u6574\u7cfb\u7edf\u4e2d\u5bf9\u591a\u7a33\u5b9a\u5468\u671f\u89e3\u6709\u91cd\u8981\u4f5c\u7528\u3002"}}
{"id": "2507.04206", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.04206", "abs": "https://arxiv.org/abs/2507.04206", "authors": ["Sibei Liu", "Zhijian Hu"], "title": "Mpemba Effect in Large-Language Model Training Dynamics: A Minimal Analysis of the Valley-River model", "comment": null, "summary": "Learning rate (LR) schedules in large language model (LLM) training often\nfollow empirical templates: warm-up, constant plateau/stable phase, and decay\n(WSD). However, the mechanistic explanation for this strategy remains\nunderexplored, and the choice of plateau height and decay schedule is largely\nheuristic. In this paper, we connect training dynamics to a thermodynamic\nanalogy via the Mpemba effect - a phenomenon in which a hotter system cools\nfaster than a colder one when quenched into the same bath. We analyze a class\nof \"valley-river\" loss landscapes, where sharp (valley) directions equilibrate\nquickly, while flatter (river) directions govern global descent. The Mpemba\neffect provides an explanation for the necessity of the warm-up phase and\nmotivates a high plateau - rather than a low one - for accelerating loss\ndecrease during decay. We show that for certain loss landscapes, there exists\nan optimal plateau learning rate - the \"strong Mpemba point\" - at which the\nslowest mode vanishes, resulting in faster convergence during the decay phase.\nWe derive analytical conditions for its existence and estimate decay dynamics\nrequired to preserve the Mpemba advantage. Our minimal model and analysis offer\na principled justification for plateau-based schedulers and provide guidance\nfor tuning LR in LLMs with minimal hyperparameter sweep.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u70ed\u529b\u5b66\u7c7b\u6bd4\uff08Mpemba\u6548\u5e94\uff09\u89e3\u91ca\u4e86LLM\u8bad\u7ec3\u4e2d\u5b66\u4e60\u7387\u8c03\u5ea6\uff08WSD\u7b56\u7565\uff09\u7684\u673a\u5236\uff0c\u63d0\u51fa\u4e86\u201c\u5f3aMpemba\u70b9\u201d\u6982\u5ff5\uff0c\u4f18\u5316\u4e86\u5b66\u4e60\u7387\u5e73\u53f0\u9ad8\u5ea6\u548c\u8870\u51cf\u7b56\u7565\u3002", "motivation": "\u73b0\u6709\u5b66\u4e60\u7387\u8c03\u5ea6\u7b56\u7565\uff08\u5982WSD\uff09\u7f3a\u4e4f\u7406\u8bba\u89e3\u91ca\uff0c\u5e73\u53f0\u9ad8\u5ea6\u548c\u8870\u51cf\u8ba1\u5212\u591a\u4f9d\u8d56\u7ecf\u9a8c\uff0c\u672c\u6587\u65e8\u5728\u63d0\u4f9b\u7406\u8bba\u652f\u6301\u3002", "method": "\u7ed3\u5408\u70ed\u529b\u5b66Mpemba\u6548\u5e94\uff0c\u5206\u6790\u201c\u8c37-\u6cb3\u201d\u635f\u5931\u666f\u89c2\uff0c\u63a8\u5bfc\u5e73\u53f0\u5b66\u4e60\u7387\u7684\u6700\u4f18\u70b9\uff08\u5f3aMpemba\u70b9\uff09\u53ca\u5176\u5b58\u5728\u6761\u4ef6\u3002", "result": "\u8bc1\u660e\u4e86\u9ad8\u5e73\u53f0\u5b66\u4e60\u7387\u80fd\u52a0\u901f\u635f\u5931\u4e0b\u964d\uff0c\u5e76\u5b58\u5728\u6700\u4f18\u5e73\u53f0\u5b66\u4e60\u7387\u70b9\uff0c\u663e\u8457\u63d0\u5347\u8870\u51cf\u9636\u6bb5\u7684\u6536\u655b\u901f\u5ea6\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5e73\u53f0\u8c03\u5ea6\u5668\u63d0\u4f9b\u4e86\u7406\u8bba\u4f9d\u636e\uff0c\u5e76\u6307\u5bfcLLM\u5b66\u4e60\u7387\u8c03\u4f18\uff0c\u51cf\u5c11\u8d85\u53c2\u6570\u641c\u7d22\u3002"}}
{"id": "2507.04910", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.04910", "abs": "https://arxiv.org/abs/2507.04910", "authors": ["Ryo Yonetani"], "title": "Piggyback Camera: Easy-to-Deploy Visual Surveillance by Mobile Sensing on Commercial Robot Vacuums", "comment": null, "summary": "This paper presents Piggyback Camera, an easy-to-deploy system for visual\nsurveillance using commercial robot vacuums. Rather than requiring access to\ninternal robot systems, our approach mounts a smartphone equipped with a camera\nand Inertial Measurement Unit (IMU) on the robot, making it applicable to any\ncommercial robot without hardware modifications. The system estimates robot\nposes through neural inertial navigation and efficiently captures images at\nregular spatial intervals throughout the cleaning task. We develop a novel\ntest-time data augmentation method called Rotation-Augmented Ensemble (RAE) to\nmitigate domain gaps in neural inertial navigation. A loop closure method that\nexploits robot cleaning patterns further refines these estimated poses. We\ndemonstrate the system with an object mapping application that analyzes\ncaptured images to geo-localize objects in the environment. Experimental\nevaluation in retail environments shows that our approach achieves 0.83 m\nrelative pose error for robot localization and 0.97 m positional error for\nobject mapping of over 100 items.", "AI": {"tldr": "Piggyback Camera\u7cfb\u7edf\u5229\u7528\u667a\u80fd\u624b\u673a\u548cIMU\u5728\u5546\u7528\u626b\u5730\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u89c6\u89c9\u76d1\u63a7\uff0c\u65e0\u9700\u786c\u4ef6\u4fee\u6539\uff0c\u901a\u8fc7\u795e\u7ecf\u60ef\u6027\u5bfc\u822a\u548cRAE\u65b9\u6cd5\u63d0\u5347\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u6700\u7ec8\u5b9e\u73b0\u7269\u4f53\u6620\u5c04\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u65e0\u9700\u4fee\u6539\u786c\u4ef6\u5373\u53ef\u5728\u5546\u7528\u626b\u5730\u673a\u5668\u4eba\u4e0a\u90e8\u7f72\u7684\u89c6\u89c9\u76d1\u63a7\u7cfb\u7edf\uff0c\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u5bf9\u673a\u5668\u4eba\u5185\u90e8\u7cfb\u7edf\u7684\u4f9d\u8d56\u95ee\u9898\u3002", "method": "\u5728\u673a\u5668\u4eba\u4e0a\u5b89\u88c5\u5e26\u6444\u50cf\u5934\u7684\u667a\u80fd\u624b\u673a\u548cIMU\uff0c\u5229\u7528\u795e\u7ecf\u60ef\u6027\u5bfc\u822a\u4f30\u8ba1\u673a\u5668\u4eba\u4f4d\u59ff\uff0c\u5e76\u901a\u8fc7RAE\u65b9\u6cd5\u51cf\u5c11\u57df\u5dee\u8ddd\uff0c\u7ed3\u5408\u6e05\u6d01\u6a21\u5f0f\u4f18\u5316\u4f4d\u59ff\u4f30\u8ba1\u3002", "result": "\u5728\u96f6\u552e\u73af\u5883\u4e2d\uff0c\u7cfb\u7edf\u5b9e\u73b0\u4e860.83\u7c73\u7684\u76f8\u5bf9\u4f4d\u59ff\u8bef\u5dee\u548c0.97\u7c73\u7684\u7269\u4f53\u6620\u5c04\u4f4d\u7f6e\u8bef\u5dee\u3002", "conclusion": "Piggyback Camera\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u4f4e\u6210\u672c\u3001\u6613\u90e8\u7f72\u7684\u89c6\u89c9\u76d1\u63a7\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5546\u7528\u673a\u5668\u4eba\u3002"}}
{"id": "2507.04283", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.04283", "abs": "https://arxiv.org/abs/2507.04283", "authors": ["Roy Uziel", "Irit Chelly", "Oren Freifeld", "Ari Pakman"], "title": "Clustering via Self-Supervised Diffusion", "comment": null, "summary": "Diffusion models, widely recognized for their success in generative tasks,\nhave not yet been applied to clustering. We introduce Clustering via Diffusion\n(CLUDI), a self-supervised framework that combines the generative power of\ndiffusion models with pre-trained Vision Transformer features to achieve robust\nand accurate clustering. CLUDI is trained via a teacher-student paradigm: the\nteacher uses stochastic diffusion-based sampling to produce diverse cluster\nassignments, which the student refines into stable predictions. This\nstochasticity acts as a novel data augmentation strategy, enabling CLUDI to\nuncover intricate structures in high-dimensional data. Extensive evaluations on\nchallenging datasets demonstrate that CLUDI achieves state-of-the-art\nperformance in unsupervised classification, setting new benchmarks in\nclustering robustness and adaptability to complex data distributions.", "AI": {"tldr": "CLUDI\u662f\u4e00\u79cd\u81ea\u76d1\u7763\u6846\u67b6\uff0c\u7ed3\u5408\u6269\u6563\u6a21\u578b\u548c\u9884\u8bad\u7ec3Vision Transformer\u7279\u5f81\uff0c\u5b9e\u73b0\u9c81\u68d2\u4e14\u51c6\u786e\u7684\u805a\u7c7b\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5c1a\u672a\u5e94\u7528\u4e8e\u805a\u7c7b\u4efb\u52a1\uff0c\u56e0\u6b64\u63a2\u7d22\u5176\u5728\u805a\u7c7b\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u91c7\u7528\u5e08\u751f\u8303\u5f0f\u8bad\u7ec3\uff1a\u6559\u5e08\u901a\u8fc7\u968f\u673a\u6269\u6563\u91c7\u6837\u751f\u6210\u591a\u6837\u805a\u7c7b\u5206\u914d\uff0c\u5b66\u751f\u5c06\u5176\u4f18\u5316\u4e3a\u7a33\u5b9a\u9884\u6d4b\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u8fbe\u5230\u65e0\u76d1\u7763\u5206\u7c7b\u7684\u6700\u65b0\u6027\u80fd\u3002", "conclusion": "CLUDI\u4e3a\u9ad8\u7ef4\u6570\u636e\u805a\u7c7b\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\uff0c\u5177\u6709\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2507.04922", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.04922", "abs": "https://arxiv.org/abs/2507.04922", "authors": ["Yichuan Shi", "Hao Liu", "Haowen Zheng", "Haowen Yu", "Xianqi Liang", "Jie Li", "Minmin Ma", "Ximin Lyu"], "title": "Automated UAV-based Wind Turbine Blade Inspection: Blade Stop Angle Estimation and Blade Detail Prioritized Exposure Adjustment", "comment": "8 pages, 7 figures, accepted by IROS 2025", "summary": "Unmanned aerial vehicles (UAVs) are critical in the automated inspection of\nwind turbine blades. Nevertheless, several issues persist in this domain.\nFirstly, existing inspection platforms encounter challenges in meeting the\ndemands of automated inspection tasks and scenarios. Moreover, current blade\nstop angle estimation methods are vulnerable to environmental factors,\nrestricting their robustness. Additionally, there is an absence of real-time\nblade detail prioritized exposure adjustment during capture, where lost details\ncannot be restored through post-optimization. To address these challenges, we\nintroduce a platform and two approaches. Initially, a UAV inspection platform\nis presented to meet the automated inspection requirements. Subsequently, a\nFermat point based blade stop angle estimation approach is introduced,\nachieving higher precision and success rates. Finally, we propose a blade\ndetail prioritized exposure adjustment approach to ensure appropriate\nbrightness and preserve details during image capture. Extensive tests,\ncomprising over 120 flights across 10 wind turbine models in 5 operational wind\nfarms, validate the effectiveness of the proposed approaches in enhancing\ninspection autonomy.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u4eba\u673a\u5e73\u53f0\u548c\u4e24\u79cd\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u98ce\u529b\u6da1\u8f6e\u673a\u53f6\u7247\u81ea\u52a8\u68c0\u6d4b\u4e2d\u7684\u95ee\u9898\uff0c\u5305\u62ec\u5e73\u53f0\u8bbe\u8ba1\u3001\u53f6\u7247\u505c\u6b62\u89d2\u5ea6\u4f30\u8ba1\u548c\u66dd\u5149\u8c03\u6574\u3002", "motivation": "\u73b0\u6709\u65e0\u4eba\u673a\u68c0\u6d4b\u5e73\u53f0\u5728\u81ea\u52a8\u5316\u68c0\u6d4b\u4efb\u52a1\u548c\u573a\u666f\u4e2d\u5b58\u5728\u4e0d\u8db3\uff0c\u53f6\u7247\u505c\u6b62\u89d2\u5ea6\u4f30\u8ba1\u6613\u53d7\u73af\u5883\u5f71\u54cd\uff0c\u4e14\u7f3a\u4e4f\u5b9e\u65f6\u7ec6\u8282\u4f18\u5148\u7684\u66dd\u5149\u8c03\u6574\u3002", "method": "1. \u8bbe\u8ba1\u65e0\u4eba\u673a\u68c0\u6d4b\u5e73\u53f0\uff1b2. \u63d0\u51fa\u57fa\u4e8e\u8d39\u9a6c\u70b9\u7684\u53f6\u7247\u505c\u6b62\u89d2\u5ea6\u4f30\u8ba1\u65b9\u6cd5\uff1b3. \u63d0\u51fa\u53f6\u7247\u7ec6\u8282\u4f18\u5148\u7684\u66dd\u5149\u8c03\u6574\u65b9\u6cd5\u3002", "result": "\u57285\u4e2a\u98ce\u7535\u573a\u300110\u79cd\u98ce\u529b\u6da1\u8f6e\u673a\u6a21\u578b\u4e0a\u8fdb\u884c\u4e86120\u591a\u6b21\u98de\u884c\u6d4b\u8bd5\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u68c0\u6d4b\u7684\u81ea\u52a8\u5316\u6c34\u5e73\u3002"}}
{"id": "2507.04299", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.04299", "abs": "https://arxiv.org/abs/2507.04299", "authors": ["Joohyung Lee", "Yunsong Meng"], "title": "Answer Set Programming Modulo Theories and Reasoning about Continuous Changes", "comment": "In Proceedings of the 23rd International Joint Conference on\n  Artificial Intelligence (IJCAI 2013), pages 990-996, 2013", "summary": "Answer Set Programming Modulo Theories (ASPMT) is a new framework of tight\nintegration of answer set programming (ASP) and satisfiability modulo theories\n(SMT). Similar to the relationship between first-order logic and SMT, it is\nbased on a recent proposal of the functional stable model semantics by fixing\ninterpretations of background theories. Analogously to a known relationship\nbetween ASP and SAT, ``tight'' ASPMT programs can be translated into SMT\ninstances. We demonstrate the usefulness of ASPMT by enhancing action language\nC+ to handle continuous changes as well as discrete changes. We reformulate the\nsemantics of C+ in terms ofASPMT, and show that SMT solvers can be used to\ncompute the language. We also show how the language can represent cumulative\neffects on continuous resources.", "AI": {"tldr": "ASPMT\u662fASP\u4e0eSMT\u7d27\u5bc6\u7ed3\u5408\u7684\u65b0\u6846\u67b6\uff0c\u7c7b\u4f3c\u4e8e\u4e00\u9636\u903b\u8f91\u4e0eSMT\u7684\u5173\u7cfb\uff0c\u901a\u8fc7\u56fa\u5b9a\u80cc\u666f\u7406\u8bba\u7684\u89e3\u91ca\u5b9e\u73b0\u3002\u7c7b\u4f3c\u4e8eASP\u4e0eSAT\u7684\u5173\u7cfb\uff0c\"\u7d27\"ASPMT\u7a0b\u5e8f\u53ef\u8f6c\u5316\u4e3aSMT\u5b9e\u4f8b\u3002\u901a\u8fc7\u589e\u5f3a\u52a8\u4f5c\u8bed\u8a00C+\u5904\u7406\u8fde\u7eed\u548c\u79bb\u6563\u53d8\u5316\uff0c\u5c55\u793a\u4e86ASPMT\u7684\u5b9e\u7528\u6027\u3002", "motivation": "\u7ed3\u5408ASP\u4e0eSMT\u7684\u4f18\u52bf\uff0c\u63d0\u4f9b\u66f4\u5f3a\u5927\u7684\u903b\u8f91\u7f16\u7a0b\u6846\u67b6\uff0c\u652f\u6301\u8fde\u7eed\u548c\u79bb\u6563\u53d8\u5316\u7684\u5efa\u6a21\u3002", "method": "\u57fa\u4e8e\u529f\u80fd\u7a33\u5b9a\u6a21\u578b\u8bed\u4e49\uff0c\u56fa\u5b9a\u80cc\u666f\u7406\u8bba\u7684\u89e3\u91ca\uff0c\u5c06ASPMT\u7a0b\u5e8f\u8f6c\u5316\u4e3aSMT\u5b9e\u4f8b\u3002", "result": "\u6210\u529f\u5c06\u52a8\u4f5c\u8bed\u8a00C+\u7684\u8bed\u4e49\u7528ASPMT\u91cd\u65b0\u8868\u8ff0\uff0c\u5e76\u5229\u7528SMT\u6c42\u89e3\u5668\u8ba1\u7b97\u8bed\u8a00\u3002", "conclusion": "ASPMT\u4e3a\u903b\u8f91\u7f16\u7a0b\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u5c24\u5176\u5728\u5904\u7406\u8fde\u7eed\u8d44\u6e90\u7d2f\u79ef\u6548\u5e94\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2507.04949", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.04949", "abs": "https://arxiv.org/abs/2507.04949", "authors": ["Teng Xue", "Amirreza Razmjoo", "Yan Zhang", "Sylvain Calinon"], "title": "Unifying Robot Optimization: Monte Carlo Tree Search with Tensor Factorization", "comment": "46 pages, 8 figures", "summary": "Many robotic tasks, such as inverse kinematics, motion planning, and optimal\ncontrol, can be formulated as optimization problems. Solving these problems\ninvolves addressing nonlinear kinematics, complex contact dynamics, and\nlong-horizon planning, each posing distinct challenges for state-of-the-art\noptimization methods. To efficiently solve a wide range of tasks across varying\nscenarios, researchers either develop specialized algorithms for the task to\nachieve, or switch between different frameworks. Monte Carlo Tree Search (MCTS)\nis a general-purpose decision-making tool that enables strategic exploration\nacross problem instances without relying on task-specific structures. However,\nMCTS suffers from combinatorial complexity, leading to slow convergence and\nhigh memory usage. To address this limitation, we propose \\emph{Tensor Train\nTree Search} (TTTS), which leverages tensor factorization to exploit the\nseparable structure of decision trees. This yields a low-rank,\nlinear-complexity representation that significantly reduces both computation\ntime and storage requirements. We prove that TTTS can efficiently reach the\nbounded global optimum within a finite time. Experimental results across\ninverse kinematics, motion planning around obstacles, multi-stage motion\nplanning, and bimanual whole-body manipulation demonstrate the efficiency of\nTTTS on a diverse set of robotic tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTTTS\uff08Tensor Train Tree Search\uff09\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f20\u91cf\u5206\u89e3\u6280\u672f\u4f18\u5316\u51b3\u7b56\u6811\u7ed3\u6784\uff0c\u663e\u8457\u964d\u4f4e\u8ba1\u7b97\u65f6\u95f4\u548c\u5b58\u50a8\u9700\u6c42\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u673a\u5668\u4eba\u4efb\u52a1\u3002", "motivation": "\u673a\u5668\u4eba\u4efb\u52a1\uff08\u5982\u9006\u8fd0\u52a8\u5b66\u3001\u8fd0\u52a8\u89c4\u5212\u7b49\uff09\u901a\u5e38\u6d89\u53ca\u590d\u6742\u7684\u4f18\u5316\u95ee\u9898\uff0c\u73b0\u6709\u65b9\u6cd5\uff08\u5982MCTS\uff09\u5b58\u5728\u7ec4\u5408\u590d\u6742\u6027\u548c\u9ad8\u8d44\u6e90\u6d88\u8017\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528\u5f20\u91cf\u5206\u89e3\u6280\u672f\u5c06\u51b3\u7b56\u6811\u8868\u793a\u4e3a\u4f4e\u79e9\u7ebf\u6027\u590d\u6742\u5ea6\u7ed3\u6784\uff0c\u63d0\u51faTTTS\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eTTTS\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u9ad8\u6548\u4e14\u80fd\u6536\u655b\u5230\u5168\u5c40\u6700\u4f18\u89e3\u3002", "conclusion": "TTTS\u662f\u4e00\u79cd\u901a\u7528\u4e14\u9ad8\u6548\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u590d\u6742\u673a\u5668\u4eba\u4efb\u52a1\u3002"}}
{"id": "2507.04338", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.04338", "abs": "https://arxiv.org/abs/2507.04338", "authors": ["Abdullah M. Zyarah", "Dhireesha Kudithipudi"], "title": "Voltage Mode Winner-Take-All Circuit for Neuromorphic Systems", "comment": null, "summary": "Recent advances in neuromorphic computing demonstrate on-device learning\ncapabilities with low power consumption. One of the key learning units in these\nsystems is the winner-take-all circuit. In this research, we propose a\nwinner-take-all circuit that can be configured to achieve k-winner and\nhysteresis properties, simulated in IBM 65 nm node. The circuit dissipated 34.9\n$\\mu$W of power with a latency of 10.4 ns, while processing 1000 inputs. The\nutility of the circuit is demonstrated for spatial filtering and\nclassification.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u914d\u7f6e\u7684winner-take-all\u7535\u8def\uff0c\u652f\u6301k-winner\u548c\u6ede\u540e\u7279\u6027\uff0c\u529f\u8017\u4f4e\u3001\u5ef6\u8fdf\u5c0f\uff0c\u9002\u7528\u4e8e\u7a7a\u95f4\u6ee4\u6ce2\u548c\u5206\u7c7b\u3002", "motivation": "\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u5728\u4f4e\u529f\u8017\u8bbe\u5907\u4e0a\u5b9e\u73b0\u5b66\u4e60\u80fd\u529b\u7684\u9700\u6c42\u63a8\u52a8\u4e86winner-take-all\u7535\u8def\u7684\u7814\u7a76\u3002", "method": "\u5728IBM 65 nm\u5de5\u827a\u8282\u70b9\u4e0a\u8bbe\u8ba1\u5e76\u4eff\u771f\u4e86\u4e00\u79cd\u53ef\u914d\u7f6e\u7684winner-take-all\u7535\u8def\uff0c\u652f\u6301k-winner\u548c\u6ede\u540e\u7279\u6027\u3002", "result": "\u7535\u8def\u529f\u8017\u4e3a34.9 \u03bcW\uff0c\u5ef6\u8fdf\u4e3a10.4 ns\uff0c\u53ef\u5904\u74061000\u4e2a\u8f93\u5165\uff0c\u9002\u7528\u4e8e\u7a7a\u95f4\u6ee4\u6ce2\u548c\u5206\u7c7b\u4efb\u52a1\u3002", "conclusion": "\u8be5\u7535\u8def\u5728\u4f4e\u529f\u8017\u548c\u9ad8\u6548\u80fd\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u5b66\u4e60\u5355\u5143\u3002"}}
{"id": "2507.05098", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.05098", "abs": "https://arxiv.org/abs/2507.05098", "authors": ["Tobias Demmler", "Jakob H\u00e4ringer", "Andreas Tamke", "Thao Dang", "Alexander Hegai", "Lars Mikelsons"], "title": "Beyond Features: How Dataset Design Influences Multi-Agent Trajectory Prediction Performance", "comment": null, "summary": "Accurate trajectory prediction is critical for safe autonomous navigation,\nyet the impact of dataset design on model performance remains understudied.\nThis work systematically examines how feature selection, cross-dataset\ntransfer, and geographic diversity influence trajectory prediction accuracy in\nmulti-agent settings. We evaluate a state-of-the-art model using our novel L4\nMotion Forecasting dataset based on our own data recordings in Germany and the\nUS. This includes enhanced map and agent features. We compare our dataset to\nthe US-centric Argoverse 2 benchmark. First, we find that incorporating\nsupplementary map and agent features unique to our dataset, yields no\nmeasurable improvement over baseline features, demonstrating that modern\narchitectures do not need extensive feature sets for optimal performance. The\nlimited features of public datasets are sufficient to capture convoluted\ninteractions without added complexity. Second, we perform cross-dataset\nexperiments to evaluate how effective domain knowledge can be transferred\nbetween datasets. Third, we group our dataset by country and check the\nknowledge transfer between different driving cultures.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u6570\u636e\u96c6\u8bbe\u8ba1\u5bf9\u591a\u667a\u80fd\u4f53\u8f68\u8ff9\u9884\u6d4b\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u73b0\u4ee3\u67b6\u6784\u65e0\u9700\u590d\u6742\u7279\u5f81\u5373\u53ef\u5b9e\u73b0\u6700\u4f18\u6027\u80fd\uff0c\u4e14\u8de8\u6570\u636e\u96c6\u548c\u5730\u7406\u591a\u6837\u6027\u7684\u77e5\u8bc6\u8f6c\u79fb\u6548\u679c\u6709\u9650\u3002", "motivation": "\u8f68\u8ff9\u9884\u6d4b\u5bf9\u81ea\u52a8\u9a7e\u9a76\u5b89\u5168\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6570\u636e\u96c6\u8bbe\u8ba1\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u5c1a\u672a\u5145\u5206\u7814\u7a76\u3002", "method": "\u4f7f\u7528L4 Motion Forecasting\u6570\u636e\u96c6\uff08\u5305\u542b\u5fb7\u56fd\u548c\u7f8e\u56fd\u6570\u636e\uff09\u548cArgoverse 2\u57fa\u51c6\uff0c\u8bc4\u4f30\u7279\u5f81\u9009\u62e9\u3001\u8de8\u6570\u636e\u96c6\u8f6c\u79fb\u548c\u5730\u7406\u591a\u6837\u6027\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u8865\u5145\u7279\u5f81\u672a\u663e\u8457\u63d0\u5347\u6027\u80fd\uff1b\u8de8\u6570\u636e\u96c6\u548c\u5730\u7406\u591a\u6837\u6027\u7684\u77e5\u8bc6\u8f6c\u79fb\u6548\u679c\u6709\u9650\u3002", "conclusion": "\u73b0\u4ee3\u8f68\u8ff9\u9884\u6d4b\u6a21\u578b\u65e0\u9700\u590d\u6742\u7279\u5f81\uff0c\u516c\u5171\u6570\u636e\u96c6\u7684\u6709\u9650\u7279\u5f81\u5df2\u8db3\u591f\uff1b\u8de8\u6570\u636e\u96c6\u548c\u5730\u7406\u591a\u6837\u6027\u7684\u77e5\u8bc6\u8f6c\u79fb\u4ecd\u9700\u6539\u8fdb\u3002"}}
{"id": "2507.04348", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.04348", "abs": "https://arxiv.org/abs/2507.04348", "authors": ["Xingyang He", "Xiao Ling", "Jie Liu"], "title": "SmartThinker: Learning to Compress and Preserve Reasoning by Step-Level Length Control", "comment": null, "summary": "Large reasoning models (LRMs) have exhibited remarkable reasoning\ncapabilities through inference-time scaling, but this progress has also\nintroduced considerable redundancy and inefficiency into their reasoning\nprocesses, resulting in substantial computational waste. Previous work has\nattempted to mitigate this issue by penalizing the overall length of generated\nsamples during reinforcement learning (RL), with the goal of encouraging a more\nconcise chains of thought. However, we observe that such global length penalty\noften lead to excessive compression of critical reasoning steps while\npreserving unnecessary details in simpler ones, yielding a suboptimal trade-off\nbetween accuracy and efficiency. To address this issue, we propose\nSmartThinker, a two-stage learnable framework designed to enable fine-grained\ncontrol over the length of reasoning chains based on the importance of each\nindividual step. In the first stage, SmartThinker adapts a reasoning model to a\nshort-form reasoning mode through rejection sampling combined with supervised\nfine-tuning (SFT). In the second stage, SmartThinker applies Step-Level Length\nControl Policy Optimization (SCPO) to refine the model output distribution,\nwhich increases the proportion of length allocated to critical steps while\nreducing redundancy in less important ones. SCPO consists of four core\ncomponents: an online importance estimator, a step-level length control reward\nfunction, a step-level generalized advantage estimation (S-GAE) and a\ndifficulty-adaptive clipping strategy. Working in concert, these components\nenable SCPO to implement differentiated length control across reasoning steps.\nEmpirical results across multiple reasoning benchmarks and various backbone\nmodels demonstrate that SmartThinker significantly reduces redundant reasoning\nwhile achieving comparable or even superior performance to existing methods.", "AI": {"tldr": "SmartThinker\u6846\u67b6\u901a\u8fc7\u4e24\u9636\u6bb5\u5b66\u4e60\u5b9e\u73b0\u5bf9\u63a8\u7406\u94fe\u957f\u5ea6\u7684\u7ec6\u7c92\u5ea6\u63a7\u5236\uff0c\u51cf\u5c11\u5197\u4f59\u63a8\u7406\uff0c\u63d0\u5347\u6548\u7387\u3002", "motivation": "\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u5728\u63a8\u7406\u65f6\u5b58\u5728\u5197\u4f59\u548c\u4f4e\u6548\u95ee\u9898\uff0c\u5168\u5c40\u957f\u5ea6\u60e9\u7f5a\u65b9\u6cd5\u6548\u679c\u4e0d\u4f73\uff0c\u9700\u66f4\u7cbe\u7ec6\u7684\u63a7\u5236\u3002", "method": "\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1\uff09\u901a\u8fc7\u62d2\u7edd\u91c7\u6837\u548c\u76d1\u7763\u5fae\u8c03\u9002\u5e94\u77ed\u63a8\u7406\u6a21\u5f0f\uff1b2\uff09\u5e94\u7528SCPO\u4f18\u5316\u6a21\u578b\u8f93\u51fa\u5206\u5e03\uff0c\u5b9e\u73b0\u6b65\u9aa4\u7ea7\u957f\u5ea6\u63a7\u5236\u3002", "result": "\u5728\u591a\u4e2a\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSmartThinker\u663e\u8457\u51cf\u5c11\u5197\u4f59\u63a8\u7406\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "SmartThinker\u901a\u8fc7\u6b65\u9aa4\u7ea7\u957f\u5ea6\u63a7\u5236\uff0c\u5728\u4fdd\u6301\u6216\u63d0\u5347\u6027\u80fd\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u63a8\u7406\u6548\u7387\u3002"}}
{"id": "2507.05118", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.05118", "abs": "https://arxiv.org/abs/2507.05118", "authors": ["Danil S. Grigorev", "Alexey K. Kovalev", "Aleksandr I. Panov"], "title": "VerifyLLM: LLM-Based Pre-Execution Task Plan Verification for Robots", "comment": "IROS 2025", "summary": "In the field of robotics, researchers face a critical challenge in ensuring\nreliable and efficient task planning. Verifying high-level task plans before\nexecution significantly reduces errors and enhance the overall performance of\nthese systems. In this paper, we propose an architecture for automatically\nverifying high-level task plans before their execution in simulator or\nreal-world environments. Leveraging Large Language Models (LLMs), our approach\nconsists of two key steps: first, the conversion of natural language\ninstructions into Linear Temporal Logic (LTL), followed by a comprehensive\nanalysis of action sequences. The module uses the reasoning capabilities of the\nLLM to evaluate logical coherence and identify potential gaps in the plan.\nRigorous testing on datasets of varying complexity demonstrates the broad\napplicability of the module to household tasks. We contribute to improving the\nreliability and efficiency of task planning and addresses the critical need for\nrobust pre-execution verification in autonomous systems. The code is available\nat https://verifyllm.github.io.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u4efb\u52a1\u8ba1\u5212\u9a8c\u8bc1\u67b6\u6784\uff0c\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u8f6c\u7ebf\u6027\u65f6\u5e8f\u903b\u8f91\uff08LTL\uff09\u548c\u52a8\u4f5c\u5e8f\u5217\u5206\u6790\uff0c\u63d0\u5347\u673a\u5668\u4eba\u4efb\u52a1\u89c4\u5212\u7684\u53ef\u9760\u6027\u548c\u6548\u7387\u3002", "motivation": "\u673a\u5668\u4eba\u4efb\u52a1\u89c4\u5212\u4e2d\uff0c\u6267\u884c\u524d\u9a8c\u8bc1\u8ba1\u5212\u53ef\u663e\u8457\u51cf\u5c11\u9519\u8bef\u5e76\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u7f3a\u4e4f\u9ad8\u6548\u53ef\u9760\u7684\u9884\u6267\u884c\u9a8c\u8bc1\u3002", "method": "\u5229\u7528LLM\u5c06\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u8f6c\u6362\u4e3aLTL\uff0c\u5e76\u5206\u6790\u52a8\u4f5c\u5e8f\u5217\u7684\u903b\u8f91\u4e00\u81f4\u6027\u548c\u6f5c\u5728\u6f0f\u6d1e\u3002", "result": "\u5728\u591a\u79cd\u590d\u6742\u5ea6\u7684\u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u5bb6\u5ead\u4efb\u52a1\u4e2d\u7684\u5e7f\u6cdb\u9002\u7528\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u89c4\u5212\u7684\u53ef\u9760\u6027\u548c\u6548\u7387\uff0c\u6ee1\u8db3\u4e86\u81ea\u4e3b\u7cfb\u7edf\u5bf9\u9884\u6267\u884c\u9a8c\u8bc1\u7684\u8feb\u5207\u9700\u6c42\u3002"}}
{"id": "2507.04370", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.04370", "abs": "https://arxiv.org/abs/2507.04370", "authors": ["Yifei Gao", "Junhong Ye", "Jiaqi Wang", "Jitao Sang"], "title": "WebSynthesis: World-Model-Guided MCTS for Efficient WebUI-Trajectory Synthesis", "comment": null, "summary": "Recent advancements in large language models (LLMs) have significantly\nimproved the capabilities of web agents. However, effectively navigating\ncomplex and dynamic web environments still requires more advanced\ntrajectory-level planning and execution. Prior studies have addressed\nself-improving agents by collecting extensive GUI trajectories from\nreal-environment interactions. Despite their effectiveness, these approaches\nencounter two critical challenges: (1) Uncontrollable environment states, where\nreal or sandboxed web environments often yield unstable and non-deterministic\nfeedback, complicating the reproduction and debugging of agent behaviors; and\n(2) High API costs, as generating even a single interaction trajectory can\ninvolve hundreds of queries, leading to considerable API usage and\ncomputational expenses. To address these limitations and enable scalable\nself-improvement for agents, we propose WebSynthesis, a novel framework for\ntrajectory synthesis and training. WebSynthesis leverages a learned world model\nto simulate virtual web environments, allowing a policy agent to perform\nefficient and reversible tree-based planning. This approach supports the\nlarge-scale generation of diverse and high-quality trajectories, which are\nsubsequently utilized to refine the agent's policy. Experimental results\ndemonstrate that an agent trained using WebSynthesis on a small-scale synthetic\ndataset achieves performance comparable to or even surpassing that of models\ntrained on large-scale real-world data.", "AI": {"tldr": "WebSynthesis\u6846\u67b6\u901a\u8fc7\u865a\u62df\u73af\u5883\u6a21\u62df\u548c\u6811\u72b6\u89c4\u5212\uff0c\u89e3\u51b3\u4e86\u771f\u5b9e\u73af\u5883\u4e2d\u72b6\u6001\u4e0d\u53ef\u63a7\u548cAPI\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u4ee3\u7406\u7684\u81ea\u5b66\u4e60\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u771f\u5b9e\u6216\u6c99\u76d2\u73af\u5883\u4e2d\u72b6\u6001\u4e0d\u7a33\u5b9a\u3001\u96be\u4ee5\u590d\u73b0\u4ee5\u53caAPI\u6210\u672c\u9ad8\u6602\u7684\u95ee\u9898\uff0c\u4ee5\u652f\u6301\u4ee3\u7406\u7684\u89c4\u6a21\u5316\u81ea\u5b66\u4e60\u3002", "method": "\u5229\u7528\u5b66\u4e60\u7684\u4e16\u754c\u6a21\u578b\u6a21\u62df\u865a\u62df\u7f51\u7edc\u73af\u5883\uff0c\u7ed3\u5408\u6811\u72b6\u89c4\u5212\u751f\u6210\u591a\u6837\u5316\u9ad8\u8d28\u91cf\u8f68\u8ff9\uff0c\u7528\u4e8e\u4f18\u5316\u4ee3\u7406\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528\u5c0f\u89c4\u6a21\u5408\u6210\u6570\u636e\u8bad\u7ec3\u7684\u4ee3\u7406\u6027\u80fd\u53ef\u5ab2\u7f8e\u6216\u8d85\u8d8a\u57fa\u4e8e\u5927\u89c4\u6a21\u771f\u5b9e\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u3002", "conclusion": "WebSynthesis\u4e3a\u4ee3\u7406\u7684\u81ea\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.05125", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.05125", "abs": "https://arxiv.org/abs/2507.05125", "authors": ["Minh Nguyen", "Sebastian Wrede", "Nico Hochgeschwender"], "title": "Automated Behaviour-Driven Acceptance Testing of Robotic Systems", "comment": "7 pages, 5 figures, to be published in 2025 IEEE/RSJ International\n  Conference on Intelligent Robots and Systems (IROS 2025)", "summary": "The specification and validation of robotics applications require bridging\nthe gap between formulating requirements and systematic testing. This often\ninvolves manual and error-prone tasks that become more complex as requirements,\ndesign, and implementation evolve. To address this challenge systematically, we\npropose extending behaviour-driven development (BDD) to define and verify\nacceptance criteria for robotic systems. In this context, we use\ndomain-specific modelling and represent composable BDD models as knowledge\ngraphs for robust querying and manipulation, facilitating the generation of\nexecutable testing models. A domain-specific language helps to efficiently\nspecify robotic acceptance criteria. We explore the potential for automated\ngeneration and execution of acceptance tests through a software architecture\nthat integrates a BDD framework, Isaac Sim, and model transformations, focusing\non acceptance criteria for pick-and-place applications. We tested this\narchitecture with an existing pick-and-place implementation and evaluated the\nexecution results, which shows how this application behaves and fails\ndifferently when tested against variations of the agent and environment. This\nresearch advances the rigorous and automated evaluation of robotic systems,\ncontributing to their reliability and trustworthiness.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u884c\u4e3a\u9a71\u52a8\u5f00\u53d1\uff08BDD\uff09\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u9886\u57df\u7279\u5b9a\u5efa\u6a21\u548c\u77e5\u8bc6\u56fe\u8868\u793a\uff0c\u5b9e\u73b0\u673a\u5668\u4eba\u7cfb\u7edf\u9a8c\u6536\u6807\u51c6\u7684\u5b9a\u4e49\u548c\u9a8c\u8bc1\uff0c\u5e76\u751f\u6210\u53ef\u6267\u884c\u6d4b\u8bd5\u6a21\u578b\u3002", "motivation": "\u673a\u5668\u4eba\u5e94\u7528\u7684\u89c4\u8303\u548c\u9a8c\u8bc1\u9700\u8981\u5f25\u5408\u9700\u6c42\u5236\u5b9a\u4e0e\u7cfb\u7edf\u6d4b\u8bd5\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u624b\u52a8\u4e14\u6613\u51fa\u9519\u7684\u4efb\u52a1\uff0c\u968f\u7740\u9700\u6c42\u3001\u8bbe\u8ba1\u548c\u5b9e\u73b0\u7684\u6f14\u53d8\u53d8\u5f97\u66f4\u52a0\u590d\u6742\u3002", "method": "\u6269\u5c55\u884c\u4e3a\u9a71\u52a8\u5f00\u53d1\uff08BDD\uff09\uff0c\u5229\u7528\u9886\u57df\u7279\u5b9a\u5efa\u6a21\u548c\u77e5\u8bc6\u56fe\u8868\u793a\u53ef\u7ec4\u5408\u7684BDD\u6a21\u578b\uff0c\u901a\u8fc7\u9886\u57df\u7279\u5b9a\u8bed\u8a00\u9ad8\u6548\u6307\u5b9a\u673a\u5668\u4eba\u9a8c\u6536\u6807\u51c6\uff0c\u5e76\u96c6\u6210BDD\u6846\u67b6\u3001Isaac Sim\u548c\u6a21\u578b\u8f6c\u6362\u5b9e\u73b0\u81ea\u52a8\u5316\u6d4b\u8bd5\u751f\u6210\u4e0e\u6267\u884c\u3002", "result": "\u901a\u8fc7\u73b0\u6709\u62fe\u53d6\u653e\u7f6e\u5e94\u7528\u7684\u6d4b\u8bd5\u9a8c\u8bc1\u4e86\u8be5\u67b6\u6784\uff0c\u5c55\u793a\u4e86\u5728\u4e0d\u540c\u4ee3\u7406\u548c\u73af\u5883\u53d8\u4f53\u4e0b\u5e94\u7528\u7684\u884c\u4e3a\u548c\u5931\u8d25\u6a21\u5f0f\u3002", "conclusion": "\u8be5\u7814\u7a76\u63a8\u52a8\u4e86\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u4e25\u683c\u81ea\u52a8\u5316\u8bc4\u4f30\uff0c\u63d0\u5347\u4e86\u5176\u53ef\u9760\u6027\u548c\u53ef\u4fe1\u5ea6\u3002"}}
{"id": "2507.04376", "categories": ["cs.AI", "cs.DC", "cs.MA", "cs.NI"], "pdf": "https://arxiv.org/pdf/2507.04376", "abs": "https://arxiv.org/abs/2507.04376", "authors": ["Georgios Ioannides", "Christos Constantinou", "Vinija Jain", "Aman Chadha", "Aaron Elkins"], "title": "MOD-X: A Modular Open Decentralized eXchange Framework proposal for Heterogeneous Interoperable Artificial Agents", "comment": null, "summary": "As Artificial Intelligence systems evolve from monolithic models to\necosystems of specialized agents, the need for standardized communication\nprotocols becomes increasingly critical. This paper introduces MOD-X (Modular\nOpen Decentralized eXchange), a novel architectural framework proposal for\nagent interoperability that addresses key limitations of existing protocols.\nUnlike current approaches, MOD-X proposes a layered architecture with a\nUniversal Message Bus, thorough state management, translation capabilities, and\nblockchain-based security mechanisms. We present MOD-X's architecture, compare\nit with existing protocols, and demonstrate its application through a worked\nexample how it enables integration between heterogeneous specialist agents\n(agents with different architectures, vendors, capabilities, and knowledge\nrepresentations--including rule-based systems, neural networks, symbolic\nreasoning engines, and legacy software with agent wrappers). MOD-X's key\ninnovations include a publish-subscribe communication model, semantic\ncapability discovery, and dynamic workflow orchestration--providing a framework\nthat bridges theoretical formalism with practical implementation. This\narchitecture addresses the growing need for truly decentralized, interoperable\nagent ecosystems that can scale effectively without the need for central\ncoordination.", "AI": {"tldr": "MOD-X\u662f\u4e00\u4e2a\u65b0\u578b\u7684\u6a21\u5757\u5316\u5f00\u653e\u53bb\u4e2d\u5fc3\u5316\u4ea4\u6362\u67b6\u6784\uff0c\u65e8\u5728\u89e3\u51b3\u5f02\u6784\u667a\u80fd\u4f53\u95f4\u7684\u4e92\u64cd\u4f5c\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u5206\u5c42\u8bbe\u8ba1\u3001\u901a\u7528\u6d88\u606f\u603b\u7ebf\u3001\u72b6\u6001\u7ba1\u7406\u548c\u533a\u5757\u94fe\u5b89\u5168\u673a\u5236\u5b9e\u73b0\u3002", "motivation": "\u968f\u7740AI\u7cfb\u7edf\u4ece\u5355\u4e00\u6a21\u578b\u53d1\u5c55\u4e3a\u4e13\u4e1a\u5316\u667a\u80fd\u4f53\u751f\u6001\u7cfb\u7edf\uff0c\u6807\u51c6\u5316\u901a\u4fe1\u534f\u8bae\u7684\u9700\u6c42\u65e5\u76ca\u8feb\u5207\u3002", "method": "MOD-X\u91c7\u7528\u5206\u5c42\u67b6\u6784\uff0c\u5305\u62ec\u901a\u7528\u6d88\u606f\u603b\u7ebf\u3001\u72b6\u6001\u7ba1\u7406\u3001\u7ffb\u8bd1\u80fd\u529b\u548c\u533a\u5757\u94fe\u5b89\u5168\u673a\u5236\uff0c\u652f\u6301\u53d1\u5e03-\u8ba2\u9605\u901a\u4fe1\u6a21\u578b\u3001\u8bed\u4e49\u80fd\u529b\u53d1\u73b0\u548c\u52a8\u6001\u5de5\u4f5c\u6d41\u7f16\u6392\u3002", "result": "MOD-X\u6210\u529f\u5b9e\u73b0\u4e86\u5f02\u6784\u667a\u80fd\u4f53\uff08\u5982\u57fa\u4e8e\u89c4\u5219\u7684\u7cfb\u7edf\u3001\u795e\u7ecf\u7f51\u7edc\u3001\u7b26\u53f7\u63a8\u7406\u5f15\u64ce\u7b49\uff09\u7684\u96c6\u6210\uff0c\u5c55\u793a\u4e86\u5176\u53bb\u4e2d\u5fc3\u5316\u548c\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "MOD-X\u4e3a\u53bb\u4e2d\u5fc3\u5316\u3001\u53ef\u4e92\u64cd\u4f5c\u7684\u667a\u80fd\u4f53\u751f\u6001\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7406\u8bba\u4e0e\u5b9e\u8df5\u7ed3\u5408\u7684\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u65e0\u9700\u4e2d\u592e\u534f\u8c03\u7684\u89c4\u6a21\u5316\u9700\u6c42\u3002"}}
{"id": "2507.05135", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.05135", "abs": "https://arxiv.org/abs/2507.05135", "authors": ["Svyatoslav Pchelintsev", "Maxim Patratskiy", "Anatoly Onishchenko", "Alexandr Korchemnyi", "Aleksandr Medvedev", "Uliana Vinogradova", "Ilya Galuzinsky", "Aleksey Postnikov", "Alexey K. Kovalev", "Aleksandr I. Panov"], "title": "LERa: Replanning with Visual Feedback in Instruction Following", "comment": "IROS 2025", "summary": "Large Language Models are increasingly used in robotics for task planning,\nbut their reliance on textual inputs limits their adaptability to real-world\nchanges and failures. To address these challenges, we propose LERa - Look,\nExplain, Replan - a Visual Language Model-based replanning approach that\nutilizes visual feedback. Unlike existing methods, LERa requires only a raw RGB\nimage, a natural language instruction, an initial task plan, and failure\ndetection - without additional information such as object detection or\npredefined conditions that may be unavailable in a given scenario. The\nreplanning process consists of three steps: (i) Look, where LERa generates a\nscene description and identifies errors; (ii) Explain, where it provides\ncorrective guidance; and (iii) Replan, where it modifies the plan accordingly.\nLERa is adaptable to various agent architectures and can handle errors from\nboth dynamic scene changes and task execution failures. We evaluate LERa on the\nnewly introduced ALFRED-ChaOS and VirtualHome-ChaOS datasets, achieving a 40%\nimprovement over baselines in dynamic environments. In tabletop manipulation\ntasks with a predefined probability of task failure within the PyBullet\nsimulator, LERa improves success rates by up to 67%. Further experiments,\nincluding real-world trials with a tabletop manipulator robot, confirm LERa's\neffectiveness in replanning. We demonstrate that LERa is a robust and adaptable\nsolution for error-aware task execution in robotics. The code is available at\nhttps://lera-robo.github.io.", "AI": {"tldr": "LERa\u662f\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u4efb\u52a1\u91cd\u89c4\u5212\u65b9\u6cd5\uff0c\u901a\u8fc7\u89c6\u89c9\u53cd\u9988\u89e3\u51b3\u52a8\u6001\u73af\u5883\u4e2d\u7684\u4efb\u52a1\u6267\u884c\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u6210\u529f\u7387\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u673a\u5668\u4eba\u4efb\u52a1\u89c4\u5212\u4e2d\u56e0\u4f9d\u8d56\u6587\u672c\u8f93\u5165\u800c\u65e0\u6cd5\u9002\u5e94\u73b0\u5b9e\u4e16\u754c\u53d8\u5316\u548c\u5931\u8d25\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faLERa\u6846\u67b6\uff0c\u5305\u62ecLook\uff08\u751f\u6210\u573a\u666f\u63cf\u8ff0\u548c\u8bc6\u522b\u9519\u8bef\uff09\u3001Explain\uff08\u63d0\u4f9b\u7ea0\u6b63\u6307\u5bfc\uff09\u548cReplan\uff08\u4fee\u6539\u8ba1\u5212\uff09\u4e09\u4e2a\u6b65\u9aa4\u3002", "result": "\u5728ALFRED-ChaOS\u548cVirtualHome-ChaOS\u6570\u636e\u96c6\u4e0a\u63d0\u534740%\u6027\u80fd\uff0c\u5728PyBullet\u6a21\u62df\u5668\u4e2d\u6210\u529f\u7387\u63d0\u534767%\u3002", "conclusion": "LERa\u662f\u4e00\u79cd\u9c81\u68d2\u4e14\u9002\u5e94\u6027\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u673a\u5668\u4eba\u4efb\u52a1\u6267\u884c\u4e2d\u7684\u9519\u8bef\u611f\u77e5\u548c\u91cd\u89c4\u5212\u3002"}}
{"id": "2507.04381", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.04381", "abs": "https://arxiv.org/abs/2507.04381", "authors": ["Bing Fan", "Shusen Ma", "Yun-Bo Zhao", "Yu Kang"], "title": "DC-Mamber: A Dual Channel Prediction Model based on Mamba and Linear Transformer for Multivariate Time Series Forecasting", "comment": null, "summary": "In multivariate time series forecasting (MTSF), existing strategies for\nprocessing sequences are typically categorized as channel-independent and\nchannel-mixing. The former treats all temporal information of each variable as\na token, focusing on capturing local temporal features of individual variables,\nwhile the latter constructs a token from the multivariate information at each\ntime step, emphasizing the modeling of global temporal dependencies. Current\nmainstream models are mostly based on Transformer and the emerging Mamba.\nTransformers excel at modeling global dependencies through self-attention\nmechanisms but exhibit limited sensitivity to local temporal patterns and\nsuffer from quadratic computational complexity, restricting their efficiency in\nlong-sequence processing. In contrast, Mamba, based on state space models\n(SSMs), achieves linear complexity and efficient long-range modeling but\nstruggles to aggregate global contextual information in parallel. To overcome\nthe limitations of both models, we propose DC-Mamber, a dual-channel\nforecasting model based on Mamba and linear Transformer for time series\nforecasting. Specifically, the Mamba-based channel employs a\nchannel-independent strategy to extract intra-variable features, while the\nTransformer-based channel adopts a channel-mixing strategy to model\ncross-timestep global dependencies. DC-Mamber first maps the raw input into two\ndistinct feature representations via separate embedding layers. These\nrepresentations are then processed by a variable encoder (built on Mamba) and a\ntemporal encoder (built on linear Transformer), respectively. Finally, a fusion\nlayer integrates the dual-channel features for prediction. Extensive\nexperiments on eight public datasets confirm DC-Mamber's superior accuracy over\nexisting models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMamba\u548c\u7ebf\u6027Transformer\u7684\u53cc\u901a\u9053\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u6a21\u578bDC-Mamber\uff0c\u7ed3\u5408\u4e86\u5c40\u90e8\u548c\u5168\u5c40\u7279\u5f81\u63d0\u53d6\u7684\u4f18\u52bf\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\uff08\u5982Transformer\u548cMamba\uff09\u5728\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u5b58\u5728\u5c40\u90e8\u6216\u5168\u5c40\u7279\u5f81\u5efa\u6a21\u7684\u5c40\u9650\u6027\uff0c\u9700\u8981\u4e00\u79cd\u517c\u987e\u4e24\u8005\u7684\u65b9\u6cd5\u3002", "method": "DC-Mamber\u91c7\u7528\u53cc\u901a\u9053\u8bbe\u8ba1\uff0cMamba\u901a\u9053\u63d0\u53d6\u53d8\u91cf\u5185\u7279\u5f81\uff08\u5c40\u90e8\uff09\uff0cTransformer\u901a\u9053\u5efa\u6a21\u8de8\u65f6\u95f4\u6b65\u5168\u5c40\u4f9d\u8d56\uff0c\u5e76\u901a\u8fc7\u878d\u5408\u5c42\u6574\u5408\u3002", "result": "\u5728\u516b\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDC-Mamber\u5728\u9884\u6d4b\u51c6\u786e\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "conclusion": "DC-Mamber\u901a\u8fc7\u7ed3\u5408Mamba\u548c\u7ebf\u6027Transformer\u7684\u4f18\u52bf\uff0c\u6709\u6548\u63d0\u5347\u4e86\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u6027\u80fd\u3002"}}
{"id": "2507.05198", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.05198", "abs": "https://arxiv.org/abs/2507.05198", "authors": ["Boyuan Wang", "Xinpan Meng", "Xiaofeng Wang", "Zheng Zhu", "Angen Ye", "Yang Wang", "Zhiqin Yang", "Chaojun Ni", "Guan Huang", "Xingang Wang"], "title": "EmbodieDreamer: Advancing Real2Sim2Real Transfer for Policy Training via Embodied World Modeling", "comment": "Project Page: https://embodiedreamer.github.io/", "summary": "The rapid advancement of Embodied AI has led to an increasing demand for\nlarge-scale, high-quality real-world data. However, collecting such embodied\ndata remains costly and inefficient. As a result, simulation environments have\nbecome a crucial surrogate for training robot policies. Yet, the significant\nReal2Sim2Real gap remains a critical bottleneck, particularly in terms of\nphysical dynamics and visual appearance. To address this challenge, we propose\nEmbodieDreamer, a novel framework that reduces the Real2Sim2Real gap from both\nthe physics and appearance perspectives. Specifically, we propose PhysAligner,\na differentiable physics module designed to reduce the Real2Sim physical gap.\nIt jointly optimizes robot-specific parameters such as control gains and\nfriction coefficients to better align simulated dynamics with real-world\nobservations. In addition, we introduce VisAligner, which incorporates a\nconditional video diffusion model to bridge the Sim2Real appearance gap by\ntranslating low-fidelity simulated renderings into photorealistic videos\nconditioned on simulation states, enabling high-fidelity visual transfer.\nExtensive experiments validate the effectiveness of EmbodieDreamer. The\nproposed PhysAligner reduces physical parameter estimation error by 3.74%\ncompared to simulated annealing methods while improving optimization speed by\n89.91\\%. Moreover, training robot policies in the generated photorealistic\nenvironment leads to a 29.17% improvement in the average task success rate\nacross real-world tasks after reinforcement learning. Code, model and data will\nbe publicly available.", "AI": {"tldr": "EmbodieDreamer\u6846\u67b6\u901a\u8fc7PhysAligner\u548cVisAligner\u5206\u522b\u4ece\u7269\u7406\u548c\u89c6\u89c9\u89d2\u5ea6\u51cf\u5c11Real2Sim2Real\u5dee\u8ddd\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u7b56\u7565\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u7531\u4e8e\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u771f\u5b9e\u4e16\u754c\u6570\u636e\u91c7\u96c6\u6210\u672c\u9ad8\u4e14\u6548\u7387\u4f4e\uff0c\u4eff\u771f\u73af\u5883\u6210\u4e3a\u8bad\u7ec3\u673a\u5668\u4eba\u7b56\u7565\u7684\u91cd\u8981\u66ff\u4ee3\uff0c\u4f46Real2Sim2Real\u5dee\u8ddd\uff08\u5c24\u5176\u662f\u7269\u7406\u52a8\u6001\u548c\u89c6\u89c9\u5916\u89c2\uff09\u4ecd\u662f\u5173\u952e\u74f6\u9888\u3002", "method": "\u63d0\u51faPhysAligner\uff08\u53ef\u5fae\u5206\u7269\u7406\u6a21\u5757\uff09\u4f18\u5316\u673a\u5668\u4eba\u53c2\u6570\u4ee5\u51cf\u5c11\u7269\u7406\u5dee\u8ddd\uff1b\u5f15\u5165VisAligner\uff08\u6761\u4ef6\u89c6\u9891\u6269\u6563\u6a21\u578b\uff09\u5c06\u4f4e\u4fdd\u771f\u4eff\u771f\u6e32\u67d3\u8f6c\u6362\u4e3a\u9ad8\u4fdd\u771f\u89c6\u9891\u4ee5\u7f29\u5c0f\u89c6\u89c9\u5dee\u8ddd\u3002", "result": "PhysAligner\u5c06\u7269\u7406\u53c2\u6570\u4f30\u8ba1\u8bef\u5dee\u964d\u4f4e3.74%\uff0c\u4f18\u5316\u901f\u5ea6\u63d0\u534789.91%\uff1b\u5728\u751f\u6210\u7684\u9ad8\u4fdd\u771f\u73af\u5883\u4e2d\u8bad\u7ec3\u7684\u7b56\u7565\u4f7f\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u5e73\u5747\u6210\u529f\u7387\u63d0\u9ad829.17%\u3002", "conclusion": "EmbodieDreamer\u6709\u6548\u51cf\u5c11\u4e86Real2Sim2Real\u5dee\u8ddd\uff0c\u63d0\u5347\u4e86\u673a\u5668\u4eba\u7b56\u7565\u7684\u8fc1\u79fb\u6027\u80fd\uff0c\u4ee3\u7801\u548c\u6a21\u578b\u5c06\u516c\u5f00\u3002"}}
{"id": "2507.04404", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.04404", "abs": "https://arxiv.org/abs/2507.04404", "authors": ["Jingze Zhu", "Yongliang Wu", "Wenbo Zhu", "Jiawang Cao", "Yanqiang Zheng", "Jiawei Chen", "Xu Yang", "Bernt Schiele", "Jonas Fischer", "Xinting Hu"], "title": "LayerCake: Token-Aware Contrastive Decoding within Large Language Model Layers", "comment": null, "summary": "Large language models (LLMs) excel at natural language understanding and\ngeneration but remain vulnerable to factual errors, limiting their reliability\nin knowledge-intensive tasks. While decoding-time strategies provide a\npromising efficient solution without training, existing methods typically treat\ntoken-level and layer-level signals in isolation, overlooking the joint\ndynamics between them. In this work, we introduce a token-aware,\nlayer-localized contrastive decoding method that aligns specific token types\nwith their most influential transformer layers to improve factual generation.\nThrough empirical attention analysis, we identify two key patterns: punctuation\ntokens receive dominant attention in early layers, while conceptual tokens\ngovern semantic reasoning in intermediate layers. By selectively suppressing\nattention to these token types at their respective depths, we achieve the\ninduction of controlled factual degradation and derive contrastive signals to\nguide the final factual decoding. Our method requires no additional training or\nmodel modification, and experiments demonstrate that our method consistently\nimproves factuality across multiple LLMs and various benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8etoken\u548clayer\u8054\u5408\u52a8\u6001\u7684\u5bf9\u6bd4\u89e3\u7801\u65b9\u6cd5\uff0c\u901a\u8fc7\u9009\u62e9\u6027\u6291\u5236\u7279\u5b9atoken\u7c7b\u578b\u7684\u6ce8\u610f\u529b\uff0c\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e8b\u5b9e\u751f\u6210\u80fd\u529b\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u56e0\u4e8b\u5b9e\u9519\u8bef\u800c\u53d7\u9650\uff0c\u73b0\u6709\u65b9\u6cd5\u672a\u5145\u5206\u5229\u7528token\u548clayer\u7684\u8054\u5408\u52a8\u6001\u3002", "method": "\u5f15\u5165token\u611f\u77e5\u3001layer\u5b9a\u4f4d\u7684\u5bf9\u6bd4\u89e3\u7801\u65b9\u6cd5\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u5206\u6790\u8bc6\u522b\u5173\u952e\u6a21\u5f0f\uff0c\u9009\u62e9\u6027\u6291\u5236\u7279\u5b9atoken\u7c7b\u578b\u7684\u6ce8\u610f\u529b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u6216\u6a21\u578b\u4fee\u6539\uff0c\u663e\u8457\u63d0\u5347\u4e86\u591a\u79cd\u5927\u8bed\u8a00\u6a21\u578b\u548c\u57fa\u51c6\u6d4b\u8bd5\u7684\u4e8b\u5b9e\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u8054\u5408\u52a8\u6001\u4f18\u5316\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u7684\u4e8b\u5b9e\u751f\u6210\u80fd\u529b\u3002"}}
{"id": "2507.05227", "categories": ["cs.RO", "cs.CV", "cs.LG", "cs.MM", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.05227", "abs": "https://arxiv.org/abs/2507.05227", "authors": ["Qucheng Peng", "Chen Bai", "Guoxiang Zhang", "Bo Xu", "Xiaotong Liu", "Xiaoyin Zheng", "Chen Chen", "Cheng Lu"], "title": "NavigScene: Bridging Local Perception and Global Navigation for Beyond-Visual-Range Autonomous Driving", "comment": "Accepted by ACM Multimedia 2025", "summary": "Autonomous driving systems have made significant advances in Q&A, perception,\nprediction, and planning based on local visual information, yet they struggle\nto incorporate broader navigational context that human drivers routinely\nutilize. We address this critical gap between local sensor data and global\nnavigation information by proposing NavigScene, an auxiliary navigation-guided\nnatural language dataset that simulates a human-like driving environment within\nautonomous driving systems. Moreover, we develop three complementary paradigms\nto leverage NavigScene: (1) Navigation-guided Reasoning, which enhances\nvision-language models by incorporating navigation context into the prompting\napproach; (2) Navigation-guided Preference Optimization, a reinforcement\nlearning method that extends Direct Preference Optimization to improve\nvision-language model responses by establishing preferences for\nnavigation-relevant summarized information; and (3) Navigation-guided\nVision-Language-Action model, which integrates navigation guidance and\nvision-language models with conventional driving models through feature fusion.\nExtensive experiments demonstrate that our approaches significantly improve\nperformance across perception, prediction, planning, and question-answering\ntasks by enabling reasoning capabilities beyond visual range and improving\ngeneralization to diverse driving scenarios. This work represents a significant\nstep toward more comprehensive autonomous driving systems capable of navigating\ncomplex, unfamiliar environments with greater reliability and safety.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faNavigScene\u6570\u636e\u96c6\u53ca\u4e09\u79cd\u5bfc\u822a\u5f15\u5bfc\u8303\u5f0f\uff0c\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u5168\u5c40\u5bfc\u822a\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5728\u5168\u5c40\u5bfc\u822a\u4fe1\u606f\u6574\u5408\u4e0a\u7684\u4e0d\u8db3\uff0c\u6a21\u62df\u4eba\u7c7b\u9a7e\u9a76\u73af\u5883\u3002", "method": "\u5f00\u53d1NavigScene\u6570\u636e\u96c6\uff0c\u63d0\u51fa\u4e09\u79cd\u5bfc\u822a\u5f15\u5bfc\u8303\u5f0f\uff1a\u5bfc\u822a\u5f15\u5bfc\u63a8\u7406\u3001\u5bfc\u822a\u5f15\u5bfc\u504f\u597d\u4f18\u5316\u3001\u5bfc\u822a\u5f15\u5bfc\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u611f\u77e5\u3001\u9884\u6d4b\u3001\u89c4\u5212\u548c\u95ee\u7b54\u4efb\u52a1\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u548c\u5b89\u5168\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.04428", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.04428", "abs": "https://arxiv.org/abs/2507.04428", "authors": ["Feiyue Wu", "Tianxing Wu", "Shenqi Jing"], "title": "ARMR: Adaptively Responsive Network for Medication Recommendation", "comment": "9 pages, accepted by IJCAI 2025", "summary": "Medication recommendation is a crucial task in healthcare, especially for\npatients with complex medical conditions. However, existing methods often\nstruggle to effectively balance the reuse of historical medications with the\nintroduction of new drugs in response to the changing patient conditions. In\norder to address this challenge, we propose an Adaptively Responsive network\nfor Medication Recommendation (ARMR), a new method which incorporates 1) a\npiecewise temporal learning component that distinguishes between recent and\ndistant patient history, enabling more nuanced temporal understanding, and 2)\nan adaptively responsive mechanism that dynamically adjusts attention to new\nand existing drugs based on the patient's current health state and medication\nhistory. Experiments on the MIMIC-III and MIMIC-IV datasets indicate that ARMR\nhas better performance compared with the state-of-the-art baselines in\ndifferent evaluation metrics, which contributes to more personalized and\naccurate medication recommendations. The source code is publicly avaiable at:\nhttps://github.com/seucoin/armr2.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u54cd\u5e94\u7f51\u7edc\uff08ARMR\uff09\uff0c\u7528\u4e8e\u836f\u7269\u63a8\u8350\uff0c\u901a\u8fc7\u5206\u6bb5\u65f6\u95f4\u5b66\u4e60\u548c\u52a8\u6001\u8c03\u6574\u673a\u5236\uff0c\u63d0\u5347\u4e2a\u6027\u5316\u63a8\u8350\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5e73\u8861\u5386\u53f2\u836f\u7269\u4e0e\u65b0\u836f\u7269\u7684\u4f7f\u7528\uff0c\u9700\u66f4\u7075\u6d3b\u9002\u5e94\u60a3\u8005\u75c5\u60c5\u53d8\u5316\u3002", "method": "\u7ed3\u5408\u5206\u6bb5\u65f6\u95f4\u5b66\u4e60\uff08\u533a\u5206\u8fd1\u671f\u4e0e\u8fdc\u671f\u5386\u53f2\uff09\u548c\u81ea\u9002\u5e94\u54cd\u5e94\u673a\u5236\uff08\u52a8\u6001\u8c03\u6574\u65b0\u65e7\u836f\u7269\u5173\u6ce8\u5ea6\uff09\u3002", "result": "\u5728MIMIC-III\u548cMIMIC-IV\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u63a8\u8350\u66f4\u51c6\u786e\u3002", "conclusion": "ARMR\u901a\u8fc7\u52a8\u6001\u9002\u5e94\u60a3\u8005\u72b6\u6001\uff0c\u63d0\u5347\u4e86\u836f\u7269\u63a8\u8350\u7684\u4e2a\u6027\u5316\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2507.05240", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.05240", "abs": "https://arxiv.org/abs/2507.05240", "authors": ["Meng Wei", "Chenyang Wan", "Xiqian Yu", "Tai Wang", "Yuqiang Yang", "Xiaohan Mao", "Chenming Zhu", "Wenzhe Cai", "Hanqing Wang", "Yilun Chen", "Xihui Liu", "Jiangmiao Pang"], "title": "StreamVLN: Streaming Vision-and-Language Navigation via SlowFast Context Modeling", "comment": null, "summary": "Vision-and-Language Navigation (VLN) in real-world settings requires agents\nto process continuous visual streams and generate actions with low latency\ngrounded in language instructions. While Video-based Large Language Models\n(Video-LLMs) have driven recent progress, current VLN methods based on\nVideo-LLM often face trade-offs among fine-grained visual understanding,\nlong-term context modeling and computational efficiency. We introduce\nStreamVLN, a streaming VLN framework that employs a hybrid slow-fast context\nmodeling strategy to support multi-modal reasoning over interleaved vision,\nlanguage and action inputs. The fast-streaming dialogue context facilitates\nresponsive action generation through a sliding-window of active dialogues,\nwhile the slow-updating memory context compresses historical visual states\nusing a 3D-aware token pruning strategy. With this slow-fast design, StreamVLN\nachieves coherent multi-turn dialogue through efficient KV cache reuse,\nsupporting long video streams with bounded context size and inference cost.\nExperiments on VLN-CE benchmarks demonstrate state-of-the-art performance with\nstable low latency, ensuring robustness and efficiency in real-world\ndeployment. The project page is:\n\\href{https://streamvln.github.io/}{https://streamvln.github.io/}.", "AI": {"tldr": "StreamVLN\u63d0\u51fa\u4e86\u4e00\u79cd\u6d41\u5f0f\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u6846\u67b6\uff0c\u901a\u8fc7\u6162-\u5feb\u4e0a\u4e0b\u6587\u5efa\u6a21\u7b56\u7565\u5e73\u8861\u89c6\u89c9\u7406\u89e3\u3001\u957f\u671f\u4e0a\u4e0b\u6587\u5efa\u6a21\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u57fa\u4e8eVideo-LLM\u7684VLN\u65b9\u6cd5\u5728\u7ec6\u7c92\u5ea6\u89c6\u89c9\u7406\u89e3\u3001\u957f\u671f\u4e0a\u4e0b\u6587\u5efa\u6a21\u548c\u8ba1\u7b97\u6548\u7387\u4e4b\u95f4\u7684\u6743\u8861\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6df7\u5408\u6162-\u5feb\u4e0a\u4e0b\u6587\u5efa\u6a21\u7b56\u7565\uff0c\u5feb\u901f\u6d41\u5f0f\u5bf9\u8bdd\u4e0a\u4e0b\u6587\u652f\u6301\u54cd\u5e94\u5f0f\u52a8\u4f5c\u751f\u6210\uff0c\u6162\u66f4\u65b0\u5185\u5b58\u4e0a\u4e0b\u6587\u901a\u8fc73D\u611f\u77e5\u4ee4\u724c\u526a\u679d\u538b\u7f29\u5386\u53f2\u89c6\u89c9\u72b6\u6001\u3002", "result": "\u5728VLN-CE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u4fdd\u6301\u7a33\u5b9a\u7684\u4f4e\u5ef6\u8fdf\u3002", "conclusion": "StreamVLN\u901a\u8fc7\u9ad8\u6548KV\u7f13\u5b58\u91cd\u7528\u548c\u4e0a\u4e0b\u6587\u7ba1\u7406\uff0c\u4e3a\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u9c81\u68d2\u6027\u548c\u9ad8\u6548\u6027\u3002"}}
{"id": "2507.04431", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.04431", "abs": "https://arxiv.org/abs/2507.04431", "authors": ["Debodeep Banerjee", "Burcu Sayin", "Stefano Teso", "Andrea Passerini"], "title": "MedGellan: LLM-Generated Medical Guidance to Support Physicians", "comment": null, "summary": "Medical decision-making is a critical task, where errors can result in\nserious, potentially life-threatening consequences. While full automation\nremains challenging, hybrid frameworks that combine machine intelligence with\nhuman oversight offer a practical alternative. In this paper, we present\nMedGellan, a lightweight, annotation-free framework that uses a Large Language\nModel (LLM) to generate clinical guidance from raw medical records, which is\nthen used by a physician to predict diagnoses. MedGellan uses a\nBayesian-inspired prompting strategy that respects the temporal order of\nclinical data. Preliminary experiments show that the guidance generated by the\nLLM with MedGellan improves diagnostic performance, particularly in recall and\n$F_1$ score.", "AI": {"tldr": "MedGellan\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u65e0\u9700\u6807\u6ce8\u7684\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ece\u539f\u59cb\u533b\u7597\u8bb0\u5f55\u751f\u6210\u4e34\u5e8a\u6307\u5bfc\uff0c\u5e2e\u52a9\u533b\u751f\u63d0\u9ad8\u8bca\u65ad\u6027\u80fd\u3002", "motivation": "\u533b\u7597\u51b3\u7b56\u81f3\u5173\u91cd\u8981\uff0c\u9519\u8bef\u53ef\u80fd\u5bfc\u81f4\u4e25\u91cd\u540e\u679c\u3002\u5b8c\u5168\u81ea\u52a8\u5316\u5c1a\u4e0d\u73b0\u5b9e\uff0c\u56e0\u6b64\u7ed3\u5408\u673a\u5668\u667a\u80fd\u4e0e\u4eba\u5de5\u76d1\u7763\u7684\u6df7\u5408\u6846\u67b6\u66f4\u5177\u5b9e\u7528\u6027\u3002", "method": "MedGellan\u91c7\u7528\u8d1d\u53f6\u65af\u542f\u53d1\u7684\u63d0\u793a\u7b56\u7565\uff0c\u5c0a\u91cd\u4e34\u5e8a\u6570\u636e\u7684\u65f6\u95f4\u987a\u5e8f\uff0c\u901a\u8fc7LLM\u751f\u6210\u4e34\u5e8a\u6307\u5bfc\u3002", "result": "\u521d\u6b65\u5b9e\u9a8c\u8868\u660e\uff0cMedGellan\u751f\u6210\u7684\u6307\u5bfc\u663e\u8457\u63d0\u9ad8\u4e86\u8bca\u65ad\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u53ec\u56de\u7387\u548cF1\u5206\u6570\u4e0a\u3002", "conclusion": "MedGellan\u4e3a\u533b\u7597\u51b3\u7b56\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u6df7\u5408\u6846\u67b6\uff0c\u7ed3\u5408LLM\u4e0e\u533b\u751f\u534f\u4f5c\uff0c\u63d0\u5347\u8bca\u65ad\u51c6\u786e\u6027\u3002"}}
{"id": "2507.05251", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.05251", "abs": "https://arxiv.org/abs/2507.05251", "authors": ["Elahe Delavari", "Feeza Khan Khanzada", "Jaerock Kwon"], "title": "Action Space Reduction Strategies for Reinforcement Learning in Autonomous Driving", "comment": null, "summary": "Reinforcement Learning (RL) offers a promising framework for autonomous\ndriving by enabling agents to learn control policies through interaction with\nenvironments. However, large and high-dimensional action spaces often used to\nsupport fine-grained control can impede training efficiency and increase\nexploration costs. In this study, we introduce and evaluate two novel\nstructured action space modification strategies for RL in autonomous driving:\ndynamic masking and relative action space reduction. These approaches are\nsystematically compared against fixed reduction schemes and full action space\nbaselines to assess their impact on policy learning and performance. Our\nframework leverages a multimodal Proximal Policy Optimization agent that\nprocesses both semantic image sequences and scalar vehicle states. The proposed\ndynamic and relative strategies incorporate real-time action masking based on\ncontext and state transitions, preserving action consistency while eliminating\ninvalid or suboptimal choices. Through comprehensive experiments across diverse\ndriving routes, we show that action space reduction significantly improves\ntraining stability and policy performance. The dynamic and relative schemes, in\nparticular, achieve a favorable balance between learning speed, control\nprecision, and generalization. These findings highlight the importance of\ncontext-aware action space design for scalable and reliable RL in autonomous\ndriving tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e24\u79cd\u65b0\u7684\u7ed3\u6784\u5316\u52a8\u4f5c\u7a7a\u95f4\u4fee\u6539\u7b56\u7565\uff08\u52a8\u6001\u63a9\u7801\u548c\u76f8\u5bf9\u52a8\u4f5c\u7a7a\u95f4\u7f29\u51cf\uff09\uff0c\u7528\u4e8e\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u4e2d\u5f3a\u5316\u5b66\u4e60\u7684\u8bad\u7ec3\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u4e2d\u5f3a\u5316\u5b66\u4e60\u7684\u52a8\u4f5c\u7a7a\u95f4\u901a\u5e38\u8f83\u5927\u4e14\u9ad8\u7ef4\uff0c\u5bfc\u81f4\u8bad\u7ec3\u6548\u7387\u4f4e\u548c\u63a2\u7d22\u6210\u672c\u9ad8\u3002", "method": "\u91c7\u7528\u52a8\u6001\u63a9\u7801\u548c\u76f8\u5bf9\u52a8\u4f5c\u7a7a\u95f4\u7f29\u51cf\u7b56\u7565\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08PPO\uff09\u4ee3\u7406\uff0c\u5904\u7406\u56fe\u50cf\u5e8f\u5217\u548c\u8f66\u8f86\u72b6\u6001\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u52a8\u4f5c\u7a7a\u95f4\u7f29\u51cf\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u7b56\u7565\u6027\u80fd\uff0c\u52a8\u6001\u548c\u76f8\u5bf9\u7b56\u7565\u5728\u901f\u5ea6\u3001\u7cbe\u5ea6\u548c\u6cdb\u5316\u6027\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u52a8\u4f5c\u7a7a\u95f4\u8bbe\u8ba1\u5bf9\u81ea\u52a8\u9a7e\u9a76\u4efb\u52a1\u4e2d\u5f3a\u5316\u5b66\u4e60\u7684\u53ef\u6269\u5c55\u6027\u548c\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2507.04439", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.04439", "abs": "https://arxiv.org/abs/2507.04439", "authors": ["Videep Venkatesha", "Mary Cati Poulos", "Christopher Steadman", "Caitlin Mills", "Anne M. Cleary", "Nathaniel Blanchard"], "title": "A Linguistic Analysis of Spontaneous Thoughts: Investigating Experiences of D\u00e9j\u00e0 Vu, Unexpected Thoughts, and Involuntary Autobiographical Memories", "comment": "Accepted at CogSci 2025", "summary": "The onset of spontaneous thoughts are reflective of dynamic interactions\nbetween cognition, emotion, and attention. Typically, these experiences are\nstudied through subjective appraisals that focus on their triggers,\nphenomenology, and emotional salience. In this work, we use linguistic\nsignatures to investigate Deja Vu, Involuntary Autobiographical Memories and\nUnexpected Thoughts. Specifically, we analyze the inherent characteristics of\nthe linguistic patterns in participant generated descriptions of these thought\ntypes. We show how, by positioning language as a window into spontaneous\ncognition, existing theories on these attentional states can be updated and\nreaffirmed. Our findings align with prior research, reinforcing that Deja Vu is\na metacognitive experience characterized by abstract and spatial language,\nInvoluntary Autobiographical Memories are rich in personal and emotionally\nsignificant detail, and Unexpected Thoughts are marked by unpredictability and\ncognitive disruption. This work is demonstrative of languages potential to\nreveal deeper insights into how internal spontaneous cognitive states manifest\nthrough expression.", "AI": {"tldr": "\u901a\u8fc7\u8bed\u8a00\u7279\u5f81\u5206\u6790\u81ea\u53d1\u8ba4\u77e5\u72b6\u6001\uff08\u5982Deja Vu\u3001\u975e\u81ea\u613f\u81ea\u4f20\u4f53\u8bb0\u5fc6\u548c\u610f\u5916\u601d\u7ef4\uff09\uff0c\u63ed\u793a\u5176\u4e0e\u8ba4\u77e5\u3001\u60c5\u611f\u548c\u6ce8\u610f\u529b\u7684\u52a8\u6001\u4e92\u52a8\u3002", "motivation": "\u7814\u7a76\u81ea\u53d1\u601d\u7ef4\u7684\u52a8\u6001\u4e92\u52a8\uff0c\u63a2\u7d22\u8bed\u8a00\u4f5c\u4e3a\u7a97\u53e3\u63ed\u793a\u8ba4\u77e5\u72b6\u6001\u7684\u53ef\u80fd\u6027\u3002", "method": "\u5206\u6790\u53c2\u4e0e\u8005\u5bf9\u4e09\u79cd\u601d\u7ef4\u7c7b\u578b\u7684\u63cf\u8ff0\u4e2d\u7684\u8bed\u8a00\u6a21\u5f0f\u7279\u5f81\u3002", "result": "Deja Vu\u8868\u73b0\u4e3a\u62bd\u8c61\u548c\u7a7a\u95f4\u8bed\u8a00\uff0c\u975e\u81ea\u613f\u81ea\u4f20\u4f53\u8bb0\u5fc6\u5bcc\u542b\u4e2a\u4eba\u60c5\u611f\u7ec6\u8282\uff0c\u610f\u5916\u601d\u7ef4\u5219\u5177\u6709\u4e0d\u53ef\u9884\u6d4b\u6027\u548c\u8ba4\u77e5\u5e72\u6270\u3002", "conclusion": "\u8bed\u8a00\u5206\u6790\u53ef\u6df1\u5316\u5bf9\u81ea\u53d1\u8ba4\u77e5\u72b6\u6001\u7684\u7406\u89e3\uff0c\u9a8c\u8bc1\u5e76\u66f4\u65b0\u73b0\u6709\u7406\u8bba\u3002"}}
{"id": "2507.04494", "categories": ["cs.AI", "cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.04494", "abs": "https://arxiv.org/abs/2507.04494", "authors": ["Niels Leadholm", "Viviane Clay", "Scott Knudstrup", "Hojae Lee", "Jeff Hawkins"], "title": "Thousand-Brains Systems: Sensorimotor Intelligence for Rapid, Robust Learning and Inference", "comment": "32 pages, 8 figures", "summary": "Current AI systems achieve impressive performance on many tasks, yet they\nlack core attributes of biological intelligence, including rapid, continual\nlearning, representations grounded in sensorimotor interactions, and structured\nknowledge that enables efficient generalization. Neuroscience theory suggests\nthat mammals evolved flexible intelligence through the replication of a\nsemi-independent, sensorimotor module, a functional unit known as a cortical\ncolumn. To address the disparity between biological and artificial\nintelligence, thousand-brains systems were proposed as a means of mirroring the\narchitecture of cortical columns and their interactions.\n  In the current work, we evaluate the unique properties of Monty, the first\nimplementation of a thousand-brains system. We focus on 3D object perception,\nand in particular, the combined task of object recognition and pose estimation.\nUtilizing the YCB dataset of household objects, we first assess Monty's use of\nsensorimotor learning to build structured representations, finding that these\nenable robust generalization. These representations include an emphasis on\nclassifying objects by their global shape, as well as a natural ability to\ndetect object symmetries. We then explore Monty's use of model-free and\nmodel-based policies to enable rapid inference by supporting principled\nmovements. We find that such policies complement Monty's modular architecture,\na design that can accommodate communication between modules to further\naccelerate inference speed via a novel `voting' algorithm. Finally, we examine\nMonty's use of associative, Hebbian-like binding to enable rapid, continual,\nand computationally efficient learning, properties that compare favorably to\ncurrent deep learning architectures. While Monty is still in a nascent stage of\ndevelopment, these findings support thousand-brains systems as a powerful and\npromising new approach to AI.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u5343\u8111\u7cfb\u7edfMonty\uff0c\u6a21\u62df\u5927\u8111\u76ae\u5c42\u67f1\u7ed3\u6784\uff0c\u57283D\u7269\u4f53\u611f\u77e5\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u652f\u6301\u5feb\u901f\u5b66\u4e60\u548c\u9ad8\u6548\u63a8\u7406\u3002", "motivation": "\u5f53\u524dAI\u7cfb\u7edf\u7f3a\u4e4f\u751f\u7269\u667a\u80fd\u7684\u6838\u5fc3\u7279\u6027\uff0c\u5982\u5feb\u901f\u6301\u7eed\u5b66\u4e60\u3001\u57fa\u4e8e\u611f\u77e5\u8fd0\u52a8\u7684\u8868\u5f81\u548c\u7ed3\u6784\u5316\u77e5\u8bc6\u3002\u53d7\u795e\u7ecf\u79d1\u5b66\u542f\u53d1\uff0c\u63d0\u51fa\u5343\u8111\u7cfb\u7edf\u4ee5\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "\u901a\u8fc7Monty\u7cfb\u7edf\u5b9e\u73b0\u5343\u8111\u67b6\u6784\uff0c\u5229\u7528\u611f\u77e5\u8fd0\u52a8\u5b66\u4e60\u6784\u5efa\u7ed3\u6784\u5316\u8868\u5f81\uff0c\u7ed3\u5408\u6a21\u578b\u65e0\u5173\u548c\u57fa\u4e8e\u6a21\u578b\u7684\u7b56\u7565\u8fdb\u884c\u5feb\u901f\u63a8\u7406\u3002", "result": "Monty\u57283D\u7269\u4f53\u8bc6\u522b\u548c\u59ff\u6001\u4f30\u8ba1\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u652f\u6301\u6cdb\u5316\u3001\u5bf9\u79f0\u6027\u68c0\u6d4b\u548c\u9ad8\u6548\u5b66\u4e60\u3002", "conclusion": "\u5343\u8111\u7cfb\u7edf\u662fAI\u9886\u57df\u4e00\u79cd\u6709\u524d\u666f\u7684\u65b0\u65b9\u6cd5\uff0cMonty\u7684\u521d\u6b65\u6210\u679c\u9a8c\u8bc1\u4e86\u5176\u6f5c\u529b\u3002"}}
{"id": "2507.04464", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.04464", "abs": "https://arxiv.org/abs/2507.04464", "authors": ["Ashish Bastola", "Mert D. Pes\u00e9", "Long Cheng", "Jonathon Smereka", "Abolfazl Razi"], "title": "Anomalous Decision Discovery using Inverse Reinforcement Learning", "comment": null, "summary": "Anomaly detection plays a critical role in Autonomous Vehicles (AVs) by\nidentifying unusual behaviors through perception systems that could compromise\nsafety and lead to hazardous situations. Current approaches, which often rely\non predefined thresholds or supervised learning paradigms, exhibit reduced\nefficacy when confronted with unseen scenarios, sensor noise, and occlusions,\nleading to potential safety-critical failures. Moreover, supervised methods\nrequire large annotated datasets, limiting their real-world feasibility. To\naddress these gaps, we propose an anomaly detection framework based on Inverse\nReinforcement Learning (IRL) to infer latent driving intentions from sequential\nperception data, thus enabling robust identification. Specifically, we present\nTrajectory-Reward Guided Adaptive Pre-training (TRAP), a novel IRL framework\nfor anomaly detection, to address two critical limitations of existing methods:\nnoise robustness and generalization to unseen scenarios. Our core innovation is\nimplicitly learning temporal credit assignments via reward and worst-case\nsupervision. We leverage pre-training with variable-horizon sampling to\nmaximize time-to-consequence, resulting in early detection of behavior\ndeviation. Experiments on 14,000+ simulated trajectories demonstrate\nstate-of-the-art performance, achieving 0.90 AUC and 82.2\\% F1-score -\noutperforming similarly trained supervised and unsupervised baselines by 39\\%\non Recall and 12\\% on F1-score, respectively. Similar performance is achieved\nwhile exhibiting robustness to various noise types and generalization to unseen\nanomaly types. Our code will be available at:\nhttps://github.com/abastola0/TRAP.git", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9006\u5f3a\u5316\u5b66\u4e60\uff08IRL\uff09\u7684\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6TRAP\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u4e2d\u8bc6\u522b\u5f02\u5e38\u884c\u4e3a\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u566a\u58f0\u548c\u672a\u89c1\u573a\u666f\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u5728\u566a\u58f0\u3001\u906e\u6321\u548c\u672a\u89c1\u573a\u666f\u4e2d\u6548\u679c\u4e0d\u4f73\uff0c\u4e14\u76d1\u7763\u5b66\u4e60\u9700\u8981\u5927\u91cf\u6807\u6ce8\u6570\u636e\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51faTRAP\u6846\u67b6\uff0c\u901a\u8fc7\u9006\u5f3a\u5316\u5b66\u4e60\u63a8\u65ad\u6f5c\u5728\u9a7e\u9a76\u610f\u56fe\uff0c\u5229\u7528\u5956\u52b1\u548c\u6700\u574f\u60c5\u51b5\u76d1\u7763\u9690\u5f0f\u5b66\u4e60\u65f6\u95f4\u4fe1\u7528\u5206\u914d\uff0c\u5e76\u901a\u8fc7\u9884\u8bad\u7ec3\u6700\u5927\u5316\u65f6\u95f4\u5230\u540e\u679c\u4ee5\u5b9e\u73b0\u65e9\u671f\u68c0\u6d4b\u3002", "result": "\u572814,000+\u6a21\u62df\u8f68\u8ff9\u4e0a\u6d4b\u8bd5\uff0cAUC\u8fbe0.90\uff0cF1\u5206\u657082.2%\uff0c\u53ec\u56de\u7387\u548cF1\u5206\u6570\u5206\u522b\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u9ad839%\u548c12%\uff0c\u4e14\u5728\u566a\u58f0\u548c\u672a\u89c1\u5f02\u5e38\u7c7b\u578b\u4e2d\u8868\u73b0\u7a33\u5065\u3002", "conclusion": "TRAP\u6846\u67b6\u5728\u5f02\u5e38\u68c0\u6d4b\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u566a\u58f0\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u5b89\u5168\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.04513", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.04513", "abs": "https://arxiv.org/abs/2507.04513", "authors": ["Gur Keinan", "Omer Ben-Porat"], "title": "Churn-Aware Recommendation Planning under Aggregated Preference Feedback", "comment": "arXiv admin note: substantial text overlap with arXiv:2502.18483", "summary": "We study a sequential decision-making problem motivated by recent regulatory\nand technological shifts that limit access to individual user data in\nrecommender systems (RSs), leaving only population-level preference\ninformation. This privacy-aware setting poses fundamental challenges in\nplanning under uncertainty: Effective personalization requires exploration to\ninfer user preferences, yet unsatisfactory recommendations risk immediate user\nchurn. To address this, we introduce the Rec-APC model, in which an anonymous\nuser is drawn from a known prior over latent user types (e.g., personas or\nclusters), and the decision-maker sequentially selects items to recommend.\nFeedback is binary -- positive responses refine the posterior via Bayesian\nupdates, while negative responses result in the termination of the session.\n  We prove that optimal policies converge to pure exploitation in finite time\nand propose a branch-and-bound algorithm to efficiently compute them.\nExperiments on synthetic and MovieLens data confirm rapid convergence and\ndemonstrate that our method outperforms the POMDP solver SARSOP, particularly\nwhen the number of user types is large or comparable to the number of content\ncategories. Our results highlight the applicability of this approach and\ninspire new ways to improve decision-making under the constraints imposed by\naggregated preference data.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u5728\u9690\u79c1\u4fdd\u62a4\u80cc\u666f\u4e0b\uff0c\u63a8\u8350\u7cfb\u7edf\u5982\u4f55\u901a\u8fc7\u6709\u9650\u7528\u6237\u6570\u636e\u8fdb\u884c\u4e2a\u6027\u5316\u63a8\u8350\uff0c\u63d0\u51fa\u4e86Rec-APC\u6a21\u578b\uff0c\u5e76\u8bc1\u660e\u5176\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u7531\u4e8e\u6cd5\u89c4\u548c\u6280\u672f\u9650\u5236\uff0c\u63a8\u8350\u7cfb\u7edf\u53ea\u80fd\u83b7\u53d6\u7fa4\u4f53\u504f\u597d\u6570\u636e\uff0c\u65e0\u6cd5\u76f4\u63a5\u8bbf\u95ee\u4e2a\u4f53\u7528\u6237\u6570\u636e\uff0c\u8fd9\u5bfc\u81f4\u4e2a\u6027\u5316\u63a8\u8350\u9762\u4e34\u6311\u6218\u3002", "method": "\u63d0\u51faRec-APC\u6a21\u578b\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u66f4\u65b0\u5904\u7406\u7528\u6237\u53cd\u9988\uff08\u6b63\u9762\u53cd\u9988\u66f4\u65b0\u540e\u9a8c\uff0c\u8d1f\u9762\u53cd\u9988\u7ec8\u6b62\u4f1a\u8bdd\uff09\uff0c\u5e76\u8bbe\u8ba1\u5206\u652f\u5b9a\u754c\u7b97\u6cd5\u8ba1\u7b97\u6700\u4f18\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cRec-APC\u5728\u5408\u6210\u548cMovieLens\u6570\u636e\u4e0a\u8868\u73b0\u4f18\u4e8ePOMDP\u6c42\u89e3\u5668SARSOP\uff0c\u5c24\u5176\u5728\u7528\u6237\u7c7b\u578b\u8f83\u591a\u65f6\u3002", "conclusion": "Rec-APC\u4e3a\u805a\u5408\u504f\u597d\u6570\u636e\u4e0b\u7684\u51b3\u7b56\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5c55\u793a\u4e86\u5176\u5728\u9690\u79c1\u4fdd\u62a4\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.04528", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.04528", "abs": "https://arxiv.org/abs/2507.04528", "authors": ["Sonal Allana", "Rozita Dara", "Xiaodong Lin", "Pulei Xiong"], "title": "Towards integration of Privacy Enhancing Technologies in Explainable Artificial Intelligence", "comment": "Under peer review", "summary": "Explainable Artificial Intelligence (XAI) is a crucial pathway in mitigating\nthe risk of non-transparency in the decision-making process of black-box\nArtificial Intelligence (AI) systems. However, despite the benefits, XAI\nmethods are found to leak the privacy of individuals whose data is used in\ntraining or querying the models. Researchers have demonstrated privacy attacks\nthat exploit explanations to infer sensitive personal information of\nindividuals. Currently there is a lack of defenses against known privacy\nattacks targeting explanations when vulnerable XAI are used in production and\nmachine learning as a service system. To address this gap, in this article, we\nexplore Privacy Enhancing Technologies (PETs) as a defense mechanism against\nattribute inference on explanations provided by feature-based XAI methods. We\nempirically evaluate 3 types of PETs, namely synthetic training data,\ndifferentially private training and noise addition, on two categories of\nfeature-based XAI. Our evaluation determines different responses from the\nmitigation methods and side-effects of PETs on other system properties such as\nutility and performance. In the best case, PETs integration in explanations\nreduced the risk of the attack by 49.47%, while maintaining model utility and\nexplanation quality. Through our evaluation, we identify strategies for using\nPETs in XAI for maximizing benefits and minimizing the success of this privacy\nattack on sensitive personal information.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u9690\u79c1\u589e\u5f3a\u6280\u672f\uff08PETs\uff09\u4f5c\u4e3a\u9632\u5fa1\u673a\u5236\uff0c\u7528\u4e8e\u5bf9\u6297\u57fa\u4e8e\u7279\u5f81\u7684XAI\u65b9\u6cd5\u4e2d\u5c5e\u6027\u63a8\u65ad\u653b\u51fb\uff0c\u5e76\u8bc4\u4f30\u4e86\u4e09\u79cdPETs\u7684\u6548\u679c\u3002", "motivation": "\u5c3d\u7ba1XAI\u6709\u52a9\u4e8e\u63d0\u9ad8AI\u7cfb\u7edf\u7684\u900f\u660e\u5ea6\uff0c\u4f46\u5176\u65b9\u6cd5\u53ef\u80fd\u6cc4\u9732\u4e2a\u4eba\u9690\u79c1\u6570\u636e\uff0c\u76ee\u524d\u7f3a\u4e4f\u9488\u5bf9\u6b64\u7c7b\u9690\u79c1\u653b\u51fb\u7684\u9632\u5fa1\u63aa\u65bd\u3002", "method": "\u7814\u7a76\u8bc4\u4f30\u4e86\u4e09\u79cdPETs\uff08\u5408\u6210\u8bad\u7ec3\u6570\u636e\u3001\u5dee\u5206\u9690\u79c1\u8bad\u7ec3\u548c\u566a\u58f0\u6dfb\u52a0\uff09\u5728\u4e24\u7c7b\u57fa\u4e8e\u7279\u5f81\u7684XAI\u65b9\u6cd5\u4e2d\u7684\u6548\u679c\u3002", "result": "\u6700\u4f73\u60c5\u51b5\u4e0b\uff0cPETs\u5c06\u653b\u51fb\u98ce\u9669\u964d\u4f4e\u4e8649.47%\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u6548\u7528\u548c\u89e3\u91ca\u8d28\u91cf\u3002", "conclusion": "\u7814\u7a76\u63d0\u51fa\u4e86\u5728XAI\u4e2d\u4f7f\u7528PETs\u7684\u7b56\u7565\uff0c\u4ee5\u6700\u5927\u5316\u5176\u76ca\u5904\u5e76\u6700\u5c0f\u5316\u9690\u79c1\u653b\u51fb\u7684\u6210\u529f\u7387\u3002"}}
{"id": "2507.04594", "categories": ["cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.04594", "abs": "https://arxiv.org/abs/2507.04594", "authors": ["Niloofar Shadab", "Tyler Cody", "Alejandro Salado", "Taylan G. Topcu", "Mohammad Shadab", "Peter Beling"], "title": "Exploring Core and Periphery Precepts in Biological and Artificial Intelligence: An Outcome-Based Perspective", "comment": null, "summary": "Engineering methodologies predominantly revolve around established principles\nof decomposition and recomposition. These principles involve partitioning\ninputs and outputs at the component level, ensuring that the properties of\nindividual components are preserved upon composition. However, this view does\nnot transfer well to intelligent systems, particularly when addressing the\nscaling of intelligence as a system property. Our prior research contends that\nthe engineering of general intelligence necessitates a fresh set of overarching\nsystems principles. As a result, we introduced the \"core and periphery\"\nprinciples, a novel conceptual framework rooted in abstract systems theory and\nthe Law of Requisite Variety. In this paper, we assert that these abstract\nconcepts hold practical significance. Through empirical evidence, we illustrate\ntheir applicability to both biological and artificial intelligence systems,\nbridging abstract theory with real-world implementations. Then, we expand on\nour previous theoretical framework by mathematically defining core-dominant vs\nperiphery-dominant systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7cfb\u7edf\u539f\u5219\u201c\u6838\u5fc3\u4e0e\u5916\u56f4\u201d\uff0c\u7528\u4e8e\u89e3\u51b3\u667a\u80fd\u7cfb\u7edf\u6269\u5c55\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5b9e\u8bc1\u9a8c\u8bc1\u5176\u5728\u5b9e\u9645\u667a\u80fd\u7cfb\u7edf\u4e2d\u7684\u9002\u7528\u6027\u3002", "motivation": "\u4f20\u7edf\u5de5\u7a0b\u65b9\u6cd5\u5728\u667a\u80fd\u7cfb\u7edf\u6269\u5c55\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u65b0\u7684\u7cfb\u7edf\u539f\u5219\u6765\u652f\u6301\u901a\u7528\u667a\u80fd\u7684\u5de5\u7a0b\u5316\u3002", "method": "\u57fa\u4e8e\u62bd\u8c61\u7cfb\u7edf\u7406\u8bba\u548c\u5fc5\u8981\u591a\u6837\u6027\u6cd5\u5219\uff0c\u63d0\u51fa\u201c\u6838\u5fc3\u4e0e\u5916\u56f4\u201d\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u6570\u5b66\u5b9a\u4e49\u6838\u5fc3\u4e3b\u5bfc\u4e0e\u5916\u56f4\u4e3b\u5bfc\u7cfb\u7edf\u3002", "result": "\u5b9e\u8bc1\u7814\u7a76\u8868\u660e\u8be5\u6846\u67b6\u9002\u7528\u4e8e\u751f\u7269\u548c\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\uff0c\u9a8c\u8bc1\u4e86\u5176\u5b9e\u9645\u610f\u4e49\u3002", "conclusion": "\u201c\u6838\u5fc3\u4e0e\u5916\u56f4\u201d\u539f\u5219\u4e3a\u667a\u80fd\u7cfb\u7edf\u5de5\u7a0b\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u57fa\u7840\uff0c\u5e76\u5c55\u793a\u4e86\u7406\u8bba\u4e0e\u5b9e\u8df5\u7684\u6865\u6881\u3002"}}
{"id": "2507.04600", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.04600", "abs": "https://arxiv.org/abs/2507.04600", "authors": ["Zhipeng Liu", "Peibo Duan", "Binwu Wang", "Xuan Tang", "Qi Chu", "Changsheng Zhang", "Yongsheng Huang", "Bin Zhang"], "title": "DisMS-TS: Eliminating Redundant Multi-Scale Features for Time Series Classification", "comment": "This paper has been accepted for presentation at the ACM\n  International Conference on Multimedia (ACM MM 2025)", "summary": "Real-world time series typically exhibit complex temporal variations, making\nthe time series classification task notably challenging. Recent advancements\nhave demonstrated the potential of multi-scale analysis approaches, which\nprovide an effective solution for capturing these complex temporal patterns.\nHowever, existing multi-scale analysis-based time series prediction methods\nfail to eliminate redundant scale-shared features across multi-scale time\nseries, resulting in the model over- or under-focusing on scale-shared\nfeatures. To address this issue, we propose a novel end-to-end Disentangled\nMulti-Scale framework for Time Series classification (DisMS-TS). The core idea\nof DisMS-TS is to eliminate redundant shared features in multi-scale time\nseries, thereby improving prediction performance. Specifically, we propose a\ntemporal disentanglement module to capture scale-shared and scale-specific\ntemporal representations, respectively. Subsequently, to effectively learn both\nscale-shared and scale-specific temporal representations, we introduce two\nregularization terms that ensure the consistency of scale-shared\nrepresentations and the disparity of scale-specific representations across all\ntemporal scales. Extensive experiments conducted on multiple datasets validate\nthe superiority of DisMS-TS over its competitive baselines, with the accuracy\nimprovement up to 9.71%.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDisMS-TS\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u591a\u5c3a\u5ea6\u65f6\u95f4\u5e8f\u5217\u4e2d\u7684\u5197\u4f59\u5171\u4eab\u7279\u5f81\uff0c\u63d0\u5347\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u7684\u65f6\u95f4\u5e8f\u5217\u901a\u5e38\u5177\u6709\u590d\u6742\u7684\u65f6\u5e8f\u53d8\u5316\uff0c\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u6709\u6548\u6d88\u9664\u591a\u5c3a\u5ea6\u65f6\u95f4\u5e8f\u5217\u4e2d\u7684\u5197\u4f59\u5171\u4eab\u7279\u5f81\uff0c\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u53d7\u9650\u3002", "method": "\u8bbe\u8ba1\u4e86\u65f6\u95f4\u89e3\u8026\u6a21\u5757\uff0c\u5206\u522b\u6355\u83b7\u5c3a\u5ea6\u5171\u4eab\u548c\u5c3a\u5ea6\u7279\u5b9a\u7684\u65f6\u5e8f\u8868\u793a\uff0c\u5e76\u5f15\u5165\u4e24\u79cd\u6b63\u5219\u5316\u9879\u4ee5\u786e\u4fdd\u5171\u4eab\u8868\u793a\u7684\u4e00\u81f4\u6027\u548c\u7279\u5b9a\u8868\u793a\u7684\u5dee\u5f02\u6027\u3002", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDisMS-TS\u7684\u51c6\u786e\u7387\u6700\u9ad8\u63d0\u53479.71%\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "DisMS-TS\u901a\u8fc7\u89e3\u8026\u591a\u5c3a\u5ea6\u65f6\u95f4\u5e8f\u5217\u4e2d\u7684\u5197\u4f59\u7279\u5f81\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u4efb\u52a1\u7684\u6027\u80fd\u3002"}}
{"id": "2507.04632", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.04632", "abs": "https://arxiv.org/abs/2507.04632", "authors": ["Yun Qu", "Qi Cheems Wang", "Yixiu Mao", "Vincent Tao Hu", "Xiangyang Ji"], "title": "Can Prompt Difficulty be Online Predicted for Accelerating RL Finetuning of Reasoning Models?", "comment": null, "summary": "Recent advances have witnessed the effectiveness of reinforcement learning\n(RL) finetuning in enhancing the reasoning capabilities of large language\nmodels (LLMs). The optimization process often requires numerous iterations to\nachieve satisfactory performance, resulting in high computational costs due to\nthe need for frequent prompt evaluations under intensive LLM interactions and\nrepeated policy updates. Appropriate online prompt selection methods reduce\niteration steps by prioritizing informative prompts during training, while the\npipeline's reliance on exhaustive prompt evaluation and subset selection for\noptimization still incurs substantial computational overhead due to frequent\nLLM inference calls. Distinguished from these direct evaluate-then-select\nschemes, this work investigates iterative approximate evaluation for arbitrary\nprompts and introduces Model Predictive Prompt Selection (MoPPS), a Bayesian\nrisk-predictive framework that online estimates prompt difficulty without\nrequiring costly LLM interactions. Technically, MoPPS models each prompt's\nsuccess rate as a latent variable, performs streaming Bayesian inference, and\nemploys posterior sampling in a constructed multi-armed bandit machine,\nenabling sample efficient and adaptive prompt selection. Extensive experiments\nacross mathematics, planning, and vision-based geometry tasks show that MoPPS\nreliably predicts prompt difficulty and accelerates training with significantly\nreduced LLM rollouts.", "AI": {"tldr": "MoPPS\u662f\u4e00\u79cd\u57fa\u4e8e\u8d1d\u53f6\u65af\u98ce\u9669\u9884\u6d4b\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u7ebf\u4f30\u8ba1\u63d0\u793a\u96be\u5ea6\uff0c\u51cf\u5c11\u5bf9LLM\u4ea4\u4e92\u7684\u9700\u6c42\uff0c\u4ece\u800c\u52a0\u901f\u8bad\u7ec3\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u9891\u7e41\u7684LLM\u63a8\u7406\u8c03\u7528\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u63d0\u793a\u9009\u62e9\u65b9\u6cd5\u3002", "method": "MoPPS\u901a\u8fc7\u5efa\u6a21\u63d0\u793a\u6210\u529f\u7387\u4e3a\u6f5c\u5728\u53d8\u91cf\uff0c\u8fdb\u884c\u6d41\u5f0f\u8d1d\u53f6\u65af\u63a8\u65ad\uff0c\u5e76\u5728\u591a\u81c2\u8001\u864e\u673a\u4e2d\u5e94\u7528\u540e\u9a8c\u91c7\u6837\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u63d0\u793a\u9009\u62e9\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMoPPS\u80fd\u53ef\u9760\u9884\u6d4b\u63d0\u793a\u96be\u5ea6\uff0c\u663e\u8457\u51cf\u5c11LLM\u8c03\u7528\u5e76\u52a0\u901f\u8bad\u7ec3\u3002", "conclusion": "MoPPS\u63d0\u4f9b\u4e86\u4e00\u79cd\u8ba1\u7b97\u9ad8\u6548\u7684\u63d0\u793a\u9009\u62e9\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4efb\u52a1\u3002"}}
{"id": "2507.04673", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.04673", "abs": "https://arxiv.org/abs/2507.04673", "authors": ["Wei Duan", "Li Qian"], "title": "Trojan Horse Prompting: Jailbreaking Conversational Multimodal Models by Forging Assistant Message", "comment": null, "summary": "The rise of conversational interfaces has greatly enhanced LLM usability by\nleveraging dialogue history for sophisticated reasoning. However, this reliance\nintroduces an unexplored attack surface. This paper introduces Trojan Horse\nPrompting, a novel jailbreak technique. Adversaries bypass safety mechanisms by\nforging the model's own past utterances within the conversational history\nprovided to its API. A malicious payload is injected into a model-attributed\nmessage, followed by a benign user prompt to trigger harmful content\ngeneration. This vulnerability stems from Asymmetric Safety Alignment: models\nare extensively trained to refuse harmful user requests but lack comparable\nskepticism towards their own purported conversational history. This implicit\ntrust in its \"past\" creates a high-impact vulnerability. Experimental\nvalidation on Google's Gemini-2.0-flash-preview-image-generation shows Trojan\nHorse Prompting achieves a significantly higher Attack Success Rate (ASR) than\nestablished user-turn jailbreaking methods. These findings reveal a fundamental\nflaw in modern conversational AI security, necessitating a paradigm shift from\ninput-level filtering to robust, protocol-level validation of conversational\ncontext integrity.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u653b\u51fb\u65b9\u6cd5\u201c\u7279\u6d1b\u4f0a\u6728\u9a6c\u63d0\u793a\u201d\uff0c\u901a\u8fc7\u4f2a\u9020\u5bf9\u8bdd\u5386\u53f2\u7ed5\u8fc7LLM\u7684\u5b89\u5168\u673a\u5236\uff0c\u63ed\u793a\u73b0\u4ee3\u5bf9\u8bddAI\u7684\u5b89\u5168\u7f3a\u9677\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7d22\u5bf9\u8bdd\u5386\u53f2\u4f9d\u8d56\u5e26\u6765\u7684\u672a\u53d1\u73b0\u653b\u51fb\u9762\uff0c\u63ed\u793aLLM\u5728\u5b89\u5168\u5bf9\u9f50\u4e2d\u7684\u4e0d\u5bf9\u79f0\u6027\u3002", "method": "\u65b9\u6cd5\u662f\u901a\u8fc7\u4f2a\u9020\u6a21\u578b\u81ea\u8eab\u7684\u5bf9\u8bdd\u5386\u53f2\uff0c\u6ce8\u5165\u6076\u610f\u8f7d\u8377\u5e76\u89e6\u53d1\u6709\u5bb3\u5185\u5bb9\u751f\u6210\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u653b\u51fb\u6210\u529f\u7387\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u66b4\u9732\u4e86\u5bf9\u8bddAI\u7684\u5b89\u5168\u6f0f\u6d1e\u3002", "conclusion": "\u7ed3\u8bba\u662f\u73b0\u4ee3\u5bf9\u8bddAI\u9700\u4ece\u8f93\u5165\u7ea7\u8fc7\u6ee4\u8f6c\u5411\u534f\u8bae\u7ea7\u9a8c\u8bc1\uff0c\u786e\u4fdd\u5bf9\u8bdd\u4e0a\u4e0b\u6587\u7684\u5b8c\u6574\u6027\u3002"}}
{"id": "2507.04719", "categories": ["cs.AI", "cs.LG", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.04719", "abs": "https://arxiv.org/abs/2507.04719", "authors": ["Roozbeh Yousefzadeh", "Xuenan Cao"], "title": "Advocate for Complete Benchmarks for Formal Reasoning with Formal/Informal Statements and Formal/Informal Proofs", "comment": null, "summary": "This position paper provides a critical but constructive discussion of\ncurrent practices in benchmarking and evaluative practices in the field of\nformal reasoning and automated theorem proving. We take the position that open\ncode, open data, and benchmarks that are complete and error-free will\naccelerate progress in this field. We identify practices that create barriers\nto contributing to this field and suggest ways to remove them. We also discuss\nsome of the practices that might produce misleading evaluative information. We\naim to create discussions that bring together people from various groups\ncontributing to automated theorem proving, autoformalization, and informal\nreasoning.", "AI": {"tldr": "\u672c\u6587\u6279\u5224\u6027\u8ba8\u8bba\u4e86\u5f62\u5f0f\u63a8\u7406\u548c\u81ea\u52a8\u5b9a\u7406\u8bc1\u660e\u9886\u57df\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e0e\u8bc4\u4f30\u5b9e\u8df5\uff0c\u4e3b\u5f20\u5f00\u653e\u4ee3\u7801\u3001\u6570\u636e\u548c\u5b8c\u6574\u65e0\u9519\u7684\u57fa\u51c6\u4ee5\u52a0\u901f\u8fdb\u5c55\u3002", "motivation": "\u5f53\u524d\u5b9e\u8df5\u5b58\u5728\u963b\u788d\u9886\u57df\u8d21\u732e\u7684\u969c\u788d\uff0c\u9700\u6539\u8fdb\u4ee5\u4fc3\u8fdb\u5408\u4f5c\u3002", "method": "\u8bc6\u522b\u95ee\u9898\u5e76\u63d0\u51fa\u6539\u8fdb\u5efa\u8bae\uff0c\u8ba8\u8bba\u8bef\u5bfc\u6027\u8bc4\u4f30\u5b9e\u8df5\u3002", "result": "\u63d0\u51fa\u5f00\u653e\u548c\u900f\u660e\u5316\u7684\u5b9e\u8df5\u65b9\u5411\uff0c\u4fc3\u8fdb\u591a\u9886\u57df\u8ba8\u8bba\u3002", "conclusion": "\u901a\u8fc7\u5f00\u653e\u548c\u534f\u4f5c\uff0c\u63a8\u52a8\u81ea\u52a8\u5b9a\u7406\u8bc1\u660e\u9886\u57df\u7684\u8fdb\u6b65\u3002"}}
{"id": "2507.04722", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.04722", "abs": "https://arxiv.org/abs/2507.04722", "authors": ["Jinzhi Wang", "Bin Li", "Qingke Peng", "Haozhou Li", "Zeyuan Zeng", "Ruimeng Li", "Biyi Zhou"], "title": "LumiCRS: Asymmetric Contrastive Prototype Learning for Long-Tail Conversational Movie Recommendation", "comment": null, "summary": "Conversational recommender systems (CRSs) often suffer from an extreme\nlong-tail distribution of dialogue data, causing a strong bias toward\nhead-frequency blockbusters that sacrifices diversity and exacerbates the\ncold-start problem. An empirical analysis of DCRS and statistics on the REDIAL\ncorpus show that only 10% of head movies account for nearly half of all\nmentions, whereas about 70% of tail movies receive merely 26% of the attention.\nThis imbalance gives rise to three critical challenges: head over-fitting, body\nrepresentation drift, and tail sparsity. To address these issues, we propose\nLumiCRS, an end-to-end framework that mitigates long-tail imbalance through\nthree mutually reinforcing layers: (i) an Adaptive Comprehensive Focal Loss\n(ACFL) that dynamically adjusts class weights and focusing factors to curb head\nover-fitting and reduce popularity bias; (ii) Prototype Learning for Long-Tail\nRecommendation, which selects semantic, affective, and contextual prototypes to\nguide clustering and stabilize body and tail representations; and (iii) a\nGPT-4o-driven prototype-guided dialogue augmentation module that automatically\ngenerates diverse long-tail conversational snippets to alleviate tail sparsity\nand distribution shift. Together, these strategies enable LumiCRS to markedly\nimprove recommendation accuracy, diversity, and fairness: on the REDIAL and\nINSPIRED benchmarks, LumiCRS boosts Recall@10 and Tail-Recall@10 by 7-15% over\nfifteen strong baselines, while human evaluations confirm superior fluency,\ninformativeness, and long-tail relevance. These results demonstrate the\neffectiveness of multi-layer collaboration in building an efficient and fair\nlong-tail conversational recommender.", "AI": {"tldr": "LumiCRS\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u7126\u70b9\u635f\u5931\u3001\u539f\u578b\u5b66\u4e60\u548cGPT-4\u9a71\u52a8\u7684\u5bf9\u8bdd\u589e\u5f3a\uff0c\u89e3\u51b3\u5bf9\u8bdd\u63a8\u8350\u7cfb\u7edf\u4e2d\u7684\u957f\u5c3e\u5206\u5e03\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u63a8\u8350\u51c6\u786e\u6027\u3001\u591a\u6837\u6027\u548c\u516c\u5e73\u6027\u3002", "motivation": "\u5bf9\u8bdd\u63a8\u8350\u7cfb\u7edf\uff08CRS\uff09\u4e2d\u6570\u636e\u7684\u957f\u5c3e\u5206\u5e03\u5bfc\u81f4\u5bf9\u9ad8\u9891\u5185\u5bb9\u7684\u8fc7\u5ea6\u62df\u5408\u548c\u4f4e\u9891\u5185\u5bb9\u7684\u7a00\u758f\u6027\uff0c\u5f71\u54cd\u63a8\u8350\u7684\u591a\u6837\u6027\u548c\u51b7\u542f\u52a8\u95ee\u9898\u3002", "method": "LumiCRS\u91c7\u7528\u4e09\u5c42\u7b56\u7565\uff1a(i) \u81ea\u9002\u5e94\u7efc\u5408\u7126\u70b9\u635f\u5931\uff08ACFL\uff09\u52a8\u6001\u8c03\u6574\u6743\u91cd\uff1b(ii) \u539f\u578b\u5b66\u4e60\u7a33\u5b9a\u8868\u793a\uff1b(iii) GPT-4\u9a71\u52a8\u7684\u5bf9\u8bdd\u589e\u5f3a\u751f\u6210\u591a\u6837\u5bf9\u8bdd\u7247\u6bb5\u3002", "result": "\u5728REDIAL\u548cINSPIRED\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLumiCRS\u7684Recall@10\u548cTail-Recall@10\u63d0\u53477-15%\uff0c\u4eba\u7c7b\u8bc4\u4f30\u663e\u793a\u5176\u6d41\u7545\u6027\u3001\u4fe1\u606f\u91cf\u548c\u957f\u5c3e\u76f8\u5173\u6027\u66f4\u4f18\u3002", "conclusion": "\u591a\u5c42\u534f\u4f5c\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u957f\u5c3e\u5206\u5e03\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u5bf9\u8bdd\u63a8\u8350\u7cfb\u7edf\u7684\u6548\u7387\u548c\u516c\u5e73\u6027\u3002"}}
{"id": "2507.04736", "categories": ["cs.AI", "cs.AR", "cs.PL"], "pdf": "https://arxiv.org/pdf/2507.04736", "abs": "https://arxiv.org/abs/2507.04736", "authors": ["Zhirong Chen", "Kaiyan Chang", "Zhuolin Li", "Xinyang He", "Chujie Chen", "Cangyuan Li", "Mengdi Wang", "Haobo Xu", "Yinhe Han", "Ying Wang"], "title": "ChipSeek-R1: Generating Human-Surpassing RTL with LLM via Hierarchical Reward-Driven Reinforcement Learning", "comment": null, "summary": "Large Language Models (LLMs) show significant potential for automating\nRegister-Transfer Level (RTL) code generation. However, current approaches face\na critical challenge: they can not simultaneously optimize for functional\ncorrectness and hardware quality (Power, Performance, Area - PPA). Methods\nbased on supervised fine-tuning often generate functionally correct but\nPPA-suboptimal code, lacking mechanisms to learn optimization principles. In\ncontrast, post-processing techniques that attempt to improve PPA metrics after\ngeneration are often inefficient because they operate externally without\nupdating the LLM's parameters, thus failing to enhance the model's intrinsic\ndesign capabilities.\n  To bridge this gap, we introduce ChipSeek-R1, a hierarchical reward-driven\nreinforcement learning framework to train LLMs to generate RTL code that\nachieves both functional correctness and optimized PPA metrics. ChipSeek-R1\nemploys a hierarchical reward system, which incorporates direct feedback on\nsyntax, functional correctness (from simulators) and PPA metrics (from\nsynthesis tools) during reinforcement learning. This enables the model to learn\ncomplex hardware design trade-offs via trial-and-error, generating RTL code\nthat is both functionally correct and PPA-optimized. Evaluating ChipSeek-R1 on\nstandard benchmarks (VerilogEval, RTLLM), we achieve state-of-the-art results\nin functional correctness. Notably, on the RTLLM benchmark, ChipSeek-R1\ngenerated 27 RTL designs surpassing the PPA metrics of the original\nhuman-written code. Our findings demonstrate the effectiveness of integrating\ntoolchain feedback into LLM training and highlight the potential for\nreinforcement learning to enable automated generation of human-surpassing RTL\ncode. We open-source our code in anonymous github.", "AI": {"tldr": "ChipSeek-R1\u901a\u8fc7\u5206\u5c42\u5956\u52b1\u9a71\u52a8\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u8bad\u7ec3LLM\u751f\u6210\u529f\u80fd\u6b63\u786e\u4e14PPA\u4f18\u5316\u7684RTL\u4ee3\u7801\uff0c\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5f53\u524d\u65b9\u6cd5\u65e0\u6cd5\u540c\u65f6\u4f18\u5316\u529f\u80fd\u6b63\u786e\u6027\u548c\u786c\u4ef6\u8d28\u91cf\uff08PPA\uff09\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u91c7\u7528\u5206\u5c42\u5956\u52b1\u7cfb\u7edf\uff0c\u7ed3\u5408\u8bed\u6cd5\u3001\u529f\u80fd\u6b63\u786e\u6027\u548cPPA\u6307\u6807\u7684\u53cd\u9988\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3LLM\u3002", "result": "\u5728RTLLM\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u751f\u6210\u768427\u4e2aRTL\u8bbe\u8ba1\u8d85\u8d8a\u4e86\u4eba\u5de5\u7f16\u5199\u7684\u4ee3\u7801\u7684PPA\u6307\u6807\u3002", "conclusion": "ChipSeek-R1\u5c55\u793a\u4e86\u5c06\u5de5\u5177\u94fe\u53cd\u9988\u96c6\u6210\u5230LLM\u8bad\u7ec3\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5f3a\u5316\u5b66\u4e60\u6709\u671b\u5b9e\u73b0\u81ea\u52a8\u5316\u751f\u6210\u8d85\u8d8a\u4eba\u5de5\u7684RTL\u4ee3\u7801\u3002"}}
{"id": "2507.04742", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.04742", "abs": "https://arxiv.org/abs/2507.04742", "authors": ["Seyedarmin Azizi", "Erfan Baghaei Potraghloo", "Massoud Pedram"], "title": "Activation Steering for Chain-of-Thought Compression", "comment": null, "summary": "Large language models (LLMs) excel at complex reasoning when they include\nintermediate steps, known as \"chains of thought\" (CoTs). However, these\nrationales are often overly verbose, even for simple problems, leading to\nwasted context, increased latency, and higher energy consumption. We observe\nthat verbose, English-heavy CoTs and concise, math-centric CoTs occupy distinct\nregions in the model's residual-stream activation space. By extracting and\ninjecting a \"steering vector\" to transition between these modes, we can\nreliably shift generation toward more concise reasoning, effectively\ncompressing CoTs without retraining. We formalize this approach as\nActivation-Steered Compression (ASC), an inference-time technique that shortens\nreasoning traces by directly modifying hidden representations. In addition, we\nprovide a theoretical analysis of the impact of ASC on the output distribution,\nderived from a closed-form KL-divergence-bounded constraint to regulate\nsteering strength. Using only 100 paired verbose and concise examples, ASC\nachieves up to 67.43% reduction in CoT length on MATH500 and GSM8K datasets,\nwhile maintaining accuracy across 7B, 8B, and 32B parameter models. As a\ntraining-free method, ASC introduces negligible runtime overhead and, on\nMATH500, delivers an average 2.73x speedup in end-to-end reasoning wall-clock\ntime on an 8B model. This makes ASC a practical and efficient tool for\nstreamlining the deployment of reasoning-capable LLMs in latency- or\ncost-sensitive settings. The code is available at:\nhttps://github.com/ArminAzizi98/ASC", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aASC\u7684\u63a8\u7406\u65f6\u6280\u672f\uff0c\u901a\u8fc7\u8c03\u6574\u9690\u85cf\u8868\u793a\u6765\u538b\u7f29\u601d\u7ef4\u94fe\uff08CoTs\uff09\uff0c\u51cf\u5c11\u5197\u4f59\u63a8\u7406\u6b65\u9aa4\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u601d\u7ef4\u94fe\uff08CoTs\uff09\u5728\u89e3\u51b3\u7b80\u5355\u95ee\u9898\u65f6\u8fc7\u4e8e\u5197\u957f\uff0c\u5bfc\u81f4\u4e0a\u4e0b\u6587\u6d6a\u8d39\u3001\u5ef6\u8fdf\u589e\u52a0\u548c\u80fd\u8017\u4e0a\u5347\u3002", "method": "\u901a\u8fc7\u63d0\u53d6\u548c\u6ce8\u5165\u201c\u8f6c\u5411\u5411\u91cf\u201d\u5728\u6a21\u578b\u7684\u6b8b\u5dee\u6d41\u6fc0\u6d3b\u7a7a\u95f4\u4e2d\u5207\u6362\u63a8\u7406\u6a21\u5f0f\uff0c\u5b9e\u73b0\u63a8\u7406\u6b65\u9aa4\u7684\u538b\u7f29\u3002", "result": "ASC\u5728MATH500\u548cGSM8K\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e8667.43%\u7684\u63a8\u7406\u6b65\u9aa4\u538b\u7f29\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\uff0c\u5e76\u57288B\u6a21\u578b\u4e0a\u5e73\u5747\u63d0\u901f2.73\u500d\u3002", "conclusion": "ASC\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u5b9e\u7528\u9ad8\u6548\u5de5\u5177\uff0c\u9002\u7528\u4e8e\u5bf9\u5ef6\u8fdf\u6216\u6210\u672c\u654f\u611f\u7684\u63a8\u7406\u4efb\u52a1\u90e8\u7f72\u3002"}}
{"id": "2507.04748", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.04748", "abs": "https://arxiv.org/abs/2507.04748", "authors": ["Sungmin Lee", "Minju Kang", "Joonhee Lee", "Seungyong Lee", "Dongju Kim", "Jingi Hong", "Jun Shin", "Pei Zhang", "JeongGil Ko"], "title": "LLM-based Question-Answer Framework for Sensor-driven HVAC System Interaction", "comment": null, "summary": "Question-answering (QA) interfaces powered by large language models (LLMs)\npresent a promising direction for improving interactivity with HVAC system\ninsights, particularly for non-expert users. However, enabling accurate,\nreal-time, and context-aware interactions with HVAC systems introduces unique\nchallenges, including the integration of frequently updated sensor data,\ndomain-specific knowledge grounding, and coherent multi-stage reasoning. In\nthis paper, we present JARVIS, a two-stage LLM-based QA framework tailored for\nsensor data-driven HVAC system interaction. JARVIS employs an Expert-LLM to\ntranslate high-level user queries into structured execution instructions, and\nan Agent that performs SQL-based data retrieval, statistical processing, and\nfinal response generation. To address HVAC-specific challenges, JARVIS\nintegrates (1) an adaptive context injection strategy for efficient HVAC and\ndeployment-specific information integration, (2) a parameterized SQL builder\nand executor to improve data access reliability, and (3) a bottom-up planning\nscheme to ensure consistency across multi-stage response generation. We\nevaluate JARVIS using real-world data collected from a commercial HVAC system\nand a ground truth QA dataset curated by HVAC experts to demonstrate its\neffectiveness in delivering accurate and interpretable responses across diverse\nqueries. Results show that JARVIS consistently outperforms baseline and\nablation variants in both automated and user-centered assessments, achieving\nhigh response quality and accuracy.", "AI": {"tldr": "JARVIS\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u4e24\u9636\u6bb5QA\u6846\u67b6\uff0c\u4e13\u4e3aHVAC\u7cfb\u7edf\u4ea4\u4e92\u8bbe\u8ba1\uff0c\u901a\u8fc7\u4e13\u5bb6LLM\u548c\u4ee3\u7406\u5b9e\u73b0\u9ad8\u6548\u67e5\u8be2\u5904\u7406\u548c\u54cd\u5e94\u751f\u6210\u3002", "motivation": "\u63d0\u5347\u975e\u4e13\u5bb6\u7528\u6237\u4e0eHVAC\u7cfb\u7edf\u7684\u4ea4\u4e92\u6027\uff0c\u89e3\u51b3\u5b9e\u65f6\u3001\u51c6\u786e\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u6846\u67b6\uff1a\u4e13\u5bb6LLM\u7ffb\u8bd1\u67e5\u8be2\uff0c\u4ee3\u7406\u6267\u884cSQL\u6570\u636e\u68c0\u7d22\u548c\u54cd\u5e94\u751f\u6210\uff1b\u96c6\u6210\u81ea\u9002\u5e94\u4e0a\u4e0b\u6587\u6ce8\u5165\u3001\u53c2\u6570\u5316SQL\u6784\u5efa\u5668\u548c\u81ea\u5e95\u5411\u4e0a\u89c4\u5212\u3002", "result": "\u5728\u771f\u5b9eHVAC\u6570\u636e\u548c\u4e13\u5bb6\u6807\u6ce8\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "JARVIS\u80fd\u6709\u6548\u63d0\u4f9b\u51c6\u786e\u4e14\u53ef\u89e3\u91ca\u7684\u54cd\u5e94\uff0c\u9002\u7528\u4e8eHVAC\u7cfb\u7edf\u4ea4\u4e92\u3002"}}
{"id": "2507.04770", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.04770", "abs": "https://arxiv.org/abs/2507.04770", "authors": ["Toan Nguyen", "Tri Le", "Quang Nguyen", "Anh Nguyen"], "title": "FurniMAS: Language-Guided Furniture Decoration using Multi-Agent System", "comment": null, "summary": "Furniture decoration is an important task in various industrial applications.\nHowever, achieving a high-quality decorative result is often time-consuming and\nrequires specialized artistic expertise. To tackle these challenges, we explore\nhow multi-agent systems can assist in automating the decoration process. We\npropose FurniMAS, a multi-agent system for automatic furniture decoration.\nSpecifically, given a human prompt and a household furniture item such as a\nworking desk or a TV stand, our system suggests relevant assets with\nappropriate styles and materials, and arranges them on the item, ensuring the\ndecorative result meets functionality, aesthetic, and ambiance preferences.\nFurniMAS assembles a hybrid team of LLM-based and non-LLM agents, each\nfulfilling distinct roles in a typical decoration project. These agents\ncollaborate through communication, logical reasoning, and validation to\ntransform the requirements into the final outcome. Extensive experiments\ndemonstrate that our FurniMAS significantly outperforms other baselines in\ngenerating high-quality 3D decor.", "AI": {"tldr": "FurniMAS\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u5bb6\u5177\u88c5\u9970\uff0c\u901a\u8fc7\u7ed3\u5408LLM\u548c\u975eLLM\u667a\u80fd\u4f53\u534f\u4f5c\u5b8c\u6210\u4efb\u52a1\uff0c\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u5bb6\u5177\u88c5\u9970\u8017\u65f6\u4e14\u9700\u8981\u4e13\u4e1a\u827a\u672f\u6280\u80fd\uff0cFurniMAS\u65e8\u5728\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u81ea\u52a8\u5316\u8fd9\u4e00\u8fc7\u7a0b\u3002", "method": "FurniMAS\u7ed3\u5408LLM\u548c\u975eLLM\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u6c9f\u901a\u3001\u903b\u8f91\u63a8\u7406\u548c\u9a8c\u8bc1\u5c06\u9700\u6c42\u8f6c\u5316\u4e3a\u6700\u7ec8\u88c5\u9970\u7ed3\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cFurniMAS\u5728\u751f\u6210\u9ad8\u8d28\u91cf3D\u88c5\u9970\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "FurniMAS\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u6210\u529f\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u9ad8\u8d28\u91cf\u7684\u5bb6\u5177\u88c5\u9970\u81ea\u52a8\u5316\u3002"}}
{"id": "2507.04803", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.04803", "abs": "https://arxiv.org/abs/2507.04803", "authors": ["George Jagadeesh", "Srikrishna Iyer", "Michal Polanowski", "Kai Xin Thia"], "title": "Application and Evaluation of Large Language Models for Forecasting the Impact of Traffic Incidents", "comment": "This paper has been accepted for publication at the 2025 IEEE 28th\n  International Conference on Intelligent Transportation Systems (ITSC), Gold\n  Coast, Australia, 2025. Copyright IEEE", "summary": "This study examines the feasibility of applying large language models (LLMs)\nfor forecasting the impact of traffic incidents on the traffic flow. The use of\nLLMs for this task has several advantages over existing machine learning-based\nsolutions such as not requiring a large training dataset and the ability to\nutilize free-text incident logs. We propose a fully LLM-based solution that\npredicts the incident impact using a combination of traffic features and\nLLM-extracted incident features. A key ingredient of this solution is an\neffective method of selecting examples for the LLM's in-context learning. We\nevaluate the performance of three advanced LLMs and two state-of-the-art\nmachine learning models on a real traffic incident dataset. The results show\nthat the best-performing LLM matches the accuracy of the most accurate machine\nlearning model, despite the former not having been trained on this prediction\ntask. The findings indicate that LLMs are a practically viable option for\ntraffic incident impact prediction.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u9884\u6d4b\u4ea4\u901a\u4e8b\u4ef6\u5bf9\u4ea4\u901a\u6d41\u5f71\u54cd\u7684\u53ef\u884c\u6027\uff0c\u5176\u4f18\u52bf\u5305\u62ec\u65e0\u9700\u5927\u91cf\u8bad\u7ec3\u6570\u636e\u5e76\u80fd\u5229\u7528\u81ea\u7531\u6587\u672c\u4e8b\u4ef6\u65e5\u5fd7\u3002\u63d0\u51fa\u7684\u5168LLM\u89e3\u51b3\u65b9\u6848\u7ed3\u5408\u4ea4\u901a\u7279\u5f81\u548cLLM\u63d0\u53d6\u7684\u4e8b\u4ef6\u7279\u5f81\u8fdb\u884c\u9884\u6d4b\uff0c\u5e76\u901a\u8fc7\u6709\u6548\u9009\u62e9\u793a\u4f8b\u4f18\u5316LLM\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u6700\u4f73LLM\u7684\u51c6\u786e\u6027\u4e0e\u6700\u5148\u8fdb\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u76f8\u5f53\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u8bad\u7ec3\u6570\u636e\uff0c\u800cLLM\u65e0\u9700\u4e13\u95e8\u8bad\u7ec3\u5373\u53ef\u5b8c\u6210\u4efb\u52a1\uff0c\u4e14\u80fd\u5229\u7528\u81ea\u7531\u6587\u672c\u6570\u636e\uff0c\u4e3a\u4ea4\u901a\u4e8b\u4ef6\u5f71\u54cd\u9884\u6d4b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "method": "\u63d0\u51fa\u5168LLM\u89e3\u51b3\u65b9\u6848\uff0c\u7ed3\u5408\u4ea4\u901a\u7279\u5f81\u548cLLM\u63d0\u53d6\u7684\u4e8b\u4ef6\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u6709\u6548\u9009\u62e9\u793a\u4f8b\u4f18\u5316LLM\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u3002\u8bc4\u4f30\u4e86\u4e09\u79cd\u5148\u8fdbLLM\u548c\u4e24\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u6700\u4f73LLM\u7684\u9884\u6d4b\u51c6\u786e\u6027\u4e0e\u6700\u5148\u8fdb\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u76f8\u5f53\uff0c\u5c3d\u7ba1LLM\u672a\u9488\u5bf9\u8be5\u4efb\u52a1\u8fdb\u884c\u4e13\u95e8\u8bad\u7ec3\u3002", "conclusion": "LLM\u662f\u4ea4\u901a\u4e8b\u4ef6\u5f71\u54cd\u9884\u6d4b\u7684\u53ef\u884c\u9009\u62e9\uff0c\u5c55\u793a\u4e86\u5176\u5728\u65e0\u9700\u4e13\u95e8\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u4ecd\u80fd\u53d6\u5f97\u4f18\u5f02\u8868\u73b0\u3002"}}
{"id": "2507.04877", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.04877", "abs": "https://arxiv.org/abs/2507.04877", "authors": ["Zewen Sun", "Ruoxiang Huang", "Jiahe Feng", "Rundong Kong", "Yuqian Wang", "Hengyu Liu", "Ziqi Gong", "Yuyuan Qin", "Yingxue Wang", "Yu Wang"], "title": "DoPI: Doctor-like Proactive Interrogation LLM for Traditional Chinese Medicine", "comment": null, "summary": "Enhancing interrogation capabilities in Traditional Chinese Medicine (TCM)\ndiagnosis through multi-turn dialogues and knowledge graphs presents a\nsignificant challenge for modern AI systems. Current large language models\n(LLMs), despite their advancements, exhibit notable limitations in medical\napplications, particularly in conducting effective multi-turn dialogues and\nproactive questioning. These shortcomings hinder their practical application\nand effectiveness in simulating real-world diagnostic scenarios. To address\nthese limitations, we propose DoPI, a novel LLM system specifically designed\nfor the TCM domain. The DoPI system introduces a collaborative architecture\ncomprising a guidance model and an expert model. The guidance model conducts\nmulti-turn dialogues with patients and dynamically generates questions based on\na knowledge graph to efficiently extract critical symptom information.\nSimultaneously, the expert model leverages deep TCM expertise to provide final\ndiagnoses and treatment plans. Furthermore, this study constructs a multi-turn\ndoctor-patient dialogue dataset to simulate realistic consultation scenarios\nand proposes a novel evaluation methodology that does not rely on manually\ncollected real-world consultation data. Experimental results show that the DoPI\nsystem achieves an accuracy rate of 84.68 percent in interrogation outcomes,\nsignificantly enhancing the model's communication ability during diagnosis\nwhile maintaining professional expertise.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDoPI\u7684\u65b0\u578bLLM\u7cfb\u7edf\uff0c\u65e8\u5728\u901a\u8fc7\u591a\u8f6e\u5bf9\u8bdd\u548c\u77e5\u8bc6\u56fe\u8c31\u63d0\u5347\u4e2d\u533b\u8bca\u65ad\u7684\u8be2\u95ee\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u533b\u5b66\u5e94\u7528\u4e2d\u5b58\u5728\u591a\u8f6e\u5bf9\u8bdd\u548c\u4e3b\u52a8\u63d0\u95ee\u7684\u5c40\u9650\u6027\uff0c\u5f71\u54cd\u4e86\u5176\u5728\u771f\u5b9e\u8bca\u65ad\u573a\u666f\u4e2d\u7684\u5b9e\u7528\u6027\u3002", "method": "DoPI\u7cfb\u7edf\u91c7\u7528\u534f\u4f5c\u67b6\u6784\uff0c\u5305\u62ec\u6307\u5bfc\u6a21\u578b\u548c\u4e13\u5bb6\u6a21\u578b\uff0c\u5206\u522b\u8d1f\u8d23\u591a\u8f6e\u5bf9\u8bdd\u548c\u8bca\u65ad\u5efa\u8bae\uff0c\u5e76\u6784\u5efa\u4e86\u6a21\u62df\u771f\u5b9e\u54a8\u8be2\u573a\u666f\u7684\u591a\u8f6e\u533b\u60a3\u5bf9\u8bdd\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0cDoPI\u7cfb\u7edf\u5728\u8be2\u95ee\u7ed3\u679c\u4e2d\u7684\u51c6\u786e\u7387\u8fbe\u523084.68%\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bca\u65ad\u6c9f\u901a\u80fd\u529b\u3002", "conclusion": "DoPI\u7cfb\u7edf\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709LLM\u5728\u4e2d\u533b\u8bca\u65ad\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u533b\u5b66AI\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.04893", "categories": ["cs.AI", "cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.04893", "abs": "https://arxiv.org/abs/2507.04893", "authors": ["Kaleem Ullah Qasim", "Jiashu Zhang"], "title": "MARBLE: A Multi-Agent Rule-Based LLM Reasoning Engine for Accident Severity Prediction", "comment": "13 pages, 5 figures", "summary": "Accident severity prediction plays a critical role in transportation safety\nsystems but is a persistently difficult task due to incomplete data, strong\nfeature dependencies, and severe class imbalance in which rare but\nhigh-severity cases are underrepresented and hard to detect. Existing methods\noften rely on monolithic models or black box prompting, which struggle to scale\nin noisy, real-world settings and offer limited interpretability. To address\nthese challenges, we propose MARBLE a multiagent rule based LLM engine that\ndecomposes the severity prediction task across a team of specialized reasoning\nagents, including an interchangeable ML-backed agent. Each agent focuses on a\nsemantic subset of features (e.g., spatial, environmental, temporal), enabling\nscoped reasoning and modular prompting without the risk of prompt saturation.\nPredictions are coordinated through either rule-based or LLM-guided consensus\nmechanisms that account for class rarity and confidence dynamics. The system\nretains structured traces of agent-level reasoning and coordination outcomes,\nsupporting in-depth interpretability and post-hoc performance diagnostics.\nAcross both UK and US datasets, MARBLE consistently outperforms traditional\nmachine learning classifiers and state-of-the-art (SOTA) prompt-based reasoning\nmethods including Chain-of-Thought (CoT), Least-to-Most (L2M), and\nTree-of-Thought (ToT) achieving nearly 90% accuracy where others plateau below\n48%. This performance redefines the practical ceiling for accident severity\nclassification under real world noise and extreme class imbalance. Our results\nposition MARBLE as a generalizable and interpretable framework for reasoning\nunder uncertainty in safety-critical applications.", "AI": {"tldr": "MARBLE\u662f\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u89c4\u5219\u9a71\u52a8\u7684LLM\u5f15\u64ce\uff0c\u901a\u8fc7\u5206\u89e3\u4efb\u52a1\u548c\u6a21\u5757\u5316\u63a8\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4ea4\u901a\u4e8b\u6545\u4e25\u91cd\u6027\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u4ea4\u901a\u4e8b\u6545\u4e25\u91cd\u6027\u9884\u6d4b\u56e0\u6570\u636e\u4e0d\u5b8c\u6574\u3001\u7279\u5f81\u4f9d\u8d56\u6027\u5f3a\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u800c\u56f0\u96be\u91cd\u91cd\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u73b0\u5b9e\u566a\u58f0\u4e14\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u3002", "method": "MARBLE\u91c7\u7528\u591a\u667a\u80fd\u4f53\u534f\u4f5c\uff0c\u6bcf\u4e2a\u667a\u80fd\u4f53\u4e13\u6ce8\u4e8e\u7279\u5b9a\u7279\u5f81\u5b50\u96c6\uff08\u5982\u7a7a\u95f4\u3001\u73af\u5883\u3001\u65f6\u95f4\uff09\uff0c\u5e76\u901a\u8fc7\u89c4\u5219\u6216LLM\u5f15\u5bfc\u7684\u5171\u8bc6\u673a\u5236\u534f\u8c03\u9884\u6d4b\u3002", "result": "\u5728\u82f1\u7f8e\u6570\u636e\u96c6\u4e0a\uff0cMARBLE\u51c6\u786e\u7387\u63a5\u8fd190%\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u548cSOTA\u63d0\u793a\u63a8\u7406\u65b9\u6cd5\uff08\u5982CoT\u3001L2M\u3001ToT\uff09\u3002", "conclusion": "MARBLE\u4e3a\u5b89\u5168\u5173\u952e\u5e94\u7528\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u63a8\u7406\u63d0\u4f9b\u4e86\u901a\u7528\u4e14\u53ef\u89e3\u91ca\u7684\u6846\u67b6\uff0c\u91cd\u65b0\u5b9a\u4e49\u4e86\u5206\u7c7b\u6027\u80fd\u7684\u4e0a\u9650\u3002"}}
{"id": "2507.04994", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.04994", "abs": "https://arxiv.org/abs/2507.04994", "authors": ["Adam Gould", "Gabriel de Olim Gaul", "Francesca Toni"], "title": "Supported Abstract Argumentation for Case-Based Reasoning", "comment": "Accepted to IARML@ICJAI2025: Workshop on the Interactions between\n  Analogical Reasoning and Machine Learning", "summary": "We introduce Supported Abstract Argumentation for Case-Based Reasoning\n(sAA-CBR), a binary classification model in which past cases engage in debates\nby arguing in favour of their labelling and attacking or supporting those with\nopposing or agreeing labels. With supports, sAA-CBR overcomes the limitation of\nits precursor AA-CBR, which can contain extraneous cases (or spikes) that are\nnot included in the debates. We prove that sAA-CBR contains no spikes, without\ntrading off key model properties", "AI": {"tldr": "sAA-CBR\u662f\u4e00\u79cd\u57fa\u4e8e\u6848\u4f8b\u63a8\u7406\u7684\u4e8c\u5143\u5206\u7c7b\u6a21\u578b\uff0c\u901a\u8fc7\u652f\u6301\u673a\u5236\u907f\u514d\u65e0\u5173\u6848\u4f8b\u7684\u5e72\u6270\uff0c\u540c\u65f6\u4fdd\u6301\u5173\u952e\u6a21\u578b\u7279\u6027\u3002", "motivation": "\u89e3\u51b3AA-CBR\u6a21\u578b\u4e2d\u53ef\u80fd\u5305\u542b\u65e0\u5173\u6848\u4f8b\uff08spikes\uff09\u7684\u95ee\u9898\uff0c\u63d0\u5347\u5206\u7c7b\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "\u5f15\u5165\u652f\u6301\u673a\u5236\uff0c\u4f7f\u6848\u4f8b\u5728\u8fa9\u8bba\u4e2d\u652f\u6301\u6216\u653b\u51fb\u5176\u4ed6\u6848\u4f8b\u7684\u6807\u7b7e\uff0c\u4ece\u800c\u907f\u514d\u65e0\u5173\u6848\u4f8b\u7684\u5e72\u6270\u3002", "result": "\u8bc1\u660esAA-CBR\u4e0d\u542b\u65e0\u5173\u6848\u4f8b\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u6a21\u578b\u7684\u5173\u952e\u7279\u6027\u3002", "conclusion": "sAA-CBR\u901a\u8fc7\u652f\u6301\u673a\u5236\u6709\u6548\u89e3\u51b3\u4e86AA-CBR\u7684\u5c40\u9650\u6027\uff0c\u662f\u4e00\u79cd\u66f4\u53ef\u9760\u7684\u5206\u7c7b\u6a21\u578b\u3002"}}
{"id": "2507.05011", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.05011", "abs": "https://arxiv.org/abs/2507.05011", "authors": ["Maxence Boels", "Harry Robertshaw", "Alejandro Granados", "Prokar Dasgupta", "Sebastien Ourselin"], "title": "When Imitation Learning Outperforms Reinforcement Learning in Surgical Action Planning", "comment": "This manuscript has been submitted to a conference and is being peer\n  reviewed", "summary": "Surgical action planning requires predicting future instrument-verb-target\ntriplets for real-time assistance. While teleoperated robotic surgery provides\nnatural expert demonstrations for imitation learning (IL), reinforcement\nlearning (RL) could potentially discover superior strategies through\nexploration. We present the first comprehensive comparison of IL versus RL for\nsurgical action planning on CholecT50. Our Dual-task Autoregressive Imitation\nLearning (DARIL) baseline achieves 34.6% action triplet recognition mAP and\n33.6% next frame prediction mAP with smooth planning degradation to 29.2% at\n10-second horizons. We evaluated three RL variants: world model-based RL,\ndirect video RL, and inverse RL enhancement. Surprisingly, all RL approaches\nunderperformed DARIL i.e. world model RL dropped to 3.1% mAP at 10s while\ndirect video RL achieved only 15.9%. Our analysis reveals that distribution\nmatching on expert-annotated test sets systematically favors IL over\npotentially valid RL policies that differ from training demonstrations. This\nchallenges assumptions about RL superiority in sequential decision making and\nprovides crucial insights for surgical AI development.", "AI": {"tldr": "\u8bba\u6587\u6bd4\u8f83\u4e86\u6a21\u4eff\u5b66\u4e60\uff08IL\uff09\u4e0e\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5728\u624b\u672f\u52a8\u4f5c\u89c4\u5212\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0IL\u4f18\u4e8eRL\u3002", "motivation": "\u63a2\u8ba8\u5728\u624b\u672f\u52a8\u4f5c\u89c4\u5212\u4e2d\uff0cIL\u548cRL\u54ea\u79cd\u65b9\u6cd5\u66f4\u6709\u6548\uff0c\u5c24\u5176\u662fRL\u662f\u5426\u5982\u9884\u671f\u4f18\u4e8eIL\u3002", "method": "\u63d0\u51fa\u4e86\u53cc\u4efb\u52a1\u81ea\u56de\u5f52\u6a21\u4eff\u5b66\u4e60\uff08DARIL\uff09\u57fa\u7ebf\uff0c\u5e76\u8bc4\u4f30\u4e86\u4e09\u79cdRL\u53d8\u4f53\uff1a\u57fa\u4e8e\u4e16\u754c\u6a21\u578b\u7684RL\u3001\u76f4\u63a5\u89c6\u9891RL\u548c\u9006RL\u589e\u5f3a\u3002", "result": "DARIL\u8868\u73b0\u6700\u4f73\uff0834.6% mAP\uff09\uff0c\u800c\u6240\u6709RL\u65b9\u6cd5\u5747\u8868\u73b0\u8f83\u5dee\uff08\u6700\u4f4e3.1% mAP\uff09\u3002\u5206\u6790\u8868\u660eIL\u5728\u4e13\u5bb6\u6807\u6ce8\u6d4b\u8bd5\u96c6\u4e0a\u66f4\u4f18\u3002", "conclusion": "\u6311\u6218\u4e86RL\u5728\u5e8f\u5217\u51b3\u7b56\u4e2d\u4f18\u4e8eIL\u7684\u5047\u8bbe\uff0c\u4e3a\u624b\u672fAI\u5f00\u53d1\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\u3002"}}
{"id": "2507.05088", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.05088", "abs": "https://arxiv.org/abs/2507.05088", "authors": ["Kilian R\u00fcckschlo\u00df", "Felix Weitk\u00e4mper"], "title": "How Rules Represent Causal Knowledge: Causal Modeling with Abductive Logic Programs", "comment": null, "summary": "Pearl observes that causal knowledge enables predicting the effects of\ninterventions, such as actions, whereas descriptive knowledge only permits\ndrawing conclusions from observation. This paper extends Pearl's approach to\ncausality and interventions to the setting of stratified abductive logic\nprograms. It shows how stable models of such programs can be given a causal\ninterpretation by building on philosophical foundations and recent work by\nBochman and Eelink et al. In particular, it provides a translation of abductive\nlogic programs into causal systems, thereby clarifying the informal causal\nreading of logic program rules and supporting principled reasoning about\nexternal actions. The main result establishes that the stable model semantics\nfor stratified programs conforms to key philosophical principles of causation,\nsuch as causal sufficiency, natural necessity, and irrelevance of unobserved\neffects. This justifies the use of stratified abductive logic programs as a\nframework for causal modeling and for predicting the effects of interventions", "AI": {"tldr": "\u672c\u6587\u6269\u5c55\u4e86Pearl\u7684\u56e0\u679c\u7406\u8bba\uff0c\u5c06\u5176\u5e94\u7528\u4e8e\u5206\u5c42\u6eaf\u56e0\u903b\u8f91\u7a0b\u5e8f\uff0c\u8bc1\u660e\u5176\u7a33\u5b9a\u6a21\u578b\u53ef\u89e3\u91ca\u4e3a\u56e0\u679c\u7cfb\u7edf\uff0c\u5e76\u9a8c\u8bc1\u5176\u7b26\u5408\u54f2\u5b66\u56e0\u679c\u539f\u5219\u3002", "motivation": "\u63a2\u8ba8\u5982\u4f55\u5c06\u56e0\u679c\u77e5\u8bc6\u5e94\u7528\u4e8e\u903b\u8f91\u7a0b\u5e8f\uff0c\u4ee5\u652f\u6301\u5bf9\u5916\u90e8\u5e72\u9884\u7684\u9884\u6d4b\u548c\u63a8\u7406\u3002", "method": "\u5c06\u5206\u5c42\u6eaf\u56e0\u903b\u8f91\u7a0b\u5e8f\u8f6c\u5316\u4e3a\u56e0\u679c\u7cfb\u7edf\uff0c\u9a8c\u8bc1\u5176\u7a33\u5b9a\u6a21\u578b\u8bed\u4e49\u662f\u5426\u7b26\u5408\u56e0\u679c\u539f\u5219\u3002", "result": "\u7a33\u5b9a\u6a21\u578b\u8bed\u4e49\u7b26\u5408\u56e0\u679c\u5145\u5206\u6027\u3001\u81ea\u7136\u5fc5\u8981\u6027\u548c\u672a\u89c2\u6d4b\u6548\u5e94\u65e0\u5173\u6027\u7b49\u54f2\u5b66\u539f\u5219\u3002", "conclusion": "\u5206\u5c42\u6eaf\u56e0\u903b\u8f91\u7a0b\u5e8f\u53ef\u4f5c\u4e3a\u56e0\u679c\u5efa\u6a21\u548c\u5e72\u9884\u9884\u6d4b\u7684\u6709\u6548\u6846\u67b6\u3002"}}
{"id": "2507.05110", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.05110", "abs": "https://arxiv.org/abs/2507.05110", "authors": ["Shixuan Liu", "Yue He", "Yunfei Wang", "Hao Zou", "Haoxiang Cheng", "Wenjing Yang", "Peng Cui", "Zhong Liu"], "title": "Rule Learning for Knowledge Graph Reasoning under Agnostic Distribution Shift", "comment": null, "summary": "Knowledge graph (KG) reasoning remains a critical research area focused on\ninferring missing knowledge by analyzing relationships among observed facts.\nDespite its success, a key limitation of existing KG reasoning methods is their\ndependence on the I.I.D assumption. This assumption can easily be violated due\nto unknown sample selection bias during training or agnostic distribution\nshifts during testing, significantly compromising model performance and\nreliability. To facilitate the deployment of KG reasoning in wild environments,\nthis study investigates learning logical rules from KGs affected by unknown\nselection bias. Additionally, we address test sets with agnostic distribution\nshifts, formally defining this challenge as out-of-distribution (OOD) KG\nreasoning-a previously underexplored problem. To solve the issue, we propose\nthe Stable Rule Learning (StableRule) framework, an end-to-end methodology that\nintegrates feature decorrelation with rule learning network, to enhance OOD\ngeneralization performance. By leveraging feature decorrelation, the StableRule\nframework mitigates the adverse effects of covariate shifts arising in OOD\nscenarios, thereby improving the robustness of the rule learning component in\neffectively deriving logical rules. Extensive experiments on seven benchmark\nKGs demonstrate the framework's superior effectiveness and stability across\ndiverse heterogeneous environments, underscoring its practical significance for\nreal-world applications.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faStableRule\u6846\u67b6\uff0c\u89e3\u51b3\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u4e2d\u56e0\u672a\u77e5\u9009\u62e9\u504f\u5dee\u548c\u5206\u5e03\u504f\u79fb\u5bfc\u81f4\u7684OOD\u95ee\u9898\uff0c\u901a\u8fc7\u7279\u5f81\u89e3\u8026\u548c\u89c4\u5219\u5b66\u4e60\u63d0\u5347\u6cdb\u5316\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u65b9\u6cd5\u4f9d\u8d56I.I.D\u5047\u8bbe\uff0c\u4f46\u5b9e\u9645\u4e2d\u53ef\u80fd\u56e0\u672a\u77e5\u9009\u62e9\u504f\u5dee\u6216\u5206\u5e03\u504f\u79fb\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\uff0c\u9650\u5236\u4e86\u5176\u5728\u5b9e\u9645\u73af\u5883\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51faStableRule\u6846\u67b6\uff0c\u7ed3\u5408\u7279\u5f81\u89e3\u8026\u548c\u89c4\u5219\u5b66\u4e60\u7f51\u7edc\uff0c\u4ee5\u589e\u5f3aOOD\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5728\u4e03\u4e2a\u57fa\u51c6\u77e5\u8bc6\u56fe\u8c31\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u5f02\u6784\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u5f02\u4e14\u7a33\u5b9a\u3002", "conclusion": "StableRule\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u77e5\u8bc6\u56fe\u8c31\u63a8\u7406\u5728OOD\u573a\u666f\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2507.05142", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.05142", "abs": "https://arxiv.org/abs/2507.05142", "authors": ["Wei Xu", "Haoran Li", "Baoyuan Ou", "Lai Xu", "Yingjie Qin", "Ruilong Su", "Ruiwen Xu"], "title": "GIST: Cross-Domain Click-Through Rate Prediction via Guided Content-Behavior Distillation", "comment": null, "summary": "Cross-domain Click-Through Rate prediction aims to tackle the data sparsity\nand the cold start problems in online advertising systems by transferring\nknowledge from source domains to a target domain. Most existing methods rely on\noverlapping users to facilitate this transfer, often focusing on joint training\nor pre-training with fine-tuning approach to connect the source and target\ndomains. However, in real-world industrial settings, joint training struggles\nto learn optimal representations with different distributions, and pre-training\nwith fine-tuning is not well-suited for continuously integrating new data. To\naddress these issues, we propose GIST, a cross-domain lifelong sequence model\nthat decouples the training processes of the source and target domains. Unlike\nprevious methods that search lifelong sequences in the source domains using\nonly content or behavior signals or their simple combinations, we innovatively\nintroduce a Content-Behavior Joint Training Module (CBJT), which aligns\ncontent-behavior distributions and combines them with guided information to\nfacilitate a more stable representation. Furthermore, we develop an Asymmetric\nSimilarity Integration strategy (ASI) to augment knowledge transfer through\nsimilarity computation. Extensive experiments demonstrate the effectiveness of\nGIST, surpassing SOTA methods on offline evaluations and an online A/B test.\nDeployed on the Xiaohongshu (RedNote) platform, GIST effectively enhances\nonline ads system performance at scale, serving hundreds of millions of daily\nactive users.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faGIST\u6a21\u578b\uff0c\u901a\u8fc7\u89e3\u8026\u6e90\u57df\u548c\u76ee\u6807\u57df\u7684\u8bad\u7ec3\u8fc7\u7a0b\uff0c\u7ed3\u5408\u5185\u5bb9-\u884c\u4e3a\u8054\u5408\u8bad\u7ec3\u6a21\u5757\uff08CBJT\uff09\u548c\u975e\u5bf9\u79f0\u76f8\u4f3c\u6027\u96c6\u6210\u7b56\u7565\uff08ASI\uff09\uff0c\u6709\u6548\u89e3\u51b3\u8de8\u57df\u70b9\u51fb\u7387\u9884\u6d4b\u4e2d\u7684\u6570\u636e\u7a00\u758f\u548c\u51b7\u542f\u52a8\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u91cd\u53e0\u7528\u6237\u8fdb\u884c\u77e5\u8bc6\u8fc1\u79fb\uff0c\u4f46\u8054\u5408\u8bad\u7ec3\u96be\u4ee5\u5904\u7406\u4e0d\u540c\u5206\u5e03\u7684\u6570\u636e\uff0c\u800c\u9884\u8bad\u7ec3\u52a0\u5fae\u8c03\u4e0d\u9002\u5408\u6301\u7eed\u96c6\u6210\u65b0\u6570\u636e\u3002", "method": "\u63d0\u51faGIST\u6a21\u578b\uff0c\u91c7\u7528CBJT\u6a21\u5757\u5bf9\u9f50\u5185\u5bb9-\u884c\u4e3a\u5206\u5e03\uff0c\u7ed3\u5408ASI\u7b56\u7565\u589e\u5f3a\u77e5\u8bc6\u8fc1\u79fb\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eGIST\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728\u5c0f\u7ea2\u4e66\u5e73\u53f0\u6210\u529f\u90e8\u7f72\uff0c\u663e\u8457\u63d0\u5347\u5e7f\u544a\u7cfb\u7edf\u6027\u80fd\u3002", "conclusion": "GIST\u901a\u8fc7\u521b\u65b0\u7684\u8bad\u7ec3\u7b56\u7565\u548c\u77e5\u8bc6\u8fc1\u79fb\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8de8\u57df\u70b9\u51fb\u7387\u9884\u6d4b\u7684\u6311\u6218\u3002"}}
{"id": "2507.05201", "categories": ["cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.05201", "abs": "https://arxiv.org/abs/2507.05201", "authors": ["Andrew Sellergren", "Sahar Kazemzadeh", "Tiam Jaroensri", "Atilla Kiraly", "Madeleine Traverse", "Timo Kohlberger", "Shawn Xu", "Fayaz Jamil", "C\u00edan Hughes", "Charles Lau", "Justin Chen", "Fereshteh Mahvar", "Liron Yatziv", "Tiffany Chen", "Bram Sterling", "Stefanie Anna Baby", "Susanna Maria Baby", "Jeremy Lai", "Samuel Schmidgall", "Lu Yang", "Kejia Chen", "Per Bjornsson", "Shashir Reddy", "Ryan Brush", "Kenneth Philbrick", "Howard Hu", "Howard Yang", "Richa Tiwari", "Sunny Jansen", "Preeti Singh", "Yun Liu", "Shekoofeh Azizi", "Aishwarya Kamath", "Johan Ferret", "Shreya Pathak", "Nino Vieillard", "Ramona Merhej", "Sarah Perrin", "Tatiana Matejovicova", "Alexandre Ram\u00e9", "Morgane Riviere", "Louis Rouillard", "Thomas Mesnard", "Geoffrey Cideron", "Jean-bastien Grill", "Sabela Ramos", "Edouard Yvinec", "Michelle Casbon", "Elena Buchatskaya", "Jean-Baptiste Alayrac", "Dmitry", "Lepikhin", "Vlad Feinberg", "Sebastian Borgeaud", "Alek Andreev", "Cassidy Hardin", "Robert Dadashi", "L\u00e9onard Hussenot", "Armand Joulin", "Olivier Bachem", "Yossi Matias", "Katherine Chou", "Avinatan Hassidim", "Kavi Goel", "Clement Farabet", "Joelle Barral", "Tris Warkentin", "Jonathon Shlens", "David Fleet", "Victor Cotruta", "Omar Sanseviero", "Gus Martins", "Phoebe Kirk", "Anand Rao", "Shravya Shetty", "David F. Steiner", "Can Kirmizibayrak", "Rory Pilgrim", "Daniel Golden", "Lin Yang"], "title": "MedGemma Technical Report", "comment": null, "summary": "Artificial intelligence (AI) has significant potential in healthcare\napplications, but its training and deployment faces challenges due to\nhealthcare's diverse data, complex tasks, and the need to preserve privacy.\nFoundation models that perform well on medical tasks and require less\ntask-specific tuning data are critical to accelerate the development of\nhealthcare AI applications. We introduce MedGemma, a collection of medical\nvision-language foundation models based on Gemma 3 4B and 27B. MedGemma\ndemonstrates advanced medical understanding and reasoning on images and text,\nsignificantly exceeding the performance of similar-sized generative models and\napproaching the performance of task-specific models, while maintaining the\ngeneral capabilities of the Gemma 3 base models. For out-of-distribution tasks,\nMedGemma achieves 2.6-10% improvement on medical multimodal question answering,\n15.5-18.1% improvement on chest X-ray finding classification, and 10.8%\nimprovement on agentic evaluations compared to the base models. Fine-tuning\nMedGemma further improves performance in subdomains, reducing errors in\nelectronic health record information retrieval by 50% and reaching comparable\nperformance to existing specialized state-of-the-art methods for pneumothorax\nclassification and histopathology patch classification. We additionally\nintroduce MedSigLIP, a medically-tuned vision encoder derived from SigLIP.\nMedSigLIP powers the visual understanding capabilities of MedGemma and as an\nencoder achieves comparable or better performance than specialized medical\nimage encoders. Taken together, the MedGemma collection provides a strong\nfoundation of medical image and text capabilities, with potential to\nsignificantly accelerate medical research and development of downstream\napplications. The MedGemma collection, including tutorials and model weights,\ncan be found at https://goo.gle/medgemma.", "AI": {"tldr": "MedGemma\u662f\u4e00\u7ec4\u57fa\u4e8eGemma 3\u7684\u533b\u5b66\u89c6\u89c9-\u8bed\u8a00\u57fa\u7840\u6a21\u578b\uff0c\u5728\u533b\u7597\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u8d85\u8d8a\u540c\u7c7b\u751f\u6210\u6a21\u578b\uff0c\u5e76\u63a5\u8fd1\u4efb\u52a1\u4e13\u7528\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u533b\u7597AI\u5e94\u7528\u4e2d\u6570\u636e\u591a\u6837\u6027\u3001\u4efb\u52a1\u590d\u6742\u6027\u548c\u9690\u79c1\u4fdd\u62a4\u7684\u6311\u6218\uff0c\u52a0\u901f\u533b\u7597AI\u53d1\u5c55\u3002", "method": "\u57fa\u4e8eGemma 3 4B\u548c27B\u6784\u5efaMedGemma\uff0c\u5e76\u5f15\u5165MedSigLIP\u4f5c\u4e3a\u89c6\u89c9\u7f16\u7801\u5668\u3002", "result": "\u5728\u591a\u9879\u533b\u7597\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5982\u533b\u5b66\u591a\u6a21\u6001\u95ee\u7b54\u3001\u80f8\u90e8X\u5149\u5206\u7c7b\u7b49\uff0c\u6027\u80fd\u63d0\u5347\u663e\u8457\u3002", "conclusion": "MedGemma\u4e3a\u533b\u5b66\u7814\u7a76\u548c\u4e0b\u6e38\u5e94\u7528\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u57fa\u7840\uff0c\u5177\u6709\u52a0\u901f\u533b\u7597AI\u53d1\u5c55\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.05241", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.05241", "abs": "https://arxiv.org/abs/2507.05241", "authors": ["Jingyi Chai", "Shuo Tang", "Rui Ye", "Yuwen Du", "Xinyu Zhu", "Mengcheng Zhou", "Yanfeng Wang", "Weinan E", "Siheng Chen"], "title": "SciMaster: Towards General-Purpose Scientific AI Agents, Part I. X-Master as Foundation: Can We Lead on Humanity's Last Exam?", "comment": "12 pages, 7 figures", "summary": "The rapid advancements of AI agents have ignited the long-held ambition of\nleveraging them to accelerate scientific discovery. Achieving this goal\nrequires a deep understanding of the frontiers of human knowledge. As such,\nHumanity's Last Exam (HLE) provides an exceptionally challenging touchstone for\nevaluating scientific AI agents. In this work, we aim to construct the\nfoundational architecture for general-purpose agents and validate the\ncapabilities through leading performance on HLE. To achieve this, we introduce\nX-Master, a tool-augmented reasoning agent designed to emulate human\nresearchers by interacting flexibly with external tools during its reasoning\nprocess. This agent, guided by the conceptualization of code as an interaction\nlanguage, can flexibly leverage built-in Python libraries and our customized\ntools to augment the reasoning. We further scale its capabilities through\nX-Masters, a scattered-and-stacked agentic workflow that systematically\nenhances breadth and depth of reasoning. Our open-source solution, X-Masters,\nsets a new state-of-the-art record on HLE with a score of 32.1%, surpassing\nOpenAI's and Google's Deep Research (26.6% and 26.9%) and becoming the first to\nexceed the 30% threshold. This work allows us to gain a deeper understanding of\ncomplex task-solving and accumulates valuable experience that can inform future\nadvancements, guiding subsequent model training.", "AI": {"tldr": "X-Master\u662f\u4e00\u79cd\u5de5\u5177\u589e\u5f3a\u63a8\u7406\u4ee3\u7406\uff0c\u901a\u8fc7\u7075\u6d3b\u4f7f\u7528\u5916\u90e8\u5de5\u5177\u6a21\u62df\u4eba\u7c7b\u7814\u7a76\u8005\uff0c\u5728HLE\u4e0a\u53d6\u5f9732.1%\u7684\u9886\u5148\u6210\u7ee9\u3002", "motivation": "\u5229\u7528AI\u52a0\u901f\u79d1\u5b66\u53d1\u73b0\uff0c\u9700\u8bc4\u4f30\u5176\u7406\u89e3\u4eba\u7c7b\u77e5\u8bc6\u524d\u6cbf\u7684\u80fd\u529b\uff0cHLE\u4e3a\u6b64\u63d0\u4f9b\u4e86\u6311\u6218\u6027\u57fa\u51c6\u3002", "method": "\u63d0\u51faX-Master\u4ee3\u7406\uff0c\u4ee5\u4ee3\u7801\u4e3a\u4ea4\u4e92\u8bed\u8a00\uff0c\u7ed3\u5408Python\u5e93\u548c\u5b9a\u5236\u5de5\u5177\u589e\u5f3a\u63a8\u7406\uff1b\u901a\u8fc7X-Masters\u5de5\u4f5c\u6d41\u6269\u5c55\u80fd\u529b\u3002", "result": "X-Masters\u5728HLE\u4e0a\u4ee532.1%\u7684\u6210\u7ee9\u521b\u4e0b\u65b0\u7eaa\u5f55\uff0c\u8d85\u8d8aOpenAI\u548cGoogle\u768426.6%\u548c26.9%\u3002", "conclusion": "X-Masters\u4e3a\u590d\u6742\u4efb\u52a1\u89e3\u51b3\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u4e3a\u672a\u6765\u6a21\u578b\u8bad\u7ec3\u79ef\u7d2f\u4e86\u7ecf\u9a8c\u3002"}}
{"id": "2507.05244", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.05244", "abs": "https://arxiv.org/abs/2507.05244", "authors": ["Benjamin Li", "Shuyang Shi", "Lucia Romero", "Huao Li", "Yaqi Xie", "Woojun Kim", "Stefanos Nikolaidis", "Michael Lewis", "Katia Sycara", "Simon Stepputtis"], "title": "Modeling Latent Partner Strategies for Adaptive Zero-Shot Human-Agent Collaboration", "comment": "Best Paper Award at the RSS 2025 Generative Models x HRI (GenAI-HRI)\n  Workshop", "summary": "In collaborative tasks, being able to adapt to your teammates is a necessary\nrequirement for success. When teammates are heterogeneous, such as in\nhuman-agent teams, agents need to be able to observe, recognize, and adapt to\ntheir human partners in real time. This becomes particularly challenging in\ntasks with time pressure and complex strategic spaces where the dynamics can\nchange rapidly. In this work, we introduce TALENTS, a strategy-conditioned\ncooperator framework that learns to represent, categorize, and adapt to a range\nof partner strategies, enabling ad-hoc teamwork. Our approach utilizes a\nvariational autoencoder to learn a latent strategy space from trajectory data.\nThis latent space represents the underlying strategies that agents employ.\nSubsequently, the system identifies different types of strategy by clustering\nthe data. Finally, a cooperator agent is trained to generate partners for each\ntype of strategy, conditioned on these clusters. In order to adapt to\npreviously unseen partners, we leverage a fixed-share regret minimization\nalgorithm that infers and adjusts the estimated partner strategy dynamically.\nWe assess our approach in a customized version of the Overcooked environment,\nposing a challenging cooperative cooking task that demands strong coordination\nacross a wide range of possible strategies. Using an online user study, we show\nthat our agent outperforms current baselines when working with unfamiliar human\npartners.", "AI": {"tldr": "TALENTS\u6846\u67b6\u901a\u8fc7\u53d8\u5206\u81ea\u7f16\u7801\u5668\u5b66\u4e60\u7b56\u7565\u7a7a\u95f4\uff0c\u52a8\u6001\u9002\u5e94\u5f02\u6784\u961f\u53cb\uff0c\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u3002", "motivation": "\u5f02\u6784\u56e2\u961f\uff08\u5982\u4eba\u673a\u534f\u4f5c\uff09\u4e2d\uff0c\u5b9e\u65f6\u9002\u5e94\u961f\u53cb\u7b56\u7565\u662f\u6210\u529f\u5173\u952e\uff0c\u5c24\u5176\u5728\u65f6\u95f4\u538b\u529b\u548c\u590d\u6742\u52a8\u6001\u4efb\u52a1\u4e2d\u3002", "method": "\u4f7f\u7528\u53d8\u5206\u81ea\u7f16\u7801\u5668\u5b66\u4e60\u7b56\u7565\u7a7a\u95f4\uff0c\u805a\u7c7b\u7b56\u7565\u7c7b\u578b\uff0c\u8bad\u7ec3\u6761\u4ef6\u5408\u4f5c\u8005\uff0c\u5e76\u5229\u7528\u9057\u61be\u6700\u5c0f\u5316\u7b97\u6cd5\u52a8\u6001\u9002\u5e94\u65b0\u961f\u53cb\u3002", "result": "\u5728Overcooked\u73af\u5883\u4e2d\uff0cTALENTS\u4f18\u4e8e\u57fa\u7ebf\uff0c\u80fd\u6709\u6548\u9002\u5e94\u964c\u751f\u4eba\u7c7b\u961f\u53cb\u3002", "conclusion": "TALENTS\u6846\u67b6\u4e3a\u5f02\u6784\u56e2\u961f\u534f\u4f5c\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u81ea\u9002\u5e94\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.05246", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.05246", "abs": "https://arxiv.org/abs/2507.05246", "authors": ["Scott Emmons", "Erik Jenner", "David K. Elson", "Rif A. Saurous", "Senthooran Rajamanoharan", "Heng Chen", "Irhum Shafkat", "Rohin Shah"], "title": "When Chain of Thought is Necessary, Language Models Struggle to Evade Monitors", "comment": null, "summary": "While chain-of-thought (CoT) monitoring is an appealing AI safety defense,\nrecent work on \"unfaithfulness\" has cast doubt on its reliability. These\nfindings highlight an important failure mode, particularly when CoT acts as a\npost-hoc rationalization in applications like auditing for bias. However, for\nthe distinct problem of runtime monitoring to prevent severe harm, we argue the\nkey property is not faithfulness but monitorability. To this end, we introduce\na conceptual framework distinguishing CoT-as-rationalization from\nCoT-as-computation. We expect that certain classes of severe harm will require\ncomplex, multi-step reasoning that necessitates CoT-as-computation. Replicating\nthe experimental setups of prior work, we increase the difficulty of the bad\nbehavior to enforce this necessity condition; this forces the model to expose\nits reasoning, making it monitorable. We then present methodology guidelines to\nstress-test CoT monitoring against deliberate evasion. Applying these\nguidelines, we find that models can learn to obscure their intentions, but only\nwhen given significant help, such as detailed human-written strategies or\niterative optimization against the monitor. We conclude that, while not\ninfallible, CoT monitoring offers a substantial layer of defense that requires\nactive protection and continued stress-testing.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u94fe\u5f0f\u601d\u7ef4\uff08CoT\uff09\u76d1\u63a7\u5728AI\u5b89\u5168\u4e2d\u7684\u53ef\u9760\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u76d1\u63a7\u6027\u7684\u91cd\u8981\u6027\uff0c\u5e76\u533a\u5206\u4e86CoT\u4f5c\u4e3a\u5408\u7406\u5316\u4e0e\u8ba1\u7b97\u7684\u4e0d\u540c\u7528\u9014\u3002", "motivation": "\u8fd1\u671f\u7814\u7a76\u53d1\u73b0CoT\u5728\u4f5c\u4e3a\u540e\u5408\u7406\u5316\u5de5\u5177\u65f6\u5b58\u5728\u4e0d\u53ef\u9760\u6027\uff0c\u5c24\u5176\u662f\u5728\u9632\u6b62\u4e25\u91cd\u5371\u5bb3\u7684\u8fd0\u884c\u65f6\u76d1\u63a7\u4e2d\uff0c\u9700\u8981\u5173\u6ce8\u76d1\u63a7\u6027\u800c\u975e\u5fe0\u5b9e\u6027\u3002", "method": "\u63d0\u51fa\u533a\u5206CoT-as-rationalization\u548cCoT-as-computation\u7684\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u589e\u52a0\u884c\u4e3a\u96be\u5ea6\u5f3a\u5236\u6a21\u578b\u66b4\u9732\u63a8\u7406\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6a21\u578b\u5728\u83b7\u5f97\u8be6\u7ec6\u7b56\u7565\u6216\u4f18\u5316\u76d1\u63a7\u65f6\u53ef\u80fd\u9690\u85cf\u610f\u56fe\uff0c\u4f46CoT\u76d1\u63a7\u4ecd\u80fd\u63d0\u4f9b\u6709\u6548\u9632\u5fa1\u3002", "conclusion": "CoT\u76d1\u63a7\u867d\u975e\u5b8c\u7f8e\uff0c\u4f46\u4f5c\u4e3a\u9632\u5fa1\u5c42\u9700\u6301\u7eed\u538b\u529b\u6d4b\u8bd5\u548c\u4e3b\u52a8\u4fdd\u62a4\u3002"}}
