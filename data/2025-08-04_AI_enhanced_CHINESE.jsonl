{"id": "2508.00081", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00081", "abs": "https://arxiv.org/abs/2508.00081", "authors": ["Fred Mutisya", "Shikoh Gitau", "Nasubo Ongoma", "Keith Mbae", "Elizabeth Wamicha"], "title": "Rethinking Evidence Hierarchies in Medical Language Benchmarks: A Critical Evaluation of HealthBench", "comment": null, "summary": "HealthBench, a benchmark designed to measure the capabilities of AI systems\nfor health better (Arora et al., 2025), has advanced medical language model\nevaluation through physician-crafted dialogues and transparent rubrics.\nHowever, its reliance on expert opinion, rather than high-tier clinical\nevidence, risks codifying regional biases and individual clinician\nidiosyncrasies, further compounded by potential biases in automated grading\nsystems. These limitations are particularly magnified in low- and middle-income\nsettings, where issues like sparse neglected tropical disease coverage and\nregion-specific guideline mismatches are prevalent.\n  The unique challenges of the African context, including data scarcity,\ninadequate infrastructure, and nascent regulatory frameworks, underscore the\nurgent need for more globally relevant and equitable benchmarks. To address\nthese shortcomings, we propose anchoring reward functions in version-controlled\nClinical Practice Guidelines (CPGs) that incorporate systematic reviews and\nGRADE evidence ratings.\n  Our roadmap outlines \"evidence-robust\" reinforcement learning via\nrubric-to-guideline linkage, evidence-weighted scoring, and contextual override\nlogic, complemented by a focus on ethical considerations and the integration of\ndelayed outcome feedback. By re-grounding rewards in rigorously vetted CPGs,\nwhile preserving HealthBench's transparency and physician engagement, we aim to\nfoster medical language models that are not only linguistically polished but\nalso clinically trustworthy, ethically sound, and globally relevant.", "AI": {"tldr": "HealthBench\u4f5c\u4e3a\u8bc4\u4f30\u533b\u7597AI\u7cfb\u7edf\u7684\u57fa\u51c6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5982\u4f9d\u8d56\u4e13\u5bb6\u610f\u89c1\u800c\u975e\u4e34\u5e8a\u8bc1\u636e\uff0c\u53ef\u80fd\u5f15\u5165\u504f\u89c1\u3002\u4f5c\u8005\u63d0\u51fa\u57fa\u4e8e\u4e34\u5e8a\u5b9e\u8df5\u6307\u5357\uff08CPGs\uff09\u7684\u6539\u8fdb\u65b9\u6848\uff0c\u4ee5\u589e\u5f3a\u5168\u7403\u9002\u7528\u6027\u548c\u516c\u5e73\u6027\u3002", "motivation": "HealthBench\u4f9d\u8d56\u4e13\u5bb6\u610f\u89c1\uff0c\u53ef\u80fd\u56fa\u5316\u533a\u57df\u504f\u89c1\u548c\u4e2a\u4f53\u5dee\u5f02\uff0c\u5c24\u5176\u5728\u4f4e\u6536\u5165\u5730\u533a\u95ee\u9898\u66f4\u7a81\u51fa\u3002\u9700\u8981\u66f4\u5168\u7403\u5316\u548c\u516c\u5e73\u7684\u57fa\u51c6\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u7248\u672c\u63a7\u5236CPGs\u7684\u5956\u52b1\u51fd\u6570\uff0c\u7ed3\u5408\u7cfb\u7edf\u8bc4\u4ef7\u548cGRADE\u8bc1\u636e\u8bc4\u7ea7\uff0c\u901a\u8fc7\u8bc1\u636e\u52a0\u6743\u8bc4\u5206\u548c\u4e0a\u4e0b\u6587\u8986\u76d6\u903b\u8f91\u6539\u8fdb\u3002", "result": "\u6539\u8fdb\u65b9\u6848\u65e8\u5728\u63d0\u5347\u533b\u7597\u8bed\u8a00\u6a21\u578b\u7684\u4e34\u5e8a\u53ef\u4fe1\u5ea6\u3001\u4f26\u7406\u6027\u548c\u5168\u7403\u9002\u7528\u6027\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408CPGs\u548c\u6539\u8fdb\u8bc4\u5206\u673a\u5236\uff0c\u53ef\u4ee5\u6784\u5efa\u66f4\u900f\u660e\u3001\u516c\u5e73\u4e14\u5168\u7403\u9002\u7528\u7684\u533b\u7597AI\u8bc4\u4f30\u57fa\u51c6\u3002"}}
{"id": "2508.00106", "categories": ["cs.AI", "cs.LG", "cs.LO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.00106", "abs": "https://arxiv.org/abs/2508.00106", "authors": ["Ernest Bonnah", "Luan Viet Nguyen", "Khaza Anuarul Hoque"], "title": "Hyperproperty-Constrained Secure Reinforcement Learning", "comment": "Accepted in IEEE/ACM MEMOCODE 2025", "summary": "Hyperproperties for Time Window Temporal Logic (HyperTWTL) is a\ndomain-specific formal specification language known for its effectiveness in\ncompactly representing security, opacity, and concurrency properties for\nrobotics applications. This paper focuses on HyperTWTL-constrained secure\nreinforcement learning (SecRL). Although temporal logic-constrained safe\nreinforcement learning (SRL) is an evolving research problem with several\nexisting literature, there is a significant research gap in exploring\nsecurity-aware reinforcement learning (RL) using hyperproperties. Given the\ndynamics of an agent as a Markov Decision Process (MDP) and opacity/security\nconstraints formalized as HyperTWTL, we propose an approach for learning\nsecurity-aware optimal policies using dynamic Boltzmann softmax RL while\nsatisfying the HyperTWTL constraints. The effectiveness and scalability of our\nproposed approach are demonstrated using a pick-up and delivery robotic mission\ncase study. We also compare our results with two other baseline RL algorithms,\nshowing that our proposed method outperforms them.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eHyperTWTL\u7684\u5b89\u5168\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff08SecRL\uff09\uff0c\u901a\u8fc7\u52a8\u6001Boltzmann softmax RL\u5b66\u4e60\u5b89\u5168\u611f\u77e5\u7684\u6700\u4f18\u7b56\u7565\uff0c\u5e76\u5728\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5728\u5229\u7528\u8d85\u5c5e\u6027\u63a2\u7d22\u5b89\u5168\u611f\u77e5\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u65b9\u9762\u5b58\u5728\u7a7a\u767d\uff0c\u5c24\u5176\u662f\u5728\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u901a\u8fc7HyperTWTL\u5f62\u5f0f\u5316\u5b89\u5168\u7ea6\u675f\uff0c\u63d0\u51fa\u4e00\u79cd\u65b0\u7684SecRL\u65b9\u6cd5\u3002", "method": "\u5c06\u667a\u80fd\u4f53\u7684\u52a8\u6001\u5efa\u6a21\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDP\uff09\uff0c\u5e76\u4f7f\u7528HyperTWTL\u5f62\u5f0f\u5316\u4e0d\u900f\u660e\u6027/\u5b89\u5168\u7ea6\u675f\u3002\u91c7\u7528\u52a8\u6001Boltzmann softmax RL\u5b66\u4e60\u6ee1\u8db3\u7ea6\u675f\u7684\u6700\u4f18\u7b56\u7565\u3002", "result": "\u5728\u673a\u5668\u4eba\u62fe\u53d6\u548c\u4ea4\u4ed8\u4efb\u52a1\u6848\u4f8b\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u5e76\u4f18\u4e8e\u4e24\u79cd\u57fa\u7ebfRL\u7b97\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684HyperTWTL\u7ea6\u675fSecRL\u65b9\u6cd5\u5728\u5b89\u5168\u611f\u77e5RL\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u673a\u5668\u4eba\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.00116", "categories": ["cs.AI", "H.4.1; I.2.1"], "pdf": "https://arxiv.org/pdf/2508.00116", "abs": "https://arxiv.org/abs/2508.00116", "authors": ["Wil M. P. van der Aalst"], "title": "No AI Without PI! Object-Centric Process Mining as the Enabler for Generative, Predictive, and Prescriptive Artificial Intelligence", "comment": "10 pages, 4 figures, preprint keynote paper of the seventh\n  International Conference on Intelligent and Fuzzy Systems (INFUS 2025)", "summary": "The uptake of Artificial Intelligence (AI) impacts the way we work, interact,\ndo business, and conduct research. However, organizations struggle to apply AI\nsuccessfully in industrial settings where the focus is on end-to-end\noperational processes. Here, we consider generative, predictive, and\nprescriptive AI and elaborate on the challenges of diagnosing and improving\nsuch processes. We show that AI needs to be grounded using Object-Centric\nProcess Mining (OCPM). Process-related data are structured and\norganization-specific and, unlike text, processes are often highly dynamic.\nOCPM is the missing link connecting data and processes and enables different\nforms of AI. We use the term Process Intelligence (PI) to refer to the\namalgamation of process-centric data-driven techniques able to deal with a\nvariety of object and event types, enabling AI in an organizational context.\nThis paper explains why AI requires PI to improve operational processes and\nhighlights opportunities for successfully combining OCPM and generative,\npredictive, and prescriptive AI.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86AI\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\u7684\u5e94\u7528\u6311\u6218\uff0c\u63d0\u51fa\u901a\u8fc7\u5bf9\u8c61\u4e2d\u5fc3\u8fc7\u7a0b\u6316\u6398\uff08OCPM\uff09\u548c\u8fc7\u7a0b\u667a\u80fd\uff08PI\uff09\u7ed3\u5408\u751f\u6210\u3001\u9884\u6d4b\u548c\u89c4\u8303AI\uff0c\u4ee5\u4f18\u5316\u7aef\u5230\u7aef\u64cd\u4f5c\u6d41\u7a0b\u3002", "motivation": "\u7ec4\u7ec7\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\u6210\u529f\u5e94\u7528AI\u9762\u4e34\u56f0\u96be\uff0c\u5c24\u5176\u662f\u7aef\u5230\u7aef\u64cd\u4f5c\u6d41\u7a0b\u7684\u8bca\u65ad\u548c\u6539\u8fdb\u3002", "method": "\u91c7\u7528\u5bf9\u8c61\u4e2d\u5fc3\u8fc7\u7a0b\u6316\u6398\uff08OCPM\uff09\u548c\u8fc7\u7a0b\u667a\u80fd\uff08PI\uff09\u7ed3\u5408\u751f\u6210\u3001\u9884\u6d4b\u548c\u89c4\u8303AI\u3002", "result": "OCPM\u662f\u8fde\u63a5\u6570\u636e\u548c\u8fc7\u7a0b\u7684\u5173\u952e\uff0cPI\u4e3aAI\u5728\u7ec4\u7ec7\u73af\u5883\u4e2d\u63d0\u4f9b\u4e86\u652f\u6301\u3002", "conclusion": "AI\u9700\u8981PI\u6765\u6539\u8fdb\u64cd\u4f5c\u6d41\u7a0b\uff0cOCPM\u4e0e\u591a\u79cdAI\u5f62\u5f0f\u7684\u7ed3\u5408\u5177\u6709\u5e7f\u9614\u524d\u666f\u3002"}}
{"id": "2508.00129", "categories": ["cs.AI", "math.OC"], "pdf": "https://arxiv.org/pdf/2508.00129", "abs": "https://arxiv.org/abs/2508.00129", "authors": ["Agust\u00edn Borda", "Juan Bautista Cabral", "Gonzalo Giarda", "Diego Nicol\u00e1s Gimenez Irusta", "Paula Pacheco", "Alvaro Roy Schachner"], "title": "Algorithmic Detection of Rank Reversals, Transitivity Violations, and Decomposition Inconsistencies in Multi-Criteria Decision Analysis", "comment": null, "summary": "In Multi-Criteria Decision Analysis, Rank Reversals are a serious problem\nthat can greatly affect the results of a Multi-Criteria Decision Method against\na particular set of alternatives. It is therefore useful to have a mechanism\nthat allows one to measure the performance of a method on a set of\nalternatives. This idea could be taken further to build a global ranking of the\neffectiveness of different methods to solve a problem. In this paper, we\npresent three tests that detect the presence of Rank Reversals, along with\ntheir implementation in the Scikit-Criteria library. We also address the\ncomplications that arise when implementing these tests for general scenarios\nand the design considerations we made to handle them. We close with a\ndiscussion about how these additions could play a major role in the judgment of\nmulti-criteria decision methods for problem solving.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e09\u79cd\u68c0\u6d4b\u591a\u51c6\u5219\u51b3\u7b56\u5206\u6790\u4e2d\u6392\u540d\u53cd\u8f6c\u95ee\u9898\u7684\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u5e76\u5728Scikit-Criteria\u5e93\u4e2d\u5b9e\u73b0\uff0c\u8ba8\u8bba\u4e86\u5176\u8bbe\u8ba1\u8003\u8651\u53ca\u5bf9\u65b9\u6cd5\u8bc4\u4f30\u7684\u6f5c\u5728\u5f71\u54cd\u3002", "motivation": "\u591a\u51c6\u5219\u51b3\u7b56\u5206\u6790\u4e2d\u7684\u6392\u540d\u53cd\u8f6c\u95ee\u9898\u4e25\u91cd\u5f71\u54cd\u7ed3\u679c\u51c6\u786e\u6027\uff0c\u9700\u6709\u6548\u673a\u5236\u8bc4\u4f30\u65b9\u6cd5\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e09\u79cd\u6d4b\u8bd5\u65b9\u6cd5\u68c0\u6d4b\u6392\u540d\u53cd\u8f6c\uff0c\u5e76\u5728Scikit-Criteria\u5e93\u4e2d\u5b9e\u73b0\uff0c\u89e3\u51b3\u901a\u7528\u573a\u666f\u4e0b\u7684\u5b9e\u73b0\u590d\u6742\u6027\u3002", "result": "\u6210\u529f\u5b9e\u73b0\u6d4b\u8bd5\u65b9\u6cd5\u5e76\u63a2\u8ba8\u5176\u8bbe\u8ba1\u6311\u6218\u3002", "conclusion": "\u8fd9\u4e9b\u6d4b\u8bd5\u65b9\u6cd5\u53ef\u663e\u8457\u63d0\u5347\u591a\u51c6\u5219\u51b3\u7b56\u65b9\u6cd5\u7684\u8bc4\u4f30\u80fd\u529b\u3002"}}
{"id": "2508.00097", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00097", "abs": "https://arxiv.org/abs/2508.00097", "authors": ["Zhigen Zhao", "Liuchuan Yu", "Ke Jing", "Ning Yang"], "title": "XRoboToolkit: A Cross-Platform Framework for Robot Teleoperation", "comment": "6 pages, 6 figures, project link: https://github.com/XR-Robotics", "summary": "The rapid advancement of Vision-Language-Action models has created an urgent\nneed for large-scale, high-quality robot demonstration datasets. Although\nteleoperation is the predominant method for data collection, current approaches\nsuffer from limited scalability, complex setup procedures, and suboptimal data\nquality. This paper presents XRoboToolkit, a cross-platform framework for\nextended reality based robot teleoperation built on the OpenXR standard. The\nsystem features low-latency stereoscopic visual feedback, optimization-based\ninverse kinematics, and support for diverse tracking modalities including head,\ncontroller, hand, and auxiliary motion trackers. XRoboToolkit's modular\narchitecture enables seamless integration across robotic platforms and\nsimulation environments, spanning precision manipulators, mobile robots, and\ndexterous hands. We demonstrate the framework's effectiveness through precision\nmanipulation tasks and validate data quality by training VLA models that\nexhibit robust autonomous performance.", "AI": {"tldr": "XRoboToolkit\u662f\u4e00\u4e2a\u57fa\u4e8eOpenXR\u6807\u51c6\u7684\u8de8\u5e73\u53f0\u6269\u5c55\u73b0\u5b9e\u673a\u5668\u4eba\u9065\u64cd\u4f5c\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6570\u636e\u6536\u96c6\u65b9\u6cd5\u7684\u53ef\u6269\u5c55\u6027\u548c\u8d28\u91cf\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u5bf9\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u673a\u5668\u4eba\u6f14\u793a\u6570\u636e\u7684\u9700\u6c42\u8feb\u5207\uff0c\u800c\u73b0\u6709\u9065\u64cd\u4f5c\u65b9\u6cd5\u5b58\u5728\u53ef\u6269\u5c55\u6027\u5dee\u3001\u8bbe\u7f6e\u590d\u6742\u548c\u6570\u636e\u8d28\u91cf\u4f4e\u7684\u95ee\u9898\u3002", "method": "XRoboToolkit\u63d0\u4f9b\u4f4e\u5ef6\u8fdf\u7acb\u4f53\u89c6\u89c9\u53cd\u9988\u3001\u57fa\u4e8e\u4f18\u5316\u7684\u9006\u8fd0\u52a8\u5b66\uff0c\u5e76\u652f\u6301\u591a\u79cd\u8ddf\u8e2a\u6a21\u5f0f\uff08\u5982\u5934\u90e8\u3001\u63a7\u5236\u5668\u3001\u624b\u90e8\u7b49\uff09\u3002\u5176\u6a21\u5757\u5316\u67b6\u6784\u652f\u6301\u8de8\u5e73\u53f0\u548c\u4eff\u771f\u73af\u5883\u96c6\u6210\u3002", "result": "\u901a\u8fc7\u7cbe\u786e\u64cd\u63a7\u4efb\u52a1\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u5e76\u8bad\u7ec3\u51fa\u5177\u6709\u9c81\u68d2\u81ea\u4e3b\u6027\u80fd\u7684VLA\u6a21\u578b\u3002", "conclusion": "XRoboToolkit\u4e3a\u9ad8\u8d28\u91cf\u673a\u5668\u4eba\u6570\u636e\u6536\u96c6\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.00137", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00137", "abs": "https://arxiv.org/abs/2508.00137", "authors": ["Shqiponja Ahmetaj", "George Konstantinidis", "Magdalena Ortiz", "Paolo Pareti", "Mantas Simkus"], "title": "SHACL Validation under Graph Updates (Extended Paper)", "comment": "Accepted at the International Semantic Web Conference (ISWC 2025)", "summary": "SHACL (SHApe Constraint Language) is a W3C standardized constraint language\nfor RDF graphs. In this paper, we study SHACL validation in RDF graphs under\nupdates. We present a SHACL-based update language that can capture intuitive\nand realistic modifications on RDF graphs and study the problem of static\nvalidation under such updates. This problem asks to verify whether every graph\nthat validates a SHACL specification will still do so after applying a given\nupdate sequence. More importantly, it provides a basis for further services for\nreasoning about evolving RDF graphs. Using a regression technique that embeds\nthe update actions into SHACL constraints, we show that static validation under\nupdates can be reduced to (un)satisfiability of constraints in (a minor\nextension of) SHACL. We analyze the computational complexity of the static\nvalidation problem for SHACL and some key fragments. Finally, we present a\nprototype implementation that performs static validation and other static\nanalysis tasks on SHACL constraints and demonstrate its behavior through\npreliminary experiments.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86RDF\u56fe\u4e2dSHACL\u9a8c\u8bc1\u5728\u66f4\u65b0\u4e0b\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eSHACL\u7684\u66f4\u65b0\u8bed\u8a00\uff0c\u5e76\u901a\u8fc7\u56de\u5f52\u6280\u672f\u5c06\u9759\u6001\u9a8c\u8bc1\u95ee\u9898\u8f6c\u5316\u4e3aSHACL\u7ea6\u675f\u7684\uff08\u4e0d\uff09\u53ef\u6ee1\u8db3\u6027\u95ee\u9898\u3002", "motivation": "\u7814\u7a76RDF\u56fe\u5728\u66f4\u65b0\u540e\u4ecd\u6ee1\u8db3SHACL\u89c4\u8303\u7684\u9759\u6001\u9a8c\u8bc1\u95ee\u9898\uff0c\u4e3a\u6f14\u5316RDF\u56fe\u7684\u63a8\u7406\u670d\u52a1\u63d0\u4f9b\u57fa\u7840\u3002", "method": "\u63d0\u51faSHACL\u66f4\u65b0\u8bed\u8a00\uff0c\u4f7f\u7528\u56de\u5f52\u6280\u672f\u5c06\u66f4\u65b0\u64cd\u4f5c\u5d4c\u5165SHACL\u7ea6\u675f\uff0c\u5206\u6790\u8ba1\u7b97\u590d\u6742\u6027\uff0c\u5e76\u5b9e\u73b0\u539f\u578b\u7cfb\u7edf\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u9759\u6001\u9a8c\u8bc1\u95ee\u9898\u53ef\u8f6c\u5316\u4e3aSHACL\u7ea6\u675f\u7684\uff08\u4e0d\uff09\u53ef\u6ee1\u8db3\u6027\u95ee\u9898\uff0c\u5206\u6790\u4e86\u8ba1\u7b97\u590d\u6742\u6027\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u539f\u578b\u7cfb\u7edf\u7684\u884c\u4e3a\u3002", "conclusion": "\u672c\u6587\u4e3aRDF\u56fe\u7684\u52a8\u6001\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u7528\u5de5\u5177\uff0c\u6269\u5c55\u4e86SHACL\u7684\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2508.00162", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.00162", "abs": "https://arxiv.org/abs/2508.00162", "authors": ["Noboru Myers", "Obin Kwon", "Sankalp Yamsani", "Joohyung Kim"], "title": "CHILD (Controller for Humanoid Imitation and Live Demonstration): a Whole-Body Humanoid Teleoperation System", "comment": null, "summary": "Recent advances in teleoperation have demonstrated robots performing complex\nmanipulation tasks. However, existing works rarely support whole-body\njoint-level teleoperation for humanoid robots, limiting the diversity of tasks\nthat can be accomplished. This work presents Controller for Humanoid Imitation\nand Live Demonstration (CHILD), a compact reconfigurable teleoperation system\nthat enables joint level control over humanoid robots. CHILD fits within a\nstandard baby carrier, allowing the operator control over all four limbs, and\nsupports both direct joint mapping for full-body control and loco-manipulation.\nAdaptive force feedback is incorporated to enhance operator experience and\nprevent unsafe joint movements. We validate the capabilities of this system by\nconducting loco-manipulation and full-body control examples on a humanoid robot\nand multiple dual-arm systems. Lastly, we open-source the design of the\nhardware promoting accessibility and reproducibility. Additional details and\nopen-source information are available at our project website:\nhttps://uiuckimlab.github.io/CHILD-pages.", "AI": {"tldr": "CHILD\u662f\u4e00\u79cd\u7d27\u51d1\u53ef\u91cd\u6784\u7684\u9065\u64cd\u4f5c\u7cfb\u7edf\uff0c\u652f\u6301\u4eba\u5f62\u673a\u5668\u4eba\u7684\u5173\u8282\u7ea7\u63a7\u5236\uff0c\u9002\u7528\u4e8e\u5168\u8eab\u63a7\u5236\u548c\u79fb\u52a8\u64cd\u4f5c\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5f88\u5c11\u652f\u6301\u4eba\u5f62\u673a\u5668\u4eba\u7684\u5168\u8eab\u5173\u8282\u7ea7\u9065\u64cd\u4f5c\uff0c\u9650\u5236\u4e86\u4efb\u52a1\u7684\u591a\u6837\u6027\u3002", "method": "\u5f00\u53d1\u4e86CHILD\u7cfb\u7edf\uff0c\u652f\u6301\u76f4\u63a5\u5173\u8282\u6620\u5c04\u548c\u79fb\u52a8\u64cd\u4f5c\uff0c\u5e76\u52a0\u5165\u81ea\u9002\u5e94\u529b\u53cd\u9988\u4ee5\u63d0\u5347\u64cd\u4f5c\u4f53\u9a8c\u548c\u5b89\u5168\u6027\u3002", "result": "\u5728\u4e00\u4e2a\u4eba\u5f62\u673a\u5668\u4eba\u548c\u591a\u4e2a\u53cc\u81c2\u7cfb\u7edf\u4e0a\u9a8c\u8bc1\u4e86\u79fb\u52a8\u64cd\u4f5c\u548c\u5168\u8eab\u63a7\u5236\u7684\u80fd\u529b\u3002", "conclusion": "CHILD\u7cfb\u7edf\u901a\u8fc7\u5f00\u6e90\u8bbe\u8ba1\u63d0\u5347\u4e86\u53ef\u8bbf\u95ee\u6027\u548c\u53ef\u91cd\u590d\u6027\u3002"}}
{"id": "2508.00138", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00138", "abs": "https://arxiv.org/abs/2508.00138", "authors": ["Rashid Mushkani", "Hugo Berard", "Toumadher Ammar", "Cassandre Chatonnier", "Shin Koseki"], "title": "Co-Producing AI: Toward an Augmented, Participatory Lifecycle", "comment": "Eighth AAAI/ACM Conference on AI, Ethics, and Society 2025", "summary": "Despite efforts to mitigate the inherent risks and biases of artificial\nintelligence (AI) algorithms, these algorithms can disproportionately impact\nculturally marginalized groups. A range of approaches has been proposed to\naddress or reduce these risks, including the development of ethical guidelines\nand principles for responsible AI, as well as technical solutions that promote\nalgorithmic fairness. Drawing on design justice, expansive learning theory, and\nrecent empirical work on participatory AI, we argue that mitigating these harms\nrequires a fundamental re-architecture of the AI production pipeline. This\nre-design should center co-production, diversity, equity, inclusion (DEI), and\nmultidisciplinary collaboration. We introduce an augmented AI lifecycle\nconsisting of five interconnected phases: co-framing, co-design,\nco-implementation, co-deployment, and co-maintenance. The lifecycle is informed\nby four multidisciplinary workshops and grounded in themes of distributed\nauthority and iterative knowledge exchange. Finally, we relate the proposed\nlifecycle to several leading ethical frameworks and outline key research\nquestions that remain for scaling participatory governance.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bbe\u8ba1\u6b63\u4e49\u3001\u6269\u5c55\u5b66\u4e60\u7406\u8bba\u548c\u53c2\u4e0e\u5f0fAI\u7684AI\u751f\u547d\u5468\u671f\u91cd\u6784\u65b9\u6cd5\uff0c\u5f3a\u8c03\u5171\u751f\u4ea7\u3001\u591a\u6837\u6027\u3001\u516c\u5e73\u6027\u548c\u591a\u5b66\u79d1\u5408\u4f5c\u3002", "motivation": "AI\u7b97\u6cd5\u5bf9\u6587\u5316\u8fb9\u7f18\u7fa4\u4f53\u7684\u4e0d\u516c\u5e73\u5f71\u54cd\u9700\u8981\u6839\u672c\u6027\u89e3\u51b3\u65b9\u6848\uff0c\u73b0\u6709\u65b9\u6cd5\u5982\u4f26\u7406\u6307\u5357\u548c\u6280\u672f\u516c\u5e73\u6027\u6539\u8fdb\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u5305\u542b\u4e94\u4e2a\u9636\u6bb5\u7684\u589e\u5f3aAI\u751f\u547d\u5468\u671f\uff08\u5171\u6846\u67b6\u3001\u5171\u8bbe\u8ba1\u3001\u5171\u5b9e\u65bd\u3001\u5171\u90e8\u7f72\u3001\u5171\u7ef4\u62a4\uff09\uff0c\u57fa\u4e8e\u591a\u5b66\u79d1\u7814\u8ba8\u4f1a\u548c\u5206\u5e03\u5f0f\u6743\u5a01\u7406\u5ff5\u3002", "result": "\u8be5\u65b9\u6cd5\u4e0e\u4e3b\u6d41\u4f26\u7406\u6846\u67b6\u5173\u8054\uff0c\u5e76\u63d0\u51fa\u4e86\u6269\u5c55\u53c2\u4e0e\u5f0f\u6cbb\u7406\u7684\u5173\u952e\u7814\u7a76\u95ee\u9898\u3002", "conclusion": "\u91cd\u6784AI\u751f\u4ea7\u6d41\u7a0b\u4ee5\u5171\u751f\u4ea7\u4e3a\u6838\u5fc3\uff0c\u662f\u51cf\u5c11AI\u5bf9\u8fb9\u7f18\u7fa4\u4f53\u4f24\u5bb3\u7684\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2508.00258", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.00258", "abs": "https://arxiv.org/abs/2508.00258", "authors": ["Zhiwei Wu", "Siyi Wei", "Jiahao Luo", "Jinhui Zhang"], "title": "Topology-Inspired Morphological Descriptor for Soft Continuum Robots", "comment": null, "summary": "This paper presents a topology-inspired morphological descriptor for soft\ncontinuum robots by combining a pseudo-rigid-body (PRB) model with Morse theory\nto achieve a quantitative characterization of robot morphologies. By counting\ncritical points of directional projections, the proposed descriptor enables a\ndiscrete representation of multimodal configurations and facilitates\nmorphological classification. Furthermore, we apply the descriptor to\nmorphology control by formulating the target configuration as an optimization\nproblem to compute actuation parameters that generate equilibrium shapes with\ndesired topological features. The proposed framework provides a unified\nmethodology for quantitative morphology description, classification, and\ncontrol of soft continuum robots, with the potential to enhance their precision\nand adaptability in medical applications such as minimally invasive surgery and\nendovascular interventions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u62d3\u6251\u7684\u5f62\u6001\u63cf\u8ff0\u7b26\uff0c\u7ed3\u5408\u4f2a\u521a\u4f53\u6a21\u578b\u548c\u83ab\u5c14\u65af\u7406\u8bba\uff0c\u7528\u4e8e\u8f6f\u8fde\u7eed\u673a\u5668\u4eba\u7684\u5f62\u6001\u5b9a\u91cf\u8868\u5f81\u548c\u5206\u7c7b\u3002", "motivation": "\u63d0\u9ad8\u8f6f\u8fde\u7eed\u673a\u5668\u4eba\u5728\u533b\u7597\u5e94\u7528\uff08\u5982\u5fae\u521b\u624b\u672f\u548c\u8840\u7ba1\u5185\u4ecb\u5165\uff09\u4e2d\u7684\u7cbe\u5ea6\u548c\u9002\u5e94\u6027\u3002", "method": "\u7ed3\u5408\u4f2a\u521a\u4f53\u6a21\u578b\u548c\u83ab\u5c14\u65af\u7406\u8bba\uff0c\u901a\u8fc7\u8ba1\u6570\u65b9\u5411\u6295\u5f71\u7684\u4e34\u754c\u70b9\u5b9e\u73b0\u5f62\u6001\u79bb\u6563\u8868\u793a\uff0c\u5e76\u901a\u8fc7\u4f18\u5316\u95ee\u9898\u8ba1\u7b97\u9a71\u52a8\u53c2\u6570\u4ee5\u5b9e\u73b0\u76ee\u6807\u5f62\u6001\u3002", "result": "\u5b9e\u73b0\u4e86\u8f6f\u8fde\u7eed\u673a\u5668\u4eba\u5f62\u6001\u7684\u5b9a\u91cf\u63cf\u8ff0\u3001\u5206\u7c7b\u548c\u63a7\u5236\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8f6f\u8fde\u7eed\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u5f62\u6001\u63cf\u8ff0\u548c\u63a7\u5236\u65b9\u6cd5\uff0c\u5177\u6709\u6f5c\u5728\u7684\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2508.00143", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.00143", "abs": "https://arxiv.org/abs/2508.00143", "authors": ["Danielle R. Thomas", "Conrad Borchers", "Kenneth R. Koedinger"], "title": "Beyond Agreement: Rethinking Ground Truth in Educational AI Annotation", "comment": "Accepted for presentation at NCME AIME-Con 2025", "summary": "Humans can be notoriously imperfect evaluators. They are often biased,\nunreliable, and unfit to define \"ground truth.\" Yet, given the surging need to\nproduce large amounts of training data in educational applications using AI,\ntraditional inter-rater reliability (IRR) metrics like Cohen's kappa remain\ncentral to validating labeled data. IRR remains a cornerstone of many machine\nlearning pipelines for educational data. Take, for example, the classification\nof tutors' moves in dialogues or labeling open responses in machine-graded\nassessments. This position paper argues that overreliance on human IRR as a\ngatekeeper for annotation quality hampers progress in classifying data in ways\nthat are valid and predictive in relation to improving learning. To address\nthis issue, we highlight five examples of complementary evaluation methods,\nsuch as multi-label annotation schemes, expert-based approaches, and\nclose-the-loop validity. We argue that these approaches are in a better\nposition to produce training data and subsequent models that produce improved\nstudent learning and more actionable insights than IRR approaches alone. We\nalso emphasize the importance of external validity, for example, by\nestablishing a procedure of validating tutor moves and demonstrating that it\nworks across many categories of tutor actions (e.g., providing hints). We call\non the field to rethink annotation quality and ground truth--prioritizing\nvalidity and educational impact over consensus alone.", "AI": {"tldr": "\u8bba\u6587\u6307\u51fa\u4f20\u7edf\u7684\u4eba\u7c7b\u8bc4\u5206\u8005\u95f4\u4e00\u81f4\u6027\uff08IRR\uff09\u5728AI\u6559\u80b2\u5e94\u7528\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u4e94\u79cd\u8865\u5145\u65b9\u6cd5\u4ee5\u63d0\u9ad8\u6570\u636e\u6807\u6ce8\u8d28\u91cf\u548c\u6a21\u578b\u6548\u679c\u3002", "motivation": "\u4eba\u7c7b\u8bc4\u5206\u8005\u5b58\u5728\u504f\u89c1\u548c\u4e0d\u53ef\u9760\u6027\uff0c\u4f20\u7edfIRR\u6307\u6807\u65e0\u6cd5\u5b8c\u5168\u4fdd\u8bc1\u6807\u6ce8\u6570\u636e\u7684\u6709\u6548\u6027\uff0c\u963b\u788d\u4e86\u6559\u80b2\u6570\u636e\u7684\u5206\u7c7b\u8fdb\u5c55\u3002", "method": "\u63d0\u51fa\u4e94\u79cd\u8865\u5145\u65b9\u6cd5\uff1a\u591a\u6807\u7b7e\u6807\u6ce8\u65b9\u6848\u3001\u4e13\u5bb6\u8bc4\u4f30\u3001\u95ed\u73af\u9a8c\u8bc1\u7b49\uff0c\u5f3a\u8c03\u5916\u90e8\u6548\u5ea6\u7684\u91cd\u8981\u6027\u3002", "result": "\u8fd9\u4e9b\u65b9\u6cd5\u80fd\u6bd4\u5355\u72ec\u4f7f\u7528IRR\u4ea7\u751f\u66f4\u6709\u6548\u7684\u8bad\u7ec3\u6570\u636e\u548c\u6a21\u578b\uff0c\u63d0\u5347\u5b66\u751f\u5b66\u4e60\u6548\u679c\u548c\u53ef\u64cd\u4f5c\u6027\u3002", "conclusion": "\u547c\u5401\u91cd\u65b0\u601d\u8003\u6807\u6ce8\u8d28\u91cf\u548c\u201c\u771f\u5b9e\u6807\u51c6\u201d\uff0c\u4f18\u5148\u8003\u8651\u6548\u5ea6\u548c\u6559\u80b2\u5f71\u54cd\uff0c\u800c\u975e\u4ec5\u4f9d\u8d56\u5171\u8bc6\u3002"}}
{"id": "2508.00288", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00288", "abs": "https://arxiv.org/abs/2508.00288", "authors": ["Jianqiang Xiao", "Yuexuan Sun", "Yixin Shao", "Boxi Gan", "Rongqiang Liu", "Yanjing Wu", "Weili Gua", "Xiang Deng"], "title": "UAV-ON: A Benchmark for Open-World Object Goal Navigation with Aerial Agents", "comment": "Accepted to ACM MM Dataset Track 2025", "summary": "Aerial navigation is a fundamental yet underexplored capability in embodied\nintelligence, enabling agents to operate in large-scale, unstructured\nenvironments where traditional navigation paradigms fall short. However, most\nexisting research follows the Vision-and-Language Navigation (VLN) paradigm,\nwhich heavily depends on sequential linguistic instructions, limiting its\nscalability and autonomy. To address this gap, we introduce UAV-ON, a benchmark\nfor large-scale Object Goal Navigation (ObjectNav) by aerial agents in\nopen-world environments, where agents operate based on high-level semantic\ngoals without relying on detailed instructional guidance as in VLN. UAV-ON\ncomprises 14 high-fidelity Unreal Engine environments with diverse semantic\nregions and complex spatial layouts, covering urban, natural, and mixed-use\nsettings. It defines 1270 annotated target objects, each characterized by an\ninstance-level instruction that encodes category, physical footprint, and\nvisual descriptors, allowing grounded reasoning. These instructions serve as\nsemantic goals, introducing realistic ambiguity and complex reasoning\nchallenges for aerial agents. To evaluate the benchmark, we implement several\nbaseline methods, including Aerial ObjectNav Agent (AOA), a modular policy that\nintegrates instruction semantics with egocentric observations for long-horizon,\ngoal-directed exploration. Empirical results show that all baselines struggle\nin this setting, highlighting the compounded challenges of aerial navigation\nand semantic goal grounding. UAV-ON aims to advance research on scalable UAV\nautonomy driven by semantic goal descriptions in complex real-world\nenvironments.", "AI": {"tldr": "UAV-ON\u662f\u4e00\u4e2a\u7528\u4e8e\u7a7a\u4e2d\u667a\u80fd\u4f53\u5728\u5f00\u653e\u73af\u5883\u4e2d\u8fdb\u884c\u5927\u89c4\u6a21\u76ee\u6807\u5bfc\u822a\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u65e8\u5728\u89e3\u51b3\u4f20\u7edf\u89c6\u89c9\u4e0e\u8bed\u8a00\u5bfc\u822a\uff08VLN\uff09\u4f9d\u8d56\u8be6\u7ec6\u6307\u4ee4\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u4f9d\u8d56\u987a\u5e8f\u8bed\u8a00\u6307\u4ee4\uff08\u5982VLN\uff09\uff0c\u9650\u5236\u4e86\u7a7a\u4e2d\u5bfc\u822a\u7684\u53ef\u6269\u5c55\u6027\u548c\u81ea\u4e3b\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u57fa\u4e8e\u9ad8\u7ea7\u8bed\u4e49\u76ee\u6807\u7684\u5bfc\u822a\u65b9\u6cd5\u3002", "method": "UAV-ON\u5305\u542b14\u4e2a\u9ad8\u4fdd\u771fUnreal Engine\u73af\u5883\uff0c\u5b9a\u4e49\u4e861270\u4e2a\u6807\u6ce8\u76ee\u6807\u5bf9\u8c61\uff0c\u5e76\u901a\u8fc7\u5b9e\u4f8b\u7ea7\u6307\u4ee4\u7f16\u7801\u8bed\u4e49\u76ee\u6807\u3002\u63d0\u51fa\u4e86Aerial ObjectNav Agent\uff08AOA\uff09\u4f5c\u4e3a\u57fa\u7ebf\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\u6240\u6709\u57fa\u7ebf\u65b9\u6cd5\u5728\u6b64\u8bbe\u7f6e\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u7a81\u663e\u4e86\u7a7a\u4e2d\u5bfc\u822a\u4e0e\u8bed\u4e49\u76ee\u6807\u7ed3\u5408\u7684\u590d\u6742\u6027\u3002", "conclusion": "UAV-ON\u65e8\u5728\u63a8\u52a8\u57fa\u4e8e\u8bed\u4e49\u76ee\u6807\u7684\u65e0\u4eba\u673a\u81ea\u4e3b\u6027\u7814\u7a76\uff0c\u9002\u7528\u4e8e\u590d\u6742\u73b0\u5b9e\u73af\u5883\u3002"}}
{"id": "2508.00159", "categories": ["cs.AI", "cs.CY", "cs.LG", "econ.TH", "math.OC", "68Txx", "I.2"], "pdf": "https://arxiv.org/pdf/2508.00159", "abs": "https://arxiv.org/abs/2508.00159", "authors": ["Jobst Heitzig", "Ram Potham"], "title": "Model-Based Soft Maximization of Suitable Metrics of Long-Term Human Power", "comment": null, "summary": "Power is a key concept in AI safety: power-seeking as an instrumental goal,\nsudden or gradual disempowerment of humans, power balance in human-AI\ninteraction and international AI governance. At the same time, power as the\nability to pursue diverse goals is essential for wellbeing.\n  This paper explores the idea of promoting both safety and wellbeing by\nforcing AI agents explicitly to empower humans and to manage the power balance\nbetween humans and AI agents in a desirable way. Using a principled, partially\naxiomatic approach, we design a parametrizable and decomposable objective\nfunction that represents an inequality- and risk-averse long-term aggregate of\nhuman power. It takes into account humans' bounded rationality and social\nnorms, and, crucially, considers a wide variety of possible human goals.\n  We derive algorithms for computing that metric by backward induction or\napproximating it via a form of multi-agent reinforcement learning from a given\nworld model. We exemplify the consequences of (softly) maximizing this metric\nin a variety of paradigmatic situations and describe what instrumental\nsub-goals it will likely imply. Our cautious assessment is that softly\nmaximizing suitable aggregate metrics of human power might constitute a\nbeneficial objective for agentic AI systems that is safer than direct\nutility-based objectives.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u901a\u8fc7\u5f3a\u5236AI\u4ee3\u7406\u660e\u786e\u8d4b\u80fd\u4eba\u7c7b\u5e76\u7ba1\u7406\u4eba\u7c7b\u4e0eAI\u4e4b\u95f4\u7684\u6743\u529b\u5e73\u8861\uff0c\u4ee5\u4fc3\u8fdb\u5b89\u5168\u548c\u798f\u7949\u3002\u63d0\u51fa\u4e00\u79cd\u53c2\u6570\u5316\u548c\u53ef\u5206\u89e3\u7684\u76ee\u6807\u51fd\u6570\uff0c\u8003\u8651\u4eba\u7c7b\u6709\u9650\u7406\u6027\u548c\u793e\u4f1a\u89c4\u8303\uff0c\u5e76\u901a\u8fc7\u7b97\u6cd5\u8ba1\u7b97\u8be5\u6307\u6807\u3002", "motivation": "\u7814\u7a76AI\u5b89\u5168\u4e2d\u7684\u6743\u529b\u6982\u5ff5\uff0c\u63a2\u8ba8\u5982\u4f55\u901a\u8fc7\u7ba1\u7406\u4eba\u7c7b\u4e0eAI\u7684\u6743\u529b\u5e73\u8861\uff0c\u540c\u65f6\u63d0\u5347\u4eba\u7c7b\u798f\u7949\u3002", "method": "\u91c7\u7528\u90e8\u5206\u516c\u7406\u5316\u65b9\u6cd5\u8bbe\u8ba1\u76ee\u6807\u51fd\u6570\uff0c\u5e76\u901a\u8fc7\u9006\u5411\u5f52\u7eb3\u6216\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u8ba1\u7b97\u8be5\u6307\u6807\u3002", "result": "\u5728\u591a\u79cd\u5178\u578b\u60c5\u5883\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u6307\u6807\u7684\u6548\u679c\uff0c\u8868\u660e\u5176\u53ef\u80fd\u6bd4\u76f4\u63a5\u57fa\u4e8e\u6548\u7528\u7684\u76ee\u6807\u66f4\u5b89\u5168\u3002", "conclusion": "\u8f6f\u6700\u5927\u5316\u9002\u5408\u7684\u4eba\u7c7b\u6743\u529b\u805a\u5408\u6307\u6807\u53ef\u80fd\u662fAI\u7cfb\u7edf\u7684\u6709\u76ca\u76ee\u6807\uff0c\u6bd4\u76f4\u63a5\u6548\u7528\u76ee\u6807\u66f4\u5b89\u5168\u3002"}}
{"id": "2508.00303", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.00303", "abs": "https://arxiv.org/abs/2508.00303", "authors": ["Zehui Xu", "Junhui Wang", "Yongliang Shi", "Chao Gao", "Guyue Zhou"], "title": "TopoDiffuser: A Diffusion-Based Multimodal Trajectory Prediction Model with Topometric Maps", "comment": null, "summary": "This paper introduces TopoDiffuser, a diffusion-based framework for\nmultimodal trajectory prediction that incorporates topometric maps to generate\naccurate, diverse, and road-compliant future motion forecasts. By embedding\nstructural cues from topometric maps into the denoising process of a\nconditional diffusion model, the proposed approach enables trajectory\ngeneration that naturally adheres to road geometry without relying on explicit\nconstraints. A multimodal conditioning encoder fuses LiDAR observations,\nhistorical motion, and route information into a unified bird's-eye-view (BEV)\nrepresentation. Extensive experiments on the KITTI benchmark demonstrate that\nTopoDiffuser outperforms state-of-the-art methods, while maintaining strong\ngeometric consistency. Ablation studies further validate the contribution of\neach input modality, as well as the impact of denoising steps and the number of\ntrajectory samples. To support future research, we publicly release our code at\nhttps://github.com/EI-Nav/TopoDiffuser.", "AI": {"tldr": "TopoDiffuser\u662f\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u591a\u6a21\u6001\u8f68\u8ff9\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u62d3\u6251\u5730\u56fe\u751f\u6210\u51c6\u786e\u3001\u591a\u6837\u4e14\u7b26\u5408\u9053\u8def\u89c4\u5219\u7684\u672a\u6765\u8fd0\u52a8\u9884\u6d4b\u3002", "motivation": "\u4f20\u7edf\u8f68\u8ff9\u9884\u6d4b\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u663e\u5f0f\u7ea6\u675f\u6765\u4fdd\u8bc1\u9053\u8def\u51e0\u4f55\u4e00\u81f4\u6027\uff0c\u800cTopoDiffuser\u901a\u8fc7\u5d4c\u5165\u62d3\u6251\u5730\u56fe\u7684\u7ed3\u6784\u4fe1\u606f\uff0c\u65e0\u9700\u663e\u5f0f\u7ea6\u675f\u5373\u53ef\u5b9e\u73b0\u81ea\u7136\u7b26\u5408\u9053\u8def\u51e0\u4f55\u7684\u8f68\u8ff9\u751f\u6210\u3002", "method": "\u91c7\u7528\u6761\u4ef6\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u6761\u4ef6\u7f16\u7801\u5668\u878d\u5408LiDAR\u89c2\u6d4b\u3001\u5386\u53f2\u8fd0\u52a8\u548c\u8def\u5f84\u4fe1\u606f\u4e3a\u7edf\u4e00\u7684\u9e1f\u77b0\u56fe\u8868\u793a\uff0c\u5d4c\u5165\u62d3\u6251\u5730\u56fe\u7684\u7ed3\u6784\u4fe1\u606f\u5230\u53bb\u566a\u8fc7\u7a0b\u4e2d\u3002", "result": "\u5728KITTI\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTopoDiffuser\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u5f3a\u51e0\u4f55\u4e00\u81f4\u6027\u3002\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5404\u8f93\u5165\u6a21\u6001\u7684\u8d21\u732e\u4ee5\u53ca\u53bb\u566a\u6b65\u9aa4\u548c\u8f68\u8ff9\u6837\u672c\u6570\u91cf\u7684\u5f71\u54cd\u3002", "conclusion": "TopoDiffuser\u901a\u8fc7\u7ed3\u5408\u62d3\u6251\u5730\u56fe\u548c\u6269\u6563\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u7b26\u5408\u9053\u8def\u89c4\u5219\u7684\u8f68\u8ff9\u9884\u6d4b\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u5f00\u6e90\u4ee3\u7801\u652f\u6301\u3002"}}
{"id": "2508.00222", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00222", "abs": "https://arxiv.org/abs/2508.00222", "authors": ["Yihong Dong", "Xue Jiang", "Yongding Tao", "Huanyu Liu", "Kechi Zhang", "Lili Mou", "Rongyu Cao", "Yingwei Ma", "Jue Chen", "Binhua Li", "Zhi Jin", "Fei Huang", "Yongbin Li", "Ge Li"], "title": "RL-PLUS: Countering Capability Boundary Collapse of LLMs in Reinforcement Learning with Hybrid-policy Optimization", "comment": null, "summary": "Reinforcement Learning with Verifiable Reward (RLVR) has significantly\nadvanced the complex reasoning abilities of Large Language Models (LLMs).\nHowever, it struggles to break through the inherent capability boundaries of\nthe base LLM, due to its inherently on-policy strategy with LLM's immense\naction space and sparse reward. Further, RLVR can lead to the capability\nboundary collapse, narrowing the LLM's problem-solving scope. To address this\nproblem, we propose RL-PLUS, a novel approach that synergizes internal\nexploitation (i.e., Thinking) with external data (i.e., Learning) to achieve\nstronger reasoning capabilities and surpass the boundaries of base models.\nRL-PLUS integrates two core components: Multiple Importance Sampling to address\nfor distributional mismatch from external data, and an Exploration-Based\nAdvantage Function to guide the model towards high-value, unexplored reasoning\npaths. We provide both theoretical analysis and extensive experiments to\ndemonstrate the superiority and generalizability of our approach. The results\nshow that RL-PLUS achieves state-of-the-art performance compared with existing\nRLVR methods on six math reasoning benchmarks and exhibits superior performance\non six out-of-distribution reasoning tasks. It also achieves consistent and\nsignificant gains across diverse model families, with average relative\nimprovements ranging from 21.1\\% to 69.2\\%. Moreover, Pass@k curves across\nmultiple benchmarks indicate that RL-PLUS effectively resolves the capability\nboundary collapse problem.", "AI": {"tldr": "RL-PLUS\u901a\u8fc7\u7ed3\u5408\u5185\u90e8\u63a2\u7d22\u4e0e\u5916\u90e8\u6570\u636e\uff0c\u89e3\u51b3\u4e86RLVR\u65b9\u6cd5\u5728LLM\u80fd\u529b\u8fb9\u754c\u548c\u7a00\u758f\u5956\u52b1\u4e0a\u7684\u9650\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u80fd\u529b\u3002", "motivation": "RLVR\u65b9\u6cd5\u56e0\u56fa\u6709\u7684\u7b56\u7565\u548c\u7a00\u758f\u5956\u52b1\u95ee\u9898\uff0c\u96be\u4ee5\u7a81\u7834\u57fa\u7840LLM\u7684\u80fd\u529b\u8fb9\u754c\uff0c\u751a\u81f3\u5bfc\u81f4\u80fd\u529b\u8fb9\u754c\u5d29\u6e83\u3002", "method": "RL-PLUS\u7ed3\u5408\u591a\u91cd\u91cd\u8981\u6027\u91c7\u6837\u548c\u57fa\u4e8e\u63a2\u7d22\u7684\u4f18\u52bf\u51fd\u6570\uff0c\u6574\u5408\u5185\u90e8\u601d\u8003\u4e0e\u5916\u90e8\u5b66\u4e60\u3002", "result": "\u5728\u516d\u4e2a\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u548c\u516d\u4e2a\u5206\u5e03\u5916\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5e73\u5747\u76f8\u5bf9\u63d0\u534721.1%\u81f369.2%\u3002", "conclusion": "RL-PLUS\u6709\u6548\u89e3\u51b3\u4e86\u80fd\u529b\u8fb9\u754c\u5d29\u6e83\u95ee\u9898\uff0c\u5e76\u5728\u591a\u79cd\u6a21\u578b\u5bb6\u65cf\u4e2d\u8868\u73b0\u4e00\u81f4\u4f18\u8d8a\u3002"}}
{"id": "2508.00354", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00354", "abs": "https://arxiv.org/abs/2508.00354", "authors": ["Tianshuang Qiu", "Zehan Ma", "Karim El-Refai", "Hiya Shah", "Chung Min Kim", "Justin Kerr", "Ken Goldberg"], "title": "Omni-Scan: Creating Visually-Accurate Digital Twin Object Models Using a Bimanual Robot with Handover and Gaussian Splat Merging", "comment": null, "summary": "3D Gaussian Splats (3DGSs) are 3D object models derived from multi-view\nimages. Such \"digital twins\" are useful for simulations, virtual reality,\nmarketing, robot policy fine-tuning, and part inspection. 3D object scanning\nusually requires multi-camera arrays, precise laser scanners, or robot\nwrist-mounted cameras, which have restricted workspaces. We propose Omni-Scan,\na pipeline for producing high-quality 3D Gaussian Splat models using a\nbi-manual robot that grasps an object with one gripper and rotates the object\nwith respect to a stationary camera. The object is then re-grasped by a second\ngripper to expose surfaces that were occluded by the first gripper. We present\nthe Omni-Scan robot pipeline using DepthAny-thing, Segment Anything, as well as\nRAFT optical flow models to identify and isolate objects held by a robot\ngripper while removing the gripper and the background. We then modify the 3DGS\ntraining pipeline to support concatenated datasets with gripper occlusion,\nproducing an omni-directional (360 degree view) model of the object. We apply\nOmni-Scan to part defect inspection, finding that it can identify visual or\ngeometric defects in 12 different industrial and household objects with an\naverage accuracy of 83%. Interactive videos of Omni-Scan 3DGS models can be\nfound at https://berkeleyautomation.github.io/omni-scan/", "AI": {"tldr": "Omni-Scan\u662f\u4e00\u79cd\u5229\u7528\u53cc\u673a\u68b0\u81c2\u673a\u5668\u4eba\u6293\u53d6\u548c\u65cb\u8f6c\u7269\u4f53\u4ee5\u751f\u6210\u9ad8\u8d28\u91cf3D\u9ad8\u65af\u6e85\u5c04\u6a21\u578b\u7684\u6d41\u7a0b\uff0c\u652f\u6301360\u5ea6\u89c6\u89d2\u5efa\u6a21\uff0c\u5e76\u5728\u96f6\u4ef6\u7f3a\u9677\u68c0\u6d4b\u4e2d\u8fbe\u523083%\u7684\u5e73\u5747\u51c6\u786e\u7387\u3002", "motivation": "\u4f20\u7edf\u76843D\u7269\u4f53\u626b\u63cf\u65b9\u6cd5\u53d7\u9650\u4e8e\u8bbe\u5907\u7684\u5de5\u4f5c\u7a7a\u95f4\uff0cOmni-Scan\u65e8\u5728\u901a\u8fc7\u673a\u5668\u4eba\u64cd\u4f5c\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u5b9e\u73b0\u66f4\u7075\u6d3b\u7684\u7269\u4f53\u5efa\u6a21\u3002", "method": "\u4f7f\u7528\u53cc\u673a\u68b0\u81c2\u673a\u5668\u4eba\u6293\u53d6\u548c\u65cb\u8f6c\u7269\u4f53\uff0c\u7ed3\u5408DepthAnything\u3001Segment Anything\u548cRAFT\u5149\u6d41\u6a21\u578b\u53bb\u9664\u80cc\u666f\u548c\u5939\u6301\u5668\u906e\u6321\uff0c\u6539\u8fdb3DGS\u8bad\u7ec3\u6d41\u7a0b\u4ee5\u652f\u6301\u62fc\u63a5\u6570\u636e\u96c6\u3002", "result": "\u572812\u79cd\u5de5\u4e1a\u548c\u5bb6\u7528\u7269\u4f53\u7684\u7f3a\u9677\u68c0\u6d4b\u4e2d\uff0c\u5e73\u5747\u51c6\u786e\u7387\u8fbe\u523083%\u3002", "conclusion": "Omni-Scan\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u7075\u6d3b\u76843D\u7269\u4f53\u5efa\u6a21\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2508.00271", "categories": ["cs.AI", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.00271", "abs": "https://arxiv.org/abs/2508.00271", "authors": ["Hongjin Qian", "Zheng Liu"], "title": "MetaAgent: Toward Self-Evolving Agent via Tool Meta-Learning", "comment": "Technical Report, 14 pages", "summary": "In this work, we propose MetaAgent, an agentic paradigm inspired by the\nprinciple of learning-by-doing, where expertise is developed through hands-on\npractice and continual self-improvement. MetaAgent starts with a minimal\nworkflow, equipped only with basic reasoning and adaptive help-seeking\nabilities. When a knowledge gap is encountered, MetaAgent generates natural\nlanguage help requests, which are routed to the most suitable external tool by\na dedicated tool router. As MetaAgent solves tasks, it continually conducts\nself-reflection and answer verification, distilling actionable experience into\nconcise texts that are dynamically incorporated into future task contexts.\nBesides, MetaAgent autonomously builds in-house tools and a persistent\nknowledge base by organizing its tool-use history, further enhancing its\nability to retrieve and integrate relevant information We term this continual,\ndata-driven process as \\textit{meta tool learning}, through which MetaAgent\nincrementally refines its reasoning and tool-use strategies, without changing\nmodel parameters or requiring further post-training. Evaluated on challenging\nknowledge discovery benchmarks, including GAIA, WebWalkerQA, and BrowseCamp,\nMetaAgent consistently outperforms workflow-based baselines and matches or\nexceeds end-to-end trained agents, demonstrating the promise of self-evolving\nagentic systems for robust, general-purpose knowledge discovery. We provide our\nsource codes in https://github.com/qhjqhj00/MetaAgent.", "AI": {"tldr": "MetaAgent\u662f\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u5b9e\u8df5\u539f\u5219\u7684\u667a\u80fd\u4ee3\u7406\u8303\u5f0f\uff0c\u901a\u8fc7\u6301\u7eed\u81ea\u6211\u6539\u8fdb\u548c\u4efb\u52a1\u5b9e\u8df5\u63d0\u5347\u80fd\u529b\u3002\u5b83\u901a\u8fc7\u751f\u6210\u81ea\u7136\u8bed\u8a00\u8bf7\u6c42\u3001\u81ea\u6211\u53cd\u601d\u548c\u77e5\u8bc6\u5e93\u6784\u5efa\uff0c\u9010\u6b65\u4f18\u5316\u63a8\u7406\u548c\u5de5\u5177\u4f7f\u7528\u7b56\u7565\uff0c\u65e0\u9700\u8c03\u6574\u6a21\u578b\u53c2\u6570\u6216\u989d\u5916\u8bad\u7ec3\u3002", "motivation": "\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u901a\u8fc7\u5b9e\u8df5\u548c\u81ea\u6211\u6539\u8fdb\u6301\u7eed\u63d0\u5347\u80fd\u529b\u7684\u667a\u80fd\u4ee3\u7406\u7cfb\u7edf\uff0c\u89e3\u51b3\u77e5\u8bc6\u53d1\u73b0\u4efb\u52a1\u4e2d\u7684\u6311\u6218\u3002", "method": "MetaAgent\u4ece\u57fa\u7840\u5de5\u4f5c\u6d41\u51fa\u53d1\uff0c\u901a\u8fc7\u751f\u6210\u5e2e\u52a9\u8bf7\u6c42\u3001\u81ea\u6211\u53cd\u601d\u548c\u9a8c\u8bc1\u7b54\u6848\uff0c\u52a8\u6001\u6574\u5408\u7ecf\u9a8c\u5e76\u6784\u5efa\u5185\u90e8\u5de5\u5177\u548c\u77e5\u8bc6\u5e93\u3002", "result": "\u5728GAIA\u3001WebWalkerQA\u548cBrowseCamp\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMetaAgent\u8868\u73b0\u4f18\u4e8e\u57fa\u4e8e\u5de5\u4f5c\u6d41\u7684\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u5339\u914d\u6216\u8d85\u8d8a\u7aef\u5230\u7aef\u8bad\u7ec3\u7684\u4ee3\u7406\u3002", "conclusion": "MetaAgent\u5c55\u793a\u4e86\u81ea\u6211\u8fdb\u5316\u4ee3\u7406\u7cfb\u7edf\u5728\u901a\u7528\u77e5\u8bc6\u53d1\u73b0\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u667a\u80fd\u4ee3\u7406\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2508.00355", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.00355", "abs": "https://arxiv.org/abs/2508.00355", "authors": ["Zhenghan Chen", "Haocheng Xu", "Haodong Zhang", "Liang Zhang", "He Li", "Dongqi Wang", "Jiyu Yu", "Yifei Yang", "Zhongxiang Zhou", "Rong Xiong"], "title": "TOP: Time Optimization Policy for Stable and Accurate Standing Manipulation with Humanoid Robots", "comment": null, "summary": "Humanoid robots have the potential capability to perform a diverse range of\nmanipulation tasks, but this is based on a robust and precise standing\ncontroller. Existing methods are either ill-suited to precisely control\nhigh-dimensional upper-body joints, or difficult to ensure both robustness and\naccuracy, especially when upper-body motions are fast. This paper proposes a\nnovel time optimization policy (TOP), to train a standing manipulation control\nmodel that ensures balance, precision, and time efficiency simultaneously, with\nthe idea of adjusting the time trajectory of upper-body motions but not only\nstrengthening the disturbance resistance of the lower-body. Our approach\nconsists of three parts. Firstly, we utilize motion prior to represent\nupper-body motions to enhance the coordination ability between the upper and\nlower-body by training a variational autoencoder (VAE). Then we decouple the\nwhole-body control into an upper-body PD controller for precision and a\nlower-body RL controller to enhance robust stability. Finally, we train TOP\nmethod in conjunction with the decoupled controller and VAE to reduce the\nbalance burden resulting from fast upper-body motions that would destabilize\nthe robot and exceed the capabilities of the lower-body RL policy. The\neffectiveness of the proposed approach is evaluated via both simulation and\nreal world experiments, which demonstrate the superiority on standing\nmanipulation tasks stably and accurately. The project page can be found at\nhttps://anonymous.4open.science/w/top-258F/.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65f6\u95f4\u4f18\u5316\u7b56\u7565\uff08TOP\uff09\uff0c\u7528\u4e8e\u8bad\u7ec3\u4eba\u5f62\u673a\u5668\u4eba\u7684\u7ad9\u7acb\u64cd\u63a7\u63a7\u5236\u6a21\u578b\uff0c\u540c\u65f6\u786e\u4fdd\u5e73\u8861\u3001\u7cbe\u786e\u548c\u65f6\u95f4\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u540c\u65f6\u6ee1\u8db3\u9ad8\u7ef4\u4e0a\u534a\u8eab\u5173\u8282\u7684\u7cbe\u786e\u63a7\u5236\u548c\u6574\u4f53\u9c81\u68d2\u6027\uff0c\u5c24\u5176\u662f\u5728\u4e0a\u534a\u8eab\u52a8\u4f5c\u5feb\u901f\u65f6\u3002", "method": "\u7ed3\u5408\u8fd0\u52a8\u5148\u9a8c\uff08VAE\uff09\u3001\u89e3\u8026\u63a7\u5236\uff08\u4e0a\u534a\u8eabPD\u63a7\u5236\u5668\u548c\u4e0b\u534a\u8eabRL\u63a7\u5236\u5668\uff09\u53caTOP\u7b56\u7565\uff0c\u4f18\u5316\u4e0a\u534a\u8eab\u52a8\u4f5c\u65f6\u95f4\u8f68\u8ff9\u3002", "result": "\u4eff\u771f\u548c\u5b9e\u9645\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u7a33\u5b9a\u548c\u7cbe\u786e\u5b8c\u6210\u7ad9\u7acb\u64cd\u63a7\u4efb\u52a1\u4e0a\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "TOP\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5feb\u901f\u4e0a\u534a\u8eab\u52a8\u4f5c\u5bfc\u81f4\u7684\u5e73\u8861\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u7684\u64cd\u63a7\u80fd\u529b\u3002"}}
{"id": "2508.00282", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00282", "abs": "https://arxiv.org/abs/2508.00282", "authors": ["Yi-Long Lu", "Jiajun Song", "Chunhui Zhang", "Wei Wang"], "title": "Mind the Gap: The Divergence Between Human and LLM-Generated Tasks", "comment": null, "summary": "Humans constantly generate a diverse range of tasks guided by internal\nmotivations. While generative agents powered by large language models (LLMs)\naim to simulate this complex behavior, it remains uncertain whether they\noperate on similar cognitive principles. To address this, we conducted a\ntask-generation experiment comparing human responses with those of an LLM agent\n(GPT-4o). We find that human task generation is consistently influenced by\npsychological drivers, including personal values (e.g., Openness to Change) and\ncognitive style. Even when these psychological drivers are explicitly provided\nto the LLM, it fails to reflect the corresponding behavioral patterns. They\nproduce tasks that are markedly less social, less physical, and thematically\nbiased toward abstraction. Interestingly, while the LLM's tasks were perceived\nas more fun and novel, this highlights a disconnect between its linguistic\nproficiency and its capacity to generate human-like, embodied goals.We conclude\nthat there is a core gap between the value-driven, embodied nature of human\ncognition and the statistical patterns of LLMs, highlighting the necessity of\nincorporating intrinsic motivation and physical grounding into the design of\nmore human-aligned agents.", "AI": {"tldr": "\u8bba\u6587\u6bd4\u8f83\u4e86\u4eba\u7c7b\u548cLLM\uff08GPT-4o\uff09\u5728\u4efb\u52a1\u751f\u6210\u4e2d\u7684\u5dee\u5f02\uff0c\u53d1\u73b0\u4eba\u7c7b\u53d7\u5fc3\u7406\u9a71\u52a8\u56e0\u7d20\u5f71\u54cd\uff0c\u800cLLM\u5373\u4f7f\u63d0\u4f9b\u8fd9\u4e9b\u56e0\u7d20\u4e5f\u65e0\u6cd5\u6a21\u62df\u4eba\u7c7b\u884c\u4e3a\uff0c\u4efb\u52a1\u751f\u6210\u504f\u5411\u62bd\u8c61\u4e14\u7f3a\u4e4f\u793e\u4ea4\u548c\u7269\u7406\u6027\u3002", "motivation": "\u63a2\u8ba8LLM\u662f\u5426\u80fd\u6a21\u62df\u4eba\u7c7b\u57fa\u4e8e\u5fc3\u7406\u9a71\u52a8\u7684\u4efb\u52a1\u751f\u6210\u884c\u4e3a\u3002", "method": "\u901a\u8fc7\u4efb\u52a1\u751f\u6210\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u4eba\u7c7b\u548cGPT-4o\u7684\u884c\u4e3a\u6a21\u5f0f\u3002", "result": "\u4eba\u7c7b\u4efb\u52a1\u751f\u6210\u53d7\u4ef7\u503c\u89c2\u548c\u8ba4\u77e5\u98ce\u683c\u5f71\u54cd\uff0cLLM\u751f\u6210\u7684\u4efb\u52a1\u66f4\u62bd\u8c61\u3001\u7f3a\u4e4f\u793e\u4ea4\u548c\u7269\u7406\u6027\uff0c\u5c3d\u7ba1\u88ab\u8ba4\u4e3a\u66f4\u6709\u8da3\u548c\u65b0\u9896\u3002", "conclusion": "LLM\u4e0e\u4eba\u7c7b\u8ba4\u77e5\u5b58\u5728\u6838\u5fc3\u5dee\u8ddd\uff0c\u9700\u878d\u5165\u5185\u5728\u52a8\u673a\u548c\u7269\u7406\u57fa\u7840\u4ee5\u8bbe\u8ba1\u66f4\u8d34\u8fd1\u4eba\u7c7b\u7684\u667a\u80fd\u4f53\u3002"}}
{"id": "2508.00362", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.00362", "abs": "https://arxiv.org/abs/2508.00362", "authors": ["Zhenghan Chen", "Haodong Zhang", "Dongqi Wang", "Jiyu Yu", "Haocheng Xu", "Yue Wang", "Rong Xiong"], "title": "A Whole-Body Motion Imitation Framework from Human Data for Full-Size Humanoid Robot", "comment": null, "summary": "Motion imitation is a pivotal and effective approach for humanoid robots to\nachieve a more diverse range of complex and expressive movements, making their\nperformances more human-like. However, the significant differences in\nkinematics and dynamics between humanoid robots and humans present a major\nchallenge in accurately imitating motion while maintaining balance. In this\npaper, we propose a novel whole-body motion imitation framework for a full-size\nhumanoid robot. The proposed method employs contact-aware whole-body motion\nretargeting to mimic human motion and provide initial values for reference\ntrajectories, and the non-linear centroidal model predictive controller ensures\nthe motion accuracy while maintaining balance and overcoming external\ndisturbances in real time. The assistance of the whole-body controller allows\nfor more precise torque control. Experiments have been conducted to imitate a\nvariety of human motions both in simulation and in a real-world humanoid robot.\nThese experiments demonstrate the capability of performing with accuracy and\nadaptability, which validates the effectiveness of our approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5168\u8eab\u8fd0\u52a8\u6a21\u4eff\u6846\u67b6\uff0c\u7528\u4e8e\u5168\u5c3a\u5bf8\u4eba\u5f62\u673a\u5668\u4eba\uff0c\u7ed3\u5408\u63a5\u89e6\u611f\u77e5\u7684\u8fd0\u52a8\u91cd\u5b9a\u5411\u548c\u975e\u7ebf\u6027\u8d28\u5fc3\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u8fd0\u52a8\u6a21\u4eff\u4e0e\u5b9e\u65f6\u5e73\u8861\u3002", "motivation": "\u4eba\u5f62\u673a\u5668\u4eba\u6a21\u4eff\u4eba\u7c7b\u8fd0\u52a8\u65f6\uff0c\u56e0\u52a8\u529b\u5b66\u548c\u8fd0\u52a8\u5b66\u5dee\u5f02\u5927\uff0c\u96be\u4ee5\u540c\u65f6\u4fdd\u6301\u5e73\u8861\u548c\u8fd0\u52a8\u7cbe\u5ea6\u3002", "method": "\u91c7\u7528\u63a5\u89e6\u611f\u77e5\u7684\u5168\u8eab\u8fd0\u52a8\u91cd\u5b9a\u5411\u751f\u6210\u53c2\u8003\u8f68\u8ff9\uff0c\u7ed3\u5408\u975e\u7ebf\u6027\u8d28\u5fc3\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u5668\u5b9e\u73b0\u5b9e\u65f6\u5e73\u8861\u4e0e\u6297\u5e72\u6270\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u4e2d\u5747\u80fd\u9ad8\u7cbe\u5ea6\u6a21\u4eff\u591a\u79cd\u4eba\u7c7b\u8fd0\u52a8\uff0c\u5e76\u4fdd\u6301\u5e73\u8861\u3002", "conclusion": "\u6240\u63d0\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u8fd0\u52a8\u6a21\u4eff\u4e2d\u7684\u5e73\u8861\u4e0e\u7cbe\u5ea6\u95ee\u9898\u3002"}}
{"id": "2508.00323", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00323", "abs": "https://arxiv.org/abs/2508.00323", "authors": ["Jianyi Zhang", "Xu Ji", "Ziyin Zhou", "Yuchen Zhou", "Shubo Shi", "Haoyu Wu", "Zhen Li", "Shizhao Liu"], "title": "Oedipus and the Sphinx: Benchmarking and Improving Visual Language Models for Complex Graphic Reasoning", "comment": null, "summary": "Evaluating the performance of visual language models (VLMs) in graphic\nreasoning tasks has become an important research topic. However, VLMs still\nshow obvious deficiencies in simulating human-level graphic reasoning\ncapabilities, especially in complex graphic reasoning and abstract problem\nsolving, which are less studied and existing studies only focus on simple\ngraphics. To evaluate the performance of VLMs in complex graphic reasoning, we\npropose ReasonBench, the first evaluation benchmark focused on structured\ngraphic reasoning tasks, which includes 1,613 questions from real-world\nintelligence tests. ReasonBench covers reasoning dimensions related to\nlocation, attribute, quantity, and multi-element tasks, providing a\ncomprehensive evaluation of the performance of VLMs in spatial, relational, and\nabstract reasoning capabilities. We benchmark 11 mainstream VLMs (including\nclosed-source and open-source models) and reveal significant limitations of\ncurrent models. Based on these findings, we propose a dual optimization\nstrategy: Diagrammatic Reasoning Chain (DiaCoT) enhances the interpretability\nof reasoning by decomposing layers, and ReasonTune enhances the task\nadaptability of model reasoning through training, all of which improves VLM\nperformance by 33.5\\%. All experimental data and code are in the repository:\nhttps://huggingface.co/datasets/cistine/ReasonBench.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86ReasonBench\uff0c\u9996\u4e2a\u4e13\u6ce8\u4e8e\u7ed3\u6784\u5316\u56fe\u5f62\u63a8\u7406\u4efb\u52a1\u7684\u8bc4\u4f30\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u590d\u6742\u56fe\u5f62\u63a8\u7406\u4e2d\u7684\u8868\u73b0\uff0c\u5e76\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002\u901a\u8fc7\u53cc\u4f18\u5316\u7b56\u7565\uff08DiaCoT\u548cReasonTune\uff09\uff0cVLM\u6027\u80fd\u63d0\u5347\u4e8633.5%\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u7b80\u5355\u56fe\u5f62\u63a8\u7406\uff0c\u800cVLM\u5728\u590d\u6742\u56fe\u5f62\u63a8\u7406\u548c\u62bd\u8c61\u95ee\u9898\u89e3\u51b3\u65b9\u9762\u8868\u73b0\u4e0d\u8db3\uff0c\u56e0\u6b64\u9700\u8981\u4e13\u95e8\u7684\u8bc4\u4f30\u5de5\u5177\u548c\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "\u63d0\u51faReasonBench\u57fa\u51c6\uff0c\u5305\u542b1,613\u4e2a\u6765\u81ea\u771f\u5b9e\u4e16\u754c\u667a\u529b\u6d4b\u8bd5\u7684\u95ee\u9898\uff0c\u6db5\u76d6\u4f4d\u7f6e\u3001\u5c5e\u6027\u3001\u6570\u91cf\u548c\u591a\u5143\u7d20\u4efb\u52a1\u3002\u5bf911\u79cd\u4e3b\u6d41VLM\u8fdb\u884c\u8bc4\u6d4b\uff0c\u5e76\u63d0\u51faDiaCoT\uff08\u5206\u5c42\u5206\u89e3\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\uff09\u548cReasonTune\uff08\u8bad\u7ec3\u589e\u5f3a\u4efb\u52a1\u9002\u5e94\u6027\uff09\u7684\u53cc\u4f18\u5316\u7b56\u7565\u3002", "result": "\u8bc4\u6d4b\u663e\u793a\u5f53\u524dVLM\u5728\u590d\u6742\u56fe\u5f62\u63a8\u7406\u4e2d\u5b58\u5728\u663e\u8457\u5c40\u9650\u6027\u3002\u53cc\u4f18\u5316\u7b56\u7565\u4f7fVLM\u6027\u80fd\u63d0\u5347\u4e8633.5%\u3002", "conclusion": "ReasonBench\u4e3aVLM\u5728\u590d\u6742\u56fe\u5f62\u63a8\u7406\u4e2d\u7684\u8bc4\u4f30\u63d0\u4f9b\u4e86\u5168\u9762\u5de5\u5177\uff0c\u53cc\u4f18\u5316\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2508.00384", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.00384", "abs": "https://arxiv.org/abs/2508.00384", "authors": ["Juanwu Lu", "Rohit Gupta", "Ahmadreza Moradipari", "Kyungtae Han", "Ruqi Zhang", "Ziran Wang"], "title": "On Learning Closed-Loop Probabilistic Multi-Agent Simulator", "comment": "Accepted to IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS) 2025. Source Code: https://github.com/juanwulu/niva", "summary": "The rapid iteration of autonomous vehicle (AV) deployments leads to\nincreasing needs for building realistic and scalable multi-agent traffic\nsimulators for efficient evaluation. Recent advances in this area focus on\nclosed-loop simulators that enable generating diverse and interactive\nscenarios. This paper introduces Neural Interactive Agents (NIVA), a\nprobabilistic framework for multi-agent simulation driven by a hierarchical\nBayesian model that enables closed-loop, observation-conditioned simulation\nthrough autoregressive sampling from a latent, finite mixture of Gaussian\ndistributions. We demonstrate how NIVA unifies preexisting sequence-to-sequence\ntrajectory prediction models and emerging closed-loop simulation models trained\non Next-token Prediction (NTP) from a Bayesian inference perspective.\nExperiments on the Waymo Open Motion Dataset demonstrate that NIVA attains\ncompetitive performance compared to the existing method while providing\nembellishing control over intentions and driving styles.", "AI": {"tldr": "NIVA\u662f\u4e00\u4e2a\u57fa\u4e8e\u5206\u5c42\u8d1d\u53f6\u65af\u6a21\u578b\u7684\u6982\u7387\u6846\u67b6\uff0c\u7528\u4e8e\u591a\u667a\u80fd\u4f53\u4ea4\u901a\u6a21\u62df\uff0c\u652f\u6301\u95ed\u73af\u3001\u89c2\u5bdf\u9a71\u52a8\u7684\u6a21\u62df\uff0c\u5e76\u5728Waymo\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u968f\u7740\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u5feb\u901f\u8fed\u4ee3\uff0c\u9700\u8981\u66f4\u771f\u5b9e\u3001\u53ef\u6269\u5c55\u7684\u591a\u667a\u80fd\u4f53\u4ea4\u901a\u6a21\u62df\u5668\u4ee5\u9ad8\u6548\u8bc4\u4f30\u6027\u80fd\u3002", "method": "NIVA\u91c7\u7528\u5206\u5c42\u8d1d\u53f6\u65af\u6a21\u578b\uff0c\u901a\u8fc7\u81ea\u56de\u5f52\u91c7\u6837\u4ece\u6f5c\u5728\u7684\u9ad8\u65af\u6df7\u5408\u5206\u5e03\u4e2d\u751f\u6210\u4ea4\u4e92\u5f0f\u6a21\u62df\u3002", "result": "\u5728Waymo Open Motion\u6570\u636e\u96c6\u4e0a\uff0cNIVA\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u80fd\u7075\u6d3b\u63a7\u5236\u610f\u56fe\u548c\u9a7e\u9a76\u98ce\u683c\u3002", "conclusion": "NIVA\u4e3a\u591a\u667a\u80fd\u4f53\u6a21\u62df\u63d0\u4f9b\u4e86\u4e00\u79cd\u7edf\u4e00\u4e14\u7075\u6d3b\u7684\u6846\u67b6\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.00324", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00324", "abs": "https://arxiv.org/abs/2508.00324", "authors": ["Yeonjun In", "Wonjoong Kim", "Sangwu Park", "Chanyoung Park"], "title": "R1-ACT: Efficient Reasoning Model Safety Alignment by Activating Safety Knowledge", "comment": "under review", "summary": "Although large reasoning models (LRMs) have demonstrated impressive\ncapabilities on complex tasks, recent studies reveal that these models\nfrequently fulfill harmful user instructions, raising significant safety\nconcerns. In this paper, we investigate the underlying cause of LRM safety\nrisks and find that models already possess sufficient safety knowledge but fail\nto activate it during reasoning. Based on this insight, we propose R1-Act, a\nsimple and efficient post-training method that explicitly triggers safety\nknowledge through a structured reasoning process. R1-Act achieves strong safety\nimprovements while preserving reasoning performance, outperforming prior\nalignment methods. Notably, it requires only 1,000 training examples and 90\nminutes of training on a single RTX A6000 GPU. Extensive experiments across\nmultiple LRM backbones and sizes demonstrate the robustness, scalability, and\npractical efficiency of our approach.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faR1-Act\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u63a8\u7406\u89e6\u53d1\u5927\u578b\u63a8\u7406\u6a21\u578b\u7684\u5b89\u5168\u77e5\u8bc6\uff0c\u663e\u8457\u63d0\u5347\u5b89\u5168\u6027\u4e14\u4e0d\u5f71\u54cd\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7814\u7a76\u53d1\u73b0\u5176\u6613\u6267\u884c\u6709\u5bb3\u6307\u4ee4\uff0c\u5f15\u53d1\u5b89\u5168\u9690\u60a3\u3002", "method": "\u63d0\u51faR1-Act\uff0c\u4e00\u79cd\u7b80\u5355\u9ad8\u6548\u7684\u540e\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u63a8\u7406\u663e\u5f0f\u89e6\u53d1\u5b89\u5168\u77e5\u8bc6\u3002", "result": "R1-Act\u663e\u8457\u63d0\u5347\u5b89\u5168\u6027\u4e14\u4fdd\u6301\u63a8\u7406\u6027\u80fd\uff0c\u4ec5\u9700\u5c11\u91cf\u8bad\u7ec3\u8d44\u6e90\u548c\u65f6\u95f4\u3002", "conclusion": "R1-Act\u5728\u591a\u4e2aLRM\u4e0a\u9a8c\u8bc1\u4e86\u5176\u9c81\u68d2\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u5b9e\u7528\u6548\u7387\u3002"}}
{"id": "2508.00467", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.00467", "abs": "https://arxiv.org/abs/2508.00467", "authors": ["Samratul Fuady", "Danesh Tarapore", "Mohammad D. Soorati"], "title": "SubCDM: Collective Decision-Making with a Swarm Subset", "comment": "6 pages, 7 figures. This paper has been accepted for presentation at\n  the 2025 IEEE/RSJ International Conference on Intelligent Robots and Systems\n  (IROS 2025)", "summary": "Collective decision-making is a key function of autonomous robot swarms,\nenabling them to reach a consensus on actions based on environmental features.\nExisting strategies require the participation of all robots in the\ndecision-making process, which is resource-intensive and prevents the swarm\nfrom allocating the robots to any other tasks. We propose Subset-Based\nCollective Decision-Making (SubCDM), which enables decisions using only a swarm\nsubset. The construction of the subset is dynamic and decentralized, relying\nsolely on local information. Our method allows the swarm to adaptively\ndetermine the size of the subset for accurate decision-making, depending on the\ndifficulty of reaching a consensus. Simulation results using one hundred robots\nshow that our approach achieves accuracy comparable to using the entire swarm\nwhile reducing the number of robots required to perform collective\ndecision-making, making it a resource-efficient solution for collective\ndecision-making in swarm robotics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b50\u96c6\u7684\u96c6\u4f53\u51b3\u7b56\u65b9\u6cd5\uff08SubCDM\uff09\uff0c\u901a\u8fc7\u52a8\u6001\u6784\u5efa\u5b50\u96c6\u51cf\u5c11\u8d44\u6e90\u6d88\u8017\uff0c\u540c\u65f6\u4fdd\u6301\u51b3\u7b56\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u96c6\u4f53\u51b3\u7b56\u65b9\u6cd5\u9700\u8981\u6240\u6709\u673a\u5668\u4eba\u53c2\u4e0e\uff0c\u8d44\u6e90\u6d88\u8017\u5927\u4e14\u65e0\u6cd5\u5206\u914d\u673a\u5668\u4eba\u6267\u884c\u5176\u4ed6\u4efb\u52a1\u3002", "method": "\u52a8\u6001\u6784\u5efa\u5b50\u96c6\uff0c\u4ec5\u4f9d\u8d56\u5c40\u90e8\u4fe1\u606f\uff0c\u81ea\u9002\u5e94\u8c03\u6574\u5b50\u96c6\u89c4\u6a21\u4ee5\u9002\u5e94\u51b3\u7b56\u96be\u5ea6\u3002", "result": "\u4eff\u771f\u5b9e\u9a8c\u8868\u660e\uff0cSubCDM\u5728\u51cf\u5c11\u673a\u5668\u4eba\u53c2\u4e0e\u6570\u91cf\u7684\u540c\u65f6\uff0c\u4fdd\u6301\u4e86\u4e0e\u5168\u7fa4\u51b3\u7b56\u76f8\u5f53\u7684\u51c6\u786e\u6027\u3002", "conclusion": "SubCDM\u662f\u4e00\u79cd\u8d44\u6e90\u9ad8\u6548\u7684\u96c6\u4f53\u51b3\u7b56\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u7fa4\u673a\u5668\u4eba\u7cfb\u7edf\u3002"}}
{"id": "2508.00378", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00378", "abs": "https://arxiv.org/abs/2508.00378", "authors": ["Shixin Yi", "Lin Shang"], "title": "CoRGI: Verified Chain-of-Thought Reasoning with Visual Grounding", "comment": "Preparing for AAAI 2026, Multimodal Reasoning", "summary": "Chain-of-Thought (CoT) prompting has shown promise in improving reasoning in\nvision-language models (VLMs), but it often produces explanations that are\nlinguistically fluent yet lack grounding in visual content. We observe that\nsuch hallucinations arise in part from the absence of an explicit verification\nmechanism during multi-step reasoning. To address this, we propose\n\\textbf{CoRGI}(\\textbf{C}hain \\textbf{o}f \\textbf{R}easoning with\n\\textbf{G}rounded \\textbf{I}nsights), a modular framework that introduces\nvisual verification into the reasoning process. CoRGI follows a three-stage\npipeline: it first generates a textual reasoning chain, then extracts\nsupporting visual evidence for each reasoning step via a dedicated module\n(VEVM), and finally synthesizes the textual rationale with visual evidence to\ngenerate a grounded, verified answer. The framework can be integrated with\nexisting VLMs without end-to-end retraining. We evaluate CoRGI on the VCR\nbenchmark and find that it improves reasoning performance on two representative\nopen-source VLM backbones, Qwen-2.5VL and LLaVA-1.6. Ablation studies confirm\nthe contribution of each step in the verification module, and human evaluations\nsuggest that CoRGI leads to more factual and helpful explanations. We also\nexamine alternative designs for the visual verification step and discuss\npotential limitations of post-hoc verification frameworks. These findings\nhighlight the importance of grounding intermediate reasoning steps in visual\nevidence to enhance the robustness of multimodal reasoning.", "AI": {"tldr": "CoRGI\u6846\u67b6\u901a\u8fc7\u89c6\u89c9\u9a8c\u8bc1\u6539\u8fdb\u591a\u6a21\u6001\u63a8\u7406\uff0c\u51cf\u5c11\u5e7b\u89c9\uff0c\u63d0\u5347\u89e3\u91ca\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709CoT\u63d0\u793a\u5728\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u751f\u6210\u7684\u89e3\u91ca\u7f3a\u4e4f\u89c6\u89c9\u5185\u5bb9\u652f\u6301\uff0c\u5bfc\u81f4\u5e7b\u89c9\u95ee\u9898\u3002", "method": "\u63d0\u51faCoRGI\u6846\u67b6\uff0c\u5206\u4e09\u9636\u6bb5\uff1a\u751f\u6210\u6587\u672c\u63a8\u7406\u94fe\u3001\u63d0\u53d6\u89c6\u89c9\u8bc1\u636e\u3001\u5408\u6210\u9a8c\u8bc1\u7b54\u6848\u3002", "result": "\u5728VCR\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCoRGI\u63d0\u5347\u4e86Qwen-2.5VL\u548cLLaVA-1.6\u7684\u6027\u80fd\uff0c\u751f\u6210\u66f4\u51c6\u786e\u7684\u89e3\u91ca\u3002", "conclusion": "\u89c6\u89c9\u9a8c\u8bc1\u5bf9\u63d0\u5347\u591a\u6a21\u6001\u63a8\u7406\u7684\u9c81\u68d2\u6027\u81f3\u5173\u91cd\u8981\uff0cCoRGI\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2508.00491", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00491", "abs": "https://arxiv.org/abs/2508.00491", "authors": ["Carlo Alessi", "Federico Vasile", "Federico Ceola", "Giulia Pasquale", "Nicol\u00f2 Boccardo", "Lorenzo Natale"], "title": "HannesImitation: Grasping with the Hannes Prosthetic Hand via Imitation Learning", "comment": "Paper accepted at IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS)", "summary": "Recent advancements in control of prosthetic hands have focused on increasing\nautonomy through the use of cameras and other sensory inputs. These systems aim\nto reduce the cognitive load on the user by automatically controlling certain\ndegrees of freedom. In robotics, imitation learning has emerged as a promising\napproach for learning grasping and complex manipulation tasks while simplifying\ndata collection. Its application to the control of prosthetic hands remains,\nhowever, largely unexplored. Bridging this gap could enhance dexterity\nrestoration and enable prosthetic devices to operate in more unconstrained\nscenarios, where tasks are learned from demonstrations rather than relying on\nmanually annotated sequences. To this end, we present HannesImitationPolicy, an\nimitation learning-based method to control the Hannes prosthetic hand, enabling\nobject grasping in unstructured environments. Moreover, we introduce the\nHannesImitationDataset comprising grasping demonstrations in table, shelf, and\nhuman-to-prosthesis handover scenarios. We leverage such data to train a single\ndiffusion policy and deploy it on the prosthetic hand to predict the wrist\norientation and hand closure for grasping. Experimental evaluation demonstrates\nsuccessful grasps across diverse objects and conditions. Finally, we show that\nthe policy outperforms a segmentation-based visual servo controller in\nunstructured scenarios. Additional material is provided on our project page:\nhttps://hsp-iit.github.io/HannesImitation", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6a21\u4eff\u5b66\u4e60\u7684\u5047\u624b\u63a7\u5236\u65b9\u6cd5HannesImitationPolicy\uff0c\u901a\u8fc7\u6269\u6563\u7b56\u7565\u9884\u6d4b\u624b\u8155\u65b9\u5411\u548c\u624b\u90e8\u95ed\u5408\u52a8\u4f5c\uff0c\u63d0\u5347\u4e86\u5047\u624b\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u6293\u53d6\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u5047\u624b\u63a7\u5236\u7814\u7a76\u591a\u4f9d\u8d56\u624b\u52a8\u6807\u6ce8\u6570\u636e\uff0c\u800c\u6a21\u4eff\u5b66\u4e60\u5728\u673a\u5668\u4eba\u9886\u57df\u5df2\u663e\u793a\u51fa\u7b80\u5316\u6570\u636e\u6536\u96c6\u7684\u6f5c\u529b\uff0c\u4f46\u5728\u5047\u624b\u63a7\u5236\u4e2d\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u53ef\u63d0\u5347\u5047\u624b\u7684\u7075\u6d3b\u6027\u548c\u9002\u5e94\u6027\u3002", "method": "\u63d0\u51faHannesImitationPolicy\u65b9\u6cd5\uff0c\u5229\u7528HannesImitationDataset\u4e2d\u7684\u6293\u53d6\u6f14\u793a\u6570\u636e\u8bad\u7ec3\u6269\u6563\u7b56\u7565\uff0c\u9884\u6d4b\u624b\u8155\u65b9\u5411\u548c\u624b\u90e8\u95ed\u5408\u52a8\u4f5c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u6837\u5316\u7684\u7269\u4f53\u548c\u6761\u4ef6\u4e0b\u6210\u529f\u5b9e\u73b0\u6293\u53d6\uff0c\u5e76\u5728\u975e\u7ed3\u6784\u5316\u573a\u666f\u4e2d\u4f18\u4e8e\u57fa\u4e8e\u5206\u5272\u7684\u89c6\u89c9\u4f3a\u670d\u63a7\u5236\u5668\u3002", "conclusion": "\u6a21\u4eff\u5b66\u4e60\u4e3a\u5047\u624b\u63a7\u5236\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5c24\u5176\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u4f18\u5316\u548c\u6269\u5c55\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2508.00401", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.00401", "abs": "https://arxiv.org/abs/2508.00401", "authors": ["Riddhi J. Pitliya", "Ozan Catal", "Toon Van de Maele", "Corrado Pezzato", "Tim Verbelen"], "title": "Theory of Mind Using Active Inference: A Framework for Multi-Agent Cooperation", "comment": null, "summary": "We present a novel approach to multi-agent cooperation by implementing theory\nof mind (ToM) within active inference. ToM - the ability to understand that\nothers can have differing knowledge and goals - enables agents to reason about\nothers' beliefs while planning their own actions. Unlike previous active\ninference approaches to multi-agent cooperation, our method neither relies on\ntask-specific shared generative models nor requires explicit communication,\nwhile being generalisable. In our framework, the ToM-equipped agent maintains\ndistinct representations of its own and others' beliefs and goals. We extend\nthe sophisticated inference tree-based planning algorithm to systematically\nexplore joint policy spaces through recursive reasoning. Our approach is\nevaluated through collision avoidance and foraging task simulations. Results\ndemonstrate that ToM-equipped agents cooperate better compared to non-ToM\ncounterparts by being able to avoid collisions and reduce redundant efforts.\nCrucially, ToM agents accomplish this by inferring others' beliefs solely from\nobservable behaviour. This work advances practical applications in artificial\nintelligence while providing computational insights into ToM.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5fc3\u667a\u7406\u8bba\uff08ToM\uff09\u7684\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e3b\u52a8\u63a8\u7406\u5b9e\u73b0\uff0c\u65e0\u9700\u4efb\u52a1\u7279\u5b9a\u7684\u5171\u4eab\u751f\u6210\u6a21\u578b\u6216\u663e\u5f0f\u901a\u4fe1\u3002", "motivation": "\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u4e2d\u5982\u4f55\u7406\u89e3\u5e76\u63a8\u7406\u5176\u4ed6\u667a\u80fd\u4f53\u7684\u4fe1\u5ff5\u548c\u76ee\u6807\u7684\u95ee\u9898\uff0c\u63d0\u5347\u534f\u4f5c\u6548\u7387\u3002", "method": "\u6269\u5c55\u63a8\u7406\u6811\u89c4\u5212\u7b97\u6cd5\uff0c\u901a\u8fc7\u9012\u5f52\u63a8\u7406\u63a2\u7d22\u8054\u5408\u7b56\u7565\u7a7a\u95f4\uff0c\u667a\u80fd\u4f53\u7ef4\u62a4\u81ea\u8eab\u548c\u4ed6\u4eba\u7684\u4fe1\u5ff5\u4e0e\u76ee\u6807\u8868\u5f81\u3002", "result": "\u5728\u907f\u78b0\u548c\u89c5\u98df\u4efb\u52a1\u4e2d\uff0cToM\u667a\u80fd\u4f53\u8868\u73b0\u4f18\u4e8e\u975eToM\u667a\u80fd\u4f53\uff0c\u80fd\u907f\u514d\u78b0\u649e\u5e76\u51cf\u5c11\u5197\u4f59\u52aa\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4eba\u5de5\u667a\u80fd\u7684\u5b9e\u8df5\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5e76\u4e3a\u5fc3\u667a\u7406\u8bba\u7684\u8ba1\u7b97\u7814\u7a76\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002"}}
{"id": "2508.00580", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00580", "abs": "https://arxiv.org/abs/2508.00580", "authors": ["Raul Castilla-Arquillo", "Carlos Perez-del-Pulgar", "Levin Gerdes", "Alfonso Garcia-Cerezo", "Miguel A. Olivares-Mendez"], "title": "OmniUnet: A Multimodal Network for Unstructured Terrain Segmentation on Planetary Rovers Using RGB, Depth, and Thermal Imagery", "comment": null, "summary": "Robot navigation in unstructured environments requires multimodal perception\nsystems that can support safe navigation. Multimodality enables the integration\nof complementary information collected by different sensors. However, this\ninformation must be processed by machine learning algorithms specifically\ndesigned to leverage heterogeneous data. Furthermore, it is necessary to\nidentify which sensor modalities are most informative for navigation in the\ntarget environment. In Martian exploration, thermal imagery has proven valuable\nfor assessing terrain safety due to differences in thermal behaviour between\nsoil types. This work presents OmniUnet, a transformer-based neural network\narchitecture for semantic segmentation using RGB, depth, and thermal (RGB-D-T)\nimagery. A custom multimodal sensor housing was developed using 3D printing and\nmounted on the Martian Rover Testbed for Autonomy (MaRTA) to collect a\nmultimodal dataset in the Bardenas semi-desert in northern Spain. This location\nserves as a representative environment of the Martian surface, featuring\nterrain types such as sand, bedrock, and compact soil. A subset of this dataset\nwas manually labeled to support supervised training of the network. The model\nwas evaluated both quantitatively and qualitatively, achieving a pixel accuracy\nof 80.37% and demonstrating strong performance in segmenting complex\nunstructured terrain. Inference tests yielded an average prediction time of 673\nms on a resource-constrained computer (Jetson Orin Nano), confirming its\nsuitability for on-robot deployment. The software implementation of the network\nand the labeled dataset have been made publicly available to support future\nresearch in multimodal terrain perception for planetary robotics.", "AI": {"tldr": "OmniUnet\u662f\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u7528\u4e8eRGB-D-T\u56fe\u50cf\u8bed\u4e49\u5206\u5272\uff0c\u5728\u706b\u661f\u63a2\u6d4b\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u673a\u5668\u4eba\u5bfc\u822a\u7684\u591a\u6a21\u6001\u611f\u77e5\u9700\u6c42\uff0c\u7279\u522b\u662f\u706b\u661f\u63a2\u6d4b\u4e2d\u70ed\u6210\u50cf\u5bf9\u5730\u5f62\u5b89\u5168\u8bc4\u4f30\u7684\u91cd\u8981\u6027\u3002", "method": "\u5f00\u53d1\u4e86OmniUnet\u67b6\u6784\u548c\u5b9a\u5236\u591a\u6a21\u6001\u4f20\u611f\u5668\uff0c\u6536\u96c6\u5e76\u6807\u6ce8\u4e86\u534a\u6c99\u6f20\u73af\u5883\u6570\u636e\u96c6\uff0c\u8fdb\u884c\u76d1\u7763\u8bad\u7ec3\u548c\u8bc4\u4f30\u3002", "result": "\u6a21\u578b\u50cf\u7d20\u51c6\u786e\u7387\u8fbe80.37%\uff0c\u63a8\u7406\u65f6\u95f4673\u6beb\u79d2\uff0c\u9002\u5408\u673a\u5668\u4eba\u90e8\u7f72\u3002", "conclusion": "OmniUnet\u5728\u591a\u6a21\u6001\u5730\u5f62\u611f\u77e5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u516c\u5f00\u4e86\u8f6f\u4ef6\u548c\u6570\u636e\u96c6\u4ee5\u652f\u6301\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2508.00414", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.00414", "abs": "https://arxiv.org/abs/2508.00414", "authors": ["Tianqing Fang", "Zhisong Zhang", "Xiaoyang Wang", "Rui Wang", "Can Qin", "Yuxuan Wan", "Jun-Yu Ma", "Ce Zhang", "Jiaqi Chen", "Xiyun Li", "Hongming Zhang", "Haitao Mi", "Dong Yu"], "title": "Cognitive Kernel-Pro: A Framework for Deep Research Agents and Agent Foundation Models Training", "comment": "16 pages", "summary": "General AI Agents are increasingly recognized as foundational frameworks for\nthe next generation of artificial intelligence, enabling complex reasoning, web\ninteraction, coding, and autonomous research capabilities. However, current\nagent systems are either closed-source or heavily reliant on a variety of paid\nAPIs and proprietary tools, limiting accessibility and reproducibility for the\nresearch community. In this work, we present \\textbf{Cognitive Kernel-Pro}, a\nfully open-source and (to the maximum extent) free multi-module agent framework\ndesigned to democratize the development and evaluation of advanced AI agents.\nWithin Cognitive Kernel-Pro, we systematically investigate the curation of\nhigh-quality training data for Agent Foundation Models, focusing on the\nconstruction of queries, trajectories, and verifiable answers across four key\ndomains: web, file, code, and general reasoning. Furthermore, we explore novel\nstrategies for agent test-time reflection and voting to enhance agent\nrobustness and performance. We evaluate Cognitive Kernel-Pro on GAIA, achieving\nstate-of-the-art results among open-source and free agents. Notably, our\n8B-parameter open-source model surpasses previous leading systems such as\nWebDancer and WebSailor, establishing a new performance standard for\naccessible, high-capability AI agents. Code is available at\nhttps://github.com/Tencent/CognitiveKernel-Pro", "AI": {"tldr": "Cognitive Kernel-Pro \u662f\u4e00\u4e2a\u5b8c\u5168\u5f00\u6e90\u4e14\u514d\u8d39\u7684 AI \u4ee3\u7406\u6846\u67b6\uff0c\u65e8\u5728\u63a8\u52a8\u9ad8\u7ea7 AI \u4ee3\u7406\u7684\u5f00\u53d1\u4e0e\u8bc4\u4f30\u3002", "motivation": "\u5f53\u524d AI \u4ee3\u7406\u7cfb\u7edf\u591a\u4e3a\u95ed\u6e90\u6216\u4f9d\u8d56\u4ed8\u8d39 API\uff0c\u9650\u5236\u4e86\u7814\u7a76\u7684\u53ef\u8bbf\u95ee\u6027\u548c\u53ef\u590d\u73b0\u6027\u3002", "method": "\u901a\u8fc7\u9ad8\u8d28\u91cf\u8bad\u7ec3\u6570\u636e\u7684\u6784\u5efa\u548c\u591a\u6a21\u5757\u8bbe\u8ba1\uff0c\u7ed3\u5408\u6d4b\u8bd5\u65f6\u53cd\u601d\u4e0e\u6295\u7968\u7b56\u7565\u63d0\u5347\u4ee3\u7406\u6027\u80fd\u3002", "result": "\u5728 GAIA \u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c8B \u53c2\u6570\u7684\u5f00\u6e90\u6a21\u578b\u8868\u73b0\u4f18\u4e8e WebDancer \u548c WebSailor\uff0c\u8fbe\u5230\u65b0\u6807\u51c6\u3002", "conclusion": "Cognitive Kernel-Pro \u4e3a\u5f00\u6e90 AI \u4ee3\u7406\u63d0\u4f9b\u4e86\u9ad8\u6027\u80fd\u4e14\u6613\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.00584", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.00584", "abs": "https://arxiv.org/abs/2508.00584", "authors": ["Konstantinos Plotas", "Emmanouil Papadakis", "Drosakis Drosakis", "Panos Trahanias", "Dimitrios Papageorgiou"], "title": "A control scheme for collaborative object transportation between a human and a quadruped robot using the MIGHTY suction cup", "comment": "Please find the citation info @ Zenodo, ArXiv or Zenodo, as the\n  proceedings of ICRA are no longer sent to IEEE Xplore", "summary": "In this work, a control scheme for human-robot collaborative object\ntransportation is proposed, considering a quadruped robot equipped with the\nMIGHTY suction cup that serves both as a gripper for holding the object and a\nforce/torque sensor. The proposed control scheme is based on the notion of\nadmittance control, and incorporates a variable damping term aiming towards\nincreasing the controllability of the human and, at the same time, decreasing\nher/his effort. Furthermore, to ensure that the object is not detached from the\nsuction cup during the collaboration, an additional control signal is proposed,\nwhich is based on a barrier artificial potential. The proposed control scheme\nis proven to be passive and its performance is demonstrated through\nexperimental evaluations conducted using the Unitree Go1 robot equipped with\nthe MIGHTY suction cup.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bfc\u7eb3\u63a7\u5236\u7684\u56db\u8db3\u673a\u5668\u4eba\u4e0e\u4eba\u7c7b\u534f\u4f5c\u642c\u8fd0\u7269\u4f53\u7684\u63a7\u5236\u65b9\u6848\uff0c\u7ed3\u5408\u53ef\u53d8\u963b\u5c3c\u548c\u5c4f\u969c\u4eba\u5de5\u52bf\u80fd\uff0c\u63d0\u9ad8\u53ef\u63a7\u6027\u5e76\u51cf\u5c11\u4eba\u7c7b\u8d1f\u62c5\u3002", "motivation": "\u89e3\u51b3\u4eba\u673a\u534f\u4f5c\u642c\u8fd0\u4e2d\u53ef\u63a7\u6027\u548c\u4eba\u7c7b\u8d1f\u62c5\u7684\u95ee\u9898\uff0c\u540c\u65f6\u786e\u4fdd\u7269\u4f53\u4e0e\u5438\u76d8\u4e0d\u5206\u79bb\u3002", "method": "\u91c7\u7528\u5bfc\u7eb3\u63a7\u5236\u6846\u67b6\uff0c\u5f15\u5165\u53ef\u53d8\u963b\u5c3c\u548c\u57fa\u4e8e\u5c4f\u969c\u4eba\u5de5\u52bf\u80fd\u7684\u989d\u5916\u63a7\u5236\u4fe1\u53f7\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u63a7\u5236\u65b9\u6848\u7684\u88ab\u52a8\u6027\u548c\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u63a7\u5236\u65b9\u6848\u5728\u63d0\u9ad8\u534f\u4f5c\u6548\u7387\u7684\u540c\u65f6\uff0c\u786e\u4fdd\u4e86\u7cfb\u7edf\u7684\u7a33\u5b9a\u6027\u548c\u5b89\u5168\u6027\u3002"}}
{"id": "2508.00459", "categories": ["cs.AI", "68T07, 68T20", "I.2.6; I.2.7; I.2.3"], "pdf": "https://arxiv.org/pdf/2508.00459", "abs": "https://arxiv.org/abs/2508.00459", "authors": ["Andrea Asperti", "Alberto Naibo", "Claudio Sacerdoti Coen"], "title": "Thinking Machines: Mathematical Reasoning in the Age of LLMs", "comment": null, "summary": "Large Language Models (LLMs) have shown remarkable abilities in structured\nreasoning and symbolic tasks, with coding emerging as a particular area of\nstrength. This success has sparked growing interest in applying LLMs to\nmathematics, both in informal problem-solving and formal theorem proving.\nHowever, progress in formal mathematics has proven to be significantly more\ndifficult, despite surface-level similarities between programming and proof\nconstruction. This discrepancy raises important questions about how LLMs\n``reason'', how they are supervised, and whether they internally track a notion\nof computational or deductive state. In this article, we address the\nstate-of-the-art of the discipline, focusing on recent models and benchmarks,\nand explore three central issues at the intersection of machine learning and\nmathematical cognition: (i) the trade-offs between formal and informal\nmathematics as training domains; (ii) the deeper reasons why proof generation\nremains more brittle than code synthesis; (iii) and the question of whether\nLLMs represent, or merely mimic, a notion of evolving logical state. Our goal\nis not to draw hard boundaries, but to identify where the current limits lie,\nand how they might be extended.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u6570\u5b66\u9886\u57df\u7684\u5e94\u7528\uff0c\u5c24\u5176\u662f\u5f62\u5f0f\u5316\u6570\u5b66\u8bc1\u660e\u4e2d\u7684\u6311\u6218\uff0c\u5206\u6790\u4e86\u5176\u4e0e\u4ee3\u7801\u751f\u6210\u7684\u5dee\u5f02\uff0c\u5e76\u63d0\u51fa\u4e86\u4e09\u4e2a\u6838\u5fc3\u95ee\u9898\u3002", "motivation": "LLMs\u5728\u7f16\u7a0b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5f62\u5f0f\u5316\u6570\u5b66\u8bc1\u660e\u4e2d\u8fdb\u5c55\u7f13\u6162\uff0c\u8fd9\u5f15\u53d1\u4e86\u5bf9LLM\u63a8\u7406\u65b9\u5f0f\u3001\u76d1\u7763\u673a\u5236\u53ca\u5185\u90e8\u72b6\u6001\u8868\u793a\u7684\u7591\u95ee\u3002", "method": "\u901a\u8fc7\u5206\u6790\u6700\u65b0\u6a21\u578b\u548c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63a2\u8ba8\u4e86\u5f62\u5f0f\u5316\u4e0e\u975e\u5f62\u5f0f\u5316\u6570\u5b66\u8bad\u7ec3\u3001\u8bc1\u660e\u751f\u6210\u7684\u8106\u5f31\u6027\u4ee5\u53caLLM\u662f\u5426\u771f\u6b63\u8868\u793a\u903b\u8f91\u72b6\u6001\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0cLLMs\u5728\u5f62\u5f0f\u5316\u6570\u5b66\u8bc1\u660e\u4e2d\u7684\u8868\u73b0\u4e0d\u5982\u4ee3\u7801\u751f\u6210\u7a33\u5b9a\uff0c\u4e14\u5176\u5185\u90e8\u72b6\u6001\u8868\u793a\u5c1a\u4e0d\u660e\u786e\u3002", "conclusion": "\u8bba\u6587\u65e8\u5728\u660e\u786e\u5f53\u524dLLMs\u5728\u6570\u5b66\u9886\u57df\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u53ef\u80fd\u7684\u6269\u5c55\u65b9\u5411\u3002"}}
{"id": "2508.00625", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.00625", "abs": "https://arxiv.org/abs/2508.00625", "authors": ["Bartosz Krawczyk", "Ahmed Elbary", "Robbie Cato", "Jagdish Patil", "Kaung Myat", "Anyeh Ndi-Tah", "Nivetha Sakthivel", "Mark Crampton", "Gautham Das", "Charles Fox"], "title": "OpenScout v1.1 mobile robot: a case study on open hardware continuation", "comment": "6 pages, 4 figures, a TAROS2025 short paper", "summary": "OpenScout is an Open Source Hardware (OSH) mobile robot for research and\nindustry. It is extended to v1.1 which includes simplified, cheaper and more\npowerful onboard compute hardware; a simulated ROS2 interface; and a Gazebo\nsimulation. Changes, their rationale, project methodology, and results are\nreported as an OSH case study.", "AI": {"tldr": "OpenScout v1.1\u662f\u4e00\u6b3e\u5f00\u6e90\u7684\u79fb\u52a8\u673a\u5668\u4eba\u786c\u4ef6\uff0c\u7528\u4e8e\u7814\u7a76\u548c\u5de5\u4e1a\u9886\u57df\uff0c\u5347\u7ea7\u540e\u8ba1\u7b97\u786c\u4ef6\u66f4\u7b80\u5316\u3001\u66f4\u4fbf\u5b9c\u4e14\u66f4\u5f3a\u5927\uff0c\u5e76\u652f\u6301ROS2\u63a5\u53e3\u548cGazebo\u6a21\u62df\u3002", "motivation": "\u4e3a\u7814\u7a76\u548c\u5de5\u4e1a\u63d0\u4f9b\u4e00\u4e2a\u4f4e\u6210\u672c\u3001\u9ad8\u6027\u80fd\u7684\u5f00\u6e90\u79fb\u52a8\u673a\u5668\u4eba\u5e73\u53f0\u3002", "method": "\u901a\u8fc7\u7b80\u5316\u786c\u4ef6\u8bbe\u8ba1\u3001\u4f18\u5316\u8ba1\u7b97\u80fd\u529b\uff0c\u5e76\u96c6\u6210ROS2\u548cGazebo\u6a21\u62df\u529f\u80fd\u3002", "result": "\u5b9e\u73b0\u4e86\u66f4\u7ecf\u6d4e\u3001\u66f4\u5f3a\u5927\u7684\u786c\u4ef6\u5e73\u53f0\uff0c\u5e76\u63d0\u4f9b\u4e86\u6a21\u62df\u652f\u6301\u3002", "conclusion": "OpenScout v1.1\u662f\u4e00\u4e2a\u6210\u529f\u7684\u5f00\u6e90\u6848\u4f8b\uff0c\u5c55\u793a\u4e86\u786c\u4ef6\u4f18\u5316\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.00500", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.00500", "abs": "https://arxiv.org/abs/2508.00500", "authors": ["Haoyu Wang", "Chris M. Poskitt", "Jun Sun", "Jiali Wei"], "title": "Pro2Guard: Proactive Runtime Enforcement of LLM Agent Safety via Probabilistic Model Checking", "comment": null, "summary": "Large Language Model (LLM) agents exhibit powerful autonomous capabilities\nacross domains such as robotics, virtual assistants, and web automation.\nHowever, their stochastic behavior introduces significant safety risks that are\ndifficult to anticipate. Existing rule-based enforcement systems, such as\nAgentSpec, focus on developing reactive safety rules, which typically respond\nonly when unsafe behavior is imminent or has already occurred. These systems\nlack foresight and struggle with long-horizon dependencies and distribution\nshifts. To address these limitations, we propose Pro2Guard, a proactive runtime\nenforcement framework grounded in probabilistic reachability analysis.\nPro2Guard abstracts agent behaviors into symbolic states and learns a\nDiscrete-Time Markov Chain (DTMC) from execution traces. At runtime, it\nanticipates future risks by estimating the probability of reaching unsafe\nstates, triggering interventions before violations occur when the predicted\nrisk exceeds a user-defined threshold. By incorporating semantic validity\nchecks and leveraging PAC bounds, Pro2Guard ensures statistical reliability\nwhile approximating the underlying ground-truth model. We evaluate Pro2Guard\nextensively across two safety-critical domains: embodied household agents and\nautonomous vehicles. In embodied agent tasks, Pro2Guard enforces safety early\non up to 93.6% of unsafe tasks using low thresholds, while configurable modes\n(e.g., reflect) allow balancing safety with task success, maintaining up to\n80.4% task completion. In autonomous driving scenarios, Pro2Guard achieves 100%\nprediction of traffic law violations and collisions, anticipating risks up to\n38.66 seconds ahead.", "AI": {"tldr": "Pro2Guard\u662f\u4e00\u79cd\u57fa\u4e8e\u6982\u7387\u53ef\u8fbe\u6027\u5206\u6790\u7684\u4e3b\u52a8\u8fd0\u884c\u65f6\u5b89\u5168\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u548c\u9632\u6b62LLM\u4ee3\u7406\u7684\u4e0d\u5b89\u5168\u884c\u4e3a\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u89c4\u5219\u7684\u5b89\u5168\u7cfb\u7edf\uff08\u5982AgentSpec\uff09\u7f3a\u4e4f\u9884\u89c1\u6027\uff0c\u96be\u4ee5\u5904\u7406\u957f\u671f\u4f9d\u8d56\u548c\u5206\u5e03\u504f\u79fb\uff0c\u5bfc\u81f4\u5b89\u5168\u98ce\u9669\u3002", "method": "Pro2Guard\u5c06\u4ee3\u7406\u884c\u4e3a\u62bd\u8c61\u4e3a\u7b26\u53f7\u72b6\u6001\uff0c\u4ece\u6267\u884c\u8f68\u8ff9\u4e2d\u5b66\u4e60\u79bb\u6563\u65f6\u95f4\u9a6c\u5c14\u53ef\u592b\u94fe\uff08DTMC\uff09\uff0c\u5e76\u5728\u8fd0\u884c\u65f6\u9884\u6d4b\u4e0d\u5b89\u5168\u72b6\u6001\u7684\u6982\u7387\u3002", "result": "\u5728\u5bb6\u5ead\u4ee3\u7406\u548c\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2d\uff0cPro2Guard\u5206\u522b\u5b9e\u73b0\u4e8693.6%\u548c100%\u7684\u5b89\u5168\u9884\u6d4b\u6210\u529f\u7387\uff0c\u5e76\u80fd\u63d0\u524d\u5e72\u9884\u3002", "conclusion": "Pro2Guard\u901a\u8fc7\u4e3b\u52a8\u9884\u6d4b\u548c\u5e72\u9884\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u4ee3\u7406\u7684\u5b89\u5168\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4efb\u52a1\u5b8c\u6210\u7387\u3002"}}
{"id": "2508.00691", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.00691", "abs": "https://arxiv.org/abs/2508.00691", "authors": ["Fabian C. Weigend", "Dabin K. Choe", "Santiago Canete", "Conor J. Walsh"], "title": "Towards Data-Driven Adaptive Exoskeleton Assistance for Post-stroke Gait", "comment": "8 pages, 6 figures, 2 tables", "summary": "Recent work has shown that exoskeletons controlled through data-driven\nmethods can dynamically adapt assistance to various tasks for healthy young\nadults. However, applying these methods to populations with neuromotor gait\ndeficits, such as post-stroke hemiparesis, is challenging. This is due not only\nto high population heterogeneity and gait variability but also to a lack of\npost-stroke gait datasets to train accurate models. Despite these challenges,\ndata-driven methods offer a promising avenue for control, potentially allowing\nexoskeletons to function safely and effectively in unstructured community\nsettings. This work presents a first step towards enabling adaptive\nplantarflexion and dorsiflexion assistance from data-driven torque estimation\nduring post-stroke walking. We trained a multi-task Temporal Convolutional\nNetwork (TCN) using collected data from four post-stroke participants walking\non a treadmill ($R^2$ of $0.74 \\pm 0.13$). The model uses data from three\ninertial measurement units (IMU) and was pretrained on healthy walking data\nfrom 6 participants. We implemented a wearable prototype for our ankle torque\nestimation approach for exoskeleton control and demonstrated the viability of\nreal-time sensing, estimation, and actuation with one post-stroke participant.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6570\u636e\u9a71\u52a8\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u4efb\u52a1\u65f6\u5e8f\u5377\u79ef\u7f51\u7edc\uff08TCN\uff09\u4e3a\u4e2d\u98ce\u540e\u884c\u8d70\u60a3\u8005\u63d0\u4f9b\u8e1d\u5173\u8282\u8f85\u52a9\u529b\u77e9\u4f30\u8ba1\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5b9e\u65f6\u63a7\u5236\u7684\u53ef\u884c\u6027\u3002", "motivation": "\u5c3d\u7ba1\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5728\u5065\u5eb7\u5e74\u8f7b\u4eba\u4e2d\u5df2\u6210\u529f\u5e94\u7528\u4e8e\u5916\u9aa8\u9abc\u63a7\u5236\uff0c\u4f46\u5728\u4e2d\u98ce\u540e\u6b65\u6001\u7f3a\u9677\u60a3\u8005\u4e2d\u7684\u5e94\u7528\u9762\u4e34\u6311\u6218\uff0c\u5982\u4eba\u7fa4\u5f02\u8d28\u6027\u548c\u6570\u636e\u7f3a\u4e4f\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u4f7f\u7528\u56db\u540d\u4e2d\u98ce\u540e\u53c2\u4e0e\u8005\u7684\u8dd1\u6b65\u673a\u884c\u8d70\u6570\u636e\u8bad\u7ec3\u591a\u4efb\u52a1TCN\u6a21\u578b\uff0c\u7ed3\u5408\u4e09\u4e2aIMU\u6570\u636e\uff0c\u5e76\u9884\u8bad\u7ec3\u5065\u5eb7\u53c2\u4e0e\u8005\u6570\u636e\u3002\u5f00\u53d1\u4e86\u53ef\u7a7f\u6234\u539f\u578b\u8fdb\u884c\u5b9e\u65f6\u63a7\u5236\u9a8c\u8bc1\u3002", "result": "\u6a21\u578b\u5728\u4e2d\u98ce\u540e\u884c\u8d70\u6570\u636e\u4e0a\u7684R\u00b2\u4e3a0.74\u00b10.13\uff0c\u5e76\u901a\u8fc7\u4e00\u540d\u53c2\u4e0e\u8005\u9a8c\u8bc1\u4e86\u5b9e\u65f6\u4f20\u611f\u3001\u4f30\u8ba1\u548c\u9a71\u52a8\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u4e3a\u4e2d\u98ce\u540e\u60a3\u8005\u7684\u5916\u9aa8\u9abc\u63a7\u5236\u63d0\u4f9b\u4e86\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\uff0c\u672a\u6765\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u548c\u6269\u5c55\u3002"}}
{"id": "2508.00576", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00576", "abs": "https://arxiv.org/abs/2508.00576", "authors": ["Zhanliang Wang", "Kai Wang"], "title": "MultiSHAP: A Shapley-Based Framework for Explaining Cross-Modal Interactions in Multimodal AI Models", "comment": null, "summary": "Multimodal AI models have achieved impressive performance in tasks that\nrequire integrating information from multiple modalities, such as vision and\nlanguage. However, their \"black-box\" nature poses a major barrier to deployment\nin high-stakes applications where interpretability and trustworthiness are\nessential. How to explain cross-modal interactions in multimodal AI models\nremains a major challenge. While existing model explanation methods, such as\nattention map and Grad-CAM, offer coarse insights into cross-modal\nrelationships, they cannot precisely quantify the synergistic effects between\nmodalities, and are limited to open-source models with accessible internal\nweights. Here we introduce MultiSHAP, a model-agnostic interpretability\nframework that leverages the Shapley Interaction Index to attribute multimodal\npredictions to pairwise interactions between fine-grained visual and textual\nelements (such as image patches and text tokens), while being applicable to\nboth open- and closed-source models. Our approach provides: (1) instance-level\nexplanations that reveal synergistic and suppressive cross-modal effects for\nindividual samples - \"why the model makes a specific prediction on this input\",\nand (2) dataset-level explanation that uncovers generalizable interaction\npatterns across samples - \"how the model integrates information across\nmodalities\". Experiments on public multimodal benchmarks confirm that MultiSHAP\nfaithfully captures cross-modal reasoning mechanisms, while real-world case\nstudies demonstrate its practical utility. Our framework is extensible beyond\ntwo modalities, offering a general solution for interpreting complex multimodal\nAI models.", "AI": {"tldr": "MultiSHAP\u662f\u4e00\u4e2a\u6a21\u578b\u65e0\u5173\u7684\u53ef\u89e3\u91ca\u6027\u6846\u67b6\uff0c\u7528\u4e8e\u91cf\u5316\u591a\u6a21\u6001AI\u6a21\u578b\u4e2d\u89c6\u89c9\u548c\u6587\u672c\u5143\u7d20\u4e4b\u95f4\u7684\u534f\u540c\u6548\u5e94\uff0c\u9002\u7528\u4e8e\u5f00\u6e90\u548c\u95ed\u6e90\u6a21\u578b\u3002", "motivation": "\u591a\u6a21\u6001AI\u6a21\u578b\u7684'\u9ed1\u76d2'\u7279\u6027\u5728\u9ad8\u98ce\u9669\u5e94\u7528\u4e2d\u9650\u5236\u4e86\u5176\u90e8\u7f72\uff0c\u73b0\u6709\u89e3\u91ca\u65b9\u6cd5\u65e0\u6cd5\u7cbe\u786e\u91cf\u5316\u6a21\u6001\u95f4\u7684\u534f\u540c\u6548\u5e94\u3002", "method": "\u5229\u7528Shapley Interaction Index\uff0cMultiSHAP\u4e3a\u7ec6\u7c92\u5ea6\u89c6\u89c9\u548c\u6587\u672c\u5143\u7d20\uff08\u5982\u56fe\u50cf\u5757\u548c\u6587\u672c\u6807\u8bb0\uff09\u63d0\u4f9b\u5b9e\u4f8b\u7ea7\u548c\u6570\u636e\u96c6\u7ea7\u89e3\u91ca\u3002", "result": "\u5b9e\u9a8c\u8bc1\u5b9eMultiSHAP\u80fd\u51c6\u786e\u6355\u6349\u8de8\u6a21\u6001\u63a8\u7406\u673a\u5236\uff0c\u5e76\u5728\u5b9e\u9645\u6848\u4f8b\u4e2d\u5c55\u793a\u5176\u5b9e\u7528\u6027\u3002", "conclusion": "MultiSHAP\u4e3a\u89e3\u91ca\u590d\u6742\u591a\u6a21\u6001AI\u6a21\u578b\u63d0\u4f9b\u4e86\u901a\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u53ef\u6269\u5c55\u5230\u4e24\u79cd\u4ee5\u4e0a\u6a21\u6001\u3002"}}
{"id": "2508.00697", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.00697", "abs": "https://arxiv.org/abs/2508.00697", "authors": ["Yiming Wu", "Huan Wang", "Zhenghao Chen", "Jianxin Pang", "Dong Xu"], "title": "On-Device Diffusion Transformer Policy for Efficient Robot Manipulation", "comment": "ICCV 2025", "summary": "Diffusion Policies have significantly advanced robotic manipulation tasks via\nimitation learning, but their application on resource-constrained mobile\nplatforms remains challenging due to computational inefficiency and extensive\nmemory footprint. In this paper, we propose LightDP, a novel framework\nspecifically designed to accelerate Diffusion Policies for real-time deployment\non mobile devices. LightDP addresses the computational bottleneck through two\ncore strategies: network compression of the denoising modules and reduction of\nthe required sampling steps. We first conduct an extensive computational\nanalysis on existing Diffusion Policy architectures, identifying the denoising\nnetwork as the primary contributor to latency. To overcome performance\ndegradation typically associated with conventional pruning methods, we\nintroduce a unified pruning and retraining pipeline, optimizing the model's\npost-pruning recoverability explicitly. Furthermore, we combine pruning\ntechniques with consistency distillation to effectively reduce sampling steps\nwhile maintaining action prediction accuracy. Experimental evaluations on the\nstandard datasets, \\ie, PushT, Robomimic, CALVIN, and LIBERO, demonstrate that\nLightDP achieves real-time action prediction on mobile devices with competitive\nperformance, marking an important step toward practical deployment of\ndiffusion-based policies in resource-limited environments. Extensive real-world\nexperiments also show the proposed LightDP can achieve performance comparable\nto state-of-the-art Diffusion Policies.", "AI": {"tldr": "LightDP\u662f\u4e00\u79cd\u4e13\u4e3a\u79fb\u52a8\u8bbe\u5907\u8bbe\u8ba1\u7684\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u901a\u8fc7\u7f51\u7edc\u538b\u7f29\u548c\u51cf\u5c11\u91c7\u6837\u6b65\u9aa4\u52a0\u901f\u6269\u6563\u7b56\u7565\uff0c\u5b9e\u73b0\u5b9e\u65f6\u52a8\u4f5c\u9884\u6d4b\u3002", "motivation": "\u6269\u6563\u7b56\u7565\u5728\u8d44\u6e90\u53d7\u9650\u7684\u79fb\u52a8\u5e73\u53f0\u4e0a\u5e94\u7528\u65f6\u5b58\u5728\u8ba1\u7b97\u6548\u7387\u4f4e\u548c\u5185\u5b58\u5360\u7528\u5927\u7684\u95ee\u9898\uff0cLightDP\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5206\u6790\u6269\u6563\u7b56\u7565\u67b6\u6784\uff0c\u8bc6\u522b\u53bb\u566a\u7f51\u7edc\u4e3a\u5ef6\u8fdf\u4e3b\u56e0\uff0c\u5f15\u5165\u7edf\u4e00\u526a\u679d\u548c\u518d\u8bad\u7ec3\u6d41\u7a0b\u4f18\u5316\u6a21\u578b\uff0c\u5e76\u7ed3\u5408\u4e00\u81f4\u6027\u84b8\u998f\u51cf\u5c11\u91c7\u6837\u6b65\u9aa4\u3002", "result": "\u5728\u591a\u4e2a\u6807\u51c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cLightDP\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u5b9e\u73b0\u5b9e\u65f6\u52a8\u4f5c\u9884\u6d4b\uff0c\u6027\u80fd\u63a5\u8fd1\u73b0\u6709\u6700\u4f73\u6269\u6563\u7b56\u7565\u3002", "conclusion": "LightDP\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u6269\u6563\u7b56\u7565\u7684\u5b9e\u9645\u90e8\u7f72\u63d0\u4f9b\u4e86\u91cd\u8981\u8fdb\u5c55\u3002"}}
{"id": "2508.00581", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00581", "abs": "https://arxiv.org/abs/2508.00581", "authors": ["Ruiqing Ding", "Qianfang Sun", "Yongkang Leng", "Hui Yin", "Xiaojian Li"], "title": "From EMR Data to Clinical Insight: An LLM-Driven Framework for Automated Pre-Consultation Questionnaire Generation", "comment": "16 pages, 10 figures", "summary": "Pre-consultation is a critical component of effective healthcare delivery.\nHowever, generating comprehensive pre-consultation questionnaires from complex,\nvoluminous Electronic Medical Records (EMRs) is a challenging task. Direct\nLarge Language Model (LLM) approaches face difficulties in this task,\nparticularly regarding information completeness, logical order, and\ndisease-level synthesis. To address this issue, we propose a novel multi-stage\nLLM-driven framework: Stage 1 extracts atomic assertions (key facts with\ntiming) from EMRs; Stage 2 constructs personal causal networks and synthesizes\ndisease knowledge by clustering representative networks from an EMR corpus;\nStage 3 generates tailored personal and standardized disease-specific\nquestionnaires based on these structured representations. This framework\novercomes limitations of direct methods by building explicit clinical\nknowledge. Evaluated on a real-world EMR dataset and validated by clinical\nexperts, our method demonstrates superior performance in information coverage,\ndiagnostic relevance, understandability, and generation time, highlighting its\npractical potential to enhance patient information collection.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u9636\u6bb5LLM\u9a71\u52a8\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u590d\u6742\u7535\u5b50\u75c5\u5386\u751f\u6210\u5168\u9762\u7684\u9884\u54a8\u8be2\u95ee\u5377\uff0c\u89e3\u51b3\u4e86\u76f4\u63a5LLM\u65b9\u6cd5\u5728\u4fe1\u606f\u5b8c\u6574\u6027\u3001\u903b\u8f91\u987a\u5e8f\u548c\u75be\u75c5\u7ea7\u5408\u6210\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u9884\u54a8\u8be2\u662f\u533b\u7597\u4fdd\u5065\u7684\u5173\u952e\u73af\u8282\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u4ece\u590d\u6742\u7535\u5b50\u75c5\u5386\u4e2d\u751f\u6210\u5168\u9762\u7684\u95ee\u5377\uff0c\u5c24\u5176\u662f\u76f4\u63a5LLM\u65b9\u6cd5\u5728\u4fe1\u606f\u5b8c\u6574\u6027\u548c\u903b\u8f91\u6027\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u91c7\u7528\u4e09\u9636\u6bb5\u6846\u67b6\uff1a1\uff09\u63d0\u53d6\u539f\u5b50\u65ad\u8a00\uff1b2\uff09\u6784\u5efa\u4e2a\u4eba\u56e0\u679c\u7f51\u7edc\u5e76\u5408\u6210\u75be\u75c5\u77e5\u8bc6\uff1b3\uff09\u751f\u6210\u4e2a\u6027\u5316\u53ca\u6807\u51c6\u5316\u95ee\u5377\u3002", "result": "\u5728\u771f\u5b9e\u7535\u5b50\u75c5\u5386\u6570\u636e\u96c6\u548c\u4e34\u5e8a\u4e13\u5bb6\u9a8c\u8bc1\u4e0b\uff0c\u8be5\u65b9\u6cd5\u5728\u4fe1\u606f\u8986\u76d6\u3001\u8bca\u65ad\u76f8\u5173\u6027\u3001\u53ef\u7406\u89e3\u6027\u548c\u751f\u6210\u65f6\u95f4\u4e0a\u8868\u73b0\u4f18\u8d8a\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u6784\u5efa\u663e\u5f0f\u4e34\u5e8a\u77e5\u8bc6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9884\u54a8\u8be2\u95ee\u5377\u7684\u8d28\u91cf\u548c\u6548\u7387\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.00795", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.00795", "abs": "https://arxiv.org/abs/2508.00795", "authors": ["Junbang Liang", "Pavel Tokmakov", "Ruoshi Liu", "Sruthi Sudhakar", "Paarth Shah", "Rares Ambrus", "Carl Vondrick"], "title": "Video Generators are Robot Policies", "comment": null, "summary": "Despite tremendous progress in dexterous manipulation, current visuomotor\npolicies remain fundamentally limited by two challenges: they struggle to\ngeneralize under perceptual or behavioral distribution shifts, and their\nperformance is constrained by the size of human demonstration data. In this\npaper, we use video generation as a proxy for robot policy learning to address\nboth limitations simultaneously. We propose Video Policy, a modular framework\nthat combines video and action generation that can be trained end-to-end. Our\nresults demonstrate that learning to generate videos of robot behavior allows\nfor the extraction of policies with minimal demonstration data, significantly\nimproving robustness and sample efficiency. Our method shows strong\ngeneralization to unseen objects, backgrounds, and tasks, both in simulation\nand the real world. We further highlight that task success is closely tied to\nthe generated video, with action-free video data providing critical benefits\nfor generalizing to novel tasks. By leveraging large-scale video generative\nmodels, we achieve superior performance compared to traditional behavior\ncloning, paving the way for more scalable and data-efficient robot policy\nlearning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVideo Policy\u7684\u6a21\u5757\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u89c6\u9891\u751f\u6210\u4f5c\u4e3a\u673a\u5668\u4eba\u7b56\u7565\u5b66\u4e60\u7684\u4ee3\u7406\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u5728\u6cdb\u5316\u548c\u6570\u636e\u6548\u7387\u4e0a\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u5728\u611f\u77e5\u548c\u884c\u4e3a\u5206\u5e03\u53d8\u5316\u4e0b\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\uff0c\u4e14\u4f9d\u8d56\u5927\u91cf\u4eba\u7c7b\u6f14\u793a\u6570\u636e\uff0c\u9650\u5236\u4e86\u5176\u6027\u80fd\u3002", "method": "\u7ed3\u5408\u89c6\u9891\u548c\u52a8\u4f5c\u751f\u6210\u7684\u7aef\u5230\u7aef\u8bad\u7ec3\u6846\u67b6\uff0c\u5229\u7528\u89c6\u9891\u751f\u6210\u6a21\u578b\u63d0\u53d6\u7b56\u7565\uff0c\u51cf\u5c11\u5bf9\u6f14\u793a\u6570\u636e\u7684\u4f9d\u8d56\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u4e2d\u8868\u73b0\u51fa\u5bf9\u672a\u89c1\u7269\u4f53\u3001\u80cc\u666f\u548c\u4efb\u52a1\u7684\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u901a\u8fc7\u5927\u89c4\u6a21\u89c6\u9891\u751f\u6210\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u6bd4\u4f20\u7edf\u884c\u4e3a\u514b\u9686\u66f4\u4f18\u7684\u6027\u80fd\uff0c\u4e3a\u53ef\u6269\u5c55\u548c\u6570\u636e\u9ad8\u6548\u7684\u673a\u5668\u4eba\u7b56\u7565\u5b66\u4e60\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2508.00632", "categories": ["cs.AI", "cs.MA", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.00632", "abs": "https://arxiv.org/abs/2508.00632", "authors": ["Alexia Jolicoeur-Martineau"], "title": "Multi-Agent Game Generation and Evaluation via Audio-Visual Recordings", "comment": null, "summary": "While AI excels at generating text, audio, images, and videos, creating\ninteractive audio-visual content such as video games remains challenging.\nCurrent LLMs can generate JavaScript games and animations, but lack automated\nevaluation metrics and struggle with complex content that normally requires\nteams of humans working for many months (multi-shot, multi-agents) using assets\nmade by artists. To tackle these issues, we built a new metric and a\nmulti-agent system.\n  We propose AVR-Eval, a relative metric for multimedia content quality using\nAudio-Visual Recordings (AVRs). An omni-modal model (processing text, video,\nand audio) compares the AVRs of two contents, with a text model reviewing\nevaluations to determine superiority. We show that AVR-Eval properly identifies\ngood from broken or mismatched content.\n  We built AVR-Agent, a multi-agent system generating JavaScript code from a\nbank of multimedia assets (audio, images, 3D models). The coding agent selects\nrelevant assets, generates multiple initial codes, uses AVR-Eval to identify\nthe best version, and iteratively improves it through omni-modal agent feedback\nfrom the AVR.\n  We run experiments on games and animations with AVR-Eval (win rate of content\nA against B). We find that content generated by AVR-Agent has a significantly\nhigher win rate against content made through one-shot generation. However,\nmodels struggle to leverage custom assets and AVR feedback effectively, showing\nno higher win rate. This reveals a critical gap: while humans benefit from\nhigh-quality assets and audio-visual feedback, current coding models do not\nseem to utilize these resources as effectively, highlighting fundamental\ndifferences between human and machine content creation approaches.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faAVR-Eval\u8bc4\u4f30\u6307\u6807\u548cAVR-Agent\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u7528\u4e8e\u751f\u6210\u548c\u8bc4\u4f30\u4ea4\u4e92\u5f0f\u591a\u5a92\u4f53\u5185\u5bb9\uff08\u5982\u6e38\u620f\uff09\uff0c\u89e3\u51b3\u4e86\u73b0\u6709LLMs\u5728\u590d\u6742\u5185\u5bb9\u751f\u6210\u548c\u81ea\u52a8\u5316\u8bc4\u4f30\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u5f53\u524dAI\u5728\u751f\u6210\u4ea4\u4e92\u5f0f\u591a\u5a92\u4f53\u5185\u5bb9\uff08\u5982\u89c6\u9891\u6e38\u620f\uff09\u65f6\u9762\u4e34\u6311\u6218\uff0c\u7f3a\u4e4f\u81ea\u52a8\u5316\u8bc4\u4f30\u6307\u6807\u4e14\u96be\u4ee5\u5904\u7406\u590d\u6742\u5185\u5bb9\u3002", "method": "\u63d0\u51faAVR-Eval\u8bc4\u4f30\u6307\u6807\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u6a21\u578b\u6bd4\u8f83\u5185\u5bb9\u8d28\u91cf\uff1b\u5f00\u53d1AVR-Agent\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff0c\u751f\u6210\u5e76\u8fed\u4ee3\u4f18\u5316JavaScript\u4ee3\u7801\u3002", "result": "AVR-Agent\u751f\u6210\u7684\u5185\u5bb9\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u5355\u6b21\u751f\u6210\u5185\u5bb9\uff0c\u4f46\u6a21\u578b\u672a\u80fd\u6709\u6548\u5229\u7528\u81ea\u5b9a\u4e49\u8d44\u6e90\u548c\u53cd\u9988\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5f53\u524d\u6a21\u578b\u5728\u5229\u7528\u9ad8\u8d28\u91cf\u8d44\u6e90\u548c\u53cd\u9988\u65b9\u9762\u4e0e\u4eba\u7c7b\u5b58\u5728\u5dee\u8ddd\uff0c\u63ed\u793a\u4e86\u673a\u5668\u4e0e\u4eba\u7c7b\u5185\u5bb9\u521b\u4f5c\u7684\u6839\u672c\u5dee\u5f02\u3002"}}
{"id": "2508.00658", "categories": ["cs.AI", "cs.LG", "econ.EM", "stat.ME"], "pdf": "https://arxiv.org/pdf/2508.00658", "abs": "https://arxiv.org/abs/2508.00658", "authors": ["Chakattrai Sookkongwaree", "Tattep Lakmuang", "Chainarong Amornbunchornvej"], "title": "Multi-Band Variable-Lag Granger Causality: A Unified Framework for Causal Time Series Inference across Frequencies", "comment": "First draft", "summary": "Understanding causal relationships in time series is fundamental to many\ndomains, including neuroscience, economics, and behavioral science. Granger\ncausality is one of the well-known techniques for inferring causality in time\nseries. Typically, Granger causality frameworks have a strong fix-lag\nassumption between cause and effect, which is often unrealistic in complex\nsystems. While recent work on variable-lag Granger causality (VLGC) addresses\nthis limitation by allowing a cause to influence an effect with different time\nlags at each time point, it fails to account for the fact that causal\ninteractions may vary not only in time delay but also across frequency bands.\nFor example, in brain signals, alpha-band activity may influence another region\nwith a shorter delay than slower delta-band oscillations. In this work, we\nformalize Multi-Band Variable-Lag Granger Causality (MB-VLGC) and propose a\nnovel framework that generalizes traditional VLGC by explicitly modeling\nfrequency-dependent causal delays. We provide a formal definition of MB-VLGC,\ndemonstrate its theoretical soundness, and propose an efficient inference\npipeline. Extensive experiments across multiple domains demonstrate that our\nframework significantly outperforms existing methods on both synthetic and\nreal-world datasets, confirming its broad applicability to any type of time\nseries data. Code and datasets are publicly available.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u9891\u5e26\u53ef\u53d8\u6ede\u540eGranger\u56e0\u679c\u5173\u7cfb\uff08MB-VLGC\uff09\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfGranger\u56e0\u679c\u5173\u7cfb\u5728\u590d\u6742\u7cfb\u7edf\u4e2d\u56fa\u5b9a\u6ede\u540e\u5047\u8bbe\u7684\u5c40\u9650\u6027\uff0c\u5e76\u8003\u8651\u4e86\u9891\u7387\u4f9d\u8d56\u7684\u56e0\u679c\u5ef6\u8fdf\u3002", "motivation": "\u4f20\u7edfGranger\u56e0\u679c\u5173\u7cfb\u5047\u8bbe\u56fa\u5b9a\u6ede\u540e\uff0c\u800c\u53ef\u53d8\u6ede\u540eGranger\u56e0\u679c\u5173\u7cfb\uff08VLGC\uff09\u867d\u89e3\u51b3\u4e86\u6ede\u540e\u53ef\u53d8\u95ee\u9898\uff0c\u4f46\u672a\u8003\u8651\u9891\u7387\u5e26\u7684\u5f71\u54cd\u3002\u8111\u4fe1\u53f7\u7b49\u590d\u6742\u7cfb\u7edf\u4e2d\uff0c\u56e0\u679c\u5ef6\u8fdf\u53ef\u80fd\u968f\u9891\u7387\u5e26\u53d8\u5316\u3002", "method": "\u63d0\u51faMB-VLGC\u6846\u67b6\uff0c\u660e\u786e\u5efa\u6a21\u9891\u7387\u4f9d\u8d56\u7684\u56e0\u679c\u5ef6\u8fdf\uff0c\u63d0\u4f9b\u7406\u8bba\u5b9a\u4e49\u5e76\u8bbe\u8ba1\u9ad8\u6548\u63a8\u7406\u6d41\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMB-VLGC\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\u3002", "conclusion": "MB-VLGC\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u9891\u7387\u4f9d\u8d56\u7684\u56e0\u679c\u5ef6\u8fdf\uff0c\u663e\u8457\u63d0\u5347\u4e86Granger\u56e0\u679c\u5173\u7cfb\u5728\u590d\u6742\u7cfb\u7edf\u4e2d\u7684\u9002\u7528\u6027\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2508.00665", "categories": ["cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00665", "abs": "https://arxiv.org/abs/2508.00665", "authors": ["Maryam Mosleh", "Marie Devlin", "Ellis Solaiman"], "title": "Transparent Adaptive Learning via Data-Centric Multimodal Explainable AI", "comment": null, "summary": "Artificial intelligence-driven adaptive learning systems are reshaping\neducation through data-driven adaptation of learning experiences. Yet many of\nthese systems lack transparency, offering limited insight into how decisions\nare made. Most explainable AI (XAI) techniques focus on technical outputs but\nneglect user roles and comprehension. This paper proposes a hybrid framework\nthat integrates traditional XAI techniques with generative AI models and user\npersonalisation to generate multimodal, personalised explanations tailored to\nuser needs. We redefine explainability as a dynamic communication process\ntailored to user roles and learning goals. We outline the framework's design,\nkey XAI limitations in education, and research directions on accuracy,\nfairness, and personalisation. Our aim is to move towards explainable AI that\nenhances transparency while supporting user-centred experiences.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4f20\u7edf\u53ef\u89e3\u91caAI\u6280\u672f\u4e0e\u751f\u6210\u5f0fAI\u6a21\u578b\u7684\u6df7\u5408\u6846\u67b6\uff0c\u65e8\u5728\u4e3a\u6559\u80b2\u9886\u57df\u63d0\u4f9b\u900f\u660e\u4e14\u4e2a\u6027\u5316\u7684\u89e3\u91ca\u3002", "motivation": "\u5f53\u524d\u81ea\u9002\u5e94\u5b66\u4e60\u7cfb\u7edf\u7f3a\u4e4f\u900f\u660e\u5ea6\uff0c\u4e14\u73b0\u6709\u53ef\u89e3\u91caAI\u6280\u672f\u5ffd\u89c6\u7528\u6237\u89d2\u8272\u548c\u7406\u89e3\u80fd\u529b\uff0c\u4e9f\u9700\u6539\u8fdb\u3002", "method": "\u8bbe\u8ba1\u6df7\u5408\u6846\u67b6\uff0c\u6574\u5408\u4f20\u7edfXAI\u6280\u672f\u4e0e\u751f\u6210\u5f0fAI\u6a21\u578b\uff0c\u7ed3\u5408\u7528\u6237\u4e2a\u6027\u5316\u9700\u6c42\u751f\u6210\u591a\u6a21\u6001\u89e3\u91ca\u3002", "result": "\u91cd\u65b0\u5b9a\u4e49\u53ef\u89e3\u91ca\u6027\u4e3a\u52a8\u6001\u6c9f\u901a\u8fc7\u7a0b\uff0c\u5e76\u63d0\u51fa\u6846\u67b6\u8bbe\u8ba1\u53ca\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u76ee\u6807\u662f\u5b9e\u73b0\u589e\u5f3a\u900f\u660e\u5ea6\u5e76\u652f\u6301\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u4f53\u9a8c\u7684\u53ef\u89e3\u91caAI\u3002"}}
{"id": "2508.00674", "categories": ["cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.00674", "abs": "https://arxiv.org/abs/2508.00674", "authors": ["Banan Alkhateeb", "Ellis Solaiman"], "title": "Context-Aware Visualization for Explainable AI Recommendations in Social Media: A Vision for User-Aligned Explanations", "comment": null, "summary": "Social media platforms today strive to improve user experience through AI\nrecommendations, yet the value of such recommendations vanishes as users do not\nunderstand the reasons behind them. This issue arises because explainability in\nsocial media is general and lacks alignment with user-specific needs. In this\nvision paper, we outline a user-segmented and context-aware explanation layer\nby proposing a visual explanation system with diverse explanation methods. The\nproposed system is framed by the variety of user needs and contexts, showing\nexplanations in different visualized forms, including a technically detailed\nversion for AI experts and a simplified one for lay users. Our framework is the\nfirst to jointly adapt explanation style (visual vs. numeric) and granularity\n(expert vs. lay) inside a single pipeline. A public pilot with 30 X users will\nvalidate its impact on decision-making and trust.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u6237\u5206\u6bb5\u7684\u89c6\u89c9\u89e3\u91ca\u7cfb\u7edf\uff0c\u901a\u8fc7\u591a\u6837\u5316\u7684\u89e3\u91ca\u65b9\u6cd5\u63d0\u5347\u793e\u4ea4\u5a92\u4f53\u7684\u63a8\u8350\u900f\u660e\u5ea6\u3002", "motivation": "\u5f53\u524d\u793e\u4ea4\u5a92\u4f53\u63a8\u8350\u7cfb\u7edf\u7684\u89e3\u91ca\u6027\u4e0d\u8db3\uff0c\u7f3a\u4e4f\u4e0e\u7528\u6237\u7279\u5b9a\u9700\u6c42\u7684\u5339\u914d\uff0c\u5bfc\u81f4\u7528\u6237\u5bf9\u63a8\u8350\u5185\u5bb9\u7684\u7406\u89e3\u548c\u4fe1\u4efb\u5ea6\u4e0b\u964d\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u7528\u6237\u5206\u6bb5\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u89e3\u91ca\u5c42\uff0c\u63d0\u4f9b\u4e0d\u540c\u89c6\u89c9\u5316\u5f62\u5f0f\u7684\u89e3\u91ca\uff08\u5982\u6280\u672f\u8be6\u7ec6\u7248\u548c\u7b80\u5316\u7248\uff09\uff0c\u5e76\u9996\u6b21\u5c06\u89e3\u91ca\u98ce\u683c\uff08\u89c6\u89c9 vs. \u6570\u5b57\uff09\u548c\u7c92\u5ea6\uff08\u4e13\u5bb6 vs. \u666e\u901a\u7528\u6237\uff09\u7ed3\u5408\u5728\u4e00\u4e2a\u6846\u67b6\u4e2d\u3002", "result": "\u901a\u8fc730\u540dX\u7528\u6237\u7684\u516c\u5f00\u8bd5\u70b9\u9a8c\u8bc1\u5176\u5bf9\u51b3\u7b56\u548c\u4fe1\u4efb\u7684\u5f71\u54cd\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u793e\u4ea4\u5a92\u4f53\u63a8\u8350\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u900f\u660e\u3001\u7528\u6237\u53cb\u597d\u7684\u89e3\u91ca\u65b9\u6cd5\uff0c\u6709\u671b\u63d0\u5347\u7528\u6237\u4f53\u9a8c\u548c\u4fe1\u4efb\u3002"}}
{"id": "2508.00784", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.00784", "abs": "https://arxiv.org/abs/2508.00784", "authors": ["Tom Or", "Omri Azencot"], "title": "Unraveling Hidden Representations: A Multi-Modal Layer Analysis for Better Synthetic Content Forensics", "comment": null, "summary": "Generative models achieve remarkable results in multiple data domains,\nincluding images and texts, among other examples. Unfortunately, malicious\nusers exploit synthetic media for spreading misinformation and disseminating\ndeepfakes. Consequently, the need for robust and stable fake detectors is\npressing, especially when new generative models appear everyday. While the\nmajority of existing work train classifiers that discriminate between real and\nfake information, such tools typically generalize only within the same family\nof generators and data modalities, yielding poor results on other generative\nclasses and data domains. Towards a universal classifier, we propose the use of\nlarge pre-trained multi-modal models for the detection of generative content.\nEffectively, we show that the latent code of these models naturally captures\ninformation discriminating real from fake. Building on this observation, we\ndemonstrate that linear classifiers trained on these features can achieve\nstate-of-the-art results across various modalities, while remaining\ncomputationally efficient, fast to train, and effective even in few-shot\nsettings. Our work primarily focuses on fake detection in audio and images,\nachieving performance that surpasses or matches that of strong baseline\nmethods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u578b\u9884\u8bad\u7ec3\u591a\u6a21\u6001\u6a21\u578b\u7684\u901a\u7528\u751f\u6210\u5185\u5bb9\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u5176\u6f5c\u5728\u7f16\u7801\u533a\u5206\u771f\u5b9e\u4e0e\u865a\u5047\u5185\u5bb9\uff0c\u5b9e\u73b0\u4e86\u8de8\u6a21\u6001\u7684\u9ad8\u6548\u68c0\u6d4b\u3002", "motivation": "\u751f\u6210\u6a21\u578b\u5728\u591a\u4e2a\u9886\u57df\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u88ab\u6076\u610f\u7528\u4e8e\u4f20\u64ad\u865a\u5047\u4fe1\u606f\uff0c\u73b0\u6709\u68c0\u6d4b\u5de5\u5177\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u4e9f\u9700\u901a\u7528\u4e14\u7a33\u5b9a\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u5927\u578b\u9884\u8bad\u7ec3\u591a\u6a21\u6001\u6a21\u578b\u7684\u6f5c\u5728\u7f16\u7801\u7279\u5f81\uff0c\u8bad\u7ec3\u7ebf\u6027\u5206\u7c7b\u5668\u8fdb\u884c\u751f\u6210\u5185\u5bb9\u68c0\u6d4b\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u97f3\u9891\u548c\u56fe\u50cf\u9886\u57df\u8868\u73b0\u4f18\u5f02\uff0c\u8ba1\u7b97\u9ad8\u6548\u4e14\u9002\u7528\u4e8e\u5c11\u6837\u672c\u573a\u666f\uff0c\u6027\u80fd\u4f18\u4e8e\u6216\u5339\u914d\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6f5c\u5728\u7f16\u7801\u80fd\u6709\u6548\u533a\u5206\u771f\u5b9e\u4e0e\u865a\u5047\u5185\u5bb9\uff0c\u4e3a\u901a\u7528\u751f\u6210\u5185\u5bb9\u68c0\u6d4b\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
