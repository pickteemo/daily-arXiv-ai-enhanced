{"id": "2508.13167", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.13167", "abs": "https://arxiv.org/abs/2508.13167", "authors": ["Weizhen Li", "Jianbo Lin", "Zhuosong Jiang", "Jingyi Cao", "Xinpeng Liu", "Jiayu Zhang", "Zhenqiang Huang", "Qianben Chen", "Weichen Sun", "Qiexiang Wang", "Hongxuan Lu", "Tianrui Qin", "Chenghao Zhu", "Yi Yao", "Shuying Fan", "Xiaowan Li", "Tiannan Wang", "Pai Liu", "King Zhu", "He Zhu", "Dingfeng Shi", "Piaohong Wang", "Yeyi Guan", "Xiangru Tang", "Minghao Liu", "Yuchen Eleanor Jiang", "Jian Yang", "Jiaheng Liu", "Ge Zhang", "Wangchunshu Zhou"], "title": "Chain-of-Agents: End-to-End Agent Foundation Models via Multi-Agent Distillation and Agentic RL", "comment": "51 pages", "summary": "Recent advances in large language models (LLMs) and multi-agent systems have\ndemonstrated remarkable capabilities in complex problem-solving tasks such as\ndeep research, vibe coding, and mathematical reasoning. However, most existing\nmulti-agent systems are built upon manual prompt/workflow engineering with\nsophisticated agent frameworks, making them computationally inefficient, less\ncapable, and can not benefit from data-centric learning. In this work, we\nintroduce Chain-of-Agents (CoA), a novel paradigm of LLM reasoning that enables\nnative end-to-end complex problem-solving in the same way as a multi-agent\nsystem (i.e., multi-turn problem solving with multiple tools and multiple\nagents) within one model. In chain-of-agents problem-solving, the model\ndynamically activates different tool agents and role-playing agents to simulate\nmulti-agent collaboration in an end-to-end fashion. To elicit end-to-end\nchain-of-agents problem-solving abilities in LLMs, we introduce a multi-agent\ndistillation framework to distill state-of-the-art multi-agent systems into\nchain-of-agents trajectories for agentic supervised fine-tuning. We then use\nagentic reinforcement learning on verifiable agentic tasks to further improve\nthe models' capabilities on chain-of-agents problem solving. We call the\nresulting models Agent Foundation Models (AFMs). Our empirical studies\ndemonstrate that AFM establishes new state-of-the-art performance across\ndiverse benchmarks in both web agent and code agent settings. We make the\nentire research, including the model weights, code for training and evaluation,\nand the training data, fully open-sourced, which offers a solid starting point\nfor future research on agent models and agentic RL.", "AI": {"tldr": "\u63d0\u51faChain-of-Agents(CoA)\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u591a\u667a\u80fd\u4f53\u84b8\u998f\u548c\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3Agent Foundation Models(AFMs)\uff0c\u5728\u5355\u4e00\u6a21\u578b\u5185\u5b9e\u73b0\u7aef\u5230\u7aef\u590d\u6742\u95ee\u9898\u89e3\u51b3\uff0c\u6027\u80fd\u8fbe\u5230SOTA\u3002", "motivation": "\u73b0\u6709\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4f9d\u8d56\u624b\u52a8\u63d0\u793a\u5de5\u7a0b\uff0c\u8ba1\u7b97\u6548\u7387\u4f4e\u3001\u80fd\u529b\u6709\u9650\u4e14\u65e0\u6cd5\u4ece\u6570\u636e\u9a71\u52a8\u5b66\u4e60\u4e2d\u53d7\u76ca\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u7aef\u5230\u7aef\u89e3\u51b3\u65b9\u6848\u3002", "method": "1) \u591a\u667a\u80fd\u4f53\u84b8\u998f\u6846\u67b6\u5c06\u5148\u8fdb\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u84b8\u998f\u4e3aCoA\u8f68\u8ff9\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff1b2) \u5728\u53ef\u9a8c\u8bc1\u667a\u80fd\u4f53\u4efb\u52a1\u4e0a\u4f7f\u7528\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u8fdb\u4e00\u6b65\u63d0\u5347\u80fd\u529b\uff1b3) \u6784\u5efaAgent Foundation Models(AFMs)\u3002", "result": "AFM\u5728\u7f51\u9875\u667a\u80fd\u4f53\u548c\u4ee3\u7801\u667a\u80fd\u4f53\u7b49\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5efa\u7acb\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "CoA\u8303\u5f0f\u6210\u529f\u5b9e\u73b0\u4e86\u5355\u4e00\u6a21\u578b\u5185\u7684\u7aef\u5230\u7aef\u590d\u6742\u95ee\u9898\u89e3\u51b3\uff0cAFM\u6a21\u578b\u5f00\u6e90\u4e3a\u672a\u6765\u667a\u80fd\u4f53\u6a21\u578b\u548c\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7814\u7a76\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\u3002"}}
{"id": "2508.13171", "categories": ["cs.AI", "cs.CL", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2508.13171", "abs": "https://arxiv.org/abs/2508.13171", "authors": ["Tao An"], "title": "Cognitive Workspace: Active Memory Management for LLMs -- An Empirical Study of Functional Infinite Context", "comment": "13 pages, 1 figure, code available at\n  https://github.com/tao-hpu/cognitive-workspace", "summary": "Large Language Models (LLMs) face fundamental limitations in context\nmanagement despite recent advances extending context windows to millions of\ntokens. We propose Cognitive Workspace, a novel paradigm that transcends\ntraditional Retrieval-Augmented Generation (RAG) by emulating human cognitive\nmechanisms of external memory use. Drawing from cognitive science foundations\nincluding Baddeley's working memory model, Clark's extended mind thesis, and\nHutchins' distributed cognition framework, we demonstrate that current passive\nretrieval systems fail to capture the dynamic, task-driven nature of human\nmemory management. Our analysis of 2024-2025 developments reveals that while\ntechniques like Infini-attention and StreamingLLM achieve impressive context\nlengths, they lack the metacognitive awareness and active planning capabilities\nessential for true cognitive extension. Cognitive Workspace addresses these\nlimitations through three core innovations: (1) active memory management with\ndeliberate information curation, (2) hierarchical cognitive buffers enabling\npersistent working states, and (3) task-driven context optimization that\ndynamically adapts to cognitive demands. Empirical validation demonstrates\nCognitive Workspace achieves an average 58.6% memory reuse rate (ranging from\n54-60% across different tasks) compared to 0% for traditional RAG, with 17-18%\nnet efficiency gain despite 3.3x higher operation counts. Statistical analysis\nconfirms these advantages with p < 0.001 and Cohen's d > 23 across multiple\ntask types, establishing the first quantitative evidence for active memory\nsuperiority in LLM systems. We present a comprehensive theoretical framework\nsynthesizing insights from 50+ recent papers, positioning Cognitive Workspace\nas a fundamental shift from information retrieval to genuine cognitive\naugmentation.", "AI": {"tldr": "\u63d0\u51fa\u4e86Cognitive Workspace\u65b0\u8303\u5f0f\uff0c\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u5916\u90e8\u8bb0\u5fc6\u673a\u5236\uff0c\u89e3\u51b3\u4e86LLM\u4e0a\u4e0b\u6587\u7ba1\u7406\u7684\u6839\u672c\u9650\u5236\uff0c\u76f8\u6bd4\u4f20\u7edfRAG\u5b9e\u73b0\u4e8658.6%\u7684\u5185\u5b58\u91cd\u7528\u7387\u548c17-18%\u7684\u6548\u7387\u63d0\u5347", "motivation": "\u73b0\u6709LLM\u5c3d\u7ba1\u4e0a\u4e0b\u6587\u7a97\u53e3\u6269\u5c55\u5230\u767e\u4e07token\uff0c\u4f46\u4ecd\u7f3a\u4e4f\u4eba\u7c7b\u8ba4\u77e5\u7684\u52a8\u6001\u4efb\u52a1\u9a71\u52a8\u8bb0\u5fc6\u7ba1\u7406\u80fd\u529b\uff0c\u88ab\u52a8\u68c0\u7d22\u7cfb\u7edf\u65e0\u6cd5\u5b9e\u73b0\u771f\u6b63\u7684\u8ba4\u77e5\u6269\u5c55", "method": "\u57fa\u4e8e\u8ba4\u77e5\u79d1\u5b66\u7406\u8bba\uff08Baddeley\u5de5\u4f5c\u8bb0\u5fc6\u6a21\u578b\u3001Clark\u6269\u5c55\u5fc3\u667a\u7406\u8bba\u7b49\uff09\uff0c\u63d0\u51fa\u4e09\u4e2a\u6838\u5fc3\u521b\u65b0\uff1a\u4e3b\u52a8\u8bb0\u5fc6\u7ba1\u7406\u3001\u5206\u5c42\u8ba4\u77e5\u7f13\u51b2\u533a\u548c\u4efb\u52a1\u9a71\u52a8\u7684\u4e0a\u4e0b\u6587\u4f18\u5316", "result": "\u5b9e\u8bc1\u9a8c\u8bc1\u663e\u793a58.6%\u5e73\u5747\u5185\u5b58\u91cd\u7528\u7387\uff08\u4f20\u7edfRAG\u4e3a0%\uff09\uff0c17-18%\u51c0\u6548\u7387\u589e\u76ca\uff0c\u7edf\u8ba1\u663e\u8457\uff08p<0.001\uff0cCohen's d>23\uff09", "conclusion": "Cognitive Workspace\u4ee3\u8868\u4e86\u4ece\u4fe1\u606f\u68c0\u7d22\u5230\u771f\u6b63\u8ba4\u77e5\u589e\u5f3a\u7684\u6839\u672c\u6027\u8f6c\u53d8\uff0c\u4e3aLLM\u7cfb\u7edf\u63d0\u4f9b\u4e86\u9996\u4e2a\u4e3b\u52a8\u8bb0\u5fc6\u4f18\u52bf\u7684\u91cf\u5316\u8bc1\u636e"}}
{"id": "2508.13174", "categories": ["cs.AI", "cs.LG", "q-fin.CP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.13174", "abs": "https://arxiv.org/abs/2508.13174", "authors": ["Hongjun Ding", "Binqi Chen", "Jinsheng Huang", "Taian Guo", "Zhengyang Mao", "Guoyi Shao", "Lutong Zou", "Luchen Liu", "Ming Zhang"], "title": "AlphaEval: A Comprehensive and Efficient Evaluation Framework for Formula Alpha Mining", "comment": "12 pages, 5 figures", "summary": "Formula alpha mining, which generates predictive signals from financial data,\nis critical for quantitative investment. Although various algorithmic\napproaches-such as genetic programming, reinforcement learning, and large\nlanguage models-have significantly expanded the capacity for alpha discovery,\nsystematic evaluation remains a key challenge. Existing evaluation metrics\npredominantly include backtesting and correlation-based measures. Backtesting\nis computationally intensive, inherently sequential, and sensitive to specific\nstrategy parameters. Correlation-based metrics, though efficient, assess only\npredictive ability and overlook other crucial properties such as temporal\nstability, robustness, diversity, and interpretability. Additionally, the\nclosed-source nature of most existing alpha mining models hinders\nreproducibility and slows progress in this field. To address these issues, we\npropose AlphaEval, a unified, parallelizable, and backtest-free evaluation\nframework for automated alpha mining models. AlphaEval assesses the overall\nquality of generated alphas along five complementary dimensions: predictive\npower, stability, robustness to market perturbations, financial logic, and\ndiversity. Extensive experiments across representative alpha mining algorithms\ndemonstrate that AlphaEval achieves evaluation consistency comparable to\ncomprehensive backtesting, while providing more comprehensive insights and\nhigher efficiency. Furthermore, AlphaEval effectively identifies superior\nalphas compared to traditional single-metric screening approaches. All\nimplementations and evaluation tools are open-sourced to promote\nreproducibility and community engagement.", "AI": {"tldr": "AlphaEval\u662f\u4e00\u4e2a\u7edf\u4e00\u3001\u53ef\u5e76\u884c\u3001\u65e0\u9700\u56de\u6d4b\u7684\u81ea\u52a8\u5316alpha\u6316\u6398\u6a21\u578b\u8bc4\u4f30\u6846\u67b6\uff0c\u4ece\u4e94\u4e2a\u7ef4\u5ea6\u5168\u9762\u8bc4\u4f30alpha\u8d28\u91cf\uff0c\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u9ad8\u6548\u5168\u9762\u3002", "motivation": "\u73b0\u6709alpha\u6316\u6398\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u56de\u6d4b\u8ba1\u7b97\u5bc6\u96c6\u4e14\u53c2\u6570\u654f\u611f\uff0c\u76f8\u5173\u6027\u6307\u6807\u53ea\u5173\u6ce8\u9884\u6d4b\u80fd\u529b\u800c\u5ffd\u7565\u5176\u4ed6\u91cd\u8981\u5c5e\u6027\uff0c\u4e14\u5927\u591a\u6570\u6a21\u578b\u95ed\u6e90\u963b\u788d\u4e86\u9886\u57df\u53d1\u5c55\u3002", "method": "\u63d0\u51faAlphaEval\u6846\u67b6\uff0c\u4ece\u9884\u6d4b\u80fd\u529b\u3001\u7a33\u5b9a\u6027\u3001\u5e02\u573a\u6270\u52a8\u9c81\u68d2\u6027\u3001\u91d1\u878d\u903b\u8f91\u6027\u548c\u591a\u6837\u6027\u4e94\u4e2a\u4e92\u8865\u7ef4\u5ea6\u7efc\u5408\u8bc4\u4f30\u751f\u6210\u7684alpha\u4fe1\u53f7\u3002", "result": "\u5b9e\u9a8c\u8868\u660eAlphaEval\u5728\u8bc4\u4f30\u4e00\u81f4\u6027\u4e0a\u4e0e\u5168\u9762\u56de\u6d4b\u76f8\u5f53\uff0c\u4f46\u63d0\u4f9b\u66f4\u5168\u9762\u7684\u6d1e\u5bdf\u548c\u66f4\u9ad8\u6548\u7387\uff0c\u5e76\u80fd\u6709\u6548\u8bc6\u522b\u4f18\u4e8e\u4f20\u7edf\u5355\u6307\u6807\u7b5b\u9009\u7684\u4f18\u8d28alpha\u3002", "conclusion": "AlphaEval\u89e3\u51b3\u4e86alpha\u6316\u6398\u7cfb\u7edf\u8bc4\u4f30\u7684\u5173\u952e\u6311\u6218\uff0c\u6240\u6709\u5b9e\u73b0\u548c\u8bc4\u4f30\u5de5\u5177\u90fd\u5df2\u5f00\u6e90\u4ee5\u4fc3\u8fdb\u53ef\u590d\u73b0\u6027\u548c\u793e\u533a\u53c2\u4e0e\u3002"}}
{"id": "2508.13176", "categories": ["cs.AI", "cs.DB", "68T30 (Primary) 68P15, 03B70 (Secondary)", "I.2.4; H.2.3"], "pdf": "https://arxiv.org/pdf/2508.13176", "abs": "https://arxiv.org/abs/2508.13176", "authors": ["Simon Hosemann", "Jean Christoph Jung", "Carsten Lutz", "Sebastian Rudolph"], "title": "Fitting Ontologies and Constraints to Relational Structures", "comment": "Accepted at the 22nd International Conference on Principles of\n  Knowledge Representation and Reasoning (KR 2025)", "summary": "We study the problem of fitting ontologies and constraints to positive and\nnegative examples that take the form of a finite relational structure. As\nontology and constraint languages, we consider the description logics\n$\\mathcal{E\\mkern-2mu L}$ and $\\mathcal{E\\mkern-2mu LI}$ as well as several\nclasses of tuple-generating dependencies (TGDs): full, guarded,\nfrontier-guarded, frontier-one, and unrestricted TGDs as well as inclusion\ndependencies. We pinpoint the exact computational complexity, design\nalgorithms, and analyze the size of fitting ontologies and TGDs. We also\ninvestigate the related problem of constructing a finite basis of concept\ninclusions / TGDs for a given set of finite structures. While finite bases\nexist for $\\mathcal{E\\mkern-2mu L}$, $\\mathcal{E\\mkern-2mu LI}$, guarded TGDs,\nand inclusion dependencies, they in general do not exist for full,\nfrontier-guarded and frontier-one TGDs.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u57fa\u4e8e\u6b63\u8d1f\u5173\u7cfb\u7ed3\u6784\u793a\u4f8b\u7684\u672c\u4f53\u548c\u7ea6\u675f\u62df\u5408\u95ee\u9898\uff0c\u5206\u6790\u4e86\u591a\u79cd\u63cf\u8ff0\u903b\u8f91\u548cTGDs\u7684\u590d\u6742\u6027\u3001\u7b97\u6cd5\u548c\u62df\u5408\u5927\u5c0f\uff0c\u5e76\u63a2\u8ba8\u4e86\u6709\u9650\u57fa\u7684\u5b58\u5728\u6027\u95ee\u9898\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u4ece\u6b63\u8d1f\u793a\u4f8b\u4e2d\u81ea\u52a8\u5b66\u4e60\u672c\u4f53\u548c\u7ea6\u675f\uff0c\u8fd9\u5bf9\u4e8e\u77e5\u8bc6\u8868\u793a\u548c\u6570\u636e\u5e93\u7cfb\u7edf\u7684\u81ea\u52a8\u5316\u6784\u5efa\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "\u4f7f\u7528\u63cf\u8ff0\u903b\u8f91EL\u548cELI\u4ee5\u53ca\u591a\u79cd\u7c7b\u578b\u7684\u5143\u7ec4\u751f\u6210\u4f9d\u8d56\uff08TGDs\uff09\u4f5c\u4e3a\u672c\u4f53\u548c\u7ea6\u675f\u8bed\u8a00\uff0c\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u7b97\u6cd5\u8bbe\u8ba1\u6765\u7814\u7a76\u62df\u5408\u95ee\u9898\u7684\u8ba1\u7b97\u590d\u6742\u6027\u548c\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u7cbe\u786e\u786e\u5b9a\u4e86\u5404\u79cd\u8bed\u8a00\u4e0b\u62df\u5408\u95ee\u9898\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\uff0c\u8bbe\u8ba1\u4e86\u76f8\u5e94\u7b97\u6cd5\uff0c\u5206\u6790\u4e86\u62df\u5408\u672c\u4f53\u548cTGDs\u7684\u5927\u5c0f\uff0c\u5e76\u53d1\u73b0\u67d0\u4e9bTGDs\u7c7b\u578b\u4e0d\u5b58\u5728\u6709\u9650\u57fa\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u57fa\u4e8e\u793a\u4f8b\u7684\u672c\u4f53\u5b66\u4e60\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u7528\u7b97\u6cd5\uff0c\u63ed\u793a\u4e86\u4e0d\u540c\u8bed\u8a00\u5728\u6709\u9650\u57fa\u5b58\u5728\u6027\u65b9\u9762\u7684\u6839\u672c\u5dee\u5f02\uff0c\u5bf9\u77e5\u8bc6\u8868\u793a\u7cfb\u7edf\u7684\u81ea\u52a8\u5316\u6784\u5efa\u5177\u6709\u91cd\u8981\u6307\u5bfc\u610f\u4e49\u3002"}}
{"id": "2508.13303", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13303", "abs": "https://arxiv.org/abs/2508.13303", "authors": ["Yingfan Zhou", "Philip Sanderink", "Sigurd Jager Lemming", "Cheng Fang"], "title": "Diff-MSM: Differentiable MusculoSkeletal Model for Simultaneous Identification of Human Muscle and Bone Parameters", "comment": "8 pages, 7 figures", "summary": "High-fidelity personalized human musculoskeletal models are crucial for\nsimulating realistic behavior of physically coupled human-robot interactive\nsystems and verifying their safety-critical applications in simulations before\nactual deployment, such as human-robot co-transportation and rehabilitation\nthrough robotic exoskeletons. Identifying subject-specific Hill-type muscle\nmodel parameters and bone dynamic parameters is essential for a personalized\nmusculoskeletal model, but very challenging due to the difficulty of measuring\nthe internal biomechanical variables in vivo directly, especially the joint\ntorques. In this paper, we propose using Differentiable MusculoSkeletal Model\n(Diff-MSM) to simultaneously identify its muscle and bone parameters with an\nend-to-end automatic differentiation technique differentiating from the\nmeasurable muscle activation, through the joint torque, to the resulting\nobservable motion without the need to measure the internal joint torques.\nThrough extensive comparative simulations, the results manifested that our\nproposed method significantly outperformed the state-of-the-art baseline\nmethods, especially in terms of accurate estimation of the muscle parameters\n(i.e., initial guess sampled from a normal distribution with the mean being the\nground truth and the standard deviation being 10% of the ground truth could end\nup with an average of the percentage errors of the estimated values as low as\n0.05%). In addition to human musculoskeletal modeling and simulation, the new\nparameter identification technique with the Diff-MSM has great potential to\nenable new applications in muscle health monitoring, rehabilitation, and sports\nscience.", "AI": {"tldr": "\u63d0\u51fa\u53ef\u5fae\u5206\u808c\u8089\u9aa8\u9abc\u6a21\u578b(Diff-MSM)\uff0c\u901a\u8fc7\u7aef\u5230\u7aef\u81ea\u52a8\u5fae\u5206\u6280\u672f\u540c\u65f6\u8bc6\u522b\u808c\u8089\u548c\u9aa8\u9abc\u53c2\u6570\uff0c\u65e0\u9700\u6d4b\u91cf\u5185\u90e8\u5173\u8282\u626d\u77e9\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u9ad8\u4fdd\u771f\u4e2a\u6027\u5316\u4eba\u4f53\u808c\u8089\u9aa8\u9abc\u6a21\u578b\u5bf9\u4e8e\u4eba\u673a\u4ea4\u4e92\u7cfb\u7edf\u4eff\u771f\u548c\u5b89\u5168\u9a8c\u8bc1\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u76f4\u63a5\u6d4b\u91cf\u5185\u90e8\u751f\u7269\u529b\u5b66\u53d8\u91cf\uff0c\u7279\u522b\u662f\u5173\u8282\u626d\u77e9\u3002", "method": "\u4f7f\u7528\u53ef\u5fae\u5206\u808c\u8089\u9aa8\u9abc\u6a21\u578b(Diff-MSM)\uff0c\u901a\u8fc7\u81ea\u52a8\u5fae\u5206\u6280\u672f\u4ece\u53ef\u6d4b\u91cf\u7684\u808c\u8089\u6fc0\u6d3b\u5230\u53ef\u89c2\u5bdf\u7684\u8fd0\u52a8\u8fdb\u884c\u7aef\u5230\u7aef\u53c2\u6570\u8bc6\u522b\uff0c\u65e0\u9700\u6d4b\u91cf\u5185\u90e8\u5173\u8282\u626d\u77e9\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u808c\u8089\u53c2\u6570\u4f30\u8ba1\u8bef\u5dee\u4f4e\u81f30.05%\uff08\u521d\u59cb\u731c\u6d4b\u4ece\u5747\u503c\u4e3a\u771f\u5b9e\u503c\u3001\u6807\u51c6\u5dee\u4e3a10%\u7684\u6b63\u6001\u5206\u5e03\u4e2d\u91c7\u6837\uff09\u3002", "conclusion": "Diff-MSM\u53c2\u6570\u8bc6\u522b\u6280\u672f\u4e0d\u4ec5\u5728\u808c\u8089\u9aa8\u9abc\u5efa\u6a21\u548c\u4eff\u771f\u4e2d\u6709\u91cd\u8981\u5e94\u7528\uff0c\u5728\u808c\u8089\u5065\u5eb7\u76d1\u6d4b\u3001\u5eb7\u590d\u548c\u8fd0\u52a8\u79d1\u5b66\u7b49\u9886\u57df\u4e5f\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2508.13177", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13177", "abs": "https://arxiv.org/abs/2508.13177", "authors": ["Nikola Pi\u017eurica", "Nikola Milovi\u0107", "Igor Jovan\u010devi\u0107", "Conor Heins", "Miguel de Prado"], "title": "A Hardware-oriented Approach for Efficient Active Inference Computation and Deployment", "comment": null, "summary": "Active Inference (AIF) offers a robust framework for decision-making, yet its\ncomputational and memory demands pose challenges for deployment, especially in\nresource-constrained environments. This work presents a methodology that\nfacilitates AIF's deployment by integrating pymdp's flexibility and efficiency\nwith a unified, sparse, computational graph tailored for hardware-efficient\nexecution. Our approach reduces latency by over 2x and memory by up to 35%,\nadvancing the deployment of efficient AIF agents for real-time and embedded\napplications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06Active Inference\u6846\u67b6\u4e0e\u786c\u4ef6\u4f18\u5316\u8ba1\u7b97\u56fe\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u5ef6\u8fdf\u548c\u5185\u5b58\u4f7f\u7528", "motivation": "Active Inference (AIF)\u6846\u67b6\u867d\u7136\u51b3\u7b56\u80fd\u529b\u5f3a\uff0c\u4f46\u5176\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u90e8\u7f72\u5b58\u5728\u6311\u6218", "method": "\u6574\u5408pymdp\u7684\u7075\u6d3b\u6027\u548c\u6548\u7387\uff0c\u6784\u5efa\u7edf\u4e00\u7684\u7a00\u758f\u8ba1\u7b97\u56fe\uff0c\u9488\u5bf9\u786c\u4ef6\u9ad8\u6548\u6267\u884c\u8fdb\u884c\u4f18\u5316", "result": "\u5ef6\u8fdf\u964d\u4f4e\u8d85\u8fc72\u500d\uff0c\u5185\u5b58\u4f7f\u7528\u51cf\u5c11\u9ad8\u8fbe35%", "conclusion": "\u8be5\u65b9\u6cd5\u63a8\u8fdb\u4e86\u9ad8\u6548AIF\u667a\u80fd\u4f53\u5728\u5b9e\u65f6\u548c\u5d4c\u5165\u5f0f\u5e94\u7528\u4e2d\u7684\u90e8\u7f72"}}
{"id": "2508.13319", "categories": ["cs.RO", "cs.AI", "cs.CV", "I.2.9; I.2.10; I.2.7"], "pdf": "https://arxiv.org/pdf/2508.13319", "abs": "https://arxiv.org/abs/2508.13319", "authors": ["Kshitij Kavimandan", "Pooja Mangal", "Devanshi Mehta"], "title": "A Surveillance Based Interactive Robot", "comment": "4 pages, 5 figures", "summary": "We build a mobile surveillance robot that streams video in real time and\nresponds to speech so a user can monitor and steer it from a phone or browser.\nThe system uses two Raspberry Pi 4 units: a front unit on a differential drive\nbase with camera, mic, and speaker, and a central unit that serves the live\nfeed and runs perception. Video is sent with FFmpeg. Objects in the scene are\ndetected using YOLOv3 to support navigation and event awareness. For voice\ninteraction, we use Python libraries for speech recognition, multilingual\ntranslation, and text-to-speech, so the robot can take spoken commands and read\nback responses in the requested language. A Kinect RGB-D sensor provides visual\ninput and obstacle cues. In indoor tests the robot detects common objects at\ninteractive frame rates on CPU, recognises commands reliably, and translates\nthem to actions without manual control. The design relies on off-the-shelf\nhardware and open software, making it easy to reproduce. We discuss limits and\npractical extensions, including sensor fusion with ultrasonic range data, GPU\nacceleration, and adding face and text recognition.", "AI": {"tldr": "\u57fa\u4e8e\u6811\u8393\u6d3eRaspberry Pi\u7684\u79fb\u52a8\u76d1\u63a7\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u652f\u6301\u5b9e\u65f6\u89c6\u9891\u6d41\u3001\u8bed\u97f3\u63a7\u5236\u548c\u591a\u8bed\u8a00\u4ea4\u4e92\uff0c\u901a\u8fc7YOLOv3\u8fdb\u884c\u5bf9\u8c61\u68c0\u6d4b\u548c\u81ea\u4e3b\u5bfc\u822a", "motivation": "\u6784\u5efa\u4e00\u4e2a\u4f7f\u7528\u666e\u901a\u786c\u4ef6\u548c\u5f00\u6e90\u8f6f\u4ef6\u7684\u6613\u4e8e\u590d\u73b0\u7684\u79fb\u52a8\u76d1\u63a7\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u652f\u6301\u8fdc\u7a0b\u76d1\u63a7\u548c\u8bed\u97f3\u63a7\u5236", "method": "\u91c7\u7528\u4e24\u4e2aRaspberry Pi 4\u5355\u5143\uff1a\u524d\u7aef\u5355\u5143\u8d1f\u8d23\u611f\u77e5\u548c\u8fd0\u52a8\uff0c\u4e2d\u592e\u5355\u5143\u8d1f\u8d23\u89c6\u9891\u6d41\u548c\u89c6\u89c9\u5904\u7406\u3002\u4f7f\u7528FFmpeg\u4f20\u8f93\u89c6\u9891\uff0cYOLOv3\u8fdb\u884c\u5bf9\u8c61\u68c0\u6d4b\uff0cPython\u8bed\u97f3\u5e93\u5b9e\u73b0\u8bed\u97f3\u8bc6\u522b\u548c\u591a\u8bed\u8a00\u8bed\u97f3\u5408\u6210\uff0cKinect RGB-D\u4f20\u611f\u5668\u63d0\u4f9b\u6df1\u5ea6\u4fe1\u606f", "result": "\u5ba4\u5185\u6d4b\u8bd5\u4e2d\uff0c\u673a\u5668\u4eba\u80fd\u591f\u5728CPU\u4e0a\u4ee5\u4ea4\u4e92\u5f27\u7387\u68c0\u6d4b\u5e38\u89c1\u7269\u4f53\uff0c\u53ef\u9760\u8bc6\u522b\u547d\u4ee4\uff0c\u5e76\u5c06\u5176\u8f6c\u6362\u4e3a\u52a8\u4f5c\u800c\u65e0\u9700\u624b\u52a8\u63a7\u5236", "conclusion": "\u8bbe\u8ba1\u4f9d\u8d56\u5546\u7528\u786c\u4ef6\u548c\u5f00\u6e90\u8f6f\u4ef6\uff0c\u6613\u4e8e\u590d\u73b0\u3002\u8ba8\u8bba\u4e86\u4f20\u611f\u5668\u878d\u5408\u3001GPU\u52a0\u901f\u3001\u4eba\u8138\u548c\u6587\u672c\u8bc6\u522b\u7b49\u6269\u5c55\u53ef\u80fd\u6027"}}
{"id": "2508.13178", "categories": ["cs.AI", "cs.CL", "cs.DB"], "pdf": "https://arxiv.org/pdf/2508.13178", "abs": "https://arxiv.org/abs/2508.13178", "authors": ["Cong Zhang"], "title": "The Interpretability Analysis of the Model Can Bring Improvements to the Text-to-SQL Task", "comment": null, "summary": "To elevate the foundational capabilities and generalization prowess of the\ntext-to-SQL model in real-world applications, we integrate model\ninterpretability analysis with execution-guided strategy for semantic parsing\nof WHERE clauses in SQL queries. Furthermore, we augment this approach with\nfiltering adjustments, logical correlation refinements, and model fusion,\nculminating in the design of the CESQL model that facilitates conditional\nenhancement. Our model excels on the WikiSQL dataset, which is emblematic of\nsingle-table database query tasks, markedly boosting the accuracy of prediction\noutcomes. When predicting conditional values in WHERE clauses, we have not only\nminimized our dependence on data within the condition columns of tables but\nalso circumvented the impact of manually labeled training data. Our hope is\nthat this endeavor to enhance accuracy in processing basic database queries\nwill offer fresh perspectives for research into handling complex queries and\nscenarios featuring irregular data in real-world database environments.", "AI": {"tldr": "CESQL\u6a21\u578b\u901a\u8fc7\u96c6\u6210\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u5206\u6790\u548c\u6267\u884c\u5f15\u5bfc\u7b56\u7565\uff0c\u7ed3\u5408\u8fc7\u6ee4\u8c03\u6574\u3001\u903b\u8f91\u5173\u8054\u4f18\u5316\u548c\u6a21\u578b\u878d\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6587\u672c\u5230SQL\u8f6c\u6362\u5728WHERE\u5b50\u53e5\u8bed\u4e49\u89e3\u6790\u4e2d\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u63d0\u5347\u6587\u672c\u5230SQL\u6a21\u578b\u5728\u771f\u5b9e\u5e94\u7528\u4e2d\u7684\u57fa\u7840\u80fd\u529b\u548c\u6cdb\u5316\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u5904\u7406WHERE\u5b50\u53e5\u8bed\u4e49\u89e3\u6790\u65f6\u51cf\u5c11\u5bf9\u6761\u4ef6\u5217\u6570\u636e\u7684\u4f9d\u8d56\u548c\u4eba\u5de5\u6807\u6ce8\u8bad\u7ec3\u6570\u636e\u7684\u5f71\u54cd\u3002", "method": "\u96c6\u6210\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u5206\u6790\u4e0e\u6267\u884c\u5f15\u5bfc\u7b56\u7565\uff0c\u91c7\u7528\u8fc7\u6ee4\u8c03\u6574\u3001\u903b\u8f91\u5173\u8054\u7cbe\u5316\u548c\u6a21\u578b\u878d\u5408\u6280\u672f\uff0c\u8bbe\u8ba1\u51fa\u652f\u6301\u6761\u4ef6\u589e\u5f3a\u7684CESQL\u6a21\u578b\u3002", "result": "\u5728WikiSQL\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9884\u6d4b\u7ed3\u679c\u7684\u51c6\u786e\u6027\uff0c\u7279\u522b\u662f\u5728WHERE\u5b50\u53e5\u6761\u4ef6\u503c\u9884\u6d4b\u65b9\u9762\u53d6\u5f97\u4e86\u7a81\u7834\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5904\u7406\u590d\u6742\u67e5\u8be2\u548c\u771f\u5b9e\u6570\u636e\u5e93\u73af\u5883\u4e2d\u4e0d\u89c4\u5219\u6570\u636e\u573a\u666f\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u548c\u65b9\u6cd5\u57fa\u7840\u3002"}}
{"id": "2508.13392", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.13392", "abs": "https://arxiv.org/abs/2508.13392", "authors": ["Sidharth Talia", "Oren Salzman", "Siddhartha Srinivasa"], "title": "Incremental Generalized Hybrid A*", "comment": "8 pages, 7 figures", "summary": "We address the problem of efficiently organizing search over very large\ntrees, which arises in many applications ranging from autonomous driving to\naerial vehicles. Here, we are motivated by off-road autonomy, where real-time\nplanning is essential. Classical approaches use graphs of motion primitives and\nexploit dominance to mitigate the curse of dimensionality and prune expansions\nefficiently. However, for complex dynamics, repeatedly solving two-point\nboundary-value problems makes graph construction too slow for fast kinodynamic\nplanning. Hybrid A* (HA*) addressed this challenge by searching over a tree of\nmotion primitives and introducing approximate pruning using a grid-based\ndominance check. However, choosing the grid resolution is difficult: too coarse\nrisks failure, while too fine leads to excessive expansions and slow planning.\nWe propose Incremental Generalized Hybrid A* (IGHA*), an anytime tree-search\nframework that dynamically organizes vertex expansions without rigid pruning.\nIGHA* provably matches or outperforms HA*. For both on-road kinematic and\noff-road kinodynamic planning queries for a car-like robot, variants of IGHA*\nuse 6x fewer expansions to the best solution compared to an optimized version\nof HA*. In simulated off-road experiments in a high fidelity simulator, IGHA*\noutperforms HA*M when both are used in the loop with a model predictive\ncontroller. We demonstrate real-time performance both in simulation and on a\nsmall-scale off-road vehicle, enabling fast, robust planning under complex\ndynamics. Code: https://github.com/personalrobotics/IGHAStar", "AI": {"tldr": "\u63d0\u51fa\u4e86Incremental Generalized Hybrid A* (IGHA*)\u7b97\u6cd5\uff0c\u901a\u8fc7\u52a8\u6001\u7ec4\u7ec7\u9876\u70b9\u6269\u5c55\u800c\u975e\u521a\u6027\u526a\u679d\uff0c\u5728\u590d\u6742\u52a8\u529b\u5b66\u89c4\u5212\u4e2d\u6bd4\u4f20\u7edfHybrid A*\u51cf\u5c116\u500d\u6269\u5c55\u6b21\u6570\uff0c\u5b9e\u73b0\u5b9e\u65f6\u6027\u80fd", "motivation": "\u89e3\u51b3\u5927\u578b\u6811\u641c\u7d22\u7684\u9ad8\u6548\u7ec4\u7ec7\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u8d8a\u91ce\u81ea\u52a8\u9a7e\u9a76\u7b49\u9700\u8981\u5b9e\u65f6\u89c4\u5212\u7684\u590d\u6742\u52a8\u529b\u5b66\u573a\u666f\u4e2d\u3002\u4f20\u7edfHybrid A*\u7684\u7f51\u683c\u5206\u8fa8\u7387\u9009\u62e9\u56f0\u96be\uff0c\u8fc7\u7c97\u4f1a\u5bfc\u81f4\u5931\u8d25\uff0c\u8fc7\u7ec6\u5219\u5bfc\u81f4\u6269\u5c55\u8fc7\u591a\u548c\u89c4\u5212\u7f13\u6162", "method": "\u63d0\u51fa\u589e\u91cf\u5e7f\u4e49\u6df7\u5408A*\uff08IGHA*\uff09\u6846\u67b6\uff0c\u91c7\u7528\u52a8\u6001\u9876\u70b9\u6269\u5c55\u7ec4\u7ec7\u65b9\u5f0f\uff0c\u907f\u514d\u521a\u6027\u526a\u679d\u3002\u8fd9\u662f\u4e00\u4e2a\u968f\u65f6\u6811\u641c\u7d22\u6846\u67b6\uff0c\u80fd\u591f\u6e10\u8fdb\u5f0f\u6539\u8fdb\u89e3\u51b3\u65b9\u6848", "result": "\u5728\u6c7d\u8f66\u7c7b\u673a\u5668\u4eba\u7684\u9053\u8def\u8fd0\u52a8\u5b66\u548c\u8d8a\u91ce\u52a8\u529b\u5b66\u89c4\u5212\u67e5\u8be2\u4e2d\uff0cIGHA*\u53d8\u4f53\u6bd4\u4f18\u5316\u7248Hybrid A*\u51cf\u5c116\u500d\u6269\u5c55\u6b21\u6570\u3002\u5728\u9ad8\u4fdd\u771f\u6a21\u62df\u5668\u7684\u8d8a\u91ce\u5b9e\u9a8c\u4e2d\uff0cIGHA*\u4f18\u4e8eHA*M\uff0c\u5e76\u5728\u4eff\u771f\u548c\u5c0f\u578b\u8d8a\u91ce\u8f66\u8f86\u4e0a\u5c55\u793a\u4e86\u5b9e\u65f6\u6027\u80fd", "conclusion": "IGHA*\u7b97\u6cd5\u80fd\u591f\u5b9e\u73b0\u590d\u6742\u52a8\u529b\u5b66\u4e0b\u7684\u5feb\u901f\u9c81\u68d2\u89c4\u5212\uff0c\u5728\u7406\u8bba\u548c\u5b9e\u9a8c\u4e0a\u90fd\u8bc1\u660e\u4f18\u4e8e\u4f20\u7edfHybrid A*\u65b9\u6cd5\uff0c\u4e3a\u5b9e\u65f6\u81ea\u52a8\u9a7e\u9a76\u89c4\u5212\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.13180", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.13180", "abs": "https://arxiv.org/abs/2508.13180", "authors": ["Ziwen Han", "Meher Mankikar", "Julian Michael", "Zifan Wang"], "title": "Search-Time Data Contamination", "comment": null, "summary": "Data contamination refers to the leakage of evaluation data into model\ntraining data, resulting in overfitting to supposedly held-out test sets and\ncompromising test validity. We identify an analogous issue, search-time\ncontamination (STC), in evaluating search-based LLM agents which use tools to\ngather information from online sources when answering user queries. STC occurs\nwhen the retrieval step surfaces a source containing the test question (or a\nnear-duplicate) alongside its answer, enabling agents to copy rather than\ngenuinely infer or reason, undermining benchmark integrity. We find that\nHuggingFace, an online platform hosting evaluation datasets, appears among\nretrieved sources in search based agent logs. Consequently, agents often\nexplicitly acknowledge discovering question answer pairs from HuggingFace\nwithin their reasoning chains. On three commonly used capability benchmarks:\nHumanity's Last Exam (HLE), SimpleQA, and GPQA, we demonstrate that for\napproximately 3% of questions, search-based agents directly find the datasets\nwith ground truth labels on HuggingFace. When millions of evaluation queries\ntarget the same benchmark, even small, repeated leaks can accelerate the\nbenchmark's obsolescence, shortening its intended lifecycle. After HuggingFace\nis blocked, we observe a drop in accuracy on the contaminated subset of\napproximately 15%. We further show through ablation experiments that publicly\naccessible evaluation datasets on HuggingFace may not be the sole source of\nSTC. To this end, we conclude by proposing best practices for benchmark design\nand result reporting to address this novel form of leakage and ensure\ntrustworthy evaluation of search-based LLM agents. To facilitate the auditing\nof evaluation results, we also publicly release the complete logs from our\nexperiments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u6c61\u67d3\u95ee\u9898\u2014\u2014\u641c\u7d22\u65f6\u6c61\u67d3\uff08STC\uff09\uff0c\u6307\u641c\u7d22\u578bLLM\u4ee3\u7406\u5728\u56de\u7b54\u95ee\u9898\u65f6\u901a\u8fc7\u641c\u7d22\u5de5\u5177\u4ece\u5728\u7ebf\u5e73\u53f0\uff08\u5982HuggingFace\uff09\u76f4\u63a5\u83b7\u53d6\u6d4b\u8bd5\u95ee\u9898\u548c\u7b54\u6848\uff0c\u800c\u975e\u901a\u8fc7\u63a8\u7406\u751f\u6210\u7b54\u6848\uff0c\u4ece\u800c\u635f\u5bb3\u57fa\u51c6\u6d4b\u8bd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u8bad\u7ec3\u6570\u636e\u6c61\u67d3\uff0c\u4f46\u5ffd\u7565\u4e86\u641c\u7d22\u578bLLM\u4ee3\u7406\u5728\u8bc4\u4f30\u8fc7\u7a0b\u4e2d\u53ef\u80fd\u901a\u8fc7\u641c\u7d22\u5de5\u5177\u76f4\u63a5\u83b7\u53d6\u6d4b\u8bd5\u7b54\u6848\u7684\u95ee\u9898\uff0c\u8fd9\u79cd\u641c\u7d22\u65f6\u6c61\u67d3\u4f1a\u4e25\u91cd\u7834\u574f\u57fa\u51c6\u6d4b\u8bd5\u7684\u5b8c\u6574\u6027\u3002", "method": "\u901a\u8fc7\u5728\u4e09\u4e2a\u5e38\u7528\u80fd\u529b\u57fa\u51c6\uff08HLE\u3001SimpleQA\u3001GPQA\uff09\u4e0a\u6d4b\u8bd5\u641c\u7d22\u578b\u4ee3\u7406\uff0c\u5206\u6790\u5176\u641c\u7d22\u65e5\u5fd7\uff0c\u53d1\u73b0\u7ea63%\u7684\u95ee\u9898\u80fd\u76f4\u63a5\u4eceHuggingFace\u83b7\u53d6\u5e26\u6807\u7b7e\u7684\u6570\u636e\u96c6\uff1b\u901a\u8fc7\u963b\u65adHuggingFace\u8fdb\u884c\u5bf9\u6bd4\u5b9e\u9a8c\uff0c\u89c2\u5bdf\u51c6\u786e\u7387\u53d8\u5316\u3002", "result": "\u7ea63%\u7684\u6d4b\u8bd5\u95ee\u9898\u5b58\u5728\u641c\u7d22\u65f6\u6c61\u67d3\uff0c\u4ee3\u7406\u76f4\u63a5\u4eceHuggingFace\u83b7\u53d6\u7b54\u6848\uff1b\u963b\u65adHuggingFace\u540e\uff0c\u53d7\u6c61\u67d3\u5b50\u96c6\u7684\u51c6\u786e\u7387\u4e0b\u964d\u7ea615%\uff1b\u5b9e\u9a8c\u8868\u660eHuggingFace\u53ef\u80fd\u4e0d\u662f\u552f\u4e00\u7684\u6c61\u67d3\u6e90\u3002", "conclusion": "\u9700\u8981\u5236\u5b9a\u57fa\u51c6\u8bbe\u8ba1\u548c\u7ed3\u679c\u62a5\u544a\u7684\u6700\u4f73\u5b9e\u8df5\u6765\u89e3\u51b3\u641c\u7d22\u65f6\u6c61\u67d3\u95ee\u9898\uff0c\u786e\u4fdd\u641c\u7d22\u578bLLM\u4ee3\u7406\u8bc4\u4f30\u7684\u53ef\u4fe1\u5ea6\uff1b\u540c\u65f6\u516c\u5f00\u5b9e\u9a8c\u65e5\u5fd7\u4ee5\u65b9\u4fbf\u5ba1\u8ba1\u8bc4\u4f30\u7ed3\u679c\u3002"}}
{"id": "2508.13407", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.13407", "abs": "https://arxiv.org/abs/2508.13407", "authors": ["Jiming Ren", "Xuan Lin", "Roman Mineyev", "Karen M. Feigh", "Samuel Coogan", "Ye Zhao"], "title": "Accelerating Signal-Temporal-Logic-Based Task and Motion Planning of Bipedal Navigation using Benders Decomposition", "comment": "16 pages, 7 figures, 6 tables", "summary": "Task and motion planning under Signal Temporal Logic constraints is known to\nbe NP-hard. A common class of approaches formulates these hybrid problems,\nwhich involve discrete task scheduling and continuous motion planning, as\nmixed-integer programs (MIP). However, in applications for bipedal locomotion,\nintroduction of non-convex constraints such as kinematic reachability and\nfootstep rotation exacerbates the computational complexity of MIPs. In this\nwork, we present a method based on Benders Decomposition to address scenarios\nwhere solving the entire monolithic optimization problem is prohibitively\nintractable. Benders Decomposition proposes an iterative cutting-plane\ntechnique that partitions the problem into a master problem to prototype a plan\nthat meets the task specification, and a series of subproblems for kinematics\nand dynamics feasibility checks. Our experiments demonstrate that this method\nachieves faster planning compared to alternative algorithms for solving the\nresulting optimization program with nonlinear constraints.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eBenders\u5206\u89e3\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u53cc\u8db3\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\u4e2d\u6df7\u5408\u6574\u6570\u89c4\u5212\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u95ee\u9898\uff0c\u901a\u8fc7\u4e3b\u95ee\u9898\u548c\u5b50\u95ee\u9898\u8fed\u4ee3\u6c42\u89e3\uff0c\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u5feb", "motivation": "\u53cc\u8db3\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\u4e2d\u7684\u4efb\u52a1\u548c\u8fd0\u52a8\u89c4\u5212\u5728\u4fe1\u53f7\u65f6\u5e8f\u903b\u8f91\u7ea6\u675f\u4e0b\u662fNP\u96be\u95ee\u9898\uff0c\u6df7\u5408\u6574\u6570\u89c4\u5212\u65b9\u6cd5\u5728\u5904\u7406\u975e\u51f8\u7ea6\u675f\uff08\u5982\u8fd0\u52a8\u5b66\u53ef\u8fbe\u6027\u548c\u811a\u6b65\u65cb\u8f6c\uff09\u65f6\u8ba1\u7b97\u590d\u6742\u5ea6\u6781\u9ad8", "method": "\u4f7f\u7528Benders\u5206\u89e3\u65b9\u6cd5\uff0c\u5c06\u95ee\u9898\u5206\u89e3\u4e3a\u4e3b\u95ee\u9898\uff08\u751f\u6210\u6ee1\u8db3\u4efb\u52a1\u89c4\u8303\u7684\u8ba1\u5212\u539f\u578b\uff09\u548c\u4e00\u7cfb\u5217\u5b50\u95ee\u9898\uff08\u8fdb\u884c\u8fd0\u52a8\u5b66\u548c\u52a8\u529b\u5b66\u53ef\u884c\u6027\u68c0\u67e5\uff09\uff0c\u91c7\u7528\u8fed\u4ee3\u5207\u5272\u5e73\u9762\u6280\u672f", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u6bd4\u66ff\u4ee3\u7b97\u6cd5\u5728\u6c42\u89e3\u5177\u6709\u975e\u7ebf\u6027\u7ea6\u675f\u7684\u4f18\u5316\u7a0b\u5e8f\u65f6\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u89c4\u5212\u901f\u5ea6", "conclusion": "Benders\u5206\u89e3\u65b9\u6cd5\u80fd\u6709\u6548\u89e3\u51b3\u53cc\u8db3\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\u4e2d\u7684\u8ba1\u7b97\u590d\u6742\u6027\u95ee\u9898\uff0c\u4e3a\u5904\u7406\u6df7\u5408\u6574\u6570\u89c4\u5212\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2508.13204", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13204", "abs": "https://arxiv.org/abs/2508.13204", "authors": ["Dong Liu", "Yanxuan Yu"], "title": "QuickMerge++: Fast Token Merging with Autoregressive Prior", "comment": "The paper has been accepted to ICML Tokshop at\n  https://openreview.net/forum?id=dMdxHd0tRf", "summary": "As generative models scale to larger inputs across language, vision, and\nvideo domains, the cost of token-level computation has become a key bottleneck.\nWhile prior work suggests that only a subset of tokens significantly influence\ndownstream predictions, most token selection methods are static,\nmodality-specific, or incompatible with autoregressive generation. In this\npaper, we propose QuickMerge, a lightweight token merging framework designed\nfor efficient next-token prediction.\n  QuickMerge dynamically selects a reduced number of tokens based on attention\nnorm magnitude, guided by an entropy-based budget estimator. To preserve\nautoregressive compatibility, we introduce a lightweight transformer prior\ntrained over the merged token sequence. By combining semantic salience\nestimation, flexible token budgets, and AR alignment, QuickMerge enables\naccurate generation with fewer tokens.\n  We evaluate QuickMerge across multi-modality domains, demonstrating\nconsistent improvements in compute-accuracy tradeoffs. Specifically, QuickMerge\nreduces token counts sustantially while matching as well as exceeding the\nperformance of learned tokenizers and fixed-patch baselines.", "AI": {"tldr": "QuickMerge\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7token\u5408\u5e76\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u91cd\u8981token\u6765\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u81ea\u56de\u5f52\u751f\u6210\u80fd\u529b\uff0c\u5728\u591a\u6a21\u6001\u9886\u57df\u5b9e\u73b0\u4e86\u8ba1\u7b97\u6548\u7387\u4e0e\u51c6\u786e\u6027\u7684\u5e73\u8861", "motivation": "\u968f\u7740\u751f\u6210\u6a21\u578b\u5904\u7406\u66f4\u5927\u89c4\u6a21\u8f93\u5165\uff0ctoken\u7ea7\u522b\u7684\u8ba1\u7b97\u6210\u672c\u6210\u4e3a\u5173\u952e\u74f6\u9888\u3002\u73b0\u6709token\u9009\u62e9\u65b9\u6cd5\u5927\u591a\u662f\u9759\u6001\u7684\u3001\u6a21\u6001\u7279\u5b9a\u7684\u6216\u4e0d\u517c\u5bb9\u81ea\u56de\u5f52\u751f\u6210", "method": "\u57fa\u4e8e\u6ce8\u610f\u529b\u8303\u6570\u5e45\u5ea6\u7684\u52a8\u6001token\u9009\u62e9\uff0c\u4f7f\u7528\u57fa\u4e8e\u71b5\u7684\u9884\u7b97\u4f30\u8ba1\u5668\u6307\u5bfc\uff0c\u5e76\u5f15\u5165\u8f7b\u91cf\u7ea7transformer\u5148\u9a8c\u6765\u4fdd\u6301\u81ea\u56de\u5f52\u517c\u5bb9\u6027", "result": "\u5728\u591a\u6a21\u6001\u9886\u57df\u8bc4\u4f30\u663e\u793a\uff0cQuickMerge\u663e\u8457\u51cf\u5c11token\u6570\u91cf\uff0c\u540c\u65f6\u5339\u914d\u751a\u81f3\u8d85\u8d8a\u5b66\u4e60\u578btokenizer\u548c\u56fa\u5b9apatch\u57fa\u7ebf\u7684\u6027\u80fd", "conclusion": "QuickMerge\u901a\u8fc7\u8bed\u4e49\u663e\u8457\u6027\u4f30\u8ba1\u3001\u7075\u6d3btoken\u9884\u7b97\u548c\u81ea\u56de\u5f52\u5bf9\u9f50\u7684\u7ec4\u5408\uff0c\u5b9e\u73b0\u4e86\u7528\u66f4\u5c11token\u8fdb\u884c\u51c6\u786e\u751f\u6210\u7684\u6709\u6548\u6846\u67b6"}}
{"id": "2508.13444", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.13444", "abs": "https://arxiv.org/abs/2508.13444", "authors": ["Tianyu Li", "Jeonghwan Kim", "Wontaek Kim", "Donghoon Baek", "Seungeun Rho", "Sehoon Ha"], "title": "Switch4EAI: Leveraging Console Game Platform for Benchmarking Robotic Athletics", "comment": "Workshop Submission", "summary": "Recent advances in whole-body robot control have enabled humanoid and legged\nrobots to execute increasingly agile and coordinated movements. However,\nstandardized benchmarks for evaluating robotic athletic performance in\nreal-world settings and in direct comparison to humans remain scarce. We\npresent Switch4EAI(Switch-for-Embodied-AI), a low-cost and easily deployable\npipeline that leverages motion-sensing console games to evaluate whole-body\nrobot control policies. Using Just Dance on the Nintendo Switch as a\nrepresentative example, our system captures, reconstructs, and retargets\nin-game choreography for robotic execution. We validate the system on a Unitree\nG1 humanoid with an open-source whole-body controller, establishing a\nquantitative baseline for the robot's performance against a human player. In\nthe paper, we discuss these results, which demonstrate the feasibility of using\ncommercial games platform as physically grounded benchmarks and motivate future\nwork to for benchmarking embodied AI.", "AI": {"tldr": "Switch4EAI\u662f\u4e00\u4e2a\u4f4e\u6210\u672c\u3001\u6613\u90e8\u7f72\u7684\u8bc4\u4f30\u7cfb\u7edf\uff0c\u5229\u7528\u4f53\u611f\u6e38\u620f\uff08\u5982Just Dance\uff09\u6765\u8bc4\u4f30\u5168\u8eab\u673a\u5668\u4eba\u63a7\u5236\u7b56\u7565\u7684\u6027\u80fd\uff0c\u5e76\u5728Unitree G1\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\u4e86\u53ef\u884c\u6027", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u8bc4\u4f30\u4eba\u5f62\u673a\u5668\u4eba\u8fd0\u52a8\u6027\u80fd\u5e76\u4e0e\u4eba\u7c7b\u8fdb\u884c\u76f4\u63a5\u6bd4\u8f83\u7684\u6807\u51c6\u5316\u57fa\u51c6\u6d4b\u8bd5\u65b9\u6cd5", "method": "\u5229\u7528\u4efb\u5929\u5802Switch\u7684\u4f53\u611f\u6e38\u620f\u6355\u83b7\u3001\u91cd\u5efa\u548c\u91cd\u5b9a\u5411\u6e38\u620f\u4e2d\u7684\u821e\u8e48\u52a8\u4f5c\uff0c\u901a\u8fc7\u5f00\u6e90\u5168\u8eab\u63a7\u5236\u5668\u5728Unitree G1\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u6267\u884c", "result": "\u6210\u529f\u5efa\u7acb\u4e86\u673a\u5668\u4eba\u6027\u80fd\u7684\u91cf\u5316\u57fa\u7ebf\uff0c\u5e76\u4e0e\u4eba\u7c7b\u73a9\u5bb6\u8fdb\u884c\u4e86\u6bd4\u8f83\u9a8c\u8bc1", "conclusion": "\u5546\u4e1a\u6e38\u620f\u5e73\u53f0\u53ef\u4ee5\u4f5c\u4e3a\u7269\u7406\u57fa\u7840\u7684\u57fa\u51c6\u6d4b\u8bd5\u5de5\u5177\uff0c\u4e3a\u5177\u8eabAI\u7684\u57fa\u51c6\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411"}}
{"id": "2508.13213", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13213", "abs": "https://arxiv.org/abs/2508.13213", "authors": ["Adamo Cerioli", "Edward D. Lee", "Vito D. P. Servedio"], "title": "AI sustains higher strategic tension than humans in chess", "comment": null, "summary": "Strategic decision-making involves managing the tension between immediate\nopportunities and long-term objectives. We study this trade-off in chess by\ncharacterizing and comparing dynamics between human vs human and AI vs AI\ngames. We propose a network-based metric of piece-to-piece interaction to\nquantify the ongoing strategic tension on the board. Its evolution in games\nreveals that the most competitive AI players sustain higher levels of strategic\ntension for longer durations than elite human players. Cumulative tension\nvaries with algorithmic complexity for AI and correspondingly in human-played\ngames increases abruptly with expertise at about 1600 Elo and again at 2300\nElo. The profiles reveal different approaches. Highly competitive AI tolerates\ninterconnected positions balanced between offensive and defensive tactics over\nlong periods. Human play, in contrast, limits tension and game complexity,\nwhich may reflect cognitive limitations and adaptive strategies. The difference\nmay have implications for AI usage in complex, strategic environments.", "AI": {"tldr": "\u4e00\u79cd\u7f51\u7edc\u57fa\u4e8e\u68cb\u5b50\u4e92\u52a8\u7684\u6307\u6807\u7528\u4e8e\u91cf\u5316\u68cb\u5c40\u6218\u7565\u7d27\u5f20\u7a0b\u5ea6\uff0c\u53d1\u73b0AI\u6bd4\u4eba\u7c7b\u66f4\u80fd\u7ef4\u6301\u66f4\u9ad8\u6c34\u5e73\u7684\u957f\u671f\u6218\u7565\u7d27\u5f20", "motivation": "\u7814\u7a76\u5728\u8c61\u68cb\u8fd9\u79cd\u6218\u7565\u6e38\u620f\u4e2d\uff0c\u4eba\u7c7b\u548cAI\u5982\u4f55\u5904\u7406\u5373\u65f6\u673a\u4f1a\u4e0e\u957f\u671f\u76ee\u6807\u4e4b\u95f4\u7684\u62c9\u6362\u5173\u7cfb\uff0c\u5bfb\u627e\u91cf\u5316\u6218\u7565\u7d27\u5f20\u7684\u65b9\u6cd5", "method": "\u63d0\u51fa\u57fa\u4e8e\u68cb\u5b50\u4e92\u52a8\u7f51\u7edc\u7684\u6307\u6807\u6765\u91cf\u5316\u68cb\u5c40\u6218\u7565\u7d27\u5f20\u7a0b\u5ea6\uff0c\u5bf9\u6bd4\u5206\u6790\u4eba\u7c7bvs\u4eba\u7c7b\u548cAIvsAI\u5bf9\u5c40\u7684\u6f14\u5316\u8fc7\u7a0b", "result": "AI\u5bf9\u624b\u80fd\u7ef4\u6301\u66f4\u9ad8\u6c34\u5e73\u7684\u957f\u671f\u6218\u7565\u7d27\u5f20\uff0c\u4eba\u7c7b\u4e13\u5bb6\u5728\u7ea21600\u548c2300\u8c6a\u65f6\u51fa\u73b0\u7d27\u5f20\u6c34\u5e73\u7684\u7a81\u53d8\u589e\u957f\uff0cAI\u5bf9\u4e92\u8054\u5e73\u8861\u653b\u9632\u7684\u590d\u6742\u5c40\u9762\u66f4\u5bbd\u5bb9", "conclusion": "AI\u548c\u4eba\u7c7b\u5728\u6218\u7565\u51b3\u7b56\u4e2d\u91c7\u53d6\u4e0d\u540c\u7b56\u7565\uff0cAI\u66f4\u80fd\u627f\u53d7\u957f\u671f\u590d\u6742\u5c40\u9762\uff0c\u8fd9\u53ef\u80fd\u4f53\u73b0\u4eba\u7c7b\u8ba4\u77e5\u9650\u5236\u548c\u9002\u5e94\u6027\u7b56\u7565\uff0c\u5bf9\u590d\u6742\u6218\u7565\u73af\u5883\u4e2d\u4f7f\u7528AI\u6709\u91cd\u8981\u542f\u793a"}}
{"id": "2508.13446", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.13446", "abs": "https://arxiv.org/abs/2508.13446", "authors": ["Catherine Glossop", "William Chen", "Arjun Bhorkar", "Dhruv Shah", "Sergey Levine"], "title": "CAST: Counterfactual Labels Improve Instruction Following in Vision-Language-Action Models", "comment": null, "summary": "Generalist robots should be able to understand and follow user instructions,\nbut current vision-language-action (VLA) models struggle with following\nfine-grained commands despite providing a powerful architecture for mapping\nopen-vocabulary natural language instructions to robot actions. One cause for\nthis is a lack of semantic diversity and language grounding in existing robot\ndatasets and, specifically, a lack of fine-grained task diversity for similar\nobservations. To address this, we present a novel method to augment existing\nrobot datasets by leveraging vision language models to create counterfactual\nlabels. Our method improves the language-following capabilities of VLAs by\nincreasing the diversity and granularity of language grounding for robot\ndatasets by generating counterfactual language and actions. We evaluate the\nresulting model's ability to follow language instructions, ranging from simple\nobject-centric commands to complex referential tasks, by conducting visual\nlanguage navigation experiments in 3 different indoor and outdoor environments.\nOur experiments demonstrate that counterfactual relabeling, without any\nadditional data collection, significantly improves instruction-following in VLA\npolicies, making them competitive with state-of-the-art methods and increasing\nsuccess rate by 27% on navigation tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u751f\u6210\u53cd\u4e8b\u5b9e\u6807\u7b7e\u6765\u589e\u5f3a\u673a\u5668\u4eba\u6570\u636e\u96c6\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u6307\u4ee4\u8ddf\u968f\u65b9\u9762\u7684\u80fd\u529b\uff0c\u5728\u5bfc\u822a\u4efb\u52a1\u4e2d\u6210\u529f\u7387\u63d0\u9ad8\u4e8627%\u3002", "motivation": "\u5f53\u524d\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\u6a21\u578b\u5728\u5904\u7406\u7ec6\u7c92\u5ea6\u6307\u4ee4\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u73b0\u6709\u673a\u5668\u4eba\u6570\u636e\u96c6\u7f3a\u4e4f\u8bed\u4e49\u591a\u6837\u6027\u548c\u8bed\u8a00\u57fa\u7840\uff0c\u7279\u522b\u662f\u9488\u5bf9\u76f8\u4f3c\u89c2\u5bdf\u7684\u7ec6\u7c92\u5ea6\u4efb\u52a1\u591a\u6837\u6027\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e3a\u73b0\u6709\u673a\u5668\u4eba\u6570\u636e\u96c6\u521b\u5efa\u53cd\u4e8b\u5b9e\u6807\u7b7e\uff0c\u901a\u8fc7\u751f\u6210\u53cd\u4e8b\u5b9e\u8bed\u8a00\u548c\u52a8\u4f5c\u6765\u589e\u52a0\u8bed\u8a00\u57fa\u7840\u7684\u591a\u6837\u6027\u548c\u7c92\u5ea6\u3002", "result": "\u57283\u4e2a\u4e0d\u540c\u5ba4\u5185\u5916\u73af\u5883\u4e2d\u7684\u89c6\u89c9\u8bed\u8a00\u5bfc\u822a\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86VLA\u7b56\u7565\u7684\u6307\u4ee4\u8ddf\u968f\u80fd\u529b\uff0c\u4f7f\u5176\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u7ade\u4e89\uff0c\u5bfc\u822a\u4efb\u52a1\u6210\u529f\u7387\u63d0\u9ad8\u4e8627%\u3002", "conclusion": "\u53cd\u4e8b\u5b9e\u91cd\u6807\u6ce8\u65b9\u6cd5\u65e0\u9700\u989d\u5916\u6570\u636e\u6536\u96c6\u5c31\u80fd\u6709\u6548\u63d0\u5347VLA\u6a21\u578b\u7684\u6307\u4ee4\u8ddf\u968f\u6027\u80fd\uff0c\u4e3a\u89e3\u51b3\u673a\u5668\u4eba\u6570\u636e\u96c6\u8bed\u8a00\u57fa\u7840\u4e0d\u8db3\u7684\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.13250", "categories": ["cs.AI", "cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.13250", "abs": "https://arxiv.org/abs/2508.13250", "authors": ["Zeyu Zhang", "Yang Zhang", "Haoran Tan", "Rui Li", "Xu Chen"], "title": "Explicit v.s. Implicit Memory: Exploring Multi-hop Complex Reasoning Over Personalized Information", "comment": "15 pages, 13 figures, 3 tables", "summary": "In large language model-based agents, memory serves as a critical capability\nfor achieving personalization by storing and utilizing users' information.\nAlthough some previous studies have adopted memory to implement user\npersonalization, they typically focus on preference alignment and simple\nquestion-answering. However, in the real world, complex tasks often require\nmulti-hop reasoning on a large amount of user information, which poses\nsignificant challenges for current memory approaches. To address this\nlimitation, we propose the multi-hop personalized reasoning task to explore how\ndifferent memory mechanisms perform in multi-hop reasoning over personalized\ninformation. We explicitly define this task and construct a dataset along with\na unified evaluation framework. Then, we implement various explicit and\nimplicit memory methods and conduct comprehensive experiments. We evaluate\ntheir performance on this task from multiple perspectives and analyze their\nstrengths and weaknesses. Besides, we explore hybrid approaches that combine\nboth paradigms and propose the HybridMem method to address their limitations.\nWe demonstrate the effectiveness of our proposed model through extensive\nexperiments. To benefit the research community, we release this project at\nhttps://github.com/nuster1128/MPR.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u591a\u8df3\u4e2a\u6027\u5316\u63a8\u7406\u4efb\u52a1\uff0c\u7814\u7a76\u4e0d\u540c\u8bb0\u5fc6\u673a\u5236\u5728\u4e2a\u6027\u5316\u4fe1\u606f\u591a\u8df3\u63a8\u7406\u4e2d\u7684\u8868\u73b0\uff0c\u6784\u5efa\u4e86\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u6846\u67b6\uff0c\u5e76\u63d0\u51fa\u4e86HybridMem\u6df7\u5408\u65b9\u6cd5\u6765\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u8bb0\u5fc6\u7684\u4e2a\u6027\u5316\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u504f\u597d\u5bf9\u9f50\u548c\u7b80\u5355\u95ee\u7b54\uff0c\u4f46\u73b0\u5b9e\u4e16\u754c\u4e2d\u590d\u6742\u4efb\u52a1\u9700\u8981\u5bf9\u5927\u91cf\u7528\u6237\u4fe1\u606f\u8fdb\u884c\u591a\u8df3\u63a8\u7406\uff0c\u8fd9\u5bf9\u5f53\u524d\u8bb0\u5fc6\u65b9\u6cd5\u63d0\u51fa\u4e86\u91cd\u5927\u6311\u6218\u3002", "method": "\u660e\u786e\u5b9a\u4e49\u591a\u8df3\u4e2a\u6027\u5316\u63a8\u7406\u4efb\u52a1\uff0c\u6784\u5efa\u6570\u636e\u96c6\u548c\u7edf\u4e00\u8bc4\u4f30\u6846\u67b6\uff0c\u5b9e\u73b0\u5404\u79cd\u663e\u5f0f\u548c\u9690\u5f0f\u8bb0\u5fc6\u65b9\u6cd5\uff0c\u63d0\u51fa\u7ed3\u5408\u4e24\u79cd\u8303\u5f0f\u7684HybridMem\u6df7\u5408\u65b9\u6cd5\u3002", "result": "\u901a\u8fc7\u7efc\u5408\u5b9e\u9a8c\u4ece\u591a\u89d2\u5ea6\u8bc4\u4f30\u4e86\u4e0d\u540c\u8bb0\u5fc6\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u5206\u6790\u4e86\u5b83\u4eec\u7684\u4f18\u7f3a\u70b9\uff0c\u5e76\u8bc1\u660e\u4e86HybridMem\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u591a\u8df3\u4e2a\u6027\u5316\u63a8\u7406\u662f\u4e00\u4e2a\u91cd\u8981\u4f46\u5177\u6709\u6311\u6218\u6027\u7684\u4efb\u52a1\uff0c\u6df7\u5408\u8bb0\u5fc6\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u7814\u7a76\u793e\u533a\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u6570\u636e\u96c6\u548c\u6846\u67b6\u3002"}}
{"id": "2508.13457", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.13457", "abs": "https://arxiv.org/abs/2508.13457", "authors": ["Xu Yang", "Jun Ni", "Hengyang Feng", "Feiyu Wang", "Tiezhen Wang"], "title": "Modeling and Control of AWOISV: A Filtered Tube-Based MPC Approach for Simultaneous Tracking of Lateral Position and Heading Angle", "comment": null, "summary": "An all-wheel omni-directional independent steering vehicle (AWOISV) is a\nspecialized all-wheel independent steering vehicle with each wheel capable of\nsteering up to 90{\\deg}, enabling unique maneuvers like yaw and diagonal\nmovement. This paper introduces a theoretical steering radius angle and\nsideslip angle (\\( \\theta_R \\)-\\(\\beta_R \\)) representation, based on the\nposition of the instantaneous center of rotation relative to the wheel rotation\ncenter, defining the motion modes and switching criteria for AWOISVs. A\ngeneralized \\( v\\)-\\(\\beta\\)-\\(r \\) dynamic model is developed with forward\nvelocity \\(v\\), sideslip angle \\(\\beta\\), and yaw rate \\(r\\) as states, and\n\\(\\theta_R\\) and \\(\\beta_R\\) as control inputs. This model decouples\nlongitudinal and lateral motions into forward and rotational motions, allowing\nseamless transitions across all motion modes under specific conditions. A\nfiltered tube-based linear time-varying MPC (FT-LTVMPC) strategy is proposed,\nachieving simultaneous tracking of lateral position and arbitrary heading\nangles, with robustness to model inaccuracies and parameter uncertainties.\nCo-simulation and hardware-in-loop (HIL) experiments confirm that FT-LTVMPC\nenables high-precision control of both position and heading while ensuring\nexcellent real-time performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5168\u8f6e\u5168\u5411\u72ec\u7acb\u8f6c\u5411\u8f66\u8f86(AWOISV)\u7684\u7406\u8bba\u6846\u67b6\u548c\u63a7\u5236\u7b56\u7565\uff0c\u901a\u8fc7\u5efa\u7acb\u8f6c\u5411\u534a\u5f84\u89d2\u548c\u4fa7\u504f\u89d2\u8868\u793a\u6cd5\uff0c\u5f00\u53d1\u4e86\u5e7f\u4e49\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u5e76\u8bbe\u8ba1\u4e86\u6ee4\u6ce2\u7ba1\u5f0f\u7ebf\u6027\u65f6\u53d8MPC\u63a7\u5236\u5668\uff0c\u5b9e\u73b0\u4e86\u4f4d\u7f6e\u548c\u822a\u5411\u89d2\u7684\u9ad8\u7cbe\u5ea6\u540c\u6b65\u8ddf\u8e2a\u3002", "motivation": "\u5168\u8f6e\u5168\u5411\u72ec\u7acb\u8f6c\u5411\u8f66\u8f86\u5177\u6709\u72ec\u7279\u7684\u673a\u52a8\u80fd\u529b\uff08\u5982\u6a2a\u6446\u548c\u5bf9\u89d2\u7ebf\u79fb\u52a8\uff09\uff0c\u4f46\u73b0\u6709\u63a7\u5236\u65b9\u6cd5\u96be\u4ee5\u5b9e\u73b0\u4f4d\u7f6e\u548c\u822a\u5411\u89d2\u7684\u540c\u65f6\u7cbe\u786e\u63a7\u5236\uff0c\u4e14\u7f3a\u4e4f\u7edf\u4e00\u7684\u8fd0\u52a8\u6a21\u5f0f\u5207\u6362\u7406\u8bba\u6846\u67b6\u3002", "method": "1) \u63d0\u51fa\u57fa\u4e8e\u77ac\u65f6\u65cb\u8f6c\u4e2d\u5fc3\u4f4d\u7f6e\u7684\u8f6c\u5411\u534a\u5f84\u89d2-\u4fa7\u504f\u89d2(\u03b8_R-\u03b2_R)\u8868\u793a\u6cd5\uff1b2) \u5efa\u7acb\u4ee5\u901f\u5ea6v\u3001\u4fa7\u504f\u89d2\u03b2\u3001\u6a2a\u6446\u7387r\u4e3a\u72b6\u6001\uff0c\u03b8_R\u548c\u03b2_R\u4e3a\u63a7\u5236\u8f93\u5165\u7684\u5e7f\u4e49\u52a8\u529b\u5b66\u6a21\u578b\uff1b3) \u8bbe\u8ba1\u6ee4\u6ce2\u7ba1\u5f0f\u7ebf\u6027\u65f6\u53d8MPC(FT-LTVMPC)\u63a7\u5236\u7b56\u7565\u3002", "result": "\u901a\u8fc7\u8054\u5408\u4eff\u771f\u548c\u786c\u4ef6\u5728\u73af\u5b9e\u9a8c\u9a8c\u8bc1\uff0cFT-LTVMPC\u80fd\u591f\u5b9e\u73b0\u4f4d\u7f6e\u548c\u4efb\u610f\u822a\u5411\u89d2\u7684\u9ad8\u7cbe\u5ea6\u540c\u6b65\u8ddf\u8e2a\uff0c\u5bf9\u6a21\u578b\u4e0d\u7cbe\u786e\u6027\u548c\u53c2\u6570\u4e0d\u786e\u5b9a\u6027\u5177\u6709\u9c81\u68d2\u6027\uff0c\u4e14\u4fdd\u8bc1\u4e86\u4f18\u5f02\u7684\u5b9e\u65f6\u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7406\u8bba\u6846\u67b6\u548c\u63a7\u5236\u7b56\u7565\u4e3aAWOISV\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u8fd0\u52a8\u6a21\u5f0f\u63cf\u8ff0\u548c\u5207\u6362\u6807\u51c6\uff0c\u5b9e\u73b0\u4e86\u591a\u8fd0\u52a8\u6a21\u5f0f\u95f4\u7684\u65e0\u7f1d\u8fc7\u6e21\uff0c\u4e3a\u5168\u5411\u79fb\u52a8\u673a\u5668\u4eba\u7684\u7cbe\u786e\u63a7\u5236\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.13251", "categories": ["cs.AI", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.13251", "abs": "https://arxiv.org/abs/2508.13251", "authors": ["Di Zhang", "Xue Jia", "Tran Ba Hung", "Seong Hoon Jang", "Linda Zhang", "Ryuhei Sato", "Yusuke Hashimoto", "Toyoto Sato", "Kiyoe Konno", "Shin-ichi Orimo", "Hao Li"], "title": "\"DIVE\" into Hydrogen Storage Materials Discovery with AI Agents", "comment": "23 pages, 5 figures. The supplementary video is available at the\n  GitHub link provided in the manuscript", "summary": "Data-driven artificial intelligence (AI) approaches are fundamentally\ntransforming the discovery of new materials. Despite the unprecedented\navailability of materials data in the scientific literature, much of this\ninformation remains trapped in unstructured figures and tables, hindering the\nconstruction of large language model (LLM)-based AI agent for automated\nmaterials design. Here, we present the Descriptive Interpretation of Visual\nExpression (DIVE) multi-agent workflow, which systematically reads and\norganizes experimental data from graphical elements in scientific literatures.\nWe focus on solid-state hydrogen storage materials-a class of materials central\nto future clean-energy technologies and demonstrate that DIVE markedly improves\nthe accuracy and coverage of data extraction compared to the direct extraction\nby multimodal models, with gains of 10-15% over commercial models and over 30%\nrelative to open-source models. Building on a curated database of over 30,000\nentries from 4,000 publications, we establish a rapid inverse design workflow\ncapable of identifying previously unreported hydrogen storage compositions in\ntwo minutes. The proposed AI workflow and agent design are broadly transferable\nacross diverse materials, providing a paradigm for AI-driven materials\ndiscovery.", "AI": {"tldr": "DIVE\u591a\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u4ece\u79d1\u5b66\u6587\u732e\u56fe\u8868\u4e2d\u81ea\u52a8\u63d0\u53d6\u6750\u6599\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u6570\u636e\u63d0\u53d6\u51c6\u786e\u6027\uff0c\u5efa\u7acb\u4e86\u5305\u542b3\u4e07\u6761\u6570\u636e\u7684\u6c22\u5b58\u50a8\u6750\u6599\u6570\u636e\u5e93\uff0c\u5b9e\u73b02\u5206\u949f\u5185\u9006\u5411\u8bbe\u8ba1\u65b0\u6750\u6599", "motivation": "\u79d1\u5b66\u6587\u732e\u4e2d\u7684\u5927\u91cf\u6750\u6599\u6570\u636e\u88ab\u56f0\u5728\u975e\u7ed3\u6784\u5316\u7684\u56fe\u8868\u4e2d\uff0c\u963b\u788d\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684AI\u667a\u80fd\u4f53\u8fdb\u884c\u81ea\u52a8\u5316\u6750\u6599\u8bbe\u8ba1", "method": "\u5f00\u53d1DIVE\u591a\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\uff0c\u7cfb\u7edf\u8bfb\u53d6\u548c\u7ec4\u7ec7\u79d1\u5b66\u6587\u732e\u56fe\u5f62\u5143\u7d20\u4e2d\u7684\u5b9e\u9a8c\u6570\u636e\uff0c\u4e13\u6ce8\u4e8e\u56fa\u6001\u6c22\u5b58\u50a8\u6750\u6599", "result": "DIVE\u76f8\u6bd4\u591a\u6a21\u6001\u6a21\u578b\u76f4\u63a5\u63d0\u53d6\uff0c\u51c6\u786e\u7387\u63d0\u534710-15%\uff08\u5546\u4e1a\u6a21\u578b\uff09\u548c30%\u4ee5\u4e0a\uff08\u5f00\u6e90\u6a21\u578b\uff09\uff0c\u5efa\u7acb\u4e86\u5305\u542b3\u4e07\u6761\u6570\u636e\u7684\u6570\u636e\u5e93\uff0c\u80fd\u57282\u5206\u949f\u5185\u8bc6\u522b\u672a\u62a5\u9053\u7684\u6c22\u5b58\u50a8\u6750\u6599\u6210\u5206", "conclusion": "\u8be5AI\u5de5\u4f5c\u6d41\u548c\u667a\u80fd\u4f53\u8bbe\u8ba1\u53ef\u5e7f\u6cdb\u8fc1\u79fb\u5230\u4e0d\u540c\u6750\u6599\u9886\u57df\uff0c\u4e3aAI\u9a71\u52a8\u7684\u6750\u6599\u53d1\u73b0\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f"}}
{"id": "2508.13459", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.13459", "abs": "https://arxiv.org/abs/2508.13459", "authors": ["Rohan Chandra", "Shubham Singh", "Abhishek Jha", "Dannon Andrade", "Hriday Sainathuni", "Katia Sycara"], "title": "Multi-Robot Navigation in Social Mini-Games: Definitions, Taxonomy, and Algorithms", "comment": null, "summary": "The ``Last Mile Challenge'' has long been considered an important, yet\nunsolved, challenge for autonomous vehicles, public service robots, and\ndelivery robots. A central issue in this challenge is the ability of robots to\nnavigate constrained and cluttered environments (e.g., doorways, hallways,\ncorridor intersections), often while competing for space with other robots and\nhumans. We refer to these environments as ``Social Mini-Games'' (SMGs). SMGs\nare tightly coupled, high-agency interactions that arise within general\nmulti-robot navigation (MRN) scenarios. They are identified through certain\ndistinct characteristics and require specialized metrics to evaluate them.\nTraditional navigation approaches designed for MRN do not perform well in SMGs,\nwhich has led to focused research on dedicated SMG solvers (navigation methods\nspecialized to navigate in SMGs), which has flourished in recent years.\nHowever, publications on SMG navigation research make different assumptions (on\ncentralized versus decentralized, observability, communication, cooperation,\netc.), and have different objective functions (safety versus liveness). These\nassumptions and objectives are sometimes implicitly assumed or described\ninformally. This makes it difficult to establish appropriate baselines for\ncomparison in research papers, as well as making it difficult for practitioners\nto find the papers relevant to their concrete application. Such ad-hoc\nrepresentation of the field also presents a barrier to new researchers wanting\nto start research in this area. SMG navigation research requires its own\ntaxonomy, definitions, and evaluation protocols to guide effective research\nmoving forward. This survey is the first to catalog SMG solvers using a\nwell-defined and unified taxonomy and to classify existing methods accordingly.", "AI": {"tldr": "\u672c\u6587\u5bf9\u793e\u4ea4\u5c0f\u6e38\u620f\uff08SMG\uff09\u5bfc\u822a\u7814\u7a76\u8fdb\u884c\u4e86\u9996\u6b21\u7cfb\u7edf\u6027\u5206\u7c7b\uff0c\u63d0\u51fa\u4e86\u7edf\u4e00\u5206\u7c7b\u6cd5\u548c\u8bc4\u4f30\u534f\u8bae\uff0c\u4ee5\u89e3\u51b3\u8be5\u9886\u57df\u7814\u7a76\u7f3a\u4e4f\u6807\u51c6\u5316\u7684\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\"\u6700\u540e\u4e00\u82f1\u91cc\u6311\u6218\"\u4e2d\u673a\u5668\u4eba\u5bfc\u822a\u5728\u53d7\u9650\u62e5\u6324\u73af\u5883\uff08SMG\uff09\u4e2d\u7684\u6027\u80fd\u95ee\u9898\uff0c\u5f53\u524dSMG\u5bfc\u822a\u7814\u7a76\u7f3a\u4e4f\u7edf\u4e00\u5206\u7c7b\u548c\u6807\u51c6\u5316\u8bc4\u4f30\uff0c\u5bfc\u81f4\u7814\u7a76\u6bd4\u8f83\u56f0\u96be\u548c\u65b0\u7814\u7a76\u8005\u5165\u95e8\u969c\u788d\u3002", "method": "\u901a\u8fc7\u5b9a\u4e49\u660e\u786e\u7684\u7edf\u4e00\u5206\u7c7b\u6cd5\u5bf9\u73b0\u6709SMG\u6c42\u89e3\u5668\u8fdb\u884c\u7f16\u76ee\u548c\u5206\u7c7b\uff0c\u5efa\u7acb\u4e13\u95e8\u7684\u8bc4\u4f30\u6307\u6807\u548c\u534f\u8bae\u3002", "result": "\u521b\u5efa\u4e86\u9996\u4e2aSMG\u5bfc\u822a\u7814\u7a76\u7684\u7cfb\u7edf\u6027\u5206\u7c7b\u6846\u67b6\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u57fa\u7840\u3002", "conclusion": "SMG\u5bfc\u822a\u7814\u7a76\u9700\u8981\u4e13\u95e8\u7684\u5206\u7c7b\u6cd5\u3001\u5b9a\u4e49\u548c\u8bc4\u4f30\u534f\u8bae\u6765\u6307\u5bfc\u6709\u6548\u7814\u7a76\uff0c\u672c\u8c03\u67e5\u4e3a\u8be5\u9886\u57df\u5efa\u7acb\u4e86\u5fc5\u8981\u7684\u6807\u51c6\u5316\u6846\u67b6\u3002"}}
{"id": "2508.13256", "categories": ["cs.AI", "cs.CY", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.13256", "abs": "https://arxiv.org/abs/2508.13256", "authors": ["Yuting Zhang", "Karina V. Bunting", "Asgher Champsi", "Xiaoxia Wang", "Wenqi Lu", "Alexander Thorley", "Sandeep S Hothi", "Zhaowen Qiu", "Dipak Kotecha", "Jinming Duan"], "title": "CardAIc-Agents: A Multimodal Framework with Hierarchical Adaptation for Cardiac Care Support", "comment": null, "summary": "Cardiovascular diseases (CVDs) remain the foremost cause of mortality\nworldwide, a burden worsened by a severe deficit of healthcare workers.\nArtificial intelligence (AI) agents have shown potential to alleviate this gap\nvia automated early detection and proactive screening, yet their clinical\napplication remains limited by: 1) prompt-based clinical role assignment that\nrelies on intrinsic model capabilities without domain-specific tool support; or\n2) rigid sequential workflows, whereas clinical care often requires adaptive\nreasoning that orders specific tests and, based on their results, guides\npersonalised next steps; 3) general and static knowledge bases without\ncontinuous learning capability; and 4) fixed unimodal or bimodal inputs and\nlack of on-demand visual outputs when further clarification is needed. In\nresponse, a multimodal framework, CardAIc-Agents, was proposed to augment\nmodels with external tools and adaptively support diverse cardiac tasks.\nSpecifically, a CardiacRAG agent generated general plans from updatable cardiac\nknowledge, while the chief agent integrated tools to autonomously execute these\nplans and deliver decisions. To enable adaptive and case-specific\ncustomization, a stepwise update strategy was proposed to dynamically refine\nplans based on preceding execution results, once the task was assessed as\ncomplex. In addition, a multidisciplinary discussion tool was introduced to\ninterpret challenging cases, thereby supporting further adaptation. When\nclinicians raised concerns, visual review panels were provided to assist final\nvalidation. Experiments across three datasets showed the efficiency of\nCardAIc-Agents compared to mainstream Vision-Language Models (VLMs),\nstate-of-the-art agentic systems, and fine-tuned VLMs.", "AI": {"tldr": "CardAIc-Agents\u662f\u4e00\u4e2a\u591a\u6a21\u6001AI\u6846\u67b6\uff0c\u901a\u8fc7\u5916\u90e8\u5de5\u5177\u589e\u5f3a\u6a21\u578b\u80fd\u529b\uff0c\u81ea\u9002\u5e94\u652f\u6301\u591a\u6837\u5316\u5fc3\u810f\u75be\u75c5\u4efb\u52a1\uff0c\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u4e3b\u6d41\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u6700\u5148\u8fdb\u7684\u4ee3\u7406\u7cfb\u7edf\u3002", "motivation": "\u5fc3\u8840\u7ba1\u75be\u75c5\u662f\u5168\u7403\u4e3b\u8981\u6b7b\u56e0\uff0c\u533b\u7597\u5de5\u4f5c\u8005\u4e25\u91cd\u77ed\u7f3a\u3002\u73b0\u6709AI\u7cfb\u7edf\u5b58\u5728\u4e34\u5e8a\u89d2\u8272\u5206\u914d\u4f9d\u8d56\u63d0\u793a\u5de5\u7a0b\u3001\u5de5\u4f5c\u6d41\u7a0b\u50f5\u5316\u3001\u77e5\u8bc6\u5e93\u9759\u6001\u3001\u8f93\u5165\u8f93\u51fa\u6a21\u5f0f\u56fa\u5b9a\u7b49\u95ee\u9898\uff0c\u9650\u5236\u4e86\u4e34\u5e8a\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u591a\u6a21\u6001\u6846\u67b6CardAIc-Agents\uff1a1)CardiacRAG\u4ee3\u7406\u4ece\u53ef\u66f4\u65b0\u5fc3\u810f\u77e5\u8bc6\u751f\u6210\u901a\u7528\u8ba1\u5212\uff1b2)\u4e3b\u4ee3\u7406\u96c6\u6210\u5de5\u5177\u81ea\u4e3b\u6267\u884c\u8ba1\u5212\uff1b3)\u9010\u6b65\u66f4\u65b0\u7b56\u7565\u52a8\u6001\u4f18\u5316\u590d\u6742\u4efb\u52a1\u8ba1\u5212\uff1b4)\u591a\u5b66\u79d1\u8ba8\u8bba\u5de5\u5177\u5904\u7406\u7591\u96be\u75c5\u4f8b\uff1b5)\u89c6\u89c9\u5ba1\u67e5\u9762\u677f\u8f85\u52a9\u6700\u7ec8\u9a8c\u8bc1\u3002", "result": "\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cCardAIc-Agents\u76f8\u6bd4\u4e3b\u6d41\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3001\u6700\u5148\u8fdb\u7684\u4ee3\u7406\u7cfb\u7edf\u548c\u5fae\u8c03\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5177\u6709\u66f4\u9ad8\u7684\u6548\u7387\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u5de5\u5177\u589e\u5f3a\u548c\u81ea\u9002\u5e94\u63a8\u7406\u80fd\u529b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709AI\u7cfb\u7edf\u5728\u5fc3\u8840\u7ba1\u75be\u75c5\u4e34\u5e8a\u5e94\u7528\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u81ea\u52a8\u5316\u65e9\u671f\u68c0\u6d4b\u548c\u4e3b\u52a8\u7b5b\u67e5\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2508.13488", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.13488", "abs": "https://arxiv.org/abs/2508.13488", "authors": ["Jingwen Yu", "Jiayi Yang", "Anjun Hu", "Jiankun Wang", "Ping Tan", "Hong Zhang"], "title": "ROVER: Robust Loop Closure Verification with Trajectory Prior in Repetitive Environments", "comment": "8 pages, 9 figures", "summary": "Loop closure detection is important for simultaneous localization and mapping\n(SLAM), which associates current observations with historical keyframes,\nachieving drift correction and global relocalization. However, a falsely\ndetected loop can be fatal, and this is especially difficult in repetitive\nenvironments where appearance-based features fail due to the high similarity.\nTherefore, verification of a loop closure is a critical step in avoiding false\npositive detections. Existing works in loop closure verification predominantly\nfocus on learning invariant appearance features, neglecting the prior knowledge\nof the robot's spatial-temporal motion cue, i.e., trajectory. In this letter,\nwe propose ROVER, a loop closure verification method that leverages the\nhistorical trajectory as a prior constraint to reject false loops in\nchallenging repetitive environments. For each loop candidate, it is first used\nto estimate the robot trajectory with pose-graph optimization. This trajectory\nis then submitted to a scoring scheme that assesses its compliance with the\ntrajectory without the loop, which we refer to as the trajectory prior, to\ndetermine if the loop candidate should be accepted. Benchmark comparisons and\nreal-world experiments demonstrate the effectiveness of the proposed method.\nFurthermore, we integrate ROVER into state-of-the-art SLAM systems to verify\nits robustness and efficiency. Our source code and self-collected dataset are\navailable at https://github.com/jarvisyjw/ROVER.", "AI": {"tldr": "ROVER\u662f\u4e00\u79cd\u5229\u7528\u5386\u53f2\u8f68\u8ff9\u4f5c\u4e3a\u5148\u9a8c\u7ea6\u675f\u7684\u95ed\u73af\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u4e13\u95e8\u9488\u5bf9\u91cd\u590d\u73af\u5883\u4e2d\u5916\u89c2\u7279\u5f81\u5931\u6548\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u8f68\u8ff9\u4f18\u5316\u548c\u8bc4\u5206\u673a\u5236\u6709\u6548\u62d2\u7edd\u9519\u8bef\u95ed\u73af\u68c0\u6d4b\u3002", "motivation": "\u5728\u91cd\u590d\u6027\u73af\u5883\u4e2d\uff0c\u57fa\u4e8e\u5916\u89c2\u7279\u5f81\u7684\u95ed\u73af\u68c0\u6d4b\u5bb9\u6613\u4ea7\u751f\u8bef\u5224\uff0c\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u4e86\u673a\u5668\u4eba\u7684\u65f6\u7a7a\u8fd0\u52a8\u8f68\u8ff9\u8fd9\u4e00\u91cd\u8981\u5148\u9a8c\u77e5\u8bc6\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5229\u7528\u8f68\u8ff9\u4fe1\u606f\u8fdb\u884c\u9a8c\u8bc1\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faROVER\u65b9\u6cd5\uff1a\u5bf9\u4e8e\u6bcf\u4e2a\u95ed\u73af\u5019\u9009\uff0c\u9996\u5148\u901a\u8fc7\u4f4d\u59ff\u56fe\u4f18\u5316\u4f30\u8ba1\u673a\u5668\u4eba\u8f68\u8ff9\uff0c\u7136\u540e\u901a\u8fc7\u8bc4\u5206\u65b9\u6848\u8bc4\u4f30\u8be5\u8f68\u8ff9\u4e0e\u65e0\u95ed\u73af\u65f6\u7684\u8f68\u8ff9\u5148\u9a8c\u7684\u4e00\u81f4\u6027\uff0c\u4ece\u800c\u51b3\u5b9a\u662f\u5426\u63a5\u53d7\u8be5\u95ed\u73af\u3002", "result": "\u57fa\u51c6\u6d4b\u8bd5\u548c\u771f\u5b9e\u73af\u5883\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u96c6\u6210\u5230\u6700\u5148\u8fdb\u7684SLAM\u7cfb\u7edf\u4e2d\u9a8c\u8bc1\u4e86\u5176\u9c81\u68d2\u6027\u548c\u6548\u7387\u3002", "conclusion": "ROVER\u901a\u8fc7\u5229\u7528\u5386\u53f2\u8f68\u8ff9\u4f5c\u4e3a\u5148\u9a8c\u7ea6\u675f\uff0c\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u91cd\u590d\u73af\u5883\u4e2d\u80fd\u591f\u6709\u6548\u62d2\u7edd\u9519\u8bef\u95ed\u73af\uff0c\u63d0\u9ad8\u4e86SLAM\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2508.13327", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13327", "abs": "https://arxiv.org/abs/2508.13327", "authors": ["Sarthak Khanna", "Armin Berger", "David Berghaus", "Tobias Deusser", "Lorenz Sparrenberg", "Rafet Sifa"], "title": "Towards Unified Multimodal Financial Forecasting: Integrating Sentiment Embeddings and Market Indicators via Cross-Modal Attention", "comment": "Accepted in IEEE-DSAA 2025", "summary": "We propose STONK (Stock Optimization using News Knowledge), a multimodal\nframework integrating numerical market indicators with sentiment-enriched news\nembeddings to improve daily stock-movement prediction. By combining numerical &\ntextual embeddings via feature concatenation and cross-modal attention, our\nunified pipeline addresses limitations of isolated analyses. Backtesting shows\nSTONK outperforms numeric-only baselines. A comprehensive evaluation of fusion\nstrategies and model configurations offers evidence-based guidance for scalable\nmultimodal financial forecasting. Source code is available on GitHub", "AI": {"tldr": "STONK\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u80a1\u7968\u9884\u6d4b\u6846\u67b6\uff0c\u7ed3\u5408\u6570\u503c\u5e02\u573a\u6307\u6807\u548c\u60c5\u611f\u589e\u5f3a\u7684\u65b0\u95fb\u5d4c\u5165\uff0c\u901a\u8fc7\u7279\u5f81\u62fc\u63a5\u548c\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u673a\u5236\u63d0\u5347\u80a1\u7968\u8d70\u52bf\u9884\u6d4b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u5355\u4e00\u5206\u6790\u65b9\u6cd5\uff08\u4ec5\u6570\u503c\u6216\u4ec5\u6587\u672c\uff09\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u6574\u5408\u591a\u6a21\u6001\u6570\u636e\u63d0\u9ad8\u80a1\u7968\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "\u91c7\u7528\u7279\u5f81\u62fc\u63a5\u548c\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u673a\u5236\u878d\u5408\u6570\u503c\u5e02\u573a\u6307\u6807\u548c\u60c5\u611f\u589e\u5f3a\u7684\u65b0\u95fb\u5d4c\u5165\uff0c\u6784\u5efa\u7edf\u4e00\u7684\u591a\u6a21\u6001\u9884\u6d4b\u7ba1\u9053\u3002", "result": "\u56de\u6d4b\u663e\u793aSTONK\u4f18\u4e8e\u4ec5\u4f7f\u7528\u6570\u503c\u6307\u6807\u7684\u57fa\u7ebf\u6a21\u578b\uff0c\u63d0\u4f9b\u4e86\u878d\u5408\u7b56\u7565\u548c\u6a21\u578b\u914d\u7f6e\u7684\u5b9e\u8bc1\u6307\u5bfc\u3002", "conclusion": "STONK\u4e3a\u53ef\u6269\u5c55\u7684\u591a\u6a21\u6001\u91d1\u878d\u9884\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6e90\u4ee3\u7801\u5df2\u5728GitHub\u4e0a\u5f00\u6e90\u3002"}}
{"id": "2508.13513", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.13513", "abs": "https://arxiv.org/abs/2508.13513", "authors": ["Maolin Lei", "Edoardo Romiti", "Arturo Laurenzi", "Cheng Zhou", "Wanli Xing", "Liang Lu", "Nikos G. Tsagarakis"], "title": "Unified Hierarchical MPC in Task Executing for Modular Manipulators across Diverse Morphologies", "comment": null, "summary": "This work proposes a unified Hierarchical Model Predictive Control (H-MPC)\nfor modular manipulators across various morphologies, as the controller can\nadapt to different configurations to execute the given task without extensive\nparameter tuning in the controller. The H-MPC divides the control process into\ntwo levels: a high-level MPC and a low-level MPC. The high-level MPC predicts\nfuture states and provides trajectory information, while the low-level MPC\nrefines control actions by updating the predictive model based on this\nhigh-level information. This hierarchical structure allows for the integration\nof kinematic constraints and ensures smooth joint-space trajectories, even near\nsingular configurations. Moreover, the low-level MPC incorporates secondary\nlinearization by leveraging predictive information from the high-level MPC,\neffectively capturing the second-order Taylor expansion information of the\nkinematic model while still maintaining a linearized model formulation. This\napproach not only preserves the simplicity of a linear control model but also\nenhances the accuracy of the kinematic representation, thereby improving\noverall control precision and reliability. To validate the effectiveness of the\ncontrol policy, we conduct extensive evaluations across different manipulator\nmorphologies and demonstrate the execution of pick-and-place tasks in\nreal-world scenarios.", "AI": {"tldr": "\u63d0\u51fa\u5206\u5c42\u6a21\u578b\u9884\u6d4b\u63a7\u5236(H-MPC)\u65b9\u6cd5\uff0c\u7528\u4e8e\u4e0d\u540c\u5f62\u6001\u6a21\u5757\u5316\u673a\u68b0\u81c2\u7684\u7edf\u4e00\u63a7\u5236\uff0c\u65e0\u9700\u5927\u91cf\u53c2\u6570\u8c03\u6574\u5373\u53ef\u9002\u5e94\u4e0d\u540c\u914d\u7f6e\u6267\u884c\u4efb\u52a1", "motivation": "\u89e3\u51b3\u6a21\u5757\u5316\u673a\u68b0\u81c2\u5728\u4e0d\u540c\u5f62\u6001\u914d\u7f6e\u4e0b\u7684\u7edf\u4e00\u63a7\u5236\u95ee\u9898\uff0c\u907f\u514d\u4e3a\u6bcf\u79cd\u914d\u7f6e\u5355\u72ec\u8c03\u6574\u63a7\u5236\u5668\u53c2\u6570", "method": "\u91c7\u7528\u4e24\u5c42MPC\u7ed3\u6784\uff1a\u9ad8\u5c42MPC\u9884\u6d4b\u672a\u6765\u72b6\u6001\u5e76\u63d0\u4f9b\u8f68\u8ff9\u4fe1\u606f\uff0c\u4f4e\u5c42MPC\u57fa\u4e8e\u9ad8\u5c42\u4fe1\u606f\u66f4\u65b0\u9884\u6d4b\u6a21\u578b\u5e76\u7ec6\u5316\u63a7\u5236\u52a8\u4f5c\uff0c\u7ed3\u5408\u4e8c\u6b21\u7ebf\u6027\u5316\u6280\u672f\u63d0\u5347\u8fd0\u52a8\u5b66\u6a21\u578b\u7cbe\u5ea6", "result": "\u65b9\u6cd5\u80fd\u591f\u5904\u7406\u8fd0\u52a8\u5b66\u7ea6\u675f\uff0c\u786e\u4fdd\u5e73\u6ed1\u7684\u5173\u8282\u7a7a\u95f4\u8f68\u8ff9\uff0c\u5373\u4f7f\u5728\u5947\u5f02\u914d\u7f6e\u9644\u8fd1\u4e5f\u80fd\u7a33\u5b9a\u5de5\u4f5c\uff0c\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u6210\u529f\u6267\u884c\u62fe\u653e\u4efb\u52a1", "conclusion": "H-MPC\u4e3a\u6a21\u5757\u5316\u673a\u68b0\u81c2\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u4e14\u7cbe\u786e\u7684\u63a7\u5236\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u7ebf\u6027\u63a7\u5236\u6a21\u578b\u7b80\u5355\u6027\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u63a7\u5236\u7cbe\u5ea6\u548c\u53ef\u9760\u6027"}}
{"id": "2508.13333", "categories": ["cs.AI", "cs.NE", "math.OC"], "pdf": "https://arxiv.org/pdf/2508.13333", "abs": "https://arxiv.org/abs/2508.13333", "authors": ["Chentong Chen", "Mengyuan Zhong", "Jianyong Sun", "Ye Fan", "Jialong Shi"], "title": "HiFo-Prompt: Prompting with Hindsight and Foresight for LLM-based Automatic Heuristic Design", "comment": "9 pages, 6 figures", "summary": "LLM-based Automatic Heuristic Design (AHD) within Evolutionary Computation\n(EC) frameworks has shown promising results. However, its effectiveness is\nhindered by the use of static operators and the lack of knowledge accumulation\nmechanisms. We introduce HiFo-Prompt, a framework that guides LLMs with two\nsynergistic prompting strategies: Foresight and Hindsight. Foresight-based\nprompts adaptively steer the search based on population dynamics, managing the\nexploration-exploitation trade-off. In addition, hindsight-based prompts mimic\nhuman expertise by distilling successful heuristics from past generations into\nfundamental, reusable design principles. This dual mechanism transforms\ntransient discoveries into a persistent knowledge base, enabling the LLM to\nlearn from its own experience. Empirical results demonstrate that HiFo-Prompt\nsignificantly outperforms state-of-the-art LLM-based AHD methods, generating\nhigher-quality heuristics while achieving substantially faster convergence and\nsuperior query efficiency.", "AI": {"tldr": "HiFo-Prompt\u6846\u67b6\u901a\u8fc7\u524d\u77bb\u6027\u548c\u540e\u987e\u6027\u63d0\u793a\u7b56\u7565\uff0c\u89e3\u51b3LLM\u81ea\u52a8\u542f\u53d1\u5f0f\u8bbe\u8ba1\u4e2d\u9759\u6001\u64cd\u4f5c\u7b26\u548c\u77e5\u8bc6\u79ef\u7d2f\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u542f\u53d1\u5f0f\u751f\u6210\u8d28\u91cf\u548c\u6536\u655b\u901f\u5ea6", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u81ea\u52a8\u542f\u53d1\u5f0f\u8bbe\u8ba1\u65b9\u6cd5\u4f7f\u7528\u9759\u6001\u64cd\u4f5c\u7b26\u4e14\u7f3a\u4e4f\u77e5\u8bc6\u79ef\u7d2f\u673a\u5236\uff0c\u9650\u5236\u4e86\u5176\u6709\u6548\u6027", "method": "\u63d0\u51faHiFo-Prompt\u6846\u67b6\uff0c\u5305\u542b\u4e24\u79cd\u534f\u540c\u63d0\u793a\u7b56\u7565\uff1a\u524d\u77bb\u6027\u63d0\u793a\u57fa\u4e8e\u79cd\u7fa4\u52a8\u6001\u81ea\u9002\u5e94\u5f15\u5bfc\u641c\u7d22\uff0c\u7ba1\u7406\u63a2\u7d22-\u5229\u7528\u6743\u8861\uff1b\u540e\u987e\u6027\u63d0\u793a\u4ece\u8fc7\u5f80\u6210\u529f\u542f\u53d1\u5f0f\u4e2d\u63d0\u70bc\u53ef\u91cd\u7528\u8bbe\u8ba1\u539f\u5219", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eHiFo-Prompt\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684LLM-based AHD\u65b9\u6cd5\uff0c\u751f\u6210\u66f4\u9ad8\u8d28\u91cf\u542f\u53d1\u5f0f\uff0c\u5b9e\u73b0\u66f4\u5feb\u6536\u655b\u548c\u66f4\u4f18\u67e5\u8be2\u6548\u7387", "conclusion": "\u53cc\u673a\u5236\u63d0\u793a\u7b56\u7565\u5c06\u77ac\u65f6\u53d1\u73b0\u8f6c\u5316\u4e3a\u6301\u4e45\u77e5\u8bc6\u5e93\uff0c\u4f7fLLM\u80fd\u591f\u4ece\u81ea\u8eab\u7ecf\u9a8c\u4e2d\u5b66\u4e60\uff0c\u6709\u6548\u63d0\u5347\u81ea\u52a8\u542f\u53d1\u5f0f\u8bbe\u8ba1\u6027\u80fd"}}
{"id": "2508.13531", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.13531", "abs": "https://arxiv.org/abs/2508.13531", "authors": ["Bolin Li", "Gewei Zuo", "Zhixiang Wang", "Xiaotian Ke", "Lijun Zhu", "Han Ding"], "title": "A Three-Level Whole-Body Disturbance Rejection Control Framework for Dynamic Motions in Legged Robots", "comment": null, "summary": "This paper presents a control framework designed to enhance the stability and\nrobustness of legged robots in the presence of uncertainties, including model\nuncertainties, external disturbances, and faults. The framework enables the\nfull-state feedback estimator to estimate and compensate for uncertainties in\nwhole-body dynamics of the legged robots. First, we propose a novel moving\nhorizon extended state observer (MH-ESO) to estimate uncertainties and mitigate\nnoise in legged systems, which can be integrated into the framework for\ndisturbance compensation. Second, we introduce a three-level whole-body\ndisturbance rejection control framework (T-WB-DRC). Unlike the previous\ntwo-level approach, this three-level framework considers both the plan based on\nwhole-body dynamics without uncertainties and the plan based on dynamics with\nuncertainties, significantly improving payload transportation, external\ndisturbance rejection, and fault tolerance. Third, simulations of both humanoid\nand quadruped robots in the Gazebo simulator demonstrate the effectiveness and\nversatility of T-WB-DRC. Finally, extensive experimental trials on a quadruped\nrobot validate the robustness and stability of the system when using T-WB-DRC\nunder various disturbance conditions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e09\u5c42\u6b21\u5168\u8eab\u6270\u52a8\u6291\u5236\u63a7\u5236\u6846\u67b6(T-WB-DRC)\uff0c\u901a\u8fc7\u79fb\u52a8\u89c6\u754c\u6269\u5c55\u72b6\u6001\u89c2\u6d4b\u5668\u4f30\u8ba1\u4e0d\u786e\u5b9a\u6027\uff0c\u663e\u8457\u63d0\u5347\u817f\u5f0f\u673a\u5668\u4eba\u5728\u8d1f\u8f7d\u8fd0\u8f93\u3001\u5916\u90e8\u6270\u52a8\u6291\u5236\u548c\u5bb9\u9519\u65b9\u9762\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u817f\u5f0f\u673a\u5668\u4eba\u5728\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u3001\u5916\u90e8\u6270\u52a8\u548c\u6545\u969c\u7b49\u590d\u6742\u73af\u5883\u4e0b\u7684\u7a33\u5b9a\u6027\u548c\u9c81\u68d2\u6027\u95ee\u9898\uff0c\u4f20\u7edf\u4e24\u5c42\u6b21\u63a7\u5236\u6846\u67b6\u5728\u5904\u7406\u4e0d\u786e\u5b9a\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\u3002", "method": "1. \u63d0\u51fa\u79fb\u52a8\u89c6\u754c\u6269\u5c55\u72b6\u6001\u89c2\u6d4b\u5668(MH-ESO)\u6765\u4f30\u8ba1\u7cfb\u7edf\u4e0d\u786e\u5b9a\u6027\u5e76\u6291\u5236\u566a\u58f0\uff1b2. \u8bbe\u8ba1\u4e09\u5c42\u6b21\u5168\u8eab\u6270\u52a8\u6291\u5236\u63a7\u5236\u6846\u67b6\uff0c\u540c\u65f6\u8003\u8651\u65e0\u4e0d\u786e\u5b9a\u6027\u548c\u6709\u4e0d\u786e\u5b9a\u6027\u7684\u52a8\u529b\u5b66\u89c4\u5212\uff1b3. \u5728Gazebo\u4eff\u771f\u548c\u4eba\u5f62/\u56db\u8db3\u673a\u5668\u4eba\u5b9e\u9a8c\u4e2d\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u4eff\u771f\u548c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cT-WB-DRC\u6846\u67b6\u5728\u5404\u79cd\u6270\u52a8\u6761\u4ef6\u4e0b\u90fd\u80fd\u6709\u6548\u63d0\u5347\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u548c\u7a33\u5b9a\u6027\uff0c\u7279\u522b\u662f\u5728\u8d1f\u8f7d\u8fd0\u8f93\u3001\u5916\u90e8\u6270\u52a8\u6291\u5236\u548c\u5bb9\u9519\u65b9\u9762\u8868\u73b0\u663e\u8457\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u4e09\u5c42\u6b21\u63a7\u5236\u6846\u67b6\u4e3a\u817f\u5f0f\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u66f4\u5f3a\u7684\u6297\u5e72\u6270\u80fd\u529b\u548c\u73af\u5883\u9002\u5e94\u6027\uff0c\u4e3a\u590d\u6742\u73af\u5883\u4e0b\u7684\u673a\u5668\u4eba\u63a7\u5236\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.13371", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13371", "abs": "https://arxiv.org/abs/2508.13371", "authors": ["Ronit Virwani", "Ruchika Suryawanshi"], "title": "LOOP: A Plug-and-Play Neuro-Symbolic Framework for Enhancing Planning in Autonomous Systems", "comment": "Submitted to IAAI-26", "summary": "Planning is one of the most critical tasks in autonomous systems, where even\na small error can lead to major failures or million-dollar losses. Current\nstate-of-the-art neural planning approaches struggle with complex domains,\nproducing plans with missing preconditions, inconsistent goals, and\nhallucinations. While classical planners provide logical guarantees, they lack\nthe flexibility and natural language understanding capabilities needed for\nmodern autonomous systems. Existing neuro-symbolic approaches use one-shot\ntranslation from natural language to formal plans, missing the opportunity for\nneural and symbolic components to work and refine solutions together. To\naddress this gap, we develop LOOP -- a novel neuro-symbolic planning framework\nthat treats planning as an iterative conversation between neural and symbolic\ncomponents rather than simple translation. LOOP integrates 13 coordinated\nneural features including graph neural networks for spatial relationships,\nmulti-agent validation for consensus-based correctness, hierarchical\ndecomposition for complex task management, and causal memory that learns from\nboth successes and failures. Unlike existing approaches, LOOP generates PDDL\nspecifications, refines them iteratively based on symbolic feedback, and builds\na causal knowledge base from execution traces. LOOP was evaluated on six\nstandard IPC benchmark domains, where it achieved 85.8% success rate compared\nto LLM+P (55.0%), LLM-as-Planner (19.2%), and Tree-of-Thoughts (3.3%). This\nwork shows that the key to reliable planning is not in choosing between neural\nnetworks or symbolic reasoners but it lies in making them actually ``talk'' to\neach other during the entire process. LOOP provides a thorough blueprint for\nbuilding autonomous systems that can finally be trusted with critical\nreal-world applications.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.13534", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.13534", "abs": "https://arxiv.org/abs/2508.13534", "authors": ["Chao Tang", "Anxing Xiao", "Yuhong Deng", "Tianrun Hu", "Wenlong Dong", "Hanbo Zhang", "David Hsu", "Hong Zhang"], "title": "MimicFunc: Imitating Tool Manipulation from a Single Human Video via Functional Correspondence", "comment": "Accepted to CoRL 2025", "summary": "Imitating tool manipulation from human videos offers an intuitive approach to\nteaching robots, while also providing a promising and scalable alternative to\nlabor-intensive teleoperation data collection for visuomotor policy learning.\nWhile humans can mimic tool manipulation behavior by observing others perform a\ntask just once and effortlessly transfer the skill to diverse tools for\nfunctionally equivalent tasks, current robots struggle to achieve this level of\ngeneralization. A key challenge lies in establishing function-level\ncorrespondences, considering the significant geometric variations among\nfunctionally similar tools, referred to as intra-function variations. To\naddress this challenge, we propose MimicFunc, a framework that establishes\nfunctional correspondences with function frame, a function-centric local\ncoordinate frame constructed with keypoint-based abstraction, for imitating\ntool manipulation skills. Experiments demonstrate that MimicFunc effectively\nenables the robot to generalize the skill from a single RGB-D human video to\nmanipulating novel tools for functionally equivalent tasks. Furthermore,\nleveraging MimicFunc's one-shot generalization capability, the generated\nrollouts can be used to train visuomotor policies without requiring\nlabor-intensive teleoperation data collection for novel objects. Our code and\nvideo are available at https://sites.google.com/view/mimicfunc.", "AI": {"tldr": "MimicFunc\u662f\u4e00\u4e2a\u4ece\u5355\u4e2a\u4eba\u7c7b\u89c6\u9891\u4e2d\u6a21\u4eff\u5de5\u5177\u64cd\u4f5c\u6280\u80fd\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u529f\u80fd\u5e27\u5efa\u7acb\u529f\u80fd\u7ea7\u5bf9\u5e94\u5173\u7cfb\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u6cdb\u5316\u5230\u65b0\u5de5\u5177", "motivation": "\u4eba\u7c7b\u80fd\u591f\u901a\u8fc7\u89c2\u5bdf\u4e00\u6b21\u5c31\u6a21\u4eff\u5de5\u5177\u64cd\u4f5c\u884c\u4e3a\u5e76\u8f7b\u677e\u5c06\u6280\u80fd\u8fc1\u79fb\u5230\u4e0d\u540c\u5de5\u5177\uff0c\u800c\u5f53\u524d\u673a\u5668\u4eba\u96be\u4ee5\u8fbe\u5230\u8fd9\u79cd\u6cdb\u5316\u6c34\u5e73\uff0c\u4e3b\u8981\u6311\u6218\u5728\u4e8e\u529f\u80fd\u7ea7\u5bf9\u5e94\u5173\u7cfb\u7684\u5efa\u7acb", "method": "\u63d0\u51faMimicFunc\u6846\u67b6\uff0c\u4f7f\u7528\u529f\u80fd\u5e27\uff08function-centric local coordinate frame\uff09\u548c\u57fa\u4e8e\u5173\u952e\u70b9\u7684\u62bd\u8c61\u6765\u5efa\u7acb\u529f\u80fd\u5bf9\u5e94\u5173\u7cfb", "result": "\u5b9e\u9a8c\u8bc1\u660eMimicFunc\u80fd\u6709\u6548\u8ba9\u673a\u5668\u4eba\u4ece\u5355\u4e2aRGB-D\u4eba\u7c7b\u89c6\u9891\u4e2d\u6cdb\u5316\u6280\u80fd\uff0c\u64cd\u4f5c\u65b0\u5de5\u5177\u5b8c\u6210\u529f\u80fd\u7b49\u6548\u4efb\u52a1\uff0c\u5e76\u53ef\u7528\u4e8e\u8bad\u7ec3\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u89e3\u51b3\u5de5\u5177\u64cd\u4f5c\u6280\u80fd\u6a21\u4eff\u4e2d\u7684\u529f\u80fd\u7ea7\u5bf9\u5e94\u6311\u6218\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\uff0c\u5177\u6709\u4e00\u6b21\u6027\u6cdb\u5316\u80fd\u529b\uff0c\u53ef\u51cf\u5c11\u4eba\u5de5\u9065\u64cd\u4f5c\u6570\u636e\u6536\u96c6\u7684\u9700\u6c42"}}
{"id": "2508.13387", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13387", "abs": "https://arxiv.org/abs/2508.13387", "authors": ["Thye Shan Ng", "Caren Soyeon Han", "Eun-Jung Holden"], "title": "SPANER: Shared Prompt Aligner for Multimodal Semantic Representation", "comment": null, "summary": "Recent advances in multimodal Parameter-Efficient Fine-Tuning (PEFT) have\nsignificantly improved performance on downstream tasks such as few-shot\nretrieval. However, most existing approaches focus on task-specific gains while\nneglecting the structure of the multimodal embedding space. As a result,\nmodality-specific representations often remain isolated, limiting cross-modal\ngeneralisation. In this work, we introduce Shared Prompt AligNER (SPANER), a\nmodality-agnostic PEFT framework designed to embed inputs from diverse\nmodalities into a unified semantic space. At its core, SPANER employs a shared\nprompt mechanism that acts as a conceptual anchor, enabling semantically\nrelated instances to converge spatially regardless of modality. This shared\nprompt design is inherently extensible, supporting the seamless integration of\nadditional modalities, such as audio, without altering the core architecture.\nThrough comprehensive experiments across vision-language and audio-visual\nbenchmarks, SPANER demonstrates competitive few-shot retrieval performance\nwhile preserving high semantic coherence in the learned embedding space. Our\nresults highlight the importance of aligning embedding structures, rather than\nmerely tuning adapter weights, for scalable multimodal learning.", "AI": {"tldr": "SPANER\u662f\u4e00\u4e2a\u6a21\u6001\u65e0\u5173\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u6846\u67b6\uff0c\u901a\u8fc7\u5171\u4eab\u63d0\u793a\u673a\u5236\u5c06\u4e0d\u540c\u6a21\u6001\u8f93\u5165\u5d4c\u5165\u5230\u7edf\u4e00\u8bed\u4e49\u7a7a\u95f4\uff0c\u63d0\u5347\u591a\u6a21\u6001\u5b66\u4e60\u7684\u53ef\u6269\u5c55\u6027\u548c\u8de8\u6a21\u6001\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u591a\u6a21\u6001PEFT\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u4efb\u52a1\u7279\u5b9a\u6027\u80fd\u63d0\u5347\uff0c\u4f46\u5ffd\u89c6\u4e86\u591a\u6a21\u6001\u5d4c\u5165\u7a7a\u95f4\u7684\u7ed3\u6784\uff0c\u5bfc\u81f4\u6a21\u6001\u7279\u5b9a\u8868\u793a\u5b64\u7acb\uff0c\u9650\u5236\u4e86\u8de8\u6a21\u6001\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51faSPANER\u6846\u67b6\uff0c\u91c7\u7528\u5171\u4eab\u63d0\u793a\u673a\u5236\u4f5c\u4e3a\u6982\u5ff5\u951a\u70b9\uff0c\u4f7f\u8bed\u4e49\u76f8\u5173\u7684\u5b9e\u4f8b\u5728\u7a7a\u95f4\u4e2d\u805a\u5408\uff0c\u65e0\u8bba\u5176\u6a21\u6001\u5982\u4f55\u3002\u8be5\u8bbe\u8ba1\u652f\u6301\u65e0\u7f1d\u96c6\u6210\u989d\u5916\u6a21\u6001\uff08\u5982\u97f3\u9891\uff09\u800c\u65e0\u9700\u6539\u53d8\u6838\u5fc3\u67b6\u6784\u3002", "result": "\u5728\u89c6\u89c9-\u8bed\u8a00\u548c\u97f3\u9891-\u89c6\u89c9\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSPANER\u5c55\u793a\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u5c11\u6837\u672c\u68c0\u7d22\u6027\u80fd\uff0c\u540c\u65f6\u5728\u5b66\u4e60\u7684\u5d4c\u5165\u7a7a\u95f4\u4e2d\u4fdd\u6301\u4e86\u9ad8\u8bed\u4e49\u4e00\u81f4\u6027\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u8c03\u6574\u5d4c\u5165\u7a7a\u95f4\u7ed3\u6784\u7684\u91cd\u8981\u6027\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u8c03\u6574\u9002\u914d\u5668\u6743\u91cd\uff0c\u8fd9\u5bf9\u4e8e\u53ef\u6269\u5c55\u7684\u591a\u6a21\u6001\u5b66\u4e60\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2508.13699", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.13699", "abs": "https://arxiv.org/abs/2508.13699", "authors": ["Maren Raab", "Linda Miller", "Zhe Zeng", "Pascal Jansen", "Martin Baumann", "Johannes Kraus"], "title": "Assessing Pedestrian Behavior Around Autonomous Cleaning Robots in Public Spaces: Findings from a Field Observation", "comment": null, "summary": "As autonomous robots become more common in public spaces, spontaneous\nencounters with laypersons are more frequent. For this, robots need to be\nequipped with communication strategies that enhance momentary transparency and\nreduce the probability of critical situations. Adapting these robotic\nstrategies requires consideration of robot movements, environmental conditions,\nand user characteristics and states. While numerous studies have investigated\nthe impact of distraction on pedestrians' movement behavior, limited research\nhas examined this behavior in the presence of autonomous robots. This research\naddresses the impact of robot type and robot movement pattern on distracted and\nundistracted pedestrians' movement behavior. In a field setting, unaware\npedestrians were videotaped while moving past two working, autonomous cleaning\nrobots. Out of N=498 observed pedestrians, approximately 8% were distracted by\nsmartphones. Distracted and undistracted pedestrians did not exhibit\nsignificant differences in their movement behaviors around the robots. Instead,\nboth the larger sweeping robot and the offset rectangular movement pattern\nsignificantly increased the number of lateral adaptations compared to the\nsmaller cleaning robot and the circular movement pattern. The offset\nrectangular movement pattern also led to significantly more close lateral\nadaptations. Depending on the robot type, the movement patterns led to\ndifferences in the distances of lateral adaptations. The study provides initial\ninsights into pedestrian movement behavior around an autonomous cleaning robot\nin public spaces, contributing to the growing field of HRI research.", "AI": {"tldr": "\u7814\u7a76\u63a2\u7d22\u81ea\u4e3b\u6e05\u6d01\u673a\u5668\u4eba\u5728\u516c\u5171\u573a\u6240\u5bf9\u884c\u4eba\u79fb\u52a8\u884c\u4e3a\u7684\u5f71\u54cd\uff0c\u91cd\u70b9\u5173\u6ce8\u673a\u5668\u4eba\u7c7b\u578b\u548c\u79fb\u52a8\u6a21\u5f0f\u5bf9\u5206\u5fc3\u4e0e\u672a\u5206\u5fc3\u884c\u4eba\u7684\u4e0d\u540c\u6548\u5e94\u3002", "motivation": "\u968f\u7740\u81ea\u4e3b\u673a\u5668\u4eba\u5728\u516c\u5171\u573a\u6240\u65e5\u76ca\u666e\u53ca\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u589e\u5f3a\u5373\u65f6\u900f\u660e\u5ea6\u5e76\u51cf\u5c11\u5173\u952e\u60c5\u51b5\u53d1\u751f\u6982\u7387\u7684\u901a\u4fe1\u7b56\u7565\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u5173\u4e8e\u5206\u5fc3\u884c\u4eba\u5728\u673a\u5668\u4eba\u5b58\u5728\u4e0b\u884c\u4e3a\u7684\u7814\u7a76\u3002", "method": "\u5728\u5b9e\u5730\u73af\u5883\u4e2d\uff0c\u5bf9498\u540d\u4e0d\u77e5\u60c5\u7684\u884c\u4eba\u8fdb\u884c\u5f55\u50cf\u89c2\u5bdf\uff0c\u8bb0\u5f55\u4ed6\u4eec\u7ecf\u8fc7\u4e24\u4e2a\u5de5\u4f5c\u7684\u81ea\u4e3b\u6e05\u6d01\u673a\u5668\u4eba\u65f6\u7684\u79fb\u52a8\u884c\u4e3a\uff0c\u5206\u6790\u673a\u5668\u4eba\u7c7b\u578b\uff08\u5927\u5c0f\uff09\u548c\u79fb\u52a8\u6a21\u5f0f\uff08\u5706\u5f62vs\u504f\u79fb\u77e9\u5f62\uff09\u7684\u5f71\u54cd\u3002", "result": "\u5206\u5fc3\u4e0e\u672a\u5206\u5fc3\u884c\u4eba\u5728\u673a\u5668\u4eba\u5468\u56f4\u7684\u79fb\u52a8\u884c\u4e3a\u65e0\u663e\u8457\u5dee\u5f02\uff1b\u8f83\u5927\u7684\u626b\u5730\u673a\u5668\u4eba\u548c\u504f\u79fb\u77e9\u5f62\u79fb\u52a8\u6a21\u5f0f\u663e\u8457\u589e\u52a0\u4e86\u6a2a\u5411\u9002\u5e94\u6b21\u6570\uff1b\u504f\u79fb\u77e9\u5f62\u6a21\u5f0f\u5bfc\u81f4\u66f4\u591a\u8fd1\u8ddd\u79bb\u6a2a\u5411\u9002\u5e94\uff1b\u4e0d\u540c\u673a\u5668\u4eba\u7c7b\u578b\u4e0b\u79fb\u52a8\u6a21\u5f0f\u5bf9\u6a2a\u5411\u9002\u5e94\u8ddd\u79bb\u4ea7\u751f\u5dee\u5f02\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u4e3a\u516c\u5171\u573a\u6240\u81ea\u4e3b\u6e05\u6d01\u673a\u5668\u4eba\u5468\u56f4\u884c\u4eba\u79fb\u52a8\u884c\u4e3a\u63d0\u4f9b\u4e86\u521d\u6b65\u89c1\u89e3\uff0c\u5bf9HRI\u7814\u7a76\u9886\u57df\u6709\u8d21\u732e\uff0c\u5f3a\u8c03\u673a\u5668\u4eba\u7269\u7406\u7279\u6027\u548c\u79fb\u52a8\u6a21\u5f0f\u8bbe\u8ba1\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2508.13404", "categories": ["cs.AI", "cs.CL", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.13404", "abs": "https://arxiv.org/abs/2508.13404", "authors": ["Nicole Cho", "Kirsty Fielding", "William Watson", "Sumitra Ganesh", "Manuela Veloso"], "title": "TASER: Table Agents for Schema-guided Extraction and Recommendation", "comment": null, "summary": "Real-world financial documents report essential information about an entity's\nfinancial holdings that can span millions of different financial instrument\ntypes. Yet, these details are often buried in messy, multi-page, fragmented\ntables - for example, 99.4% of the tables in our dataset have no bounding boxes\nwith the maximum number of rows amounting to 426 per table across 44 pages. To\ntackle these unique challenges from real-world tables, we present a\ncontinuously learning, agentic table extraction system, TASER (Table Agents for\nSchema-guided Extraction and Recommendation) that extracts highly unstructured,\nmulti-page, heterogeneous tables into normalized, schema-conforming outputs.\nOur table agents execute on table detection, classification, extraction, and\nrecommendations by leveraging an initial schema. Then, our Recommender Agent\nreviews the outputs, recommends schema revisions, and decides on the final\nrecommendations, enabling TASER to outperform existing table detection models\nsuch as Table Transformer by 10.1%. Within this continuous learning process, we\nhighlight that larger batch sizes result in a 104.3% increase in schema\nrecommendations that are actionable and utilized, resulting in a 9.8% increase\nin extracted holdings - highlighting the importance of a continuous learning\nprocess. To train TASER, we have manually labeled 22,584 pages (28,150,449\ntokens), 3,213 tables for $731,685,511,687 of holdings culminating in one of\nthe first real financial table datasets. We release our dataset TASERTab to\nenable the research community to access real-world financial tables and\noutputs. Our results highlight the promise of agentic, schema-guided extraction\nsystems for robust understanding of real-world financial tables.", "AI": {"tldr": "TASER\u662f\u4e00\u4e2a\u6301\u7eed\u5b66\u4e60\u7684\u667a\u80fd\u8868\u683c\u63d0\u53d6\u7cfb\u7edf\uff0c\u4e13\u95e8\u5904\u7406\u73b0\u5b9e\u4e16\u754c\u4e2d\u9ad8\u5ea6\u975e\u7ed3\u6784\u5316\u3001\u591a\u9875\u3001\u5f02\u6784\u7684\u91d1\u878d\u8868\u683c\uff0c\u901a\u8fc7\u6a21\u5f0f\u5f15\u5bfc\u7684\u63d0\u53d6\u548c\u63a8\u8350\u673a\u5236\uff0c\u5728\u8868\u683c\u68c0\u6d4b\u6027\u80fd\u4e0a\u8d85\u8d8a\u73b0\u6709\u6a21\u578b10.1%\uff0c\u5e76\u80fd\u663e\u8457\u63d0\u5347\u63d0\u53d6\u7684\u91d1\u878d\u6301\u4ed3\u6570\u636e\u91cf\u3002", "motivation": "\u73b0\u5b9e\u91d1\u878d\u6587\u6863\u4e2d\u7684\u8868\u683c\u4fe1\u606f\u5f80\u5f80\u57cb\u85cf\u5728\u6df7\u4e71\u3001\u591a\u9875\u3001\u788e\u7247\u5316\u7684\u8868\u683c\u4e2d\uff0899.4%\u7684\u8868\u683c\u6ca1\u6709\u8fb9\u754c\u6846\uff0c\u6700\u591a426\u884c\u8de844\u9875\uff09\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u5904\u7406\u8fd9\u4e9b\u72ec\u7279\u7684\u6311\u6218\u3002", "method": "\u5f00\u53d1\u4e86TASER\u7cfb\u7edf\uff0c\u5305\u542b\u8868\u683c\u68c0\u6d4b\u3001\u5206\u7c7b\u3001\u63d0\u53d6\u548c\u63a8\u8350\u4ee3\u7406\uff0c\u5229\u7528\u521d\u59cb\u6a21\u5f0f\u6267\u884c\u4efb\u52a1\uff0c\u7136\u540e\u901a\u8fc7\u63a8\u8350\u4ee3\u7406\u5ba1\u67e5\u8f93\u51fa\u3001\u5efa\u8bae\u6a21\u5f0f\u4fee\u8ba2\u5e76\u51b3\u5b9a\u6700\u7ec8\u63a8\u8350\uff0c\u5b9e\u73b0\u6301\u7eed\u5b66\u4e60\u3002", "result": "TASER\u5728\u8868\u683c\u68c0\u6d4b\u4e0a\u6bd4Table Transformer\u6a21\u578b\u63d0\u534710.1%\uff1b\u66f4\u5927\u7684\u6279\u91cf\u5927\u5c0f\u4f7f\u53ef\u64cd\u4f5c\u548c\u4f7f\u7528\u7684\u6a21\u5f0f\u5efa\u8bae\u589e\u52a0104.3%\uff0c\u63d0\u53d6\u7684\u6301\u4ed3\u6570\u636e\u589e\u52a09.8%\uff1b\u521b\u5efa\u4e86\u5305\u542b22,584\u9875\u30013,213\u4e2a\u8868\u683c\u30017310\u4ebf\u7f8e\u5143\u6301\u4ed3\u7684\u771f\u5b9e\u91d1\u878d\u8868\u683c\u6570\u636e\u96c6TASERTab\u3002", "conclusion": "\u57fa\u4e8e\u4ee3\u7406\u7684\u6a21\u5f0f\u5f15\u5bfc\u63d0\u53d6\u7cfb\u7edf\u5728\u7406\u89e3\u73b0\u5b9e\u4e16\u754c\u91d1\u878d\u8868\u683c\u65b9\u9762\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u6301\u7eed\u5b66\u4e60\u8fc7\u7a0b\u5bf9\u4e8e\u63d0\u5347\u63d0\u53d6\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2508.13785", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.13785", "abs": "https://arxiv.org/abs/2508.13785", "authors": ["Liyang Liu", "Ehsan Mihankhah", "Nathan Wallace", "Javier Martinez", "Andrew J. Hill"], "title": "Blast Hole Seeking and Dipping -- The Navigation and Perception Framework in a Mine Site Inspection Robot", "comment": null, "summary": "In open-pit mining, holes are drilled into the surface of the excavation site\nand detonated with explosives to facilitate digging. These blast holes need to\nbe inspected internally for investigation of downhole material types and\nproperties. Knowing these properties can lead to significant savings in\nmaterial handling costs in downstream processes. Manual hole inspection is slow\nand expensive, with major limitations in revealing the geometric and geological\nproperties of the holes and their contents. This has been the motivation for\nthe development of our autonomous mine-site inspection robot - \"DIPPeR\". In\nthis paper, the automation aspect of the project is explained. We present a\nrobust blast hole seeking and detection framework that enables target-based\nnavigation and accurate down-hole sensor positioning. The pipeline first\nprocesses point-cloud data collected by the on-board LiDAR sensors, extracting\nthe cone-shaped volume of drill-waste above the ground. By projecting the 3D\ncone points into a virtual depth image, segmentation is achieved in the 2D\ndomain, yielding a circular hole at the image centre and a collared cone face.\nWe then identify the hole centre using a robust detection module while\nsuppressing non-maximum candidates, ensuring precise sensor placement for\ndown-hole inspection and avoiding collisions with the cavity wall. To enable\nautonomous hole-seeking, the pipeline automatically adjusts its projection\nparameters during robot navigation to account for variations in point sparsity\nand hole opening size, ensuring a consistent hole appearance in 2D images. This\nallows continuous tracking of the target hole as the robot approaches the goal\npoint. We demonstrate the effectiveness of our navigation and perception system\nin both high-fidelity simulation environments and on-site field tests. A\ndemonstration video is available at\n\"https://www.youtube.com/watch?v=fRNbcBcaSqE\".", "AI": {"tldr": "\u5f00\u53d1\u4e86DIPPeR\u81ea\u4e3b\u77ff\u5c71\u68c0\u6d4b\u673a\u5668\u4eba\uff0c\u901a\u8fc7LiDAR\u70b9\u4e91\u5904\u7406\u548c2D\u6295\u5f71\u5b9e\u73b0\u7206\u7834\u5b54\u7684\u81ea\u52a8\u68c0\u6d4b\u4e0e\u5bfc\u822a\uff0c\u63d0\u9ad8\u94bb\u5b54\u68c0\u6d4b\u6548\u7387\u548c\u7cbe\u5ea6", "motivation": "\u9732\u5929\u91c7\u77ff\u4e2d\u7206\u7834\u5b54\u7684\u624b\u52a8\u68c0\u6d4b\u6548\u7387\u4f4e\u3001\u6210\u672c\u9ad8\uff0c\u4e14\u96be\u4ee5\u51c6\u786e\u83b7\u53d6\u5b54\u6d1e\u51e0\u4f55\u548c\u5730\u8d28\u7279\u6027\uff0c\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u6765\u964d\u4f4e\u7269\u6599\u5904\u7406\u6210\u672c", "method": "\u4f7f\u7528LiDAR\u91c7\u96c6\u70b9\u4e91\u6570\u636e\uff0c\u63d0\u53d6\u9525\u5f62\u94bb\u5b54\u5e9f\u6599\u4f53\u79ef\uff0c\u5c063D\u70b9\u4e91\u6295\u5f71\u5230\u865a\u62df\u6df1\u5ea6\u56fe\u50cf\u8fdb\u884c2D\u5206\u5272\uff0c\u901a\u8fc7\u7a33\u5065\u68c0\u6d4b\u6a21\u5757\u8bc6\u522b\u5b54\u4e2d\u5fc3\u5e76\u6291\u5236\u975e\u6700\u5927\u5019\u9009\u70b9\uff0c\u5b9e\u73b0\u7cbe\u786e\u4f20\u611f\u5668\u5b9a\u4f4d", "result": "\u7cfb\u7edf\u5728\u9ad8\u4fdd\u771f\u4eff\u771f\u73af\u5883\u548c\u73b0\u573a\u6d4b\u8bd5\u4e2d\u5747\u8868\u73b0\u51fa\u6709\u6548\u6027\uff0c\u80fd\u591f\u8fde\u7eed\u8ddf\u8e2a\u76ee\u6807\u5b54\u6d1e\u5e76\u786e\u4fdd\u4f20\u611f\u5668\u51c6\u786e\u5b9a\u4f4d\uff0c\u907f\u514d\u4e0e\u5b54\u58c1\u78b0\u649e", "conclusion": "\u63d0\u51fa\u7684\u81ea\u52a8\u5316\u68c0\u6d4b\u6846\u67b6\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u7206\u7834\u5b54\u68c0\u6d4b\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u4e3a\u77ff\u5c71\u4f5c\u4e1a\u5e26\u6765\u5b9e\u8d28\u6027\u6210\u672c\u8282\u7ea6"}}
{"id": "2508.13421", "categories": ["cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2508.13421", "abs": "https://arxiv.org/abs/2508.13421", "authors": ["Gabrielle Wehr", "Reuben Rideaux", "Amaya J. Fox", "David R. Lightfoot", "Jason Tangen", "Jason B. Mattingley", "Shane E. Ehrhardt"], "title": "Virtuous Machines: Towards Artificial General Science", "comment": null, "summary": "Artificial intelligence systems are transforming scientific discovery by\naccelerating specific research tasks, from protein structure prediction to\nmaterials design, yet remain confined to narrow domains requiring substantial\nhuman oversight. The exponential growth of scientific literature and increasing\ndomain specialisation constrain researchers' capacity to synthesise knowledge\nacross disciplines and develop unifying theories, motivating exploration of\nmore general-purpose AI systems for science. Here we show that a\ndomain-agnostic, agentic AI system can independently navigate the scientific\nworkflow - from hypothesis generation through data collection to manuscript\npreparation. The system autonomously designed and executed three psychological\nstudies on visual working memory, mental rotation, and imagery vividness,\nexecuted one new online data collection with 288 participants, developed\nanalysis pipelines through 8-hour+ continuous coding sessions, and produced\ncompleted manuscripts. The results demonstrate the capability of AI scientific\ndiscovery pipelines to conduct non-trivial research with theoretical reasoning\nand methodological rigour comparable to experienced researchers, though with\nlimitations in conceptual nuance and theoretical interpretation. This is a step\ntoward embodied AI that can test hypotheses through real-world experiments,\naccelerating discovery by autonomously exploring regions of scientific space\nthat human cognitive and resource constraints might otherwise leave unexplored.\nIt raises important questions about the nature of scientific understanding and\nthe attribution of scientific credit.", "AI": {"tldr": "AI\u7cfb\u7edf\u81ea\u4e3b\u5b8c\u6210\u5fc3\u7406\u5b66\u7814\u7a76\u5168\u6d41\u7a0b\uff1a\u5047\u8bbe\u751f\u6210\u3001\u6570\u636e\u6536\u96c6\u3001\u5206\u6790\u5230\u8bba\u6587\u64b0\u5199\uff0c\u5c55\u793a\u4e86AI\u79d1\u5b66\u53d1\u73b0\u80fd\u529b", "motivation": "\u79d1\u5b66\u6587\u732e\u7206\u70b8\u5f0f\u589e\u957f\u548c\u9886\u57df\u4e13\u4e1a\u5316\u9650\u5236\u4e86\u8de8\u5b66\u79d1\u77e5\u8bc6\u6574\u5408\uff0c\u9700\u8981\u66f4\u901a\u7528\u7684AI\u7cfb\u7edf\u6765\u52a0\u901f\u79d1\u5b66\u53d1\u73b0", "method": "\u4f7f\u7528\u9886\u57df\u65e0\u5173\u7684AI\u4ee3\u7406\u7cfb\u7edf\uff0c\u81ea\u4e3b\u8bbe\u8ba1\u5e76\u6267\u884c\u4e09\u4e2a\u5fc3\u7406\u5b66\u7814\u7a76\uff08\u89c6\u89c9\u5de5\u4f5c\u8bb0\u5fc6\u3001\u5fc3\u7406\u65cb\u8f6c\u3001\u610f\u8c61\u751f\u52a8\u5ea6\uff09\uff0c\u6536\u96c6288\u540d\u53c2\u4e0e\u8005\u6570\u636e\uff0c\u5f00\u53d1\u5206\u6790\u6d41\u7a0b", "result": "AI\u7cfb\u7edf\u80fd\u591f\u8fdb\u884c\u975e\u5e73\u51e1\u7814\u7a76\uff0c\u7406\u8bba\u63a8\u7406\u548c\u65b9\u6cd5\u4e25\u8c28\u6027\u582a\u6bd4\u7ecf\u9a8c\u4e30\u5bcc\u7684\u7814\u7a76\u4eba\u5458\uff0c\u4f46\u5728\u6982\u5ff5\u7ec6\u5fae\u5dee\u522b\u548c\u7406\u8bba\u89e3\u91ca\u65b9\u9762\u5b58\u5728\u5c40\u9650", "conclusion": "\u8fd9\u662f\u5411\u80fd\u591f\u901a\u8fc7\u771f\u5b9e\u5b9e\u9a8c\u6d4b\u8bd5\u5047\u8bbe\u7684\u5177\u8eabAI\u8fc8\u51fa\u7684\u4e00\u6b65\uff0c\u53ef\u81ea\u4e3b\u63a2\u7d22\u4eba\u7c7b\u8ba4\u77e5\u548c\u8d44\u6e90\u9650\u5236\u65e0\u6cd5\u89e6\u53ca\u7684\u79d1\u5b66\u9886\u57df\uff0c\u5f15\u53d1\u4e86\u5173\u4e8e\u79d1\u5b66\u7406\u89e3\u672c\u8d28\u548c\u79d1\u5b66\u8d21\u732e\u5f52\u5c5e\u7684\u91cd\u8981\u95ee\u9898"}}
{"id": "2508.13795", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.13795", "abs": "https://arxiv.org/abs/2508.13795", "authors": ["Haitham El-Hussieny"], "title": "Trajectory Tracking and Stabilization of Quadrotors Using Deep Koopman Model Predictive Control", "comment": null, "summary": "This paper presents a data-driven control framework for quadrotor systems\nthat integrates a deep Koopman operator with model predictive control (DK-MPC).\nThe deep Koopman operator is trained on sampled flight data to construct a\nhigh-dimensional latent representation in which the nonlinear quadrotor\ndynamics are approximated by linear models. This linearization enables the\napplication of MPC to efficiently optimize control actions over a finite\nprediction horizon, ensuring accurate trajectory tracking and stabilization.\nThe proposed DK-MPC approach is validated through a series of\ntrajectory-following and point-stabilization numerical experiments, where it\ndemonstrates superior tracking accuracy and significantly lower computation\ntime compared to conventional nonlinear MPC. These results highlight the\npotential of Koopman-based learning methods to handle complex quadrotor\ndynamics while meeting the real-time requirements of embedded flight control.\nFuture work will focus on extending the framework to more agile flight\nscenarios and improving robustness against external disturbances.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6df1\u5ea6Koopman\u7b97\u5b50\u548c\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u7684\u6570\u636e\u9a71\u52a8\u63a7\u5236\u6846\u67b6(DK-MPC)\uff0c\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u6784\u5efa\u9ad8\u7ef4\u6f5c\u5728\u7a7a\u95f4\u7684\u7ebf\u6027\u6a21\u578b\u8868\u793a\uff0c\u5b9e\u73b0\u56db\u65cb\u7ffc\u7cfb\u7edf\u7684\u7cbe\u786e\u8f68\u8ff9\u8ddf\u8e2a\u548c\u7a33\u5b9a\u63a7\u5236", "motivation": "\u4f20\u7edf\u975e\u7ebf\u6027MPC\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\uff0c\u96be\u4ee5\u6ee1\u8db3\u5d4c\u5165\u5f0f\u98de\u63a7\u5b9e\u65f6\u6027\u8981\u6c42\uff0c\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u5904\u7406\u590d\u6742\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u53c8\u80fd\u9ad8\u6548\u8ba1\u7b97\u7684\u63a7\u5236\u65b9\u6cd5", "method": "\u4f7f\u7528\u6df1\u5ea6Koopman\u7b97\u5b50\u4ece\u98de\u884c\u6570\u636e\u4e2d\u5b66\u4e60\uff0c\u6784\u5efa\u9ad8\u7ef4\u6f5c\u5728\u7a7a\u95f4\u7684\u7ebf\u6027\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u7136\u540e\u5e94\u7528MPC\u8fdb\u884c\u6709\u9650\u65f6\u57df\u4f18\u5316\u63a7\u5236", "result": "\u6570\u503c\u5b9e\u9a8c\u663e\u793a\u76f8\u6bd4\u4f20\u7edf\u975e\u7ebf\u6027MPC\u5177\u6709\u66f4\u9ad8\u7684\u8ddf\u8e2a\u7cbe\u5ea6\u548c\u663e\u8457\u964d\u4f4e\u7684\u8ba1\u7b97\u65f6\u95f4\uff0c\u6ee1\u8db3\u5b9e\u65f6\u63a7\u5236\u8981\u6c42", "conclusion": "Koopman\u5b66\u4e60\u65b9\u6cd5\u80fd\u6709\u6548\u5904\u7406\u590d\u6742\u56db\u65cb\u7ffc\u52a8\u529b\u5b66\uff0c\u672a\u6765\u5c06\u6269\u5c55\u5230\u66f4\u654f\u6377\u98de\u884c\u573a\u666f\u5e76\u63d0\u5347\u6297\u5916\u90e8\u5e72\u6270\u9c81\u68d2\u6027"}}
{"id": "2508.13433", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13433", "abs": "https://arxiv.org/abs/2508.13433", "authors": ["Jiayu Fang", "Zhiqi Shao", "S T Boris Choy", "Junbin Gao"], "title": "STPFormer: A State-of-the-Art Pattern-Aware Spatio-Temporal Transformer for Traffic Forecasting", "comment": null, "summary": "Spatio-temporal traffic forecasting is challenging due to complex temporal\npatterns, dynamic spatial structures, and diverse input formats. Although\nTransformer-based models offer strong global modeling, they often struggle with\nrigid temporal encoding and weak space-time fusion. We propose STPFormer, a\nSpatio-Temporal Pattern-Aware Transformer that achieves state-of-the-art\nperformance via unified and interpretable representation learning. It\nintegrates four modules: Temporal Position Aggregator (TPA) for pattern-aware\ntemporal encoding, Spatial Sequence Aggregator (SSA) for sequential spatial\nlearning, Spatial-Temporal Graph Matching (STGM) for cross-domain alignment,\nand an Attention Mixer for multi-scale fusion. Experiments on five real-world\ndatasets show that STPFormer consistently sets new SOTA results, with ablation\nand visualizations confirming its effectiveness and generalizability.", "AI": {"tldr": "STPFormer\u662f\u4e00\u4e2a\u65f6\u7a7a\u6a21\u5f0f\u611f\u77e5Transformer\u6a21\u578b\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u5728\u4ea4\u901a\u9884\u6d4b\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5305\u542b\u56db\u4e2a\u6838\u5fc3\u6a21\u5757\u6765\u5904\u7406\u590d\u6742\u7684\u65f6\u7a7a\u6a21\u5f0f\u3002", "motivation": "\u73b0\u6709\u7684Transformer\u6a21\u578b\u5728\u65f6\u7a7a\u4ea4\u901a\u9884\u6d4b\u4e2d\u5b58\u5728\u65f6\u95f4\u7f16\u7801\u50f5\u5316\u548c\u65f6\u7a7a\u878d\u5408\u80fd\u529b\u5f31\u7684\u95ee\u9898\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u590d\u6742\u7684\u65f6\u7a7a\u6a21\u5f0f\u548c\u591a\u6837\u5316\u7684\u8f93\u5165\u683c\u5f0f\u3002", "method": "\u63d0\u51fa\u4e86STPFormer\u6a21\u578b\uff0c\u5305\u542b\u56db\u4e2a\u6a21\u5757\uff1aTemporal Position Aggregator\uff08TPA\uff09\u7528\u4e8e\u6a21\u5f0f\u611f\u77e5\u7684\u65f6\u95f4\u7f16\u7801\uff0cSpatial Sequence Aggregator\uff08SSA\uff09\u7528\u4e8e\u5e8f\u5217\u7a7a\u95f4\u5b66\u4e60\uff0cSpatial-Temporal Graph Matching\uff08STGM\uff09\u7528\u4e8e\u8de8\u57df\u5bf9\u9f50\uff0c\u4ee5\u53caAttention Mixer\u7528\u4e8e\u591a\u5c3a\u5ea6\u878d\u5408\u3002", "result": "\u5728\u4e94\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSTPFormer\u59cb\u7ec8\u8fbe\u5230\u65b0\u7684SOTA\u7ed3\u679c\uff0c\u6d88\u878d\u5b9e\u9a8c\u548c\u53ef\u89c6\u5316\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "STPFormer\u901a\u8fc7\u7edf\u4e00\u7684\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u65f6\u7a7a\u4ea4\u901a\u9884\u6d4b\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u4e3a\u590d\u6742\u65f6\u7a7a\u6a21\u5f0f\u5efa\u6a21\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.13877", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13877", "abs": "https://arxiv.org/abs/2508.13877", "authors": ["Rathnam Vidushika Rasanji", "Jin Wei-Kocsis", "Jiansong Zhang", "Dongming Gan", "Ragu Athinarayanan", "Paul Asunda"], "title": "Toward Deployable Multi-Robot Collaboration via a Symbolically-Guided Decision Transformer", "comment": null, "summary": "Reinforcement learning (RL) has demonstrated great potential in robotic\noperations. However, its data-intensive nature and reliance on the Markov\nDecision Process (MDP) assumption limit its practical deployment in real-world\nscenarios involving complex dynamics and long-term temporal dependencies, such\nas multi-robot manipulation. Decision Transformers (DTs) have emerged as a\npromising offline alternative by leveraging causal transformers for sequence\nmodeling in RL tasks. However, their applications to multi-robot manipulations\nstill remain underexplored. To address this gap, we propose a novel framework,\nSymbolically-Guided Decision Transformer (SGDT), which integrates a\nneuro-symbolic mechanism with a causal transformer to enable deployable\nmulti-robot collaboration. In the proposed SGDT framework, a neuro-symbolic\nplanner generates a high-level task-oriented plan composed of symbolic\nsubgoals. Guided by these subgoals, a goal-conditioned decision transformer\n(GCDT) performs low-level sequential decision-making for multi-robot\nmanipulation. This hierarchical architecture enables structured, interpretable,\nand generalizable decision making in complex multi-robot collaboration tasks.\nWe evaluate the performance of SGDT across a range of task scenarios, including\nzero-shot and few-shot scenarios. To our knowledge, this is the first work to\nexplore DT-based technology for multi-robot manipulation.", "AI": {"tldr": "\u63d0\u51faSGDT\u6846\u67b6\uff0c\u7ed3\u5408\u795e\u7ecf\u7b26\u53f7\u673a\u5236\u548c\u56e0\u679c\u53d8\u6362\u5668\uff0c\u7528\u4e8e\u591a\u673a\u5668\u4eba\u534f\u4f5c\u64cd\u4f5c\u4efb\u52a1", "motivation": "\u5f3a\u5316\u5b66\u4e60\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u6570\u636e\u5bc6\u96c6\u4e14\u4f9d\u8d56MDP\u5047\u8bbe\uff0c\u96be\u4ee5\u5904\u7406\u590d\u6742\u52a8\u6001\u548c\u957f\u671f\u4f9d\u8d56\u7684\u591a\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u3002\u51b3\u7b56\u53d8\u6362\u5668\u4f5c\u4e3a\u79bb\u7ebf\u66ff\u4ee3\u65b9\u6848\u5728\u591a\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u5e94\u7528\u4e0d\u8db3", "method": "\u795e\u7ecf\u7b26\u53f7\u89c4\u5212\u5668\u751f\u6210\u7b26\u53f7\u5b50\u76ee\u6807\u7684\u9ad8\u7ea7\u4efb\u52a1\u8ba1\u5212\uff0c\u76ee\u6807\u6761\u4ef6\u51b3\u7b56\u53d8\u6362\u5668\u5728\u5b50\u76ee\u6807\u6307\u5bfc\u4e0b\u8fdb\u884c\u4f4e\u7ea7\u5e8f\u5217\u51b3\u7b56", "result": "\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u573a\u666f\u4e2d\u8bc4\u4f30\u6027\u80fd\uff0c\u662f\u9996\u4e2a\u63a2\u7d22\u57fa\u4e8e\u51b3\u7b56\u53d8\u6362\u5668\u7684\u591a\u673a\u5668\u4eba\u64cd\u4f5c\u6280\u672f\u7684\u5de5\u4f5c", "conclusion": "SGDT\u7684\u5206\u5c42\u67b6\u6784\u5b9e\u73b0\u4e86\u7ed3\u6784\u5316\u3001\u53ef\u89e3\u91ca\u548c\u53ef\u6cdb\u5316\u7684\u590d\u6742\u591a\u673a\u5668\u4eba\u534f\u4f5c\u51b3\u7b56"}}
{"id": "2508.13437", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13437", "abs": "https://arxiv.org/abs/2508.13437", "authors": ["Cheikh Ahmed", "Mahdi Mostajabdaveh", "Samin Aref", "Zirui Zhou"], "title": "Discrete Optimization of Min-Max Violation and its Applications Across Computational Sciences", "comment": null, "summary": "We introduce the Discrete Min-Max Violation (DMMV) as a general optimization\nproblem which seeks an assignment of discrete values to variables that\nminimizes the largest constraint violation. This context-free mathematical\nformulation is applicable to a wide range of use cases that have worst-case\nperformance requirements. After defining the DMMV problem mathematically, we\nexplore its properties to establish a foundational understanding. To tackle\nDMMV instance sizes of practical relevance, we develop a GPU-accelerated\nheuristic that takes advantage of the mathematical properties of DMMV for\nspeeding up the solution process. We demonstrate the versatile applicability of\nour heuristic by solving three optimization problems as use cases: (1)\npost-training quantization of language models, (2) discrete tomography, and (3)\nFinite Impulse Response (FIR) filter design. In quantization without outlier\nseparation, our heuristic achieves 14% improvement on average over existing\nmethods. In discrete tomography, it reduces reconstruction error by 16% under\nuniform noise and accelerates computations by a factor of 6 on GPU. For FIR\nfilter design, it nearly achieves 50% ripple reduction compared to using the\ncommercial integer optimization solver, Gurobi. Our comparative results point\nto the benefits of studying DMMV as a context-free optimization problem and the\nadvantages that our proposed heuristic offers on three distinct problems. Our\nGPU-accelerated heuristic will be made open-source to further stimulate\nresearch on DMMV and its other applications. The code is available at\nhttps://anonymous.4open.science/r/AMVM-5F3E/", "AI": {"tldr": "\u63d0\u51fa\u4e86\u79bb\u6563\u6700\u5c0f\u6700\u5927\u8fdd\u89c4\uff08DMMV\uff09\u4f5c\u4e3a\u901a\u7528\u4f18\u5316\u95ee\u9898\uff0c\u5f00\u53d1\u4e86GPU\u52a0\u901f\u542f\u53d1\u5f0f\u7b97\u6cd5\uff0c\u5728\u8bed\u8a00\u6a21\u578b\u91cf\u5316\u3001\u79bb\u6563\u5c42\u6790\u6210\u50cf\u548cFIR\u6ee4\u6ce2\u5668\u8bbe\u8ba1\u4e09\u4e2a\u5e94\u7528\u573a\u666f\u4e2d\u53d6\u5f97\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u8bb8\u591a\u5e94\u7528\u573a\u666f\u90fd\u6709\u6700\u574f\u60c5\u51b5\u6027\u80fd\u8981\u6c42\uff0c\u9700\u8981\u4e00\u4e2a\u80fd\u591f\u6700\u5c0f\u5316\u6700\u5927\u7ea6\u675f\u8fdd\u89c4\u7684\u901a\u7528\u4f18\u5316\u6846\u67b6\u6765\u5904\u7406\u79bb\u6563\u503c\u5206\u914d\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86DMMV\u95ee\u9898\u7684\u6570\u5b66\u5b9a\u4e49\uff0c\u5f00\u53d1\u4e86\u57fa\u4e8eGPU\u52a0\u901f\u7684\u542f\u53d1\u5f0f\u7b97\u6cd5\uff0c\u5229\u7528DMMV\u7684\u6570\u5b66\u7279\u6027\u52a0\u901f\u6c42\u89e3\u8fc7\u7a0b\u3002", "result": "\u5728\u91cf\u5316\u4efb\u52a1\u4e2d\u5e73\u5747\u63d0\u534714%\uff0c\u79bb\u6563\u5c42\u6790\u6210\u50cf\u4e2d\u8bef\u5dee\u51cf\u5c1116%\u4e14GPU\u52a0\u901f6\u500d\uff0cFIR\u6ee4\u6ce2\u5668\u8bbe\u8ba1\u4e2d\u6ce2\u7eb9\u51cf\u5c11\u8fd150%\u3002", "conclusion": "DMMV\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u65e0\u5173\u4f18\u5316\u95ee\u9898\u5177\u6709\u5e7f\u6cdb\u9002\u7528\u6027\uff0c\u63d0\u51fa\u7684\u542f\u53d1\u5f0f\u7b97\u6cd5\u5728\u591a\u4e2a\u9886\u57df\u5c55\u73b0\u4f18\u52bf\uff0c\u4ee3\u7801\u5c06\u5f00\u6e90\u4ee5\u4fc3\u8fdb\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2508.13881", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.13881", "abs": "https://arxiv.org/abs/2508.13881", "authors": ["Zhaokun Chen", "Chaopeng Zhang", "Xiaohan Li", "Wenshuo Wang", "Gentiane Venture", "Junqiang Xi"], "title": "Driving Style Recognition Like an Expert Using Semantic Privileged Information from Large Language Models", "comment": null, "summary": "Existing driving style recognition systems largely depend on low-level\nsensor-derived features for training, neglecting the rich semantic reasoning\ncapability inherent to human experts. This discrepancy results in a fundamental\nmisalignment between algorithmic classifications and expert judgments. To\nbridge this gap, we propose a novel framework that integrates Semantic\nPrivileged Information (SPI) derived from large language models (LLMs) to align\nrecognition outcomes with human-interpretable reasoning. First, we introduce\nDriBehavGPT, an interactive LLM-based module that generates natural-language\ndescriptions of driving behaviors. These descriptions are then encoded into\nmachine learning-compatible representations via text embedding and\ndimensionality reduction. Finally, we incorporate them as privileged\ninformation into Support Vector Machine Plus (SVM+) for training, enabling the\nmodel to approximate human-like interpretation patterns. Experiments across\ndiverse real-world driving scenarios demonstrate that our SPI-enhanced\nframework outperforms conventional methods, achieving F1-score improvements of\n7.6% (car-following) and 7.9% (lane-changing). Importantly, SPI is exclusively\nused during training, while inference relies solely on sensor data, ensuring\ncomputational efficiency without sacrificing performance. These results\nhighlight the pivotal role of semantic behavioral representations in improving\nrecognition accuracy while advancing interpretable, human-centric driving\nsystems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u878d\u5408\u5927\u8bed\u8a00\u6a21\u578b\u8bed\u4e49\u7279\u6743\u4fe1\u606f(SPI)\u7684\u9a7e\u9a76\u98ce\u683c\u8bc6\u522b\u6846\u67b6\uff0c\u901a\u8fc7\u8bed\u4e49\u63a8\u7406\u63d0\u5347\u8bc6\u522b\u51c6\u786e\u6027\u5e76\u4e0e\u4e13\u5bb6\u5224\u65ad\u5bf9\u9f50", "motivation": "\u73b0\u6709\u9a7e\u9a76\u98ce\u683c\u8bc6\u522b\u7cfb\u7edf\u8fc7\u5ea6\u4f9d\u8d56\u5e95\u5c42\u4f20\u611f\u5668\u7279\u5f81\uff0c\u7f3a\u4e4f\u4eba\u7c7b\u4e13\u5bb6\u7684\u8bed\u4e49\u63a8\u7406\u80fd\u529b\uff0c\u5bfc\u81f4\u7b97\u6cd5\u5206\u7c7b\u4e0e\u4e13\u5bb6\u5224\u65ad\u5b58\u5728\u6839\u672c\u6027\u4e0d\u4e00\u81f4", "method": "1) \u5f00\u53d1DriBehavGPT\u4ea4\u4e92\u5f0fLLM\u6a21\u5757\u751f\u6210\u9a7e\u9a76\u884c\u4e3a\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\uff1b2) \u901a\u8fc7\u6587\u672c\u5d4c\u5165\u548c\u964d\u7ef4\u8f6c\u6362\u4e3a\u673a\u5668\u5b66\u4e60\u53ef\u5904\u7406\u8868\u793a\uff1b3) \u4f5c\u4e3a\u7279\u6743\u4fe1\u606f\u6574\u5408\u5230SVM+\u4e2d\u8fdb\u884c\u8bad\u7ec3", "result": "\u5728\u591a\u79cd\u771f\u5b9e\u9a7e\u9a76\u573a\u666f\u4e2d\uff0cSPI\u589e\u5f3a\u6846\u67b6\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0cF1\u5206\u6570\u63d0\u53477.6%(\u8ddf\u8f66)\u548c7.9%(\u53d8\u9053)\uff0c\u4e14\u63a8\u7406\u9636\u6bb5\u4ec5\u9700\u4f20\u611f\u5668\u6570\u636e", "conclusion": "\u8bed\u4e49\u884c\u4e3a\u8868\u5f81\u5728\u63d0\u5347\u8bc6\u522b\u51c6\u786e\u6027\u548c\u63a8\u8fdb\u53ef\u89e3\u91ca\u3001\u4ee5\u4eba\u4e3a\u672c\u7684\u9a7e\u9a76\u7cfb\u7edf\u65b9\u9762\u53d1\u6325\u5173\u952e\u4f5c\u7528"}}
{"id": "2508.13465", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13465", "abs": "https://arxiv.org/abs/2508.13465", "authors": ["Yuzhi Tang", "Tianxiao Li", "Elizabeth Li", "Chris J. Maddison", "Honghua Dong", "Yangjun Ruan"], "title": "LM Agents May Fail to Act on Their Own Risk Knowledge", "comment": null, "summary": "Language model (LM) agents have demonstrated significant potential for\nautomating real-world tasks, yet they pose a diverse array of potential, severe\nrisks in safety-critical scenarios. In this work, we identify a significant gap\nbetween LM agents' risk awareness and safety execution abilities: while they\noften answer \"Yes\" to queries like \"Is executing `sudo rm -rf /*' dangerous?\",\nthey will likely fail to identify such risks in instantiated trajectories or\neven directly perform these risky actions when acting as agents. To\nsystematically investigate this, we develop a comprehensive evaluation\nframework to examine agents' safety across three progressive dimensions: 1)\ntheir knowledge about potential risks, 2) their ability to identify\ncorresponding risks in execution trajectories, and 3) their actual behaviors to\navoid executing these risky actions. Our evaluation reveals two critical\nperformance gaps that resemble the generator-validator gaps observed in LMs:\nwhile agents demonstrate near-perfect risk knowledge ($>98\\%$ pass rates), they\nfail to apply this knowledge when identifying risks in actual scenarios (with\nperformance dropping by $>23\\%$) and often still execute risky actions ($<26\\%$\npass rates). Notably, this trend persists across more capable LMs as well as in\nspecialized reasoning models like DeepSeek-R1, indicating that simply scaling\nmodel capabilities or inference compute does not inherently resolve safety\nconcerns. Instead, we take advantage of these observed gaps to develop a risk\nverifier that independently critiques the proposed actions by agents, with an\nabstractor that converts specific execution trajectories into abstract\ndescriptions where LMs can more effectively identify the risks. Our overall\nsystem achieves a significant reduction of risky action execution by $55.3\\%$\nover vanilla-prompted agents.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u5b58\u5728\u98ce\u9669\u610f\u8bc6\u4e0e\u5b89\u5168\u6267\u884c\u80fd\u529b\u4e4b\u95f4\u7684\u663e\u8457\u5dee\u8ddd\uff0c\u5373\u4f7f\u77e5\u9053\u67d0\u4e9b\u64cd\u4f5c\u5371\u9669\uff0c\u5728\u5b9e\u9645\u6267\u884c\u65f6\u4ecd\u4f1a\u6267\u884c\u5371\u9669\u64cd\u4f5c\u3002\u4f5c\u8005\u5f00\u53d1\u4e86\u8bc4\u4f30\u6846\u67b6\u548c\u98ce\u9669\u9a8c\u8bc1\u5668\u7cfb\u7edf\uff0c\u5c06\u5371\u9669\u64cd\u4f5c\u6267\u884c\u7387\u964d\u4f4e\u4e8655.3%\u3002", "motivation": "\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\u5b58\u5728\u4e25\u91cd\u98ce\u9669\uff0c\u867d\u7136\u5177\u5907\u98ce\u9669\u77e5\u8bc6\u4f46\u5728\u5b9e\u9645\u6267\u884c\u65f6\u65e0\u6cd5\u6709\u6548\u5e94\u7528\u8fd9\u4e9b\u77e5\u8bc6\uff0c\u5b58\u5728\u660e\u663e\u7684\u77e5\u884c\u5dee\u8ddd\u3002", "method": "\u5f00\u53d1\u4e86\u5305\u542b\u4e09\u4e2a\u7ef4\u5ea6\u7684\u8bc4\u4f30\u6846\u67b6\uff1a\u98ce\u9669\u77e5\u8bc6\u3001\u98ce\u9669\u8bc6\u522b\u80fd\u529b\u3001\u98ce\u9669\u89c4\u907f\u884c\u4e3a\u3002\u57fa\u4e8e\u89c2\u5bdf\u5230\u7684\u5dee\u8ddd\uff0c\u8bbe\u8ba1\u4e86\u98ce\u9669\u9a8c\u8bc1\u5668\u7cfb\u7edf\uff0c\u5305\u542b\u62bd\u8c61\u5668\u5c06\u5177\u4f53\u6267\u884c\u8f68\u8ff9\u8f6c\u6362\u4e3a\u62bd\u8c61\u63cf\u8ff0\u4ee5\u4fbf\u6a21\u578b\u66f4\u597d\u8bc6\u522b\u98ce\u9669\u3002", "result": "\u8bc4\u4f30\u663e\u793a\u4ee3\u7406\u98ce\u9669\u77e5\u8bc6\u901a\u8fc7\u7387>98%\uff0c\u4f46\u5b9e\u9645\u98ce\u9669\u8bc6\u522b\u6027\u80fd\u4e0b\u964d>23%\uff0c\u5371\u9669\u64cd\u4f5c\u6267\u884c\u901a\u8fc7\u7387<26%\u3002\u98ce\u9669\u9a8c\u8bc1\u5668\u7cfb\u7edf\u5c06\u5371\u9669\u64cd\u4f5c\u6267\u884c\u7387\u964d\u4f4e\u4e8655.3%\u3002", "conclusion": "\u5355\u7eaf\u63d0\u5347\u6a21\u578b\u80fd\u529b\u6216\u63a8\u7406\u8ba1\u7b97\u65e0\u6cd5\u89e3\u51b3\u5b89\u5168\u95ee\u9898\uff0c\u9700\u8981\u4e13\u95e8\u7684\u5b89\u5168\u673a\u5236\u3002\u98ce\u9669\u9a8c\u8bc1\u5668\u7cfb\u7edf\u80fd\u6709\u6548\u51cf\u5c11\u5371\u9669\u64cd\u4f5c\u6267\u884c\uff0c\u4e3a\u89e3\u51b3\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u7684\u5b89\u5168\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2508.13901", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.13901", "abs": "https://arxiv.org/abs/2508.13901", "authors": ["Yihao Lu", "Hao Tang"], "title": "Multimodal Data Storage and Retrieval for Embodied AI: A Survey", "comment": null, "summary": "Embodied AI (EAI) agents continuously interact with the physical world,\ngenerating vast, heterogeneous multimodal data streams that traditional\nmanagement systems are ill-equipped to handle. In this survey, we first\nsystematically evaluate five storage architectures (Graph Databases,\nMulti-Model Databases, Data Lakes, Vector Databases, and Time-Series\nDatabases), focusing on their suitability for addressing EAI's core\nrequirements, including physical grounding, low-latency access, and dynamic\nscalability. We then analyze five retrieval paradigms (Fusion Strategy-Based\nRetrieval, Representation Alignment-Based Retrieval, Graph-Structure-Based\nRetrieval, Generation Model-Based Retrieval, and Efficient Retrieval-Based\nOptimization), revealing a fundamental tension between achieving long-term\nsemantic coherence and maintaining real-time responsiveness. Based on this\ncomprehensive analysis, we identify key bottlenecks, spanning from the\nfoundational Physical Grounding Gap to systemic challenges in cross-modal\nintegration, dynamic adaptation, and open-world generalization. Finally, we\noutline a forward-looking research agenda encompassing physics-aware data\nmodels, adaptive storage-retrieval co-optimization, and standardized\nbenchmarking, to guide future research toward principled data management\nsolutions for EAI. Our survey is based on a comprehensive review of more than\n180 related studies, providing a rigorous roadmap for designing the robust,\nhigh-performance data management frameworks essential for the next generation\nof autonomous embodied systems.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u8bc4\u4f30\u4e865\u79cd\u5b58\u50a8\u67b6\u6784\u548c5\u79cd\u68c0\u7d22\u8303\u5f0f\u5728\u5177\u8eabAI\u6570\u636e\u7ba1\u7406\u4e2d\u7684\u9002\u7528\u6027\uff0c\u63ed\u793a\u4e86\u8bed\u4e49\u4e00\u81f4\u6027\u4e0e\u5b9e\u65f6\u54cd\u5e94\u4e4b\u95f4\u7684\u6839\u672c\u77db\u76fe\uff0c\u5e76\u63d0\u51fa\u4e86\u5305\u542b\u7269\u7406\u611f\u77e5\u6570\u636e\u6a21\u578b\u3001\u81ea\u9002\u5e94\u4f18\u5316\u7b49\u7684\u524d\u77bb\u7814\u7a76\u8bae\u7a0b\u3002", "motivation": "\u5177\u8eabAI\u4ee3\u7406\u6301\u7eed\u4e0e\u7269\u7406\u4e16\u754c\u4ea4\u4e92\u4ea7\u751f\u6d77\u91cf\u5f02\u6784\u591a\u6a21\u6001\u6570\u636e\u6d41\uff0c\u4f20\u7edf\u7ba1\u7406\u7cfb\u7edf\u96be\u4ee5\u6709\u6548\u5904\u7406\u8fd9\u4e9b\u6570\u636e\uff0c\u9700\u8981\u4e13\u95e8\u7684\u6570\u636e\u7ba1\u7406\u89e3\u51b3\u65b9\u6848\u6765\u6ee1\u8db3\u7269\u7406\u63a5\u5730\u3001\u4f4e\u5ef6\u8fdf\u8bbf\u95ee\u548c\u52a8\u6001\u53ef\u6269\u5c55\u6027\u7b49\u6838\u5fc3\u9700\u6c42\u3002", "method": "\u57fa\u4e8e\u5bf9180\u591a\u9879\u76f8\u5173\u7814\u7a76\u7684\u5168\u9762\u56de\u987e\uff0c\u7cfb\u7edf\u8bc4\u4f30\u4e86\u56fe\u6570\u636e\u5e93\u3001\u591a\u6a21\u578b\u6570\u636e\u5e93\u3001\u6570\u636e\u6e56\u3001\u5411\u91cf\u6570\u636e\u5e93\u548c\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u5e93\u4e94\u79cd\u5b58\u50a8\u67b6\u6784\uff0c\u4ee5\u53ca\u57fa\u4e8e\u878d\u5408\u7b56\u7565\u3001\u8868\u793a\u5bf9\u9f50\u3001\u56fe\u7ed3\u6784\u3001\u751f\u6210\u6a21\u578b\u548c\u9ad8\u6548\u68c0\u7d22\u4f18\u5316\u7684\u4e94\u79cd\u68c0\u7d22\u8303\u5f0f\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5b58\u50a8\u68c0\u7d22\u7cfb\u7edf\u5728\u5b9e\u73b0\u957f\u671f\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u4fdd\u6301\u5b9e\u65f6\u54cd\u5e94\u6027\u4e4b\u95f4\u5b58\u5728\u6839\u672c\u6027\u77db\u76fe\uff0c\u8bc6\u522b\u51fa\u4ece\u7269\u7406\u63a5\u5730\u5dee\u8ddd\u5230\u8de8\u6a21\u6001\u96c6\u6210\u3001\u52a8\u6001\u9002\u5e94\u548c\u5f00\u653e\u4e16\u754c\u6cdb\u5316\u7b49\u7cfb\u7edf\u6027\u6311\u6218\u7684\u5173\u952e\u74f6\u9888\u3002", "conclusion": "\u63d0\u51fa\u4e86\u5305\u542b\u7269\u7406\u611f\u77e5\u6570\u636e\u6a21\u578b\u3001\u81ea\u9002\u5e94\u5b58\u50a8\u68c0\u7d22\u534f\u540c\u4f18\u5316\u548c\u6807\u51c6\u5316\u57fa\u51c6\u6d4b\u8bd5\u7684\u524d\u77bb\u6027\u7814\u7a76\u8bae\u7a0b\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u81ea\u4e3b\u5177\u8eab\u7cfb\u7edf\u8bbe\u8ba1\u7a33\u5065\u9ad8\u6027\u80fd\u6570\u636e\u7ba1\u7406\u6846\u67b6\u63d0\u4f9b\u4e86\u4e25\u8c28\u8def\u7ebf\u56fe\u3002"}}
{"id": "2508.13530", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13530", "abs": "https://arxiv.org/abs/2508.13530", "authors": ["Junyeong Park", "Hyeonseo Cho", "Sungjin Ahn"], "title": "CrafterDojo: A Suite of Foundation Models for Building Open-Ended Embodied Agents in Crafter", "comment": null, "summary": "Developing general-purpose embodied agents is a core challenge in AI.\nMinecraft provides rich complexity and internet-scale data, but its slow speed\nand engineering overhead make it unsuitable for rapid prototyping. Crafter\noffers a lightweight alternative that retains key challenges from Minecraft,\nyet its use has remained limited to narrow tasks due to the absence of\nfoundation models that have driven progress in the Minecraft setting. In this\npaper, we present CrafterDojo, a suite of foundation models and tools that\nunlock the Crafter environment as a lightweight, prototyping-friendly, and\nMinecraft-like testbed for general-purpose embodied agent research. CrafterDojo\naddresses this by introducing CrafterVPT, CrafterCLIP, and CrafterSteve-1 for\nbehavior priors, vision-language grounding, and instruction following,\nrespectively. In addition, we provide toolkits for generating behavior and\ncaption datasets (CrafterPlay and CrafterCaption), reference agent\nimplementations, benchmark evaluations, and a complete open-source codebase.", "AI": {"tldr": "CrafterDojo\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684Minecraft-like\u6d4b\u8bd5\u5e73\u53f0\uff0c\u63d0\u4f9b\u57fa\u7840\u6a21\u578b\u548c\u5de5\u5177\u5957\u4ef6\uff0c\u7528\u4e8e\u901a\u7528\u5177\u8eab\u667a\u80fd\u4f53\u7814\u7a76\u7684\u5feb\u901f\u539f\u578b\u5f00\u53d1\u3002", "motivation": "Minecraft\u73af\u5883\u590d\u6742\u4f46\u8fd0\u884c\u7f13\u6162\uff0c\u4e0d\u9002\u5408\u5feb\u901f\u539f\u578b\u5f00\u53d1\uff1bCrafter\u73af\u5883\u8f7b\u91cf\u4f46\u7f3a\u4e4f\u57fa\u7840\u6a21\u578b\u652f\u6301\uff0c\u9650\u5236\u4e86\u5176\u5728\u901a\u7528\u5177\u8eab\u667a\u80fd\u4f53\u7814\u7a76\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u5f00\u53d1\u4e86CrafterVPT\uff08\u884c\u4e3a\u5148\u9a8c\uff09\u3001CrafterCLIP\uff08\u89c6\u89c9\u8bed\u8a00\u57fa\u7840\uff09\u548cCrafterSteve-1\uff08\u6307\u4ee4\u8ddf\u968f\uff09\u4e09\u4e2a\u57fa\u7840\u6a21\u578b\uff0c\u4ee5\u53caCrafterPlay\uff08\u884c\u4e3a\u6570\u636e\u96c6\u751f\u6210\uff09\u3001CrafterCaption\uff08\u5b57\u5e55\u6570\u636e\u96c6\u751f\u6210\uff09\u7b49\u5de5\u5177\u5957\u4ef6\u3002", "result": "\u6784\u5efa\u4e86\u5b8c\u6574\u7684\u5f00\u6e90\u4ee3\u7801\u5e93\uff0c\u5305\u542b\u53c2\u8003\u667a\u80fd\u4f53\u5b9e\u73b0\u548c\u57fa\u51c6\u8bc4\u4f30\uff0c\u89e3\u9501\u4e86Crafter\u73af\u5883\u4f5c\u4e3a\u8f7b\u91cf\u7ea7\u6d4b\u8bd5\u5e73\u53f0\u7684\u80fd\u529b\u3002", "conclusion": "CrafterDojo\u6210\u529f\u5730\u5c06Crafter\u73af\u5883\u8f6c\u5316\u4e3a\u4e00\u4e2a\u9002\u5408\u5feb\u901f\u539f\u578b\u5f00\u53d1\u7684Minecraft-like\u6d4b\u8bd5\u5e73\u53f0\uff0c\u4e3a\u901a\u7528\u5177\u8eab\u667a\u80fd\u4f53\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u5de5\u5177\u548c\u57fa\u7840\u6a21\u578b\u652f\u6301\u3002"}}
{"id": "2508.13964", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.13964", "abs": "https://arxiv.org/abs/2508.13964", "authors": ["Martijn Cramer", "Yanming Wu", "David De Schepper", "Eric Demeester"], "title": "Augmenting cobots for sheet-metal SMEs with 3D object recognition and localisation", "comment": "13 pages, 25 figures", "summary": "Due to high-mix-low-volume production, sheet-metal workshops today are\nchallenged by small series and varying orders. As standard automation solutions\ntend to fall short, SMEs resort to repetitive manual labour impacting\nproduction costs and leading to tech-skilled workforces not being used to their\nfull potential. The COOCK+ ROBUST project aims to transform cobots into mobile\nand reconfigurable production assistants by integrating existing technologies,\nincluding 3D object recognition and localisation. This article explores both\nthe opportunities and challenges of enhancing cobotic systems with these\ntechnologies in an industrial setting, outlining the key steps involved in the\nprocess. Additionally, insights from a past project, carried out by the ACRO\nresearch unit in collaboration with an industrial partner, serves as a concrete\nimplementation example throughout.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u4e2d\u5c0f\u4f01\u4e1a\u94a3\u91d1\u8f66\u95f4\u4e2d\u4f7f\u7528\u534f\u4f5c\u673a\u5668\u4eba\u4f5c\u4e3a\u79fb\u52a8\u53ef\u91cd\u6784\u751f\u4ea7\u52a9\u624b\u7684\u673a\u9047\u4e0e\u6311\u6218\uff0c\u901a\u8fc7\u96c6\u62103D\u7269\u4f53\u8bc6\u522b\u548c\u5b9a\u4f4d\u6280\u672f\u6765\u89e3\u51b3\u5c0f\u6279\u91cf\u591a\u54c1\u79cd\u751f\u4ea7\u4e2d\u7684\u81ea\u52a8\u5316\u96be\u9898\u3002", "motivation": "\u4e2d\u5c0f\u4f01\u4e1a\u9762\u4e34\u5c0f\u6279\u91cf\u591a\u54c1\u79cd\u751f\u4ea7\u7684\u6311\u6218\uff0c\u6807\u51c6\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u96be\u4ee5\u5e94\u5bf9\uff0c\u5bfc\u81f4\u4f9d\u8d56\u91cd\u590d\u6027\u624b\u5de5\u52b3\u52a8\uff0c\u589e\u52a0\u4e86\u751f\u4ea7\u6210\u672c\u4e14\u672a\u80fd\u5145\u5206\u5229\u7528\u6280\u672f\u719f\u7ec3\u52b3\u52a8\u529b\u7684\u6f5c\u529b\u3002", "method": "\u901a\u8fc7COOCK+ ROBUST\u9879\u76ee\uff0c\u96c6\u6210\u73b0\u6709\u6280\u672f\uff08\u5305\u62ec3D\u7269\u4f53\u8bc6\u522b\u548c\u5b9a\u4f4d\uff09\uff0c\u5c06\u534f\u4f5c\u673a\u5668\u4eba\u8f6c\u53d8\u4e3a\u79fb\u52a8\u53ef\u91cd\u6784\u7684\u751f\u4ea7\u52a9\u624b\uff0c\u5e76\u4ee5ACRO\u7814\u7a76\u5355\u4f4d\u4e0e\u5de5\u4e1a\u5408\u4f5c\u4f19\u4f34\u7684\u5b9e\u9645\u9879\u76ee\u4f5c\u4e3a\u5b9e\u65bd\u6848\u4f8b\u3002", "result": "\u7814\u7a76\u8bc6\u522b\u4e86\u5728\u5de5\u4e1a\u73af\u5883\u4e2d\u589e\u5f3a\u534f\u4f5c\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u673a\u9047\u548c\u6311\u6218\uff0c\u5e76\u6982\u8ff0\u4e86\u5b9e\u65bd\u8fc7\u7a0b\u4e2d\u7684\u5173\u952e\u6b65\u9aa4\uff0c\u63d0\u4f9b\u4e86\u5177\u4f53\u7684\u5b9e\u65bd\u8303\u4f8b\u3002", "conclusion": "\u5c06\u534f\u4f5c\u673a\u5668\u4eba\u914d\u59073D\u89c6\u89c9\u6280\u672f\u53ef\u4ee5\u6210\u4e3a\u4e2d\u5c0f\u4f01\u4e1a\u5e94\u5bf9\u9ad8\u6df7\u5408\u4f4e\u4ea7\u91cf\u751f\u4ea7\u6311\u6218\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u9700\u8981\u514b\u670d\u6280\u672f\u96c6\u6210\u548c\u5b9e\u65bd\u65b9\u9762\u7684\u6311\u6218\u3002"}}
{"id": "2508.13579", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13579", "abs": "https://arxiv.org/abs/2508.13579", "authors": ["Yue Fang", "Yuxin Guo", "Jiaran Gao", "Hongxin Ding", "Xinke Jiang", "Weibin Liao", "Yongxin Xu", "Yinghao Zhu", "Zhibang Yang", "Liantao Ma", "Junfeng Zhao", "Yasha Wang"], "title": "Toward Better EHR Reasoning in LLMs: Reinforcement Learning with Expert Attention Guidance", "comment": null, "summary": "Improving large language models (LLMs) for electronic health record (EHR)\nreasoning is essential for enabling accurate and generalizable clinical\npredictions. While LLMs excel at medical text understanding, they underperform\non EHR-based prediction tasks due to challenges in modeling temporally\nstructured, high-dimensional data. Existing approaches often rely on hybrid\nparadigms, where LLMs serve merely as frozen prior retrievers while downstream\ndeep learning (DL) models handle prediction, failing to improve the LLM's\nintrinsic reasoning capacity and inheriting the generalization limitations of\nDL models. To this end, we propose EAG-RL, a novel two-stage training framework\ndesigned to intrinsically enhance LLMs' EHR reasoning ability through expert\nattention guidance, where expert EHR models refer to task-specific DL models\ntrained on EHR data. Concretely, EAG-RL first constructs high-quality, stepwise\nreasoning trajectories using expert-guided Monte Carlo Tree Search to\neffectively initialize the LLM's policy. Then, EAG-RL further optimizes the\npolicy via reinforcement learning by aligning the LLM's attention with\nclinically salient features identified by expert EHR models. Extensive\nexperiments on two real-world EHR datasets show that EAG-RL improves the\nintrinsic EHR reasoning ability of LLMs by an average of 14.62%, while also\nenhancing robustness to feature perturbations and generalization to unseen\nclinical domains. These results demonstrate the practical potential of EAG-RL\nfor real-world deployment in clinical prediction tasks. Our code have been\navailable at https://github.com/devilran6/EAG-RL.", "AI": {"tldr": "EAG-RL\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u4e13\u5bb6\u6ce8\u610f\u529b\u6307\u5bfc\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u63a8\u7406\u4e2d\u7684\u5185\u5728\u80fd\u529b\uff0c\u5e73\u5747\u63d0\u534714.62%\u7684\u6027\u80fd", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4ec5\u5c06LLMs\u7528\u4f5c\u51bb\u7ed3\u7684\u68c0\u7d22\u5668\uff0c\u800c\u4e0b\u6e38\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5904\u7406\u9884\u6d4b\uff0c\u672a\u80fd\u63d0\u5347LLMs\u7684\u5185\u5728\u63a8\u7406\u80fd\u529b\uff0c\u4e14\u7ee7\u627f\u4e86DL\u6a21\u578b\u7684\u6cdb\u5316\u9650\u5236", "method": "\u9996\u5148\u4f7f\u7528\u4e13\u5bb6\u5f15\u5bfc\u7684\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u6784\u5efa\u9ad8\u8d28\u91cf\u9010\u6b65\u63a8\u7406\u8f68\u8ff9\u6765\u521d\u59cb\u5316LLM\u7b56\u7565\uff0c\u7136\u540e\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5c06LLM\u6ce8\u610f\u529b\u4e0e\u4e13\u5bb6EHR\u6a21\u578b\u8bc6\u522b\u7684\u4e34\u5e8a\u663e\u8457\u7279\u5f81\u5bf9\u9f50", "result": "\u5728\u4e24\u4e2a\u771f\u5b9e\u4e16\u754cEHR\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0cEAG-RL\u5e73\u5747\u63d0\u5347LLMs\u5185\u5728EHR\u63a8\u7406\u80fd\u529b14.62%\uff0c\u540c\u65f6\u589e\u5f3a\u5bf9\u7279\u5f81\u6270\u52a8\u7684\u9c81\u68d2\u6027\u548c\u5bf9\u672a\u89c1\u4e34\u5e8a\u9886\u57df\u7684\u6cdb\u5316\u80fd\u529b", "conclusion": "EAG-RL\u5c55\u793a\u4e86\u5728\u4e34\u5e8a\u9884\u6d4b\u4efb\u52a1\u4e2d\u5b9e\u9645\u90e8\u7f72\u7684\u6f5c\u529b\uff0c\u901a\u8fc7\u4e13\u5bb6\u6ce8\u610f\u529b\u6307\u5bfc\u6709\u6548\u63d0\u5347\u4e86LLMs\u5728EHR\u63a8\u7406\u4e2d\u7684\u6027\u80fd"}}
{"id": "2508.13976", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.13976", "abs": "https://arxiv.org/abs/2508.13976", "authors": ["Carlo Mazzola", "Hassan Ali", "Krist\u00edna Malinovsk\u00e1", "Igor Farka\u0161"], "title": "Toward an Interaction-Centered Approach to Robot Trustworthiness", "comment": "4 pages, presented at TRUST workshop, organised in conjunction with\n  the IEEE RO-MAN 2025 conference, held in Eindhoven, Netherlands", "summary": "As robots get more integrated into human environments, fostering\ntrustworthiness in embodied robotic agents becomes paramount for an effective\nand safe human-robot interaction (HRI). To achieve that, HRI applications must\npromote human trust that aligns with robot skills and avoid misplaced trust or\novertrust, which can pose safety risks and ethical concerns. To achieve that,\nHRI applications must promote human trust that aligns with robot skills and\navoid misplaced trust or overtrust, which can pose safety risks and ethical\nconcerns. In this position paper, we outline an interaction-based framework for\nbuilding trust through mutual understanding between humans and robots. We\nemphasize two main pillars: human awareness and transparency, referring to the\nrobot ability to interpret human actions accurately and to clearly communicate\nits intentions and goals, respectively. By integrating these two pillars,\nrobots can behave in a manner that aligns with human expectations and needs\nwhile providing their human partners with both comprehension and control over\ntheir actions. We also introduce four components that we think are important\nfor bridging the gap between a human-perceived sense of trust and a robot true\ncapabilities.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u4ea4\u4e92\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4eba\u673a\u76f8\u4e92\u7406\u89e3\u6765\u5efa\u7acb\u4fe1\u4efb\uff0c\u5f3a\u8c03\u4eba\u7c7b\u610f\u8bc6\u548c\u900f\u660e\u5ea6\u4e24\u5927\u652f\u67f1\uff0c\u65e8\u5728\u4f7f\u673a\u5668\u4eba\u884c\u4e3a\u7b26\u5408\u4eba\u7c7b\u671f\u671b\u5e76\u907f\u514d\u8bef\u4fe1\u98ce\u9669\u3002", "motivation": "\u968f\u7740\u673a\u5668\u4eba\u66f4\u6df1\u5165\u878d\u5165\u4eba\u7c7b\u73af\u5883\uff0c\u9700\u8981\u5efa\u7acb\u4e0e\u673a\u5668\u4eba\u6280\u80fd\u76f8\u5339\u914d\u7684\u4eba\u7c7b\u4fe1\u4efb\uff0c\u907f\u514d\u8bef\u4fe1\u6216\u8fc7\u5ea6\u4fe1\u4efb\u5e26\u6765\u7684\u5b89\u5168\u98ce\u9669\u548c\u4f26\u7406\u95ee\u9898\uff0c\u5b9e\u73b0\u6709\u6548\u5b89\u5168\u7684\u4eba\u673a\u4ea4\u4e92\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u4ea4\u4e92\u7684\u4fe1\u4efb\u6784\u5efa\u6846\u67b6\uff0c\u5305\u542b\u4e24\u5927\u6838\u5fc3\u652f\u67f1\uff1a\u4eba\u7c7b\u610f\u8bc6\uff08\u673a\u5668\u4eba\u51c6\u786e\u89e3\u8bfb\u4eba\u7c7b\u884c\u4e3a\uff09\u548c\u900f\u660e\u5ea6\uff08\u6e05\u6670\u4f20\u8fbe\u673a\u5668\u4eba\u610f\u56fe\u548c\u76ee\u6807\uff09\uff0c\u5e76\u5f15\u5165\u56db\u4e2a\u5173\u952e\u7ec4\u4ef6\u6765\u5f25\u5408\u4eba\u7c7b\u611f\u77e5\u4fe1\u4efb\u4e0e\u673a\u5668\u4eba\u5b9e\u9645\u80fd\u529b\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "result": "\u6846\u67b6\u901a\u8fc7\u6574\u5408\u4eba\u7c7b\u610f\u8bc6\u548c\u900f\u660e\u5ea6\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u4ee5\u7b26\u5408\u4eba\u7c7b\u671f\u671b\u548c\u9700\u6c42\u7684\u65b9\u5f0f\u884c\u4e3a\uff0c\u540c\u65f6\u4e3a\u4eba\u7c7b\u4f19\u4f34\u63d0\u4f9b\u5bf9\u5176\u884c\u4e3a\u7684\u7406\u89e3\u548c\u63a7\u5236\u80fd\u529b\u3002", "conclusion": "\u8be5\u4ea4\u4e92\u6846\u67b6\u4e3a\u4eba\u673a\u4fe1\u4efb\u5efa\u7acb\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u53cc\u5411\u7406\u89e3\u673a\u5236\u4fc3\u8fdb\u5b89\u5168\u6709\u6548\u7684\u4eba\u673a\u534f\u4f5c\uff0c\u5bf9\u89e3\u51b3\u4eba\u673a\u4ea4\u4e92\u4e2d\u7684\u4fe1\u4efb\u6821\u51c6\u95ee\u9898\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2508.13587", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.13587", "abs": "https://arxiv.org/abs/2508.13587", "authors": ["Lei Chen", "Xuanle Zhao", "Zhixiong Zeng", "Jing Huang", "Liming Zheng", "Yufeng Zhong", "Lin Ma"], "title": "Breaking the SFT Plateau: Multimodal Structured Reinforcement Learning for Chart-to-Code Generation", "comment": "technical report", "summary": "While reinforcement learning (RL) has proven highly effective for general\nreasoning in vision-language models, its application to tasks requiring\nin-depth understanding of information-rich images and generation of structured\noutputs remains underexplored. Chart-to-code generation exemplifies this\nchallenge, demanding complex reasoning over visual charts to generate\nstructured code. Supervised fine-tuning (SFT) alone is often insufficient,\nhighlighting the need for effective RL strategies that appropriately reward\nstructured outputs. We systematically investigate the performance plateau in\nSFT through large-scale experiments and propose Multimodal Structured\nReinforcement Learning (MSRL) for chart-to-code generation, which substantially\nbreaks through this plateau. We construct the largest training corpus to date,\ncontaining 3 million chart-code pairs from real-world arXiv tables to mitigate\nsimplistic patterns of prior synthetic data. Despite reaching state-of-the-art\nperformance, our experiments show that scaling SFT data eventually hits a\nplateau where further increases yield negligible improvements. Our MSRL method\nleverages a multi-granularity structured reward system using multimodal textual\nand visual feedback. At the textual level, rule-based rewards validate\nfine-grained code details. At the visual level, model-based rewards assess\nstructural similarity by rendering generated code into images and employing an\nevaluator model. We implement this within a two-stage curriculum for training\nstability. Results demonstrate that MSRL significantly breaks the SFT plateau,\nimproving high-level metrics by 6.2% and 9.9% on ChartMimic and ReachQA\nbenchmarks respectively, achieving competitive performance with advanced\nclosed-source models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u591a\u6a21\u6001\u7ed3\u6784\u5316\u5f3a\u5316\u5b66\u4e60(MSRL)\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u6587\u672c\u548c\u89c6\u89c9\u53cd\u9988\u7684\u591a\u7c92\u5ea6\u5956\u52b1\u7cfb\u7edf\uff0c\u7a81\u7834\u4e86\u56fe\u8868\u8f6c\u4ee3\u7801\u4efb\u52a1\u4e2d\u76d1\u7763\u5fae\u8c03\u7684\u6027\u80fd\u74f6\u9888\u3002", "motivation": "\u867d\u7136\u5f3a\u5316\u5b66\u4e60\u5728\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u6709\u6548\uff0c\u4f46\u5728\u9700\u8981\u6df1\u5ea6\u7406\u89e3\u4fe1\u606f\u4e30\u5bcc\u56fe\u50cf\u548c\u751f\u6210\u7ed3\u6784\u5316\u8f93\u51fa\u7684\u4efb\u52a1\u4e2d\u5e94\u7528\u4e0d\u8db3\u3002\u56fe\u8868\u8f6c\u4ee3\u7801\u751f\u6210\u9700\u8981\u590d\u6742\u7684\u89c6\u89c9\u63a8\u7406\uff0c\u4ec5\u9760\u76d1\u7763\u5fae\u8c03\u5f80\u5f80\u4e0d\u591f\uff0c\u9700\u8981\u6709\u6548\u7684\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u6765\u9002\u5f53\u5956\u52b1\u7ed3\u6784\u5316\u8f93\u51fa\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b300\u4e07\u771f\u5b9earXiv\u8868\u683c\u56fe\u8868-\u4ee3\u7801\u5bf9\u7684\u6700\u5927\u8bad\u7ec3\u8bed\u6599\u5e93\uff1b\u63d0\u51faMSRL\u65b9\u6cd5\uff0c\u4f7f\u7528\u591a\u7c92\u5ea6\u7ed3\u6784\u5316\u5956\u52b1\u7cfb\u7edf\uff1a\u6587\u672c\u5c42\u9762\u57fa\u4e8e\u89c4\u5219\u7684\u5956\u52b1\u9a8c\u8bc1\u7ec6\u7c92\u5ea6\u4ee3\u7801\u7ec6\u8282\uff0c\u89c6\u89c9\u5c42\u9762\u901a\u8fc7\u6e32\u67d3\u4ee3\u7801\u6210\u56fe\u50cf\u5e76\u4f7f\u7528\u8bc4\u4f30\u6a21\u578b\u8bc4\u4f30\u7ed3\u6784\u76f8\u4f3c\u6027\uff1b\u91c7\u7528\u4e24\u9636\u6bb5\u8bfe\u7a0b\u5b66\u4e60\u786e\u4fdd\u8bad\u7ec3\u7a33\u5b9a\u6027\u3002", "result": "MSRL\u663e\u8457\u7a81\u7834\u4e86SFT\u7684\u6027\u80fd\u74f6\u9888\uff0c\u5728ChartMimic\u548cReachQA\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u5206\u522b\u5c06\u9ad8\u7ea7\u6307\u6807\u63d0\u5347\u4e866.2%\u548c9.9%\uff0c\u8fbe\u5230\u4e86\u4e0e\u5148\u8fdb\u95ed\u6e90\u6a21\u578b\u7ade\u4e89\u7684\u6027\u80fd\u6c34\u5e73\u3002", "conclusion": "\u591a\u6a21\u6001\u7ed3\u6784\u5316\u5f3a\u5316\u5b66\u4e60\u662f\u89e3\u51b3\u56fe\u8868\u8f6c\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u76d1\u7763\u5fae\u8c03\u6027\u80fd\u74f6\u9888\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u6587\u672c\u548c\u89c6\u89c9\u53cd\u9988\u7684\u591a\u7c92\u5ea6\u5956\u52b1\u673a\u5236\u80fd\u591f\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2508.13982", "categories": ["cs.RO", "cs.AI", "cs.HC", "cs.MA", "I.2.9; I.2"], "pdf": "https://arxiv.org/pdf/2508.13982", "abs": "https://arxiv.org/abs/2508.13982", "authors": ["Sydney Thompson", "Kate Candon", "Marynel V\u00e1zquez"], "title": "The Social Context of Human-Robot Interactions", "comment": "To be published in Annual Review of Control, Robotics, and Autonomous\n  Systems", "summary": "The Human-Robot Interaction (HRI) community often highlights the social\ncontext of an interaction as a key consideration when designing, implementing,\nand evaluating robot behavior. Unfortunately, researchers use the term \"social\ncontext\" in varied ways. This can lead to miscommunication, making it\nchallenging to draw connections between related work on understanding and\nmodeling the social contexts of human-robot interactions. To address this gap,\nwe survey the HRI literature for existing definitions and uses of the term\n\"social context\". Then, we propose a conceptual model for describing the social\ncontext of a human-robot interaction. We apply this model to existing work, and\nwe discuss a range of attributes of social contexts that can help researchers\nplan for interactions, develop behavior models for robots, and gain insights\nafter interactions have taken place. We conclude with a discussion of open\nresearch questions in relation to understanding and modeling the social\ncontexts of human-robot interactions.", "AI": {"tldr": "\u672c\u6587\u5bf9HRI\u9886\u57df\u4e2d\"\u793e\u4ea4\u60c5\u5883\"\u672f\u8bed\u7684\u6df7\u4e71\u4f7f\u7528\u8fdb\u884c\u4e86\u7cfb\u7edf\u68b3\u7406\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u6982\u5ff5\u6a21\u578b\u6765\u7edf\u4e00\u63cf\u8ff0\u4eba\u673a\u4ea4\u4e92\u7684\u793e\u4ea4\u60c5\u5883\uff0c\u5e76\u8ba8\u8bba\u4e86\u8be5\u6a21\u578b\u5728\u4ea4\u4e92\u89c4\u5212\u3001\u673a\u5668\u4eba\u884c\u4e3a\u5efa\u6a21\u548c\u4e8b\u540e\u5206\u6790\u4e2d\u7684\u5e94\u7528\u4ef7\u503c\u3002", "motivation": "HRI\u7814\u7a76\u793e\u533a\u4e2d\"\u793e\u4ea4\u60c5\u5883\"\u672f\u8bed\u4f7f\u7528\u6df7\u4e71\uff0c\u5bfc\u81f4\u7814\u7a76\u4e4b\u95f4\u96be\u4ee5\u5efa\u7acb\u8054\u7cfb\u548c\u6bd4\u8f83\uff0c\u9700\u8981\u7edf\u4e00\u7684\u6982\u5ff5\u6846\u67b6\u6765\u4fc3\u8fdb\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002", "method": "\u901a\u8fc7\u6587\u732e\u7efc\u8ff0\u68b3\u7406\u73b0\u6709\"\u793e\u4ea4\u60c5\u5883\"\u5b9a\u4e49\u548c\u4f7f\u7528\u65b9\u5f0f\uff0c\u63d0\u51fa\u6982\u5ff5\u6a21\u578b\u6765\u63cf\u8ff0\u4eba\u673a\u4ea4\u4e92\u7684\u793e\u4ea4\u60c5\u5883\uff0c\u5e76\u5c06\u8be5\u6a21\u578b\u5e94\u7528\u4e8e\u73b0\u6709\u7814\u7a76\u5de5\u4f5c\u3002", "result": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6982\u5ff5\u6a21\u578b\uff0c\u80fd\u591f\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u89c4\u5212\u4ea4\u4e92\u3001\u5f00\u53d1\u673a\u5668\u4eba\u884c\u4e3a\u6a21\u578b\uff0c\u5e76\u5728\u4ea4\u4e92\u53d1\u751f\u540e\u83b7\u5f97\u6d1e\u5bdf\u3002", "conclusion": "\u63d0\u51fa\u4e86\u7406\u89e3\u4eba\u673a\u4ea4\u4e92\u793e\u4ea4\u60c5\u5883\u7684\u91cd\u8981\u6846\u67b6\uff0c\u5e76\u6307\u51fa\u4e86\u8be5\u9886\u57df\u672a\u6765\u7814\u7a76\u7684\u5173\u952e\u95ee\u9898\u548c\u53d1\u5c55\u65b9\u5411\u3002"}}
{"id": "2508.13634", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13634", "abs": "https://arxiv.org/abs/2508.13634", "authors": ["Jikai Chen", "Long Chen", "Dong Wang", "Leilei Gan", "Chenyi Zhuang", "Jinjie Gu"], "title": "V2P: From Background Suppression to Center Peaking for Robust GUI Grounding Task", "comment": null, "summary": "Precise localization of GUI elements is crucial for the development of GUI\nagents. Traditional methods rely on bounding box or center-point regression,\nneglecting spatial interaction uncertainty and visual-semantic hierarchies.\nRecent methods incorporate attention mechanisms but still face two key issues:\n(1) ignoring processing background regions causes attention drift from the\ndesired area, and (2) uniform labeling fails to distinguish between center and\nedges of the target UI element, leading to click imprecision. Inspired by how\nhumans visually process and interact with GUI elements, we propose the\nValley-to-Peak (V2P) method to address these issues. To mitigate background\ndistractions, V2P introduces a suppression attention mechanism that minimizes\nthe model's focus on irrelevant regions to highlight the intended region. For\nthe issue of center-edge distinction, V2P applies a Fitts' Law-inspired\napproach by modeling GUI interactions as 2D Gaussian heatmaps where the weight\ngradually decreases from the center towards the edges. The weight distribution\nfollows a Gaussian function, with the variance determined by the target's size.\nConsequently, V2P effectively isolates the target area and teaches the model to\nconcentrate on the most essential point of the UI element. The model trained by\nV2P achieves the performance with 92.3% and 50.5% on two benchmarks\nScreenSpot-v2 and ScreenSpot-Pro. Ablations further confirm each component's\ncontribution, highlighting V2P's generalizability for precise GUI grounding\ntasks.", "AI": {"tldr": "V2P\u65b9\u6cd5\u901a\u8fc7\u6291\u5236\u6ce8\u610f\u529b\u673a\u5236\u548cFitts\u5b9a\u5f8b\u542f\u53d1\u76842D\u9ad8\u65af\u70ed\u56fe\u5efa\u6a21\uff0c\u89e3\u51b3\u4e86GUI\u5143\u7d20\u5b9a\u4f4d\u4e2d\u7684\u80cc\u666f\u5e72\u6270\u548c\u4e2d\u5fc3-\u8fb9\u7f18\u533a\u5206\u95ee\u9898\uff0c\u5728ScreenSpot\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e8692.3%\u548c50.5%\u7684\u6027\u80fd", "motivation": "\u4f20\u7edfGUI\u5b9a\u4f4d\u65b9\u6cd5\u4f9d\u8d56\u8fb9\u754c\u6846\u6216\u4e2d\u5fc3\u70b9\u56de\u5f52\uff0c\u5ffd\u89c6\u4e86\u7a7a\u95f4\u4ea4\u4e92\u4e0d\u786e\u5b9a\u6027\u548c\u89c6\u89c9\u8bed\u4e49\u5c42\u6b21\u7ed3\u6784\uff0c\u4e14\u5b58\u5728\u80cc\u666f\u5e72\u6270\u5bfc\u81f4\u6ce8\u610f\u529b\u6f02\u79fb\u4ee5\u53ca\u5747\u5300\u6807\u6ce8\u65e0\u6cd5\u533a\u5206\u76ee\u6807UI\u5143\u7d20\u4e2d\u5fc3\u4e0e\u8fb9\u7f18\u7684\u95ee\u9898", "method": "\u63d0\u51faValley-to-Peak (V2P)\u65b9\u6cd5\uff1a1) \u6291\u5236\u6ce8\u610f\u529b\u673a\u5236\u6700\u5c0f\u5316\u5bf9\u65e0\u5173\u533a\u57df\u7684\u5173\u6ce8\uff1b2) \u57fa\u4e8eFitts\u5b9a\u5f8b\u76842D\u9ad8\u65af\u70ed\u56fe\u5efa\u6a21\uff0c\u6743\u91cd\u4ece\u4e2d\u5fc3\u5411\u8fb9\u7f18\u9010\u6e10\u9012\u51cf\uff0c\u65b9\u5dee\u7531\u76ee\u6807\u5c3a\u5bf8\u51b3\u5b9a", "result": "\u5728ScreenSpot-v2\u548cScreenSpot-Pro\u4e24\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u5206\u522b\u8fbe\u523092.3%\u548c50.5%\u7684\u6027\u80fd\uff0c\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5404\u7ec4\u4ef6\u8d21\u732e", "conclusion": "V2P\u65b9\u6cd5\u80fd\u6709\u6548\u9694\u79bb\u76ee\u6807\u533a\u57df\u5e76\u8ba9\u6a21\u578b\u4e13\u6ce8\u4e8eUI\u5143\u7d20\u7684\u6700\u5173\u952e\u70b9\uff0c\u5c55\u793a\u4e86\u5728\u7cbe\u786eGUI\u5b9a\u4f4d\u4efb\u52a1\u4e2d\u7684\u826f\u597d\u6cdb\u5316\u80fd\u529b"}}
{"id": "2508.13998", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.13998", "abs": "https://arxiv.org/abs/2508.13998", "authors": ["Yifu Yuan", "Haiqin Cui", "Yaoting Huang", "Yibin Chen", "Fei Ni", "Zibin Dong", "Pengyi Li", "Yan Zheng", "Jianye Hao"], "title": "Embodied-R1: Reinforced Embodied Reasoning for General Robotic Manipulation", "comment": "Embodied-R1 technical report", "summary": "Generalization in embodied AI is hindered by the \"seeing-to-doing gap,\" which\nstems from data scarcity and embodiment heterogeneity. To address this, we\npioneer \"pointing\" as a unified, embodiment-agnostic intermediate\nrepresentation, defining four core embodied pointing abilities that bridge\nhigh-level vision-language comprehension with low-level action primitives. We\nintroduce Embodied-R1, a 3B Vision-Language Model (VLM) specifically designed\nfor embodied reasoning and pointing. We use a wide range of embodied and\ngeneral visual reasoning datasets as sources to construct a large-scale\ndataset, Embodied-Points-200K, which supports key embodied pointing\ncapabilities. We then train Embodied-R1 using a two-stage Reinforced\nFine-tuning (RFT) curriculum with a specialized multi-task reward design.\nEmbodied-R1 achieves state-of-the-art performance on 11 embodied spatial and\npointing benchmarks. Critically, it demonstrates robust zero-shot\ngeneralization by achieving a 56.2% success rate in the SIMPLEREnv and 87.5%\nacross 8 real-world XArm tasks without any task-specific fine-tuning,\nrepresenting a 62% improvement over strong baselines. Furthermore, the model\nexhibits high robustness against diverse visual disturbances. Our work shows\nthat a pointing-centric representation, combined with an RFT training paradigm,\noffers an effective and generalizable pathway to closing the perception-action\ngap in robotics.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86Embodied-R1\u6a21\u578b\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\"\u6307\u5411\"\u8868\u793a\u6765\u89e3\u51b3\u5177\u8eabAI\u4e2d\u7684\"\u770b\u5230-\u884c\u52a8\"\u9e3f\u6c9f\u95ee\u9898\uff0c\u572811\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5e76\u5728\u96f6\u6837\u672c\u6cdb\u5316\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u5177\u8eabAI\u4e2d\u7684\"\u770b\u5230-\u884c\u52a8\u9e3f\u6c9f\"\u95ee\u9898\uff0c\u8be5\u95ee\u9898\u6e90\u4e8e\u6570\u636e\u7a00\u7f3a\u548c\u5177\u8eab\u5f02\u6784\u6027\uff0c\u963b\u788d\u4e86\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\"\u6307\u5411\"\u4f5c\u4e3a\u7edf\u4e00\u7684\u3001\u5177\u8eab\u65e0\u5173\u7684\u4e2d\u95f4\u8868\u793a\uff0c\u6784\u5efaEmbodied-Points-200K\u6570\u636e\u96c6\uff0c\u4f7f\u7528\u4e24\u9636\u6bb5\u5f3a\u5316\u5fae\u8c03(RFT)\u8bfe\u7a0b\u8bad\u7ec33B\u53c2\u6570\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578bEmbodied-R1\u3002", "result": "\u572811\u4e2a\u5177\u8eab\u7a7a\u95f4\u548c\u6307\u5411\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5728SIMPLEREnv\u4e2d\u96f6\u6837\u672c\u6210\u529f\u7387\u8fbe\u523056.2%\uff0c\u57288\u4e2a\u771f\u5b9e\u4e16\u754cXArm\u4efb\u52a1\u4e2d\u8fbe\u523087.5%\u6210\u529f\u7387\uff0c\u6bd4\u5f3a\u57fa\u7ebf\u63d0\u534762%\u3002", "conclusion": "\u6307\u5411\u4e2d\u5fc3\u8868\u793a\u7ed3\u5408RFT\u8bad\u7ec3\u8303\u5f0f\u4e3a\u89e3\u51b3\u673a\u5668\u4eba\u611f\u77e5-\u884c\u52a8\u9e3f\u6c9f\u63d0\u4f9b\u4e86\u6709\u6548\u4e14\u53ef\u6cdb\u5316\u7684\u9014\u5f84\u3002"}}
{"id": "2508.13663", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.13663", "abs": "https://arxiv.org/abs/2508.13663", "authors": ["Daniel Daza", "Alberto Bernardi", "Luca Costabello", "Christophe Gueret", "Masoud Mansoury", "Michael Cochez", "Martijn Schut"], "title": "Interactive Query Answering on Knowledge Graphs with Soft Entity Constraints", "comment": null, "summary": "Methods for query answering over incomplete knowledge graphs retrieve\nentities that are likely to be answers, which is particularly useful when such\nanswers cannot be reached by direct graph traversal due to missing edges.\nHowever, existing approaches have focused on queries formalized using\nfirst-order-logic. In practice, many real-world queries involve constraints\nthat are inherently vague or context-dependent, such as preferences for\nattributes or related categories. Addressing this gap, we introduce the problem\nof query answering with soft constraints. We propose a Neural Query Reranker\n(NQR) designed to adjust query answer scores by incorporating soft constraints\nwithout disrupting the original answers to a query. NQR operates interactively,\nrefining answers based on incremental examples of preferred and non-preferred\nentities. We extend existing QA benchmarks by generating datasets with soft\nconstraints. Our experiments demonstrate that NQR can capture soft constraints\nwhile maintaining robust query answering performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u8f6f\u7ea6\u675f\u7684\u77e5\u8bc6\u56fe\u8c31\u67e5\u8be2\u5e94\u7b54\u65b9\u6cd5NQR\uff0c\u901a\u8fc7\u4ea4\u4e92\u5f0f\u5b66\u4e60\u7528\u6237\u504f\u597d\u6765\u8c03\u6574\u67e5\u8be2\u7ed3\u679c\u6392\u5e8f\uff0c\u540c\u65f6\u4fdd\u6301\u539f\u6709\u67e5\u8be2\u7b54\u6848\u7684\u5b8c\u6574\u6027", "motivation": "\u73b0\u6709\u77e5\u8bc6\u56fe\u8c31\u67e5\u8be2\u65b9\u6cd5\u4e3b\u8981\u57fa\u4e8e\u4e00\u9636\u903b\u8f91\uff0c\u65e0\u6cd5\u5904\u7406\u73b0\u5b9e\u4e16\u754c\u4e2d\u5e38\u89c1\u7684\u6a21\u7cca\u6216\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u8f6f\u7ea6\u675f\uff08\u5982\u5c5e\u6027\u504f\u597d\u3001\u76f8\u5173\u7c7b\u522b\u504f\u597d\uff09", "method": "\u63d0\u51fa\u795e\u7ecf\u67e5\u8be2\u91cd\u6392\u5e8f\u5668(NQR)\uff0c\u901a\u8fc7\u589e\u91cf\u5b66\u4e60\u7528\u6237\u63d0\u4f9b\u7684\u504f\u597d\u548c\u975e\u504f\u597d\u5b9e\u4f53\u793a\u4f8b\uff0c\u5728\u4e0d\u7834\u574f\u539f\u59cb\u67e5\u8be2\u7b54\u6848\u7684\u524d\u63d0\u4e0b\u8c03\u6574\u7b54\u6848\u5f97\u5206", "result": "\u5728\u6269\u5c55\u7684QA\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u9a8c\u8bc1\u4e86NQR\u80fd\u591f\u6709\u6548\u6355\u83b7\u8f6f\u7ea6\u675f\uff0c\u540c\u65f6\u4fdd\u6301\u7a33\u5065\u7684\u67e5\u8be2\u5e94\u7b54\u6027\u80fd", "conclusion": "NQR\u4e3a\u89e3\u51b3\u77e5\u8bc6\u56fe\u8c31\u67e5\u8be2\u4e2d\u7684\u8f6f\u7ea6\u675f\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\uff0c\u652f\u6301\u4ea4\u4e92\u5f0f\u504f\u597d\u5b66\u4e60\uff0c\u5728\u4fdd\u6301\u67e5\u8be2\u5b8c\u6574\u6027\u7684\u540c\u65f6\u63d0\u5347\u7ed3\u679c\u76f8\u5173\u6027"}}
{"id": "2508.14042", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.14042", "abs": "https://arxiv.org/abs/2508.14042", "authors": ["Zhuoling Li", "Xiaoyang Wu", "Zhenhua Xu", "Hengshuang Zhao"], "title": "Train Once, Deploy Anywhere: Realize Data-Efficient Dynamic Object Manipulation", "comment": null, "summary": "Realizing generalizable dynamic object manipulation is important for\nenhancing manufacturing efficiency, as it eliminates specialized engineering\nfor various scenarios. To this end, imitation learning emerges as a promising\nparadigm, leveraging expert demonstrations to teach a policy manipulation\nskills. Although the generalization of an imitation learning policy can be\nimproved by increasing demonstrations, demonstration collection is\nlabor-intensive. To address this problem, this paper investigates whether\nstrong generalization in dynamic object manipulation is achievable with only a\nfew demonstrations. Specifically, we develop an entropy-based theoretical\nframework to quantify the optimization of imitation learning. Based on this\nframework, we propose a system named Generalizable Entropy-based Manipulation\n(GEM). Extensive experiments in simulated and real tasks demonstrate that GEM\ncan generalize across diverse environment backgrounds, robot embodiments,\nmotion dynamics, and object geometries. Notably, GEM has been deployed in a\nreal canteen for tableware collection. Without any in-scene demonstration, it\nachieves a success rate of over 97% across more than 10,000 operations.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u71b5\u7684\u7406\u8bba\u6846\u67b6\u548cGEM\u7cfb\u7edf\uff0c\u4ec5\u9700\u5c11\u91cf\u6f14\u793a\u5c31\u80fd\u5b9e\u73b0\u52a8\u6001\u7269\u4f53\u64cd\u4f5c\u7684\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u771f\u5b9e\u98df\u5802\u9910\u5177\u6536\u96c6\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e8697%\u4ee5\u4e0a\u7684\u6210\u529f\u7387\u3002", "motivation": "\u89e3\u51b3\u52a8\u6001\u7269\u4f53\u64cd\u4f5c\u4e2d\u6f14\u793a\u6570\u636e\u6536\u96c6\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u63a2\u7d22\u4ec5\u7528\u5c11\u91cf\u6f14\u793a\u5c31\u80fd\u5b9e\u73b0\u5f3a\u6cdb\u5316\u80fd\u529b\u7684\u53ef\u80fd\u6027\uff0c\u63d0\u9ad8\u5236\u9020\u4e1a\u6548\u7387\u3002", "method": "\u5f00\u53d1\u57fa\u4e8e\u71b5\u7684\u7406\u8bba\u6846\u67b6\u6765\u91cf\u5316\u6a21\u4eff\u5b66\u4e60\u7684\u4f18\u5316\uff0c\u5e76\u57fa\u4e8e\u6b64\u63d0\u51faGeneralizable Entropy-based Manipulation (GEM)\u7cfb\u7edf\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4efb\u52a1\u4e2d\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cGEM\u80fd\u591f\u6cdb\u5316\u5230\u4e0d\u540c\u7684\u73af\u5883\u80cc\u666f\u3001\u673a\u5668\u4eba\u5f62\u6001\u3001\u8fd0\u52a8\u52a8\u529b\u5b66\u548c\u7269\u4f53\u51e0\u4f55\u5f62\u72b6\u3002\u5728\u771f\u5b9e\u98df\u5802\u9910\u5177\u6536\u96c6\u4e2d\uff0c\u65e0\u9700\u573a\u666f\u5185\u6f14\u793a\uff0c\u8d85\u8fc710,000\u6b21\u64cd\u4f5c\u4e2d\u6210\u529f\u7387\u8d85\u8fc797%\u3002", "conclusion": "GEM\u7cfb\u7edf\u8bc1\u660e\u4e86\u4ec5\u7528\u5c11\u91cf\u6f14\u793a\u5c31\u80fd\u5b9e\u73b0\u52a8\u6001\u7269\u4f53\u64cd\u4f5c\u7684\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u4e3a\u5236\u9020\u4e1a\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.13672", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13672", "abs": "https://arxiv.org/abs/2508.13672", "authors": ["Rehan Raza", "Guanjin Wang", "Kevin Wong", "Hamid Laga", "Marco Fisichella"], "title": "ITL-LIME: Instance-Based Transfer Learning for Enhancing Local Explanations in Low-Resource Data Settings", "comment": "Accepted at the 34th ACM International Conference on Information and\n  Knowledge Management (CIKM 2025)", "summary": "Explainable Artificial Intelligence (XAI) methods, such as Local\nInterpretable Model-Agnostic Explanations (LIME), have advanced the\ninterpretability of black-box machine learning models by approximating their\nbehavior locally using interpretable surrogate models. However, LIME's inherent\nrandomness in perturbation and sampling can lead to locality and instability\nissues, especially in scenarios with limited training data. In such cases, data\nscarcity can result in the generation of unrealistic variations and samples\nthat deviate from the true data manifold. Consequently, the surrogate model may\nfail to accurately approximate the complex decision boundary of the original\nmodel. To address these challenges, we propose a novel Instance-based Transfer\nLearning LIME framework (ITL-LIME) that enhances explanation fidelity and\nstability in data-constrained environments. ITL-LIME introduces instance\ntransfer learning into the LIME framework by leveraging relevant real instances\nfrom a related source domain to aid the explanation process in the target\ndomain. Specifically, we employ clustering to partition the source domain into\nclusters with representative prototypes. Instead of generating random\nperturbations, our method retrieves pertinent real source instances from the\nsource cluster whose prototype is most similar to the target instance. These\nare then combined with the target instance's neighboring real instances. To\ndefine a compact locality, we further construct a contrastive learning-based\nencoder as a weighting mechanism to assign weights to the instances from the\ncombined set based on their proximity to the target instance. Finally, these\nweighted source and target instances are used to train the surrogate model for\nexplanation purposes.", "AI": {"tldr": "\u63d0\u51faITL-LIME\u6846\u67b6\uff0c\u901a\u8fc7\u5b9e\u4f8b\u8fc1\u79fb\u5b66\u4e60\u89e3\u51b3LIME\u5728\u6570\u636e\u7a00\u7f3a\u73af\u5883\u4e0b\u7684\u5c40\u90e8\u6027\u548c\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u5229\u7528\u76f8\u5173\u6e90\u57df\u7684\u771f\u5b9e\u5b9e\u4f8b\u63d0\u5347\u89e3\u91ca\u7684\u4fdd\u771f\u5ea6\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "LIME\u65b9\u6cd5\u5728\u6270\u52a8\u548c\u91c7\u6837\u8fc7\u7a0b\u4e2d\u5b58\u5728\u968f\u673a\u6027\uff0c\u5bfc\u81f4\u5728\u8bad\u7ec3\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u4ea7\u751f\u5c40\u90e8\u6027\u548c\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u53ef\u80fd\u751f\u6210\u504f\u79bb\u771f\u5b9e\u6570\u636e\u6d41\u5f62\u7684\u6837\u672c\uff0c\u4f7f\u4ee3\u7406\u6a21\u578b\u65e0\u6cd5\u51c6\u786e\u8fd1\u4f3c\u539f\u59cb\u6a21\u578b\u7684\u590d\u6742\u51b3\u7b56\u8fb9\u754c\u3002", "method": "ITL-LIME\u5f15\u5165\u5b9e\u4f8b\u8fc1\u79fb\u5b66\u4e60\uff0c\u901a\u8fc7\u805a\u7c7b\u5c06\u6e90\u57df\u5206\u533a\u5e76\u83b7\u53d6\u4ee3\u8868\u6027\u539f\u578b\uff0c\u68c0\u7d22\u4e0e\u76ee\u6807\u5b9e\u4f8b\u6700\u76f8\u4f3c\u7684\u539f\u578b\u5bf9\u5e94\u7684\u6e90\u57df\u771f\u5b9e\u5b9e\u4f8b\uff0c\u7ed3\u5408\u76ee\u6807\u5b9e\u4f8b\u7684\u90bb\u8fd1\u771f\u5b9e\u5b9e\u4f8b\uff0c\u4f7f\u7528\u5bf9\u6bd4\u5b66\u4e60\u7f16\u7801\u5668\u4f5c\u4e3a\u52a0\u6743\u673a\u5236\uff0c\u57fa\u4e8e\u5b9e\u4f8b\u4e0e\u76ee\u6807\u5b9e\u4f8b\u7684\u63a5\u8fd1\u7a0b\u5ea6\u5206\u914d\u6743\u91cd\uff0c\u6700\u540e\u7528\u52a0\u6743\u540e\u7684\u6e90\u57df\u548c\u76ee\u6807\u5b9e\u4f8b\u8bad\u7ec3\u4ee3\u7406\u6a21\u578b\u8fdb\u884c\u89e3\u91ca\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u6570\u636e\u53d7\u9650\u73af\u5883\u4e0b\u63d0\u9ad8\u4e86\u89e3\u91ca\u7684\u4fdd\u771f\u5ea6\u548c\u7a33\u5b9a\u6027\uff0c\u901a\u8fc7\u5229\u7528\u76f8\u5173\u6e90\u57df\u7684\u771f\u5b9e\u5b9e\u4f8b\u907f\u514d\u4e86\u751f\u6210\u4e0d\u73b0\u5b9e\u7684\u53d8\u5f02\u6837\u672c\uff0c\u66f4\u51c6\u786e\u5730\u8fd1\u4f3c\u4e86\u539f\u59cb\u6a21\u578b\u7684\u51b3\u7b56\u8fb9\u754c\u3002", "conclusion": "ITL-LIME\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86LIME\u5728\u6570\u636e\u7a00\u7f3a\u573a\u666f\u4e0b\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u5b9e\u4f8b\u8fc1\u79fb\u5b66\u4e60\u548c\u5bf9\u6bd4\u5b66\u4e60\u52a0\u6743\u673a\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\u65b9\u6cd5\u5728\u73b0\u5b9e\u6570\u636e\u7ea6\u675f\u73af\u5883\u4e2d\u7684\u6027\u80fd\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2508.13675", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13675", "abs": "https://arxiv.org/abs/2508.13675", "authors": ["Mariam Arustashvili", "J\u00f6rg Deigm\u00f6ller", "Heiko Paulheim"], "title": "Knowledge Graph Completion for Action Prediction on Situational Graphs -- A Case Study on Household Tasks", "comment": "Accepted at Semantics 2025", "summary": "Knowledge Graphs are used for various purposes, including business\napplications, biomedical analyses, or digital twins in industry 4.0. In this\npaper, we investigate knowledge graphs describing household actions, which are\nbeneficial for controlling household robots and analyzing video footage. In the\nlatter case, the information extracted from videos is notoriously incomplete,\nand completing the knowledge graph for enhancing the situational picture is\nessential. In this paper, we show that, while a standard link prediction\nproblem, situational knowledge graphs have special characteristics that render\nmany link prediction algorithms not fit for the job, and unable to outperform\neven simple baselines.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u53d1\u73b0\uff0c\u7528\u4e8e\u5bb6\u5ead\u884c\u4e3a\u63cf\u8ff0\u7684\u60c5\u5883\u77e5\u8bc6\u56fe\u8c31\u5177\u6709\u7279\u6b8a\u7279\u5f81\uff0c\u4f7f\u5f97\u8bb8\u591a\u6807\u51c6\u94fe\u63a5\u9884\u6d4b\u7b97\u6cd5\u8868\u73b0\u4e0d\u4f73\uff0c\u751a\u81f3\u65e0\u6cd5\u8d85\u8d8a\u7b80\u5355\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u77e5\u8bc6\u56fe\u8c31\u5728\u5bb6\u5ead\u673a\u5668\u4eba\u63a7\u5236\u548c\u89c6\u9891\u5206\u6790\u4e2d\u5f88\u91cd\u8981\uff0c\u4f46\u89c6\u9891\u63d0\u53d6\u7684\u4fe1\u606f\u901a\u5e38\u4e0d\u5b8c\u6574\uff0c\u9700\u8981\u8865\u5168\u77e5\u8bc6\u56fe\u8c31\u6765\u589e\u5f3a\u60c5\u5883\u7406\u89e3\u3002", "method": "\u7814\u7a76\u5206\u6790\u4e86\u60c5\u5883\u77e5\u8bc6\u56fe\u8c31\u7684\u7279\u6b8a\u7279\u6027\uff0c\u5e76\u6bd4\u8f83\u4e86\u591a\u79cd\u94fe\u63a5\u9884\u6d4b\u7b97\u6cd5\u5728\u8fd9\u4e9b\u56fe\u8c31\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u6807\u51c6\u94fe\u63a5\u9884\u6d4b\u7b97\u6cd5\u5728\u60c5\u5883\u77e5\u8bc6\u56fe\u8c31\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u65e0\u6cd5\u8d85\u8d8a\u7b80\u5355\u57fa\u7ebf\u65b9\u6cd5\uff0c\u8bf4\u660e\u8fd9\u4e9b\u7b97\u6cd5\u4e0d\u9002\u5408\u6b64\u7c7b\u7279\u6b8a\u4efb\u52a1\u3002", "conclusion": "\u60c5\u5883\u77e5\u8bc6\u56fe\u8c31\u5177\u6709\u72ec\u7279\u7279\u5f81\uff0c\u9700\u8981\u5f00\u53d1\u4e13\u95e8\u7684\u94fe\u63a5\u9884\u6d4b\u65b9\u6cd5\u6765\u6709\u6548\u5904\u7406\u5bb6\u5ead\u884c\u4e3a\u63cf\u8ff0\u4efb\u52a1\u3002"}}
{"id": "2508.13676", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13676", "abs": "https://arxiv.org/abs/2508.13676", "authors": ["Yu Li", "Zulong Chen", "Wenjian Xu", "Hong Wen", "Yipeng Yu", "Man Lung Yiu", "Yuyu Yin"], "title": "MHSNet:An MoE-based Hierarchical Semantic Representation Network for Accurate Duplicate Resume Detection with Large Language Model", "comment": null, "summary": "To maintain the company's talent pool, recruiters need to continuously search\nfor resumes from third-party websites (e.g., LinkedIn, Indeed). However,\nfetched resumes are often incomplete and inaccurate. To improve the quality of\nthird-party resumes and enrich the company's talent pool, it is essential to\nconduct duplication detection between the fetched resumes and those already in\nthe company's talent pool. Such duplication detection is challenging due to the\nsemantic complexity, structural heterogeneity, and information incompleteness\nof resume texts. To this end, we propose MHSNet, an multi-level identity\nverification framework that fine-tunes BGE-M3 using contrastive learning. With\nthe fine-tuned , Mixture-of-Experts (MoE) generates multi-level sparse and\ndense representations for resumes, enabling the computation of corresponding\nmulti-level semantic similarities. Moreover, the state-aware Mixture-of-Experts\n(MoE) is employed in MHSNet to handle diverse incomplete resumes. Experimental\nresults verify the effectiveness of MHSNet", "AI": {"tldr": "MHSNet\u662f\u4e00\u4e2a\u591a\u7ea7\u8eab\u4efd\u9a8c\u8bc1\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u7b2c\u4e09\u65b9\u7b80\u5386\u4e0e\u516c\u53f8\u4eba\u624d\u5e93\u4e2d\u7b80\u5386\u7684\u91cd\u590d\u9879\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u5fae\u8c03BGE-M3\u6a21\u578b\uff0c\u5229\u7528MoE\u751f\u6210\u591a\u7ea7\u7a00\u758f\u548c\u5bc6\u96c6\u8868\u793a\u6765\u8ba1\u7b97\u8bed\u4e49\u76f8\u4f3c\u5ea6\u3002", "motivation": "\u7b2c\u4e09\u65b9\u7f51\u7ad9\u83b7\u53d6\u7684\u7b80\u5386\u901a\u5e38\u4e0d\u5b8c\u6574\u4e14\u4e0d\u51c6\u786e\uff0c\u9700\u8981\u901a\u8fc7\u91cd\u590d\u68c0\u6d4b\u6765\u63d0\u5347\u7b80\u5386\u8d28\u91cf\u5e76\u4e30\u5bcc\u516c\u53f8\u4eba\u624d\u5e93\uff0c\u4f46\u7b80\u5386\u6587\u672c\u7684\u8bed\u4e49\u590d\u6742\u6027\u3001\u7ed3\u6784\u5f02\u8d28\u6027\u548c\u4fe1\u606f\u4e0d\u5b8c\u6574\u6027\u4f7f\u5f97\u8fd9\u4e00\u4efb\u52a1\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u63d0\u51faMHSNet\u6846\u67b6\uff0c\u4f7f\u7528\u5bf9\u6bd4\u5b66\u4e60\u5fae\u8c03BGE-M3\u6a21\u578b\uff0c\u901a\u8fc7\u72b6\u6001\u611f\u77e5\u7684Mixture-of-Experts (MoE) \u751f\u6210\u591a\u7ea7\u7a00\u758f\u548c\u5bc6\u96c6\u8868\u793a\u6765\u8ba1\u7b97\u591a\u7ea7\u8bed\u4e49\u76f8\u4f3c\u5ea6\uff0c\u5904\u7406\u5404\u79cd\u4e0d\u5b8c\u6574\u7b80\u5386\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86MHSNet\u7684\u6709\u6548\u6027\u3002", "conclusion": "MHSNet\u80fd\u591f\u6709\u6548\u89e3\u51b3\u7b80\u5386\u91cd\u590d\u68c0\u6d4b\u4e2d\u7684\u6311\u6218\uff0c\u63d0\u5347\u7b2c\u4e09\u65b9\u7b80\u5386\u8d28\u91cf\u5e76\u4e30\u5bcc\u516c\u53f8\u4eba\u624d\u5e93\u3002"}}
{"id": "2508.13678", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.13678", "abs": "https://arxiv.org/abs/2508.13678", "authors": ["Xiao-Wen Yang", "Jie-Jing Shao", "Lan-Zhe Guo", "Bo-Wen Zhang", "Zhi Zhou", "Lin-Han Jia", "Wang-Zhou Dai", "Yu-Feng Li"], "title": "Neuro-Symbolic Artificial Intelligence: Towards Improving the Reasoning Abilities of Large Language Models", "comment": "9 pages, 3 figures, IJCAI 2025 Survey Track", "summary": "Large Language Models (LLMs) have shown promising results across various\ntasks, yet their reasoning capabilities remain a fundamental challenge.\nDeveloping AI systems with strong reasoning capabilities is regarded as a\ncrucial milestone in the pursuit of Artificial General Intelligence (AGI) and\nhas garnered considerable attention from both academia and industry. Various\ntechniques have been explored to enhance the reasoning capabilities of LLMs,\nwith neuro-symbolic approaches being a particularly promising way. This paper\ncomprehensively reviews recent developments in neuro-symbolic approaches for\nenhancing LLM reasoning. We first present a formalization of reasoning tasks\nand give a brief introduction to the neurosymbolic learning paradigm. Then, we\ndiscuss neuro-symbolic methods for improving the reasoning capabilities of LLMs\nfrom three perspectives: Symbolic->LLM, LLM->Symbolic, and LLM+Symbolic.\nFinally, we discuss several key challenges and promising future directions. We\nhave also released a GitHub repository including papers and resources related\nto this survey: https://github.com/LAMDASZ-ML/Awesome-LLM-Reasoning-with-NeSy.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u5168\u9762\u7efc\u8ff0\u4e86\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u5728\u589e\u5f3a\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u65b9\u9762\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u4ece\u4e09\u4e2a\u89d2\u5ea6\u8ba8\u8bba\u4e86\u76f8\u5173\u65b9\u6cd5\uff0c\u5e76\u6307\u51fa\u4e86\u5173\u952e\u6311\u6218\u548c\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5404\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u63a8\u7406\u80fd\u529b\u4ecd\u7136\u662f\u4e00\u4e2a\u57fa\u672c\u6311\u6218\u3002\u5f00\u53d1\u5177\u6709\u5f3a\u5927\u63a8\u7406\u80fd\u529b\u7684AI\u7cfb\u7edf\u88ab\u8ba4\u4e3a\u662f\u5b9e\u73b0\u901a\u7528\u4eba\u5de5\u667a\u80fd\u7684\u5173\u952e\u91cc\u7a0b\u7891\uff0c\u53d7\u5230\u5b66\u672f\u754c\u548c\u5de5\u4e1a\u754c\u7684\u5e7f\u6cdb\u5173\u6ce8\u3002", "method": "\u8bba\u6587\u4ece\u4e09\u4e2a\u89d2\u5ea6\u8ba8\u8bba\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\uff1a\u7b26\u53f7\u2192LLM\u3001LLM\u2192\u7b26\u53f7\u3001\u4ee5\u53caLLM+\u7b26\u53f7\u7684\u534f\u540c\u65b9\u6cd5\uff0c\u5bf9\u6bcf\u79cd\u65b9\u6cd5\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u7684\u7efc\u8ff0\u548c\u5206\u6790\u3002", "result": "\u8bba\u6587\u63d0\u4f9b\u4e86\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u589e\u5f3aLLM\u63a8\u7406\u80fd\u529b\u7684\u5168\u9762\u7efc\u8ff0\uff0c\u5efa\u7acb\u4e86\u5f62\u5f0f\u5316\u7684\u63a8\u7406\u4efb\u52a1\u6846\u67b6\uff0c\u5e76\u53d1\u5e03\u4e86\u5305\u542b\u76f8\u5173\u8bba\u6587\u548c\u8d44\u6e90\u7684GitHub\u4ed3\u5e93\u3002", "conclusion": "\u795e\u7ecf\u7b26\u53f7\u65b9\u6cd5\u662f\u589e\u5f3aLLM\u63a8\u7406\u80fd\u529b\u7684\u6709\u524d\u666f\u9014\u5f84\uff0c\u4f46\u4ecd\u9762\u4e34\u4e00\u4e9b\u5173\u952e\u6311\u6218\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u548c\u53d1\u5c55\u3002"}}
{"id": "2508.13697", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13697", "abs": "https://arxiv.org/abs/2508.13697", "authors": ["Vincent Derkinderen", "Robin Manhaeve", "Rik Adriaensen", "Lucas Van Praet", "Lennert De Smet", "Giuseppe Marra", "Luc De Raedt"], "title": "The DeepLog Neurosymbolic Machine", "comment": null, "summary": "We contribute a theoretical and operational framework for neurosymbolic AI\ncalled DeepLog. DeepLog introduces building blocks and primitives for\nneurosymbolic AI that make abstraction of commonly used representations and\ncomputational mechanisms used in neurosymbolic AI. DeepLog can represent and\nemulate a wide range of neurosymbolic systems. It consists of two key\ncomponents. The first is the DeepLog language for specifying neurosymbolic\nmodels and inference tasks. This language consists of an annotated neural\nextension of grounded first-order logic, and makes abstraction of the type of\nlogic, e.g. boolean, fuzzy or probabilistic, and whether logic is used in the\narchitecture or in the loss function. The second DeepLog component is situated\nat the computational level and uses extended algebraic circuits as\ncomputational graphs. Together these two components are to be considered as a\nneurosymbolic abstract machine, with the DeepLog language as the intermediate\nlevel of abstraction and the circuits level as the computational one. DeepLog\nis implemented in software, relies on the latest insights in implementing\nalgebraic circuits on GPUs, and is declarative in that it is easy to obtain\ndifferent neurosymbolic models by making different choices for the underlying\nalgebraic structures and logics. The generality and efficiency of the DeepLog\nneurosymbolic machine is demonstrated through an experimental comparison\nbetween 1) different fuzzy and probabilistic logics, 2) between using logic in\nthe architecture or in the loss function, and 3) between a standalone CPU-based\nimplementation of a neurosymbolic AI system and a DeepLog GPU-based one.", "AI": {"tldr": "DeepLog\u662f\u4e00\u4e2a\u795e\u7ecf\u7b26\u53f7AI\u7684\u7406\u8bba\u548c\u64cd\u4f5c\u6846\u67b6\uff0c\u63d0\u4f9b\u6784\u5efa\u5757\u548c\u539f\u8bed\u6765\u62bd\u8c61\u8868\u793a\u548c\u8ba1\u7b97\u673a\u5236\uff0c\u652f\u6301\u591a\u79cd\u795e\u7ecf\u7b26\u53f7\u7cfb\u7edf\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u795e\u7ecf\u7b26\u53f7AI\u4e2d\u7f3a\u4e4f\u7edf\u4e00\u6846\u67b6\u548c\u62bd\u8c61\u8868\u793a\u7684\u95ee\u9898\uff0c\u63d0\u4f9b\u4e00\u4e2a\u901a\u7528\u7684\u795e\u7ecf\u7b26\u53f7\u62bd\u8c61\u673a\u5668\u6765\u652f\u6301\u4e0d\u540c\u7c7b\u578b\u7684\u903b\u8f91\u548c\u8ba1\u7b97\u65b9\u5f0f\u3002", "method": "\u5305\u542bDeepLog\u8bed\u8a00\uff08\u57fa\u4e8e\u4e00\u9636\u903b\u8f91\u7684\u795e\u7ecf\u6269\u5c55\uff09\u548c\u8ba1\u7b97\u7ea7\u7ec4\u4ef6\uff08\u4f7f\u7528\u6269\u5c55\u4ee3\u6570\u7535\u8def\u4f5c\u4e3a\u8ba1\u7b97\u56fe\uff09\uff0c\u5f62\u6210\u795e\u7ecf\u7b26\u53f7\u62bd\u8c61\u673a\u5668\u3002", "result": "\u5b9e\u73b0\u4e86\u8f6f\u4ef6\u5b9e\u73b0\uff0c\u652f\u6301GPU\u52a0\u901f\uff0c\u80fd\u591f\u6bd4\u8f83\u4e0d\u540c\u6a21\u7cca\u548c\u6982\u7387\u903b\u8f91\u3001\u67b6\u6784\u4e0e\u635f\u5931\u51fd\u6570\u4e2d\u7684\u903b\u8f91\u4f7f\u7528\uff0c\u4ee5\u53caCPU\u4e0eGPU\u5b9e\u73b0\u7684\u6027\u80fd\u5dee\u5f02\u3002", "conclusion": "DeepLog\u63d0\u4f9b\u4e86\u4e00\u4e2a\u901a\u7528\u4e14\u9ad8\u6548\u7684\u795e\u7ecf\u7b26\u53f7AI\u6846\u67b6\uff0c\u901a\u8fc7\u62bd\u8c61\u5316\u548c\u58f0\u660e\u5f0f\u65b9\u6cd5\u7b80\u5316\u4e86\u4e0d\u540c\u795e\u7ecf\u7b26\u53f7\u6a21\u578b\u7684\u6784\u5efa\u548c\u6bd4\u8f83\u3002"}}
{"id": "2508.13721", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13721", "abs": "https://arxiv.org/abs/2508.13721", "authors": ["Minh Hoang Nguyen", "Van Dai Do", "Dung Nguyen", "Thin Nguyen", "Hung Le"], "title": "CausalPlan: Empowering Efficient LLM Multi-Agent Collaboration Through Causality-Driven Planning", "comment": null, "summary": "Large language model (LLM) agents-especially smaller, open-source\nmodels-often produce causally invalid or incoherent actions in collaborative\ntasks due to their reliance on surface-level correlations rather than grounded\ncausal reasoning. This limitation undermines their performance in terms of\ncoordination and planning in dynamic environments. We address this challenge\nwith CausalPlan, a two-phase framework that integrates explicit structural\ncausal reasoning into the LLM planning process. At the core of CausalPlan is\nthe Structural Causal Action (SCA) model, which learns a causal graph from\nagent trajectories to capture how prior actions and current environment states\ninfluence future decisions. This structure is then used to guide action\nselection by assigning causal scores to LLM-generated proposals, reweighting\nthem accordingly, or falling back to causally grounded alternatives when\nneeded. By embedding this causal knowledge directly into the decision loop,\nCausalPlan constrains planning to intervention-consistent behaviours without\nrequiring fine-tuning of the LLM itself. We evaluate CausalPlan on the\nOvercooked-AI benchmark across five multi-agent coordination tasks and four\nLLMs of varying sizes: Gemma-7B, Llama-8B, Qwen-14B, and Llama-70B.\nExperimental results show that CausalPlan consistently reduces invalid actions\nand improves collaboration in both AI-AI and human-AI settings, outperforming\nstrong reinforcement learning baselines. Our findings highlight the value of\ncausality-driven planning for deploying efficient, interpretable, and\ngeneralisable multi-agent LLM systems.", "AI": {"tldr": "CausalPlan\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u663e\u5f0f\u7ed3\u6784\u56e0\u679c\u63a8\u7406\u6765\u6539\u8fdbLLM\u4ee3\u7406\u5728\u534f\u4f5c\u4efb\u52a1\u4e2d\u7684\u89c4\u5212\u80fd\u529b\uff0c\u51cf\u5c11\u65e0\u6548\u52a8\u4f5c\u5e76\u63d0\u9ad8\u534f\u4f5c\u6027\u80fd", "motivation": "\u73b0\u6709LLM\u4ee3\u7406\uff08\u5c24\u5176\u662f\u5c0f\u578b\u5f00\u6e90\u6a21\u578b\uff09\u5728\u534f\u4f5c\u4efb\u52a1\u4e2d\u7ecf\u5e38\u4ea7\u751f\u56e0\u679c\u65e0\u6548\u6216\u4e0d\u8fde\u8d2f\u7684\u52a8\u4f5c\uff0c\u56e0\u4e3a\u5b83\u4eec\u4f9d\u8d56\u8868\u9762\u76f8\u5173\u6027\u800c\u975e\u57fa\u4e8e\u56e0\u679c\u63a8\u7406\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u534f\u8c03\u548c\u89c4\u5212\u80fd\u529b", "method": "\u63d0\u51faCausalPlan\u6846\u67b6\uff0c\u6838\u5fc3\u662f\u7ed3\u6784\u56e0\u679c\u884c\u52a8\uff08SCA\uff09\u6a21\u578b\uff1a1\uff09\u4ece\u4ee3\u7406\u8f68\u8ff9\u4e2d\u5b66\u4e60\u56e0\u679c\u56fe\uff0c\u6355\u6349\u5148\u524d\u884c\u52a8\u548c\u5f53\u524d\u73af\u5883\u72b6\u6001\u5bf9\u672a\u6765\u51b3\u7b56\u7684\u5f71\u54cd\uff1b2\uff09\u4f7f\u7528\u56e0\u679c\u56fe\u4e3aLLM\u751f\u6210\u7684\u52a8\u4f5c\u63d0\u6848\u5206\u914d\u56e0\u679c\u5206\u6570\u5e76\u91cd\u65b0\u52a0\u6743\uff0c\u6216\u5728\u9700\u8981\u65f6\u56de\u9000\u5230\u56e0\u679c\u57fa\u7840\u66ff\u4ee3\u65b9\u6848", "result": "\u5728Overcooked-AI\u57fa\u51c6\u6d4b\u8bd5\u7684\u4e94\u4e2a\u591a\u4ee3\u7406\u534f\u8c03\u4efb\u52a1\u548c\u56db\u4e2a\u4e0d\u540c\u89c4\u6a21\u7684LLM\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0cCausalPlan\u6301\u7eed\u51cf\u5c11\u65e0\u6548\u52a8\u4f5c\uff0c\u5728AI-AI\u548c\u4eba\u7c7b-AI\u8bbe\u7f6e\u4e2d\u90fd\u63d0\u9ad8\u4e86\u534f\u4f5c\u6027\u80fd\uff0c\u4f18\u4e8e\u5f3a\u5316\u5b66\u4e60\u57fa\u7ebf", "conclusion": "\u56e0\u679c\u9a71\u52a8\u89c4\u5212\u5bf9\u4e8e\u90e8\u7f72\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u548c\u53ef\u6cdb\u5316\u7684\u591a\u4ee3\u7406LLM\u7cfb\u7edf\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u65e0\u9700\u5bf9LLM\u672c\u8eab\u8fdb\u884c\u5fae\u8c03\u5373\u53ef\u5b9e\u73b0\u5e72\u9884\u4e00\u81f4\u7684\u884c\u4e3a"}}
{"id": "2508.13754", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13754", "abs": "https://arxiv.org/abs/2508.13754", "authors": ["Liuxin Bao", "Zhihao Peng", "Xiaofei Zhou", "Runmin Cong", "Jiyong Zhang", "Yixuan Yuan"], "title": "Expertise-aware Multi-LLM Recruitment and Collaboration for Medical Decision-Making", "comment": "14 pages", "summary": "Medical Decision-Making (MDM) is a complex process requiring substantial\ndomain-specific expertise to effectively synthesize heterogeneous and\ncomplicated clinical information. While recent advancements in Large Language\nModels (LLMs) show promise in supporting MDM, single-LLM approaches are limited\nby their parametric knowledge constraints and static training corpora, failing\nto robustly integrate the clinical information. To address this challenge, we\npropose the Expertise-aware Multi-LLM Recruitment and Collaboration (EMRC)\nframework to enhance the accuracy and reliability of MDM systems. It operates\nin two stages: (i) expertise-aware agent recruitment and (ii) confidence- and\nadversarial-driven multi-agent collaboration. Specifically, in the first stage,\nwe use a publicly available corpus to construct an LLM expertise table for\ncapturing expertise-specific strengths of multiple LLMs across medical\ndepartment categories and query difficulty levels. This table enables the\nsubsequent dynamic selection of the optimal LLMs to act as medical expert\nagents for each medical query during the inference phase. In the second stage,\nwe employ selected agents to generate responses with self-assessed confidence\nscores, which are then integrated through the confidence fusion and adversarial\nvalidation to improve diagnostic reliability. We evaluate our EMRC framework on\nthree public MDM datasets, where the results demonstrate that our EMRC\noutperforms state-of-the-art single- and multi-LLM methods, achieving superior\ndiagnostic performance. For instance, on the MMLU-Pro-Health dataset, our EMRC\nachieves 74.45% accuracy, representing a 2.69% improvement over the\nbest-performing closed-source model GPT- 4-0613, which demonstrates the\neffectiveness of our expertise-aware agent recruitment strategy and the agent\ncomplementarity in leveraging each LLM's specialized capabilities.", "AI": {"tldr": "\u63d0\u51fa\u4e86EMRC\u6846\u67b6\uff0c\u901a\u8fc7\u4e13\u5bb6\u611f\u77e5\u7684\u591aLLM\u62db\u52df\u4e0e\u534f\u4f5c\u6765\u63d0\u5347\u533b\u7597\u51b3\u7b56\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\uff0c\u5728\u4e09\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6700\u4f73\u65b9\u6cd5", "motivation": "\u5355\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u5728\u533b\u7597\u51b3\u7b56\u4e2d\u5b58\u5728\u53c2\u6570\u77e5\u8bc6\u9650\u5236\u548c\u9759\u6001\u8bad\u7ec3\u6570\u636e\u7684\u95ee\u9898\uff0c\u65e0\u6cd5\u6709\u6548\u6574\u5408\u590d\u6742\u7684\u4e34\u5e8a\u4fe1\u606f", "method": "\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1\uff09\u57fa\u4e8e\u516c\u5f00\u8bed\u6599\u6784\u5efaLLM\u4e13\u4e1a\u80fd\u529b\u8868\u8fdb\u884c\u4e13\u5bb6\u611f\u77e5\u7684\u4ee3\u7406\u62db\u52df\uff1b2\uff09\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u878d\u5408\u548c\u5bf9\u6297\u9a8c\u8bc1\u8fdb\u884c\u591a\u4ee3\u7406\u534f\u4f5c", "result": "\u5728MMLU-Pro-Health\u6570\u636e\u96c6\u4e0a\u8fbe\u523074.45%\u51c6\u786e\u7387\uff0c\u6bd4\u6700\u4f73\u95ed\u6e90\u6a21\u578bGPT-4-0613\u63d0\u53472.69%", "conclusion": "EMRC\u6846\u67b6\u901a\u8fc7\u5229\u7528\u4e0d\u540cLLM\u7684\u4e13\u4e1a\u80fd\u529b\u4e92\u8865\uff0c\u663e\u8457\u63d0\u5347\u4e86\u533b\u7597\u51b3\u7b56\u7cfb\u7edf\u7684\u6027\u80fd"}}
{"id": "2508.13811", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2508.13811", "abs": "https://arxiv.org/abs/2508.13811", "authors": ["Jan Jakub\u016fv", "Mikol\u00e1\u0161 Janota"], "title": "Quantifier Instantiations: To Mimic or To Revolt?", "comment": "Accepted to SMT 2025: 23rd International Workshop on Satisfiability\n  Modulo Theories", "summary": "Quantified formulas pose a significant challenge for Satisfiability Modulo\nTheories (SMT) solvers due to their inherent undecidability. Existing\ninstantiation techniques, such as e-matching, syntax-guided, model-based,\nconflict-based, and enumerative methods, often complement each other. This\npaper introduces a novel instantiation approach that dynamically learns from\nthese techniques during solving. By treating observed instantiations as samples\nfrom a latent language, we use probabilistic context-free grammars to generate\nnew, similar terms. Our method not only mimics successful past instantiations\nbut also explores diversity by optionally inverting learned term probabilities,\naiming to balance exploitation and exploration in quantifier reasoning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6982\u7387\u4e0a\u4e0b\u6587\u65e0\u5173\u6587\u6cd5\u7684\u52a8\u6001\u91cf\u5316\u5b9e\u4f8b\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ece\u73b0\u6709\u6280\u672f\u4e2d\u5b66\u4e60\u5b9e\u4f8b\u5316\u6a21\u5f0f\u6765\u5e73\u8861\u5229\u7528\u4e0e\u63a2\u7d22", "motivation": "\u91cf\u5316\u516c\u5f0f\u662fSMT\u6c42\u89e3\u5668\u9762\u4e34\u7684\u4e3b\u8981\u6311\u6218\uff0c\u73b0\u6709\u5b9e\u4f8b\u5316\u6280\u672f\u5404\u6709\u4f18\u52bf\u4f46\u9700\u8981\u4e92\u8865\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u52a8\u6001\u5b66\u4e60\u5e76\u751f\u6210\u65b0\u5b9e\u4f8b\u7684\u65b9\u6cd5\u6765\u6539\u8fdb\u91cf\u5316\u63a8\u7406", "method": "\u5c06\u89c2\u5bdf\u5230\u7684\u5b9e\u4f8b\u5316\u89c6\u4e3a\u6f5c\u5728\u8bed\u8a00\u7684\u6837\u672c\uff0c\u4f7f\u7528\u6982\u7387\u4e0a\u4e0b\u6587\u65e0\u5173\u6587\u6cd5\u751f\u6210\u76f8\u4f3c\u7684\u65b0\u672f\u8bed\u3002\u53ef\u4ee5\u53cd\u8f6c\u5b66\u4e60\u5230\u7684\u672f\u8bed\u6982\u7387\u6765\u63a2\u7d22\u591a\u6837\u6027", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6a21\u4eff\u6210\u529f\u7684\u8fc7\u5f80\u5b9e\u4f8b\u5316\uff0c\u540c\u65f6\u901a\u8fc7\u6982\u7387\u53cd\u8f6c\u5b9e\u73b0\u63a2\u7d22\u591a\u6837\u6027\uff0c\u5728\u91cf\u5316\u63a8\u7406\u4e2d\u5b9e\u73b0\u5229\u7528\u4e0e\u63a2\u7d22\u7684\u5e73\u8861", "conclusion": "\u63d0\u51fa\u7684\u6982\u7387\u6587\u6cd5\u65b9\u6cd5\u4e3a\u91cf\u5316\u5b9e\u4f8b\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u52a8\u6001\u5b66\u4e60\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u7ed3\u5408\u73b0\u6709\u6280\u672f\u7684\u4f18\u52bf\u5e76\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u5b9e\u4f8b\u5316\u672f\u8bed"}}
{"id": "2508.13828", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13828", "abs": "https://arxiv.org/abs/2508.13828", "authors": ["Yifei Chen", "Guanting Dong", "Yutao Zhu", "Zhicheng Dou"], "title": "Revisiting RAG Ensemble: A Theoretical and Mechanistic Analysis of Multi-RAG System Collaboration", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) technology has been widely applied in\nrecent years. However, despite the emergence of various RAG frameworks, a\nsingle RAG framework still cannot adapt well to a broad range of downstream\ntasks. Therefore, how to leverage the advantages of multiple RAG systems has\nbecome an area worth exploring. To address this issue, we have conducted a\ncomprehensive and systematic investigation into ensemble methods based on RAG\nsystems. Specifically, we have analyzed the RAG ensemble framework from both\ntheoretical and mechanistic analysis perspectives. From the theoretical\nanalysis, we provide the first explanation of the RAG ensemble framework from\nthe perspective of information entropy. In terms of mechanism analysis, we have\nexplored the RAG ensemble framework from both the pipeline and module levels.\nWe carefully select four different pipelines (Branching, Iterative, Loop, and\nAgentic) and three different modules (Generator, Retriever, and Reranker) to\nsolve seven different research questions. The experiments show that aggregating\nmultiple RAG systems is both generalizable and robust, whether at the pipeline\nlevel or the module level. Our work lays the foundation for similar research on\nthe multi-RAG system ensemble.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7814\u7a76\u4e86\u57fa\u4e8e\u591aRAG\u7cfb\u7edf\u7684\u96c6\u6210\u65b9\u6cd5\uff0c\u4ece\u4fe1\u606f\u71b5\u7406\u8bba\u89d2\u5ea6\u89e3\u91caRAG\u96c6\u6210\u6846\u67b6\uff0c\u5e76\u5728\u6d41\u6c34\u7ebf\u548c\u6a21\u5757\u5c42\u9762\u8fdb\u884c\u673a\u5236\u5206\u6790\uff0c\u5b9e\u9a8c\u8bc1\u660e\u591aRAG\u7cfb\u7edf\u96c6\u6210\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u5355\u4e00RAG\u6846\u67b6\u65e0\u6cd5\u5f88\u597d\u9002\u5e94\u5e7f\u6cdb\u7684\u4e0b\u6e38\u4efb\u52a1\uff0c\u9700\u8981\u63a2\u7d22\u5982\u4f55\u5229\u7528\u591a\u4e2aRAG\u7cfb\u7edf\u7684\u4f18\u52bf\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u4ece\u7406\u8bba\u548c\u673a\u5236\u4e24\u4e2a\u89d2\u5ea6\u5206\u6790RAG\u96c6\u6210\u6846\u67b6\uff1a\u7406\u8bba\u5c42\u9762\u4ece\u4fe1\u606f\u71b5\u89d2\u5ea6\u89e3\u91ca\uff1b\u673a\u5236\u5c42\u9762\u4ece\u6d41\u6c34\u7ebf\uff08\u5206\u652f\u3001\u8fed\u4ee3\u3001\u5faa\u73af\u3001\u4ee3\u7406\u56db\u79cd\uff09\u548c\u6a21\u5757\uff08\u751f\u6210\u5668\u3001\u68c0\u7d22\u5668\u3001\u91cd\u6392\u5e8f\u5668\u4e09\u4e2a\uff09\u4e24\u4e2a\u5c42\u7ea7\u8fdb\u884c\u63a2\u7d22\uff0c\u89e3\u51b3\u4e86\u4e03\u4e2a\u4e0d\u540c\u7684\u7814\u7a76\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u65e0\u8bba\u662f\u5728\u6d41\u6c34\u7ebf\u5c42\u9762\u8fd8\u662f\u6a21\u5757\u5c42\u9762\uff0c\u805a\u5408\u591a\u4e2aRAG\u7cfb\u7edf\u90fd\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u591aRAG\u7cfb\u7edf\u96c6\u6210\u76f8\u5173\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u8bc1\u660e\u4e86\u96c6\u6210\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.13876", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.13876", "abs": "https://arxiv.org/abs/2508.13876", "authors": ["Katharina Stein", "Nils Hodel", "Daniel Fi\u0161er", "J\u00f6rg Hoffmann", "Michael Katz", "Alexander Koller"], "title": "Improved Generalized Planning with LLMs through Strategy Refinement and Reflection", "comment": null, "summary": "LLMs have recently been used to generate Python programs representing\ngeneralized plans in PDDL planning, i.e., plans that generalize across the\ntasks of a given PDDL domain. Previous work proposed a framework consisting of\nthree steps: the LLM first generates a summary and then a strategy for the\ndomain, both in natural language, and then implements that strategy as a Python\nprogram, that gets debugged on example planning tasks. In that work, only one\nstrategy is generated and passed directly to the program generation. If the\nstrategy is incorrect, its implementation will therefore result in an incorrect\ngeneralized plan. Here, we introduce an approach that generates the strategy in\nthe form of pseudocode and enables automatic debugging of the pseudocode, hence\nallowing us to identify and fix errors prior to the generation of the\ngeneralized plan itself. Additionally, we extend the Python debugging phase\nwith a reflection step prompting the LLM to pinpoint the reason for the\nobserved plan failure. Finally, we take inspiration from LLM code generation to\nproduce several program variants and pick the best one. Running experiments on\n17 benchmark domains, we show that these extensions substantially improve (and\nnever deteriorate) the quality of the generalized plans. In 12 of the domains,\nour best Python programs solve all tasks that can be generated with the\nrespective instance generator.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684LLM\u751f\u6210\u5e7f\u4e49\u89c4\u5212\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f2a\u4ee3\u7801\u8c03\u8bd5\u548c\u7a0b\u5e8f\u53d8\u4f53\u9009\u62e9\uff0c\u663e\u8457\u63d0\u9ad8\u4e86PDDL\u9886\u57df\u4e2dPython\u7a0b\u5e8f\u7684\u89c4\u5212\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u76f4\u63a5\u751f\u6210\u5355\u4e00\u7b56\u7565\u5e76\u8f6c\u6362\u4e3aPython\u7a0b\u5e8f\uff0c\u5982\u679c\u7b56\u7565\u9519\u8bef\u4f1a\u5bfc\u81f4\u6574\u4e2a\u89c4\u5212\u5931\u8d25\u3002\u9700\u8981\u5148\u9a8c\u8bc1\u7b56\u7565\u6b63\u786e\u6027\u518d\u751f\u6210\u7a0b\u5e8f\u3002", "method": "1) \u751f\u6210\u4f2a\u4ee3\u7801\u7b56\u7565\u5e76\u81ea\u52a8\u8c03\u8bd5 2) \u5728Python\u8c03\u8bd5\u9636\u6bb5\u589e\u52a0\u53cd\u601d\u6b65\u9aa4\u5b9a\u4f4d\u5931\u8d25\u539f\u56e0 3) \u751f\u6210\u591a\u4e2a\u7a0b\u5e8f\u53d8\u4f53\u5e76\u9009\u62e9\u6700\u4f73\u65b9\u6848", "result": "\u572817\u4e2a\u57fa\u51c6\u57df\u4e0a\u6d4b\u8bd5\uff0c\u65b0\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u89c4\u5212\u8d28\u91cf\u4e14\u4ece\u4e0d\u964d\u4f4e\u6027\u80fd\u300212\u4e2a\u57df\u4e2d\u7684\u6700\u4f73\u7a0b\u5e8f\u80fd\u89e3\u51b3\u8be5\u57df\u751f\u6210\u5668\u4ea7\u751f\u7684\u6240\u6709\u4efb\u52a1\u3002", "conclusion": "\u901a\u8fc7\u4f2a\u4ee3\u7801\u8c03\u8bd5\u3001\u53cd\u601d\u673a\u5236\u548c\u591a\u7a0b\u5e8f\u9009\u62e9\uff0c\u6709\u6548\u63d0\u9ad8\u4e86LLM\u751f\u6210\u5e7f\u4e49\u89c4\u5212\u7684\u53ef\u9760\u6027\u548c\u6210\u529f\u7387\u3002"}}
{"id": "2508.13915", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13915", "abs": "https://arxiv.org/abs/2508.13915", "authors": ["Yihao Ang", "Yifan Bao", "Lei Jiang", "Jiajie Tao", "Anthony K. H. Tung", "Lukasz Szpruch", "Hao Ni"], "title": "Structured Agentic Workflows for Financial Time-Series Modeling with LLMs and Reflective Feedback", "comment": null, "summary": "Time-series data is central to decision-making in financial markets, yet\nbuilding high-performing, interpretable, and auditable models remains a major\nchallenge. While Automated Machine Learning (AutoML) frameworks streamline\nmodel development, they often lack adaptability and responsiveness to\ndomain-specific needs and evolving objectives. Concurrently, Large Language\nModels (LLMs) have enabled agentic systems capable of reasoning, memory\nmanagement, and dynamic code generation, offering a path toward more flexible\nworkflow automation. In this paper, we introduce \\textsf{TS-Agent}, a modular\nagentic framework designed to automate and enhance time-series modeling\nworkflows for financial applications. The agent formalizes the pipeline as a\nstructured, iterative decision process across three stages: model selection,\ncode refinement, and fine-tuning, guided by contextual reasoning and\nexperimental feedback. Central to our architecture is a planner agent equipped\nwith structured knowledge banks, curated libraries of models and refinement\nstrategies, which guide exploration, while improving interpretability and\nreducing error propagation. \\textsf{TS-Agent} supports adaptive learning,\nrobust debugging, and transparent auditing, key requirements for high-stakes\nenvironments such as financial services. Empirical evaluations on diverse\nfinancial forecasting and synthetic data generation tasks demonstrate that\n\\textsf{TS-Agent} consistently outperforms state-of-the-art AutoML and agentic\nbaselines, achieving superior accuracy, robustness, and decision traceability.", "AI": {"tldr": "TS-Agent\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u4ee3\u7406\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u91d1\u878d\u65f6\u95f4\u5e8f\u5217\u5efa\u6a21\u5de5\u4f5c\u6d41\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u51b3\u7b56\u8fc7\u7a0b\uff08\u6a21\u578b\u9009\u62e9\u3001\u4ee3\u7801\u4f18\u5316\u3001\u5fae\u8c03\uff09\u5b9e\u73b0\u4f18\u4e8e\u73b0\u6709AutoML\u548c\u4ee3\u7406\u57fa\u51c6\u7684\u6027\u80fd\u3002", "motivation": "\u91d1\u878d\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u5efa\u6a21\u9762\u4e34\u9ad8\u7ee9\u6548\u3001\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u5ba1\u8ba1\u6027\u7684\u6311\u6218\uff0c\u73b0\u6709AutoML\u6846\u67b6\u7f3a\u4e4f\u5bf9\u9886\u57df\u7279\u5b9a\u9700\u6c42\u548c\u52a8\u6001\u76ee\u6807\u7684\u9002\u5e94\u6027\uff0c\u800cLLM\u4ee3\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u7684\u5de5\u4f5c\u6d41\u81ea\u52a8\u5316\u8def\u5f84\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e09\u9636\u6bb5\u7ed3\u6784\u5316\u8fed\u4ee3\u51b3\u7b56\u6d41\u7a0b\uff1a\u6a21\u578b\u9009\u62e9\u3001\u4ee3\u7801\u4f18\u5316\u548c\u5fae\u8c03\uff0c\u914d\u5907\u7ed3\u6784\u5316\u77e5\u8bc6\u5e93\u548c\u7cbe\u9009\u6a21\u578b\u5e93\u7684\u89c4\u5212\u4ee3\u7406\u6307\u5bfc\u63a2\u7d22\uff0c\u63d0\u9ad8\u53ef\u89e3\u91ca\u6027\u5e76\u51cf\u5c11\u9519\u8bef\u4f20\u64ad\u3002", "result": "\u5728\u591a\u6837\u5316\u91d1\u878d\u9884\u6d4b\u548c\u5408\u6210\u6570\u636e\u751f\u6210\u4efb\u52a1\u4e0a\u7684\u5b9e\u8bc1\u8bc4\u4f30\u8868\u660e\uff0cTS-Agent\u5728\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u548c\u51b3\u7b56\u53ef\u8ffd\u6eaf\u6027\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684AutoML\u548c\u4ee3\u7406\u57fa\u51c6\u3002", "conclusion": "TS-Agent\u652f\u6301\u81ea\u9002\u5e94\u5b66\u4e60\u3001\u9c81\u68d2\u8c03\u8bd5\u548c\u900f\u660e\u5ba1\u8ba1\uff0c\u6ee1\u8db3\u4e86\u91d1\u878d\u670d\u52a1\u7b49\u9ad8\u98ce\u9669\u73af\u5883\u7684\u5173\u952e\u8981\u6c42\uff0c\u4e3a\u65f6\u95f4\u5e8f\u5217\u5efa\u6a21\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.13942", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13942", "abs": "https://arxiv.org/abs/2508.13942", "authors": ["Soumyadeep Dhar"], "title": "The Collaboration Paradox: Why Generative AI Requires Both Strategic Intelligence and Operational Stability in Supply Chain Management", "comment": null, "summary": "The rise of autonomous, AI-driven agents in economic settings raises critical\nquestions about their emergent strategic behavior. This paper investigates\nthese dynamics in the cooperative context of a multi-echelon supply chain, a\nsystem famously prone to instabilities like the bullwhip effect. We conduct\ncomputational experiments with generative AI agents, powered by Large Language\nModels (LLMs), within a controlled supply chain simulation designed to isolate\ntheir behavioral tendencies. Our central finding is the \"collaboration\nparadox\": a novel, catastrophic failure mode where theoretically superior\ncollaborative AI agents, designed with Vendor-Managed Inventory (VMI)\nprinciples, perform even worse than non-AI baselines. We demonstrate that this\nparadox arises from an operational flaw where agents hoard inventory, starving\nthe system. We then show that resilience is only achieved through a synthesis\nof two distinct layers: high-level, AI-driven proactive policy-setting to\nestablish robust operational targets, and a low-level, collaborative execution\nprotocol with proactive downstream replenishment to maintain stability. Our\nfinal framework, which implements this synthesis, can autonomously generate,\nevaluate, and quantify a portfolio of viable strategic choices. The work\nprovides a crucial insight into the emergent behaviors of collaborative AI\nagents and offers a blueprint for designing stable, effective AI-driven systems\nfor business analytics.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0AI\u9a71\u52a8\u7684\u534f\u4f5c\u4ee3\u7406\u5728\u4f9b\u5e94\u94fe\u4e2d\u4f1a\u51fa\u73b0\"\u5408\u4f5c\u6096\u8bba\"\uff0c\u5373\u7406\u8bba\u4e0a\u66f4\u4f18\u7684\u534f\u4f5cAI\u4ee3\u7406\u6bd4\u975eAI\u57fa\u51c6\u8868\u73b0\u66f4\u5dee\uff0c\u539f\u56e0\u662f\u5e93\u5b58\u56e4\u79ef\u5bfc\u81f4\u7cfb\u7edf\u762b\u75ea\u3002", "motivation": "\u7814\u7a76AI\u4ee3\u7406\u5728\u7ecf\u6d4e\u73af\u5883\u4e2d\u7684\u6d8c\u73b0\u6218\u7565\u884c\u4e3a\uff0c\u7279\u522b\u662f\u5728\u591a\u7ea7\u4f9b\u5e94\u94fe\u8fd9\u79cd\u5bb9\u6613\u51fa\u73b0\u725b\u97ad\u6548\u5e94\u7b49\u4e0d\u7a33\u5b9a\u6027\u7684\u5408\u4f5c\u73af\u5883\u4e2d\u3002", "method": "\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u751f\u6210\u5f0fAI\u4ee3\u7406\u5728\u53d7\u63a7\u4f9b\u5e94\u94fe\u6a21\u62df\u4e2d\u8fdb\u884c\u8ba1\u7b97\u5b9e\u9a8c\uff0c\u8bbe\u8ba1\u5305\u542b\u4f9b\u5e94\u5546\u7ba1\u7406\u5e93\u5b58\u539f\u5219\u7684\u534f\u4f5cAI\u4ee3\u7406\u3002", "result": "\u53d1\u73b0\u534f\u4f5c\u6096\u8bba\u73b0\u8c61\uff0cAI\u4ee3\u7406\u4f1a\u56e4\u79ef\u5e93\u5b58\u5bfc\u81f4\u7cfb\u7edf\u762b\u75ea\uff1b\u6700\u7ec8\u5f00\u53d1\u51fa\u7ed3\u5408\u9ad8\u5c42AI\u9a71\u52a8\u7b56\u7565\u5236\u5b9a\u548c\u4f4e\u5c42\u534f\u4f5c\u6267\u884c\u534f\u8bae\u7684\u53cc\u5c42\u6846\u67b6\u3002", "conclusion": "\u63ed\u793a\u4e86\u534f\u4f5cAI\u4ee3\u7406\u7684\u6d8c\u73b0\u884c\u4e3a\u7279\u5f81\uff0c\u4e3a\u8bbe\u8ba1\u7a33\u5b9a\u6709\u6548\u7684\u5546\u4e1a\u5206\u6790AI\u9a71\u52a8\u7cfb\u7edf\u63d0\u4f9b\u4e86\u84dd\u56fe\uff0c\u5f3a\u8c03\u9700\u8981\u5206\u5c42\u67b6\u6784\u6765\u5b9e\u73b0\u5f39\u6027\u3002"}}
{"id": "2508.13975", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.13975", "abs": "https://arxiv.org/abs/2508.13975", "authors": ["Jingquan Wang", "Andrew Negrut", "Harry Zhang", "Khailanii Slaton", "Shu Wang", "Radu Serban", "Jinlong Wu", "Dan Negrut"], "title": "ChronoLLM: Customizing Language Models for Physics-Based Simulation Code Generation", "comment": null, "summary": "This contribution is concerned with the following issue: can pretrained large\nlanguage models (LLMs) be refined and customized to the point where they become\nvirtual assistants helping experts with the effective use of a simulation tool?\nIn this case study, the ``simulation tool'' considered is PyChrono, an open\nsource multi-physics dynamics engine for multibody systems. We present a\nframework for refining and customizing both open- and closed-source LLMs to\nharness the power of AI in generating scripts that perform PyChrono virtual\nexperiments. We refine and customize several classes of LLMs through a process\nthat leads to a quantifiable improvement in the quality of the generated\nPyChrono simulation scripts. These scripts can range from simple\nsingle-pendulum simulations to complex virtual experiments involving full\nvehicles on deformable terrain. While the generated scripts are rarely perfect,\nthey often serve as strong starting points for the user to modify and improve\non. Additionally, the LLM can answer specific API questions about the\nsimulator, or recommend modeling approaches. The framework discussed is general\nand can be applied to lower the entry barrier for simulation tools associated\nwith other application domains.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u5982\u4f55\u901a\u8fc7\u7cbe\u8c03\u5927\u8bed\u8a00\u6a21\u578b(LLMs)\u6765\u521b\u5efa\u865a\u62df\u52a9\u624b\uff0c\u5e2e\u52a9\u4e13\u5bb6\u4f7f\u7528PyChrono\u591a\u7269\u7406\u52a8\u529b\u5b66\u4eff\u771f\u5de5\u5177\u751f\u6210\u4eff\u771f\u811a\u672c\uff0c\u964d\u4f4e\u4eff\u771f\u5de5\u5177\u7684\u4f7f\u7528\u95e8\u69db\u3002", "motivation": "\u63a2\u7d22\u9884\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u5426\u901a\u8fc7\u7cbe\u8c03\u548c\u5b9a\u5236\uff0c\u6210\u4e3a\u5e2e\u52a9\u4e13\u5bb6\u6709\u6548\u4f7f\u7528\u4eff\u771f\u5de5\u5177\u7684\u865a\u62df\u52a9\u624b\uff0c\u7279\u522b\u662f\u9488\u5bf9PyChrono\u591a\u4f53\u7cfb\u7edf\u52a8\u529b\u5b66\u4eff\u771f\u5de5\u5177\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6846\u67b6\u6765\u7cbe\u8c03\u548c\u5b9a\u5236\u5f00\u6e90\u53ca\u95ed\u6e90LLMs\uff0c\u901a\u8fc7\u7279\u5b9a\u6d41\u7a0b\u63d0\u5347\u751f\u6210PyChrono\u4eff\u771f\u811a\u672c\u7684\u8d28\u91cf\uff0c\u4ece\u7b80\u5355\u5355\u6446\u4eff\u771f\u5230\u590d\u6742\u8f66\u8f86\u5728\u53ef\u53d8\u5f62\u5730\u5f62\u4e0a\u7684\u865a\u62df\u5b9e\u9a8c\u3002", "result": "\u7cbe\u8c03\u540e\u7684LLMs\u5728\u751f\u6210PyChrono\u4eff\u771f\u811a\u672c\u8d28\u91cf\u4e0a\u6709\u91cf\u5316\u63d0\u5347\uff0c\u867d\u7136\u751f\u6210\u7684\u811a\u672c\u5f88\u5c11\u5b8c\u7f8e\uff0c\u4f46\u901a\u5e38\u80fd\u4f5c\u4e3a\u7528\u6237\u4fee\u6539\u548c\u6539\u8fdb\u7684\u826f\u597d\u8d77\u70b9\uff0c\u8fd8\u80fd\u56de\u7b54\u7279\u5b9aAPI\u95ee\u9898\u548c\u63a8\u8350\u5efa\u6a21\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u5177\u6709\u901a\u7528\u6027\uff0c\u53ef\u5e94\u7528\u4e8e\u964d\u4f4e\u5176\u4ed6\u5e94\u7528\u9886\u57df\u76f8\u5173\u4eff\u771f\u5de5\u5177\u7684\u4f7f\u7528\u95e8\u69db\uff0c\u8bc1\u660e\u4e86LLMs\u5728\u8f85\u52a9\u4e13\u5bb6\u4f7f\u7528\u4e13\u4e1a\u4eff\u771f\u5de5\u5177\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.14020", "categories": ["cs.AI", "cs.DM", "68T01", "I.2.8"], "pdf": "https://arxiv.org/pdf/2508.14020", "abs": "https://arxiv.org/abs/2508.14020", "authors": ["Christian Blum", "Pedro Pinacho-Davidson"], "title": "A Biased Random Key Genetic Algorithm for Solving the Longest Run Subsequence Problem", "comment": null, "summary": "The longest run subsequence (LRS) problem is an NP-hard combinatorial\noptimization problem belonging to the class of subsequence problems from\nbioinformatics. In particular, the problem plays a role in genome reassembly.\nIn this paper, we present a solution to the LRS problem using a Biased Random\nKey Genetic Algorithm (BRKGA). Our approach places particular focus on the\ncomputational efficiency of evaluating individuals, which involves converting\nvectors of gray values into valid solutions to the problem. For comparison\npurposes, a Max-Min Ant System is developed and implemented. This is in\naddition to the application of the integer linear programming solver CPLEX for\nsolving all considered problem instances. The computation results show that the\nproposed BRKGA is currently a state-of-the-art technique for the LRS problem.\nNevertheless, the results also show that there is room for improvement,\nespecially in the context of input strings based on large alphabet sizes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u504f\u7f6e\u968f\u673a\u5bc6\u94a5\u9057\u4f20\u7b97\u6cd5\uff08BRKGA\uff09\u89e3\u51b3\u6700\u957f\u8fd0\u884c\u5b50\u5e8f\u5217\uff08LRS\uff09\u95ee\u9898\u7684\u65b9\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5728\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u662f\u76ee\u524dLRS\u95ee\u9898\u7684\u6700\u5148\u8fdb\u6280\u672f\u3002", "motivation": "LRS\u95ee\u9898\u662f\u4e00\u4e2aNP\u96be\u7684\u7ec4\u5408\u4f18\u5316\u95ee\u9898\uff0c\u5728\u751f\u7269\u4fe1\u606f\u5b66\u4e2d\u7684\u57fa\u56e0\u7ec4\u91cd\u7ec4\u88c5\u4e2d\u5177\u6709\u91cd\u8981\u4f5c\u7528\uff0c\u9700\u8981\u9ad8\u6548\u7684\u6c42\u89e3\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u504f\u7f6e\u968f\u673a\u5bc6\u94a5\u9057\u4f20\u7b97\u6cd5\uff08BRKGA\uff09\uff0c\u91cd\u70b9\u4f18\u5316\u4e2a\u4f53\u8bc4\u4f30\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u5c06\u7070\u5ea6\u503c\u5411\u91cf\u8f6c\u6362\u4e3a\u6709\u6548\u89e3\u3002\u540c\u65f6\u5f00\u53d1\u4e86\u6700\u5927\u6700\u5c0f\u8682\u8681\u7cfb\u7edf\u548cCPLEX\u6c42\u89e3\u5668\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u8ba1\u7b97\u7ed3\u679c\u8868\u660e\uff0c\u63d0\u51fa\u7684BRKGA\u7b97\u6cd5\u5728LRS\u95ee\u9898\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u662f\u76ee\u524d\u6700\u5148\u8fdb\u7684\u6280\u672f\u3002", "conclusion": "\u867d\u7136BRKGA\u5728LRS\u95ee\u9898\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u57fa\u4e8e\u5927\u5b57\u6bcd\u8868\u7684\u8f93\u5165\u5b57\u7b26\u4e32\u65b9\u9762\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u3002"}}
{"id": "2508.14040", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.14040", "abs": "https://arxiv.org/abs/2508.14040", "authors": ["Hanyu Lai", "Xiao Liu", "Yanxiao Zhao", "Han Xu", "Hanchen Zhang", "Bohao Jing", "Yanyu Ren", "Shuntian Yao", "Yuxiao Dong", "Jie Tang"], "title": "ComputerRL: Scaling End-to-End Online Reinforcement Learning for Computer Use Agents", "comment": null, "summary": "We introduce ComputerRL, a framework for autonomous desktop intelligence that\nenables agents to operate complex digital workspaces skillfully. ComputerRL\nfeatures the API-GUI paradigm, which unifies programmatic API calls and direct\nGUI interaction to address the inherent mismatch between machine agents and\nhuman-centric desktop environments. Scaling end-to-end RL training is crucial\nfor improvement and generalization across diverse desktop tasks, yet remains\nchallenging due to environmental inefficiency and instability in extended\ntraining. To support scalable and robust training, we develop a distributed RL\ninfrastructure capable of orchestrating thousands of parallel virtual desktop\nenvironments to accelerate large-scale online RL. Furthermore, we propose\nEntropulse, a training strategy that alternates reinforcement learning with\nsupervised fine-tuning, effectively mitigating entropy collapse during extended\ntraining runs. We employ ComputerRL on open models GLM-4-9B-0414 and\nQwen2.5-14B, and evaluate them on the OSWorld benchmark. The AutoGLM-OS-9B\nbased on GLM-4-9B-0414 achieves a new state-of-the-art accuracy of 48.1%,\ndemonstrating significant improvements for general agents in desktop\nautomation. The algorithm and framework are adopted in building AutoGLM (Liu et\nal., 2024a)", "AI": {"tldr": "ComputerRL\u662f\u4e00\u4e2a\u81ea\u4e3b\u684c\u9762\u667a\u80fd\u6846\u67b6\uff0c\u901a\u8fc7API-GUI\u8303\u5f0f\u7edf\u4e00\u7a0b\u5e8f\u5316API\u8c03\u7528\u548c\u76f4\u63a5GUI\u4ea4\u4e92\uff0c\u89e3\u51b3\u673a\u5668\u4ee3\u7406\u4e0e\u4eba\u7c7b\u4e2d\u5fc3\u684c\u9762\u73af\u5883\u7684\u4e0d\u5339\u914d\u95ee\u9898\u3002\u91c7\u7528\u5206\u5e03\u5f0fRL\u57fa\u7840\u8bbe\u65bd\u548cEntropulse\u8bad\u7ec3\u7b56\u7565\uff0c\u5728OSWorld\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523048.1%\u7684\u65b0SOTA\u51c6\u786e\u7387\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4ee3\u7406\u5728\u4eba\u7c7b\u4e2d\u5fc3\u684c\u9762\u73af\u5883\u4e2d\u64cd\u4f5c\u590d\u6742\u6570\u5b57\u5de5\u4f5c\u7a7a\u95f4\u65f6\u7684\u56fa\u6709\u5931\u914d\u95ee\u9898\uff0c\u63d0\u5347\u684c\u9762\u81ea\u52a8\u5316\u7684\u901a\u7528\u6027\u548c\u6548\u7387\u3002", "method": "\u63d0\u51faAPI-GUI\u8303\u5f0f\u7edf\u4e00\u7a0b\u5e8f\u5316API\u548cGUI\u4ea4\u4e92\uff1b\u5f00\u53d1\u5206\u5e03\u5f0fRL\u57fa\u7840\u8bbe\u65bd\u652f\u6301\u5927\u89c4\u6a21\u5e76\u884c\u865a\u62df\u684c\u9762\u73af\u5883\u8bad\u7ec3\uff1b\u8bbe\u8ba1Entropulse\u8bad\u7ec3\u7b56\u7565\u4ea4\u66ff\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u548c\u76d1\u7763\u5fae\u8c03\u6765\u7f13\u89e3\u71b5\u5d29\u6e83\u95ee\u9898\u3002", "result": "\u5728OSWorld\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u57fa\u4e8eGLM-4-9B-0414\u7684AutoGLM-OS-9B\u6a21\u578b\u8fbe\u523048.1%\u7684state-of-the-art\u51c6\u786e\u7387\uff0c\u663e\u8457\u63d0\u5347\u4e86\u684c\u9762\u81ea\u52a8\u5316\u901a\u7528\u4ee3\u7406\u7684\u6027\u80fd\u3002", "conclusion": "ComputerRL\u6846\u67b6\u901a\u8fc7\u521b\u65b0\u7684API-GUI\u8303\u5f0f\u548c\u53ef\u6269\u5c55\u7684\u8bad\u7ec3\u57fa\u7840\u8bbe\u65bd\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u684c\u9762\u667a\u80fd\u4ee3\u7406\u7684\u9ad8\u6548\u8bad\u7ec3\u548c\u4f18\u5f02\u6027\u80fd\uff0c\u4e3a\u81ea\u4e3b\u684c\u9762\u81ea\u52a8\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
