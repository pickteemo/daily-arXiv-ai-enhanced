{"id": "2508.06559", "categories": ["cs.AI", "cs.GT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06559", "abs": "https://arxiv.org/abs/2508.06559", "authors": ["Sina Baghal"], "title": "Solving Pasur Using GPU-Accelerated Counterfactual Regret Minimization", "comment": null, "summary": "Pasur is a fishing card game played over six rounds and is played similarly\nto games such as Cassino and Scopa, and Bastra. This paper introduces a\nCUDA-accelerated computational framework for simulating Pasur, emphasizing\nefficient memory management. We use our framework to compute near-Nash\nequilibria via Counterfactual Regret Minimization (CFR), a well-known algorithm\nfor solving large imperfect-information games.\n  Solving Pasur presents unique challenges due to its intricate rules and the\nlarge size of its game tree. We handle rule complexity using PyTorch CUDA\ntensors and to address the memory-intensive nature of the game, we decompose\nthe game tree into two key components: (1) actual game states, and (2)\ninherited scores from previous rounds. We construct the Full Game Tree by\npairing card states with accumulated scores in the Unfolding Process. This\ndesign reduces memory overhead by storing only essential strategy values and\nnode connections. To further manage computational complexity, we apply a\nround-by-round backward training strategy, starting from the final round and\nrecursively propagating average utilities to earlier stages. Our approach\nconstructs the complete game tree, which on average consists of over $10^9$\nnodes. We provide detailed implementation snippets.\n  After computing a near-Nash equilibrium strategy, we train a tree-based model\nto predict these strategies for use during gameplay. We then estimate the fair\nvalue of each deck through large-scale self-play between equilibrium strategies\nby simulating, for instance, 10,000 games per matchup, executed in parallel\nusing GPU acceleration.\n  Similar frameworks can be extended to other reinforcement learning algorithms\nwhere the action tree naturally decomposes into multiple rounds such as\nturn-based strategy games or sequential trading decisions in financial markets.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u57fa\u4e8eCUDA\u52a0\u901f\u7684Pasur\u7eb8\u724c\u6e38\u620f\u8ba1\u7b97\u6846\u67b6\uff0c\u901a\u8fc7\u9ad8\u6548\u5185\u5b58\u7ba1\u7406\u548cCFR\u7b97\u6cd5\u6c42\u89e3\u8fd1\u7eb3\u4ec0\u5747\u8861\uff0c\u5e76\u8bad\u7ec3\u6811\u6a21\u578b\u7528\u4e8e\u5b9e\u9645\u6e38\u620f\u3002", "motivation": "Pasur\u6e38\u620f\u7684\u590d\u6742\u89c4\u5219\u548c\u5927\u89c4\u6a21\u6e38\u620f\u6811\u5e26\u6765\u4e86\u72ec\u7279\u7684\u8ba1\u7b97\u6311\u6218\uff0c\u9700\u8981\u9ad8\u6548\u7684\u5185\u5b58\u7ba1\u7406\u548c\u8ba1\u7b97\u4f18\u5316\u3002", "method": "\u4f7f\u7528PyTorch CUDA\u5f20\u91cf\u5904\u7406\u89c4\u5219\u590d\u6742\u6027\uff0c\u901a\u8fc7\u5206\u89e3\u6e38\u620f\u6811\u548c\u8f6e\u6b21\u9006\u5411\u8bad\u7ec3\u7b56\u7565\u964d\u4f4e\u5185\u5b58\u5f00\u9500\uff0c\u6784\u5efa\u5305\u542b\u8d85\u8fc710^9\u8282\u70b9\u7684\u5b8c\u6574\u6e38\u620f\u6811\u3002", "result": "\u6210\u529f\u8ba1\u7b97\u4e86\u8fd1\u7eb3\u4ec0\u5747\u8861\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u5927\u89c4\u6a21\u81ea\u5bf9\u5f08\u4f30\u8ba1\u4e86\u6bcf\u526f\u724c\u7684\u516c\u5e73\u4ef7\u503c\u3002", "conclusion": "\u8be5\u6846\u67b6\u53ef\u6269\u5c55\u81f3\u5176\u4ed6\u591a\u8f6e\u6b21\u5f3a\u5316\u5b66\u4e60\u573a\u666f\uff0c\u5982\u56de\u5408\u5236\u7b56\u7565\u6e38\u620f\u6216\u91d1\u878d\u5e02\u573a\u987a\u5e8f\u4ea4\u6613\u51b3\u7b56\u3002"}}
{"id": "2508.06569", "categories": ["cs.AI", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.06569", "abs": "https://arxiv.org/abs/2508.06569", "authors": ["Lance Yao", "Suman Samantray", "Ayana Ghosh", "Kevin Roccapriore", "Libor Kovarik", "Sarah Allec", "Maxim Ziatdinov"], "title": "Operationalizing Serendipity: Multi-Agent AI Workflows for Enhanced Materials Characterization with Theory-in-the-Loop", "comment": null, "summary": "The history of science is punctuated by serendipitous discoveries, where\nunexpected observations, rather than targeted hypotheses, opened new fields of\ninquiry. While modern autonomous laboratories excel at accelerating hypothesis\ntesting, their optimization for efficiency risks overlooking these crucial,\nunplanned findings. To address this gap, we introduce SciLink, an open-source,\nmulti-agent artificial intelligence framework designed to operationalize\nserendipity in materials research by creating a direct, automated link between\nexperimental observation, novelty assessment, and theoretical simulations. The\nframework employs a hybrid AI strategy where specialized machine learning\nmodels perform quantitative analysis of experimental data, while large language\nmodels handle higher-level reasoning. These agents autonomously convert raw\ndata from materials characterization techniques into falsifiable scientific\nclaims, which are then quantitatively scored for novelty against the published\nliterature. We demonstrate the framework's versatility across diverse research\nscenarios, showcasing its application to atomic-resolution and hyperspectral\ndata, its capacity to integrate real-time human expert guidance, and its\nability to close the research loop by proposing targeted follow-up experiments.\nBy systematically analyzing all observations and contextualizing them, SciLink\nprovides a practical framework for AI-driven materials research that not only\nenhances efficiency but also actively cultivates an environment ripe for\nserendipitous discoveries, thereby bridging the gap between automated\nexperimentation and open-ended scientific exploration.", "AI": {"tldr": "SciLink\u662f\u4e00\u4e2a\u5f00\u6e90\u591a\u667a\u80fd\u4f53AI\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u81ea\u52a8\u5316\u94fe\u63a5\u5b9e\u9a8c\u89c2\u5bdf\u3001\u65b0\u9896\u6027\u8bc4\u4f30\u548c\u7406\u8bba\u6a21\u62df\uff0c\u4fc3\u8fdb\u6750\u6599\u7814\u7a76\u4e2d\u7684\u610f\u5916\u53d1\u73b0\u3002", "motivation": "\u73b0\u4ee3\u81ea\u4e3b\u5b9e\u9a8c\u5ba4\u867d\u7136\u80fd\u9ad8\u6548\u9a8c\u8bc1\u5047\u8bbe\uff0c\u4f46\u53ef\u80fd\u5ffd\u7565\u610f\u5916\u53d1\u73b0\u3002SciLink\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u901a\u8fc7AI\u6846\u67b6\u7cfb\u7edf\u5316\u5730\u6355\u6349\u548c\u5206\u6790\u610f\u5916\u89c2\u5bdf\u3002", "method": "\u91c7\u7528\u6df7\u5408AI\u7b56\u7565\uff0c\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u6a21\u578b\u8fdb\u884c\u5b9a\u91cf\u5206\u6790\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u9ad8\u5c42\u63a8\u7406\uff0c\u5c06\u539f\u59cb\u6570\u636e\u8f6c\u5316\u4e3a\u53ef\u9a8c\u8bc1\u7684\u79d1\u5b66\u4e3b\u5f20\uff0c\u5e76\u6839\u636e\u6587\u732e\u8bc4\u4f30\u65b0\u9896\u6027\u3002", "result": "SciLink\u5728\u591a\u79cd\u7814\u7a76\u573a\u666f\u4e2d\u5c55\u793a\u4e86\u5176\u591a\u529f\u80fd\u6027\uff0c\u5305\u62ec\u539f\u5b50\u5206\u8fa8\u7387\u548c\u8d85\u5149\u8c31\u6570\u636e\u5904\u7406\u3001\u5b9e\u65f6\u6574\u5408\u4e13\u5bb6\u6307\u5bfc\u4ee5\u53ca\u63d0\u51fa\u9488\u5bf9\u6027\u540e\u7eed\u5b9e\u9a8c\u3002", "conclusion": "SciLink\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u7814\u7a76\u6548\u7387\uff0c\u8fd8\u901a\u8fc7\u7cfb\u7edf\u5316\u5206\u6790\u6240\u6709\u89c2\u5bdf\u7ed3\u679c\uff0c\u4e3a\u610f\u5916\u53d1\u73b0\u521b\u9020\u4e86\u6709\u5229\u73af\u5883\uff0c\u5f25\u5408\u4e86\u81ea\u52a8\u5316\u5b9e\u9a8c\u4e0e\u5f00\u653e\u5f0f\u79d1\u5b66\u63a2\u7d22\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2508.06571", "categories": ["cs.AI", "cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06571", "abs": "https://arxiv.org/abs/2508.06571", "authors": ["Anqing Jiang", "Yu Gao", "Yiru Wang", "Zhigang Sun", "Shuo Wang", "Yuwen Heng", "Hao Sun", "Shichen Tang", "Lijuan Zhu", "Jinhao Chai", "Jijun Wang", "Zichong Gu", "Hao Jiang", "Li Sun"], "title": "IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model", "comment": "9 pagres, 2 figures", "summary": "Vision-Language-Action (VLA) models have demonstrated potential in autonomous\ndriving. However, two critical challenges hinder their development: (1)\nExisting VLA architectures are typically based on imitation learning in\nopen-loop setup which tends to capture the recorded behaviors in the dataset,\nleading to suboptimal and constrained performance, (2) Close-loop training\nrelies heavily on high-fidelity sensor simulation, where domain gaps and\ncomputational inefficiencies pose significant barriers. In this paper, we\nintroduce IRL-VLA, a novel close-loop Reinforcement Learning via\n\\textbf{I}nverse \\textbf{R}einforcement \\textbf{L}earning reward world model\nwith a self-built VLA approach. Our framework proceeds in a three-stage\nparadigm: In the first stage, we propose a VLA architecture and pretrain the\nVLA policy via imitation learning. In the second stage, we construct a\nlightweight reward world model via inverse reinforcement learning to enable\nefficient close-loop reward computation. To further enhance planning\nperformance, finally, we design specialized reward world model guidence\nreinforcement learning via PPO(Proximal Policy Optimization) to effectively\nbalance the safety incidents, comfortable driving, and traffic efficiency. Our\napproach achieves state-of-the-art performance in NAVSIM v2 end-to-end driving\nbenchmark, 1st runner up in CVPR2025 Autonomous Grand Challenge. We hope that\nour framework will accelerate VLA research in close-loop autonomous driving.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86IRL-VLA\u6846\u67b6\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u65b9\u6cd5\u89e3\u51b3VLA\u6a21\u578b\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u95ed\u73af\u8bad\u7ec3\u95ee\u9898\uff0c\u7ed3\u5408\u6a21\u4eff\u5b66\u4e60\u548c\u9006\u5f3a\u5316\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709VLA\u6a21\u578b\u5728\u5f00\u73af\u6a21\u4eff\u5b66\u4e60\u4e2d\u8868\u73b0\u53d7\u9650\uff0c\u95ed\u73af\u8bad\u7ec3\u4f9d\u8d56\u9ad8\u4fdd\u771f\u6a21\u62df\u5668\u4e14\u6548\u7387\u4f4e\uff0c\u4e9f\u9700\u65b0\u65b9\u6cd5\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "1. \u6a21\u4eff\u5b66\u4e60\u9884\u8bad\u7ec3VLA\u7b56\u7565\uff1b2. \u9006\u5f3a\u5316\u5b66\u4e60\u6784\u5efa\u8f7b\u91cf\u7ea7\u5956\u52b1\u4e16\u754c\u6a21\u578b\uff1b3. PPO\u4f18\u5316\u5956\u52b1\u6a21\u578b\u6307\u5bfc\u7684\u5f3a\u5316\u5b66\u4e60\u3002", "result": "\u5728NAVSIM v2\u548cCVPR2025\u7ade\u8d5b\u4e2d\u53d6\u5f97\u9886\u5148\u6210\u7ee9\u3002", "conclusion": "IRL-VLA\u6846\u67b6\u4e3a\u95ed\u73af\u81ea\u52a8\u9a7e\u9a76\u7684VLA\u7814\u7a76\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.06518", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06518", "abs": "https://arxiv.org/abs/2508.06518", "authors": ["Ray Wai Man Kong"], "title": "Automated Seam Folding and Sewing Machine on Pleated Pants for Apparel Manufacturing", "comment": "13 pages, 9 figures", "summary": "The applied research is the design and development of an automated folding\nand sewing machine for pleated pants. It represents a significant advancement\nin addressing the challenges associated with manual sewing processes.\nTraditional methods for creating pleats are labour-intensive, prone to\ninconsistencies, and require high levels of skill, making automation a critical\nneed in the apparel industry. This research explores the technical feasibility\nand operational benefits of integrating advanced technologies into garment\nproduction, focusing on the creation of an automated machine capable of precise\nfolding and sewing operations and eliminating the marking operation.\n  The proposed machine incorporates key features such as a precision folding\nmechanism integrated into the automated sewing unit with real-time monitoring\ncapabilities. The results demonstrate remarkable improvements: the standard\nlabour time has been reduced by 93%, dropping from 117 seconds per piece to\njust 8 seconds with the automated system. Similarly, machinery time improved by\n73%, and the total output rate increased by 72%. These enhancements translate\ninto a cycle time reduction from 117 seconds per piece to an impressive 33\nseconds, enabling manufacturers to meet customer demand more swiftly. By\neliminating manual marking processes, the machine not only reduces labour costs\nbut also minimizes waste through consistent pleat formation. This automation\naligns with industry trends toward sustainability and efficiency, potentially\nreducing environmental impact by decreasing material waste and energy\nconsumption.", "AI": {"tldr": "\u7814\u7a76\u8bbe\u8ba1\u4e86\u4e00\u6b3e\u81ea\u52a8\u5316\u6298\u53e0\u4e0e\u7f1d\u7eab\u673a\uff0c\u7528\u4e8e\u8936\u88e5\u88e4\u7684\u751f\u4ea7\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6548\u7387\u5e76\u51cf\u5c11\u4e86\u4eba\u5de5\u64cd\u4f5c\u7684\u9700\u6c42\u3002", "motivation": "\u4f20\u7edf\u8936\u88e5\u5236\u4f5c\u65b9\u6cd5\u52b3\u52a8\u5bc6\u96c6\u3001\u6613\u51fa\u9519\u4e14\u4f9d\u8d56\u9ad8\u6280\u80fd\u5de5\u4eba\uff0c\u4e9f\u9700\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u6b3e\u96c6\u6210\u7cbe\u5bc6\u6298\u53e0\u673a\u5236\u548c\u5b9e\u65f6\u76d1\u63a7\u529f\u80fd\u7684\u81ea\u52a8\u5316\u7f1d\u7eab\u673a\uff0c\u6d88\u9664\u4e86\u6807\u8bb0\u6b65\u9aa4\u3002", "result": "\u52b3\u52a8\u529b\u65f6\u95f4\u51cf\u5c1193%\uff0c\u673a\u5668\u65f6\u95f4\u63d0\u534773%\uff0c\u603b\u4ea7\u51fa\u7387\u589e\u52a072%\uff0c\u5468\u671f\u65f6\u95f4\u4ece117\u79d2\u964d\u81f333\u79d2\u3002", "conclusion": "\u8be5\u81ea\u52a8\u5316\u673a\u5668\u4e0d\u4ec5\u964d\u4f4e\u6210\u672c\u4e0e\u6d6a\u8d39\uff0c\u8fd8\u7b26\u5408\u53ef\u6301\u7eed\u6027\u4e0e\u6548\u7387\u7684\u884c\u4e1a\u8d8b\u52bf\u3002"}}
{"id": "2508.06585", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06585", "abs": "https://arxiv.org/abs/2508.06585", "authors": ["Jayant Sravan Tamarapalli", "Rynaa Grover", "Nilay Pande", "Sahiti Yerramilli"], "title": "CountQA: How Well Do MLLMs Count in the Wild?", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) demonstrate remarkable fluency in\nunderstanding visual scenes, yet they exhibit a critical lack in a fundamental\ncognitive skill: object counting. This blind spot severely limits their\nreliability in real-world applications. To date, this capability has been\nlargely unevaluated in complex scenarios, as existing benchmarks either feature\nsparse object densities or are confined to specific visual domains, failing to\ntest models under realistic conditions. Addressing this gap, we introduce\nCountQA, a challenging new benchmark designed to probe this deficiency.\nComprising over 1,500 question-answer pairs, CountQA features real-world images\nwith high object density, clutter, and occlusion. We investigate this weakness\nby evaluating 15 prominent MLLMs on the CountQA benchmark and reveal that the\ntop-performing model achieves a mere 42.9% accuracy, with performance declining\nas object counts rise. By providing a dedicated benchmark to diagnose and\nrectify this core weakness, CountQA paves the way for a new generation of MLLMs\nthat are not only descriptively fluent but also numerically grounded and\nspatially aware. We will open-source the dataset and code upon paper acceptance\nto foster further research.", "AI": {"tldr": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u89c6\u89c9\u573a\u666f\u7406\u89e3\u4e0a\u8868\u73b0\u6d41\u7545\uff0c\u4f46\u5728\u5bf9\u8c61\u8ba1\u6570\u80fd\u529b\u4e0a\u5b58\u5728\u663e\u8457\u7f3a\u9677\u3002CountQA\u662f\u4e00\u4e2a\u65b0\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u63d0\u5347MLLMs\u7684\u8ba1\u6570\u80fd\u529b\u3002", "motivation": "\u73b0\u6709MLLMs\u5728\u5bf9\u8c61\u8ba1\u6570\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u4e14\u7f3a\u4e4f\u590d\u6742\u573a\u666f\u7684\u8bc4\u4f30\u57fa\u51c6\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u53ef\u9760\u6027\u3002", "method": "\u5f15\u5165CountQA\u57fa\u51c6\uff0c\u5305\u542b1,500\u591a\u4e2a\u95ee\u9898-\u7b54\u6848\u5bf9\uff0c\u8986\u76d6\u9ad8\u5bc6\u5ea6\u3001\u6742\u4e71\u548c\u906e\u6321\u7684\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u3002\u8bc4\u4f30\u4e8615\u79cd\u4e3b\u6d41MLLMs\u3002", "result": "\u8868\u73b0\u6700\u4f73\u7684\u6a21\u578b\u51c6\u786e\u7387\u4ec5\u4e3a42.9%\uff0c\u4e14\u968f\u7740\u5bf9\u8c61\u6570\u91cf\u589e\u52a0\u6027\u80fd\u4e0b\u964d\u3002", "conclusion": "CountQA\u4e3a\u8bca\u65ad\u548c\u89e3\u51b3MLLMs\u8ba1\u6570\u7f3a\u9677\u63d0\u4f9b\u4e86\u5de5\u5177\uff0c\u63a8\u52a8\u672a\u6765\u6a21\u578b\u5728\u6570\u503c\u548c\u7a7a\u95f4\u611f\u77e5\u4e0a\u7684\u8fdb\u6b65\u3002"}}
{"id": "2508.06520", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.06520", "abs": "https://arxiv.org/abs/2508.06520", "authors": ["Liwei Chen", "Tong Qin", "Zhenhua Huangfu", "Li Li", "Wei Wei"], "title": "Optimization of Flip-Landing Trajectories for Starship based on a Deep Learned Simulator", "comment": null, "summary": "We propose a differentiable optimization framework for flip-and-landing\ntrajectory design of reusable spacecraft, exemplified by the Starship vehicle.\nA deep neural network surrogate, trained on high-fidelity CFD data, predicts\naerodynamic forces and moments, and is tightly coupled with a differentiable\nrigid-body dynamics solver. This enables end-to-end gradient-based trajectory\noptimization without linearization or convex relaxation. The framework handles\nactuator limits and terminal landing constraints, producing physically\nconsistent, optimized control sequences. Both standard automatic\ndifferentiation and Neural ODEs are applied to support long-horizon rollouts.\nResults demonstrate the framework's effectiveness in modeling and optimizing\ncomplex maneuvers with high nonlinearities. This work lays the groundwork for\nfuture extensions involving unsteady aerodynamics, plume interactions, and\nintelligent guidance design.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u5fae\u5206\u4f18\u5316\u7684\u53ef\u91cd\u590d\u4f7f\u7528\u822a\u5929\u5668\u7ffb\u8f6c\u4e0e\u7740\u9646\u8f68\u8ff9\u8bbe\u8ba1\u6846\u67b6\uff0c\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u4e0e\u52a8\u529b\u5b66\u6c42\u89e3\u5668\uff0c\u5b9e\u73b0\u7aef\u5230\u7aef\u68af\u5ea6\u4f18\u5316\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u8f68\u8ff9\u4f18\u5316\u65b9\u6cd5\u4e2d\u56e0\u7ebf\u6027\u5316\u6216\u51f8\u677e\u5f1b\u5bfc\u81f4\u7684\u7269\u7406\u4e00\u81f4\u6027\u4e0d\u8db3\u95ee\u9898\uff0c\u63d0\u5347\u590d\u6742\u975e\u7ebf\u6027\u673a\u52a8\u4efb\u52a1\u7684\u5efa\u6a21\u4e0e\u4f18\u5316\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4ee3\u7406\u6a21\u578b\u9884\u6d4b\u6c14\u52a8\u529b\u4e0e\u529b\u77e9\uff0c\u7ed3\u5408\u53ef\u5fae\u5206\u521a\u4f53\u52a8\u529b\u5b66\u6c42\u89e3\u5668\uff0c\u652f\u6301\u7aef\u5230\u7aef\u68af\u5ea6\u4f18\u5316\u3002\u540c\u65f6\u5904\u7406\u6267\u884c\u5668\u9650\u5236\u4e0e\u7ec8\u7aef\u7740\u9646\u7ea6\u675f\u3002", "result": "\u6846\u67b6\u6210\u529f\u5efa\u6a21\u5e76\u4f18\u5316\u4e86\u9ad8\u975e\u7ebf\u6027\u590d\u6742\u673a\u52a8\u4efb\u52a1\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u4e3a\u672a\u6765\u6d89\u53ca\u975e\u5b9a\u5e38\u6c14\u52a8\u529b\u5b66\u3001\u7fbd\u6d41\u4ea4\u4e92\u548c\u667a\u80fd\u5236\u5bfc\u8bbe\u8ba1\u7684\u6269\u5c55\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.06668", "categories": ["cs.AI", "cs.IR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.06668", "abs": "https://arxiv.org/abs/2508.06668", "authors": ["Jessie Galasso"], "title": "Formal Concept Analysis: a Structural Framework for Variability Extraction and Analysis", "comment": null, "summary": "Formal Concept Analysis (FCA) is a mathematical framework for knowledge\nrepresentation and discovery. It performs a hierarchical clustering over a set\nof objects described by attributes, resulting in conceptual structures in which\nobjects are organized depending on the attributes they share. These conceptual\nstructures naturally highlight commonalities and variabilities among similar\nobjects by categorizing them into groups which are then arranged by similarity,\nmaking it particularly appropriate for variability extraction and analysis.\nDespite the potential of FCA, determining which of its properties can be\nleveraged for variability-related tasks (and how) is not always\nstraightforward, partly due to the mathematical orientation of its foundational\nliterature. This paper attempts to bridge part of this gap by gathering a\nselection of properties of the framework which are essential to variability\nanalysis, and how they can be used to interpret diverse variability information\nwithin the resulting conceptual structures.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5f62\u5f0f\u6982\u5ff5\u5206\u6790\uff08FCA\uff09\u5728\u53d8\u5f02\u6027\u5206\u6790\u4e2d\u7684\u5173\u952e\u5c5e\u6027\u53ca\u5176\u5e94\u7528\u65b9\u6cd5\u3002", "motivation": "FCA\u5728\u77e5\u8bc6\u8868\u793a\u548c\u53d1\u73b0\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u5176\u6570\u5b66\u57fa\u7840\u6587\u732e\u4f7f\u5176\u5728\u53d8\u5f02\u6027\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u4e0d\u591f\u76f4\u89c2\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u7b5b\u9009FCA\u6846\u67b6\u4e2d\u5bf9\u53d8\u5f02\u6027\u5206\u6790\u81f3\u5173\u91cd\u8981\u7684\u5c5e\u6027\uff0c\u5e76\u89e3\u91ca\u5176\u5728\u6982\u5ff5\u7ed3\u6784\u4e2d\u7684\u53d8\u5f02\u6027\u4fe1\u606f\u3002", "result": "\u660e\u786e\u4e86FCA\u4e2d\u53ef\u7528\u4e8e\u53d8\u5f02\u6027\u5206\u6790\u7684\u5173\u952e\u5c5e\u6027\u53ca\u5176\u5e94\u7528\u65b9\u5f0f\u3002", "conclusion": "FCA\u7684\u5c5e\u6027\u53ef\u7528\u4e8e\u6709\u6548\u89e3\u91ca\u548c\u5206\u6790\u53d8\u5f02\u6027\u4fe1\u606f\uff0c\u4e3a\u76f8\u5173\u4efb\u52a1\u63d0\u4f9b\u652f\u6301\u3002"}}
{"id": "2508.06521", "categories": ["cs.RO", "68T40, 93C85, 70E60"], "pdf": "https://arxiv.org/pdf/2508.06521", "abs": "https://arxiv.org/abs/2508.06521", "authors": ["H. Liu", "L. S. Moreu", "T. S. Andersen", "V. V. Puche", "M. Fumagalli"], "title": "Stinger Robot: A Self-Bracing Robotic Platform for Autonomous Drilling in Confined Underground Environments", "comment": "7 pages, submitted", "summary": "The increasing demand for critical raw materials has revitalized interest in\nabandoned underground mines, which pose extreme challenges for conventional\ndrilling machinery due to confined, unstructured, and infrastructure-less\nenvironments. This paper presents the Stinger Robot, a novel compact robotic\nplatform specifically designed for autonomous high-force drilling in such\nsettings. The robot features a mechanically self-locking tri-leg bracing\nmechanism that enables stable anchoring to irregular tunnel surfaces. A key\ninnovation lies in its force-aware, closed-loop control strategy, which enables\nforce interaction with unstructured environments during bracing and drilling.\nImplemented as a finite-state machine in ROS 2, the control policy dynamically\nadapts leg deployment based on real-time contact feedback and load thresholds,\nensuring stability without external supports. We demonstrate, through\nsimulation and preliminary hardware tests, that the Stinger Robot can\nautonomously stabilize and drill in conditions previously inaccessible to\nnowadays mining machines. This work constitutes the first validated robotic\narchitecture to integrate distributed force-bracing and autonomous drilling in\nunderground environments, laying the groundwork for future collaborative mining\noperations using modular robot systems.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aStinger Robot\u7684\u65b0\u578b\u7d27\u51d1\u673a\u5668\u4eba\u5e73\u53f0\uff0c\u4e13\u4e3a\u5728\u5e9f\u5f03\u5730\u4e0b\u77ff\u5c71\u7b49\u6781\u7aef\u73af\u5883\u4e2d\u8fdb\u884c\u81ea\u4e3b\u9ad8\u529b\u94bb\u5b54\u800c\u8bbe\u8ba1\u3002", "motivation": "\u7531\u4e8e\u5e9f\u5f03\u5730\u4e0b\u77ff\u5c71\u73af\u5883\u72ed\u7a84\u3001\u65e0\u7ed3\u6784\u4e14\u7f3a\u4e4f\u57fa\u7840\u8bbe\u65bd\uff0c\u4f20\u7edf\u94bb\u63a2\u673a\u68b0\u96be\u4ee5\u5e94\u5bf9\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u65b0\u578b\u673a\u5668\u4eba\u5e73\u53f0\u3002", "method": "\u673a\u5668\u4eba\u91c7\u7528\u673a\u68b0\u81ea\u9501\u4e09\u817f\u652f\u6491\u673a\u5236\uff0c\u7ed3\u5408\u529b\u611f\u77e5\u95ed\u73af\u63a7\u5236\u7b56\u7565\uff0c\u901a\u8fc7ROS 2\u4e2d\u7684\u6709\u9650\u72b6\u6001\u673a\u52a8\u6001\u8c03\u6574\u817f\u90e8\u90e8\u7f72\u3002", "result": "\u4eff\u771f\u548c\u521d\u6b65\u786c\u4ef6\u6d4b\u8bd5\u8868\u660e\uff0c\u8be5\u673a\u5668\u4eba\u80fd\u5728\u4f20\u7edf\u91c7\u77ff\u673a\u68b0\u65e0\u6cd5\u5de5\u4f5c\u7684\u73af\u5883\u4e2d\u81ea\u4e3b\u7a33\u5b9a\u94bb\u5b54\u3002", "conclusion": "\u8be5\u7814\u7a76\u9996\u6b21\u9a8c\u8bc1\u4e86\u5206\u5e03\u5f0f\u529b\u652f\u6491\u4e0e\u81ea\u4e3b\u94bb\u5b54\u7ed3\u5408\u7684\u673a\u5668\u4eba\u67b6\u6784\uff0c\u4e3a\u672a\u6765\u6a21\u5757\u5316\u673a\u5668\u4eba\u534f\u4f5c\u91c7\u77ff\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.06674", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06674", "abs": "https://arxiv.org/abs/2508.06674", "authors": ["Weijie Shi", "Yue Cui", "Hao Chen", "Jiaming Li", "Mengze Li", "Jia Zhu", "Jiajie Xu", "Xiaofang Zhou"], "title": "Zero-Shot Cellular Trajectory Map Matching", "comment": null, "summary": "Cellular Trajectory Map-Matching (CTMM) aims to align cellular location\nsequences to road networks, which is a necessary preprocessing in\nlocation-based services on web platforms like Google Maps, including navigation\nand route optimization. Current approaches mainly rely on ID-based features and\nregion-specific data to learn correlations between cell towers and roads,\nlimiting their adaptability to unexplored areas. To enable high-accuracy CTMM\nwithout additional training in target regions, Zero-shot CTMM requires to\nextract not only region-adaptive features, but also sequential and location\nuncertainty to alleviate positioning errors in cellular data. In this paper, we\npropose a pixel-based trajectory calibration assistant for zero-shot CTMM,\nwhich takes advantage of transferable geospatial knowledge to calibrate\npixelated trajectory, and then guide the path-finding process at the road\nnetwork level. To enhance knowledge sharing across similar regions, a Gaussian\nmixture model is incorporated into VAE, enabling the identification of\nscenario-adaptive experts through soft clustering. To mitigate high positioning\nerrors, a spatial-temporal awareness module is designed to capture sequential\nfeatures and location uncertainty, thereby facilitating the inference of\napproximate user positions. Finally, a constrained path-finding algorithm is\nemployed to reconstruct the road ID sequence, ensuring topological validity\nwithin the road network. This process is guided by the calibrated trajectory\nwhile optimizing for the shortest feasible path, thus minimizing unnecessary\ndetours. Extensive experiments demonstrate that our model outperforms existing\nmethods in zero-shot CTMM by 16.8\\%.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u50cf\u7d20\u7684\u8f68\u8ff9\u6821\u51c6\u8f85\u52a9\u65b9\u6cd5\uff0c\u7528\u4e8e\u96f6\u6837\u672c\u8702\u7a9d\u8f68\u8ff9\u5730\u56fe\u5339\u914d\uff08CTMM\uff09\uff0c\u901a\u8fc7\u8fc1\u79fb\u5730\u7406\u7a7a\u95f4\u77e5\u8bc6\u6821\u51c6\u8f68\u8ff9\uff0c\u5e76\u7ed3\u5408\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u548c\u65f6\u7a7a\u611f\u77e5\u6a21\u5757\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709CTMM\u65b9\u6cd5\u4f9d\u8d56\u533a\u57df\u7279\u5b9a\u6570\u636e\u548cID\u7279\u5f81\uff0c\u96be\u4ee5\u9002\u5e94\u672a\u63a2\u7d22\u533a\u57df\uff0c\u9700\u5f00\u53d1\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u7684\u96f6\u6837\u672c\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408\u50cf\u7d20\u8f68\u8ff9\u6821\u51c6\u3001\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u7684VAE\u3001\u65f6\u7a7a\u611f\u77e5\u6a21\u5757\u548c\u7ea6\u675f\u8def\u5f84\u67e5\u627e\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6a21\u578b\u5728\u96f6\u6837\u672cCTMM\u4e2d\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd516.8%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u77e5\u8bc6\u8fc1\u79fb\u548c\u8bef\u5dee\u7f13\u89e3\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u96f6\u6837\u672cCTMM\u3002"}}
{"id": "2508.06534", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06534", "abs": "https://arxiv.org/abs/2508.06534", "authors": ["Aishan Liu", "Jiakai Wang", "Tianyuan Zhang", "Hainan Li", "Jiangfan Liu", "Siyuan Liang", "Yilong Ren", "Xianglong Liu", "Dacheng Tao"], "title": "MetAdv: A Unified and Interactive Adversarial Testing Platform for Autonomous Driving", "comment": "Accepted by ACM MM 2025 Demo/Videos track", "summary": "Evaluating and ensuring the adversarial robustness of autonomous driving (AD)\nsystems is a critical and unresolved challenge. This paper introduces MetAdv, a\nnovel adversarial testing platform that enables realistic, dynamic, and\ninteractive evaluation by tightly integrating virtual simulation with physical\nvehicle feedback. At its core, MetAdv establishes a hybrid virtual-physical\nsandbox, within which we design a three-layer closed-loop testing environment\nwith dynamic adversarial test evolution. This architecture facilitates\nend-to-end adversarial evaluation, ranging from high-level unified adversarial\ngeneration, through mid-level simulation-based interaction, to low-level\nexecution on physical vehicles. Additionally, MetAdv supports a broad spectrum\nof AD tasks, algorithmic paradigms (e.g., modular deep learning pipelines,\nend-to-end learning, vision-language models). It supports flexible 3D vehicle\nmodeling and seamless transitions between simulated and physical environments,\nwith built-in compatibility for commercial platforms such as Apollo and Tesla.\nA key feature of MetAdv is its human-in-the-loop capability: besides flexible\nenvironmental configuration for more customized evaluation, it enables\nreal-time capture of physiological signals and behavioral feedback from\ndrivers, offering new insights into human-machine trust under adversarial\nconditions. We believe MetAdv can offer a scalable and unified framework for\nadversarial assessment, paving the way for safer AD.", "AI": {"tldr": "MetAdv\u662f\u4e00\u4e2a\u65b0\u578b\u5bf9\u6297\u6d4b\u8bd5\u5e73\u53f0\uff0c\u901a\u8fc7\u865a\u62df\u4eff\u771f\u4e0e\u7269\u7406\u8f66\u8f86\u53cd\u9988\u7684\u7d27\u5bc6\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u52a8\u6001\u3001\u4ea4\u4e92\u5f0f\u5bf9\u6297\u8bc4\u4f30\u3002", "motivation": "\u8bc4\u4f30\u548c\u786e\u4fdd\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u5bf9\u6297\u9c81\u68d2\u6027\u662f\u4e00\u4e2a\u5173\u952e\u4e14\u672a\u89e3\u51b3\u7684\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u73b0\u5b9e\u3001\u52a8\u6001\u7684\u6d4b\u8bd5\u65b9\u6cd5\u3002", "method": "MetAdv\u6784\u5efa\u4e86\u4e00\u4e2a\u6df7\u5408\u865a\u62df-\u7269\u7406\u6c99\u76d2\uff0c\u8bbe\u8ba1\u4e86\u4e09\u5c42\u95ed\u73af\u6d4b\u8bd5\u73af\u5883\uff0c\u652f\u6301\u4ece\u9ad8\u5c42\u5bf9\u6297\u751f\u6210\u5230\u4f4e\u5c42\u7269\u7406\u8f66\u8f86\u6267\u884c\u7684\u7aef\u5230\u7aef\u8bc4\u4f30\u3002", "result": "\u5e73\u53f0\u652f\u6301\u591a\u79cd\u81ea\u52a8\u9a7e\u9a76\u4efb\u52a1\u548c\u7b97\u6cd5\u8303\u5f0f\uff0c\u517c\u5bb9\u5546\u4e1a\u5e73\u53f0\uff0c\u5e76\u5177\u5907\u4eba\u673a\u4ea4\u4e92\u80fd\u529b\uff0c\u53ef\u5b9e\u65f6\u6355\u83b7\u9a7e\u9a76\u5458\u751f\u7406\u4fe1\u53f7\u548c\u884c\u4e3a\u53cd\u9988\u3002", "conclusion": "MetAdv\u4e3a\u5bf9\u6297\u8bc4\u4f30\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u66f4\u5b89\u5168\u7684\u81ea\u52a8\u9a7e\u9a76\u53d1\u5c55\u3002"}}
{"id": "2508.06706", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2508.06706", "abs": "https://arxiv.org/abs/2508.06706", "authors": ["Jaikrishna Manojkumar Patil", "Nathaniel Lee", "Al Mehdi Saadat Chowdhury", "YooJung Choi", "Paulo Shakarian"], "title": "Probabilistic Circuits for Knowledge Graph Completion with Reduced Rule Sets", "comment": null, "summary": "Rule-based methods for knowledge graph completion provide explainable results\nbut often require a significantly large number of rules to achieve competitive\nperformance. This can hinder explainability due to overwhelmingly large rule\nsets. We discover rule contexts (meaningful subsets of rules that work\ntogether) from training data and use learned probability distribution (i.e.\nprobabilistic circuits) over these rule contexts to more rapidly achieve\nperformance of the full rule set. Our approach achieves a 70-96% reduction in\nnumber of rules used while outperforming baseline by up to 31$\\times$ when\nusing equivalent minimal number of rules and preserves 91% of peak baseline\nperformance even when comparing our minimal rule sets against baseline's full\nrule sets. We show that our framework is grounded in well-known semantics of\nprobabilistic logic, does not require independence assumptions, and that our\ntractable inference procedure provides both approximate lower bounds and exact\nprobability of a given query. The efficacy of our method is validated by\nempirical studies on 8 standard benchmark datasets where we show competitive\nperformance by using only a fraction of the rules required by AnyBURL's\nstandard inference method, the current state-of-the-art for rule-based\nknowledge graph completion. This work may have further implications for general\nprobabilistic reasoning over learned sets of rules.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c4\u5219\u4e0a\u4e0b\u6587\u548c\u6982\u7387\u7535\u8def\u7684\u65b9\u6cd5\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u6240\u9700\u7684\u89c4\u5219\u6570\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6027\u80fd\u3002", "motivation": "\u89c4\u5219\u65b9\u6cd5\u867d\u7136\u53ef\u89e3\u91ca\uff0c\u4f46\u9700\u8981\u5927\u91cf\u89c4\u5219\u624d\u80fd\u8fbe\u5230\u9ad8\u6027\u80fd\uff0c\u5bfc\u81f4\u89c4\u5219\u96c6\u8fc7\u5927\u53cd\u800c\u5f71\u54cd\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u4ece\u8bad\u7ec3\u6570\u636e\u4e2d\u53d1\u73b0\u89c4\u5219\u4e0a\u4e0b\u6587\uff0c\u5e76\u5229\u7528\u6982\u7387\u7535\u8def\u5b66\u4e60\u5176\u5206\u5e03\uff0c\u5feb\u901f\u5b9e\u73b0\u5168\u89c4\u5219\u96c6\u7684\u6027\u80fd\u3002", "result": "\u89c4\u5219\u6570\u91cf\u51cf\u5c1170-96%\uff0c\u6027\u80fd\u63d0\u534731\u500d\uff0c\u4fdd\u7559\u57fa\u7ebf91%\u7684\u5cf0\u503c\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u57288\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u6709\u6548\uff0c\u4e3a\u57fa\u4e8e\u89c4\u5219\u7684\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2508.06538", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.06538", "abs": "https://arxiv.org/abs/2508.06538", "authors": ["Gioele Buriani", "Jingyue Liu", "Maximilian St\u00f6lzle", "Cosimo Della Santina", "Jiatao Ding"], "title": "Symbolic Learning of Interpretable Reduced-Order Models for Jumping Quadruped Robots", "comment": "8 pages, under review", "summary": "Reduced-order models are essential for motion planning and control of\nquadruped robots, as they simplify complex dynamics while preserving critical\nbehaviors. This paper introduces a novel methodology for deriving such\ninterpretable dynamic models, specifically for jumping. We capture the\nhigh-dimensional, nonlinear jumping dynamics in a low-dimensional latent space\nby proposing a learning architecture combining Sparse Identification of\nNonlinear Dynamics (SINDy) with physical structural priors on the jump\ndynamics. Our approach demonstrates superior accuracy to the traditional\nactuated Spring-loaded Inverted Pendulum (aSLIP) model and is validated through\nsimulation and hardware experiments across different jumping strategies.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408SINDy\u4e0e\u7269\u7406\u7ed3\u6784\u5148\u9a8c\u7684\u5b66\u4e60\u67b6\u6784\uff0c\u7528\u4e8e\u6784\u5efa\u56db\u8db3\u673a\u5668\u4eba\u8df3\u8dc3\u7684\u53ef\u89e3\u91ca\u964d\u9636\u6a21\u578b\uff0c\u5176\u7cbe\u5ea6\u4f18\u4e8e\u4f20\u7edfaSLIP\u6a21\u578b\u3002", "motivation": "\u964d\u9636\u6a21\u578b\u5bf9\u56db\u8db3\u673a\u5668\u4eba\u7684\u8fd0\u52a8\u89c4\u5212\u4e0e\u63a7\u5236\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9700\u5728\u7b80\u5316\u590d\u6742\u52a8\u529b\u5b66\u7684\u540c\u65f6\u4fdd\u7559\u5173\u952e\u884c\u4e3a\u3002", "method": "\u7ed3\u5408Sparse Identification of Nonlinear Dynamics (SINDy)\u4e0e\u8df3\u8dc3\u52a8\u529b\u5b66\u7684\u7269\u7406\u7ed3\u6784\u5148\u9a8c\uff0c\u6784\u5efa\u4f4e\u7ef4\u6f5c\u5728\u7a7a\u95f4\u6a21\u578b\u3002", "result": "\u65b0\u65b9\u6cd5\u5728\u4eff\u771f\u548c\u786c\u4ef6\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u4f18\u4e8e\u4f20\u7edfaSLIP\u6a21\u578b\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u56db\u8db3\u673a\u5668\u4eba\u8df3\u8dc3\u63d0\u4f9b\u4e86\u66f4\u7cbe\u786e\u7684\u53ef\u89e3\u91ca\u964d\u9636\u6a21\u578b\u3002"}}
{"id": "2508.06716", "categories": ["cs.AI", "cs.LG", "cs.LO"], "pdf": "https://arxiv.org/pdf/2508.06716", "abs": "https://arxiv.org/abs/2508.06716", "authors": ["Blair Johnson", "Clayton Kerce", "Faramarz Fekri"], "title": "GLIDR: Graph-Like Inductive Logic Programming with Differentiable Reasoning", "comment": null, "summary": "Differentiable inductive logic programming (ILP) techniques have proven\neffective at finding approximate rule-based solutions to link prediction and\nnode classification problems on knowledge graphs; however, the common\nassumption of chain-like rule structure can hamper the performance and\ninterpretability of existing approaches. We introduce GLIDR, a differentiable\nrule learning method that models the inference of logic rules with more\nexpressive syntax than previous methods. GLIDR uses a differentiable message\npassing inference algorithm that generalizes previous chain-like rule learning\nmethods to allow rules with features like branches and cycles. GLIDR has a\nsimple and expressive rule search space which is parameterized by a limit on\nthe maximum number of free variables that may be included in a rule. Explicit\nlogic rules can be extracted from the weights of a GLIDR model for use with\nsymbolic solvers. We demonstrate that GLIDR can significantly outperform\nexisting rule learning methods on knowledge graph completion tasks and even\ncompete with embedding methods despite the inherent disadvantage of being a\nstructure-only prediction method. We show that rules extracted from GLIDR\nretain significant predictive performance, and that GLIDR is highly robust to\ntraining data noise. Finally, we demonstrate that GLIDR can be chained with\ndeep neural networks and optimized end-to-end for rule learning on arbitrary\ndata modalities.", "AI": {"tldr": "GLIDR\u662f\u4e00\u79cd\u53ef\u5fae\u5206\u7684\u89c4\u5219\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u66f4\u4e30\u5bcc\u7684\u8bed\u6cd5\u7ed3\u6784\uff08\u5982\u5206\u652f\u548c\u5faa\u73af\uff09\u6539\u8fdb\u77e5\u8bc6\u56fe\u8c31\u4efb\u52a1\u4e2d\u7684\u89c4\u5219\u5b66\u4e60\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u94fe\u5f0f\u89c4\u5219\u7ed3\u6784\u9650\u5236\u4e86\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\uff0cGLIDR\u65e8\u5728\u901a\u8fc7\u66f4\u7075\u6d3b\u7684\u89c4\u5219\u8bed\u6cd5\u63d0\u5347\u6548\u679c\u3002", "method": "GLIDR\u4f7f\u7528\u53ef\u5fae\u5206\u6d88\u606f\u4f20\u9012\u7b97\u6cd5\uff0c\u652f\u6301\u590d\u6742\u89c4\u5219\u7ed3\u6784\uff0c\u5e76\u901a\u8fc7\u81ea\u7531\u53d8\u91cf\u6570\u91cf\u9650\u5236\u89c4\u5219\u641c\u7d22\u7a7a\u95f4\u3002", "result": "GLIDR\u5728\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u89c4\u5219\u5b66\u4e60\u65b9\u6cd5\uff0c\u751a\u81f3\u53ef\u4e0e\u5d4c\u5165\u65b9\u6cd5\u7ade\u4e89\uff0c\u4e14\u5bf9\u566a\u58f0\u6570\u636e\u9c81\u68d2\u3002", "conclusion": "GLIDR\u4e0d\u4ec5\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\uff0c\u8fd8\u80fd\u4e0e\u6df1\u5ea6\u5b66\u4e60\u7ed3\u5408\u8fdb\u884c\u7aef\u5230\u7aef\u4f18\u5316\uff0c\u9002\u7528\u4e8e\u591a\u6a21\u6001\u6570\u636e\u3002"}}
{"id": "2508.06547", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06547", "abs": "https://arxiv.org/abs/2508.06547", "authors": ["Heran Wu", "Zirun Zhou", "Jingfeng Zhang"], "title": "A tutorial note on collecting simulated data for vision-language-action models", "comment": "This is a tutorial note for educational purposes", "summary": "Traditional robotic systems typically decompose intelligence into independent\nmodules for computer vision, natural language processing, and motion control.\nVision-Language-Action (VLA) models fundamentally transform this approach by\nemploying a single neural network that can simultaneously process visual\nobservations, understand human instructions, and directly output robot actions\n-- all within a unified framework. However, these systems are highly dependent\non high-quality training datasets that can capture the complex relationships\nbetween visual observations, language instructions, and robotic actions. This\ntutorial reviews three representative systems: the PyBullet simulation\nframework for flexible customized data generation, the LIBERO benchmark suite\nfor standardized task definition and evaluation, and the RT-X dataset\ncollection for large-scale multi-robot data acquisition. We demonstrated\ndataset generation approaches in PyBullet simulation and customized data\ncollection within LIBERO, and provide an overview of the characteristics and\nroles of the RT-X dataset for large-scale multi-robot data acquisition.", "AI": {"tldr": "VLA\u6a21\u578b\u901a\u8fc7\u5355\u4e00\u795e\u7ecf\u7f51\u7edc\u7edf\u4e00\u5904\u7406\u89c6\u89c9\u3001\u8bed\u8a00\u548c\u52a8\u4f5c\uff0c\u4f9d\u8d56\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u3002\u6559\u7a0b\u56de\u987e\u4e86PyBullet\u3001LIBERO\u548cRT-X\u4e09\u79cd\u4ee3\u8868\u6027\u7cfb\u7edf\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u4eba\u7cfb\u7edf\u5c06\u667a\u80fd\u5206\u89e3\u4e3a\u72ec\u7acb\u6a21\u5757\uff0cVLA\u6a21\u578b\u901a\u8fc7\u7edf\u4e00\u6846\u67b6\u63d0\u5347\u6548\u7387\uff0c\u4f46\u9700\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u652f\u6301\u3002", "method": "\u4f7f\u7528PyBullet\u751f\u6210\u5b9a\u5236\u6570\u636e\uff0cLIBERO\u5b9a\u4e49\u6807\u51c6\u5316\u4efb\u52a1\uff0cRT-X\u6536\u96c6\u5927\u89c4\u6a21\u591a\u673a\u5668\u4eba\u6570\u636e\u3002", "result": "\u5c55\u793a\u4e86PyBullet\u4e2d\u7684\u6570\u636e\u751f\u6210\u65b9\u6cd5\u548cLIBERO\u4e2d\u7684\u5b9a\u5236\u6570\u636e\u6536\u96c6\uff0c\u6982\u8ff0\u4e86RT-X\u7684\u7279\u70b9\u548c\u4f5c\u7528\u3002", "conclusion": "VLA\u6a21\u578b\u4f9d\u8d56\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff0cPyBullet\u3001LIBERO\u548cRT-X\u4e3a\u6570\u636e\u751f\u6210\u548c\u6536\u96c6\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\u3002"}}
{"id": "2508.06736", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06736", "abs": "https://arxiv.org/abs/2508.06736", "authors": ["Alican Yilmaz", "Junyang Cai", "Serdar Kadioglu", "Bistra Dilkina"], "title": "ParBalans: Parallel Multi-Armed Bandits-based Adaptive Large Neighborhood Search", "comment": null, "summary": "Solving Mixed-Integer Programming (MIP) problems often requires substantial\ncomputational resources due to their combinatorial nature. Parallelization has\nemerged as a critical strategy to accelerate solution times and enhance\nscalability to tackle large, complex instances. This paper investigates the\nparallelization capabilities of Balans, a recently proposed multi-armed\nbandits-based adaptive large neighborhood search for MIPs. While Balans's\nmodular architecture inherently supports parallel exploration of diverse\nparameter configurations, this potential has not been thoroughly examined. To\naddress this gap, we introduce ParBalans, an extension that leverages both\nsolver-level and algorithmic-level parallelism to improve performance on\nchallenging MIP instances. Our experimental results demonstrate that ParBalans\nexhibits competitive performance compared to the state-of-the-art commercial\nsolver Gurobi, particularly on hard optimization benchmarks.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86Balans\u7684\u5e76\u884c\u5316\u80fd\u529b\uff0c\u63d0\u51faParBalans\uff0c\u7ed3\u5408\u6c42\u89e3\u5668\u548c\u7b97\u6cd5\u7ea7\u5e76\u884c\uff0c\u663e\u8457\u63d0\u5347MIP\u6c42\u89e3\u6027\u80fd\u3002", "motivation": "\u6df7\u5408\u6574\u6570\u89c4\u5212\uff08MIP\uff09\u95ee\u9898\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u9ad8\uff0c\u5e76\u884c\u5316\u662f\u52a0\u901f\u6c42\u89e3\u7684\u5173\u952e\u7b56\u7565\u3002Balans\u7684\u5e76\u884c\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u6269\u5c55Balans\u4e3aParBalans\uff0c\u5229\u7528\u6c42\u89e3\u5668\u548c\u7b97\u6cd5\u7ea7\u5e76\u884c\uff0c\u4f18\u5316\u590d\u6742MIP\u5b9e\u4f8b\u7684\u6c42\u89e3\u3002", "result": "\u5b9e\u9a8c\u663e\u793aParBalans\u5728\u786c\u4f18\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u5546\u4e1a\u6c42\u89e3\u5668Gurobi\u3002", "conclusion": "ParBalans\u901a\u8fc7\u5e76\u884c\u5316\u663e\u8457\u63d0\u5347MIP\u6c42\u89e3\u6548\u7387\uff0c\u5177\u6709\u7ade\u4e89\u529b\u3002"}}
{"id": "2508.06554", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06554", "abs": "https://arxiv.org/abs/2508.06554", "authors": ["Abdelhaleem Saad", "Waseem Akram", "Irfan Hussain"], "title": "AquaChat++: LLM-Assisted Multi-ROV Inspection for Aquaculture Net Pens with Integrated Battery Management and Thruster Fault Tolerance", "comment": null, "summary": "Inspection of aquaculture net pens is essential for ensuring the structural\nintegrity and sustainable operation of offshore fish farming systems.\nTraditional methods, typically based on manually operated or single-ROV\nsystems, offer limited adaptability to real-time constraints such as energy\nconsumption, hardware faults, and dynamic underwater conditions. This paper\nintroduces AquaChat++, a novel multi-ROV inspection framework that uses Large\nLanguage Models (LLMs) to enable adaptive mission planning, coordinated task\nexecution, and fault-tolerant control in complex aquaculture environments. The\nproposed system consists of a two-layered architecture. The high-level plan\ngeneration layer employs an LLM, such as ChatGPT-4, to translate natural\nlanguage user commands into symbolic, multi-agent inspection plans. A task\nmanager dynamically allocates and schedules actions among ROVs based on their\nreal-time status and operational constraints, including thruster faults and\nbattery levels. The low-level control layer ensures accurate trajectory\ntracking and integrates thruster fault detection and compensation mechanisms.\nBy incorporating real-time feedback and event-triggered replanning, AquaChat++\nenhances system robustness and operational efficiency. Simulated experiments in\na physics-based aquaculture environment demonstrate improved inspection\ncoverage, energy-efficient behavior, and resilience to actuator failures. These\nfindings highlight the potential of LLM-driven frameworks to support scalable,\nintelligent, and autonomous underwater robotic operations within the\naquaculture sector.", "AI": {"tldr": "AquaChat++\u662f\u4e00\u4e2a\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u591aROV\u68c0\u67e5\u6846\u67b6\uff0c\u7528\u4e8e\u6c34\u4ea7\u517b\u6b96\u7f51\u7bb1\u7684\u81ea\u9002\u5e94\u4efb\u52a1\u89c4\u5212\u548c\u5bb9\u9519\u63a7\u5236\u3002", "motivation": "\u4f20\u7edf\u6c34\u4ea7\u517b\u6b96\u7f51\u7bb1\u68c0\u67e5\u65b9\u6cd5\u9002\u5e94\u6027\u5dee\uff0c\u65e0\u6cd5\u5e94\u5bf9\u5b9e\u65f6\u7ea6\u675f\uff08\u5982\u80fd\u8017\u3001\u786c\u4ef6\u6545\u969c\u548c\u52a8\u6001\u6c34\u4e0b\u73af\u5883\uff09\u3002", "method": "\u91c7\u7528\u4e24\u5c42\u67b6\u6784\uff1a\u9ad8\u5c42\u4f7f\u7528LLM\uff08\u5982ChatGPT-4\uff09\u5c06\u81ea\u7136\u8bed\u8a00\u547d\u4ee4\u8f6c\u5316\u4e3a\u591a\u4ee3\u7406\u68c0\u67e5\u8ba1\u5212\uff1b\u4f4e\u5c42\u5b9e\u73b0\u8f68\u8ff9\u8ddf\u8e2a\u548c\u6545\u969c\u68c0\u6d4b\u8865\u507f\u3002", "result": "\u6a21\u62df\u5b9e\u9a8c\u663e\u793a\uff0cAquaChat++\u63d0\u9ad8\u4e86\u68c0\u67e5\u8986\u76d6\u7387\u3001\u80fd\u6548\u548c\u6297\u6545\u969c\u80fd\u529b\u3002", "conclusion": "LLM\u9a71\u52a8\u7684\u6846\u67b6\u6709\u671b\u652f\u6301\u6c34\u4ea7\u517b\u6b96\u4e2d\u53ef\u6269\u5c55\u3001\u667a\u80fd\u548c\u81ea\u4e3b\u7684\u6c34\u4e0b\u673a\u5668\u4eba\u64cd\u4f5c\u3002"}}
{"id": "2508.06746", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06746", "abs": "https://arxiv.org/abs/2508.06746", "authors": ["Xin Tang", "Qian Chen", "Fengshun Li", "Youchun Gong", "Yinqiu Liu", "Wen Tian", "Shaowen Qin", "Xiaohuan Li"], "title": "Topology Generation of UAV Covert Communication Networks: A Graph Diffusion Approach with Incentive Mechanism", "comment": null, "summary": "With the growing demand for Uncrewed Aerial Vehicle (UAV) networks in\nsensitive applications, such as urban monitoring, emergency response, and\nsecure sensing, ensuring reliable connectivity and covert communication has\nbecome increasingly vital. However, dynamic mobility and exposure risks pose\nsignificant challenges. To tackle these challenges, this paper proposes a\nself-organizing UAV network framework combining Graph Diffusion-based Policy\nOptimization (GDPO) with a Stackelberg Game (SG)-based incentive mechanism. The\nGDPO method uses generative AI to dynamically generate sparse but\nwell-connected topologies, enabling flexible adaptation to changing node\ndistributions and Ground User (GU) demands. Meanwhile, the Stackelberg Game\n(SG)-based incentive mechanism guides self-interested UAVs to choose relay\nbehaviors and neighbor links that support cooperation and enhance covert\ncommunication. Extensive experiments are conducted to validate the\neffectiveness of the proposed framework in terms of model convergence, topology\ngeneration quality, and enhancement of covert communication performance.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u56fe\u6269\u6563\u7b56\u7565\u4f18\u5316\uff08GDPO\uff09\u548cStackelberg\u535a\u5f08\uff08SG\uff09\u6fc0\u52b1\u673a\u5236\u7684\u65e0\u4eba\u673a\u7f51\u7edc\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u52a8\u6001\u79fb\u52a8\u6027\u548c\u9690\u853d\u901a\u4fe1\u7684\u6311\u6218\u3002", "motivation": "\u968f\u7740\u65e0\u4eba\u673a\u7f51\u7edc\u5728\u654f\u611f\u5e94\u7528\u4e2d\u7684\u9700\u6c42\u589e\u957f\uff0c\u786e\u4fdd\u53ef\u9760\u8fde\u63a5\u548c\u9690\u853d\u901a\u4fe1\u53d8\u5f97\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u52a8\u6001\u79fb\u52a8\u6027\u548c\u66b4\u9732\u98ce\u9669\u5e26\u6765\u4e86\u663e\u8457\u6311\u6218\u3002", "method": "\u91c7\u7528GDPO\u65b9\u6cd5\u751f\u6210\u7a00\u758f\u4f46\u8fde\u63a5\u826f\u597d\u7684\u62d3\u6251\u7ed3\u6784\uff0c\u5e76\u7ed3\u5408SG\u6fc0\u52b1\u673a\u5236\u5f15\u5bfc\u65e0\u4eba\u673a\u9009\u62e9\u652f\u6301\u5408\u4f5c\u7684\u8f6c\u53d1\u884c\u4e3a\u548c\u90bb\u5c45\u94fe\u63a5\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6846\u67b6\u5728\u6a21\u578b\u6536\u655b\u6027\u3001\u62d3\u6251\u751f\u6210\u8d28\u91cf\u548c\u9690\u853d\u901a\u4fe1\u6027\u80fd\u63d0\u5347\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u6709\u6548\u9002\u5e94\u52a8\u6001\u73af\u5883\u5e76\u63d0\u5347\u65e0\u4eba\u673a\u7f51\u7edc\u7684\u9690\u853d\u901a\u4fe1\u80fd\u529b\u3002"}}
{"id": "2508.06568", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06568", "abs": "https://arxiv.org/abs/2508.06568", "authors": ["Amin Yazdanshenas", "Reza Faieghi"], "title": "Robust and Agile Quadrotor Flight via Adaptive Unwinding-Free Quaternion Sliding Mode Control", "comment": null, "summary": "This paper presents a new adaptive sliding mode control (SMC) framework for\nquadrotors that achieves robust and agile flight under tight computational\nconstraints. The proposed controller addresses key limitations of prior SMC\nformulations, including (i) the slow convergence and almost-global stability of\n$\\mathrm{SO(3)}$-based methods, (ii) the oversimplification of rotational\ndynamics in Euler-based controllers, (iii) the unwinding phenomenon in\nquaternion-based formulations, and (iv) the gain overgrowth problem in adaptive\nSMC schemes. Leveraging nonsmooth stability analysis, we provide rigorous\nglobal stability proofs for both the nonsmooth attitude sliding dynamics\ndefined on $\\mathbb{S}^3$ and the position sliding dynamics. Our controller is\ncomputationally efficient and runs reliably on a resource-constrained nano\nquadrotor, achieving 250 Hz and 500 Hz refresh rates for position and attitude\ncontrol, respectively. In an extensive set of hardware experiments with over\n130 flight trials, the proposed controller consistently outperforms three\nbenchmark methods, demonstrating superior trajectory tracking accuracy and\nrobustness with relatively low control effort. The controller enables\naggressive maneuvers such as dynamic throw launches, flip maneuvers, and\naccelerations exceeding 3g, which is remarkable for a 32-gram nano quadrotor.\nThese results highlight promising potential for real-world applications,\nparticularly in scenarios requiring robust, high-performance flight control\nunder significant external disturbances and tight computational constraints.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u578b\u81ea\u9002\u5e94\u6ed1\u6a21\u63a7\u5236\u6846\u67b6\uff0c\u7528\u4e8e\u56db\u65cb\u7ffc\u65e0\u4eba\u673a\u5728\u8ba1\u7b97\u8d44\u6e90\u53d7\u9650\u4e0b\u5b9e\u73b0\u9c81\u68d2\u548c\u654f\u6377\u98de\u884c\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6ed1\u6a21\u63a7\u5236\u5728\u6536\u655b\u901f\u5ea6\u3001\u7a33\u5b9a\u6027\u3001\u65cb\u8f6c\u52a8\u529b\u5b66\u7b80\u5316\u3001\u589e\u76ca\u589e\u957f\u7b49\u95ee\u9898\u4e0a\u7684\u4e0d\u8db3\u3002", "method": "\u5229\u7528\u975e\u5149\u6ed1\u7a33\u5b9a\u6027\u5206\u6790\uff0c\u8bbe\u8ba1\u5168\u5c40\u7a33\u5b9a\u7684\u59ff\u6001\u548c\u4f4d\u7f6e\u6ed1\u6a21\u52a8\u529b\u5b66\uff0c\u5e76\u5728\u8d44\u6e90\u53d7\u9650\u7684\u7eb3\u7c73\u56db\u65cb\u7ffc\u4e0a\u9ad8\u6548\u8fd0\u884c\u3002", "result": "\u5728130\u591a\u6b21\u98de\u884c\u8bd5\u9a8c\u4e2d\uff0c\u63a7\u5236\u5668\u6027\u80fd\u4f18\u4e8e\u4e09\u79cd\u57fa\u51c6\u65b9\u6cd5\uff0c\u652f\u6301\u9ad8\u52a8\u6001\u673a\u52a8\uff08\u59823g\u52a0\u901f\u5ea6\uff09\u3002", "conclusion": "\u8be5\u63a7\u5236\u5668\u5728\u5916\u90e8\u5e72\u6270\u548c\u8ba1\u7b97\u7ea6\u675f\u4e0b\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.06753", "categories": ["cs.AI", "cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2508.06753", "abs": "https://arxiv.org/abs/2508.06753", "authors": ["Evangelos Georganas", "Dhiraj Kalamkar", "Alexander Heinecke"], "title": "Pushing the Envelope of LLM Inference on AI-PC", "comment": null, "summary": "The advent of ultra-low-bit LLM models (1/1.58/2-bit), which match the\nperplexity and end-task performance of their full-precision counterparts using\nthe same model size, is ushering in a new era of LLM inference for\nresource-constrained environments such as edge devices and AI PCs. While these\nquantization advances promise models that are more cost-effective in terms of\nlatency, memory, throughput, and energy consumption, the computational\nefficiency of state-of-the-art (SOTA) inference runtimes (e.g., bitnet.cpp)\nused to deploy them remains underexplored. In this work, we take a bottom-up\napproach: we first design and implement 1-bit and 2-bit microkernels optimized\nfor modern CPUs, achieving peak computational efficiency across a variety of\nCPU platforms. We integrate these microkernels into a state-of-the-art LLM\ninference framework, namely PyTorch-TPP, and present end-to-end inference\nresults with 2-bit models that outperform the current SOTA runtime bitnet.cpp\nby up to 2.2x, and deliver up to 7x speedup compared to the 16-bit model\ninference. Our optimized runtime advances the state of LLM inference on AI PCs\nand edge devices, paving the way for efficient deployment of ultra-low-bit LLM\nmodels.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf91/1.58/2\u4f4d\u8d85\u4f4e\u6bd4\u7279LLM\u6a21\u578b\u7684\u4f18\u5316\u5fae\u5185\u6838\uff0c\u96c6\u6210\u5230PyTorch-TPP\u6846\u67b6\u4e2d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6548\u7387\u3002", "motivation": "\u8d85\u4f4e\u6bd4\u7279LLM\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u73b0\u6709\u63a8\u7406\u8fd0\u884c\u65f6\u7684\u8ba1\u7b97\u6548\u7387\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u9488\u5bf9\u73b0\u4ee3CPU\u4f18\u5316\u76841\u4f4d\u548c2\u4f4d\u5fae\u5185\u6838\uff0c\u96c6\u6210\u5230PyTorch-TPP\u6846\u67b6\u4e2d\u3002", "result": "2\u4f4d\u6a21\u578b\u63a8\u7406\u6548\u7387\u6bd4\u5f53\u524dSOTA\u8fd0\u884c\u65f6bitnet.cpp\u5feb2.2\u500d\uff0c\u6bd416\u4f4d\u6a21\u578b\u5feb7\u500d\u3002", "conclusion": "\u4f18\u5316\u540e\u7684\u8fd0\u884c\u4e3aAI PC\u548c\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684LLM\u63a8\u7406\u63d0\u4f9b\u4e86\u9ad8\u6548\u90e8\u7f72\u65b9\u6848\u3002"}}
{"id": "2508.06575", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06575", "abs": "https://arxiv.org/abs/2508.06575", "authors": ["Rui Zhou"], "title": "Efficient Safety Testing of Autonomous Vehicles via Adaptive Search over Crash-Derived Scenarios", "comment": null, "summary": "Ensuring the safety of autonomous vehicles (AVs) is paramount in their\ndevelopment and deployment. Safety-critical scenarios pose more severe\nchallenges, necessitating efficient testing methods to validate AVs safety.\nThis study focuses on designing an accelerated testing algorithm for AVs in\nsafety-critical scenarios, enabling swift recognition of their driving\ncapabilities. First, typical logical scenarios were extracted from real-world\ncrashes in the China In-depth Mobility Safety Study-Traffic Accident (CIMSS-TA)\ndatabase, obtaining pre-crash features through reconstruction. Second, Baidu\nApollo, an advanced black-box automated driving system (ADS) is integrated to\ncontrol the behavior of the ego vehicle. Third, we proposed an adaptive\nlarge-variable neighborhood-simulated annealing algorithm (ALVNS-SA) to\nexpedite the testing process. Experimental results demonstrate a significant\nenhancement in testing efficiency when utilizing ALVNS-SA. It achieves an\n84.00% coverage of safety-critical scenarios, with crash scenario coverage of\n96.83% and near-crash scenario coverage of 92.07%. Compared to genetic\nalgorithm (GA), adaptive large neighborhood-simulated annealing algorithm\n(ALNS-SA), and random testing, ALVNS-SA exhibits substantially higher coverage\nin safety-critical scenarios.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u52a0\u901f\u6d4b\u8bd5\u7b97\u6cd5\uff08ALVNS-SA\uff09\uff0c\u7528\u4e8e\u9a8c\u8bc1\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\u7684\u5b89\u5168\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6d4b\u8bd5\u6548\u7387\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u5b89\u5168\u6027\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\uff0c\u9700\u8981\u9ad8\u6548\u7684\u6d4b\u8bd5\u65b9\u6cd5\u9a8c\u8bc1\u5176\u9a7e\u9a76\u80fd\u529b\u3002", "method": "\u4ece\u771f\u5b9e\u4e8b\u6545\u6570\u636e\u5e93\u4e2d\u63d0\u53d6\u5178\u578b\u903b\u8f91\u573a\u666f\uff0c\u96c6\u6210\u767e\u5ea6Apollo\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\uff0c\u5e76\u63d0\u51faALVNS-SA\u7b97\u6cd5\u52a0\u901f\u6d4b\u8bd5\u8fc7\u7a0b\u3002", "result": "ALVNS-SA\u7b97\u6cd5\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\u7684\u8986\u76d6\u7387\u8fbe\u523084.00%\uff0c\u5176\u4e2d\u78b0\u649e\u573a\u666f\u8986\u76d6\u7387\u4e3a96.83%\uff0c\u63a5\u8fd1\u78b0\u649e\u573a\u666f\u8986\u76d6\u7387\u4e3a92.07%\uff0c\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u7b97\u6cd5\u3002", "conclusion": "ALVNS-SA\u7b97\u6cd5\u5728\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u5b89\u5168\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u9ad8\u6548\u6027\u548c\u4f18\u8d8a\u6027\uff0c\u9002\u7528\u4e8e\u5b89\u5168\u5173\u952e\u573a\u666f\u7684\u5feb\u901f\u9a8c\u8bc1\u3002"}}
{"id": "2508.06754", "categories": ["cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2508.06754", "abs": "https://arxiv.org/abs/2508.06754", "authors": ["Vanessa Figueiredo"], "title": "A Fuzzy Logic Prompting Framework for Large Language Models in Adaptive and Uncertain Tasks", "comment": null, "summary": "We introduce a modular prompting framework that supports safer and more\nadaptive use of large language models (LLMs) across dynamic, user-centered\ntasks. Grounded in human learning theory, particularly the Zone of Proximal\nDevelopment (ZPD), our method combines a natural language boundary prompt with\na control schema encoded with fuzzy scaffolding logic and adaptation rules.\nThis architecture enables LLMs to modulate behavior in response to user state\nwithout requiring fine-tuning or external orchestration. In a simulated\nintelligent tutoring setting, the framework improves scaffolding quality,\nadaptivity, and instructional alignment across multiple models, outperforming\nstandard prompting baselines. Evaluation is conducted using rubric-based LLM\ngraders at scale. While initially developed for education, the framework has\nshown promise in other interaction-heavy domains, such as procedural content\ngeneration for games. Designed for safe deployment, it provides a reusable\nmethodology for structuring interpretable, goal-aligned LLM behavior in\nuncertain or evolving contexts.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u5757\u5316\u63d0\u793a\u6846\u67b6\uff0c\u652f\u6301\u5728\u52a8\u6001\u3001\u7528\u6237\u4e2d\u5fc3\u4efb\u52a1\u4e2d\u66f4\u5b89\u5168\u3001\u81ea\u9002\u5e94\u5730\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u3002", "motivation": "\u57fa\u4e8e\u4eba\u7c7b\u5b66\u4e60\u7406\u8bba\uff08\u5982\u6700\u8fd1\u53d1\u5c55\u533aZPD\uff09\uff0c\u65e8\u5728\u63d0\u5347LLMs\u5728\u7528\u6237\u72b6\u6001\u53d8\u5316\u65f6\u7684\u884c\u4e3a\u8c03\u8282\u80fd\u529b\uff0c\u65e0\u9700\u5fae\u8c03\u6216\u5916\u90e8\u534f\u8c03\u3002", "method": "\u7ed3\u5408\u81ea\u7136\u8bed\u8a00\u8fb9\u754c\u63d0\u793a\u4e0e\u63a7\u5236\u6a21\u5f0f\uff0c\u91c7\u7528\u6a21\u7cca\u652f\u67b6\u903b\u8f91\u548c\u9002\u5e94\u89c4\u5219\uff0c\u5b9e\u73b0LLMs\u7684\u81ea\u9002\u5e94\u884c\u4e3a\u8c03\u8282\u3002", "result": "\u5728\u6a21\u62df\u667a\u80fd\u8f85\u5bfc\u73af\u5883\u4e2d\uff0c\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u652f\u67b6\u8d28\u91cf\u3001\u9002\u5e94\u6027\u548c\u6559\u5b66\u5bf9\u9f50\u6027\uff0c\u4f18\u4e8e\u6807\u51c6\u63d0\u793a\u57fa\u7ebf\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e0d\u4ec5\u9002\u7528\u4e8e\u6559\u80b2\u9886\u57df\uff0c\u8fd8\u53ef\u6269\u5c55\u81f3\u5176\u4ed6\u4ea4\u4e92\u5bc6\u96c6\u578b\u573a\u666f\uff08\u5982\u6e38\u620f\u5185\u5bb9\u751f\u6210\uff09\uff0c\u4e3a\u4e0d\u786e\u5b9a\u6216\u52a8\u6001\u73af\u5883\u4e2d\u7684LLM\u884c\u4e3a\u63d0\u4f9b\u53ef\u89e3\u91ca\u3001\u76ee\u6807\u5bf9\u9f50\u7684\u65b9\u6cd5\u3002"}}
{"id": "2508.06687", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06687", "abs": "https://arxiv.org/abs/2508.06687", "authors": ["Sreeja Roy-Singh", "Vinay Ravindra", "Richard Levinson", "Mahta Moghaddam", "Jan Mandel", "Adam Kochanski", "Angel Farguell Caus", "Kurtis Nelson", "Samira Alkaee Taleghan", "Archana Kannan", "Amer Melebari"], "title": "Optimal Planning and Machine Learning for Responsive Tracking and Enhanced Forecasting of Wildfires using a Spacecraft Constellation", "comment": null, "summary": "We propose a novel concept of operations using optimal planning methods and\nmachine learning (ML) to collect spaceborne data that is unprecedented for\nmonitoring wildfires, process it to create new or enhanced products in the\ncontext of wildfire danger or spread monitoring, and assimilate them to improve\nexisting, wildfire decision support tools delivered to firefighters within\nlatency appropriate for time-critical applications. The concept is studied with\nrespect to NASA's CYGNSS Mission, a constellation of passive microwave\nreceivers that measure specular GNSS-R reflections despite clouds and smoke.\nOur planner uses a Mixed Integer Program formulation to schedule joint\nobservation data collection and downlink for all satellites. Optimal solutions\nare found quickly that collect 98-100% of available observation opportunities.\nML-based fire predictions that drive the planner objective are greater than 40%\nmore correlated with ground truth than existing state-of-art. The presented\ncase study on the TX Smokehouse Creek fire in 2024 and LA fires in 2025\nrepresents the first high-resolution data collected by CYGNSS of active fires.\nCreation of Burnt Area Maps (BAM) using ML applied to the data during active\nfires and BAM assimilation into NASA's Weather Research and Forecasting Model\nusing ML to broadcast fire spread are novel outcomes. BAM and CYGNSS obtained\nsoil moisture are integrated for the first time into USGS fire danger maps.\nInclusion of CYGNSS data in ML-based burn predictions boosts accuracy by 13%,\nand inclusion of high-resolution data boosts ML recall by another 15%. The\nproposed workflow has an expected latency of 6-30h, improving on the current\ndelivery time of multiple days. All components in the proposed concept are\nshown to be computationally scalable and globally generalizable, with\nsustainability considerations such as edge efficiency and low latency on small\ndevices.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4f18\u5316\u89c4\u5212\u548c\u673a\u5668\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u6536\u96c6\u548c\u5904\u7406\u592a\u7a7a\u6570\u636e\u4ee5\u76d1\u6d4b\u91ce\u706b\uff0c\u5e76\u901a\u8fc7\u4f4e\u5ef6\u8fdf\u5de5\u5177\u652f\u6301\u6d88\u9632\u51b3\u7b56\u3002", "motivation": "\u73b0\u6709\u91ce\u706b\u76d1\u6d4b\u548c\u51b3\u7b56\u652f\u6301\u5de5\u5177\u7684\u5ef6\u8fdf\u548c\u6570\u636e\u5206\u8fa8\u7387\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u6539\u8fdb\u3002", "method": "\u91c7\u7528\u6df7\u5408\u6574\u6570\u89c4\u5212\u8c03\u5ea6\u536b\u661f\u89c2\u6d4b\u548c\u6570\u636e\u4e0b\u884c\uff0c\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u9884\u6d4b\u91ce\u706b\u5e76\u751f\u6210\u65b0\u4ea7\u54c1\uff08\u5982\u71c3\u70e7\u533a\u57df\u5730\u56fe\uff09\u3002", "result": "\u4f18\u5316\u89c4\u5212\u5b9e\u73b0\u4e8698-100%\u7684\u89c2\u6d4b\u673a\u4f1a\u6355\u83b7\uff0c\u673a\u5668\u5b66\u4e60\u9884\u6d4b\u51c6\u786e\u7387\u63d0\u534740%\uff0c\u6570\u636e\u96c6\u6210\u4f7f\u71c3\u70e7\u9884\u6d4b\u51c6\u786e\u7387\u63d0\u534713%-15%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u91ce\u706b\u76d1\u6d4b\u548c\u9884\u6d4b\u80fd\u529b\uff0c\u5177\u6709\u4f4e\u5ef6\u8fdf\u3001\u53ef\u6269\u5c55\u6027\u548c\u5168\u7403\u9002\u7528\u6027\u3002"}}
{"id": "2508.06823", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06823", "abs": "https://arxiv.org/abs/2508.06823", "authors": ["Xuan Zhao", "Jun Tao"], "title": "Natural Language-Driven Viewpoint Navigation for Volume Exploration via Semantic Block Representation", "comment": "Accepted by IEEE VIS 2025", "summary": "Exploring volumetric data is crucial for interpreting scientific datasets.\nHowever, selecting optimal viewpoints for effective navigation can be\nchallenging, particularly for users without extensive domain expertise or\nfamiliarity with 3D navigation. In this paper, we propose a novel framework\nthat leverages natural language interaction to enhance volumetric data\nexploration. Our approach encodes volumetric blocks to capture and\ndifferentiate underlying structures. It further incorporates a CLIP Score\nmechanism, which provides semantic information to the blocks to guide\nnavigation. The navigation is empowered by a reinforcement learning framework\nthat leverage these semantic cues to efficiently search for and identify\ndesired viewpoints that align with the user's intent. The selected viewpoints\nare evaluated using CLIP Score to ensure that they best reflect the user\nqueries. By automating viewpoint selection, our method improves the efficiency\nof volumetric data navigation and enhances the interpretability of complex\nscientific phenomena.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u7684\u6846\u67b6\uff0c\u901a\u8fc7CLIP Score\u673a\u5236\u548c\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u4f53\u6570\u636e\u63a2\u7d22\u7684\u89c6\u89d2\u9009\u62e9\u3002", "motivation": "\u4f53\u6570\u636e\u63a2\u7d22\u5bf9\u79d1\u5b66\u6570\u636e\u5206\u6790\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7f3a\u4e4f\u9886\u57df\u77e5\u8bc6\u62163D\u5bfc\u822a\u7ecf\u9a8c\u7684\u7528\u6237\u96be\u4ee5\u9009\u62e9\u6700\u4f73\u89c6\u89d2\u3002", "method": "\u5c06\u4f53\u6570\u636e\u5757\u7f16\u7801\u4ee5\u533a\u5206\u7ed3\u6784\uff0c\u7ed3\u5408CLIP Score\u63d0\u4f9b\u8bed\u4e49\u4fe1\u606f\uff0c\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u641c\u7d22\u7b26\u5408\u7528\u6237\u610f\u56fe\u7684\u89c6\u89d2\u3002", "result": "\u81ea\u52a8\u5316\u89c6\u89d2\u9009\u62e9\u63d0\u9ad8\u4e86\u4f53\u6570\u636e\u5bfc\u822a\u6548\u7387\uff0c\u5e76\u589e\u5f3a\u4e86\u590d\u6742\u79d1\u5b66\u73b0\u8c61\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u548c\u8bed\u4e49\u5f15\u5bfc\uff0c\u663e\u8457\u6539\u5584\u4e86\u4f53\u6570\u636e\u63a2\u7d22\u7684\u6548\u7387\u548c\u7528\u6237\u4f53\u9a8c\u3002"}}
{"id": "2508.06722", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06722", "abs": "https://arxiv.org/abs/2508.06722", "authors": ["Justin London"], "title": "Improved Obstacle Avoidance for Autonomous Robots with ORCA-FLC", "comment": null, "summary": "Obstacle avoidance enables autonomous agents and robots to operate safely and\nefficiently in dynamic and complex environments, reducing the risk of\ncollisions and damage. For a robot or autonomous system to successfully\nnavigate through obstacles, it must be able to detect such obstacles. While\nnumerous collision avoidance algorithms like the dynamic window approach (DWA),\ntimed elastic bands (TEB), and reciprocal velocity obstacles (RVO) have been\nproposed, they may lead to suboptimal paths due to fixed weights, be\ncomputationally expensive, or have limited adaptability to dynamic obstacles in\nmulti-agent environments. Optimal reciprocal collision avoidance (ORCA), which\nimproves on RVO, provides smoother trajectories and stronger collision\navoidance guarantees. We propose ORCA-FL to improve on ORCA by using fuzzy\nlogic controllers (FLCs) to better handle uncertainty and imprecision for\nobstacle avoidance in path planning. Numerous multi-agent experiments are\nconducted and it is shown that ORCA-FL can outperform ORCA in reducing the\nnumber of collision if the agent has a velocity that exceeds a certain\nthreshold. In addition, a proposed algorithm for improving ORCA-FL using fuzzy\nQ reinforcement learning (FQL) is detailed for optimizing and tuning FLCs.", "AI": {"tldr": "ORCA-FL\u901a\u8fc7\u6a21\u7cca\u903b\u8f91\u63a7\u5236\u5668\u6539\u8fdbORCA\u7b97\u6cd5\uff0c\u63d0\u5347\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e2d\u7684\u907f\u969c\u6027\u80fd\uff0c\u5e76\u5728\u901f\u5ea6\u8d85\u8fc7\u9608\u503c\u65f6\u51cf\u5c11\u78b0\u649e\u6b21\u6570\u3002", "motivation": "\u73b0\u6709\u907f\u969c\u7b97\u6cd5\uff08\u5982DWA\u3001TEB\u3001RVO\uff09\u5b58\u5728\u8def\u5f84\u6b21\u4f18\u3001\u8ba1\u7b97\u6210\u672c\u9ad8\u6216\u52a8\u6001\u9002\u5e94\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0cORCA\u867d\u6539\u8fdbRVO\u4f46\u4ecd\u9700\u4f18\u5316\u3002", "method": "\u63d0\u51faORCA-FL\uff0c\u7ed3\u5408\u6a21\u7cca\u903b\u8f91\u63a7\u5236\u5668\uff08FLCs\uff09\u5904\u7406\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u5f15\u5165\u6a21\u7ccaQ\u5f3a\u5316\u5b66\u4e60\uff08FQL\uff09\u4f18\u5316FLCs\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cORCA-FL\u5728\u901f\u5ea6\u8d85\u8fc7\u9608\u503c\u65f6\u80fd\u51cf\u5c11\u78b0\u649e\u6b21\u6570\uff0c\u4f18\u4e8eORCA\u3002", "conclusion": "ORCA-FL\u901a\u8fc7\u6a21\u7cca\u903b\u8f91\u548c\u5f3a\u5316\u5b66\u4e60\u63d0\u5347\u4e86\u907f\u969c\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u52a8\u6001\u591a\u667a\u80fd\u4f53\u73af\u5883\u3002"}}
{"id": "2508.06832", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06832", "abs": "https://arxiv.org/abs/2508.06832", "authors": ["Haifeng Li", "Wang Guo", "Haiyang Wu", "Mengwei Wu", "Jipeng Zhang", "Qing Zhu", "Yu Liu", "Xin Huang", "Chao Tao"], "title": "Remote Sensing Image Intelligent Interpretation with the Language-Centered Perspective: Principles, Methods and Challenges", "comment": null, "summary": "The mainstream paradigm of remote sensing image interpretation has long been\ndominated by vision-centered models, which rely on visual features for semantic\nunderstanding. However, these models face inherent limitations in handling\nmulti-modal reasoning, semantic abstraction, and interactive decision-making.\nWhile recent advances have introduced Large Language Models (LLMs) into remote\nsensing workflows, existing studies primarily focus on downstream applications,\nlacking a unified theoretical framework that explains the cognitive role of\nlanguage. This review advocates a paradigm shift from vision-centered to\nlanguage-centered remote sensing interpretation. Drawing inspiration from the\nGlobal Workspace Theory (GWT) of human cognition, We propose a\nlanguage-centered framework for remote sensing interpretation that treats LLMs\nas the cognitive central hub integrating perceptual, task, knowledge and action\nspaces to enable unified understanding, reasoning, and decision-making. We\nfirst explore the potential of LLMs as the central cognitive component in\nremote sensing interpretation, and then summarize core technical challenges,\nincluding unified multimodal representation, knowledge association, and\nreasoning and decision-making. Furthermore, we construct a global\nworkspace-driven interpretation mechanism and review how language-centered\nsolutions address each challenge. Finally, we outline future research\ndirections from four perspectives: adaptive alignment of multimodal data, task\nunderstanding under dynamic knowledge constraints, trustworthy reasoning, and\nautonomous interaction. This work aims to provide a conceptual foundation for\nthe next generation of remote sensing interpretation systems and establish a\nroadmap toward cognition-driven intelligent geospatial analysis.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4ece\u89c6\u89c9\u4e3a\u4e2d\u5fc3\u8f6c\u5411\u8bed\u8a00\u4e3a\u4e2d\u5fc3\u7684\u9065\u611f\u56fe\u50cf\u89e3\u91ca\u6846\u67b6\uff0c\u501f\u9274\u5168\u7403\u5de5\u4f5c\u7a7a\u95f4\u7406\u8bba\uff08GWT\uff09\uff0c\u5c06\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4f5c\u4e3a\u8ba4\u77e5\u4e2d\u67a2\uff0c\u6574\u5408\u611f\u77e5\u3001\u4efb\u52a1\u3001\u77e5\u8bc6\u548c\u884c\u52a8\u7a7a\u95f4\uff0c\u5b9e\u73b0\u7edf\u4e00\u7406\u89e3\u3001\u63a8\u7406\u548c\u51b3\u7b56\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9\u4e3a\u4e2d\u5fc3\u7684\u9065\u611f\u56fe\u50cf\u89e3\u91ca\u6a21\u578b\u5728\u591a\u6a21\u6001\u63a8\u7406\u3001\u8bed\u4e49\u62bd\u8c61\u548c\u4ea4\u4e92\u51b3\u7b56\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7406\u8bba\u6846\u67b6\u89e3\u91ca\u8bed\u8a00\u5728\u8ba4\u77e5\u4e2d\u7684\u4f5c\u7528\u3002", "method": "\u63d0\u51fa\u8bed\u8a00\u4e3a\u4e2d\u5fc3\u7684\u6846\u67b6\uff0c\u4ee5LLMs\u4e3a\u6838\u5fc3\uff0c\u6574\u5408\u591a\u6a21\u6001\u8868\u793a\u3001\u77e5\u8bc6\u5173\u8054\u3001\u63a8\u7406\u4e0e\u51b3\u7b56\uff0c\u6784\u5efa\u5168\u5c40\u5de5\u4f5c\u7a7a\u95f4\u9a71\u52a8\u7684\u89e3\u91ca\u673a\u5236\u3002", "result": "\u603b\u7ed3\u4e86\u8bed\u8a00\u4e3a\u4e2d\u5fc3\u89e3\u51b3\u65b9\u6848\u5982\u4f55\u5e94\u5bf9\u6838\u5fc3\u6311\u6218\uff0c\u5305\u62ec\u7edf\u4e00\u591a\u6a21\u6001\u8868\u793a\u3001\u77e5\u8bc6\u5173\u8054\u53ca\u63a8\u7406\u51b3\u7b56\u3002", "conclusion": "\u672a\u6765\u7814\u7a76\u65b9\u5411\u5305\u62ec\u591a\u6a21\u6001\u6570\u636e\u81ea\u9002\u5e94\u5bf9\u9f50\u3001\u52a8\u6001\u77e5\u8bc6\u7ea6\u675f\u4e0b\u7684\u4efb\u52a1\u7406\u89e3\u3001\u53ef\u4fe1\u63a8\u7406\u548c\u81ea\u4e3b\u4ea4\u4e92\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u9065\u611f\u89e3\u91ca\u7cfb\u7edf\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2508.06742", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.06742", "abs": "https://arxiv.org/abs/2508.06742", "authors": ["Alejandro Murillo-Gonzalez", "Junhong Xu", "Lantao Liu"], "title": "Learning Causal Structure Distributions for Robust Planning", "comment": null, "summary": "Structural causal models describe how the components of a robotic system\ninteract. They provide both structural and functional information about the\nrelationships that are present in the system. The structural information\noutlines the variables among which there is interaction. The functional\ninformation describes how such interactions work, via equations or learned\nmodels. In this paper we find that learning the functional relationships while\naccounting for the uncertainty about the structural information leads to more\nrobust dynamics models which improves downstream planning, while using\nsignificantly lower computational resources. This in contrast with common\nmodel-learning methods that ignore the causal structure and fail to leverage\nthe sparsity of interactions in robotic systems. We achieve this by estimating\na causal structure distribution that is used to sample causal graphs that\ninform the latent-space representations in an encoder-multidecoder\nprobabilistic model. We show that our model can be used to learn the dynamics\nof a robot, which together with a sampling-based planner can be used to perform\nnew tasks in novel environments, provided an objective function for the new\nrequirement is available. We validate our method using manipulators and mobile\nrobots in both simulation and the real-world. Additionally, we validate the\nlearned dynamics' adaptability and increased robustness to corrupted inputs and\nchanges in the environment, which is highly desirable in challenging real-world\nrobotics scenarios. Video: https://youtu.be/X6k5t7OOnNc.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7ed3\u6784\u56e0\u679c\u6a21\u578b\u548c\u529f\u80fd\u5173\u7cfb\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u63d0\u5347\u673a\u5668\u4eba\u7cfb\u7edf\u52a8\u6001\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u5b66\u4e60\u65b9\u6cd5\u5e38\u5ffd\u7565\u56e0\u679c\u5173\u7cfb\u548c\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u4ea4\u4e92\u7684\u7a00\u758f\u6027\uff0c\u5bfc\u81f4\u6a21\u578b\u4e0d\u591f\u9c81\u68d2\u4e14\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u5927\u3002", "method": "\u901a\u8fc7\u4f30\u8ba1\u56e0\u679c\u7ed3\u6784\u5206\u5e03\u5e76\u91c7\u6837\u56e0\u679c\u56fe\uff0c\u7ed3\u5408\u7f16\u7801\u5668-\u591a\u89e3\u7801\u5668\u6982\u7387\u6a21\u578b\u5b66\u4e60\u52a8\u6001\u5173\u7cfb\u3002", "result": "\u6a21\u578b\u5728\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u63d0\u5347\u4e86\u52a8\u6001\u5b66\u4e60\u7684\u9002\u5e94\u6027\u548c\u5bf9\u8f93\u5165\u566a\u58f0\u53ca\u73af\u5883\u53d8\u5316\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u52a8\u6001\u6a21\u578b\u7684\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u590d\u6742\u73b0\u5b9e\u573a\u666f\u3002"}}
{"id": "2508.06836", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06836", "abs": "https://arxiv.org/abs/2508.06836", "authors": ["Xutong Zhao", "Yaqi Xie"], "title": "Multi-level Advantage Credit Assignment for Cooperative Multi-Agent Reinforcement Learning", "comment": "Accepted at AISTATS 2025", "summary": "Cooperative multi-agent reinforcement learning (MARL) aims to coordinate\nmultiple agents to achieve a common goal. A key challenge in MARL is credit\nassignment, which involves assessing each agent's contribution to the shared\nreward. Given the diversity of tasks, agents may perform different types of\ncoordination, with rewards attributed to diverse and often overlapping agent\nsubsets. In this work, we formalize the credit assignment level as the number\nof agents cooperating to obtain a reward, and address scenarios with multiple\ncoexisting levels. We introduce a multi-level advantage formulation that\nperforms explicit counterfactual reasoning to infer credits across distinct\nlevels. Our method, Multi-level Advantage Credit Assignment (MACA), captures\nagent contributions at multiple levels by integrating advantage functions that\nreason about individual, joint, and correlated actions. Utilizing an\nattention-based framework, MACA identifies correlated agent relationships and\nconstructs multi-level advantages to guide policy learning. Comprehensive\nexperiments on challenging Starcraft v1\\&v2 tasks demonstrate MACA's superior\nperformance, underscoring its efficacy in complex credit assignment scenarios.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u7ea7\u4f18\u52bf\u4fe1\u7528\u5206\u914d\u65b9\u6cd5\uff08MACA\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u4fe1\u7528\u5206\u914d\u95ee\u9898\uff0c\u901a\u8fc7\u591a\u7ea7\u4f18\u52bf\u51fd\u6570\u548c\u6ce8\u610f\u529b\u673a\u5236\u6355\u6349\u4e0d\u540c\u534f\u4f5c\u5c42\u6b21\u7684\u8d21\u732e\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u4e2d\u7684\u4fe1\u7528\u5206\u914d\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u4efb\u52a1\u591a\u6837\u6027\u548c\u667a\u80fd\u4f53\u534f\u4f5c\u5c42\u6b21\u590d\u6742\u7684\u60c5\u51b5\u4e0b\u3002", "method": "MACA\u901a\u8fc7\u591a\u7ea7\u4f18\u52bf\u51fd\u6570\u8fdb\u884c\u663e\u5f0f\u53cd\u4e8b\u5b9e\u63a8\u7406\uff0c\u7ed3\u5408\u6ce8\u610f\u529b\u673a\u5236\u8bc6\u522b\u667a\u80fd\u4f53\u95f4\u7684\u76f8\u5173\u6027\uff0c\u6784\u5efa\u591a\u7ea7\u4f18\u52bf\u4ee5\u6307\u5bfc\u7b56\u7565\u5b66\u4e60\u3002", "result": "\u5728Starcraft v1&v2\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMACA\u5728\u590d\u6742\u4fe1\u7528\u5206\u914d\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "MACA\u901a\u8fc7\u591a\u7ea7\u4f18\u52bf\u4fe1\u7528\u5206\u914d\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u667a\u80fd\u4f53\u534f\u4f5c\u4e2d\u7684\u4fe1\u7528\u5206\u914d\u95ee\u9898\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u534f\u4f5c\u5c42\u6b21\u7684\u4efb\u52a1\u3002"}}
{"id": "2508.06744", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.06744", "abs": "https://arxiv.org/abs/2508.06744", "authors": ["Yunke Ao", "Manish Prajapat", "Yarden As", "Yassine Taoudi-Benchekroun", "Fabio Carrillo", "Hooman Esfandiari", "Benjamin F. Grewe", "Andreas Krause", "Philipp F\u00fcrnstahl"], "title": "Robust-Sub-Gaussian Model Predictive Control for Safe Ultrasound-Image-Guided Robotic Spinal Surgery", "comment": null, "summary": "Safety-critical control using high-dimensional sensory feedback from optical\ndata (e.g., images, point clouds) poses significant challenges in domains like\nautonomous driving and robotic surgery. Control can rely on low-dimensional\nstates estimated from high-dimensional data. However, the estimation errors\noften follow complex, unknown distributions that standard probabilistic models\nfail to capture, making formal safety guarantees challenging. In this work, we\nintroduce a novel characterization of these general estimation errors using\nsub-Gaussian noise with bounded mean. We develop a new technique for\nuncertainty propagation of proposed noise characterization in linear systems,\nwhich combines robust set-based methods with the propagation of sub-Gaussian\nvariance proxies. We further develop a Model Predictive Control (MPC) framework\nthat provides closed-loop safety guarantees for linear systems under the\nproposed noise assumption. We apply this MPC approach in an\nultrasound-image-guided robotic spinal surgery pipeline, which contains\ndeep-learning-based semantic segmentation, image-based registration, high-level\noptimization-based planning, and low-level robotic control. To validate the\npipeline, we developed a realistic simulation environment integrating real\nhuman anatomy, robot dynamics, efficient ultrasound simulation, as well as\nin-vivo data of breathing motion and drilling force. Evaluation results in\nsimulation demonstrate the potential of our approach for solving complex\nimage-guided robotic surgery task while ensuring safety.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ad8\u7ef4\u5149\u5b66\u6570\u636e\u7684\u5b89\u5168\u5173\u952e\u63a7\u5236\u65b9\u6cd5\uff0c\u5229\u7528\u5b50\u9ad8\u65af\u566a\u58f0\u6a21\u578b\u5904\u7406\u4f30\u8ba1\u8bef\u5dee\uff0c\u5e76\u7ed3\u5408MPC\u6846\u67b6\u786e\u4fdd\u7ebf\u6027\u7cfb\u7edf\u7684\u95ed\u73af\u5b89\u5168\u6027\uff0c\u5e94\u7528\u4e8e\u673a\u5668\u4eba\u810a\u67f1\u624b\u672f\u3002", "motivation": "\u9ad8\u7ef4\u5149\u5b66\u6570\u636e\uff08\u5982\u56fe\u50cf\u3001\u70b9\u4e91\uff09\u5728\u81ea\u52a8\u9a7e\u9a76\u548c\u673a\u5668\u4eba\u624b\u672f\u7b49\u9886\u57df\u7684\u63a7\u5236\u4e2d\u5b58\u5728\u4f30\u8ba1\u8bef\u5dee\u5206\u5e03\u590d\u6742\u3001\u96be\u4ee5\u5efa\u6a21\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u5b89\u5168\u4fdd\u8bc1\u56f0\u96be\u3002", "method": "\u63d0\u51fa\u5b50\u9ad8\u65af\u566a\u58f0\u6a21\u578b\u63cf\u8ff0\u4f30\u8ba1\u8bef\u5dee\uff0c\u7ed3\u5408\u9c81\u68d2\u96c6\u65b9\u6cd5\u548c\u5b50\u9ad8\u65af\u65b9\u5dee\u4f20\u64ad\u6280\u672f\uff0c\u5f00\u53d1\u4e86\u5177\u6709\u5b89\u5168\u4fdd\u8bc1\u7684MPC\u6846\u67b6\u3002", "result": "\u5728\u673a\u5668\u4eba\u810a\u67f1\u624b\u672f\u7684\u4eff\u771f\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u786e\u4fdd\u4e86\u4efb\u52a1\u7684\u5b89\u5168\u6267\u884c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u9ad8\u7ef4\u6570\u636e\u9a71\u52a8\u7684\u5b89\u5168\u63a7\u5236\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5c24\u5176\u5728\u590d\u6742\u56fe\u50cf\u5f15\u5bfc\u7684\u673a\u5668\u4eba\u624b\u672f\u4e2d\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2508.06851", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.06851", "abs": "https://arxiv.org/abs/2508.06851", "authors": ["Pengfei Zhou", "Xiaopeng Peng", "Fanrui Zhang", "Zhaopan Xu", "Jiaxin Ai", "Yansheng Qiu", "Chuanhao Li", "Zhen Li", "Ming Li", "Yukang Feng", "Jianwen Sun", "Haoquan Zhang", "Zizhen Li", "Xiaofeng Mao", "Zekai Li", "Wangbo Zhao", "Kai Wang", "Xiaojun Chang", "Wenqi Shao", "Yang You", "Kaipeng Zhang"], "title": "MDK12-Bench: A Comprehensive Evaluation of Multimodal Large Language Models on Multidisciplinary Exams", "comment": "35 pages, 33 figures", "summary": "Multimodal large language models (MLLMs), which integrate language and visual\ncues for problem-solving, are crucial for advancing artificial general\nintelligence (AGI). However, current benchmarks for measuring the intelligence\nof MLLMs suffer from limited scale, narrow coverage, and unstructured\nknowledge, offering only static and undifferentiated evaluations. To bridge\nthis gap, we introduce MDK12-Bench, a large-scale multidisciplinary benchmark\nbuilt from real-world K-12 exams spanning six disciplines with 141K instances\nand 6,225 knowledge points organized in a six-layer taxonomy. Covering five\nquestion formats with difficulty and year annotations, it enables comprehensive\nevaluation to capture the extent to which MLLMs perform over four dimensions:\n1) difficulty levels, 2) temporal (cross-year) shifts, 3) contextual shifts,\nand 4) knowledge-driven reasoning. We propose a novel dynamic evaluation\nframework that introduces unfamiliar visual, textual, and question form shifts\nto challenge model generalization while improving benchmark objectivity and\nlongevity by mitigating data contamination. We further evaluate knowledge-point\nreference-augmented generation (KP-RAG) to examine the role of knowledge in\nproblem-solving. Key findings reveal limitations in current MLLMs in multiple\naspects and provide guidance for enhancing model robustness, interpretability,\nand AI-assisted education.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86MDK12-Bench\uff0c\u4e00\u4e2a\u57fa\u4e8eK-12\u8003\u8bd5\u7684\u591a\u5b66\u79d1\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u5168\u9762\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u6027\u80fd\uff0c\u5e76\u63d0\u51fa\u52a8\u6001\u8bc4\u4f30\u6846\u67b6\u548c\u77e5\u8bc6\u589e\u5f3a\u751f\u6210\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524dMLLMs\u7684\u8bc4\u6d4b\u57fa\u51c6\u5b58\u5728\u89c4\u6a21\u5c0f\u3001\u8986\u76d6\u7a84\u3001\u77e5\u8bc6\u65e0\u7ed3\u6784\u5316\u7b49\u95ee\u9898\uff0c\u65e0\u6cd5\u5168\u9762\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u3002", "method": "\u6784\u5efaMDK12-Bench\u57fa\u51c6\uff0c\u5305\u542b141K\u5b9e\u4f8b\u548c6,225\u4e2a\u77e5\u8bc6\u70b9\uff0c\u63d0\u51fa\u52a8\u6001\u8bc4\u4f30\u6846\u67b6\u548cKP-RAG\u65b9\u6cd5\u3002", "result": "\u53d1\u73b0\u5f53\u524dMLLMs\u5728\u591a\u4e2a\u7ef4\u5ea6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u4e3a\u6a21\u578b\u9c81\u68d2\u6027\u548cAI\u8f85\u52a9\u6559\u80b2\u63d0\u4f9b\u6307\u5bfc\u3002", "conclusion": "MDK12-Bench\u4e3aMLLMs\u7684\u5168\u9762\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u4e0d\u8db3\u5e76\u6307\u660e\u6539\u8fdb\u65b9\u5411\u3002"}}
{"id": "2508.06779", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06779", "abs": "https://arxiv.org/abs/2508.06779", "authors": ["Minku Kim", "Brian Acosta", "Pratik Chaudhari", "Michael Posa"], "title": "Learning a Vision-Based Footstep Planner for Hierarchical Walking Control", "comment": "8 pages, 8 figures, accepted to 2025 IEEE-RAS 24th International\n  Conference on Humanoid Robots", "summary": "Bipedal robots demonstrate potential in navigating challenging terrains\nthrough dynamic ground contact. However, current frameworks often depend solely\non proprioception or use manually designed visual pipelines, which are fragile\nin real-world settings and complicate real-time footstep planning in\nunstructured environments. To address this problem, we present a vision-based\nhierarchical control framework that integrates a reinforcement learning\nhigh-level footstep planner, which generates footstep commands based on a local\nelevation map, with a low-level Operational Space Controller that tracks the\ngenerated trajectories. We utilize the Angular Momentum Linear Inverted\nPendulum model to construct a low-dimensional state representation to capture\nan informative encoding of the dynamics while reducing complexity. We evaluate\nour method across different terrain conditions using the underactuated bipedal\nrobot Cassie and investigate the capabilities and challenges of our approach\nthrough simulation and hardware experiments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u7684\u5206\u5c42\u63a7\u5236\u6846\u67b6\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u7684\u9ad8\u5c42\u811a\u6b65\u89c4\u5212\u5668\u548c\u4f4e\u5c42\u64cd\u4f5c\u7a7a\u95f4\u63a7\u5236\u5668\uff0c\u7528\u4e8e\u53cc\u8db3\u673a\u5668\u4eba\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u5b9e\u65f6\u811a\u6b65\u89c4\u5212\u3002", "motivation": "\u5f53\u524d\u53cc\u8db3\u673a\u5668\u4eba\u7684\u6846\u67b6\u4f9d\u8d56\u672c\u4f53\u611f\u77e5\u6216\u624b\u52a8\u8bbe\u8ba1\u7684\u89c6\u89c9\u7ba1\u9053\uff0c\u5728\u73b0\u5b9e\u73af\u5883\u4e2d\u8106\u5f31\u4e14\u96be\u4ee5\u5b9e\u65f6\u89c4\u5212\u811a\u6b65\u3002", "method": "\u91c7\u7528\u5206\u5c42\u63a7\u5236\u6846\u67b6\uff0c\u9ad8\u5c42\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u751f\u6210\u811a\u6b65\u547d\u4ee4\uff0c\u4f4e\u5c42\u8ddf\u8e2a\u8f68\u8ff9\uff1b\u5229\u7528\u89d2\u52a8\u91cf\u7ebf\u6027\u5012\u7acb\u6446\u6a21\u578b\u7b80\u5316\u72b6\u6001\u8868\u793a\u3002", "result": "\u5728\u4eff\u771f\u548c\u786c\u4ef6\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u5728\u4e0d\u540c\u5730\u5f62\u6761\u4ef6\u4e0b\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u53cc\u8db3\u673a\u5668\u4eba\u5728\u590d\u6742\u5730\u5f62\u4e2d\u7684\u5bfc\u822a\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u4ecd\u5b58\u5728\u6311\u6218\u3002"}}
{"id": "2508.06859", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06859", "abs": "https://arxiv.org/abs/2508.06859", "authors": ["Shuo Tang", "Jian Xu", "Jiadong Zhang", "Yi Chen", "Qizhao Jin", "Lingdong Shen", "Chenglin Liu", "Shiming Xiang"], "title": "MeteorPred: A Meteorological Multimodal Large Model and Dataset for Severe Weather Event Prediction", "comment": null, "summary": "Timely and accurate severe weather warnings are critical for disaster\nmitigation. However, current forecasting systems remain heavily reliant on\nmanual expert interpretation, introducing subjectivity and significant\noperational burdens. With the rapid development of AI technologies, the\nend-to-end \"AI weather station\" is gradually emerging as a new trend in\npredicting severe weather events. Three core challenges impede the development\nof end-to-end AI severe weather system: (1) scarcity of severe weather event\nsamples; (2) imperfect alignment between high-dimensional meteorological data\nand textual warnings; (3) existing multimodal language models are unable to\nhandle high-dimensional meteorological data and struggle to fully capture the\ncomplex dependencies across temporal sequences, vertical pressure levels, and\nspatial dimensions. To address these challenges, we introduce MP-Bench, the\nfirst large-scale temporal multimodal dataset for severe weather events\nprediction, comprising 421,363 pairs of raw multi-year meteorological data and\ncorresponding text caption, covering a wide range of severe weather scenarios\nacross China. On top of this dataset, we develop a meteorology multimodal large\nmodel (MMLM) that directly ingests 4D meteorological inputs. In addition, it is\ndesigned to accommodate the unique characteristics of 4D meteorological data\nflow, incorporating three plug-and-play adaptive fusion modules that enable\ndynamic feature extraction and integration across temporal sequences, vertical\npressure layers, and spatial dimensions. Extensive experiments on MP-Bench\ndemonstrate that MMLM performs exceptionally well across multiple tasks,\nhighlighting its effectiveness in severe weather understanding and marking a\nkey step toward realizing automated, AI-driven weather forecasting systems. Our\nsource code and dataset will be made publicly available.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eAI\u7684\u7aef\u5230\u7aef\u5929\u6c14\u9884\u8b66\u7cfb\u7edfMP-Bench\u548c\u6c14\u8c61\u591a\u6a21\u6001\u5927\u6a21\u578b\uff08MMLM\uff09\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7cfb\u7edf\u4f9d\u8d56\u4eba\u5de5\u3001\u6570\u636e\u5bf9\u9f50\u4e0d\u5b8c\u5584\u7b49\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u5f53\u524d\u5929\u6c14\u9884\u8b66\u7cfb\u7edf\u4f9d\u8d56\u4e13\u5bb6\u624b\u52a8\u89e3\u91ca\uff0c\u5b58\u5728\u4e3b\u89c2\u6027\u548c\u64cd\u4f5c\u8d1f\u62c5\u3002AI\u6280\u672f\u7684\u53d1\u5c55\u4e3a\u81ea\u52a8\u5316\u5929\u6c14\u9884\u6d4b\u63d0\u4f9b\u4e86\u65b0\u53ef\u80fd\uff0c\u4f46\u9762\u4e34\u6570\u636e\u7a00\u7f3a\u3001\u9ad8\u7ef4\u6c14\u8c61\u6570\u636e\u4e0e\u6587\u672c\u5bf9\u9f50\u56f0\u96be\u7b49\u6311\u6218\u3002", "method": "\u63d0\u51faMP-Bench\u6570\u636e\u96c6\u548cMMLM\u6a21\u578b\uff0c\u76f4\u63a5\u5904\u74064D\u6c14\u8c61\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u81ea\u9002\u5e94\u878d\u5408\u6a21\u5757\u52a8\u6001\u63d0\u53d6\u548c\u6574\u5408\u7279\u5f81\u3002", "result": "\u5b9e\u9a8c\u8868\u660eMMLM\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728\u5929\u6c14\u9884\u6d4b\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "MMLM\u4e3a\u81ea\u52a8\u5316AI\u5929\u6c14\u9884\u6d4b\u7cfb\u7edf\u8fc8\u51fa\u4e86\u5173\u952e\u4e00\u6b65\uff0c\u672a\u6765\u5c06\u516c\u5f00\u6e90\u4ee3\u7801\u548c\u6570\u636e\u96c6\u3002"}}
{"id": "2508.06804", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06804", "abs": "https://arxiv.org/abs/2508.06804", "authors": ["Shu-Ang Yu", "Feng Gao", "Yi Wu", "Chao Yu", "Yu Wang"], "title": "D3P: Dynamic Denoising Diffusion Policy via Reinforcement Learning", "comment": null, "summary": "Diffusion policies excel at learning complex action distributions for robotic\nvisuomotor tasks, yet their iterative denoising process poses a major\nbottleneck for real-time deployment. Existing acceleration methods apply a\nfixed number of denoising steps per action, implicitly treating all actions as\nequally important. However, our experiments reveal that robotic tasks often\ncontain a mix of \\emph{crucial} and \\emph{routine} actions, which differ in\ntheir impact on task success. Motivated by this finding, we propose\n\\textbf{D}ynamic \\textbf{D}enoising \\textbf{D}iffusion \\textbf{P}olicy\n\\textbf{(D3P)}, a diffusion-based policy that adaptively allocates denoising\nsteps across actions at test time. D3P uses a lightweight, state-aware adaptor\nto allocate the optimal number of denoising steps for each action. We jointly\noptimize the adaptor and base diffusion policy via reinforcement learning to\nbalance task performance and inference efficiency. On simulated tasks, D3P\nachieves an averaged 2.2$\\times$ inference speed-up over baselines without\ndegrading success. Furthermore, we demonstrate D3P's effectiveness on a\nphysical robot, achieving a 1.9$\\times$ acceleration over the baseline.", "AI": {"tldr": "D3P\u662f\u4e00\u79cd\u52a8\u6001\u53bb\u566a\u6269\u6563\u7b56\u7565\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5206\u914d\u53bb\u566a\u6b65\u9aa4\u63d0\u5347\u673a\u5668\u4eba\u4efb\u52a1\u7684\u5b9e\u65f6\u6027\u80fd\u3002", "motivation": "\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u5173\u952e\u52a8\u4f5c\u548c\u5e38\u89c4\u52a8\u4f5c\u5bf9\u4efb\u52a1\u6210\u529f\u7684\u5f71\u54cd\u4e0d\u540c\uff0c\u56fa\u5b9a\u53bb\u566a\u6b65\u9aa4\u6548\u7387\u4f4e\u3002", "method": "\u63d0\u51faD3P\uff0c\u4f7f\u7528\u8f7b\u91cf\u7ea7\u72b6\u6001\u611f\u77e5\u9002\u914d\u5668\u52a8\u6001\u5206\u914d\u53bb\u566a\u6b65\u9aa4\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8054\u5408\u4f18\u5316\u3002", "result": "\u5728\u6a21\u62df\u4efb\u52a1\u4e2d\u5b9e\u73b02.2\u500d\u52a0\u901f\uff0c\u7269\u7406\u673a\u5668\u4eba\u4e0a\u5b9e\u73b01.9\u500d\u52a0\u901f\uff0c\u4e14\u4e0d\u5f71\u54cd\u4efb\u52a1\u6210\u529f\u7387\u3002", "conclusion": "D3P\u663e\u8457\u63d0\u5347\u6269\u6563\u7b56\u7565\u7684\u5b9e\u65f6\u6027\u80fd\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u673a\u5668\u4eba\u4efb\u52a1\u3002"}}
{"id": "2508.06894", "categories": ["cs.AI", "cs.LG", "68T05"], "pdf": "https://arxiv.org/pdf/2508.06894", "abs": "https://arxiv.org/abs/2508.06894", "authors": ["Giovanni Varricchione", "Toryn Q. Klassen", "Natasha Alechina", "Mehdi Dastani", "Brian Logan", "Sheila A. McIlraith"], "title": "Pushdown Reward Machines for Reinforcement Learning", "comment": null, "summary": "Reward machines (RMs) are automata structures that encode (non-Markovian)\nreward functions for reinforcement learning (RL). RMs can reward any behaviour\nrepresentable in regular languages and, when paired with RL algorithms that\nexploit RM structure, have been shown to significantly improve sample\nefficiency in many domains. In this work, we present pushdown reward machines\n(pdRMs), an extension of reward machines based on deterministic pushdown\nautomata. pdRMs can recognize and reward temporally extended behaviours\nrepresentable in deterministic context-free languages, making them more\nexpressive than reward machines. We introduce two variants of pdRM-based\npolicies, one which has access to the entire stack of the pdRM, and one which\ncan only access the top $k$ symbols (for a given constant $k$) of the stack. We\npropose a procedure to check when the two kinds of policies (for a given\nenvironment, pdRM, and constant $k$) achieve the same optimal expected reward.\nWe then provide theoretical results establishing the expressive power of pdRMs,\nand space complexity results about the proposed learning problems. Finally, we\nprovide experimental results showing how agents can be trained to perform tasks\nrepresentable in deterministic context-free languages using pdRMs.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u786e\u5b9a\u6027\u4e0b\u63a8\u81ea\u52a8\u673a\u7684\u6269\u5c55\u5956\u52b1\u673a\u5668\uff08pdRMs\uff09\uff0c\u80fd\u591f\u8bc6\u522b\u548c\u5956\u52b1\u786e\u5b9a\u6027\u4e0a\u4e0b\u6587\u65e0\u5173\u8bed\u8a00\u8868\u793a\u7684\u884c\u4e3a\uff0c\u6bd4\u4f20\u7edf\u5956\u52b1\u673a\u5668\u66f4\u5177\u8868\u8fbe\u529b\u3002", "motivation": "\u4f20\u7edf\u5956\u52b1\u673a\u5668\uff08RMs\uff09\u53ea\u80fd\u5904\u7406\u6b63\u5219\u8bed\u8a00\u8868\u793a\u7684\u884c\u4e3a\uff0c\u800cpdRMs\u901a\u8fc7\u5f15\u5165\u4e0b\u63a8\u81ea\u52a8\u673a\u7ed3\u6784\uff0c\u6269\u5c55\u4e86\u5bf9\u66f4\u590d\u6742\u884c\u4e3a\u7684\u8868\u8fbe\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e24\u79cd\u57fa\u4e8epdRM\u7684\u7b56\u7565\uff1a\u4e00\u79cd\u53ef\u4ee5\u8bbf\u95ee\u6574\u4e2a\u5806\u6808\uff0c\u53e6\u4e00\u79cd\u53ea\u80fd\u8bbf\u95ee\u5806\u6808\u9876\u90e8\u7684k\u4e2a\u7b26\u53f7\u3002\u5e76\u63d0\u4f9b\u4e86\u68c0\u67e5\u4e24\u79cd\u7b56\u7565\u6700\u4f18\u6027\u7684\u65b9\u6cd5\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660epdRMs\u5177\u6709\u66f4\u5f3a\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u5728\u5b9e\u9645\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "pdRMs\u4e3a\u5f3a\u5316\u5b66\u4e60\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u5de5\u5177\uff0c\u80fd\u591f\u5904\u7406\u66f4\u590d\u6742\u7684\u4efb\u52a1\u3002"}}
{"id": "2508.06921", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06921", "abs": "https://arxiv.org/abs/2508.06921", "authors": ["Zhongyu Chen", "Chenyang Li", "Xuesong Li", "Dianye Huang", "Zhongliang Jiang", "Stefanie Speidel", "Xiangyu Chu", "K. W. Samuel Au"], "title": "Vibration-Based Energy Metric for Restoring Needle Alignment in Autonomous Robotic Ultrasound", "comment": null, "summary": "Precise needle alignment is essential for percutaneous needle insertion in\nrobotic ultrasound-guided procedures. However, inherent challenges such as\nspeckle noise, needle-like artifacts, and low image resolution make robust\nneedle detection difficult, particularly when visibility is reduced or lost. In\nthis paper, we propose a method to restore needle alignment when the ultrasound\nimaging plane and the needle insertion plane are misaligned. Unlike many\nexisting approaches that rely heavily on needle visibility in ultrasound\nimages, our method uses a more robust feature by periodically vibrating the\nneedle using a mechanical system. Specifically, we propose a vibration-based\nenergy metric that remains effective even when the needle is fully out of\nplane. Using this metric, we develop a control strategy to reposition the\nultrasound probe in response to misalignments between the imaging plane and the\nneedle insertion plane in both translation and rotation. Experiments conducted\non ex-vivo porcine tissue samples using a dual-arm robotic ultrasound-guided\nneedle insertion system demonstrate the effectiveness of the proposed approach.\nThe experimental results show the translational error of 0.41$\\pm$0.27 mm and\nthe rotational error of 0.51$\\pm$0.19 degrees.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u632f\u52a8\u7684\u80fd\u91cf\u5ea6\u91cf\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u8d85\u58f0\u5f15\u5bfc\u9488\u63d2\u5165\u8fc7\u7a0b\u4e2d\u6062\u590d\u9488\u7684\u5bf9\u9f50\uff0c\u5373\u4f7f\u9488\u5b8c\u5168\u8131\u79bb\u5e73\u9762\u4e5f\u80fd\u6709\u6548\u5de5\u4f5c\u3002", "motivation": "\u5728\u8d85\u58f0\u5f15\u5bfc\u9488\u63d2\u5165\u8fc7\u7a0b\u4e2d\uff0c\u9488\u7684\u5bf9\u9f50\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u9488\u5728\u8d85\u58f0\u56fe\u50cf\u4e2d\u7684\u53ef\u89c1\u6027\uff0c\u800c\u5b9e\u9645\u4e2d\u5e38\u56e0\u566a\u58f0\u3001\u4f2a\u5f71\u6216\u4f4e\u5206\u8fa8\u7387\u5bfc\u81f4\u9488\u96be\u4ee5\u68c0\u6d4b\u3002", "method": "\u901a\u8fc7\u673a\u68b0\u7cfb\u7edf\u5468\u671f\u6027\u632f\u52a8\u9488\uff0c\u63d0\u51fa\u4e00\u79cd\u632f\u52a8\u80fd\u91cf\u5ea6\u91cf\uff0c\u5e76\u5f00\u53d1\u63a7\u5236\u7b56\u7565\u4ee5\u8c03\u6574\u8d85\u58f0\u63a2\u5934\u4f4d\u7f6e\uff0c\u5e94\u5bf9\u6210\u50cf\u5e73\u9762\u4e0e\u9488\u63d2\u5165\u5e73\u9762\u7684\u9519\u4f4d\u3002", "result": "\u5728\u79bb\u4f53\u732a\u7ec4\u7ec7\u5b9e\u9a8c\u4e2d\uff0c\u5e73\u79fb\u8bef\u5dee\u4e3a0.41\u00b10.27\u6beb\u7c73\uff0c\u65cb\u8f6c\u8bef\u5dee\u4e3a0.51\u00b10.19\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u9488\u4e0d\u53ef\u89c1\u65f6\u4ecd\u80fd\u6709\u6548\u6062\u590d\u5bf9\u9f50\uff0c\u63d0\u9ad8\u4e86\u8d85\u58f0\u5f15\u5bfc\u9488\u63d2\u5165\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.06899", "categories": ["cs.AI", "cs.DM"], "pdf": "https://arxiv.org/pdf/2508.06899", "abs": "https://arxiv.org/abs/2508.06899", "authors": ["Yanchen Deng", "Xinrun Wang", "Bo An"], "title": "GDBA Revisited: Unleashing the Power of Guided Local Search for Distributed Constraint Optimization", "comment": null, "summary": "Local search is an important class of incomplete algorithms for solving\nDistributed Constraint Optimization Problems (DCOPs) but it often converges to\npoor local optima. While GDBA provides a comprehensive rule set to escape\npremature convergence, its empirical benefits remain marginal on general-valued\nproblems. In this work, we systematically examine GDBA and identify three\nfactors that potentially lead to its inferior performance, i.e.,\nover-aggressive constraint violation conditions, unbounded penalty\naccumulation, and uncoordinated penalty updates. To address these issues, we\npropose Distributed Guided Local Search (DGLS), a novel GLS framework for DCOPs\nthat incorporates an adaptive violation condition to selectively penalize\nconstraints with high cost, a penalty evaporation mechanism to control the\nmagnitude of penalization, and a synchronization scheme for coordinated penalty\nupdates. We theoretically show that the penalty values are bounded, and agents\nplay a potential game in our DGLS. Our extensive empirical results on various\nstandard benchmarks demonstrate the great superiority of DGLS over\nstate-of-the-art baselines. Particularly, compared to Damped Max-sum with high\ndamping factors (e.g., 0.7 or 0.9), our DGLS achieves competitive performance\non general-valued problems, and outperforms it by significant margins\n(\\textbf{3.77\\%--66.3\\%}) on structured problems in terms of anytime results.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u5206\u5e03\u5f0f\u7ea6\u675f\u4f18\u5316\u95ee\u9898\uff08DCOP\uff09\u5c40\u90e8\u641c\u7d22\u7b97\u6cd5DGLS\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u7ea6\u675f\u8fdd\u53cd\u6761\u4ef6\u3001\u60e9\u7f5a\u84b8\u53d1\u673a\u5236\u548c\u540c\u6b65\u66f4\u65b0\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709GDBA\u7b97\u6cd5\u5728\u89e3\u51b3DCOP\u65f6\u5bb9\u6613\u9677\u5165\u5c40\u90e8\u6700\u4f18\u4e14\u6027\u80fd\u6709\u9650\uff0c\u8bba\u6587\u65e8\u5728\u89e3\u51b3\u5176\u4e09\u4e2a\u5173\u952e\u95ee\u9898\uff1a\u8fc7\u5ea6\u6fc0\u8fdb\u7684\u7ea6\u675f\u8fdd\u53cd\u6761\u4ef6\u3001\u65e0\u9650\u5236\u7684\u60e9\u7f5a\u79ef\u7d2f\u548c\u4e0d\u534f\u8c03\u7684\u60e9\u7f5a\u66f4\u65b0\u3002", "method": "\u63d0\u51faDGLS\u6846\u67b6\uff0c\u5305\u62ec\u81ea\u9002\u5e94\u7ea6\u675f\u8fdd\u53cd\u6761\u4ef6\u9009\u62e9\u9ad8\u6210\u672c\u7ea6\u675f\u3001\u60e9\u7f5a\u84b8\u53d1\u673a\u5236\u63a7\u5236\u60e9\u7f5a\u5e45\u5ea6\uff0c\u4ee5\u53ca\u540c\u6b65\u65b9\u6848\u534f\u8c03\u60e9\u7f5a\u66f4\u65b0\u3002", "result": "\u7406\u8bba\u8bc1\u660eDGLS\u7684\u60e9\u7f5a\u503c\u6709\u754c\u4e14\u4ee3\u7406\u53c2\u4e0e\u6f5c\u5728\u535a\u5f08\uff0c\u5b9e\u9a8c\u8868\u660eDGLS\u5728\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u7ed3\u6784\u5316\u95ee\u9898\u4e0a\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "DGLS\u901a\u8fc7\u7cfb\u7edf\u6027\u6539\u8fdb\u663e\u8457\u63d0\u5347\u4e86DCOP\u5c40\u90e8\u641c\u7d22\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u7ed3\u6784\u5316\u95ee\u9898\u4e0a\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2508.06969", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06969", "abs": "https://arxiv.org/abs/2508.06969", "authors": ["Bingkun Huang", "Evgeniy Kotov", "Arkady Yuschenko"], "title": "Manipulator for people with limited abilities", "comment": "105 pages, in Russian language", "summary": "The topic of this final qualification work was chosen due to the importance\nof developing robotic systems designed to assist people with disabilities.\nAdvances in robotics and automation technologies have opened up new prospects\nfor creating devices that can significantly improve the quality of life for\nthese people. In this context, designing a robotic hand with a control system\nadapted to the needs of people with disabilities is a major scientific and\npractical challenge. This work addresses the problem of developing and\nmanufacturing a four-degree-of-freedom robotic hand suitable for practical\nmanipulation. Addressing this issue requires a comprehensive approach,\nencompassing the design of the hand's mechanical structure, the development of\nits control system, and its integration with a technical vision system and\nsoftware based on the Robot Operating System (ROS).", "AI": {"tldr": "\u5f00\u53d1\u4e00\u79cd\u56db\u81ea\u7531\u5ea6\u673a\u68b0\u624b\uff0c\u7528\u4e8e\u8f85\u52a9\u6b8b\u75be\u4eba\uff0c\u7ed3\u5408\u673a\u68b0\u8bbe\u8ba1\u3001\u63a7\u5236\u7cfb\u7edf\u53caROS\u8f6f\u4ef6\u96c6\u6210\u3002", "motivation": "\u673a\u5668\u4eba\u6280\u672f\u8fdb\u6b65\u4e3a\u6b8b\u75be\u4eba\u8f85\u52a9\u8bbe\u5907\u63d0\u4f9b\u4e86\u65b0\u53ef\u80fd\uff0c\u63d0\u5347\u5176\u751f\u6d3b\u8d28\u91cf\u3002", "method": "\u8bbe\u8ba1\u673a\u68b0\u7ed3\u6784\u3001\u5f00\u53d1\u63a7\u5236\u7cfb\u7edf\uff0c\u5e76\u96c6\u6210\u6280\u672f\u89c6\u89c9\u7cfb\u7edf\u4e0eROS\u8f6f\u4ef6\u3002", "result": "\u6210\u529f\u5f00\u53d1\u51fa\u9002\u7528\u4e8e\u5b9e\u9645\u64cd\u4f5c\u7684\u673a\u68b0\u624b\u539f\u578b\u3002", "conclusion": "\u8be5\u673a\u68b0\u624b\u8bbe\u8ba1\u4e3a\u6b8b\u75be\u4eba\u8f85\u52a9\u8bbe\u5907\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u5177\u6709\u79d1\u5b66\u548c\u5b9e\u8df5\u610f\u4e49\u3002"}}
{"id": "2508.06931", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06931", "abs": "https://arxiv.org/abs/2508.06931", "authors": ["Wangyue Lu", "Lun Du", "Sirui Li", "Ke Weng", "Haozhe Sun", "Hengyu Liu", "Minghe Yu", "Tiancheng Zhang", "Ge Yu"], "title": "Automated Formalization via Conceptual Retrieval-Augmented LLMs", "comment": null, "summary": "Interactive theorem provers (ITPs) require manual formalization, which is\nlabor-intensive and demands expert knowledge. While automated formalization\noffers a potential solution, it faces two major challenges: model hallucination\n(e.g., undefined predicates, symbol misuse, and version incompatibility) and\nthe semantic gap caused by ambiguous or missing premises in natural language\ndescriptions. To address these issues, we propose CRAMF, a Concept-driven\nRetrieval-Augmented Mathematical Formalization framework. CRAMF enhances\nLLM-based autoformalization by retrieving formal definitions of core\nmathematical concepts, providing contextual grounding during code generation.\nHowever, applying retrieval-augmented generation (RAG) in this setting is\nnon-trivial due to the lack of structured knowledge bases, the polymorphic\nnature of mathematical concepts, and the high precision required in formal\nretrieval. We introduce a framework for automatically constructing a\nconcept-definition knowledge base from Mathlib4, the standard mathematical\nlibrary for the Lean 4 theorem prover, indexing over 26,000 formal definitions\nand 1,000+ core mathematical concepts. To address conceptual polymorphism, we\npropose contextual query augmentation with domain- and application-level\nsignals. In addition, we design a dual-channel hybrid retrieval strategy with\nreranking to ensure accurate and relevant definition retrieval. Experiments on\nminiF2F, ProofNet, and our newly proposed AdvancedMath benchmark show that\nCRAMF can be seamlessly integrated into LLM-based autoformalizers, yielding\nconsistent improvements in translation accuracy, achieving up to 62.1% and an\naverage of 29.9% relative improvement.", "AI": {"tldr": "CRAMF\u6846\u67b6\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u7684\u6570\u5b66\u5f62\u5f0f\u5316\uff0c\u89e3\u51b3\u4e86\u81ea\u52a8\u5f62\u5f0f\u5316\u4e2d\u7684\u6a21\u578b\u5e7b\u89c9\u548c\u8bed\u4e49\u9e3f\u6c9f\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86LLM\u7684\u7ffb\u8bd1\u51c6\u786e\u6027\u3002", "motivation": "\u4ea4\u4e92\u5f0f\u5b9a\u7406\u8bc1\u660e\u5668\u7684\u624b\u52a8\u5f62\u5f0f\u5316\u8017\u65f6\u4e14\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\uff0c\u81ea\u52a8\u5f62\u5f0f\u5316\u9762\u4e34\u6a21\u578b\u5e7b\u89c9\u548c\u8bed\u4e49\u9e3f\u6c9f\u7684\u6311\u6218\u3002", "method": "\u63d0\u51faCRAMF\u6846\u67b6\uff0c\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6280\u672f\uff0c\u4eceMathlib4\u6784\u5efa\u6982\u5ff5\u5b9a\u4e49\u77e5\u8bc6\u5e93\uff0c\u5e76\u8bbe\u8ba1\u53cc\u901a\u9053\u6df7\u5408\u68c0\u7d22\u7b56\u7565\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cCRAMF\u663e\u8457\u63d0\u5347\u4e86\u7ffb\u8bd1\u51c6\u786e\u6027\uff0c\u6700\u9ad8\u76f8\u5bf9\u6539\u8fdb\u8fbe62.1%\uff0c\u5e73\u574729.9%\u3002", "conclusion": "CRAMF\u4e3a\u81ea\u52a8\u5f62\u5f0f\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u5728\u5904\u7406\u6570\u5b66\u6982\u5ff5\u7684\u591a\u6001\u6027\u548c\u7cbe\u786e\u68c0\u7d22\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2508.06990", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06990", "abs": "https://arxiv.org/abs/2508.06990", "authors": ["Yue Hu", "Junzhe Wu", "Ruihan Xu", "Hang Liu", "Avery Xi", "Henry X. Liu", "Ram Vasudevan", "Maani Ghaffari"], "title": "Imaginative World Modeling with Scene Graphs for Embodied Agent Navigation", "comment": "23 pages", "summary": "Semantic navigation requires an agent to navigate toward a specified target\nin an unseen environment. Employing an imaginative navigation strategy that\npredicts future scenes before taking action, can empower the agent to find\ntarget faster. Inspired by this idea, we propose SGImagineNav, a novel\nimaginative navigation framework that leverages symbolic world modeling to\nproactively build a global environmental representation. SGImagineNav maintains\nan evolving hierarchical scene graphs and uses large language models to predict\nand explore unseen parts of the environment. While existing methods solely\nrelying on past observations, this imaginative scene graph provides richer\nsemantic context, enabling the agent to proactively estimate target locations.\nBuilding upon this, SGImagineNav adopts an adaptive navigation strategy that\nexploits semantic shortcuts when promising and explores unknown areas otherwise\nto gather additional context. This strategy continuously expands the known\nenvironment and accumulates valuable semantic contexts, ultimately guiding the\nagent toward the target. SGImagineNav is evaluated in both real-world scenarios\nand simulation benchmarks. SGImagineNav consistently outperforms previous\nmethods, improving success rate to 65.4 and 66.8 on HM3D and HSSD, and\ndemonstrating cross-floor and cross-room navigation in real-world environments,\nunderscoring its effectiveness and generalizability.", "AI": {"tldr": "SGImagineNav\u662f\u4e00\u79cd\u65b0\u9896\u7684\u5bfc\u822a\u6846\u67b6\uff0c\u901a\u8fc7\u7b26\u53f7\u5316\u4e16\u754c\u5efa\u6a21\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9884\u6d4b\u672a\u6765\u573a\u666f\uff0c\u63d0\u5347\u8bed\u4e49\u5bfc\u822a\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4ec5\u4f9d\u8d56\u8fc7\u53bb\u89c2\u5bdf\uff0c\u7f3a\u4e4f\u5bf9\u672a\u6765\u573a\u666f\u7684\u9884\u6d4b\u80fd\u529b\uff0c\u9650\u5236\u4e86\u5bfc\u822a\u6548\u7387\u3002SGImagineNav\u65e8\u5728\u901a\u8fc7\u4e3b\u52a8\u6784\u5efa\u5168\u5c40\u73af\u5883\u8868\u793a\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "SGImagineNav\u91c7\u7528\u5206\u5c42\u573a\u666f\u56fe\u5efa\u6a21\uff0c\u7ed3\u5408\u5927\u578b\u8bed\u8a00\u6a21\u578b\u9884\u6d4b\u672a\u63a2\u7d22\u533a\u57df\uff0c\u5e76\u81ea\u9002\u5e94\u5730\u9009\u62e9\u8bed\u4e49\u6377\u5f84\u6216\u63a2\u7d22\u672a\u77e5\u533a\u57df\u3002", "result": "\u5728HM3D\u548cHSSD\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSGImagineNav\u6210\u529f\u7387\u8fbe\u523065.4%\u548c66.8%\uff0c\u5e76\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u5c55\u793a\u4e86\u8de8\u697c\u5c42\u548c\u8de8\u623f\u95f4\u5bfc\u822a\u80fd\u529b\u3002", "conclusion": "SGImagineNav\u901a\u8fc7\u4e3b\u52a8\u9884\u6d4b\u548c\u81ea\u9002\u5e94\u7b56\u7565\u663e\u8457\u63d0\u5347\u4e86\u8bed\u4e49\u5bfc\u822a\u7684\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.06939", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06939", "abs": "https://arxiv.org/abs/2508.06939", "authors": ["Hiba Najjar", "Deepak Pathak", "Marlon Nuske", "Andreas Dengel"], "title": "Intrinsic Explainability of Multimodal Learning for Crop Yield Prediction", "comment": null, "summary": "Multimodal learning enables various machine learning tasks to benefit from\ndiverse data sources, effectively mimicking the interplay of different factors\nin real-world applications, particularly in agriculture. While the\nheterogeneous nature of involved data modalities may necessitate the design of\ncomplex architectures, the model interpretability is often overlooked. In this\nstudy, we leverage the intrinsic explainability of Transformer-based models to\nexplain multimodal learning networks, focusing on the task of crop yield\nprediction at the subfield level. The large datasets used cover various crops,\nregions, and years, and include four different input modalities: multispectral\nsatellite and weather time series, terrain elevation maps and soil properties.\nBased on the self-attention mechanism, we estimate feature attributions using\ntwo methods, namely the Attention Rollout (AR) and Generic Attention (GA), and\nevaluate their performance against Shapley-based model-agnostic estimations,\nShapley Value Sampling (SVS). Additionally, we propose the Weighted Modality\nActivation (WMA) method to assess modality attributions and compare it with SVS\nattributions. Our findings indicate that Transformer-based models outperform\nother architectures, specifically convolutional and recurrent networks,\nachieving R2 scores that are higher by 0.10 and 0.04 at the subfield and field\nlevels, respectively. AR is shown to provide more robust and reliable temporal\nattributions, as confirmed through qualitative and quantitative evaluation,\ncompared to GA and SVS values. Information about crop phenology stages was\nleveraged to interpret the explanation results in the light of established\nagronomic knowledge. Furthermore, modality attributions revealed varying\npatterns across the two methods compared.[...]", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u591a\u6a21\u6001\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f5c\u7269\u4ea7\u91cf\u9884\u6d4b\uff0c\u5e76\u5229\u7528\u81ea\u6ce8\u610f\u529b\u673a\u5236\u89e3\u91ca\u6a21\u578b\uff0c\u6bd4\u8f83\u4e86\u4e0d\u540c\u7279\u5f81\u548c\u6a21\u6001\u5f52\u56e0\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u591a\u6a21\u6001\u5b66\u4e60\u5728\u519c\u4e1a\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u5e38\u88ab\u5ffd\u89c6\u3002\u672c\u7814\u7a76\u65e8\u5728\u5229\u7528Transformer\u7684\u56fa\u6709\u53ef\u89e3\u91ca\u6027\uff0c\u63d0\u5347\u4f5c\u7269\u4ea7\u91cf\u9884\u6d4b\u7684\u900f\u660e\u5ea6\u548c\u6027\u80fd\u3002", "method": "\u91c7\u7528Transformer\u6a21\u578b\u5904\u7406\u591a\u6a21\u6001\u6570\u636e\uff08\u536b\u661f\u3001\u6c14\u8c61\u3001\u5730\u5f62\u548c\u571f\u58e4\uff09\uff0c\u63d0\u51faAttention Rollout\u548cGeneric Attention\u4e24\u79cd\u7279\u5f81\u5f52\u56e0\u65b9\u6cd5\uff0c\u5e76\u4e0eShapley Value Sampling\u5bf9\u6bd4\u3002\u8fd8\u63d0\u51faWeighted Modality Activation\u8bc4\u4f30\u6a21\u6001\u5f52\u56e0\u3002", "result": "Transformer\u6a21\u578b\u5728\u5b50\u7530\u5757\u548c\u7530\u5757\u7ea7\u522b\u7684\u9884\u6d4b\u6027\u80fd\u4f18\u4e8e\u5377\u79ef\u548c\u5faa\u73af\u7f51\u7edc\uff08R2\u5206\u522b\u63d0\u9ad80.10\u548c0.04\uff09\u3002Attention Rollout\u5728\u65f6\u95f4\u5f52\u56e0\u4e0a\u66f4\u7a33\u5065\uff0c\u6a21\u6001\u5f52\u56e0\u65b9\u6cd5\u663e\u793a\u4e0d\u540c\u6a21\u5f0f\u3002", "conclusion": "Transformer\u6a21\u578b\u5728\u591a\u6a21\u6001\u4f5c\u7269\u4ea7\u91cf\u9884\u6d4b\u4e2d\u8868\u73b0\u4f18\u8d8a\uff0c\u4e14\u81ea\u6ce8\u610f\u529b\u673a\u5236\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u6027\uff0c\u4e3a\u519c\u4e1a\u51b3\u7b56\u63d0\u4f9b\u4e86\u900f\u660e\u652f\u6301\u3002"}}
{"id": "2508.07003", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07003", "abs": "https://arxiv.org/abs/2508.07003", "authors": ["Siyu Chen", "Shenghai Yuan", "Thien-Minh Nguyen", "Zhuyu Huang", "Chenyang Shi", "Jin Jing", "Lihua Xie"], "title": "EGS-SLAM: RGB-D Gaussian Splatting SLAM with Events", "comment": "Accepted by IEEE RAL", "summary": "Gaussian Splatting SLAM (GS-SLAM) offers a notable improvement over\ntraditional SLAM methods, enabling photorealistic 3D reconstruction that\nconventional approaches often struggle to achieve. However, existing GS-SLAM\nsystems perform poorly under persistent and severe motion blur commonly\nencountered in real-world scenarios, leading to significantly degraded tracking\naccuracy and compromised 3D reconstruction quality. To address this limitation,\nwe propose EGS-SLAM, a novel GS-SLAM framework that fuses event data with RGB-D\ninputs to simultaneously reduce motion blur in images and compensate for the\nsparse and discrete nature of event streams, enabling robust tracking and\nhigh-fidelity 3D Gaussian Splatting reconstruction. Specifically, our system\nexplicitly models the camera's continuous trajectory during exposure,\nsupporting event- and blur-aware tracking and mapping on a unified 3D Gaussian\nSplatting scene. Furthermore, we introduce a learnable camera response function\nto align the dynamic ranges of events and images, along with a no-event loss to\nsuppress ringing artifacts during reconstruction. We validate our approach on a\nnew dataset comprising synthetic and real-world sequences with significant\nmotion blur. Extensive experimental results demonstrate that EGS-SLAM\nconsistently outperforms existing GS-SLAM systems in both trajectory accuracy\nand photorealistic 3D Gaussian Splatting reconstruction. The source code will\nbe available at https://github.com/Chensiyu00/EGS-SLAM.", "AI": {"tldr": "EGS-SLAM\u901a\u8fc7\u878d\u5408\u4e8b\u4ef6\u6570\u636e\u548cRGB-D\u8f93\u5165\uff0c\u89e3\u51b3\u4e86GS-SLAM\u5728\u8fd0\u52a8\u6a21\u7cca\u4e0b\u7684\u6027\u80fd\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u8ddf\u8e2a\u7cbe\u5ea6\u548c3D\u91cd\u5efa\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edfGS-SLAM\u5728\u4e25\u91cd\u8fd0\u52a8\u6a21\u7cca\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u5bfc\u81f4\u8ddf\u8e2a\u548c\u91cd\u5efa\u8d28\u91cf\u4e0b\u964d\u3002", "method": "\u63d0\u51faEGS-SLAM\u6846\u67b6\uff0c\u7ed3\u5408\u4e8b\u4ef6\u6570\u636e\u548cRGB-D\u8f93\u5165\uff0c\u5efa\u6a21\u76f8\u673a\u8fde\u7eed\u8f68\u8ff9\uff0c\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u76f8\u673a\u54cd\u5e94\u51fd\u6570\u548c\u65e0\u4e8b\u4ef6\u635f\u5931\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\uff0cEGS-SLAM\u5728\u8f68\u8ff9\u7cbe\u5ea6\u548c3D\u91cd\u5efa\u8d28\u91cf\u4e0a\u4f18\u4e8e\u73b0\u6709GS-SLAM\u7cfb\u7edf\u3002", "conclusion": "EGS-SLAM\u663e\u8457\u63d0\u5347\u4e86\u5728\u8fd0\u52a8\u6a21\u7cca\u573a\u666f\u4e0b\u7684\u6027\u80fd\uff0c\u4ee3\u7801\u5c06\u5f00\u6e90\u3002"}}
{"id": "2508.06950", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06950", "abs": "https://arxiv.org/abs/2508.06950", "authors": ["Sarah Schr\u00f6der", "Thekla Morgenroth", "Ulrike Kuhl", "Valerie Vaquet", "Benjamin Paa\u00dfen"], "title": "Large Language Models Do Not Simulate Human Psychology", "comment": null, "summary": "Large Language Models (LLMs),such as ChatGPT, are increasingly used in\nresearch, ranging from simple writing assistance to complex data annotation\ntasks. Recently, some research has suggested that LLMs may even be able to\nsimulate human psychology and can, hence, replace human participants in\npsychological studies. We caution against this approach. We provide conceptual\narguments against the hypothesis that LLMs simulate human psychology. We then\npresent empiric evidence illustrating our arguments by demonstrating that\nslight changes to wording that correspond to large changes in meaning lead to\nnotable discrepancies between LLMs' and human responses, even for the recent\nCENTAUR model that was specifically fine-tuned on psychological responses.\nAdditionally, different LLMs show very different responses to novel items,\nfurther illustrating their lack of reliability. We conclude that LLMs do not\nsimulate human psychology and recommend that psychological researchers should\ntreat LLMs as useful but fundamentally unreliable tools that need to be\nvalidated against human responses for every new application.", "AI": {"tldr": "\u8bba\u6587\u8b66\u544a\u4e0d\u8981\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u66ff\u4ee3\u4eba\u7c7b\u53c2\u4e0e\u8005\u8fdb\u884c\u5fc3\u7406\u5b66\u7814\u7a76\uff0c\u5e76\u63d0\u4f9b\u4e86\u7406\u8bba\u548c\u5b9e\u8bc1\u8bc1\u636e\u652f\u6301\u5176\u4e0d\u53ef\u9760\u6027\u3002", "motivation": "\u63a2\u8ba8LLM\u662f\u5426\u80fd\u6a21\u62df\u4eba\u7c7b\u5fc3\u7406\u5b66\uff0c\u5e76\u8bc4\u4f30\u5176\u5728\u5fc3\u7406\u5b66\u7814\u7a76\u4e2d\u7684\u9002\u7528\u6027\u3002", "method": "\u901a\u8fc7\u6982\u5ff5\u8bba\u8bc1\u548c\u5b9e\u8bc1\u7814\u7a76\uff08\u5982\u8c03\u6574\u63aa\u8f9e\u5bf9\u6bd4LLM\u4e0e\u4eba\u7c7b\u53cd\u5e94\u5dee\u5f02\uff09\u9a8c\u8bc1LLM\u7684\u4e0d\u53ef\u9760\u6027\u3002", "result": "LLM\u5bf9\u63aa\u8f9e\u654f\u611f\u4e14\u53cd\u5e94\u4e0d\u4e00\u81f4\uff0c\u65e0\u6cd5\u53ef\u9760\u6a21\u62df\u4eba\u7c7b\u5fc3\u7406\u5b66\u3002", "conclusion": "LLM\u4e0d\u80fd\u6a21\u62df\u4eba\u7c7b\u5fc3\u7406\u5b66\uff0c\u5fc3\u7406\u5b66\u7814\u7a76\u9700\u4ee5\u4eba\u7c7b\u53cd\u5e94\u4e3a\u57fa\u51c6\u9a8c\u8bc1LLM\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2508.07033", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07033", "abs": "https://arxiv.org/abs/2508.07033", "authors": ["Shengli Zhou", "Xiangchen Wang", "Jinrui Zhang", "Ruozai Tian", "Rongtao Xu", "Feng Zheng"], "title": "$\\mathcal{P}^3$: Toward Versatile Embodied Agents", "comment": "16 pages, 8 figures", "summary": "Embodied agents have shown promising generalization capabilities across\ndiverse physical environments, making them essential for a wide range of\nreal-world applications. However, building versatile embodied agents poses\ncritical challenges due to three key issues: dynamic environment perception,\nopen-ended tool usage, and complex multi-task planning. Most previous works\nrely solely on feedback from tool agents to perceive environmental changes and\ntask status, which limits adaptability to real-time dynamics, causes error\naccumulation, and restricts tool flexibility. Furthermore, multi-task\nscheduling has received limited attention, primarily due to the inherent\ncomplexity of managing task dependencies and balancing competing priorities in\ndynamic and complex environments. To overcome these challenges, we introduce\n$\\mathcal{P}^3$, a unified framework that integrates real-time perception and\ndynamic scheduling. Specifically, $\\mathcal{P}^3$ enables 1) \\textbf Perceive\nrelevant task information actively from the environment, 2) \\textbf Plug and\nutilize any tool without feedback requirement, and 3) \\textbf Plan multi-task\nexecution based on prioritizing urgent tasks and dynamically adjusting task\norder based on dependencies. Extensive real-world experiments show that our\napproach bridges the gap between benchmarks and practical deployment,\ndelivering highly transferable, general-purpose embodied agents. Code and data\nwill be released soon.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.06960", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06960", "abs": "https://arxiv.org/abs/2508.06960", "authors": ["Keyu Li", "Mohan Jiang", "Dayuan Fu", "Yunze Wu", "Xiangkun Hu", "Dequan Wang", "Pengfei Liu"], "title": "DatasetResearch: Benchmarking Agent Systems for Demand-Driven Dataset Discovery", "comment": null, "summary": "The rapid advancement of large language models has fundamentally shifted the\nbottleneck in AI development from computational power to data availability-with\ncountless valuable datasets remaining hidden across specialized repositories,\nresearch appendices, and domain platforms. As reasoning capabilities and deep\nresearch methodologies continue to evolve, a critical question emerges: can AI\nagents transcend conventional search to systematically discover any dataset\nthat meets specific user requirements, enabling truly autonomous demand-driven\ndata curation? We introduce DatasetResearch, the first comprehensive benchmark\nevaluating AI agents' ability to discover and synthesize datasets from 208\nreal-world demands across knowledge-intensive and reasoning-intensive tasks.\nOur tri-dimensional evaluation framework reveals a stark reality: even advanced\ndeep research systems achieve only 22% score on our challenging\nDatasetResearch-pro subset, exposing the vast gap between current capabilities\nand perfect dataset discovery. Our analysis uncovers a fundamental\ndichotomy-search agents excel at knowledge tasks through retrieval breadth,\nwhile synthesis agents dominate reasoning challenges via structured\ngeneration-yet both catastrophically fail on \"corner cases\" outside existing\ndistributions. These findings establish the first rigorous baseline for dataset\ndiscovery agents and illuminate the path toward AI systems capable of finding\nany dataset in the digital universe. Our benchmark and comprehensive analysis\nprovide the foundation for the next generation of self-improving AI systems and\nare publicly available at https://github.com/GAIR-NLP/DatasetResearch.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86DatasetResearch\u57fa\u51c6\uff0c\u8bc4\u4f30AI\u4ee3\u7406\u5728\u53d1\u73b0\u548c\u5408\u6210\u6570\u636e\u96c6\u65b9\u9762\u7684\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6280\u672f\u4e0e\u5b8c\u7f8e\u6570\u636e\u96c6\u53d1\u73b0\u4e4b\u95f4\u7684\u5de8\u5927\u5dee\u8ddd\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u6570\u636e\u53ef\u7528\u6027\u6210\u4e3aAI\u5f00\u53d1\u7684\u74f6\u9888\uff0c\u5982\u4f55\u8ba9AI\u4ee3\u7406\u81ea\u4e3b\u53d1\u73b0\u7b26\u5408\u9700\u6c42\u7684\u6570\u636e\u96c6\u6210\u4e3a\u5173\u952e\u95ee\u9898\u3002", "method": "\u901a\u8fc7208\u4e2a\u771f\u5b9e\u9700\u6c42\u6784\u5efa\u4e86DatasetResearch\u57fa\u51c6\uff0c\u91c7\u7528\u4e09\u7ef4\u8bc4\u4f30\u6846\u67b6\u5206\u6790AI\u4ee3\u7406\u7684\u8868\u73b0\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u5373\u4f7f\u662f\u5148\u8fdb\u7684\u6df1\u5ea6\u7814\u7a76\u7cfb\u7edf\u5728\u6311\u6218\u6027\u5b50\u96c6\u4e0a\u4ec5\u5f9722\u5206\uff0c\u641c\u7d22\u4ee3\u7406\u548c\u5408\u6210\u4ee3\u7406\u5728\u4e0d\u540c\u4efb\u52a1\u4e0a\u8868\u73b0\u5404\u5f02\uff0c\u4f46\u5728\u6781\u7aef\u6848\u4f8b\u4e2d\u5747\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "\u7814\u7a76\u4e3a\u6570\u636e\u96c6\u53d1\u73b0\u4ee3\u7406\u8bbe\u7acb\u4e86\u9996\u4e2a\u4e25\u683c\u57fa\u51c6\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u81ea\u6539\u8fdbAI\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.07045", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.07045", "abs": "https://arxiv.org/abs/2508.07045", "authors": ["Dennis Benders", "Johannes K\u00f6hler", "Robert Babu\u0161ka", "Javier Alonso-Mora", "Laura Ferranti"], "title": "From Data to Safe Mobile Robot Navigation: An Efficient and Modular Robust MPC Design Pipeline", "comment": "8 pages, 5 figures", "summary": "Model predictive control (MPC) is a powerful strategy for planning and\ncontrol in autonomous mobile robot navigation. However, ensuring safety in\nreal-world deployments remains challenging due to the presence of disturbances\nand measurement noise. Existing approaches often rely on idealized assumptions,\nneglect the impact of noisy measurements, and simply heuristically guess\nunrealistic bounds. In this work, we present an efficient and modular robust\nMPC design pipeline that systematically addresses these limitations. The\npipeline consists of an iterative procedure that leverages closed-loop\nexperimental data to estimate disturbance bounds and synthesize a robust\noutput-feedback MPC scheme. We provide the pipeline in the form of\ndeterministic and reproducible code to synthesize the robust output-feedback\nMPC from data. We empirically demonstrate robust constraint satisfaction and\nrecursive feasibility in quadrotor simulations using Gazebo.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u5757\u5316\u3001\u9ad8\u6548\u7684\u9c81\u68d2MPC\u8bbe\u8ba1\u6d41\u7a0b\uff0c\u901a\u8fc7\u95ed\u73af\u5b9e\u9a8c\u6570\u636e\u4f30\u8ba1\u6270\u52a8\u8fb9\u754c\uff0c\u5e76\u5728\u56db\u65cb\u7ffc\u4eff\u771f\u4e2d\u9a8c\u8bc1\u4e86\u9c81\u68d2\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709MPC\u65b9\u6cd5\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u56e0\u566a\u58f0\u548c\u6270\u52a8\u5bfc\u81f4\u7684\u5b89\u5168\u6027\u95ee\u9898\uff0c\u907f\u514d\u4f9d\u8d56\u7406\u60f3\u5316\u5047\u8bbe\u548c\u542f\u53d1\u5f0f\u8fb9\u754c\u731c\u6d4b\u3002", "method": "\u91c7\u7528\u8fed\u4ee3\u6d41\u7a0b\uff0c\u5229\u7528\u95ed\u73af\u5b9e\u9a8c\u6570\u636e\u4f30\u8ba1\u6270\u52a8\u8fb9\u754c\uff0c\u5e76\u5408\u6210\u9c81\u68d2\u8f93\u51fa\u53cd\u9988MPC\u65b9\u6848\u3002", "result": "\u5728Gazebo\u7684\u56db\u65cb\u7ffc\u4eff\u771f\u4e2d\u5b9e\u73b0\u4e86\u9c81\u68d2\u7ea6\u675f\u6ee1\u8db3\u548c\u9012\u5f52\u53ef\u884c\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86MPC\u5728\u566a\u58f0\u548c\u6270\u52a8\u73af\u5883\u4e2d\u7684\u5b89\u5168\u6027\u95ee\u9898\uff0c\u5177\u6709\u5b9e\u7528\u6027\u548c\u53ef\u91cd\u590d\u6027\u3002"}}
{"id": "2508.06963", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06963", "abs": "https://arxiv.org/abs/2508.06963", "authors": ["Changqing Li", "Tianlin Li", "Xiaohan Zhang", "Aishan Liu", "Li Pan"], "title": "MASteer: Multi-Agent Adaptive Steer Strategy for End-to-End LLM Trustworthiness Repair", "comment": null, "summary": "Large Language Models (LLMs) face persistent and evolving trustworthiness\nissues, motivating developers to seek automated and flexible repair methods\nthat enable convenient deployment across diverse scenarios. Existing repair\nmethods like supervised fine-tuning (SFT) and reinforcement learning with human\nfeedback (RLHF) are costly and slow, while prompt engineering lacks robustness\nand scalability. Representation engineering, which steers model behavior by\ninjecting targeted concept vectors during inference, offers a lightweight,\ntraining-free alternative. However, current approaches depend on manually\ncrafted samples and fixed steering strategies, limiting automation and\nadaptability. To overcome these challenges, we propose MASteer, the first\nend-to-end framework for trustworthiness repair in LLMs based on representation\nengineering. MASteer integrates two core components: AutoTester, a multi-agent\nsystem that generates diverse, high-quality steer samples tailored to developer\nneeds; and AutoRepairer, which constructs adaptive steering strategies with\nanchor vectors for automated, context-aware strategy selection during\ninference. Experiments on standard and customized trustworthiness tasks show\nMASteer consistently outperforms baselines, improving metrics by 15.36% on\nLLaMA-3.1-8B-Chat and 4.21% on Qwen-3-8B-Chat, while maintaining general model\ncapabilities. MASteer demonstrates strong robustness, generalization, and\npractical value for scalable, efficient trustworthiness repair.", "AI": {"tldr": "MASteer\u662f\u4e00\u4e2a\u57fa\u4e8e\u8868\u5f81\u5de5\u7a0b\u7684\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u7528\u4e8e\u4fee\u590d\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u53ef\u4fe1\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u81ea\u52a8\u751f\u6210\u6837\u672c\u548c\u81ea\u9002\u5e94\u7b56\u7565\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u4fee\u590d\u65b9\u6cd5\uff08\u5982SFT\u548cRLHF\uff09\u6210\u672c\u9ad8\u4e14\u6162\uff0c\u800c\u63d0\u793a\u5de5\u7a0b\u7f3a\u4e4f\u9c81\u68d2\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u9700\u8981\u66f4\u8f7b\u91cf\u3001\u81ea\u52a8\u5316\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "MASteer\u7ed3\u5408AutoTester\uff08\u591a\u667a\u80fd\u4f53\u751f\u6210\u6837\u672c\uff09\u548cAutoRepairer\uff08\u81ea\u9002\u5e94\u7b56\u7565\u9009\u62e9\uff09\uff0c\u5b9e\u73b0\u81ea\u52a8\u5316\u4fee\u590d\u3002", "result": "\u5728LLaMA-3.1-8B-Chat\u548cQwen-3-8B-Chat\u4e0a\u5206\u522b\u63d0\u534715.36%\u548c4.21%\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u901a\u7528\u80fd\u529b\u3002", "conclusion": "MASteer\u5c55\u793a\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u53ef\u4fe1\u6027\u4fee\u590d\u80fd\u529b\uff0c\u5177\u6709\u5f3a\u9c81\u68d2\u6027\u548c\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2508.07079", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.07079", "abs": "https://arxiv.org/abs/2508.07079", "authors": ["Mohamed Parvez Aslam", "Bojan Derajic", "Mohamed-Khalil Bouzidi", "Sebastian Bernhard", "Jan Oliver Ringert"], "title": "Model Predictive Control for Crowd Navigation via Learning-Based Trajectory Prediction", "comment": null, "summary": "Safe navigation in pedestrian-rich environments remains a key challenge for\nautonomous robots. This work evaluates the integration of a deep learning-based\nSocial-Implicit (SI) pedestrian trajectory predictor within a Model Predictive\nControl (MPC) framework on the physical Continental Corriere robot. Tested\nacross varied pedestrian densities, the SI-MPC system is compared to a\ntraditional Constant Velocity (CV) model in both open-loop prediction and\nclosed-loop navigation. Results show that SI improves trajectory prediction -\nreducing errors by up to 76% in low-density settings - and enhances safety and\nmotion smoothness in crowded scenes. Moreover, real-world deployment reveals\ndiscrepancies between open-loop metrics and closed-loop performance, as the SI\nmodel yields broader, more cautious predictions. These findings emphasize the\nimportance of system-level evaluation and highlight the SI-MPC framework's\npromise for safer, more adaptive navigation in dynamic, human-populated\nenvironments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u5728\u884c\u4eba\u5bc6\u96c6\u73af\u5883\u4e2d\uff0c\u5c06\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684Social-Implicit\uff08SI\uff09\u884c\u4eba\u8f68\u8ff9\u9884\u6d4b\u5668\u4e0e\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u6846\u67b6\u7ed3\u5408\uff0c\u4ee5\u63d0\u5347\u673a\u5668\u4eba\u5bfc\u822a\u7684\u5b89\u5168\u6027\u548c\u6d41\u7545\u6027\u3002", "motivation": "\u5728\u884c\u4eba\u5bc6\u96c6\u73af\u5883\u4e2d\uff0c\u81ea\u4e3b\u673a\u5668\u4eba\u7684\u5b89\u5168\u5bfc\u822a\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\u3002\u4f20\u7edf\u65b9\u6cd5\uff08\u5982\u6052\u5b9a\u901f\u5ea6\u6a21\u578b\uff09\u5728\u52a8\u6001\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u667a\u80fd\u7684\u9884\u6d4b\u548c\u63a7\u5236\u65b9\u6cd5\u3002", "method": "\u8bba\u6587\u5c06SI\u884c\u4eba\u8f68\u8ff9\u9884\u6d4b\u5668\u96c6\u6210\u5230MPC\u6846\u67b6\u4e2d\uff0c\u5e76\u5728\u7269\u7406\u673a\u5668\u4eba\u4e0a\u6d4b\u8bd5\u3002\u901a\u8fc7\u5bf9\u6bd4SI-MPC\u4e0e\u4f20\u7edfCV\u6a21\u578b\u5728\u5f00\u73af\u9884\u6d4b\u548c\u95ed\u73af\u5bfc\u822a\u4e2d\u7684\u8868\u73b0\uff0c\u8bc4\u4f30\u5176\u6027\u80fd\u3002", "result": "SI\u6a21\u578b\u663e\u8457\u964d\u4f4e\u4e86\u8f68\u8ff9\u9884\u6d4b\u8bef\u5dee\uff08\u4f4e\u5bc6\u5ea6\u73af\u5883\u4e0b\u51cf\u5c1176%\uff09\uff0c\u5e76\u5728\u62e5\u6324\u573a\u666f\u4e2d\u63d0\u5347\u4e86\u5b89\u5168\u6027\u548c\u8fd0\u52a8\u6d41\u7545\u6027\u3002\u5b9e\u9645\u90e8\u7f72\u8fd8\u53d1\u73b0\u5f00\u73af\u6307\u6807\u4e0e\u95ed\u73af\u6027\u80fd\u7684\u5dee\u5f02\u3002", "conclusion": "SI-MPC\u6846\u67b6\u5728\u52a8\u6001\u3001\u4eba\u591a\u7684\u73af\u5883\u4e2d\u5c55\u73b0\u51fa\u66f4\u5b89\u5168\u548c\u81ea\u9002\u5e94\u7684\u5bfc\u822a\u6f5c\u529b\uff0c\u5f3a\u8c03\u4e86\u7cfb\u7edf\u7ea7\u8bc4\u4f30\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2508.06972", "categories": ["cs.AI", "cs.CR", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06972", "abs": "https://arxiv.org/abs/2508.06972", "authors": ["Dan Ivanov", "Tristan Freiberg", "Haruna Isah"], "title": "DSperse: A Framework for Targeted Verification in Zero-Knowledge Machine Learning", "comment": "12 pages, 8 figures, and 10 tables", "summary": "DSperse is a modular framework for distributed machine learning inference\nwith strategic cryptographic verification. Operating within the emerging\nparadigm of distributed zero-knowledge machine learning, DSperse avoids the\nhigh cost and rigidity of full-model circuitization by enabling targeted\nverification of strategically chosen subcomputations. These verifiable\nsegments, or \"slices\", may cover part or all of the inference pipeline, with\nglobal consistency enforced through audit, replication, or economic incentives.\nThis architecture supports a pragmatic form of trust minimization, localizing\nzero-knowledge proofs to the components where they provide the greatest value.\nWe evaluate DSperse using multiple proving systems and report empirical results\non memory usage, runtime, and circuit behavior under sliced and unsliced\nconfigurations. By allowing proof boundaries to align flexibly with the model's\nlogical structure, DSperse supports scalable, targeted verification strategies\nsuited to diverse deployment needs.", "AI": {"tldr": "DSperse\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u5e03\u5f0f\u673a\u5668\u5b66\u4e60\u63a8\u7406\uff0c\u901a\u8fc7\u6218\u7565\u6027\u7684\u5bc6\u7801\u5b66\u9a8c\u8bc1\u5b9e\u73b0\u9ad8\u6548\u548c\u7075\u6d3b\u6027\u3002", "motivation": "\u5728\u5206\u5e03\u5f0f\u96f6\u77e5\u8bc6\u673a\u5668\u5b66\u4e60\u7684\u80cc\u666f\u4e0b\uff0c\u907f\u514d\u5168\u6a21\u578b\u7535\u8def\u5316\u7684\u9ad8\u6210\u672c\u548c\u50f5\u5316\uff0c\u901a\u8fc7\u9488\u5bf9\u6027\u9a8c\u8bc1\u5173\u952e\u5b50\u8ba1\u7b97\u5b9e\u73b0\u4fe1\u4efb\u6700\u5c0f\u5316\u3002", "method": "\u91c7\u7528\u6a21\u5757\u5316\u8bbe\u8ba1\uff0c\u652f\u6301\u5bf9\u63a8\u7406\u7ba1\u9053\u4e2d\u7684\u90e8\u5206\u6216\u5168\u90e8\u5b50\u8ba1\u7b97\uff08\u201c\u5207\u7247\u201d\uff09\u8fdb\u884c\u9a8c\u8bc1\uff0c\u5e76\u901a\u8fc7\u5ba1\u8ba1\u3001\u590d\u5236\u6216\u7ecf\u6d4e\u6fc0\u52b1\u786e\u4fdd\u5168\u5c40\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u4e86\u591a\u79cd\u8bc1\u660e\u7cfb\u7edf\uff0c\u5c55\u793a\u4e86\u5207\u7247\u548c\u975e\u5207\u7247\u914d\u7f6e\u4e0b\u7684\u5185\u5b58\u4f7f\u7528\u3001\u8fd0\u884c\u65f6\u95f4\u548c\u7535\u8def\u884c\u4e3a\u3002", "conclusion": "DSperse\u901a\u8fc7\u7075\u6d3b\u7684\u9a8c\u8bc1\u8fb9\u754c\u8bbe\u8ba1\uff0c\u652f\u6301\u53ef\u6269\u5c55\u7684\u3001\u9488\u5bf9\u6027\u7684\u9a8c\u8bc1\u7b56\u7565\uff0c\u9002\u5e94\u591a\u6837\u5316\u90e8\u7f72\u9700\u6c42\u3002"}}
{"id": "2508.07080", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07080", "abs": "https://arxiv.org/abs/2508.07080", "authors": ["Haolin Liu", "Zijun Guo", "Yanbo Chen", "Jiaqi Chen", "Huilong Yu", "Junqiang Xi"], "title": "An Evolutionary Game-Theoretic Merging Decision-Making Considering Social Acceptance for Autonomous Driving", "comment": null, "summary": "Highway on-ramp merging is of great challenge for autonomous vehicles (AVs),\nsince they have to proactively interact with surrounding vehicles to enter the\nmain road safely within limited time. However, existing decision-making\nalgorithms fail to adequately address dynamic complexities and social\nacceptance of AVs, leading to suboptimal or unsafe merging decisions. To\naddress this, we propose an evolutionary game-theoretic (EGT) merging\ndecision-making framework, grounded in the bounded rationality of human\ndrivers, which dynamically balances the benefits of both AVs and main-road\nvehicles (MVs). We formulate the cut-in decision-making process as an EGT\nproblem with a multi-objective payoff function that reflects human-like driving\npreferences. By solving the replicator dynamic equation for the evolutionarily\nstable strategy (ESS), the optimal cut-in timing is derived, balancing\nefficiency, comfort, and safety for both AVs and MVs. A real-time driving style\nestimation algorithm is proposed to adjust the game payoff function online by\nobserving the immediate reactions of MVs. Empirical results demonstrate that we\nimprove the efficiency, comfort and safety of both AVs and MVs compared with\nexisting game-theoretic and traditional planning approaches across multi-object\nmetrics.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fdb\u5316\u535a\u5f08\u8bba\uff08EGT\uff09\u7684\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\uff08AVs\uff09\u531d\u9053\u5408\u5e76\u51b3\u7b56\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u5e73\u8861AVs\u548c\u4e3b\u8def\u8f66\u8f86\uff08MVs\uff09\u7684\u5229\u76ca\uff0c\u4f18\u5316\u5408\u5e76\u6548\u7387\u3001\u8212\u9002\u6027\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u73b0\u6709\u51b3\u7b56\u7b97\u6cd5\u672a\u80fd\u5145\u5206\u5e94\u5bf9\u52a8\u6001\u590d\u6742\u6027\u548c\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u7684\u793e\u4f1a\u63a5\u53d7\u5ea6\uff0c\u5bfc\u81f4\u5408\u5e76\u51b3\u7b56\u6b21\u4f18\u6216\u4e0d\u5b89\u5168\u3002", "method": "\u91c7\u7528\u8fdb\u5316\u535a\u5f08\u8bba\u6846\u67b6\uff0c\u7ed3\u5408\u4eba\u7c7b\u9a7e\u9a76\u5458\u7684\u6709\u9650\u7406\u6027\uff0c\u8bbe\u8ba1\u591a\u76ee\u6807\u6536\u76ca\u51fd\u6570\uff0c\u5e76\u901a\u8fc7\u6c42\u89e3\u590d\u5236\u52a8\u6001\u65b9\u7a0b\u5f97\u5230\u6700\u4f18\u5408\u5e76\u65f6\u673a\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6548\u7387\u3001\u8212\u9002\u6027\u548c\u5b89\u5168\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u535a\u5f08\u8bba\u548c\u4f20\u7edf\u89c4\u5212\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u531d\u9053\u5408\u5e76\u63d0\u4f9b\u4e86\u66f4\u4f18\u7684\u51b3\u7b56\u65b9\u6848\uff0c\u5e73\u8861\u4e86\u591a\u65b9\u5229\u76ca\u3002"}}
{"id": "2508.06980", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06980", "abs": "https://arxiv.org/abs/2508.06980", "authors": ["Aswin Paul", "Moein Khajehnejad", "Forough Habibollahi", "Brett J. Kagan", "Adeel Razi"], "title": "Simulating Biological Intelligence: Active Inference with Experiment-Informed Generative Model", "comment": "18 pages, 8 figures", "summary": "With recent and rapid advancements in artificial intelligence (AI),\nunderstanding the foundation of purposeful behaviour in autonomous agents is\ncrucial for developing safe and efficient systems. While artificial neural\nnetworks have dominated the path to AI, recent studies are exploring the\npotential of biologically based systems, such as networks of living biological\nneuronal networks. Along with promises of high power and data efficiency, these\nsystems may also inform more explainable and biologically plausible models. In\nthis work, we propose a framework rooted in active inference, a general theory\nof behaviour, to model decision-making in embodied agents. Using\nexperiment-informed generative models, we simulate decision-making processes in\na simulated game-play environment, mirroring experimental setups that use\nbiological neurons. Our results demonstrate learning in these agents, providing\ninsights into the role of memory-based learning and predictive planning in\nintelligent decision-making. This work contributes to the growing field of\nexplainable AI by offering a biologically grounded and scalable approach to\nunderstanding purposeful behaviour in agents.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e3b\u52a8\u63a8\u7406\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u6a21\u62df\u5177\u8eab\u4ee3\u7406\u7684\u51b3\u7b56\u8fc7\u7a0b\uff0c\u63a2\u7d22\u751f\u7269\u795e\u7ecf\u5143\u7f51\u7edc\u7684\u6f5c\u529b\uff0c\u5e76\u5c55\u793a\u4e86\u8bb0\u5fc6\u5b66\u4e60\u548c\u9884\u6d4b\u89c4\u5212\u5728\u667a\u80fd\u51b3\u7b56\u4e2d\u7684\u4f5c\u7528\u3002", "motivation": "\u968f\u7740\u4eba\u5de5\u667a\u80fd\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u7406\u89e3\u81ea\u4e3b\u4ee3\u7406\u7684\u6709\u76ee\u7684\u884c\u4e3a\u57fa\u7840\u5bf9\u5f00\u53d1\u5b89\u5168\u9ad8\u6548\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002\u751f\u7269\u795e\u7ecf\u5143\u7f51\u7edc\u53ef\u80fd\u63d0\u4f9b\u66f4\u9ad8\u80fd\u6548\u548c\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u4f7f\u7528\u5b9e\u9a8c\u542f\u53d1\u7684\u751f\u6210\u6a21\u578b\uff0c\u5728\u6a21\u62df\u6e38\u620f\u73af\u5883\u4e2d\u6a21\u62df\u51b3\u7b56\u8fc7\u7a0b\uff0c\u7ed3\u5408\u4e3b\u52a8\u63a8\u7406\u7406\u8bba\u5efa\u6a21\u3002", "result": "\u7ed3\u679c\u8868\u660e\u4ee3\u7406\u80fd\u591f\u5b66\u4e60\uff0c\u63ed\u793a\u4e86\u8bb0\u5fc6\u5b66\u4e60\u548c\u9884\u6d4b\u89c4\u5212\u5728\u667a\u80fd\u51b3\u7b56\u4e2d\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u53ef\u89e3\u91caAI\u63d0\u4f9b\u4e86\u751f\u7269\u57fa\u7840\u548c\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u7406\u89e3\u4ee3\u7406\u7684\u6709\u76ee\u7684\u884c\u4e3a\u3002"}}
{"id": "2508.07118", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07118", "abs": "https://arxiv.org/abs/2508.07118", "authors": ["Aiden Swann", "Alex Qiu", "Matthew Strong", "Angelina Zhang", "Samuel Morstein", "Kai Rayle", "Monroe Kennedy III"], "title": "DexFruit: Dexterous Manipulation and Gaussian Splatting Inspection of Fruit", "comment": "8 pages, 5 figures", "summary": "DexFruit is a robotic manipulation framework that enables gentle, autonomous\nhandling of fragile fruit and precise evaluation of damage. Many fruits are\nfragile and prone to bruising, thus requiring humans to manually harvest them\nwith care. In this work, we demonstrate by using optical tactile sensing,\nautonomous manipulation of fruit with minimal damage can be achieved. We show\nthat our tactile informed diffusion policies outperform baselines in both\nreduced bruising and pick-and-place success rate across three fruits:\nstrawberries, tomatoes, and blackberries. In addition, we introduce FruitSplat,\na novel technique to represent and quantify visual damage in high-resolution 3D\nrepresentation via 3D Gaussian Splatting (3DGS). Existing metrics for measuring\ndamage lack quantitative rigor or require expensive equipment. With FruitSplat,\nwe distill a 2D strawberry mask as well as a 2D bruise segmentation mask into\nthe 3DGS representation. Furthermore, this representation is modular and\ngeneral, compatible with any relevant 2D model. Overall, we demonstrate a 92%\ngrasping policy success rate, up to a 20% reduction in visual bruising, and up\nto an 31% improvement in grasp success rate on challenging fruit compared to\nour baselines across our three tested fruits. We rigorously evaluate this\nresult with over 630 trials. Please checkout our website at\nhttps://dex-fruit.github.io .", "AI": {"tldr": "DexFruit\u662f\u4e00\u4e2a\u673a\u5668\u4eba\u64cd\u4f5c\u6846\u67b6\uff0c\u901a\u8fc7\u5149\u5b66\u89e6\u89c9\u4f20\u611f\u5b9e\u73b0\u8106\u5f31\u6c34\u679c\u7684\u8f7b\u67d4\u81ea\u4e3b\u5904\u7406\uff0c\u51cf\u5c11\u635f\u4f24\u3002FruitSplat\u6280\u672f\u901a\u8fc73D\u9ad8\u65af\u6cfc\u6e85\u91cf\u5316\u89c6\u89c9\u635f\u4f24\uff0c\u63d0\u5347\u6293\u53d6\u6210\u529f\u7387\u3002", "motivation": "\u8bb8\u591a\u6c34\u679c\u6613\u788e\u4e14\u6613\u53d7\u635f\u4f24\uff0c\u9700\u4eba\u5de5\u5c0f\u5fc3\u91c7\u6458\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u673a\u5668\u4eba\u6280\u672f\u5b9e\u73b0\u8f7b\u67d4\u81ea\u4e3b\u64cd\u4f5c\uff0c\u51cf\u5c11\u635f\u4f24\u3002", "method": "\u4f7f\u7528\u5149\u5b66\u89e6\u89c9\u4f20\u611f\u548c\u89e6\u89c9\u4fe1\u606f\u6269\u6563\u7b56\u7565\uff0c\u7ed3\u5408FruitSplat\u6280\u672f\uff083D\u9ad8\u65af\u6cfc\u6e85\uff09\u91cf\u5316\u635f\u4f24\u3002", "result": "\u6293\u53d6\u6210\u529f\u738792%\uff0c\u89c6\u89c9\u635f\u4f24\u51cf\u5c1120%\uff0c\u6293\u53d6\u6210\u529f\u7387\u63d0\u534731%\u3002", "conclusion": "DexFruit\u6846\u67b6\u663e\u8457\u63d0\u5347\u6c34\u679c\u64cd\u4f5c\u7684\u8f7b\u67d4\u6027\u548c\u6210\u529f\u7387\uff0cFruitSplat\u6280\u672f\u4e3a\u635f\u4f24\u91cf\u5316\u63d0\u4f9b\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2508.07015", "categories": ["cs.AI", "cs.DS"], "pdf": "https://arxiv.org/pdf/2508.07015", "abs": "https://arxiv.org/abs/2508.07015", "authors": ["Hannes Ihalainen", "Dieter Vandesande", "Andr\u00e9 Schidler", "Jeremias Berg", "Bart Bogaerts", "Matti J\u00e4rvisalo"], "title": "Efficient and Reliable Hitting-Set Computations for the Implicit Hitting Set Approach", "comment": null, "summary": "The implicit hitting set (IHS) approach offers a general framework for\nsolving computationally hard combinatorial optimization problems declaratively.\nIHS iterates between a decision oracle used for extracting sources of\ninconsistency and an optimizer for computing so-called hitting sets (HSs) over\nthe accumulated sources of inconsistency. While the decision oracle is\nlanguage-specific, the optimizers is usually instantiated through integer\nprogramming.\n  We explore alternative algorithmic techniques for hitting set optimization\nbased on different ways of employing pseudo-Boolean (PB) reasoning as well as\nstochastic local search. We extensively evaluate the practical feasibility of\nthe alternatives in particular in the context of pseudo-Boolean (0-1 IP)\noptimization as one of the most recent instantiations of IHS. Highlighting a\ntrade-off between efficiency and reliability, while a commercial IP solver\nturns out to remain the most effective way to instantiate HS computations, it\ncan cause correctness issues due to numerical instability; in fact, we show\nthat exact HS computations instantiated via PB reasoning can be made\ncompetitive with a numerically exact IP solver. Furthermore, the use of PB\nreasoning as a basis for HS computations allows for obtaining certificates for\nthe correctness of IHS computations, generally applicable to any IHS\ninstantiation in which reasoning in the declarative language at hand can be\ncaptured in the PB-based proof format we employ.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u9690\u5f0f\u51fb\u4e2d\u96c6\uff08IHS\uff09\u6846\u67b6\u4e2d\u66ff\u4ee3\u6574\u6570\u89c4\u5212\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u5305\u62ec\u4f2a\u5e03\u5c14\u63a8\u7406\u548c\u968f\u673a\u5c40\u90e8\u641c\u7d22\uff0c\u5e76\u8bc4\u4f30\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u53ef\u884c\u6027\u4e0e\u6548\u7387\u3002", "motivation": "\u7814\u7a76IHS\u6846\u67b6\u4e2d\u66ff\u4ee3\u6574\u6570\u89c4\u5212\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u6570\u503c\u4e0d\u7a33\u5b9a\u6027\u548c\u63d0\u9ad8\u8ba1\u7b97\u53ef\u9760\u6027\u3002", "method": "\u91c7\u7528\u4f2a\u5e03\u5c14\u63a8\u7406\u548c\u968f\u673a\u5c40\u90e8\u641c\u7d22\u4f5c\u4e3a\u66ff\u4ee3\u4f18\u5316\u6280\u672f\uff0c\u5e76\u4e0e\u5546\u4e1a\u6574\u6570\u89c4\u5212\u6c42\u89e3\u5668\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "\u5546\u4e1a\u6574\u6570\u89c4\u5212\u6c42\u89e3\u5668\u6548\u7387\u6700\u9ad8\u4f46\u5b58\u5728\u6570\u503c\u4e0d\u7a33\u5b9a\u6027\uff0c\u800c\u4f2a\u5e03\u5c14\u63a8\u7406\u5728\u7cbe\u786e\u8ba1\u7b97\u4e2d\u8868\u73b0\u7ade\u4e89\u529b\uff0c\u5e76\u80fd\u63d0\u4f9b\u6b63\u786e\u6027\u8bc1\u660e\u3002", "conclusion": "\u4f2a\u5e03\u5c14\u63a8\u7406\u53ef\u4f5c\u4e3aIHS\u6846\u67b6\u4e2d\u9ad8\u6548\u4e14\u53ef\u9760\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u9700\u8981\u6b63\u786e\u6027\u8bc1\u660e\u7684\u573a\u666f\u4e2d\u3002"}}
{"id": "2508.07163", "categories": ["cs.RO", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2508.07163", "abs": "https://arxiv.org/abs/2508.07163", "authors": ["Kamal Acharya", "Iman Sharifi", "Mehul Lad", "Liang Sun", "Houbing Song"], "title": "Integrating Neurosymbolic AI in Advanced Air Mobility: A Comprehensive Survey", "comment": "9 pages, 4 figures, IJCAI-2025 (accepted)", "summary": "Neurosymbolic AI combines neural network adaptability with symbolic\nreasoning, promising an approach to address the complex regulatory,\noperational, and safety challenges in Advanced Air Mobility (AAM). This survey\nreviews its applications across key AAM domains such as demand forecasting,\naircraft design, and real-time air traffic management. Our analysis reveals a\nfragmented research landscape where methodologies, including Neurosymbolic\nReinforcement Learning, have shown potential for dynamic optimization but still\nface hurdles in scalability, robustness, and compliance with aviation\nstandards. We classify current advancements, present relevant case studies, and\noutline future research directions aimed at integrating these approaches into\nreliable, transparent AAM systems. By linking advanced AI techniques with AAM's\noperational demands, this work provides a concise roadmap for researchers and\npractitioners developing next-generation air mobility solutions.", "AI": {"tldr": "\u795e\u7ecf\u7b26\u53f7AI\u7ed3\u5408\u795e\u7ecf\u7f51\u7edc\u4e0e\u7b26\u53f7\u63a8\u7406\uff0c\u4e3a\u9ad8\u7ea7\u7a7a\u4e2d\u4ea4\u901a\uff08AAM\uff09\u7684\u590d\u6742\u6311\u6218\u63d0\u4f9b\u89e3\u51b3\u65b9\u6848\u3002\u672c\u6587\u7efc\u8ff0\u4e86\u5176\u5728\u9700\u6c42\u9884\u6d4b\u3001\u98de\u673a\u8bbe\u8ba1\u548c\u5b9e\u65f6\u4ea4\u901a\u7ba1\u7406\u7b49\u9886\u57df\u7684\u5e94\u7528\uff0c\u6307\u51fa\u5f53\u524d\u7814\u7a76\u5728\u53ef\u6269\u5c55\u6027\u548c\u5408\u89c4\u6027\u65b9\u9762\u4ecd\u5b58\u6311\u6218\u3002", "motivation": "\u9ad8\u7ea7\u7a7a\u4e2d\u4ea4\u901a\uff08AAM\uff09\u9762\u4e34\u590d\u6742\u7684\u76d1\u7ba1\u3001\u8fd0\u8425\u548c\u5b89\u5168\u6311\u6218\uff0c\u795e\u7ecf\u7b26\u53f7AI\u6709\u671b\u901a\u8fc7\u7ed3\u5408\u795e\u7ecf\u7f51\u7edc\u7684\u9002\u5e94\u6027\u4e0e\u7b26\u53f7\u63a8\u7406\u7684\u900f\u660e\u6027\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5206\u7c7b\u5f53\u524d\u8fdb\u5c55\u3001\u5c55\u793a\u6848\u4f8b\u7814\u7a76\uff0c\u5e76\u5206\u6790\u795e\u7ecf\u7b26\u53f7\u5f3a\u5316\u5b66\u4e60\u7b49\u65b9\u6cd5\u5728\u52a8\u6001\u4f18\u5316\u4e2d\u7684\u6f5c\u529b\u4e0e\u5c40\u9650\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u795e\u7ecf\u7b26\u53f7AI\u5728AAM\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u5728\u53ef\u6269\u5c55\u6027\u3001\u9c81\u68d2\u6027\u548c\u5408\u89c4\u6027\u65b9\u9762\u4ecd\u9700\u6539\u8fdb\u3002", "conclusion": "\u672c\u6587\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u4ece\u4e1a\u8005\u63d0\u4f9b\u4e86\u6574\u5408\u795e\u7ecf\u7b26\u53f7AI\u5230\u53ef\u9760\u3001\u900f\u660e\u7684AAM\u7cfb\u7edf\u4e2d\u7684\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2508.07022", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.07022", "abs": "https://arxiv.org/abs/2508.07022", "authors": ["Shengtao Wen", "Haodong Chen", "Yadong Wang", "Zhongying Pan", "Xiang Chen", "Yu Tian", "Bo Qian", "Dong Liang", "Sheng-Jun Huang"], "title": "MultiMedEdit: A Scenario-Aware Benchmark for Evaluating Knowledge Editing in Medical VQA", "comment": "Under Review", "summary": "Knowledge editing (KE) provides a scalable approach for updating factual\nknowledge in large language models without full retraining. While previous\nstudies have demonstrated effectiveness in general domains and medical QA\ntasks, little attention has been paid to KE in multimodal medical scenarios.\nUnlike text-only settings, medical KE demands integrating updated knowledge\nwith visual reasoning to support safe and interpretable clinical decisions. To\naddress this gap, we propose MultiMedEdit, the first benchmark tailored to\nevaluating KE in clinical multimodal tasks. Our framework spans both\nunderstanding and reasoning task types, defines a three-dimensional metric\nsuite (reliability, generality, and locality), and supports cross-paradigm\ncomparisons across general and domain-specific models. We conduct extensive\nexperiments under single-editing and lifelong-editing settings. Results suggest\nthat current methods struggle with generalization and long-tail reasoning,\nparticularly in complex clinical workflows. We further present an efficiency\nanalysis (e.g., edit latency, memory footprint), revealing practical trade-offs\nin real-world deployment across KE paradigms. Overall, MultiMedEdit not only\nreveals the limitations of current approaches but also provides a solid\nfoundation for developing clinically robust knowledge editing techniques in the\nfuture.", "AI": {"tldr": "MultiMedEdit\u662f\u9996\u4e2a\u9488\u5bf9\u4e34\u5e8a\u591a\u6a21\u6001\u4efb\u52a1\u7684\u77e5\u8bc6\u7f16\u8f91\uff08KE\uff09\u8bc4\u4f30\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u65b9\u6cd5\u5728\u590d\u6742\u4e34\u5e8a\u5de5\u4f5c\u6d41\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u6539\u8fdb\u65b9\u5411\u3002", "motivation": "\u73b0\u6709\u77e5\u8bc6\u7f16\u8f91\u7814\u7a76\u591a\u96c6\u4e2d\u4e8e\u6587\u672c\u9886\u57df\uff0c\u800c\u591a\u6a21\u6001\u533b\u7597\u573a\u666f\u4e0b\u7684\u77e5\u8bc6\u7f16\u8f91\u9700\u6c42\u672a\u88ab\u5145\u5206\u63a2\u7d22\uff0c\u9700\u7ed3\u5408\u89c6\u89c9\u63a8\u7406\u652f\u6301\u4e34\u5e8a\u51b3\u7b56\u3002", "method": "\u63d0\u51faMultiMedEdit\u6846\u67b6\uff0c\u6db5\u76d6\u7406\u89e3\u548c\u63a8\u7406\u4efb\u52a1\uff0c\u5b9a\u4e49\u4e09\u7ef4\u5ea6\u91cf\u6807\u51c6\uff08\u53ef\u9760\u6027\u3001\u901a\u7528\u6027\u3001\u5c40\u90e8\u6027\uff09\uff0c\u652f\u6301\u8de8\u8303\u5f0f\u6bd4\u8f83\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u5f53\u524d\u65b9\u6cd5\u5728\u6cdb\u5316\u548c\u957f\u5c3e\u63a8\u7406\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u5728\u590d\u6742\u4e34\u5e8a\u5de5\u4f5c\u6d41\u4e2d\u3002\u6548\u7387\u5206\u6790\u63ed\u793a\u4e86\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u6743\u8861\u3002", "conclusion": "MultiMedEdit\u4e3a\u672a\u6765\u5f00\u53d1\u4e34\u5e8a\u7a33\u5065\u7684\u77e5\u8bc6\u7f16\u8f91\u6280\u672f\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5e76\u63ed\u793a\u4e86\u5f53\u524d\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002"}}
{"id": "2508.07182", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07182", "abs": "https://arxiv.org/abs/2508.07182", "authors": ["Xuesong Li", "Lars Petersson", "Vivien Rolland"], "title": "3D Gaussian Representations with Motion Trajectory Field for Dynamic Scene Reconstruction", "comment": null, "summary": "This paper addresses the challenge of novel-view synthesis and motion\nreconstruction of dynamic scenes from monocular video, which is critical for\nmany robotic applications. Although Neural Radiance Fields (NeRF) and 3D\nGaussian Splatting (3DGS) have demonstrated remarkable success in rendering\nstatic scenes, extending them to reconstruct dynamic scenes remains\nchallenging. In this work, we introduce a novel approach that combines 3DGS\nwith a motion trajectory field, enabling precise handling of complex object\nmotions and achieving physically plausible motion trajectories. By decoupling\ndynamic objects from static background, our method compactly optimizes the\nmotion trajectory field. The approach incorporates time-invariant motion\ncoefficients and shared motion trajectory bases to capture intricate motion\npatterns while minimizing optimization complexity. Extensive experiments\ndemonstrate that our approach achieves state-of-the-art results in both\nnovel-view synthesis and motion trajectory recovery from monocular video,\nadvancing the capabilities of dynamic scene reconstruction.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u54083D\u9ad8\u65af\u6cfc\u6e85\u4e0e\u8fd0\u52a8\u8f68\u8ff9\u573a\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u52a8\u6001\u573a\u666f\u7684\u65b0\u89c6\u89d2\u5408\u6210\u4e0e\u8fd0\u52a8\u91cd\u5efa\u3002", "motivation": "\u89e3\u51b3\u5355\u76ee\u89c6\u9891\u4e2d\u52a8\u6001\u573a\u666f\u7684\u65b0\u89c6\u89d2\u5408\u6210\u4e0e\u8fd0\u52a8\u91cd\u5efa\u95ee\u9898\uff0c\u586b\u8865NeRF\u548c3DGS\u5728\u52a8\u6001\u573a\u666f\u4e2d\u7684\u4e0d\u8db3\u3002", "method": "\u7ed3\u54083DGS\u4e0e\u8fd0\u52a8\u8f68\u8ff9\u573a\uff0c\u89e3\u8026\u52a8\u6001\u7269\u4f53\u4e0e\u9759\u6001\u80cc\u666f\uff0c\u4f18\u5316\u8fd0\u52a8\u8f68\u8ff9\u573a\uff0c\u4f7f\u7528\u65f6\u95f4\u4e0d\u53d8\u8fd0\u52a8\u7cfb\u6570\u548c\u5171\u4eab\u8fd0\u52a8\u8f68\u8ff9\u57fa\u3002", "result": "\u5728\u5355\u76ee\u89c6\u9891\u7684\u65b0\u89c6\u89d2\u5408\u6210\u548c\u8fd0\u52a8\u8f68\u8ff9\u6062\u590d\u4e2d\u53d6\u5f97\u6700\u5148\u8fdb\u7ed3\u679c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u52a8\u6001\u573a\u666f\u91cd\u5efa\u7684\u80fd\u529b\u3002"}}
{"id": "2508.07043", "categories": ["cs.AI", "cs.MA", "q-bio.GN", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2508.07043", "abs": "https://arxiv.org/abs/2508.07043", "authors": ["Orion Li", "Vinayak Agarwal", "Summer Zhou", "Ashwin Gopinath", "Timothy Kassis"], "title": "K-Dense Analyst: Towards Fully Automated Scientific Analysis", "comment": null, "summary": "The complexity of modern bioinformatics analysis has created a critical gap\nbetween data generation and developing scientific insights. While large\nlanguage models (LLMs) have shown promise in scientific reasoning, they remain\nfundamentally limited when dealing with real-world analytical workflows that\ndemand iterative computation, tool integration and rigorous validation. We\nintroduce K-Dense Analyst, a hierarchical multi-agent system that achieves\nautonomous bioinformatics analysis through a dual-loop architecture. K-Dense\nAnalyst, part of the broader K-Dense platform, couples planning with validated\nexecution using specialized agents to decompose complex objectives into\nexecutable, verifiable tasks within secure computational environments. On\nBixBench, a comprehensive benchmark for open-ended biological analysis, K-Dense\nAnalyst achieves 29.2% accuracy, surpassing the best-performing language model\n(GPT-5) by 6.3 percentage points, representing nearly 27% improvement over what\nis widely considered the most powerful LLM available. Remarkably, K-Dense\nAnalyst achieves this performance using Gemini 2.5 Pro, which attains only\n18.3% accuracy when used directly, demonstrating that our architectural\ninnovations unlock capabilities far beyond the underlying model's baseline\nperformance. Our insights demonstrate that autonomous scientific reasoning\nrequires more than enhanced language models, it demands purpose-built systems\nthat can bridge the gap between high-level scientific objectives and low-level\ncomputational execution. These results represent a significant advance toward\nfully autonomous computational biologists capable of accelerating discovery\nacross the life sciences.", "AI": {"tldr": "K-Dense Analyst\u662f\u4e00\u4e2a\u591a\u4ee3\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7\u53cc\u5faa\u73af\u67b6\u6784\u5b9e\u73b0\u81ea\u4e3b\u751f\u7269\u4fe1\u606f\u5b66\u5206\u6790\uff0c\u6027\u80fd\u4f18\u4e8eGPT-5\u3002", "motivation": "\u73b0\u4ee3\u751f\u7269\u4fe1\u606f\u5b66\u5206\u6790\u7684\u590d\u6742\u6027\u5bfc\u81f4\u6570\u636e\u751f\u6210\u4e0e\u79d1\u5b66\u6d1e\u5bdf\u4e4b\u95f4\u5b58\u5728\u9e3f\u6c9f\uff0c\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8fed\u4ee3\u8ba1\u7b97\u548c\u5de5\u5177\u96c6\u6210\u65b9\u9762\u53d7\u9650\u3002", "method": "\u91c7\u7528\u5206\u5c42\u591a\u4ee3\u7406\u7cfb\u7edf\u548c\u53cc\u5faa\u73af\u67b6\u6784\uff0c\u5c06\u590d\u6742\u76ee\u6807\u5206\u89e3\u4e3a\u53ef\u6267\u884c\u3001\u53ef\u9a8c\u8bc1\u7684\u4efb\u52a1\u3002", "result": "\u5728BixBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cK-Dense Analyst\u7684\u51c6\u786e\u7387\u8fbe\u523029.2%\uff0c\u6bd4GPT-5\u9ad86.3\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "\u81ea\u4e3b\u79d1\u5b66\u63a8\u7406\u9700\u8981\u4e13\u95e8\u6784\u5efa\u7684\u7cfb\u7edf\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u589e\u5f3a\u7684\u8bed\u8a00\u6a21\u578b\uff0cK-Dense Analyst\u4e3a\u751f\u547d\u79d1\u5b66\u9886\u57df\u7684\u53d1\u73b0\u63d0\u4f9b\u4e86\u91cd\u8981\u8fdb\u5c55\u3002"}}
{"id": "2508.07244", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07244", "abs": "https://arxiv.org/abs/2508.07244", "authors": ["Ayesha Jena", "Stefan Reitmann", "Elin Anna Topp"], "title": "Impact of Gaze-Based Interaction and Augmentation on Human-Robot Collaboration in Critical Tasks", "comment": null, "summary": "We present a user study analyzing head-gaze-based robot control and foveated\nvisual augmentation in a simulated search-and-rescue task. Results show that\nfoveated augmentation significantly improves task performance, reduces\ncognitive load by 38%, and shortens task time by over 60%. Head-gaze patterns\nanalysed over both the entire task duration and shorter time segments show that\nnear and far attention capture is essential to better understand user intention\nin critical scenarios. Our findings highlight the potential of foveation as an\naugmentation technique and the need to further study gaze measures to leverage\nthem during critical tasks.", "AI": {"tldr": "\u7814\u7a76\u5206\u6790\u4e86\u57fa\u4e8e\u5934\u90e8\u6ce8\u89c6\u7684\u673a\u5668\u4eba\u63a7\u5236\u548c\u805a\u7126\u89c6\u89c9\u589e\u5f3a\u5728\u6a21\u62df\u641c\u6551\u4efb\u52a1\u4e2d\u7684\u6548\u679c\uff0c\u7ed3\u679c\u663e\u793a\u805a\u7126\u589e\u5f3a\u663e\u8457\u63d0\u5347\u4efb\u52a1\u8868\u73b0\uff0c\u964d\u4f4e\u8ba4\u77e5\u8d1f\u837738%\uff0c\u7f29\u77ed\u4efb\u52a1\u65f6\u95f460%\u4ee5\u4e0a\u3002", "motivation": "\u63a2\u7d22\u805a\u7126\u89c6\u89c9\u589e\u5f3a\u548c\u5934\u90e8\u6ce8\u89c6\u6a21\u5f0f\u5728\u641c\u6551\u4efb\u52a1\u4e2d\u7684\u6548\u679c\uff0c\u4ee5\u4f18\u5316\u7528\u6237\u610f\u56fe\u7406\u89e3\u548c\u4efb\u52a1\u8868\u73b0\u3002", "method": "\u901a\u8fc7\u7528\u6237\u7814\u7a76\u5206\u6790\u5934\u90e8\u6ce8\u89c6\u6a21\u5f0f\u548c\u805a\u7126\u89c6\u89c9\u589e\u5f3a\u5728\u6a21\u62df\u641c\u6551\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u3002", "result": "\u805a\u7126\u589e\u5f3a\u663e\u8457\u63d0\u5347\u4efb\u52a1\u8868\u73b0\uff0c\u964d\u4f4e\u8ba4\u77e5\u8d1f\u837738%\uff0c\u7f29\u77ed\u4efb\u52a1\u65f6\u95f460%\u4ee5\u4e0a\uff1b\u5934\u90e8\u6ce8\u89c6\u6a21\u5f0f\u5206\u6790\u663e\u793a\u8fdc\u8fd1\u6ce8\u610f\u529b\u6355\u6349\u5bf9\u7406\u89e3\u7528\u6237\u610f\u56fe\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u805a\u7126\u589e\u5f3a\u6280\u672f\u5177\u6709\u6f5c\u529b\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u6ce8\u89c6\u6d4b\u91cf\u4ee5\u5728\u5173\u952e\u4efb\u52a1\u4e2d\u53d1\u6325\u4f5c\u7528\u3002"}}
{"id": "2508.07063", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07063", "abs": "https://arxiv.org/abs/2508.07063", "authors": ["Naseem Machlovi", "Maryam Saleki", "Innocent Ababio", "Ruhul Amin"], "title": "Towards Safer AI Moderation: Evaluating LLM Moderators Through a Unified Benchmark Dataset and Advocating a Human-First Approach", "comment": null, "summary": "As AI systems become more integrated into daily life, the need for safer and\nmore reliable moderation has never been greater. Large Language Models (LLMs)\nhave demonstrated remarkable capabilities, surpassing earlier models in\ncomplexity and performance. Their evaluation across diverse tasks has\nconsistently showcased their potential, enabling the development of adaptive\nand personalized agents. However, despite these advancements, LLMs remain prone\nto errors, particularly in areas requiring nuanced moral reasoning. They\nstruggle with detecting implicit hate, offensive language, and gender biases\ndue to the subjective and context-dependent nature of these issues. Moreover,\ntheir reliance on training data can inadvertently reinforce societal biases,\nleading to inconsistencies and ethical concerns in their outputs. To explore\nthe limitations of LLMs in this role, we developed an experimental framework\nbased on state-of-the-art (SOTA) models to assess human emotions and offensive\nbehaviors. The framework introduces a unified benchmark dataset encompassing 49\ndistinct categories spanning the wide spectrum of human emotions, offensive and\nhateful text, and gender and racial biases. Furthermore, we introduced SafePhi,\na QLoRA fine-tuned version of Phi-4, adapting diverse ethical contexts and\noutperforming benchmark moderators by achieving a Macro F1 score of 0.89, where\nOpenAI Moderator and Llama Guard score 0.77 and 0.74, respectively. This\nresearch also highlights the critical domains where LLM moderators consistently\nunderperformed, pressing the need to incorporate more heterogeneous and\nrepresentative data with human-in-the-loop, for better model robustness and\nexplainability.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5185\u5bb9\u5ba1\u6838\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u6846\u67b6SafePhi\uff0c\u5176\u5728\u4f26\u7406\u8bed\u5883\u4e0b\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u4f46\u4ecd\u9700\u6539\u8fdb\u6570\u636e\u591a\u6837\u6027\u548c\u4eba\u7c7b\u53c2\u4e0e\u3002", "motivation": "\u968f\u7740AI\u7cfb\u7edf\u5728\u65e5\u5e38\u751f\u6d3b\u4e2d\u7684\u666e\u53ca\uff0c\u786e\u4fdd\u5176\u5b89\u5168\u53ef\u9760\u7684\u5ba1\u6838\u80fd\u529b\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u5c3d\u7ba1LLMs\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u6d89\u53ca\u9053\u5fb7\u63a8\u7406\u3001\u9690\u6666\u4ec7\u6068\u548c\u6027\u522b\u504f\u89c1\u7b49\u4e3b\u89c2\u95ee\u9898\u65f6\u4ecd\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8eSOTA\u6a21\u578b\u7684\u5b9e\u9a8c\u6846\u67b6\uff0c\u8bc4\u4f30\u4eba\u7c7b\u60c5\u611f\u548c\u653b\u51fb\u6027\u884c\u4e3a\uff0c\u5e76\u5f15\u5165\u7edf\u4e00\u57fa\u51c6\u6570\u636e\u96c6\u3002\u540c\u65f6\u63d0\u51fa\u4e86SafePhi\uff0c\u4e00\u79cdQLoRA\u5fae\u8c03\u7684Phi-4\u6a21\u578b\u3002", "result": "SafePhi\u5728Macro F1\u5f97\u5206\u4e0a\u8fbe\u52300.89\uff0c\u4f18\u4e8eOpenAI Moderator\uff080.77\uff09\u548cLlama Guard\uff080.74\uff09\u3002\u7814\u7a76\u8fd8\u63ed\u793a\u4e86LLMs\u5728\u5173\u952e\u9886\u57df\u7684\u6301\u7eed\u4e0d\u8db3\u3002", "conclusion": "\u9700\u901a\u8fc7\u66f4\u5f02\u6784\u548c\u4ee3\u8868\u6027\u7684\u6570\u636e\u53ca\u4eba\u7c7b\u53c2\u4e0e\u63d0\u5347\u6a21\u578b\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2508.07267", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07267", "abs": "https://arxiv.org/abs/2508.07267", "authors": ["Daria de Tinguy", "Tim Verbelen", "Emilio Gamba", "Bart Dhoedt"], "title": "Bio-Inspired Topological Autonomous Navigation with Active Inference in Robotics", "comment": "Conference ICCAS 2025 - accepted (in processing)", "summary": "Achieving fully autonomous exploration and navigation remains a critical\nchallenge in robotics, requiring integrated solutions for localisation,\nmapping, decision-making and motion planning. Existing approaches either rely\non strict navigation rules lacking adaptability or on pre-training, which\nrequires large datasets. These AI methods are often computationally intensive\nor based on static assumptions, limiting their adaptability in dynamic or\nunknown environments. This paper introduces a bio-inspired agent based on the\nActive Inference Framework (AIF), which unifies mapping, localisation, and\nadaptive decision-making for autonomous navigation, including exploration and\ngoal-reaching. Our model creates and updates a topological map of the\nenvironment in real-time, planning goal-directed trajectories to explore or\nreach objectives without requiring pre-training. Key contributions include a\nprobabilistic reasoning framework for interpretable navigation, robust\nadaptability to dynamic changes, and a modular ROS2 architecture compatible\nwith existing navigation systems. Our method was tested in simulated and\nreal-world environments. The agent successfully explores large-scale simulated\nenvironments and adapts to dynamic obstacles and drift, proving to be\ncomparable to other exploration strategies such as Gbplanner, FAEL and\nFrontiers. This approach offers a scalable and transparent approach for\nnavigating complex, unstructured environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e3b\u52a8\u63a8\u7406\u6846\u67b6\uff08AIF\uff09\u7684\u751f\u7269\u542f\u53d1\u667a\u80fd\u4f53\uff0c\u7528\u4e8e\u81ea\u4e3b\u5bfc\u822a\uff0c\u65e0\u9700\u9884\u8bad\u7ec3\u5373\u53ef\u5b9e\u65f6\u6784\u5efa\u548c\u66f4\u65b0\u73af\u5883\u62d3\u6251\u56fe\uff0c\u5b9e\u73b0\u63a2\u7d22\u548c\u76ee\u6807\u5230\u8fbe\u3002", "motivation": "\u73b0\u6709\u81ea\u4e3b\u5bfc\u822a\u65b9\u6cd5\u4f9d\u8d56\u4e25\u683c\u89c4\u5219\u6216\u9884\u8bad\u7ec3\uff0c\u7f3a\u4e4f\u52a8\u6001\u73af\u5883\u9002\u5e94\u6027\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u3002\u672c\u6587\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u7edf\u4e00\u3001\u81ea\u9002\u5e94\u4e14\u900f\u660e\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u4e3b\u52a8\u63a8\u7406\u6846\u67b6\uff08AIF\uff09\uff0c\u7ed3\u5408\u6982\u7387\u63a8\u7406\u548c\u6a21\u5757\u5316ROS2\u67b6\u6784\uff0c\u5b9e\u65f6\u6784\u5efa\u62d3\u6251\u56fe\u5e76\u89c4\u5212\u76ee\u6807\u5bfc\u5411\u8f68\u8ff9\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u6d4b\u8bd5\uff0c\u667a\u80fd\u4f53\u6210\u529f\u63a2\u7d22\u5927\u89c4\u6a21\u73af\u5883\u5e76\u9002\u5e94\u52a8\u6001\u969c\u788d\uff0c\u6027\u80fd\u4e0eGbplanner\u3001FAEL\u7b49\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u590d\u6742\u975e\u7ed3\u6784\u5316\u73af\u5883\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u900f\u660e\u7684\u5bfc\u822a\u65b9\u6848\u3002"}}
{"id": "2508.07107", "categories": ["cs.AI", "cs.CY", "K.3.1; I.2.6; H.4"], "pdf": "https://arxiv.org/pdf/2508.07107", "abs": "https://arxiv.org/abs/2508.07107", "authors": ["Timothy Oluwapelumi Adeyemi", "Nadiah Fahad AlOtaibi"], "title": "Designing a Feedback-Driven Decision Support System for Dynamic Student Intervention", "comment": "10 pages, 1 figure, 3 tables", "summary": "Accurate prediction of student performance is essential for timely academic\nintervention. However, most machine learning models in education are static and\ncannot adapt when new data, such as post-intervention outcomes, become\navailable. To address this limitation, we propose a Feedback-Driven Decision\nSupport System (DSS) with a closed-loop architecture that enables continuous\nmodel refinement. The system integrates a LightGBM-based regressor with\nincremental retraining, allowing educators to input updated student results,\nwhich automatically trigger model updates. This adaptive mechanism improves\nprediction accuracy by learning from real-world academic progress. The platform\nfeatures a Flask-based web interface for real-time interaction and incorporates\nSHAP for explainability, ensuring transparency. Experimental results show a\n10.7\\% reduction in RMSE after retraining, with consistent upward adjustments\nin predicted scores for intervened students. By transforming static predictors\ninto self-improving systems, our approach advances educational analytics toward\nhuman-centered, data-driven, and responsive AI. The framework is designed for\nintegration into LMS and institutional dashboards.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cd\u9988\u9a71\u52a8\u7684\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\uff08DSS\uff09\uff0c\u901a\u8fc7\u95ed\u73af\u67b6\u6784\u548c\u589e\u91cf\u8bad\u7ec3\u63d0\u5347\u5b66\u751f\u6210\u7ee9\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u5b66\u4e60\u6a21\u578b\u591a\u4e3a\u9759\u6001\uff0c\u65e0\u6cd5\u9002\u5e94\u65b0\u6570\u636e\uff08\u5982\u5e72\u9884\u540e\u7ed3\u679c\uff09\uff0c\u9700\u52a8\u6001\u8c03\u6574\u4ee5\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u3002", "method": "\u7ed3\u5408LightGBM\u56de\u5f52\u5668\u548c\u589e\u91cf\u8bad\u7ec3\uff0c\u652f\u6301\u5b9e\u65f6\u6570\u636e\u8f93\u5165\u548c\u6a21\u578b\u66f4\u65b0\uff0c\u5e76\u91c7\u7528SHAP\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u5b9e\u9a8c\u663e\u793aRMSE\u964d\u4f4e10.7%\uff0c\u5e72\u9884\u5b66\u751f\u9884\u6d4b\u5206\u6570\u6301\u7eed\u63d0\u5347\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u5c06\u9759\u6001\u9884\u6d4b\u8f6c\u5316\u4e3a\u81ea\u4f18\u5316\u7cfb\u7edf\uff0c\u63a8\u52a8\u6559\u80b2\u5206\u6790\u5411\u4ee5\u4eba\u4e3a\u672c\u3001\u6570\u636e\u9a71\u52a8\u548c\u54cd\u5e94\u5f0fAI\u53d1\u5c55\u3002"}}
{"id": "2508.07269", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07269", "abs": "https://arxiv.org/abs/2508.07269", "authors": ["Daria de Tinguy", "Tim Verbelen", "Bart Dhoedt"], "title": "Navigation and Exploration with Active Inference: from Biology to Industry", "comment": "conference IWAI 2025 - accepted (in processing)", "summary": "By building and updating internal cognitive maps, animals exhibit\nextraordinary navigation abilities in complex, dynamic environments. Inspired\nby these biological mechanisms, we present a real time robotic navigation\nsystem grounded in the Active Inference Framework (AIF). Our model\nincrementally constructs a topological map, infers the agent's location, and\nplans actions by minimising expected uncertainty and fulfilling perceptual\ngoals without any prior training. Integrated into the ROS2 ecosystem, we\nvalidate its adaptability and efficiency across both 2D and 3D environments\n(simulated and real world), demonstrating competitive performance with\ntraditional and state of the art exploration approaches while offering a\nbiologically inspired navigation approach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e3b\u52a8\u63a8\u7406\u6846\u67b6\uff08AIF\uff09\u7684\u5b9e\u65f6\u673a\u5668\u4eba\u5bfc\u822a\u7cfb\u7edf\uff0c\u65e0\u9700\u9884\u5148\u8bad\u7ec3\uff0c\u901a\u8fc7\u6784\u5efa\u62d3\u6251\u5730\u56fe\u548c\u6700\u5c0f\u5316\u4e0d\u786e\u5b9a\u6027\u5b9e\u73b0\u9ad8\u6548\u5bfc\u822a\u3002", "motivation": "\u53d7\u52a8\u7269\u901a\u8fc7\u6784\u5efa\u8ba4\u77e5\u5730\u56fe\u5b9e\u73b0\u590d\u6742\u73af\u5883\u5bfc\u822a\u7684\u542f\u53d1\uff0c\u5f00\u53d1\u4e00\u79cd\u751f\u7269\u542f\u53d1\u7684\u673a\u5668\u4eba\u5bfc\u822a\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u4e3b\u52a8\u63a8\u7406\u6846\u67b6\uff0c\u589e\u91cf\u6784\u5efa\u62d3\u6251\u5730\u56fe\uff0c\u63a8\u65ad\u4f4d\u7f6e\u5e76\u89c4\u5212\u52a8\u4f5c\uff0c\u96c6\u6210\u4e8eROS2\u751f\u6001\u7cfb\u7edf\u3002", "result": "\u57282D\u548c3D\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u7684\u9002\u5e94\u6027\u548c\u6548\u7387\uff0c\u6027\u80fd\u4e0e\u4f20\u7edf\u53ca\u524d\u6cbf\u65b9\u6cd5\u76f8\u5f53\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u751f\u7269\u542f\u53d1\u7684\u5bfc\u822a\u65b9\u6cd5\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.07186", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.07186", "abs": "https://arxiv.org/abs/2508.07186", "authors": ["Amit Dhanda"], "title": "Multi-Dimensional Summarization Agents with Context-Aware Reasoning over Enterprise Tables", "comment": null, "summary": "We propose a novel framework for summarizing structured enterprise data\nacross multiple dimensions using large language model (LLM)-based agents.\nTraditional table-to-text models often lack the capacity to reason across\nhierarchical structures and context-aware deltas, which are essential in\nbusiness reporting tasks. Our method introduces a multi-agent pipeline that\nextracts, analyzes, and summarizes multi-dimensional data using agents for\nslicing, variance detection, context construction, and LLM-based generation.\nOur results show that the proposed framework outperforms traditional\napproaches, achieving 83\\% faithfulness to underlying data, superior coverage\nof significant changes, and high relevance scores (4.4/5) for decision-critical\ninsights. The improvements are especially pronounced in categories involving\nsubtle trade-offs, such as increased revenue due to price changes amid\ndeclining unit volumes, which competing methods either overlook or address with\nlimited specificity. We evaluate the framework on Kaggle datasets and\ndemonstrate significant improvements in faithfulness, relevance, and insight\nquality over baseline table summarization approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u8de8\u7ef4\u5ea6\u6c47\u603b\u7ed3\u6784\u5316\u4f01\u4e1a\u6570\u636e\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u8868\u683c\u5230\u6587\u672c\u6a21\u578b\u7f3a\u4e4f\u5bf9\u5c42\u6b21\u7ed3\u6784\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u5dee\u5f02\u7684\u63a8\u7406\u80fd\u529b\uff0c\u65e0\u6cd5\u6ee1\u8db3\u5546\u4e1a\u62a5\u544a\u9700\u6c42\u3002", "method": "\u91c7\u7528\u591a\u667a\u80fd\u4f53\u7ba1\u9053\uff0c\u5305\u62ec\u6570\u636e\u5207\u7247\u3001\u65b9\u5dee\u68c0\u6d4b\u3001\u4e0a\u4e0b\u6587\u6784\u5efa\u548cLLM\u751f\u6210\u3002", "result": "\u6846\u67b6\u5728\u6570\u636e\u5fe0\u5b9e\u5ea6\uff0883%\uff09\u3001\u663e\u8457\u53d8\u5316\u8986\u76d6\u548c\u51b3\u7b56\u5173\u952e\u6d1e\u5bdf\u76f8\u5173\u6027\uff084.4/5\uff09\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u590d\u6742\u5546\u4e1a\u573a\u666f\u4e2d\uff08\u5982\u6536\u5165\u4e0e\u9500\u91cf\u6743\u8861\uff09\u8868\u73b0\u7a81\u51fa\uff0c\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002"}}
{"id": "2508.07287", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07287", "abs": "https://arxiv.org/abs/2508.07287", "authors": ["Liwen Zhang", "Dong Zhou", "Shibo Shao", "Zihao Su", "Guanghui Sun"], "title": "Multimodal Spiking Neural Network for Space Robotic Manipulation", "comment": null, "summary": "This paper presents a multimodal control framework based on spiking neural\nnetworks (SNNs) for robotic arms aboard space stations. It is designed to cope\nwith the constraints of limited onboard resources while enabling autonomous\nmanipulation and material transfer in space operations. By combining geometric\nstates with tactile and semantic information, the framework strengthens\nenvironmental awareness and contributes to more robust control strategies. To\nguide the learning process progressively, a dual-channel, three-stage\ncurriculum reinforcement learning (CRL) scheme is further integrated into the\nsystem. The framework was tested across a range of tasks including target\napproach, object grasping, and stable lifting with wall-mounted robotic arms,\ndemonstrating reliable performance throughout. Experimental evaluations\ndemonstrate that the proposed method consistently outperforms baseline\napproaches in both task success rate and energy efficiency. These findings\nhighlight its suitability for real-world aerospace applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNN\uff09\u7684\u591a\u6a21\u6001\u63a7\u5236\u6846\u67b6\uff0c\u7528\u4e8e\u7a7a\u95f4\u7ad9\u4e0a\u7684\u673a\u68b0\u81c2\u64cd\u4f5c\uff0c\u65e8\u5728\u89e3\u51b3\u8d44\u6e90\u6709\u9650\u7684\u95ee\u9898\uff0c\u540c\u65f6\u5b9e\u73b0\u81ea\u4e3b\u64cd\u4f5c\u548c\u6750\u6599\u8f6c\u79fb\u3002", "motivation": "\u7a7a\u95f4\u7ad9\u4e0a\u7684\u673a\u68b0\u81c2\u64cd\u4f5c\u9762\u4e34\u8d44\u6e90\u6709\u9650\u7684\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u9ad8\u6548\u4e14\u81ea\u4e3b\u7684\u63a7\u5236\u65b9\u6cd5\u3002", "method": "\u7ed3\u5408\u51e0\u4f55\u72b6\u6001\u3001\u89e6\u89c9\u548c\u8bed\u4e49\u4fe1\u606f\uff0c\u91c7\u7528\u53cc\u901a\u9053\u4e09\u9636\u6bb5\u8bfe\u7a0b\u5f3a\u5316\u5b66\u4e60\uff08CRL\uff09\u65b9\u6848\u3002", "result": "\u5728\u76ee\u6807\u63a5\u8fd1\u3001\u7269\u4f53\u6293\u53d6\u548c\u7a33\u5b9a\u63d0\u5347\u7b49\u4efb\u52a1\u4e2d\u8868\u73b0\u53ef\u9760\uff0c\u4efb\u52a1\u6210\u529f\u7387\u548c\u80fd\u6548\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u9002\u7528\u4e8e\u5b9e\u9645\u822a\u7a7a\u822a\u5929\u5e94\u7528\uff0c\u5177\u6709\u8f83\u9ad8\u7684\u5b9e\u7528\u4ef7\u503c\u3002"}}
{"id": "2508.07292", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.07292", "abs": "https://arxiv.org/abs/2508.07292", "authors": ["Yi Tang", "Kaini Wang", "Yang Chen", "Guangquan Zhou"], "title": "EndoAgent: A Memory-Guided Reflective Agent for Intelligent Endoscopic Vision-to-Decision Reasoning", "comment": null, "summary": "Developing general artificial intelligence (AI) systems to support endoscopic\nimage diagnosis is an emerging research priority. Existing methods based on\nlarge-scale pretraining often lack unified coordination across tasks and\nstruggle to handle the multi-step processes required in complex clinical\nworkflows. While AI agents have shown promise in flexible instruction parsing\nand tool integration across domains, their potential in endoscopy remains\nunderexplored. To address this gap, we propose EndoAgent, the first\nmemory-guided agent for vision-to-decision endoscopic analysis that integrates\niterative reasoning with adaptive tool selection and collaboration. Built on a\ndual-memory design, it enables sophisticated decision-making by ensuring\nlogical coherence through short-term action tracking and progressively\nenhancing reasoning acuity through long-term experiential learning. To support\ndiverse clinical tasks, EndoAgent integrates a suite of expert-designed tools\nwithin a unified reasoning loop. We further introduce EndoAgentBench, a\nbenchmark of 5,709 visual question-answer pairs that assess visual\nunderstanding and language generation capabilities in realistic scenarios.\nExtensive experiments show that EndoAgent consistently outperforms both general\nand medical multimodal models, exhibiting its strong flexibility and reasoning\ncapabilities.", "AI": {"tldr": "EndoAgent\u662f\u4e00\u79cd\u57fa\u4e8e\u53cc\u8bb0\u5fc6\u8bbe\u8ba1\u7684AI\u4ee3\u7406\uff0c\u7528\u4e8e\u5185\u7aa5\u955c\u56fe\u50cf\u5206\u6790\uff0c\u7ed3\u5408\u8fed\u4ee3\u63a8\u7406\u548c\u81ea\u9002\u5e94\u5de5\u5177\u9009\u62e9\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u591a\u6a21\u6001\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u7684\u65b9\u6cd5\u5728\u591a\u4efb\u52a1\u534f\u8c03\u548c\u590d\u6742\u4e34\u5e8a\u6d41\u7a0b\u5904\u7406\u4e0a\u8868\u73b0\u4e0d\u8db3\uff0cAI\u4ee3\u7406\u5728\u5185\u7aa5\u955c\u9886\u57df\u7684\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u5f00\u53d1\u3002", "method": "EndoAgent\u91c7\u7528\u53cc\u8bb0\u5fc6\u8bbe\u8ba1\uff0c\u77ed\u671f\u884c\u52a8\u8ddf\u8e2a\u786e\u4fdd\u903b\u8f91\u8fde\u8d2f\uff0c\u957f\u671f\u7ecf\u9a8c\u5b66\u4e60\u63d0\u5347\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u96c6\u6210\u4e13\u5bb6\u8bbe\u8ba1\u5de5\u5177\u3002", "result": "\u5b9e\u9a8c\u8868\u660eEndoAgent\u5728\u89c6\u89c9\u7406\u89e3\u548c\u8bed\u8a00\u751f\u6210\u4efb\u52a1\u4e0a\u4f18\u4e8e\u901a\u7528\u53ca\u533b\u5b66\u591a\u6a21\u6001\u6a21\u578b\u3002", "conclusion": "EndoAgent\u5c55\u793a\u4e86\u5f3a\u5927\u7684\u7075\u6d3b\u6027\u548c\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u5185\u7aa5\u955c\u8bca\u65adAI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2508.07319", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07319", "abs": "https://arxiv.org/abs/2508.07319", "authors": ["Yanzhao Yu", "Haotian Yang", "Junbo Tan", "Xueqian Wang"], "title": "A Hybrid Force-Position Strategy for Shape Control of Deformable Linear Objects With Graph Attention Networks", "comment": null, "summary": "Manipulating deformable linear objects (DLOs) such as wires and cables is\ncrucial in various applications like electronics assembly and medical\nsurgeries. However, it faces challenges due to DLOs' infinite degrees of\nfreedom, complex nonlinear dynamics, and the underactuated nature of the\nsystem. To address these issues, this paper proposes a hybrid force-position\nstrategy for DLO shape control. The framework, combining both force and\nposition representations of DLO, integrates state trajectory planning in the\nforce space and Model Predictive Control (MPC) in the position space. We\npresent a dynamics model with an explicit action encoder, a property extractor\nand a graph processor based on Graph Attention Networks. The model is used in\nthe MPC to enhance prediction accuracy. Results from both simulations and\nreal-world experiments demonstrate the effectiveness of our approach in\nachieving efficient and stable shape control of DLOs. Codes and videos are\navailable at https://sites.google.com/view/dlom.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u529b-\u4f4d\u7f6e\u7b56\u7565\uff0c\u7528\u4e8e\u53d8\u5f62\u7ebf\u6027\u7269\u4f53\uff08DLO\uff09\u7684\u5f62\u72b6\u63a7\u5236\uff0c\u7ed3\u5408\u529b\u7a7a\u95f4\u7684\u72b6\u6001\u8f68\u8ff9\u89c4\u5212\u548c\u4f4d\u7f6e\u7a7a\u95f4\u7684\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u3002", "motivation": "DLO\uff08\u5982\u7535\u7ebf\u3001\u7535\u7f06\uff09\u7684\u64cd\u63a7\u5728\u7535\u5b50\u7ec4\u88c5\u548c\u533b\u7597\u624b\u672f\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u5176\u65e0\u9650\u81ea\u7531\u5ea6\u3001\u590d\u6742\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u548c\u7cfb\u7edf\u6b20\u9a71\u52a8\u7279\u6027\uff0c\u9762\u4e34\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408\u529b\u548c\u4f4d\u7f6e\u8868\u793a\u7684\u6846\u67b6\uff0c\u5305\u62ec\u529b\u7a7a\u95f4\u7684\u72b6\u6001\u8f68\u8ff9\u89c4\u5212\u548c\u4f4d\u7f6e\u7a7a\u95f4\u7684MPC\u3002\u6a21\u578b\u5305\u542b\u52a8\u4f5c\u7f16\u7801\u5668\u3001\u5c5e\u6027\u63d0\u53d6\u5668\u548c\u57fa\u4e8e\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\u7684\u56fe\u5904\u7406\u5668\u3002", "result": "\u4eff\u771f\u548c\u5b9e\u9645\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u9ad8\u6548\u7a33\u5b9a\u5730\u63a7\u5236DLO\u7684\u5f62\u72b6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86DLO\u5f62\u72b6\u63a7\u5236\u7684\u6311\u6218\uff0c\u4ee3\u7801\u548c\u89c6\u9891\u5df2\u516c\u5f00\u3002"}}
{"id": "2508.07334", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07334", "abs": "https://arxiv.org/abs/2508.07334", "authors": ["Quan Shi", "Wang Xi", "Zenghui Ding", "Jianqing Gao", "Xianjun Yang"], "title": "Hallucination as a Computational Boundary: A Hierarchy of Inevitability and the Oracle Escape", "comment": "8 pages, 6 figures", "summary": "The illusion phenomenon of large language models (LLMs) is the core obstacle\nto their reliable deployment. This article formalizes the large language model\nas a probabilistic Turing machine by constructing a \"computational necessity\nhierarchy\", and for the first time proves the illusions are inevitable on\ndiagonalization, incomputability, and information theory boundaries supported\nby the new \"learner pump lemma\". However, we propose two \"escape routes\": one\nis to model Retrieval Enhanced Generations (RAGs) as oracle machines, proving\ntheir absolute escape through \"computational jumps\", providing the first formal\ntheory for the effectiveness of RAGs; The second is to formalize continuous\nlearning as an \"internalized oracle\" mechanism and implement this path through\na novel neural game theory framework.Finally, this article proposes a", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5f62\u5f0f\u5316\u5927\u8bed\u8a00\u6a21\u578b\u4e3a\u6982\u7387\u56fe\u7075\u673a\uff0c\u8bc1\u660e\u5e7b\u89c9\u73b0\u8c61\u5728\u8ba1\u7b97\u5fc5\u8981\u6027\u5c42\u6b21\u4e0a\u662f\u4e0d\u53ef\u907f\u514d\u7684\uff0c\u5e76\u63d0\u51fa\u4e24\u79cd\u89e3\u51b3\u65b9\u6848\uff1a\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u548c\u6301\u7eed\u5b66\u4e60\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5e7b\u89c9\u73b0\u8c61\u7684\u6838\u5fc3\u969c\u788d\uff0c\u4e3a\u5176\u53ef\u9760\u90e8\u7f72\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u3002", "method": "\u6784\u5efa'\u8ba1\u7b97\u5fc5\u8981\u6027\u5c42\u6b21'\uff0c\u8bc1\u660e\u5e7b\u89c9\u7684\u4e0d\u53ef\u907f\u514d\u6027\uff1b\u63d0\u51faRAG\u4f5c\u4e3a\u9884\u8a00\u673a\u6a21\u578b\u548c\u6301\u7eed\u5b66\u4e60\u4f5c\u4e3a'\u5185\u5316\u9884\u8a00\u673a'\u673a\u5236\u3002", "result": "\u8bc1\u660e\u4e86\u5e7b\u89c9\u7684\u4e0d\u53ef\u907f\u514d\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e24\u79cd\u53ef\u80fd\u7684\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u901a\u8fc7\u5f62\u5f0f\u5316\u7406\u8bba\u548c\u521b\u65b0\u6846\u67b6\uff0c\u4e3aLLMs\u7684\u53ef\u9760\u90e8\u7f72\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u548c\u5b9e\u8df5\u8def\u5f84\u3002"}}
{"id": "2508.07323", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.07323", "abs": "https://arxiv.org/abs/2508.07323", "authors": ["Adeetya Uppal", "Rakesh Kumar Sahoo", "Manoranjan Sinha"], "title": "Collision-Free Trajectory Planning and control of Robotic Manipulator using Energy-Based Artificial Potential Field (E-APF)", "comment": null, "summary": "Robotic trajectory planning in dynamic and cluttered environments remains a\ncritical challenge, particularly when striving for both time efficiency and\nmotion smoothness under actuation constraints. Traditional path planner, such\nas Artificial Potential Field (APF), offer computational efficiency but suffer\nfrom local minima issue due to position-based potential field functions and\noscillatory motion near the obstacles due to Newtonian mechanics. To address\nthis limitation, an Energy-based Artificial Potential Field (APF) framework is\nproposed in this paper that integrates position and velocity-dependent\npotential functions. E-APF ensures dynamic adaptability and mitigates local\nminima, enabling uninterrupted progression toward the goal. The proposed\nframework integrates E-APF with a hybrid trajectory optimizer that jointly\nminimizes jerk and execution time under velocity and acceleration constraints,\nensuring geometric smoothness and time efficiency. The entire framework is\nvalidated in simulation using the 7-degree-of-freedom Kinova Gen3 robotic\nmanipulator. The results demonstrate collision-free, smooth, time-efficient,\nand oscillation-free trajectory in the presence of obstacles, highlighting the\nefficacy of the combined trajectory optimization and real-time obstacle\navoidance approach. This work lays the foundation for future integration with\nreactive control strategies and physical hardware deployment in real-world\nmanipulation tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u80fd\u91cf\u7684APF\u6846\u67b6\uff08E-APF\uff09\uff0c\u7ed3\u5408\u4f4d\u7f6e\u548c\u901f\u5ea6\u4f9d\u8d56\u7684\u52bf\u80fd\u51fd\u6570\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfAPF\u7684\u5c40\u90e8\u6781\u5c0f\u503c\u548c\u632f\u8361\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u6df7\u5408\u8f68\u8ff9\u4f18\u5316\u5668\u5b9e\u73b0\u5e73\u6ed1\u4e14\u9ad8\u6548\u7684\u8f68\u8ff9\u89c4\u5212\u3002", "motivation": "\u52a8\u6001\u548c\u590d\u6742\u73af\u5883\u4e2d\u673a\u5668\u4eba\u8f68\u8ff9\u89c4\u5212\u7684\u5173\u952e\u6311\u6218\u5728\u4e8e\u540c\u65f6\u6ee1\u8db3\u65f6\u95f4\u6548\u7387\u548c\u8fd0\u52a8\u5e73\u6ed1\u6027\uff0c\u4f20\u7edfAPF\u65b9\u6cd5\u5b58\u5728\u5c40\u90e8\u6781\u5c0f\u503c\u548c\u632f\u8361\u95ee\u9898\u3002", "method": "\u63d0\u51faE-APF\u6846\u67b6\uff0c\u7ed3\u5408\u4f4d\u7f6e\u548c\u901f\u5ea6\u4f9d\u8d56\u7684\u52bf\u80fd\u51fd\u6570\uff0c\u5e76\u4e0e\u6df7\u5408\u8f68\u8ff9\u4f18\u5316\u5668\u8054\u5408\u4f18\u5316\uff0c\u6700\u5c0f\u5316\u6025\u52a8\u548c\u6267\u884c\u65f6\u95f4\u3002", "result": "\u57287\u81ea\u7531\u5ea6Kinova Gen3\u673a\u68b0\u81c2\u4eff\u771f\u4e2d\u9a8c\u8bc1\uff0c\u5b9e\u73b0\u4e86\u65e0\u78b0\u649e\u3001\u5e73\u6ed1\u3001\u9ad8\u6548\u4e14\u65e0\u632f\u8361\u7684\u8f68\u8ff9\u3002", "conclusion": "E-APF\u6846\u67b6\u4e3a\u672a\u6765\u4e0e\u53cd\u5e94\u63a7\u5236\u7b56\u7565\u548c\u5b9e\u9645\u786c\u4ef6\u90e8\u7f72\u7684\u96c6\u6210\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.07353", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07353", "abs": "https://arxiv.org/abs/2508.07353", "authors": ["Rubing Chen", "Jiaxin Wu", "Jian Wang", "Xulu Zhang", "Wenqi Fan", "Chenghua Lin", "Xiao-Yong Wei", "Qing Li"], "title": "Rethinking Domain-Specific LLM Benchmark Construction: A Comprehensiveness-Compactness Approach", "comment": null, "summary": "Numerous benchmarks have been built to evaluate the domain-specific abilities\nof large language models (LLMs), highlighting the need for effective and\nefficient benchmark construction. Existing domain-specific benchmarks primarily\nfocus on the scaling law, relying on massive corpora for supervised fine-tuning\nor generating extensive question sets for broad coverage. However, the impact\nof corpus and question-answer (QA) set design on the precision and recall of\ndomain-specific LLMs remains unexplored. In this paper, we address this gap and\ndemonstrate that the scaling law is not always the optimal principle for\nbenchmark construction in specific domains. Instead, we propose Comp-Comp, an\niterative benchmarking framework based on a comprehensiveness-compactness\nprinciple. Here, comprehensiveness ensures semantic recall of the domain, while\ncompactness enhances precision, guiding both corpus and QA set construction. To\nvalidate our framework, we conducted a case study in a well-renowned\nuniversity, resulting in the creation of XUBench, a large-scale and\ncomprehensive closed-domain benchmark. Although we use the academic domain as\nthe case in this work, our Comp-Comp framework is designed to be extensible\nbeyond academia, providing valuable insights for benchmark construction across\nvarious domains.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faComp-Comp\u6846\u67b6\uff0c\u901a\u8fc7\u7efc\u5408\u6027\u548c\u7d27\u51d1\u6027\u539f\u5219\u4f18\u5316\u9886\u57df\u7279\u5b9a\u57fa\u51c6\u6784\u5efa\uff0c\u6311\u6218\u4e86\u4f20\u7edf\u7684\u6269\u5c55\u6cd5\u5219\u3002", "motivation": "\u73b0\u6709\u9886\u57df\u7279\u5b9a\u57fa\u51c6\u4e3b\u8981\u4f9d\u8d56\u6269\u5c55\u6cd5\u5219\uff0c\u4f46\u8bed\u6599\u548cQA\u96c6\u8bbe\u8ba1\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u672a\u88ab\u5145\u5206\u7814\u7a76\u3002", "method": "\u63d0\u51faComp-Comp\u6846\u67b6\uff0c\u7ed3\u5408\u7efc\u5408\u6027\u548c\u7d27\u51d1\u6027\u539f\u5219\uff0c\u6307\u5bfc\u8bed\u6599\u548cQA\u96c6\u6784\u5efa\uff0c\u5e76\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u3002", "result": "\u521b\u5efa\u4e86XUBench\u57fa\u51c6\uff0c\u8bc1\u660eComp-Comp\u6846\u67b6\u5728\u5b66\u672f\u9886\u57df\u7684\u6709\u6548\u6027\uff0c\u5e76\u5c55\u793a\u5176\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "Comp-Comp\u6846\u67b6\u4e3a\u9886\u57df\u7279\u5b9a\u57fa\u51c6\u6784\u5efa\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u9886\u57df\u3002"}}
{"id": "2508.07387", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07387", "abs": "https://arxiv.org/abs/2508.07387", "authors": ["Basant Sharma", "Prajyot Jadhav", "Pranjal Paul", "K. Madhava Krishna", "Arun Kumar Singh"], "title": "MonoMPC: Monocular Vision Based Navigation with Learned Collision Model and Risk-Aware Model Predictive Control", "comment": null, "summary": "Navigating unknown environments with a single RGB camera is challenging, as\nthe lack of depth information prevents reliable collision-checking. While some\nmethods use estimated depth to build collision maps, we found that depth\nestimates from vision foundation models are too noisy for zero-shot navigation\nin cluttered environments.\n  We propose an alternative approach: instead of using noisy estimated depth\nfor direct collision-checking, we use it as a rich context input to a learned\ncollision model. This model predicts the distribution of minimum obstacle\nclearance that the robot can expect for a given control sequence. At inference,\nthese predictions inform a risk-aware MPC planner that minimizes estimated\ncollision risk. Our joint learning pipeline co-trains the collision model and\nrisk metric using both safe and unsafe trajectories. Crucially, our\njoint-training ensures optimal variance in our collision model that improves\nnavigation in highly cluttered environments. Consequently, real-world\nexperiments show 9x and 7x improvements in success rates over NoMaD and the ROS\nstack, respectively. Ablation studies further validate the effectiveness of our\ndesign choices.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u78b0\u649e\u6a21\u578b\uff0c\u5229\u7528RGB\u76f8\u673a\u4f30\u8ba1\u7684\u6df1\u5ea6\u4fe1\u606f\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u8f93\u5165\uff0c\u9884\u6d4b\u969c\u788d\u7269\u6700\u5c0f\u95f4\u8ddd\u5206\u5e03\uff0c\u7ed3\u5408\u98ce\u9669\u611f\u77e5MPC\u89c4\u5212\u5668\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5bfc\u822a\u6210\u529f\u7387\u3002", "motivation": "\u5355RGB\u76f8\u673a\u5728\u672a\u77e5\u73af\u5883\u4e2d\u5bfc\u822a\u65f6\u7f3a\u4e4f\u6df1\u5ea6\u4fe1\u606f\uff0c\u4f20\u7edf\u6df1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\u566a\u58f0\u8fc7\u5927\uff0c\u65e0\u6cd5\u53ef\u9760\u7528\u4e8e\u78b0\u649e\u68c0\u6d4b\u3002", "method": "\u4f7f\u7528\u4f30\u8ba1\u6df1\u5ea6\u4f5c\u4e3a\u5b66\u4e60\u78b0\u649e\u6a21\u578b\u7684\u8f93\u5165\uff0c\u9884\u6d4b\u969c\u788d\u7269\u6700\u5c0f\u95f4\u8ddd\u5206\u5e03\uff0c\u7ed3\u5408\u98ce\u9669\u611f\u77e5MPC\u89c4\u5212\u5668\u8fdb\u884c\u5bfc\u822a\u3002\u901a\u8fc7\u8054\u5408\u8bad\u7ec3\u4f18\u5316\u78b0\u649e\u6a21\u578b\u548c\u98ce\u9669\u5ea6\u91cf\u3002", "result": "\u5728\u771f\u5b9e\u73af\u5883\u4e2d\uff0c\u5bfc\u822a\u6210\u529f\u7387\u6bd4NoMaD\u548cROS\u5806\u6808\u5206\u522b\u63d0\u9ad8\u4e869\u500d\u548c7\u500d\u3002", "conclusion": "\u63d0\u51fa\u7684\u8054\u5408\u5b66\u4e60\u6846\u67b6\u548c\u98ce\u9669\u611f\u77e5\u89c4\u5212\u5668\u663e\u8457\u63d0\u5347\u4e86\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u6027\u80fd\uff0c\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8bbe\u8ba1\u6709\u6548\u6027\u3002"}}
{"id": "2508.07382", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07382", "abs": "https://arxiv.org/abs/2508.07382", "authors": ["He Kong", "Die Hu", "Jingguo Ge", "Liangxiong Li", "Hui Li", "Tong Li"], "title": "Pentest-R1: Towards Autonomous Penetration Testing Reasoning Optimized via Two-Stage Reinforcement Learning", "comment": null, "summary": "Automating penetration testing is crucial for enhancing cybersecurity, yet\ncurrent Large Language Models (LLMs) face significant limitations in this\ndomain, including poor error handling, inefficient reasoning, and an inability\nto perform complex end-to-end tasks autonomously. To address these challenges,\nwe introduce Pentest-R1, a novel framework designed to optimize LLM reasoning\ncapabilities for this task through a two-stage reinforcement learning pipeline.\nWe first construct a dataset of over 500 real-world, multi-step walkthroughs,\nwhich Pentest-R1 leverages for offline reinforcement learning (RL) to instill\nfoundational attack logic. Subsequently, the LLM is fine-tuned via online RL in\nan interactive Capture The Flag (CTF) environment, where it learns directly\nfrom environmental feedback to develop robust error self-correction and\nadaptive strategies. Our extensive experiments on the Cybench and AutoPenBench\nbenchmarks demonstrate the framework's effectiveness. On AutoPenBench,\nPentest-R1 achieves a 24.2\\% success rate, surpassing most state-of-the-art\nmodels and ranking second only to Gemini 2.5 Flash. On Cybench, it attains a\n15.0\\% success rate in unguided tasks, establishing a new state-of-the-art for\nopen-source LLMs and matching the performance of top proprietary models.\nAblation studies confirm that the synergy of both training stages is critical\nto its success.", "AI": {"tldr": "Pentest-R1\u662f\u4e00\u4e2a\u901a\u8fc7\u4e24\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u4f18\u5316LLM\u5728\u6e17\u900f\u6d4b\u8bd5\u4e2d\u63a8\u7406\u80fd\u529b\u7684\u65b0\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u5f53\u524dLLM\u5728\u6e17\u900f\u6d4b\u8bd5\u4e2d\u5b58\u5728\u9519\u8bef\u5904\u7406\u5dee\u3001\u63a8\u7406\u6548\u7387\u4f4e\u548c\u65e0\u6cd5\u81ea\u4e3b\u5b8c\u6210\u590d\u6742\u4efb\u52a1\u7b49\u95ee\u9898\uff0cPentest-R1\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\uff1a\u79bb\u7ebfRL\u5b66\u4e60\u57fa\u7840\u653b\u51fb\u903b\u8f91\uff0c\u5728\u7ebfRL\u5728CTF\u73af\u5883\u4e2d\u901a\u8fc7\u73af\u5883\u53cd\u9988\u5fae\u8c03\u6a21\u578b\u3002", "result": "\u5728AutoPenBench\u4e0a\u6210\u529f\u738724.2%\uff0cCybench\u4e0a15.0%\uff0c\u6027\u80fd\u63a5\u8fd1\u9876\u7ea7\u4e13\u6709\u6a21\u578b\u3002", "conclusion": "\u4e24\u9636\u6bb5\u8bad\u7ec3\u534f\u540c\u662fPentest-R1\u6210\u529f\u7684\u5173\u952e\uff0c\u4e3a\u5f00\u6e90LLM\u8bbe\u7acb\u4e86\u65b0\u6807\u6746\u3002"}}
{"id": "2508.07406", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07406", "abs": "https://arxiv.org/abs/2508.07406", "authors": ["Xiaobei Zhao", "Xingqi Lyu", "Xiang Li"], "title": "AgriVLN: Vision-and-Language Navigation for Agricultural Robots", "comment": null, "summary": "Agricultural robots have emerged as powerful members in agricultural tasks,\nnevertheless, still heavily rely on manual operation or untransportable railway\nfor movement, resulting in limited mobility and poor adaptability.\nVision-and-Language Navigation (VLN) enables robots to navigate to the target\ndestinations following natural language instructions, demonstrating strong\nperformance on several domains. However, none of the existing benchmarks or\nmethods is specifically designed for agricultural scenes. To bridge this gap,\nwe propose Agriculture to Agriculture (A2A) benchmark, containing 1,560\nepisodes across six diverse agricultural scenes, in which all realistic RGB\nvideos are captured by front-facing camera on a quadruped robot at a height of\n0.38 meters, aligning with the practical deployment conditions. Meanwhile, we\npropose Vision-and-Language Navigation for Agricultural Robots (AgriVLN)\nbaseline based on Vision-Language Model (VLM) prompted with carefully crafted\ntemplates, which can understand both given instructions and agricultural\nenvironments to generate appropriate low-level actions for robot control. When\nevaluated on A2A, AgriVLN performs well on short instructions but struggles\nwith long instructions, because it often fails to track which part of the\ninstruction is currently being executed. To address this, we further propose\nSubtask List (STL) instruction decomposition module and integrate it into\nAgriVLN, improving Success Rate (SR) from 0.33 to 0.47. We additionally compare\nAgriVLN with several existing VLN methods, demonstrating the state-of-the-art\nperformance in the agricultural domain.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u519c\u4e1a\u573a\u666f\u4e0b\u7684\u89c6\u89c9\u4e0e\u8bed\u8a00\u5bfc\u822a\u57fa\u51c6A2A\u548c\u57fa\u7ebf\u65b9\u6cd5AgriVLN\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u519c\u4e1a\u9886\u57df\u7684\u4e0d\u8db3\uff0c\u5e76\u901a\u8fc7\u6307\u4ee4\u5206\u89e3\u6a21\u5757\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u519c\u4e1a\u673a\u5668\u4eba\u4f9d\u8d56\u4eba\u5de5\u64cd\u4f5c\u6216\u56fa\u5b9a\u8f68\u9053\uff0c\u79fb\u52a8\u6027\u5dee\uff1b\u73b0\u6709\u89c6\u89c9\u4e0e\u8bed\u8a00\u5bfc\u822a\u65b9\u6cd5\u672a\u9488\u5bf9\u519c\u4e1a\u573a\u666f\u8bbe\u8ba1\u3002", "method": "\u63d0\u51faA2A\u57fa\u51c6\u548cAgriVLN\u57fa\u7ebf\u65b9\u6cd5\uff0c\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u5f15\u5165\u6307\u4ee4\u5206\u89e3\u6a21\u5757STL\u3002", "result": "AgriVLN\u5728\u77ed\u6307\u4ee4\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u957f\u6307\u4ee4\u6267\u884c\u8f83\u5dee\uff1bSTL\u6a21\u5757\u5c06\u6210\u529f\u7387\u4ece0.33\u63d0\u5347\u81f30.47\u3002", "conclusion": "AgriVLN\u5728\u519c\u4e1a\u9886\u57df\u8868\u73b0\u4f18\u5f02\uff0cSTL\u6a21\u5757\u663e\u8457\u63d0\u5347\u957f\u6307\u4ee4\u6267\u884c\u80fd\u529b\u3002"}}
{"id": "2508.07388", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07388", "abs": "https://arxiv.org/abs/2508.07388", "authors": ["Zhaoyu Chen", "Hongnan Lin", "Yongwei Nie", "Fei Ma", "Xuemiao Xu", "Fei Yu", "Chengjiang Long"], "title": "Invert4TVG: A Temporal Video Grounding Framework with Inversion Tasks for Enhanced Action Understanding", "comment": null, "summary": "Temporal Video Grounding (TVG) seeks to localize video segments matching a\ngiven textual query. Current methods, while optimizing for high temporal\nIntersection-over-Union (IoU), often overfit to this metric, compromising\nsemantic action understanding in the video and query, a critical factor for\nrobust TVG. To address this, we introduce Inversion Tasks for TVG (Invert4TVG),\na novel framework that enhances both localization accuracy and action\nunderstanding without additional data. Our approach leverages three inversion\ntasks derived from existing TVG annotations: (1) Verb Completion, predicting\nmasked action verbs in queries from video segments; (2) Action Recognition,\nidentifying query-described actions; and (3) Video Description, generating\ndescriptions of video segments that explicitly embed query-relevant actions.\nThese tasks, integrated with TVG via a reinforcement learning framework with\nwell-designed reward functions, ensure balanced optimization of localization\nand semantics. Experiments show our method outperforms state-of-the-art\napproaches, achieving a 7.1\\% improvement in R1@0.7 on Charades-STA for a 3B\nmodel compared to Time-R1. By inverting TVG to derive query-related actions\nfrom segments, our approach strengthens semantic understanding, significantly\nraising the ceiling of localization accuracy.", "AI": {"tldr": "Invert4TVG\u6846\u67b6\u901a\u8fc7\u4e09\u4e2a\u53cd\u8f6c\u4efb\u52a1\uff08\u52a8\u8bcd\u8865\u5168\u3001\u52a8\u4f5c\u8bc6\u522b\u548c\u89c6\u9891\u63cf\u8ff0\uff09\u589e\u5f3a\u89c6\u9891\u7247\u6bb5\u5b9a\u4f4d\u548c\u8bed\u4e49\u7406\u89e3\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u5f53\u524dTVG\u65b9\u6cd5\u8fc7\u5ea6\u4f18\u5316\u65f6\u95f4IoU\uff0c\u727a\u7272\u8bed\u4e49\u7406\u89e3\uff0c\u5f71\u54cd\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51fa\u4e09\u4e2a\u53cd\u8f6c\u4efb\u52a1\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u4f18\u5316\u5b9a\u4f4d\u548c\u8bed\u4e49\u3002", "result": "\u5728Charades-STA\u4e0aR1@0.7\u63d0\u53477.1%\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u53cd\u8f6c\u4efb\u52a1\u589e\u5f3a\u8bed\u4e49\u7406\u89e3\uff0c\u663e\u8457\u63d0\u9ad8\u5b9a\u4f4d\u7cbe\u5ea6\u4e0a\u9650\u3002"}}
{"id": "2508.07421", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07421", "abs": "https://arxiv.org/abs/2508.07421", "authors": ["Zixi Jia", "Hongbin Gao", "Fashe Li", "Jiqiang Liu", "Hexiao Li", "Qinghua Liu"], "title": "Triple-S: A Collaborative Multi-LLM Framework for Solving Long-Horizon Implicative Tasks in Robotics", "comment": "Accepted to IROS 2025", "summary": "Leveraging Large Language Models (LLMs) to write policy code for controlling\nrobots has gained significant attention. However, in long-horizon implicative\ntasks, this approach often results in API parameter, comments and sequencing\nerrors, leading to task failure. To address this problem, we propose a\ncollaborative Triple-S framework that involves multiple LLMs. Through\nIn-Context Learning, different LLMs assume specific roles in a closed-loop\nSimplification-Solution-Summary process, effectively improving success rates\nand robustness in long-horizon implicative tasks. Additionally, a novel\ndemonstration library update mechanism which learned from success allows it to\ngeneralize to previously failed tasks. We validate the framework in the\nLong-horizon Desktop Implicative Placement (LDIP) dataset across various\nbaseline models, where Triple-S successfully executes 89% of tasks in both\nobservable and partially observable scenarios. Experiments in both simulation\nand real-world robot settings further validated the effectiveness of Triple-S.\nOur code and dataset is available at: https://github.com/Ghbbbbb/Triple-S.", "AI": {"tldr": "Triple-S\u6846\u67b6\u901a\u8fc7\u591aLLM\u534f\u4f5c\uff0c\u6539\u8fdb\u957f\u65f6\u9690\u5f0f\u4efb\u52a1\u4e2d\u7684\u653f\u7b56\u4ee3\u7801\u751f\u6210\uff0c\u6210\u529f\u7387\u8fbe89%\u3002", "motivation": "\u89e3\u51b3LLM\u5728\u957f\u65f6\u9690\u5f0f\u4efb\u52a1\u4e2d\u751f\u6210\u653f\u7b56\u4ee3\u7801\u65f6\u7684API\u53c2\u6570\u3001\u6ce8\u91ca\u548c\u987a\u5e8f\u9519\u8bef\u95ee\u9898\u3002", "method": "\u63d0\u51faTriple-S\u6846\u67b6\uff0c\u5229\u7528\u591aLLM\u5728\u7b80\u5316-\u89e3\u51b3-\u603b\u7ed3\u95ed\u73af\u8fc7\u7a0b\u4e2d\u5206\u5de5\u534f\u4f5c\uff0c\u5e76\u7ed3\u5408\u6f14\u793a\u5e93\u66f4\u65b0\u673a\u5236\u3002", "result": "\u5728LDIP\u6570\u636e\u96c6\u4e2d\uff0cTriple-S\u5728\u53ef\u89c2\u5bdf\u548c\u90e8\u5206\u53ef\u89c2\u5bdf\u573a\u666f\u4e0b\u4efb\u52a1\u6210\u529f\u7387\u8fbe89%\u3002", "conclusion": "Triple-S\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u957f\u65f6\u9690\u5f0f\u4efb\u52a1\u7684\u6267\u884c\u6210\u529f\u7387\u548c\u9c81\u68d2\u6027\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2508.07405", "categories": ["cs.AI", "cs.CL", "cs.LG", "I.2.7; I.5.4"], "pdf": "https://arxiv.org/pdf/2508.07405", "abs": "https://arxiv.org/abs/2508.07405", "authors": ["Jesse Ponnock"], "title": "Generative AI for Strategic Plan Development", "comment": "11 pages, 9 figures", "summary": "Given recent breakthroughs in Generative Artificial Intelligence (GAI) and\nLarge Language Models (LLMs), more and more professional services are being\naugmented through Artificial Intelligence (AI), which once seemed impossible to\nautomate. This paper presents a modular model for leveraging GAI in developing\nstrategic plans for large scale government organizations and evaluates leading\nmachine learning techniques in their application towards one of the identified\nmodules. Specifically, the performance of BERTopic and Non-negative Matrix\nFactorization (NMF) are evaluated in their ability to use topic modeling to\ngenerate themes representative of Vision Elements within a strategic plan. To\naccomplish this, BERTopic and NMF models are trained using a large volume of\nreports from the Government Accountability Office (GAO). The generated topics\nfrom each model are then scored for similarity against the Vision Elements of a\npublished strategic plan and the results are compared. Our results show that\nthese techniques are capable of generating themes similar to 100% of the\nelements being evaluated against. Further, we conclude that BERTopic performs\nbest in this application with more than half of its correlated topics achieving\na \"medium\" or \"strong\" correlation. A capability of GAI-enabled strategic plan\ndevelopment impacts a multi-billion dollar industry and assists the federal\ngovernment in overcoming regulatory requirements which are crucial to the\npublic good. Further work will focus on the operationalization of the concept\nproven in this study as well as viability of the remaining modules in the\nproposed model for GAI-generated strategic plans.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\uff08GAI\uff09\u4e3a\u5927\u578b\u653f\u5e9c\u7ec4\u7ec7\u5236\u5b9a\u6218\u7565\u8ba1\u5212\u7684\u6a21\u5757\u5316\u6a21\u578b\uff0c\u5e76\u8bc4\u4f30\u4e86BERTopic\u548c\u975e\u8d1f\u77e9\u9635\u5206\u89e3\uff08NMF\uff09\u5728\u4e3b\u9898\u5efa\u6a21\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\uff08GAI\uff09\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u7a81\u7834\uff0c\u8d8a\u6765\u8d8a\u591a\u7684\u4e13\u4e1a\u670d\u52a1\u901a\u8fc7AI\u589e\u5f3a\uff0c\u672c\u6587\u65e8\u5728\u63a2\u7d22GAI\u5728\u6218\u7565\u8ba1\u5212\u5236\u5b9a\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u4f7f\u7528BERTopic\u548cNMF\u6a21\u578b\u5bf9\u653f\u5e9c\u95ee\u8d23\u529e\u516c\u5ba4\uff08GAO\uff09\u7684\u5927\u91cf\u62a5\u544a\u8fdb\u884c\u4e3b\u9898\u5efa\u6a21\uff0c\u751f\u6210\u4e0e\u6218\u7565\u8ba1\u5212\u4e2d\u613f\u666f\u8981\u7d20\u76f8\u4f3c\u7684\u4e3b\u9898\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff0c\u8fd9\u4e9b\u6280\u672f\u80fd\u591f\u751f\u6210\u4e0e100%\u7684\u613f\u666f\u8981\u7d20\u76f8\u4f3c\u7684\u4e3b\u9898\uff0c\u4e14BERTopic\u8868\u73b0\u6700\u4f73\uff0c\u8d85\u8fc7\u4e00\u534a\u7684\u4e3b\u9898\u8fbe\u5230\u201c\u4e2d\u7b49\u201d\u6216\u201c\u5f3a\u201d\u76f8\u5173\u6027\u3002", "conclusion": "GAI\u5728\u6218\u7565\u8ba1\u5212\u5236\u5b9a\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u672a\u6765\u7814\u7a76\u5c06\u805a\u7126\u4e8e\u6a21\u578b\u7684\u5b9e\u9645\u5e94\u7528\u548c\u5269\u4f59\u6a21\u5757\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2508.07502", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07502", "abs": "https://arxiv.org/abs/2508.07502", "authors": ["Mateus Salom\u00e3o", "Tiany\u00fc Ren", "Alexander K\u00f6nig"], "title": "A Learning-Based Framework for Collision-Free Motion Planning", "comment": null, "summary": "This paper presents a learning-based extension to a Circular Field (CF)-based\nmotion planner for efficient, collision-free trajectory generation in cluttered\nenvironments. The proposed approach overcomes the limitations of hand-tuned\nforce field parameters by employing a deep neural network trained to infer\noptimal planner gains from a single depth image of the scene. The pipeline\nincorporates a CUDA-accelerated perception module, a predictive agent-based\nplanning strategy, and a dataset generated through Bayesian optimization in\nsimulation. The resulting framework enables real-time planning without manual\nparameter tuning and is validated both in simulation and on a Franka Emika\nPanda robot. Experimental results demonstrate successful task completion and\nimproved generalization compared to classical planners.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u73af\u5f62\u573a\u8fd0\u52a8\u89c4\u5212\u5668\u6269\u5c55\uff0c\u901a\u8fc7\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u4ece\u5355\u5f20\u6df1\u5ea6\u56fe\u50cf\u63a8\u65ad\u6700\u4f18\u89c4\u5212\u589e\u76ca\uff0c\u5b9e\u73b0\u9ad8\u6548\u65e0\u78b0\u649e\u8f68\u8ff9\u751f\u6210\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u624b\u52a8\u8c03\u6574\u529b\u573a\u53c2\u6570\u7684\u5c40\u9650\u6027\uff0c\u63d0\u9ad8\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u89c4\u5212\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u7ed3\u5408CUDA\u52a0\u901f\u7684\u611f\u77e5\u6a21\u5757\u3001\u57fa\u4e8e\u9884\u6d4b\u7684\u4ee3\u7406\u89c4\u5212\u7b56\u7565\uff0c\u4ee5\u53ca\u901a\u8fc7\u8d1d\u53f6\u65af\u4f18\u5316\u751f\u6210\u7684\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5b9e\u65f6\u89c4\u5212\u65e0\u9700\u624b\u52a8\u8c03\u53c2\uff0c\u4efb\u52a1\u5b8c\u6210\u6210\u529f\u7387\u9ad8\uff0c\u6cdb\u5316\u80fd\u529b\u4f18\u4e8e\u4f20\u7edf\u89c4\u5212\u5668\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u4eff\u771f\u548c\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u4e3a\u590d\u6742\u73af\u5883\u4e2d\u7684\u8fd0\u52a8\u89c4\u5212\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.07407", "categories": ["cs.AI", "cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.07407", "abs": "https://arxiv.org/abs/2508.07407", "authors": ["Jinyuan Fang", "Yanwen Peng", "Xi Zhang", "Yingxu Wang", "Xinhao Yi", "Guibin Zhang", "Yi Xu", "Bin Wu", "Siwei Liu", "Zihao Li", "Zhaochun Ren", "Nikos Aletras", "Xi Wang", "Han Zhou", "Zaiqiao Meng"], "title": "A Comprehensive Survey of Self-Evolving AI Agents: A New Paradigm Bridging Foundation Models and Lifelong Agentic Systems", "comment": null, "summary": "Recent advances in large language models have sparked growing interest in AI\nagents capable of solving complex, real-world tasks. However, most existing\nagent systems rely on manually crafted configurations that remain static after\ndeployment, limiting their ability to adapt to dynamic and evolving\nenvironments. To this end, recent research has explored agent evolution\ntechniques that aim to automatically enhance agent systems based on interaction\ndata and environmental feedback. This emerging direction lays the foundation\nfor self-evolving AI agents, which bridge the static capabilities of foundation\nmodels with the continuous adaptability required by lifelong agentic systems.\nIn this survey, we provide a comprehensive review of existing techniques for\nself-evolving agentic systems. Specifically, we first introduce a unified\nconceptual framework that abstracts the feedback loop underlying the design of\nself-evolving agentic systems. The framework highlights four key components:\nSystem Inputs, Agent System, Environment, and Optimisers, serving as a\nfoundation for understanding and comparing different strategies. Based on this\nframework, we systematically review a wide range of self-evolving techniques\nthat target different components of the agent system. We also investigate\ndomain-specific evolution strategies developed for specialised fields such as\nbiomedicine, programming, and finance, where optimisation objectives are\ntightly coupled with domain constraints. In addition, we provide a dedicated\ndiscussion on the evaluation, safety, and ethical considerations for\nself-evolving agentic systems, which are critical to ensuring their\neffectiveness and reliability. This survey aims to provide researchers and\npractitioners with a systematic understanding of self-evolving AI agents,\nlaying the foundation for the development of more adaptive, autonomous, and\nlifelong agentic systems.", "AI": {"tldr": "\u7efc\u8ff0\u4e86\u81ea\u8fdb\u5316AI\u4ee3\u7406\u7cfb\u7edf\u7684\u6280\u672f\uff0c\u63d0\u51fa\u4e86\u7edf\u4e00\u6846\u67b6\u5e76\u63a2\u8ba8\u4e86\u9886\u57df\u4e13\u7528\u7b56\u7565\u53ca\u4f26\u7406\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u4ee3\u7406\u7cfb\u7edf\u4f9d\u8d56\u9759\u6001\u914d\u7f6e\uff0c\u65e0\u6cd5\u9002\u5e94\u52a8\u6001\u73af\u5883\uff0c\u9700\u7814\u7a76\u81ea\u8fdb\u5316\u6280\u672f\u4ee5\u5b9e\u73b0\u6301\u7eed\u9002\u5e94\u6027\u3002", "method": "\u63d0\u51fa\u7edf\u4e00\u6846\u67b6\uff0c\u7cfb\u7edf\u56de\u987e\u81ea\u8fdb\u5316\u6280\u672f\uff0c\u5e76\u5206\u6790\u9886\u57df\u4e13\u7528\u7b56\u7565\u53ca\u8bc4\u4f30\u3001\u5b89\u5168\u3001\u4f26\u7406\u95ee\u9898\u3002", "result": "\u603b\u7ed3\u4e86\u81ea\u8fdb\u5316\u4ee3\u7406\u7cfb\u7edf\u7684\u5173\u952e\u7ec4\u4ef6\u548c\u6280\u672f\uff0c\u4e3a\u5f00\u53d1\u81ea\u9002\u5e94\u7cfb\u7edf\u5960\u5b9a\u57fa\u7840\u3002", "conclusion": "\u81ea\u8fdb\u5316AI\u4ee3\u7406\u662f\u672a\u6765\u53d1\u5c55\u65b9\u5411\uff0c\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u4ee5\u786e\u4fdd\u5176\u6709\u6548\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2508.07560", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07560", "abs": "https://arxiv.org/abs/2508.07560", "authors": ["Yan Gong", "Naibang Wang", "Jianli Lu", "Xinyu Zhang", "Yongsheng Gao", "Jie Zhao", "Zifan Huang", "Haozhi Bai", "Nanxin Zeng", "Nayu Su", "Lei Yang", "Ziying Song", "Xiaoxi Hu", "Xinmin Jiang", "Xiaojuan Zhang", "Susanto Rahardja"], "title": "Progressive Bird's Eye View Perception for Safety-Critical Autonomous Driving: A Comprehensive Survey", "comment": null, "summary": "Bird's-Eye-View (BEV) perception has become a foundational paradigm in\nautonomous driving, enabling unified spatial representations that support\nrobust multi-sensor fusion and multi-agent collaboration. As autonomous\nvehicles transition from controlled environments to real-world deployment,\nensuring the safety and reliability of BEV perception in complex scenarios -\nsuch as occlusions, adverse weather, and dynamic traffic - remains a critical\nchallenge. This survey provides the first comprehensive review of BEV\nperception from a safety-critical perspective, systematically analyzing\nstate-of-the-art frameworks and implementation strategies across three\nprogressive stages: single-modality vehicle-side, multimodal vehicle-side, and\nmulti-agent collaborative perception. Furthermore, we examine public datasets\nencompassing vehicle-side, roadside, and collaborative settings, evaluating\ntheir relevance to safety and robustness. We also identify key open-world\nchallenges - including open-set recognition, large-scale unlabeled data, sensor\ndegradation, and inter-agent communication latency - and outline future\nresearch directions, such as integration with end-to-end autonomous driving\nsystems, embodied intelligence, and large language models.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u81ea\u52a8\u9a7e\u9a76\u4e2d\u9e1f\u77b0\u56fe\uff08BEV\uff09\u611f\u77e5\u7684\u5b89\u5168\u5173\u952e\u89c6\u89d2\uff0c\u5206\u6790\u4e86\u5355\u6a21\u6001\u3001\u591a\u6a21\u6001\u53ca\u591a\u4ee3\u7406\u534f\u4f5c\u611f\u77e5\u7684\u6846\u67b6\uff0c\u5e76\u63a2\u8ba8\u4e86\u5f00\u653e\u4e16\u754c\u4e2d\u7684\u6311\u6218\u4e0e\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u81ea\u52a8\u9a7e\u9a76\u8f66\u8f86\u4ece\u53d7\u63a7\u73af\u5883\u8f6c\u5411\u5b9e\u9645\u90e8\u7f72\uff0c\u786e\u4fddBEV\u611f\u77e5\u5728\u590d\u6742\u573a\u666f\uff08\u5982\u906e\u6321\u3001\u6076\u52a3\u5929\u6c14\u548c\u52a8\u6001\u4ea4\u901a\uff09\u4e2d\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u6210\u4e3a\u5173\u952e\u6311\u6218\u3002", "method": "\u7cfb\u7edf\u5206\u6790\u4e86BEV\u611f\u77e5\u7684\u4e09\u4e2a\u6e10\u8fdb\u9636\u6bb5\uff1a\u5355\u6a21\u6001\u8f66\u8f86\u4fa7\u3001\u591a\u6a21\u6001\u8f66\u8f86\u4fa7\u548c\u591a\u4ee3\u7406\u534f\u4f5c\u611f\u77e5\uff0c\u5e76\u8bc4\u4f30\u4e86\u76f8\u5173\u516c\u5171\u6570\u636e\u96c6\u7684\u5b89\u5168\u6027\u548c\u9c81\u68d2\u6027\u3002", "result": "\u8bc6\u522b\u4e86\u5f00\u653e\u4e16\u754c\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u5982\u5f00\u653e\u96c6\u8bc6\u522b\u3001\u5927\u89c4\u6a21\u672a\u6807\u8bb0\u6570\u636e\u3001\u4f20\u611f\u5668\u9000\u5316\u548c\u4ee3\u7406\u95f4\u901a\u4fe1\u5ef6\u8fdf\u3002", "conclusion": "\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u5305\u62ec\u4e0e\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u3001\u5177\u8eab\u667a\u80fd\u548c\u5927\u8bed\u8a00\u6a21\u578b\u7684\u96c6\u6210\u3002"}}
{"id": "2508.07466", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07466", "abs": "https://arxiv.org/abs/2508.07466", "authors": ["Dom Huh", "Prasant Mohapatra"], "title": "Grounding Natural Language for Multi-agent Decision-Making with Multi-agentic LLMs", "comment": null, "summary": "Language is a ubiquitous tool that is foundational to reasoning and\ncollaboration, ranging from everyday interactions to sophisticated\nproblem-solving tasks. The establishment of a common language can serve as a\npowerful asset in ensuring clear communication and understanding amongst\nagents, facilitating desired coordination and strategies. In this work, we\nextend the capabilities of large language models (LLMs) by integrating them\nwith advancements in multi-agent decision-making algorithms. We propose a\nsystematic framework for the design of multi-agentic large language models\n(LLMs), focusing on key integration practices. These include advanced prompt\nengineering techniques, the development of effective memory architectures,\nmulti-modal information processing, and alignment strategies through\nfine-tuning algorithms. We evaluate these design choices through extensive\nablation studies on classic game settings with significant underlying social\ndilemmas and game-theoretic considerations.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u6846\u67b6\uff0c\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e0e\u591a\u667a\u80fd\u4f53\u51b3\u7b56\u7b97\u6cd5\u7ed3\u5408\uff0c\u4ee5\u589e\u5f3a\u5176\u534f\u4f5c\u548c\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u8bed\u8a00\u662f\u534f\u4f5c\u548c\u63a8\u7406\u7684\u57fa\u7840\uff0c\u901a\u8fc7\u5efa\u7acb\u5171\u540c\u8bed\u8a00\u53ef\u4ee5\u4fc3\u8fdb\u667a\u80fd\u4f53\u95f4\u7684\u6e05\u6670\u6c9f\u901a\u548c\u534f\u8c03\u3002", "method": "\u63d0\u51fa\u591a\u667a\u80fd\u4f53LLMs\u7684\u8bbe\u8ba1\u6846\u67b6\uff0c\u5305\u62ec\u9ad8\u7ea7\u63d0\u793a\u5de5\u7a0b\u3001\u5185\u5b58\u67b6\u6784\u3001\u591a\u6a21\u6001\u4fe1\u606f\u5904\u7406\u548c\u5fae\u8c03\u7b97\u6cd5\u3002", "result": "\u5728\u7ecf\u5178\u6e38\u620f\u73af\u5883\u4e2d\u8fdb\u884c\u6d88\u878d\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u8bbe\u8ba1\u9009\u62e9\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u591a\u667a\u80fd\u4f53LLMs\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u7684\u65b9\u6cd5\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u534f\u4f5c\u548c\u51b3\u7b56\u80fd\u529b\u3002"}}
{"id": "2508.07566", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07566", "abs": "https://arxiv.org/abs/2508.07566", "authors": ["Conor K. Trygstad", "Cody R. Longwell", "Francisco M. F. R. Gon\u00e7alves", "Elijah K. Blankenship", "N\u00e9stor O. P\u00e9rez-Arancibia"], "title": "Feedback Control of a Single-Tail Bioinspired 59-mg Swimmer", "comment": "To be presented at the 2025 IEEE/RSJ International Conference on\n  Intelligent Robots and Systems (IROS 2025)", "summary": "We present an evolved steerable version of the single-tail\nFish-&-Ribbon-Inspired Small Swimming Harmonic roBot (FRISSHBot), a 59-mg\nbiologically inspired swimmer, which is driven by a new shape-memory alloy\n(SMA)-based bimorph actuator. The new FRISSHBot is controllable in the\ntwo-dimensional (2D) space, which enabled the first demonstration of\nfeedback-controlled trajectory tracking of a single-tail aquatic robot with\nonboard actuation at the subgram scale. These new capabilities are the result\nof a physics-informed design with an enlarged head and shortened tail relative\nto those of the original platform. Enhanced by its design, this new platform\nachieves forward swimming speeds of up to 13.6 mm/s (0.38 Bl/s), which is over\nfour times that of the original platform. Furthermore, when following 2D\nreferences in closed loop, the tested FRISSHBot prototype attains forward\nswimming speeds of up to 9.1 mm/s, root-mean-square (RMS) tracking errors as\nlow as 2.6 mm, turning rates of up to 13.1 {\\deg}/s, and turning radii as small\nas 10 mm.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u6539\u8fdb\u7248\u7684FRISSHBot\uff0c\u4e00\u79cd\u53d7\u751f\u7269\u542f\u53d1\u7684\u5fae\u578b\u6e38\u6cf3\u673a\u5668\u4eba\uff0c\u901a\u8fc7\u65b0\u578bSMA\u53cc\u538b\u7535\u6676\u7247\u9a71\u52a8\u5668\u5b9e\u73b0\u4e8c\u7ef4\u7a7a\u95f4\u63a7\u5236\uff0c\u5e76\u9996\u6b21\u5c55\u793a\u4e86\u4e9a\u514b\u7ea7\u5355\u5c3e\u6c34\u4e0b\u673a\u5668\u4eba\u7684\u53cd\u9988\u63a7\u5236\u8f68\u8ff9\u8ddf\u8e2a\u3002", "motivation": "\u6539\u8fdb\u539f\u59cbFRISSHBot\u7684\u8bbe\u8ba1\uff0c\u4ee5\u5b9e\u73b0\u66f4\u9ad8\u7684\u6e38\u6cf3\u901f\u5ea6\u548c\u7cbe\u786e\u7684\u8f68\u8ff9\u8ddf\u8e2a\u80fd\u529b\u3002", "method": "\u91c7\u7528\u7269\u7406\u4fe1\u606f\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u589e\u5927\u5934\u90e8\u5e76\u7f29\u77ed\u5c3e\u90e8\uff0c\u7ed3\u5408\u65b0\u578bSMA\u9a71\u52a8\u5668\u3002", "result": "\u6539\u8fdb\u540e\u7684\u673a\u5668\u4eba\u6700\u9ad8\u6e38\u6cf3\u901f\u5ea6\u8fbe13.6 mm/s\uff08\u662f\u539f\u7248\u7684\u56db\u500d\uff09\uff0c\u95ed\u73af\u8ddf\u8e2a\u65f6\u901f\u5ea6\u8fbe9.1 mm/s\uff0c\u8ddf\u8e2a\u8bef\u5dee\u4f4e\u81f32.6 mm\uff0c\u8f6c\u5f2f\u534a\u5f84\u5c0f\u81f310 mm\u3002", "conclusion": "\u65b0\u578bFRISSHBot\u5728\u901f\u5ea6\u548c\u7cbe\u786e\u63a7\u5236\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u539f\u7248\uff0c\u4e3a\u5fae\u578b\u6c34\u4e0b\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u65b0\u53ef\u80fd\u3002"}}
{"id": "2508.07468", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.07468", "abs": "https://arxiv.org/abs/2508.07468", "authors": ["Stefan Szeider"], "title": "CP-Agent: Agentic Constraint Programming", "comment": null, "summary": "Translating natural language problem descriptions into formal constraint\nmodels remains a fundamental challenge in constraint programming, requiring\ndeep expertise in both the problem domain and modeling frameworks. Previous\napproaches to automating this translation have employed fixed workflows with\npredetermined modeling steps, failing on a significant number of benchmark\nproblems. We present a new approach using a pure agentic strategy without any\nfixed pipeline. We developed a general-purpose Python coding agent based on the\nReAct (Reason and Act) principle, utilizing a persistent IPython kernel for\nstateful code execution and iterative development. Rather than embedding\nconstraint programming logic into the agent architecture, domain-specific\nexpertise is injected solely through a carefully crafted project prompt. The\nagent combines this prompt-encoded knowledge with access to file operations and\ncode execution tools, enabling it to test hypotheses, debug failures, and\nverify solutions dynamically. Implemented in just a few hundred lines of code,\nthis architecture successfully solves all 101 problems of the CP-Bench\nconstraint programming benchmark set. The results suggest that constraint\nmodeling tasks require the combination of general coding tools and domain\nexpertise encoded in prompts, rather than specialized agent architectures or\npredefined workflows.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7eaf\u4ee3\u7406\u7b56\u7565\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7ReAct\u539f\u5219\u548cIPython\u5185\u6838\u5b9e\u73b0\u52a8\u6001\u4ee3\u7801\u6267\u884c\uff0c\u6210\u529f\u89e3\u51b3\u4e86CP-Bench\u57fa\u51c6\u96c6\u4e2d\u7684\u6240\u6709101\u4e2a\u95ee\u9898\u3002", "motivation": "\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u63cf\u8ff0\u5230\u5f62\u5f0f\u7ea6\u675f\u6a21\u578b\u7684\u7ffb\u8bd1\u9700\u8981\u6df1\u539a\u7684\u4e13\u4e1a\u77e5\u8bc6\u548c\u5efa\u6a21\u6846\u67b6\uff0c\u73b0\u6709\u56fa\u5b9a\u6d41\u7a0b\u65b9\u6cd5\u5728\u57fa\u51c6\u95ee\u9898\u4e0a\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8eReAct\u539f\u5219\u7684\u901a\u7528Python\u7f16\u7801\u4ee3\u7406\uff0c\u5229\u7528IPython\u5185\u6838\u8fdb\u884c\u72b6\u6001\u5316\u4ee3\u7801\u6267\u884c\u548c\u8fed\u4ee3\u5f00\u53d1\uff0c\u901a\u8fc7\u9879\u76ee\u63d0\u793a\u6ce8\u5165\u9886\u57df\u77e5\u8bc6\u3002", "result": "\u8be5\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86CP-Bench\u57fa\u51c6\u96c6\u4e2d\u7684\u6240\u6709101\u4e2a\u95ee\u9898\u3002", "conclusion": "\u7ea6\u675f\u5efa\u6a21\u4efb\u52a1\u9700\u8981\u901a\u7528\u7f16\u7801\u5de5\u5177\u548c\u63d0\u793a\u7f16\u7801\u7684\u9886\u57df\u77e5\u8bc6\uff0c\u800c\u975e\u4e13\u7528\u4ee3\u7406\u67b6\u6784\u6216\u9884\u5b9a\u4e49\u6d41\u7a0b\u3002"}}
{"id": "2508.07606", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07606", "abs": "https://arxiv.org/abs/2508.07606", "authors": ["Hongtao Li", "Ziyuan Jiao", "Xiaofeng Liu", "Hangxin Liu", "Zilong Zheng"], "title": "In-situ Value-aligned Human-Robot Interactions with Physical Constraints", "comment": "8 pages, 7 figures", "summary": "Equipped with Large Language Models (LLMs), human-centered robots are now\ncapable of performing a wide range of tasks that were previously deemed\nchallenging or unattainable. However, merely completing tasks is insufficient\nfor cognitive robots, who should learn and apply human preferences to future\nscenarios. In this work, we propose a framework that combines human preferences\nwith physical constraints, requiring robots to complete tasks while considering\nboth. Firstly, we developed a benchmark of everyday household activities, which\nare often evaluated based on specific preferences. We then introduced\nIn-Context Learning from Human Feedback (ICLHF), where human feedback comes\nfrom direct instructions and adjustments made intentionally or unintentionally\nin daily life. Extensive sets of experiments, testing the ICLHF to generate\ntask plans and balance physical constraints with preferences, have demonstrated\nthe efficiency of our approach.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4eba\u7c7b\u504f\u597d\u4e0e\u7269\u7406\u7ea6\u675f\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\u4eba\u7c7b\u53cd\u9988\uff08ICLHF\uff09\u6765\u5e2e\u52a9\u673a\u5668\u4eba\u5b8c\u6210\u4efb\u52a1\u5e76\u9002\u5e94\u672a\u6765\u573a\u666f\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4f7f\u673a\u5668\u4eba\u80fd\u5b8c\u6210\u590d\u6742\u4efb\u52a1\uff0c\u4f46\u4ec5\u5b8c\u6210\u4efb\u52a1\u4e0d\u8db3\u4ee5\u6ee1\u8db3\u8ba4\u77e5\u673a\u5668\u4eba\u7684\u9700\u6c42\uff0c\u673a\u5668\u4eba\u8fd8\u9700\u5b66\u4e60\u5e76\u5e94\u7528\u4eba\u7c7b\u504f\u597d\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u65e5\u5e38\u5bb6\u5ead\u6d3b\u52a8\u57fa\u51c6\uff0c\u5e76\u5f15\u5165ICLHF\u6846\u67b6\uff0c\u901a\u8fc7\u4eba\u7c7b\u76f4\u63a5\u6307\u4ee4\u548c\u65e5\u5e38\u8c03\u6574\u7684\u53cd\u9988\u6765\u5b66\u4e60\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cICLHF\u80fd\u9ad8\u6548\u751f\u6210\u4efb\u52a1\u8ba1\u5212\u5e76\u5e73\u8861\u7269\u7406\u7ea6\u675f\u4e0e\u4eba\u7c7b\u504f\u597d\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u673a\u5668\u4eba\u5b66\u4e60\u4eba\u7c7b\u504f\u597d\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u5176\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\u3002"}}
{"id": "2508.07485", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07485", "abs": "https://arxiv.org/abs/2508.07485", "authors": ["Alexander Duffy", "Samuel J Paech", "Ishana Shastri", "Elizabeth Karpinski", "Baptiste Alloui-Cros", "Tyler Marques", "Matthew Lyle Olson"], "title": "Democratizing Diplomacy: A Harness for Evaluating Any Large Language Model on Full-Press Diplomacy", "comment": null, "summary": "We present the first evaluation harness that enables any out-of-the-box,\nlocal, Large Language Models (LLMs) to play full-press Diplomacy without\nfine-tuning or specialized training. Previous work required frontier LLMs, or\nfine-tuning, due to the high complexity and information density of Diplomacy's\ngame state. Combined with the high variance of matches, these factors made\nDiplomacy prohibitive for study. In this work, we used data-driven iteration to\noptimize a textual game state representation such that a 24B model can reliably\ncomplete matches without any fine tuning. We develop tooling to facilitate\nhypothesis testing and statistical analysis, and we present case studies on\npersuasion, aggressive playstyles, and performance across a range of models. We\nconduct a variety of experiments across many popular LLMs, finding the larger\nmodels perform the best, but the smaller models still play adequately. We also\nintroduce Critical State Analysis: an experimental protocol for rapidly\niterating and analyzing key moments in a game at depth. Our harness\ndemocratizes the evaluation of strategic reasoning in LLMs by eliminating the\nneed for fine-tuning, and it provides insights into how these capabilities\nemerge naturally from widely used LLMs. Our code is available in the supplement\nand will be open sourced.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u65e0\u9700\u5fae\u8c03\u6216\u4e13\u95e8\u8bad\u7ec3\u7684\u8bc4\u4f30\u5de5\u5177\uff0c\u4f7f\u672c\u5730\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u80fd\u73a9\u5b8c\u6574\u7248\u300a\u5916\u4ea4\u300b\u6e38\u620f\u3002\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u7684\u8fed\u4ee3\u4f18\u5316\u6e38\u620f\u72b6\u6001\u8868\u793a\uff0c\u4f7f24B\u6a21\u578b\u80fd\u53ef\u9760\u5b8c\u6210\u6bd4\u8d5b\u3002", "motivation": "\u89e3\u51b3\u300a\u5916\u4ea4\u300b\u6e38\u620f\u7684\u9ad8\u590d\u6742\u6027\u548c\u4fe1\u606f\u5bc6\u5ea6\u5bfc\u81f4\u7684\u7814\u7a76\u56f0\u96be\uff0c\u6d88\u9664\u5bf9\u524d\u6cbfLLMs\u6216\u5fae\u8c03\u7684\u4f9d\u8d56\u3002", "method": "\u4f7f\u7528\u6570\u636e\u9a71\u52a8\u8fed\u4ee3\u4f18\u5316\u6e38\u620f\u72b6\u6001\u8868\u793a\uff0c\u5f00\u53d1\u5de5\u5177\u652f\u6301\u5047\u8bbe\u6d4b\u8bd5\u548c\u7edf\u8ba1\u5206\u6790\u3002", "result": "\u8f83\u5927\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u8f83\u5c0f\u6a21\u578b\u4e5f\u80fd\u80dc\u4efb\uff1b\u5f15\u5165\u5173\u952e\u72b6\u6001\u5206\u6790\u534f\u8bae\u6df1\u5165\u5206\u6790\u6e38\u620f\u5173\u952e\u70b9\u3002", "conclusion": "\u8be5\u5de5\u5177\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u8bc4\u4f30LLMs\u7684\u6218\u7565\u63a8\u7406\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u8fd9\u4e9b\u80fd\u529b\u5982\u4f55\u81ea\u7136\u5730\u4ece\u5e7f\u6cdb\u4f7f\u7528\u7684LLMs\u4e2d\u6d8c\u73b0\u3002"}}
{"id": "2508.07611", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07611", "abs": "https://arxiv.org/abs/2508.07611", "authors": ["Zifan Wang", "Xun Yang", "Jianzhuang Zhao", "Jiaming Zhou", "Teli Ma", "Ziyao Gao", "Arash Ajoudani", "Junwei Liang"], "title": "End-to-End Humanoid Robot Safe and Comfortable Locomotion Policy", "comment": null, "summary": "The deployment of humanoid robots in unstructured, human-centric environments\nrequires navigation capabilities that extend beyond simple locomotion to\ninclude robust perception, provable safety, and socially aware behavior.\nCurrent reinforcement learning approaches are often limited by blind\ncontrollers that lack environmental awareness or by vision-based systems that\nfail to perceive complex 3D obstacles. In this work, we present an end-to-end\nlocomotion policy that directly maps raw, spatio-temporal LiDAR point clouds to\nmotor commands, enabling robust navigation in cluttered dynamic scenes. We\nformulate the control problem as a Constrained Markov Decision Process (CMDP)\nto formally separate safety from task objectives. Our key contribution is a\nnovel methodology that translates the principles of Control Barrier Functions\n(CBFs) into costs within the CMDP, allowing a model-free Penalized Proximal\nPolicy Optimization (P3O) to enforce safety constraints during training.\nFurthermore, we introduce a set of comfort-oriented rewards, grounded in\nhuman-robot interaction research, to promote motions that are smooth,\npredictable, and less intrusive. We demonstrate the efficacy of our framework\nthrough a successful sim-to-real transfer to a physical humanoid robot, which\nexhibits agile and safe navigation around both static and dynamic 3D obstacles.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7aef\u5230\u7aef\u8fd0\u52a8\u7b56\u7565\uff0c\u5c06LiDAR\u70b9\u4e91\u76f4\u63a5\u6620\u5c04\u4e3a\u7535\u673a\u547d\u4ee4\uff0c\u7ed3\u5408CMDP\u548cCBFs\u786e\u4fdd\u5b89\u5168\uff0c\u5e76\u901a\u8fc7P3O\u8bad\u7ec3\u5b9e\u73b0\u5b89\u5168\u7ea6\u675f\u3002", "motivation": "\u4eba\u5f62\u673a\u5668\u4eba\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u9700\u8981\u66f4\u5f3a\u7684\u5bfc\u822a\u80fd\u529b\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u73af\u5883\u611f\u77e5\u548c\u5b89\u5168\u7ea6\u675f\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u4f7f\u7528CMDP\u5206\u79bb\u5b89\u5168\u4e0e\u4efb\u52a1\u76ee\u6807\uff0c\u5c06CBFs\u8f6c\u5316\u4e3a\u6210\u672c\u51fd\u6570\uff0c\u7ed3\u5408P3O\u8bad\u7ec3\uff0c\u5e76\u5f15\u5165\u8212\u9002\u6027\u5956\u52b1\u3002", "result": "\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\uff0c\u673a\u5668\u4eba\u80fd\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u5b89\u5168\u5bfc\u822a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u5b89\u5168\u7ea6\u675f\u548c\u8212\u9002\u6027\u5956\u52b1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u7684\u5bfc\u822a\u80fd\u529b\u3002"}}
{"id": "2508.07575", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07575", "abs": "https://arxiv.org/abs/2508.07575", "authors": ["Shiqing Fan", "Xichen Ding", "Liang Zhang", "Linjian Mo"], "title": "MCPToolBench++: A Large Scale AI Agent Model Context Protocol MCP Tool Use Benchmark", "comment": "Benchmarks and Source Code Released", "summary": "LLMs' capabilities are enhanced by using function calls to integrate various\ndata sources or API results into the context window. Typical tools include\nsearch, web crawlers, maps, financial data, file systems, and browser usage,\netc. Integrating these data sources or functions requires a standardized\nmethod. The Model Context Protocol (MCP) provides a standardized way to supply\ncontext to LLMs. However, the evaluation of LLMs and AI Agents' MCP tool use\nabilities suffer from several issues. First, there's a lack of comprehensive\ndatasets or benchmarks to evaluate various MCP tools. Second, the diverse\nformats of response from MCP tool call execution further increase the\ndifficulty of evaluation. Additionally, unlike existing tool-use benchmarks\nwith high success rates in functions like programming and math functions, the\nsuccess rate of real-world MCP tool is not guaranteed and varies across\ndifferent MCP servers. Furthermore, the LLMs' context window also limits the\nnumber of available tools that can be called in a single run, because the\ntextual descriptions of tool and the parameters have long token length for an\nLLM to process all at once. To help address the challenges of evaluating LLMs'\nperformance on calling MCP tools, we propose MCPToolBench++, a large-scale,\nmulti-domain AI Agent tool use benchmark. As of July 2025, this benchmark is\nbuild upon marketplace of over 4k MCP servers from more than 40 categories,\ncollected from the MCP marketplaces and GitHub communities. The datasets\nconsist of both single-step and multi-step tool calls across different\ncategories. We evaluated SOTA LLMs with agentic abilities on this benchmark and\nreported the results.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faMCPToolBench++\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30LLMs\u8c03\u7528MCP\u5de5\u5177\u80fd\u529b\u7684\u5927\u89c4\u6a21\u591a\u9886\u57df\u57fa\u51c6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u7f3a\u4e4f\u5168\u9762\u7684\u6570\u636e\u96c6\u548c\u57fa\u51c6\uff0c\u4e14MCP\u5de5\u5177\u54cd\u5e94\u683c\u5f0f\u591a\u6837\uff0c\u6210\u529f\u7387\u548c\u53ef\u7528\u5de5\u5177\u6570\u91cf\u53d7\u9650\u3002", "method": "\u6784\u5efaMCPToolBench++\u57fa\u51c6\uff0c\u5305\u542b\u6765\u81ea40\u591a\u4e2a\u7c7b\u522b\u76844k\u591a\u4e2aMCP\u670d\u52a1\u5668\u6570\u636e\uff0c\u652f\u6301\u5355\u6b65\u548c\u591a\u6b65\u5de5\u5177\u8c03\u7528\u8bc4\u4f30\u3002", "result": "\u8bc4\u4f30\u4e86\u5177\u6709\u4ee3\u7406\u80fd\u529b\u7684SOTA LLMs\uff0c\u5e76\u62a5\u544a\u4e86\u7ed3\u679c\u3002", "conclusion": "MCPToolBench++\u4e3a\u8bc4\u4f30LLMs\u5728MCP\u5de5\u5177\u8c03\u7528\u4e0a\u7684\u6027\u80fd\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.07648", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07648", "abs": "https://arxiv.org/abs/2508.07648", "authors": ["Mehrshad Zandigohar", "Mallesham Dasari", "Gunar Schirner"], "title": "Grasp-HGN: Grasping the Unexpected", "comment": "Paper accepted at ACM Transactions on Embedded Computing Systems", "summary": "For transradial amputees, robotic prosthetic hands promise to regain the\ncapability to perform daily living activities. To advance next-generation\nprosthetic hand control design, it is crucial to address current shortcomings\nin robustness to out of lab artifacts, and generalizability to new\nenvironments. Due to the fixed number of object to interact with in existing\ndatasets, contrasted with the virtually infinite variety of objects encountered\nin the real world, current grasp models perform poorly on unseen objects,\nnegatively affecting users' independence and quality of life.\n  To address this: (i) we define semantic projection, the ability of a model to\ngeneralize to unseen object types and show that conventional models like YOLO,\ndespite 80% training accuracy, drop to 15% on unseen objects. (ii) we propose\nGrasp-LLaVA, a Grasp Vision Language Model enabling human-like reasoning to\ninfer the suitable grasp type estimate based on the object's physical\ncharacteristics resulting in a significant 50.2% accuracy over unseen object\ntypes compared to 36.7% accuracy of an SOTA grasp estimation model.\n  Lastly, to bridge the performance-latency gap, we propose Hybrid Grasp\nNetwork (HGN), an edge-cloud deployment infrastructure enabling fast grasp\nestimation on edge and accurate cloud inference as a fail-safe, effectively\nexpanding the latency vs. accuracy Pareto. HGN with confidence calibration (DC)\nenables dynamic switching between edge and cloud models, improving semantic\nprojection accuracy by 5.6% (to 42.3%) with 3.5x speedup over the unseen object\ntypes. Over a real-world sample mix, it reaches 86% average accuracy (12.2%\ngain over edge-only), and 2.2x faster inference than Grasp-LLaVA alone.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faGrasp-LLaVA\u548cHGN\u6a21\u578b\uff0c\u89e3\u51b3\u5047\u80a2\u624b\u63a7\u5236\u4e2d\u672a\u89c1\u7269\u4f53\u7684\u6293\u53d6\u6cdb\u5316\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u51c6\u786e\u6027\u548c\u5ef6\u8fdf\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5047\u80a2\u624b\u6293\u53d6\u6a21\u578b\u5bf9\u672a\u89c1\u7269\u4f53\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u5f71\u54cd\u7528\u6237\u72ec\u7acb\u6027\u548c\u751f\u6d3b\u8d28\u91cf\u3002", "method": "\u63d0\u51faGrasp-LLaVA\uff08\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff09\u548cHGN\uff08\u8fb9\u7f18-\u4e91\u6df7\u5408\u90e8\u7f72\u67b6\u6784\uff09\uff0c\u7ed3\u5408\u8bed\u4e49\u63a8\u7406\u548c\u52a8\u6001\u5207\u6362\u7b56\u7565\u3002", "result": "Grasp-LLaVA\u5728\u672a\u89c1\u7269\u4f53\u4e0a\u51c6\u786e\u7387\u8fbe50.2%\uff0cHGN\u8fdb\u4e00\u6b65\u5c06\u51c6\u786e\u7387\u63d0\u5347\u81f342.3%\uff0c\u5ef6\u8fdf\u964d\u4f4e3.5\u500d\u3002", "conclusion": "Grasp-LLaVA\u548cHGN\u663e\u8457\u63d0\u5347\u5047\u80a2\u624b\u5bf9\u672a\u89c1\u7269\u4f53\u7684\u6293\u53d6\u80fd\u529b\uff0c\u5e73\u8861\u4e86\u51c6\u786e\u6027\u4e0e\u5ef6\u8fdf\u3002"}}
{"id": "2508.07586", "categories": ["cs.AI", "cs.NI"], "pdf": "https://arxiv.org/pdf/2508.07586", "abs": "https://arxiv.org/abs/2508.07586", "authors": ["Wenjing Zhang", "Ye Hu", "Tao Luo", "Zhilong Zhang", "Mingzhe Chen"], "title": "Optimization of Private Semantic Communication Performance: An Uncooperative Covert Communication Method", "comment": null, "summary": "In this paper, a novel covert semantic communication framework is\ninvestigated. Within this framework, a server extracts and transmits the\nsemantic information, i.e., the meaning of image data, to a user over several\ntime slots. An attacker seeks to detect and eavesdrop the semantic transmission\nto acquire details of the original image. To avoid data meaning being\neavesdropped by an attacker, a friendly jammer is deployed to transmit jamming\nsignals to interfere the attacker so as to hide the transmitted semantic\ninformation. Meanwhile, the server will strategically select time slots for\nsemantic information transmission. Due to limited energy, the jammer will not\ncommunicate with the server and hence the server does not know the transmit\npower of the jammer. Therefore, the server must jointly optimize the semantic\ninformation transmitted at each time slot and the corresponding transmit power\nto maximize the privacy and the semantic information transmission quality of\nthe user. To solve this problem, we propose a prioritised sampling assisted\ntwin delayed deep deterministic policy gradient algorithm to jointly determine\nthe transmitted semantic information and the transmit power per time slot\nwithout the communications between the server and the jammer. Compared to\nstandard reinforcement learning methods, the propose method uses an additional\nQ network to estimate Q values such that the agent can select the action with a\nlower Q value from the two Q networks thus avoiding local optimal action\nselection and estimation bias of Q values. Simulation results show that the\nproposed algorithm can improve the privacy and the semantic information\ntransmission quality by up to 77.8% and 14.3% compared to the traditional\nreinforcement learning methods.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u9690\u853d\u8bed\u4e49\u901a\u4fe1\u6846\u67b6\uff0c\u901a\u8fc7\u53cb\u597d\u5e72\u6270\u5668\u5e72\u6270\u653b\u51fb\u8005\uff0c\u4f18\u5316\u8bed\u4e49\u4fe1\u606f\u548c\u4f20\u8f93\u529f\u7387\uff0c\u63d0\u5347\u9690\u79c1\u548c\u4f20\u8f93\u8d28\u91cf\u3002", "motivation": "\u7814\u7a76\u5982\u4f55\u5728\u653b\u51fb\u8005\u8bd5\u56fe\u7a83\u53d6\u8bed\u4e49\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\uff0c\u4fdd\u62a4\u56fe\u50cf\u6570\u636e\u7684\u8bed\u4e49\u4f20\u8f93\u9690\u79c1\u3002", "method": "\u91c7\u7528\u4f18\u5148\u91c7\u6837\u8f85\u52a9\u7684\u53cc\u5ef6\u8fdf\u6df1\u5ea6\u786e\u5b9a\u6027\u7b56\u7565\u68af\u5ea6\u7b97\u6cd5\uff0c\u8054\u5408\u4f18\u5316\u8bed\u4e49\u4fe1\u606f\u548c\u4f20\u8f93\u529f\u7387\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u76f8\u6bd4\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u9690\u79c1\u548c\u4f20\u8f93\u8d28\u91cf\u5206\u522b\u63d0\u534777.8%\u548c14.3%\u3002", "conclusion": "\u63d0\u51fa\u7684\u7b97\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u8bed\u4e49\u901a\u4fe1\u7684\u9690\u79c1\u6027\u548c\u4f20\u8f93\u8d28\u91cf\uff0c\u907f\u514d\u4e86\u5c40\u90e8\u6700\u4f18\u548cQ\u503c\u4f30\u8ba1\u504f\u5dee\u3002"}}
{"id": "2508.07650", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07650", "abs": "https://arxiv.org/abs/2508.07650", "authors": ["Helong Huang", "Min Cen", "Kai Tan", "Xingyue Quan", "Guowei Huang", "Hong Zhang"], "title": "GraphCoT-VLA: A 3D Spatial-Aware Reasoning Vision-Language-Action Model for Robotic Manipulation with Ambiguous Instructions", "comment": "10 pages, 6 figures", "summary": "Vision-language-action models have emerged as a crucial paradigm in robotic\nmanipulation. However, existing VLA models exhibit notable limitations in\nhandling ambiguous language instructions and unknown environmental states.\nFurthermore, their perception is largely constrained to static two-dimensional\nobservations, lacking the capability to model three-dimensional interactions\nbetween the robot and its environment. To address these challenges, this paper\nproposes GraphCoT-VLA, an efficient end-to-end model. To enhance the model's\nability to interpret ambiguous instructions and improve task planning, we\ndesign a structured Chain-of-Thought reasoning module that integrates\nhigh-level task understanding and planning, failed task feedback, and low-level\nimaginative reasoning about future object positions and robot actions.\nAdditionally, we construct a real-time updatable 3D Pose-Object graph, which\ncaptures the spatial configuration of robot joints and the topological\nrelationships between objects in 3D space, enabling the model to better\nunderstand and manipulate their interactions. We further integrates a dropout\nhybrid reasoning strategy to achieve efficient control outputs. Experimental\nresults across multiple real-world robotic tasks demonstrate that GraphCoT-VLA\nsignificantly outperforms existing methods in terms of task success rate and\nresponse speed, exhibiting strong generalization and robustness in open\nenvironments and under uncertain instructions.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faGraphCoT-VLA\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u601d\u7ef4\u94fe\u548c3D\u59ff\u6001-\u7269\u4f53\u56fe\u89e3\u51b3\u73b0\u6709VLA\u6a21\u578b\u5728\u6a21\u7cca\u6307\u4ee4\u548c\u672a\u77e5\u73af\u5883\u72b6\u6001\u4e0b\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4efb\u52a1\u6210\u529f\u7387\u548c\u54cd\u5e94\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709VLA\u6a21\u578b\u5728\u6a21\u7cca\u8bed\u8a00\u6307\u4ee4\u548c\u672a\u77e5\u73af\u5883\u72b6\u6001\u5904\u7406\u4e0a\u8868\u73b0\u4e0d\u8db3\uff0c\u4e14\u611f\u77e5\u5c40\u9650\u4e8e\u9759\u6001\u4e8c\u7ef4\u89c2\u5bdf\uff0c\u7f3a\u4e4f\u4e09\u7ef4\u4ea4\u4e92\u5efa\u6a21\u80fd\u529b\u3002", "method": "\u8bbe\u8ba1\u7ed3\u6784\u5316\u601d\u7ef4\u94fe\u63a8\u7406\u6a21\u5757\u548c\u5b9e\u65f6\u66f4\u65b0\u76843D\u59ff\u6001-\u7269\u4f53\u56fe\uff0c\u7ed3\u5408\u6df7\u5408\u63a8\u7406\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cGraphCoT-VLA\u5728\u4efb\u52a1\u6210\u529f\u7387\u548c\u54cd\u5e94\u901f\u5ea6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5177\u6709\u5f3a\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "GraphCoT-VLA\u901a\u8fc7\u6539\u8fdb\u63a8\u7406\u548c\u4e09\u7ef4\u5efa\u6a21\u80fd\u529b\uff0c\u6709\u6548\u63d0\u5347\u4e86\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6027\u80fd\u3002"}}
{"id": "2508.07602", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07602", "abs": "https://arxiv.org/abs/2508.07602", "authors": ["Wenpeng Xing", "Zhipeng Chen", "Changting Lin", "Meng Han"], "title": "HGMF: A Hierarchical Gaussian Mixture Framework for Scalable Tool Invocation within the Model Context Protocol", "comment": null, "summary": "Invoking external tools enables Large Language Models (LLMs) to perform\ncomplex, real-world tasks, yet selecting the correct tool from large,\nhierarchically-structured libraries remains a significant challenge. The\nlimited context windows of LLMs and noise from irrelevant options often lead to\nlow selection accuracy and high computational costs. To address this, we\npropose the Hierarchical Gaussian Mixture Framework (HGMF), a probabilistic\npruning method for scalable tool invocation. HGMF first maps the user query and\nall tool descriptions into a unified semantic space. The framework then\noperates in two stages: it clusters servers using a Gaussian Mixture Model\n(GMM) and filters them based on the query's likelihood. Subsequently, it\napplies the same GMM-based clustering and filtering to the tools associated\nwith the selected servers. This hierarchical process produces a compact,\nhigh-relevance candidate set, simplifying the final selection task for the LLM.\nExperiments on a public dataset show that HGMF significantly improves tool\nselection accuracy while reducing inference latency, confirming the framework's\nscalability and effectiveness for large-scale tool libraries.", "AI": {"tldr": "HGMF\u901a\u8fc7\u5206\u5c42\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u63d0\u9ad8LLM\u5de5\u5177\u9009\u62e9\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u89e3\u51b3LLM\u5728\u5927\u89c4\u6a21\u3001\u5c42\u6b21\u5316\u5de5\u5177\u5e93\u4e2d\u9009\u62e9\u5de5\u5177\u65f6\u56e0\u4e0a\u4e0b\u6587\u9650\u5236\u548c\u566a\u58f0\u5bfc\u81f4\u7684\u4f4e\u51c6\u786e\u6027\u548c\u9ad8\u8ba1\u7b97\u6210\u672c\u95ee\u9898\u3002", "method": "HGMF\u5c06\u67e5\u8be2\u548c\u5de5\u5177\u63cf\u8ff0\u6620\u5c04\u5230\u7edf\u4e00\u8bed\u4e49\u7a7a\u95f4\uff0c\u5206\u4e24\u9636\u6bb5\u4f7f\u7528\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u805a\u7c7b\u548c\u8fc7\u6ee4\uff0c\u751f\u6210\u9ad8\u76f8\u5173\u6027\u5019\u9009\u96c6\u3002", "result": "\u5b9e\u9a8c\u8868\u660eHGMF\u663e\u8457\u63d0\u9ad8\u5de5\u5177\u9009\u62e9\u51c6\u786e\u6027\u5e76\u964d\u4f4e\u63a8\u7406\u5ef6\u8fdf\u3002", "conclusion": "HGMF\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u6709\u6548\u7684\u5927\u89c4\u6a21\u5de5\u5177\u5e93\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.07657", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07657", "abs": "https://arxiv.org/abs/2508.07657", "authors": ["Zhuoli Tian", "Yuyang Zhang", "Jinsheng Wei", "Meng Guo"], "title": "MoRoCo: Multi-operator-robot Coordination, Interaction and Exploration under Restricted Communication", "comment": "38 pages, 28 figures, Submitted to the International Journal of\n  Robotics Research (IJRR). Project website: https://zl-tian.github.io/MoRoCo/", "summary": "Fleets of autonomous robots are increasingly deployed alongside multiple\nhuman operators to explore unknown environments, identify salient features, and\nperform complex tasks in scenarios such as subterranean exploration,\nreconnaissance, and search-and-rescue missions. In these contexts,\ncommunication is often severely limited to short-range exchanges via ad-hoc\nnetworks, posing challenges to coordination. While recent studies have\naddressed multi-robot exploration under communication constraints, they largely\noverlook the essential role of human operators and their real-time interaction\nwith robotic teams. Operators may demand timely updates on the exploration\nprogress and robot status, reprioritize or cancel tasks dynamically, or request\nlive video feeds and control access. Conversely, robots may seek human\nconfirmation for anomalous events or require help recovering from motion or\nplanning failures. To enable such bilateral, context-aware interactions under\nrestricted communication, this work proposes MoRoCo, a unified framework for\nonline coordination and exploration in multi-operator, multi-robot systems.\nMoRoCo enables the team to adaptively switch among three coordination modes:\nspread mode for parallelized exploration with intermittent data sharing,\nmigrate mode for coordinated relocation, and chain mode for maintaining\nhigh-bandwidth connectivity through multi-hop links. These transitions are\nmanaged through distributed algorithms via only local communication. Extensive\nlarge-scale human-in-the-loop simulations and hardware experiments validate the\nnecessity of incorporating human robot interactions and demonstrate that MoRoCo\nenables efficient, reliable coordination under limited communication, marking a\nsignificant step toward robust human-in-the-loop multi-robot autonomy in\nchallenging environments.", "AI": {"tldr": "MoRoCo\u6846\u67b6\u652f\u6301\u591a\u64cd\u4f5c\u5458\u4e0e\u591a\u673a\u5668\u4eba\u5728\u6709\u9650\u901a\u4fe1\u4e0b\u7684\u5b9e\u65f6\u4ea4\u4e92\u4e0e\u534f\u8c03\uff0c\u901a\u8fc7\u4e09\u79cd\u6a21\u5f0f\uff08spread\u3001migrate\u3001chain\uff09\u5b9e\u73b0\u9ad8\u6548\u63a2\u7d22\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u591a\u5ffd\u89c6\u4eba\u7c7b\u64cd\u4f5c\u5458\u4e0e\u673a\u5668\u4eba\u56e2\u961f\u7684\u5b9e\u65f6\u4ea4\u4e92\u9700\u6c42\uff0c\u800c\u5b9e\u9645\u4efb\u52a1\u4e2d\u9700\u52a8\u6001\u8c03\u6574\u4efb\u52a1\u3001\u83b7\u53d6\u72b6\u6001\u66f4\u65b0\u7b49\u3002", "method": "\u63d0\u51faMoRoCo\u6846\u67b6\uff0c\u652f\u6301\u4e09\u79cd\u534f\u8c03\u6a21\u5f0f\u5207\u6362\uff08spread\u3001migrate\u3001chain\uff09\uff0c\u901a\u8fc7\u5206\u5e03\u5f0f\u7b97\u6cd5\u7ba1\u7406\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1MoRoCo\u5728\u6709\u9650\u901a\u4fe1\u4e0b\u5b9e\u73b0\u9ad8\u6548\u534f\u8c03\uff0c\u63d0\u5347\u4eba\u673a\u4ea4\u4e92\u53ef\u9760\u6027\u3002", "conclusion": "MoRoCo\u4e3a\u590d\u6742\u73af\u5883\u4e2d\u4eba\u673a\u534f\u540c\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u63d0\u4f9b\u4e86\u9c81\u68d2\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.07616", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07616", "abs": "https://arxiv.org/abs/2508.07616", "authors": ["Aswin RRV", "Jacob Dineen", "Divij Handa", "Md Nayem Uddin", "Mihir Parmar", "Chitta Baral", "Ben Zhou"], "title": "ThinkTuning: Instilling Cognitive Reflections without Distillation", "comment": "15 pages", "summary": "Recent advances in test-time scaling have led to the emergence of thinking\nLLMs that exhibit self-reflective behaviors and multi-step reasoning. While RL\ndrives this self-improvement paradigm, a recent study (Gandhi et al., 2025)\nshows that RL alone does not truly instill these new reasoning abilities - it\nmerely draws out behaviors already present in the base models. This raises a\nquestion: How can we train the models that don't exhibit such thinking behavior\nto develop it in the first place? To this end, we propose ThinkTuning, a\nGRPO-based interactive training approach where we augment the rollouts of a\nstudent model with the guidance from a teacher model. A simple idea from\nclassroom practice inspires our method: a teacher poses a problem, lets the\nstudent try an answer, then gives corrective feedback -- enough to point the\nmind in the right direction and then show the solution. Each piece of feedback\nreshapes the student's thoughts, leading them to arrive at the correct\nsolution. Similarly, we find that this type of implicit supervision through\nfeedback from a teacher model of the same size improves the reasoning\ncapabilities of the student model. In particular, on average, our method shows\na 3.85% improvement over zero-shot baselines across benchmarks, and on\nMATH-500, AIME and GPQA-Diamond it shows 2.08%, 2.23% and 3.99% improvements\nover the vanilla-GRPO baseline. Source code is available at\nhttps://github.com/3rdAT/ThinkTuning.", "AI": {"tldr": "ThinkTuning\u662f\u4e00\u79cd\u57fa\u4e8eGRPO\u7684\u4ea4\u4e92\u5f0f\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u6559\u5e08\u6a21\u578b\u7684\u53cd\u9988\u63d0\u5347\u5b66\u751f\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5b9e\u9a8c\u663e\u793a\u5728\u591a\u4efb\u52a1\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u8868\u660e\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4ec5\u80fd\u6fc0\u53d1\u57fa\u7840\u6a21\u578b\u5df2\u6709\u7684\u63a8\u7406\u884c\u4e3a\uff0c\u800c\u65e0\u6cd5\u771f\u6b63\u57f9\u517b\u65b0\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u8ba9\u4e0d\u5177\u5907\u6b64\u7c7b\u884c\u4e3a\u7684\u6a21\u578b\u4e5f\u80fd\u53d1\u5c55\u51fa\u63a8\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51faThinkTuning\u65b9\u6cd5\uff0c\u901a\u8fc7\u6559\u5e08\u6a21\u578b\u5bf9\u5b66\u751f\u6a21\u578b\u7684\u5c1d\u8bd5\u7b54\u6848\u63d0\u4f9b\u53cd\u9988\uff0c\u9010\u6b65\u5f15\u5bfc\u5b66\u751f\u6a21\u578b\u6539\u8fdb\u63a8\u7406\u80fd\u529b\u3002", "result": "\u5728MATH-500\u3001AIME\u548cGPQA-Diamond\u7b49\u4efb\u52a1\u4e0a\uff0cThinkTuning\u5206\u522b\u6bd4\u57fa\u7ebf\u63d0\u53472.08%\u30012.23%\u548c3.99%\uff0c\u5e73\u5747\u63d0\u53473.85%\u3002", "conclusion": "ThinkTuning\u901a\u8fc7\u6559\u5e08\u6a21\u578b\u7684\u9690\u5f0f\u76d1\u7763\u6709\u6548\u63d0\u5347\u4e86\u5b66\u751f\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4e3a\u57f9\u517b\u6a21\u578b\u7684\u81ea\u4e3b\u601d\u8003\u80fd\u529b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2508.07686", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07686", "abs": "https://arxiv.org/abs/2508.07686", "authors": ["Mingyue Lei", "Zewei Zhou", "Hongchen Li", "Jiaqi Ma", "Jia Hu"], "title": "Risk Map As Middleware: Towards Interpretable Cooperative End-to-end Autonomous Driving for Risk-Aware Planning", "comment": null, "summary": "End-to-end paradigm has emerged as a promising approach to autonomous\ndriving. However, existing single-agent end-to-end pipelines are often\nconstrained by occlusion and limited perception range, resulting in hazardous\ndriving. Furthermore, their black-box nature prevents the interpretability of\nthe driving behavior, leading to an untrustworthiness system. To address these\nlimitations, we introduce Risk Map as Middleware (RiskMM) and propose an\ninterpretable cooperative end-to-end driving framework. The risk map learns\ndirectly from the driving data and provides an interpretable spatiotemporal\nrepresentation of the scenario from the upstream perception and the\ninteractions between the ego vehicle and the surrounding environment for\ndownstream planning. RiskMM first constructs a multi-agent spatiotemporal\nrepresentation with unified Transformer-based architecture, then derives\nrisk-aware representations by modeling interactions among surrounding\nenvironments with attention. These representations are subsequently fed into a\nlearning-based Model Predictive Control (MPC) module. The MPC planner\ninherently accommodates physical constraints and different vehicle types and\ncan provide interpretation by aligning learned parameters with explicit MPC\nelements. Evaluations conducted on the real-world V2XPnP-Seq dataset confirm\nthat RiskMM achieves superior and robust performance in risk-aware trajectory\nplanning, significantly enhancing the interpretability of the cooperative\nend-to-end driving framework. The codebase will be released to facilitate\nfuture research in this field.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u98ce\u9669\u5730\u56fe\u7684\u4e2d\u95f4\u4ef6\uff08RiskMM\uff09\u548c\u53ef\u89e3\u91ca\u7684\u534f\u4f5c\u7aef\u5230\u7aef\u9a7e\u9a76\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u5355\u667a\u80fd\u4f53\u7aef\u5230\u7aef\u9a7e\u9a76\u4e2d\u7684\u906e\u6321\u3001\u611f\u77e5\u8303\u56f4\u9650\u5236\u548c\u9ed1\u76d2\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u5355\u667a\u80fd\u4f53\u7aef\u5230\u7aef\u9a7e\u9a76\u7cfb\u7edf\u56e0\u906e\u6321\u548c\u6709\u9650\u611f\u77e5\u8303\u56f4\u5bfc\u81f4\u5371\u9669\u9a7e\u9a76\uff0c\u4e14\u9ed1\u76d2\u7279\u6027\u4f7f\u5176\u884c\u4e3a\u96be\u4ee5\u89e3\u91ca\uff0c\u7f3a\u4e4f\u53ef\u4fe1\u5ea6\u3002", "method": "\u901a\u8fc7\u98ce\u9669\u5730\u56fe\u5b66\u4e60\u9a7e\u9a76\u6570\u636e\uff0c\u6784\u5efa\u591a\u667a\u80fd\u4f53\u65f6\u7a7a\u8868\u793a\uff0c\u5229\u7528\u6ce8\u610f\u529b\u5efa\u6a21\u73af\u5883\u4ea4\u4e92\uff0c\u5e76\u7ed3\u5408\u57fa\u4e8e\u5b66\u4e60\u7684\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u6a21\u5757\u8fdb\u884c\u89c4\u5212\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6V2XPnP-Seq\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0cRiskMM\u5728\u98ce\u9669\u611f\u77e5\u8f68\u8ff9\u89c4\u5212\u4e2d\u8868\u73b0\u4f18\u8d8a\u4e14\u7a33\u5065\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6846\u67b6\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "RiskMM\u901a\u8fc7\u98ce\u9669\u5730\u56fe\u548cMPC\u6a21\u5757\u7684\u7ed3\u5408\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7aef\u5230\u7aef\u9a7e\u9a76\u7684\u5c40\u9650\u6027\u548c\u53ef\u89e3\u91ca\u6027\u95ee\u9898\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.07628", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07628", "abs": "https://arxiv.org/abs/2508.07628", "authors": ["Daniel Essien", "Suresh Neethirajan"], "title": "Multimodal AI Systems for Enhanced Laying Hen Welfare Assessment and Productivity Optimization", "comment": "66 pages, 7 figures, 11 tables", "summary": "The future of poultry production depends on a paradigm shift replacing\nsubjective, labor-intensive welfare checks with data-driven, intelligent\nmonitoring ecosystems. Traditional welfare assessments-limited by human\nobservation and single-sensor data-cannot fully capture the complex,\nmultidimensional nature of laying hen welfare in modern farms. Multimodal\nArtificial Intelligence (AI) offers a breakthrough, integrating visual,\nacoustic, environmental, and physiological data streams to reveal deeper\ninsights into avian welfare dynamics. This investigation highlights multimodal\nAs transformative potential, showing that intermediate (feature-level) fusion\nstrategies achieve the best balance between robustness and performance under\nreal-world poultry conditions, and offer greater scalability than early or late\nfusion approaches. Key adoption barriers include sensor fragility in harsh farm\nenvironments, high deployment costs, inconsistent behavioral definitions, and\nlimited cross-farm generalizability. To address these, we introduce two novel\nevaluation tools - the Domain Transfer Score (DTS) to measure model\nadaptability across diverse farm settings, and the Data Reliability Index (DRI)\nto assess sensor data quality under operational constraints. We also propose a\nmodular, context-aware deployment framework designed for laying hen\nenvironments, enabling scalable and practical integration of multimodal\nsensing. This work lays the foundation for a transition from reactive, unimodal\nmonitoring to proactive, precision-driven welfare systems that unite\nproductivity with ethical, science based animal care.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u5229\u7528\u591a\u6a21\u6001AI\u6280\u672f\u53d6\u4ee3\u4f20\u7edf\u4e3b\u89c2\u3001\u52b3\u52a8\u5bc6\u96c6\u578b\u7684\u5bb6\u79bd\u798f\u5229\u76d1\u6d4b\uff0c\u901a\u8fc7\u6574\u5408\u89c6\u89c9\u3001\u58f0\u97f3\u3001\u73af\u5883\u548c\u751f\u7406\u6570\u636e\u63d0\u5347\u86cb\u9e21\u798f\u5229\u8bc4\u4f30\u7684\u51c6\u786e\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u4f20\u7edf\u5bb6\u79bd\u798f\u5229\u76d1\u6d4b\u4f9d\u8d56\u4eba\u5de5\u89c2\u5bdf\u548c\u5355\u4e00\u4f20\u611f\u5668\u6570\u636e\uff0c\u65e0\u6cd5\u5168\u9762\u53cd\u6620\u73b0\u4ee3\u519c\u573a\u4e2d\u86cb\u9e21\u798f\u5229\u7684\u591a\u7ef4\u590d\u6742\u6027\uff0c\u4e9f\u9700\u6570\u636e\u9a71\u52a8\u7684\u667a\u80fd\u76d1\u6d4b\u7cfb\u7edf\u3002", "method": "\u91c7\u7528\u591a\u6a21\u6001AI\u6280\u672f\uff0c\u7ed3\u5408\u7279\u5f81\u7ea7\u878d\u5408\u7b56\u7565\uff0c\u5e76\u5f15\u5165\u9886\u57df\u8f6c\u79fb\u8bc4\u5206\uff08DTS\uff09\u548c\u6570\u636e\u53ef\u9760\u6027\u6307\u6570\uff08DRI\uff09\u5de5\u5177\uff0c\u63d0\u51fa\u6a21\u5757\u5316\u3001\u60c5\u5883\u611f\u77e5\u7684\u90e8\u7f72\u6846\u67b6\u3002", "result": "\u7279\u5f81\u7ea7\u878d\u5408\u7b56\u7565\u5728\u771f\u5b9e\u519c\u573a\u6761\u4ef6\u4e0b\u8868\u73b0\u6700\u4f73\uff0c\u5e73\u8861\u4e86\u9c81\u68d2\u6027\u548c\u6027\u80fd\uff0c\u540c\u65f6\u63d0\u51fa\u7684DTS\u548cDRI\u5de5\u5177\u80fd\u6709\u6548\u8bc4\u4f30\u6a21\u578b\u9002\u5e94\u6027\u548c\u6570\u636e\u8d28\u91cf\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u4ece\u88ab\u52a8\u3001\u5355\u6a21\u6001\u76d1\u6d4b\u8f6c\u5411\u4e3b\u52a8\u3001\u7cbe\u51c6\u9a71\u52a8\u7684\u798f\u5229\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u7ed3\u5408\u751f\u4ea7\u6548\u7387\u548c\u79d1\u5b66\u4f26\u7406\u7684\u52a8\u7269\u5173\u6000\u3002"}}
{"id": "2508.07689", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07689", "abs": "https://arxiv.org/abs/2508.07689", "authors": ["Christian Eichmann", "Sabine Bellmann", "Nicolas H\u00fcgel", "Louis-Elias Enslin", "Carsten Plasberg", "Georg Heppner", "Arne Roennau", "Ruediger Dillmann"], "title": "LAURON VI: A Six-Legged Robot for Dynamic Walking", "comment": null, "summary": "Legged locomotion enables robotic systems to traverse extremely challenging\nterrains. In many real-world scenarios, the terrain is not that difficult and\nthese mixed terrain types introduce the need for flexible use of different\nwalking strategies to achieve mission goals in a fast, reliable, and\nenergy-efficient way. Six-legged robots have a high degree of flexibility and\ninherent stability that aids them in traversing even some of the most difficult\nterrains, such as collapsed buildings. However, their lack of fast walking\ngaits for easier surfaces is one reason why they are not commonly applied in\nthese scenarios.\n  This work presents LAURON VI, a six-legged robot platform for research on\ndynamic walking gaits as well as on autonomy for complex field missions. The\nrobot's 18 series elastic joint actuators offer high-frequency interfaces for\nCartesian impedance and pure torque control. We have designed, implemented, and\ncompared three control approaches: kinematic-based, model-predictive, and\nreinforcement-learned controllers. The robot hardware and the different control\napproaches were extensively tested in a lab environment as well as on a Mars\nanalog mission. The introduction of fast locomotion strategies for LAURON VI\nmakes six-legged robots vastly more suitable for a wide range of real-world\napplications.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u516d\u8db3\u673a\u5668\u4ebaLAURON VI\uff0c\u65e8\u5728\u901a\u8fc7\u52a8\u6001\u6b65\u6001\u548c\u81ea\u4e3b\u63a7\u5236\u7814\u7a76\u63d0\u5347\u5176\u5728\u6df7\u5408\u5730\u5f62\u4e2d\u7684\u9002\u5e94\u6027\u3002", "motivation": "\u516d\u8db3\u673a\u5668\u4eba\u5728\u590d\u6742\u5730\u5f62\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u7b80\u5355\u5730\u5f62\u4e2d\u7f3a\u4e4f\u5feb\u901f\u6b65\u6001\uff0c\u9650\u5236\u4e86\u5176\u5e7f\u6cdb\u5e94\u7528\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e09\u79cd\u63a7\u5236\u65b9\u6cd5\uff1a\u57fa\u4e8e\u8fd0\u52a8\u5b66\u7684\u3001\u6a21\u578b\u9884\u6d4b\u7684\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u63a7\u5236\u5668\uff0c\u5e76\u5728\u5b9e\u9a8c\u5ba4\u548c\u706b\u661f\u6a21\u62df\u4efb\u52a1\u4e2d\u6d4b\u8bd5\u3002", "result": "\u901a\u8fc7\u5f15\u5165\u5feb\u901f\u8fd0\u52a8\u7b56\u7565\uff0cLAURON VI\u663e\u8457\u63d0\u5347\u4e86\u516d\u8db3\u673a\u5668\u4eba\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u9002\u7528\u6027\u3002", "conclusion": "LAURON VI\u7684\u7814\u7a76\u4e3a\u516d\u8db3\u673a\u5668\u4eba\u5728\u66f4\u5e7f\u6cdb\u573a\u666f\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6280\u672f\u57fa\u7840\u3002"}}
{"id": "2508.07642", "categories": ["cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07642", "abs": "https://arxiv.org/abs/2508.07642", "authors": ["Tianyi Ma", "Yue Zhang", "Zehao Wang", "Parisa Kordjamshidi"], "title": "Breaking Down and Building Up: Mixture of Skill-Based Vision-and-Language Navigation Agents", "comment": "18 pages, 5 Figures,", "summary": "Vision-and-Language Navigation (VLN) poses significant challenges in enabling\nagents to interpret natural language instructions and navigate complex 3D\nenvironments. While recent progress has been driven by large-scale pre-training\nand data augmentation, current methods still struggle to generalize to unseen\nscenarios, particularly when complex spatial and temporal reasoning is\nrequired. In this work, we propose SkillNav, a modular framework that\nintroduces structured, skill-based reasoning into Transformer-based VLN agents.\nOur method decomposes navigation into a set of interpretable atomic skills\n(e.g., Vertical Movement, Area and Region Identification, Stop and Pause), each\nhandled by a specialized agent. We then introduce a novel zero-shot\nVision-Language Model (VLM)-based router, which dynamically selects the most\nsuitable agent at each time step by aligning sub-goals with visual observations\nand historical actions. SkillNav achieves a new state-of-the-art performance on\nthe R2R benchmark and demonstrates strong generalization to the GSA-R2R\nbenchmark that includes novel instruction styles and unseen environments.", "AI": {"tldr": "SkillNav\u662f\u4e00\u4e2a\u6a21\u5757\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u6280\u80fd\u5206\u89e3\u548c\u52a8\u6001\u8def\u7531\u63d0\u5347VLN\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u7a7a\u95f4\u548c\u65f6\u95f4\u63a8\u7406\u7684\u672a\u89c1\u573a\u666f\u4e2d\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002", "method": "\u5c06\u5bfc\u822a\u4efb\u52a1\u5206\u89e3\u4e3a\u53ef\u89e3\u91ca\u7684\u539f\u5b50\u6280\u80fd\uff0c\u5e76\u4f7f\u7528VLM\u52a8\u6001\u8def\u7531\u9009\u62e9\u6700\u4f73\u6280\u80fd\u4ee3\u7406\u3002", "result": "\u5728R2R\u548cGSA-R2R\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u65b0SOTA\uff0c\u6cdb\u5316\u80fd\u529b\u663e\u8457\u3002", "conclusion": "SkillNav\u901a\u8fc7\u6a21\u5757\u5316\u548c\u6280\u80fd\u5206\u89e3\u6709\u6548\u63d0\u5347\u4e86VLN\u4efb\u52a1\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.07758", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07758", "abs": "https://arxiv.org/abs/2508.07758", "authors": ["Antonio Rosales", "Alaa Abderrahim", "Markku Suomalainen", "Mikael Haag", "Tapio Heikkil\u00e4"], "title": "Robot and Overhead Crane Collaboration Scheme to Enhance Payload Manipulation", "comment": null, "summary": "This paper presents a scheme to enhance payload manipulation using a robot\ncollaborating with an overhead crane. In the current industrial practice, when\nthe crane's payload has to be accurately manipulated and located in a desired\nposition, the task becomes laborious and risky since the operators have to\nguide the fine motions of the payload by hand. In the proposed collaborative\nscheme, the crane lifts the payload while the robot's end-effector guides it\ntoward the desired position. The only link between the robot and the crane is\nthe interaction force produced during the guiding of the payload. Two\nadmittance transfer functions are considered to accomplish harmless and smooth\ncontact with the payload. The first is used in a position-based admittance\ncontrol integrated with the robot. The second one adds compliance to the crane\nby processing the interaction force through the admittance transfer function to\ngenerate a crane's velocity command that makes the crane follow the payload.\nThen the robot's end-effector and the crane move collaboratively to guide the\npayload to the desired location. A method is presented to design the admittance\ncontrollers that accomplish a fluent robot-crane collaboration. Simulations and\nexperiments validating the scheme potential are shown.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u673a\u5668\u4eba\u534f\u4f5c\u540a\u8f66\u589e\u5f3a\u8f7d\u8377\u64cd\u63a7\u7684\u65b9\u6848\uff0c\u901a\u8fc7\u529b\u4ea4\u4e92\u5b9e\u73b0\u7cbe\u786e\u5f15\u5bfc\u3002", "motivation": "\u5f53\u524d\u5de5\u4e1a\u5b9e\u8df5\u4e2d\uff0c\u540a\u8f66\u8f7d\u8377\u7684\u7cbe\u786e\u64cd\u63a7\u4f9d\u8d56\u4eba\u5de5\uff0c\u6548\u7387\u4f4e\u4e14\u98ce\u9669\u9ad8\u3002", "method": "\u91c7\u7528\u4e24\u79cd\u5bfc\u7eb3\u4f20\u9012\u51fd\u6570\uff0c\u673a\u5668\u4eba\u57fa\u4e8e\u4f4d\u7f6e\u7684\u5bfc\u7eb3\u63a7\u5236\u4e0e\u540a\u8f66\u7684\u5bfc\u7eb3\u63a7\u5236\u534f\u4f5c\u3002", "result": "\u4eff\u771f\u4e0e\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6848\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u534f\u4f5c\u65b9\u6848\u63d0\u9ad8\u4e86\u8f7d\u8377\u64cd\u63a7\u7684\u7cbe\u786e\u6027\u4e0e\u5b89\u5168\u6027\u3002"}}
{"id": "2508.07649", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07649", "abs": "https://arxiv.org/abs/2508.07649", "authors": ["Jie Li", "Haoye Dong", "Zhengyang Wu", "Zetao Zheng", "Mingrong Lin"], "title": "Disentangling Multiplex Spatial-Temporal Transition Graph Representation Learning for Socially Enhanced POI Recommendation", "comment": null, "summary": "Next Point-of-Interest (POI) recommendation is a research hotspot in business\nintelligence, where users' spatial-temporal transitions and social\nrelationships play key roles. However, most existing works model spatial and\ntemporal transitions separately, leading to misaligned representations of the\nsame spatial-temporal key nodes. This misalignment introduces redundant\ninformation during fusion, increasing model uncertainty and reducing\ninterpretability. To address this issue, we propose DiMuST, a socially enhanced\nPOI recommendation model based on disentangled representation learning over\nmultiplex spatial-temporal transition graphs. The model employs a novel\nDisentangled variational multiplex graph Auto-Encoder (DAE), which first\ndisentangles shared and private distributions using a multiplex\nspatial-temporal graph strategy. It then fuses the shared features via a\nProduct of Experts (PoE) mechanism and denoises the private features through\ncontrastive constraints. The model effectively captures the spatial-temporal\ntransition representations of POIs while preserving the intrinsic correlation\nof their spatial-temporal relationships. Experiments on two challenging\ndatasets demonstrate that our DiMuST significantly outperforms existing methods\nacross multiple metrics.", "AI": {"tldr": "DiMuST\u662f\u4e00\u4e2a\u57fa\u4e8e\u89e3\u8026\u8868\u793a\u5b66\u4e60\u7684POI\u63a8\u8350\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u65f6\u7a7a\u8f6c\u6362\u56fe\u548c\u793e\u4f1a\u5173\u7cfb\u63d0\u5347\u63a8\u8350\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06\u65f6\u7a7a\u8f6c\u6362\u5206\u5f00\u5efa\u6a21\uff0c\u5bfc\u81f4\u5173\u952e\u8282\u70b9\u8868\u793a\u4e0d\u4e00\u81f4\uff0c\u589e\u52a0\u4e86\u5197\u4f59\u4fe1\u606f\u548c\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u63d0\u51faDiMuST\u6a21\u578b\uff0c\u91c7\u7528\u89e3\u8026\u53d8\u5206\u591a\u56fe\u81ea\u7f16\u7801\u5668\uff08DAE\uff09\uff0c\u5206\u79bb\u5171\u4eab\u548c\u79c1\u6709\u5206\u5e03\uff0c\u5e76\u901a\u8fc7PoE\u673a\u5236\u878d\u5408\u5171\u4eab\u7279\u5f81\uff0c\u5bf9\u6bd4\u7ea6\u675f\u53bb\u566a\u79c1\u6709\u7279\u5f81\u3002", "result": "\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\uff0cDiMuST\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DiMuST\u6709\u6548\u6355\u6349\u4e86POI\u7684\u65f6\u7a7a\u8f6c\u6362\u8868\u793a\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u65f6\u7a7a\u5173\u7cfb\u7684\u56fa\u6709\u76f8\u5173\u6027\u3002"}}
{"id": "2508.07770", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07770", "abs": "https://arxiv.org/abs/2508.07770", "authors": ["Yizheng Zhang", "Zhenjun Yu", "Jiaxin Lai", "Cewu Lu", "Lei Han"], "title": "AgentWorld: An Interactive Simulation Platform for Scene Construction and Mobile Robotic Manipulation", "comment": "Accepted by Conference on Robot Learning 2025", "summary": "We introduce AgentWorld, an interactive simulation platform for developing\nhousehold mobile manipulation capabilities. Our platform combines automated\nscene construction that encompasses layout generation, semantic asset\nplacement, visual material configuration, and physics simulation, with a\ndual-mode teleoperation system supporting both wheeled bases and humanoid\nlocomotion policies for data collection. The resulting AgentWorld Dataset\ncaptures diverse tasks ranging from primitive actions (pick-and-place,\npush-pull, etc.) to multistage activities (serve drinks, heat up food, etc.)\nacross living rooms, bedrooms, and kitchens. Through extensive benchmarking of\nimitation learning methods including behavior cloning, action chunking\ntransformers, diffusion policies, and vision-language-action models, we\ndemonstrate the dataset's effectiveness for sim-to-real transfer. The\nintegrated system provides a comprehensive solution for scalable robotic skill\nacquisition in complex home environments, bridging the gap between\nsimulation-based training and real-world deployment. The code, datasets will be\navailable at https://yizhengzhang1.github.io/agent_world/", "AI": {"tldr": "AgentWorld\u662f\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u6a21\u62df\u5e73\u53f0\uff0c\u7528\u4e8e\u5f00\u53d1\u5bb6\u5ead\u79fb\u52a8\u64cd\u4f5c\u80fd\u529b\uff0c\u7ed3\u5408\u81ea\u52a8\u5316\u573a\u666f\u6784\u5efa\u548c\u53cc\u6a21\u5f0f\u8fdc\u7a0b\u64cd\u4f5c\u7cfb\u7edf\uff0c\u652f\u6301\u4ece\u57fa\u7840\u52a8\u4f5c\u5230\u591a\u9636\u6bb5\u6d3b\u52a8\u7684\u4efb\u52a1\u6570\u636e\u6536\u96c6\uff0c\u5e76\u901a\u8fc7\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u590d\u6742\u5bb6\u5ead\u73af\u5883\u4e2d\u673a\u5668\u4eba\u6280\u80fd\u7684\u53ef\u6269\u5c55\u83b7\u53d6\u95ee\u9898\uff0c\u7f29\u5c0f\u4eff\u771f\u8bad\u7ec3\u4e0e\u5b9e\u9645\u90e8\u7f72\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "\u5e73\u53f0\u7ed3\u5408\u81ea\u52a8\u5316\u573a\u666f\u6784\u5efa\uff08\u5e03\u5c40\u751f\u6210\u3001\u8bed\u4e49\u8d44\u4ea7\u653e\u7f6e\u3001\u89c6\u89c9\u6750\u8d28\u914d\u7f6e\u3001\u7269\u7406\u6a21\u62df\uff09\u548c\u53cc\u6a21\u5f0f\u8fdc\u7a0b\u64cd\u4f5c\u7cfb\u7edf\uff08\u8f6e\u5f0f\u5e95\u5ea7\u548c\u4eba\u5f62\u8fd0\u52a8\u7b56\u7565\uff09\uff0c\u6536\u96c6\u591a\u6837\u5316\u4efb\u52a1\u6570\u636e\u3002", "result": "\u901a\u8fc7\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\uff08\u5982\u884c\u4e3a\u514b\u9686\u3001\u52a8\u4f5c\u5206\u5757\u53d8\u6362\u5668\u3001\u6269\u6563\u7b56\u7565\u548c\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff09\u9a8c\u8bc1\u4e86\u6570\u636e\u96c6\u7684\u4eff\u771f\u5230\u73b0\u5b9e\u8f6c\u79fb\u6548\u679c\u3002", "conclusion": "AgentWorld\u4e3a\u590d\u6742\u5bb6\u5ead\u73af\u5883\u4e2d\u7684\u673a\u5668\u4eba\u6280\u80fd\u83b7\u53d6\u63d0\u4f9b\u4e86\u5168\u9762\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5c06\u516c\u5f00\u3002"}}
{"id": "2508.07667", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07667", "abs": "https://arxiv.org/abs/2508.07667", "authors": ["Wenkai Li", "Liwen Sun", "Zhenxiang Guan", "Xuhui Zhou", "Maarten Sap"], "title": "1-2-3 Check: Enhancing Contextual Privacy in LLM via Multi-Agent Reasoning", "comment": null, "summary": "Addressing contextual privacy concerns remains challenging in interactive\nsettings where large language models (LLMs) process information from multiple\nsources (e.g., summarizing meetings with private and public information). We\nintroduce a multi-agent framework that decomposes privacy reasoning into\nspecialized subtasks (extraction, classification), reducing the information\nload on any single agent while enabling iterative validation and more reliable\nadherence to contextual privacy norms. To understand how privacy errors emerge\nand propagate, we conduct a systematic ablation over information-flow\ntopologies, revealing when and why upstream detection mistakes cascade into\ndownstream leakage. Experiments on the ConfAIde and PrivacyLens benchmark with\nseveral open-source and closed-sourced LLMs demonstrate that our best\nmulti-agent configuration substantially reduces private information leakage\n(\\textbf{18\\%} on ConfAIde and \\textbf{19\\%} on PrivacyLens with GPT-4o) while\npreserving the fidelity of public content, outperforming single-agent\nbaselines. These results highlight the promise of principled information-flow\ndesign in multi-agent systems for contextual privacy with LLMs.", "AI": {"tldr": "\u63d0\u51fa\u591a\u667a\u80fd\u4f53\u6846\u67b6\u89e3\u51b3LLMs\u5904\u7406\u591a\u6e90\u4fe1\u606f\u65f6\u7684\u9690\u79c1\u95ee\u9898\uff0c\u901a\u8fc7\u5206\u89e3\u9690\u79c1\u63a8\u7406\u4efb\u52a1\u51cf\u5c11\u5355\u667a\u80fd\u4f53\u8d1f\u62c5\uff0c\u5b9e\u9a8c\u663e\u793a\u663e\u8457\u964d\u4f4e\u9690\u79c1\u6cc4\u9732\u3002", "motivation": "\u89e3\u51b3LLMs\u5728\u591a\u6e90\u4fe1\u606f\u4ea4\u4e92\u573a\u666f\u4e2d\u7684\u9690\u79c1\u95ee\u9898\uff0c\u907f\u514d\u9690\u79c1\u4fe1\u606f\u6cc4\u9732\u3002", "method": "\u5f15\u5165\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5206\u89e3\u9690\u79c1\u63a8\u7406\u4e3a\u63d0\u53d6\u3001\u5206\u7c7b\u7b49\u5b50\u4efb\u52a1\uff0c\u5e76\u901a\u8fc7\u4fe1\u606f\u6d41\u62d3\u6251\u5206\u6790\u9690\u79c1\u9519\u8bef\u4f20\u64ad\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u591a\u667a\u80fd\u4f53\u914d\u7f6e\u663e\u8457\u51cf\u5c11\u9690\u79c1\u6cc4\u9732\uff08ConfAIde 18%\uff0cPrivacyLens 19%\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u516c\u5171\u5185\u5bb9\u51c6\u786e\u6027\u3002", "conclusion": "\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u5728LLMs\u7684\u4e0a\u4e0b\u6587\u9690\u79c1\u4fdd\u62a4\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4fe1\u606f\u6d41\u8bbe\u8ba1\u662f\u5173\u952e\u3002"}}
{"id": "2508.07814", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07814", "abs": "https://arxiv.org/abs/2508.07814", "authors": ["Malaika Zafar", "Roohan Ahmed Khan", "Faryal Batool", "Yasheerah Yaqoot", "Ziang Guo", "Mikhail Litvinov", "Aleksey Fedoseev", "Dzmitry Tsetserukou"], "title": "SwarmVLM: VLM-Guided Impedance Control for Autonomous Navigation of Heterogeneous Robots in Dynamic Warehousing", "comment": null, "summary": "With the growing demand for efficient logistics, unmanned aerial vehicles\n(UAVs) are increasingly being paired with automated guided vehicles (AGVs).\nWhile UAVs offer the ability to navigate through dense environments and varying\naltitudes, they are limited by battery life, payload capacity, and flight\nduration, necessitating coordinated ground support.\n  Focusing on heterogeneous navigation, SwarmVLM addresses these limitations by\nenabling semantic collaboration between UAVs and ground robots through\nimpedance control. The system leverages the Vision Language Model (VLM) and the\nRetrieval-Augmented Generation (RAG) to adjust impedance control parameters in\nresponse to environmental changes. In this framework, the UAV acts as a leader\nusing Artificial Potential Field (APF) planning for real-time navigation, while\nthe ground robot follows via virtual impedance links with adaptive link\ntopology to avoid collisions with short obstacles.\n  The system demonstrated a 92% success rate across 12 real-world trials. Under\noptimal lighting conditions, the VLM-RAG framework achieved 8% accuracy in\nobject detection and selection of impedance parameters. The mobile robot\nprioritized short obstacle avoidance, occasionally resulting in a lateral\ndeviation of up to 50 cm from the UAV path, which showcases safe navigation in\na cluttered setting.", "AI": {"tldr": "SwarmVLM\u901a\u8fc7\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u963b\u6297\u63a7\u5236\u5b9e\u73b0\u65e0\u4eba\u673a\u4e0e\u5730\u9762\u673a\u5668\u4eba\u7684\u8bed\u4e49\u534f\u4f5c\uff0c\u89e3\u51b3\u65e0\u4eba\u673a\u7535\u6c60\u5bff\u547d\u548c\u8d1f\u8f7d\u9650\u5236\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u7269\u6d41\u9700\u6c42\u589e\u957f\uff0c\u65e0\u4eba\u673a\u4e0e\u81ea\u52a8\u5bfc\u5f15\u8f66\u534f\u540c\u5de5\u4f5c\u6210\u4e3a\u8d8b\u52bf\uff0c\u4f46\u65e0\u4eba\u673a\u53d7\u9650\u4e8e\u7535\u6c60\u5bff\u547d\u548c\u98de\u884c\u65f6\u95f4\uff0c\u9700\u5730\u9762\u652f\u6301\u3002", "method": "SwarmVLM\u7ed3\u5408VLM\u548cRAG\u6280\u672f\uff0c\u901a\u8fc7\u963b\u6297\u63a7\u5236\u8c03\u6574\u53c2\u6570\uff0c\u65e0\u4eba\u673a\u4f5c\u4e3a\u9886\u5bfc\u8005\u4f7f\u7528APF\u89c4\u5212\u5b9e\u65f6\u5bfc\u822a\uff0c\u5730\u9762\u673a\u5668\u4eba\u901a\u8fc7\u865a\u62df\u963b\u6297\u94fe\u63a5\u8ddf\u968f\u3002", "result": "\u7cfb\u7edf\u572812\u6b21\u771f\u5b9e\u8bd5\u9a8c\u4e2d\u6210\u529f\u7387\u8fbe92%\uff0cVLM-RAG\u5728\u7406\u60f3\u5149\u7167\u4e0b\u7269\u4f53\u68c0\u6d4b\u548c\u53c2\u6570\u9009\u62e9\u51c6\u786e\u7387\u4e3a8%\uff0c\u5730\u9762\u673a\u5668\u4eba\u80fd\u5b89\u5168\u907f\u5f00\u77ed\u969c\u788d\u7269\u3002", "conclusion": "SwarmVLM\u5c55\u793a\u4e86\u5f02\u6784\u5bfc\u822a\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u65e0\u4eba\u673a\u4e0e\u5730\u9762\u673a\u5668\u4eba\u534f\u4f5c\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.07671", "categories": ["cs.AI", "cs.CY", "cs.HC", "cs.MA", "stat.AP", "68T07, 68T42, 68T50, 91F20, 62P25", "I.2.11; I.2.1; H.1.2; J.4; K.4.2"], "pdf": "https://arxiv.org/pdf/2508.07671", "abs": "https://arxiv.org/abs/2508.07671", "authors": ["Mohamed Rayan Barhdadi", "Mehmet Tuncel", "Erchin Serpedin", "Hasan Kurban"], "title": "EMPATHIA: Multi-Faceted Human-AI Collaboration for Refugee Integration", "comment": "19 pages, 3 figures (plus 6 figures in supplementary), 2 tables, 1\n  algorithm. Submitted to NeurIPS 2025 Creative AI Track: Humanity", "summary": "Current AI approaches to refugee integration optimize narrow objectives such\nas employment and fail to capture the cultural, emotional, and ethical\ndimensions critical for long-term success. We introduce EMPATHIA (Enriched\nMultimodal Pathways for Agentic Thinking in Humanitarian Immigrant Assistance),\na multi-agent framework addressing the central Creative AI question: how do we\npreserve human dignity when machines participate in life-altering decisions?\nGrounded in Kegan's Constructive Developmental Theory, EMPATHIA decomposes\nintegration into three modules: SEED (Socio-cultural Entry and Embedding\nDecision) for initial placement, RISE (Rapid Integration and Self-sufficiency\nEngine) for early independence, and THRIVE (Transcultural Harmony and\nResilience through Integrated Values and Engagement) for sustained outcomes.\nSEED employs a selector-validator architecture with three specialized agents -\nemotional, cultural, and ethical - that deliberate transparently to produce\ninterpretable recommendations. Experiments on the UN Kakuma dataset (15,026\nindividuals, 7,960 eligible adults 15+ per ILO/UNHCR standards) and\nimplementation on 6,359 working-age refugees (15+) with 150+ socioeconomic\nvariables achieved 87.4% validation convergence and explainable assessments\nacross five host countries. EMPATHIA's weighted integration of cultural,\nemotional, and ethical factors balances competing value systems while\nsupporting practitioner-AI collaboration. By augmenting rather than replacing\nhuman expertise, EMPATHIA provides a generalizable framework for AI-driven\nallocation tasks where multiple values must be reconciled.", "AI": {"tldr": "EMPATHIA\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u96be\u6c11\u6574\u5408\u4e2d\u7684\u6587\u5316\u3001\u60c5\u611f\u548c\u4f26\u7406\u95ee\u9898\uff0c\u901a\u8fc7\u4e09\u4e2a\u6a21\u5757\uff08SEED\u3001RISE\u3001THRIVE\uff09\u5b9e\u73b0\u900f\u660e\u548c\u53ef\u89e3\u91ca\u7684\u51b3\u7b56\u3002", "motivation": "\u5f53\u524dAI\u65b9\u6cd5\u5728\u96be\u6c11\u6574\u5408\u4e2d\u8fc7\u4e8e\u5173\u6ce8\u5c31\u4e1a\u7b49\u72ed\u7a84\u76ee\u6807\uff0c\u5ffd\u89c6\u4e86\u6587\u5316\u3001\u60c5\u611f\u548c\u4f26\u7406\u7b49\u957f\u671f\u6210\u529f\u7684\u5173\u952e\u7ef4\u5ea6\u3002", "method": "\u57fa\u4e8eKegan\u7684\u5efa\u6784\u53d1\u5c55\u7406\u8bba\uff0cEMPATHIA\u5206\u89e3\u6574\u5408\u8fc7\u7a0b\u4e3a\u4e09\u4e2a\u6a21\u5757\uff0c\u91c7\u7528\u9009\u62e9\u5668-\u9a8c\u8bc1\u5668\u67b6\u6784\uff0c\u7531\u60c5\u611f\u3001\u6587\u5316\u548c\u4f26\u7406\u4e09\u4e2a\u667a\u80fd\u4f53\u5171\u540c\u51b3\u7b56\u3002", "result": "\u5728UN Kakuma\u6570\u636e\u96c6\u4e0a\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u6536\u655b\u7387\u8fbe87.4%\uff0c\u5e76\u5728\u4e94\u4e2a\u4e1c\u9053\u56fd\u5b9e\u73b0\u53ef\u89e3\u91ca\u7684\u8bc4\u4f30\u3002", "conclusion": "EMPATHIA\u901a\u8fc7\u5e73\u8861\u591a\u79cd\u4ef7\u503c\u7cfb\u7edf\u5e76\u652f\u6301\u4eba\u673a\u534f\u4f5c\uff0c\u4e3aAI\u9a71\u52a8\u7684\u5206\u914d\u4efb\u52a1\u63d0\u4f9b\u4e86\u901a\u7528\u6846\u67b6\u3002"}}
{"id": "2508.07839", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07839", "abs": "https://arxiv.org/abs/2508.07839", "authors": ["Qiaoqiao Ren", "Tony Belpaeme"], "title": "Touch Speaks, Sound Feels: A Multimodal Approach to Affective and Social Touch from Robots to Humans", "comment": null, "summary": "Affective tactile interaction constitutes a fundamental component of human\ncommunication. In natural human-human encounters, touch is seldom experienced\nin isolation; rather, it is inherently multisensory. Individuals not only\nperceive the physical sensation of touch but also register the accompanying\nauditory cues generated through contact. The integration of haptic and auditory\ninformation forms a rich and nuanced channel for emotional expression. While\nextensive research has examined how robots convey emotions through facial\nexpressions and speech, their capacity to communicate social gestures and\nemotions via touch remains largely underexplored. To address this gap, we\ndeveloped a multimodal interaction system incorporating a 5*5 grid of 25\nvibration motors synchronized with audio playback, enabling robots to deliver\ncombined haptic-audio stimuli. In an experiment involving 32 Chinese\nparticipants, ten emotions and six social gestures were presented through\nvibration, sound, or their combination. Participants rated each stimulus on\narousal and valence scales. The results revealed that (1) the combined\nhaptic-audio modality significantly enhanced decoding accuracy compared to\nsingle modalities; (2) each individual channel-vibration or sound-effectively\nsupported certain emotions recognition, with distinct advantages depending on\nthe emotional expression; and (3) gestures alone were generally insufficient\nfor conveying clearly distinguishable emotions. These findings underscore the\nimportance of multisensory integration in affective human-robot interaction and\nhighlight the complementary roles of haptic and auditory cues in enhancing\nemotional communication.", "AI": {"tldr": "\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u7ed3\u5408\u89e6\u89c9\u548c\u542c\u89c9\u7684\u591a\u6a21\u6001\u4ea4\u4e92\u7cfb\u7edf\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u60c5\u611f\u8868\u8fbe\uff0c\u5b9e\u9a8c\u8868\u660e\u591a\u6a21\u6001\u663e\u8457\u63d0\u9ad8\u4e86\u60c5\u611f\u89e3\u7801\u51c6\u786e\u6027\u3002", "motivation": "\u63a2\u7d22\u673a\u5668\u4eba\u901a\u8fc7\u89e6\u89c9\u548c\u542c\u89c9\u7ed3\u5408\u7684\u65b9\u5f0f\u8868\u8fbe\u60c5\u611f\u548c\u793e\u4ea4\u624b\u52bf\u7684\u80fd\u529b\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5305\u542b25\u4e2a\u632f\u52a8\u7535\u673a\u548c\u97f3\u9891\u64ad\u653e\u7684\u591a\u6a21\u6001\u7cfb\u7edf\uff0c\u901a\u8fc7\u632f\u52a8\u3001\u58f0\u97f3\u6216\u7ec4\u5408\u65b9\u5f0f\u5448\u73b0\u60c5\u611f\u548c\u624b\u52bf\uff0c32\u540d\u53c2\u4e0e\u8005\u8bc4\u4f30\u4e86\u523a\u6fc0\u7684\u5524\u9192\u5ea6\u548c\u6548\u4ef7\u3002", "result": "\u591a\u6a21\u6001\u663e\u8457\u63d0\u9ad8\u4e86\u89e3\u7801\u51c6\u786e\u6027\uff1b\u89e6\u89c9\u548c\u542c\u89c9\u5404\u81ea\u5bf9\u67d0\u4e9b\u60c5\u611f\u8bc6\u522b\u6709\u4f18\u52bf\uff1b\u624b\u52bf\u5355\u72ec\u96be\u4ee5\u6e05\u6670\u4f20\u8fbe\u60c5\u611f\u3002", "conclusion": "\u591a\u611f\u5b98\u6574\u5408\u5bf9\u60c5\u611f\u4eba\u673a\u4ea4\u4e92\u81f3\u5173\u91cd\u8981\uff0c\u89e6\u89c9\u548c\u542c\u89c9\u7ebf\u7d22\u5728\u60c5\u611f\u6c9f\u901a\u4e2d\u5177\u6709\u4e92\u8865\u4f5c\u7528\u3002"}}
{"id": "2508.07673", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07673", "abs": "https://arxiv.org/abs/2508.07673", "authors": ["Gianluca Bontempi"], "title": "Ethics2vec: aligning automatic agents and human preferences", "comment": null, "summary": "Though intelligent agents are supposed to improve human experience (or make\nit more efficient), it is hard from a human perspective to grasp the ethical\nvalues which are explicitly or implicitly embedded in an agent behaviour. This\nis the well-known problem of alignment, which refers to the challenge of\ndesigning AI systems that align with human values, goals and preferences. This\nproblem is particularly challenging since most human ethical considerations\nrefer to \\emph{incommensurable} (i.e. non-measurable and/or incomparable)\nvalues and criteria. Consider, for instance, a medical agent prescribing a\ntreatment to a cancerous patient. How could it take into account (and/or weigh)\nincommensurable aspects like the value of a human life and the cost of the\ntreatment? Now, the alignment between human and artificial values is possible\nonly if we define a common space where a metric can be defined and used. This\npaper proposes to extend to ethics the conventional Anything2vec approach,\nwhich has been successful in plenty of similar and hard-to-quantify domains\n(ranging from natural language processing to recommendation systems and graph\nanalysis). This paper proposes a way to map an automatic agent decision-making\n(or control law) strategy to a multivariate vector representation, which can be\nused to compare and assess the alignment with human values. The Ethics2Vec\nmethod is first introduced in the case of an automatic agent performing binary\ndecision-making. Then, a vectorisation of an automatic control law (like in the\ncase of a self-driving car) is discussed to show how the approach can be\nextended to automatic control settings.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEthics2Vec\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5411\u91cf\u5316AI\u4ee3\u7406\u7684\u51b3\u7b56\u884c\u4e3a\uff0c\u8bc4\u4f30\u5176\u4e0e\u4eba\u7c7b\u4f26\u7406\u4ef7\u503c\u89c2\u7684\u5bf9\u9f50\u7a0b\u5ea6\u3002", "motivation": "AI\u4ee3\u7406\u7684\u884c\u4e3a\u4e2d\u9690\u542b\u7684\u4f26\u7406\u4ef7\u503c\u89c2\u96be\u4ee5\u4ece\u4eba\u7c7b\u89d2\u5ea6\u7406\u89e3\u548c\u8861\u91cf\uff0c\u5c24\u5176\u662f\u5728\u6d89\u53ca\u4e0d\u53ef\u6bd4\u8f83\u7684\u4ef7\u503c\u89c2\u65f6\uff08\u5982\u751f\u547d\u4ef7\u503c\u4e0e\u6cbb\u7597\u6210\u672c\uff09\u3002", "method": "\u6269\u5c55Anything2vec\u65b9\u6cd5\uff0c\u5c06\u4ee3\u7406\u7684\u51b3\u7b56\u7b56\u7565\u6620\u5c04\u4e3a\u591a\u5143\u5411\u91cf\u8868\u793a\uff0c\u7528\u4e8e\u6bd4\u8f83\u548c\u8bc4\u4f30\u4e0e\u4eba\u7c7b\u4ef7\u503c\u89c2\u7684\u5bf9\u9f50\u3002", "result": "\u63d0\u51fa\u4e86Ethics2Vec\u65b9\u6cd5\uff0c\u5e76\u5728\u4e8c\u5143\u51b3\u7b56\u548c\u81ea\u52a8\u9a7e\u9a76\u63a7\u5236\u573a\u666f\u4e2d\u9a8c\u8bc1\u5176\u53ef\u884c\u6027\u3002", "conclusion": "Ethics2Vec\u4e3aAI\u4f26\u7406\u5bf9\u9f50\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u91cf\u5316\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u81ea\u52a8\u51b3\u7b56\u573a\u666f\u3002"}}
{"id": "2508.07842", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07842", "abs": "https://arxiv.org/abs/2508.07842", "authors": ["Yutong Shen", "Hangxu Liu", "Penghui Liu", "Ruizhe Xia", "Tianyi Yao", "Yitong Sun", "Tongtong Feng"], "title": "DETACH: Cross-domain Learning for Long-Horizon Tasks via Mixture of Disentangled Experts", "comment": "14 pages,8 figures. Submitted to AAAI'26", "summary": "Long-Horizon (LH) tasks in Human-Scene Interaction (HSI) are complex\nmulti-step tasks that require continuous planning, sequential decision-making,\nand extended execution across domains to achieve the final goal. However,\nexisting methods heavily rely on skill chaining by concatenating pre-trained\nsubtasks, with environment observations and self-state tightly coupled, lacking\nthe ability to generalize to new combinations of environments and skills,\nfailing to complete various LH tasks across domains. To solve this problem,\nthis paper presents DETACH, a cross-domain learning framework for LH tasks via\nbiologically inspired dual-stream disentanglement. Inspired by the brain's\n\"where-what\" dual pathway mechanism, DETACH comprises two core modules: i) an\nenvironment learning module for spatial understanding, which captures object\nfunctions, spatial relationships, and scene semantics, achieving cross-domain\ntransfer through complete environment-self disentanglement; ii) a skill\nlearning module for task execution, which processes self-state information\nincluding joint degrees of freedom and motor patterns, enabling cross-skill\ntransfer through independent motor pattern encoding. We conducted extensive\nexperiments on various LH tasks in HSI scenes. Compared with existing methods,\nDETACH can achieve an average subtasks success rate improvement of 23% and\naverage execution efficiency improvement of 29%.", "AI": {"tldr": "DETACH\u6846\u67b6\u901a\u8fc7\u53cc\u6d41\u89e3\u8026\u65b9\u6cd5\u63d0\u5347\u957f\u65f6\u7a0b\u4efb\u52a1\u7684\u8de8\u57df\u6cdb\u5316\u80fd\u529b\uff0c\u5e73\u5747\u5b50\u4efb\u52a1\u6210\u529f\u7387\u63d0\u534723%\uff0c\u6267\u884c\u6548\u7387\u63d0\u534729%\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6280\u80fd\u94fe\uff0c\u65e0\u6cd5\u6cdb\u5316\u5230\u65b0\u73af\u5883\u548c\u6280\u80fd\u7ec4\u5408\uff0c\u5bfc\u81f4\u8de8\u57df\u957f\u65f6\u7a0b\u4efb\u52a1\u5931\u8d25\u3002", "method": "DETACH\u91c7\u7528\u751f\u7269\u542f\u53d1\u7684\u53cc\u6d41\u89e3\u8026\u673a\u5236\uff0c\u5305\u62ec\u73af\u5883\u5b66\u4e60\u6a21\u5757\uff08\u7a7a\u95f4\u7406\u89e3\uff09\u548c\u6280\u80fd\u5b66\u4e60\u6a21\u5757\uff08\u4efb\u52a1\u6267\u884c\uff09\u3002", "result": "\u5b9e\u9a8c\u663e\u793aDETACH\u5728\u5b50\u4efb\u52a1\u6210\u529f\u7387\u548c\u6267\u884c\u6548\u7387\u4e0a\u5206\u522b\u5e73\u5747\u63d0\u534723%\u548c29%\u3002", "conclusion": "DETACH\u901a\u8fc7\u89e3\u8026\u73af\u5883\u4e0e\u81ea\u8eab\u72b6\u6001\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8de8\u57df\u957f\u65f6\u7a0b\u4efb\u52a1\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.07743", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07743", "abs": "https://arxiv.org/abs/2508.07743", "authors": ["Markus Fritzsche", "Elliot Gestrin", "Jendrik Seipp"], "title": "Symmetry-Aware Transformer Training for Automated Planning", "comment": null, "summary": "While transformers excel in many settings, their application in the field of\nautomated planning is limited. Prior work like PlanGPT, a state-of-the-art\ndecoder-only transformer, struggles with extrapolation from easy to hard\nplanning problems. This in turn stems from problem symmetries: planning tasks\ncan be represented with arbitrary variable names that carry no meaning beyond\nbeing identifiers. This causes a combinatorial explosion of equivalent\nrepresentations that pure transformers cannot efficiently learn from. We\npropose a novel contrastive learning objective to make transformers\nsymmetry-aware and thereby compensate for their lack of inductive bias.\nCombining this with architectural improvements, we show that transformers can\nbe efficiently trained for either plan-generation or heuristic-prediction. Our\nresults across multiple planning domains demonstrate that our symmetry-aware\ntraining effectively and efficiently addresses the limitations of PlanGPT.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5bf9\u6bd4\u5b66\u4e60\u76ee\u6807\uff0c\u4f7fTransformer\u80fd\u591f\u611f\u77e5\u5bf9\u79f0\u6027\uff0c\u4ece\u800c\u89e3\u51b3\u5176\u5728\u81ea\u52a8\u89c4\u5212\u9886\u57df\u7684\u5c40\u9650\u6027\u3002", "motivation": "Transformer\u5728\u81ea\u52a8\u89c4\u5212\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3b\u8981\u7531\u4e8e\u65e0\u6cd5\u6709\u6548\u5904\u7406\u95ee\u9898\u5bf9\u79f0\u6027\u5bfc\u81f4\u7684\u7ec4\u5408\u7206\u70b8\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u5bf9\u6bd4\u5b66\u4e60\u76ee\u6807\uff0c\u7ed3\u5408\u67b6\u6784\u6539\u8fdb\uff0c\u4f7fTransformer\u80fd\u591f\u9ad8\u6548\u5b66\u4e60\u89c4\u5212\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5bf9\u79f0\u611f\u77e5\u8bad\u7ec3\u6709\u6548\u89e3\u51b3\u4e86PlanGPT\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u5bf9\u79f0\u611f\u77e5\u8bad\u7ec3\u663e\u8457\u63d0\u5347\u4e86Transformer\u5728\u81ea\u52a8\u89c4\u5212\u4e2d\u7684\u8868\u73b0\u3002"}}
{"id": "2508.07885", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.07885", "abs": "https://arxiv.org/abs/2508.07885", "authors": ["Shoaib Ahmmad", "Zubayer Ahmed Aditto", "Md Mehrab Hossain", "Noushin Yeasmin", "Shorower Hossain"], "title": "Autonomous Navigation of Cloud-Controlled Quadcopters in Confined Spaces Using Multi-Modal Perception and LLM-Driven High Semantic Reasoning", "comment": null, "summary": "This paper introduces an advanced AI-driven perception system for autonomous\nquadcopter navigation in GPS-denied indoor environments. The proposed framework\nleverages cloud computing to offload computationally intensive tasks and\nincorporates a custom-designed printed circuit board (PCB) for efficient sensor\ndata acquisition, enabling robust navigation in confined spaces. The system\nintegrates YOLOv11 for object detection, Depth Anything V2 for monocular depth\nestimation, a PCB equipped with Time-of-Flight (ToF) sensors and an Inertial\nMeasurement Unit (IMU), and a cloud-based Large Language Model (LLM) for\ncontext-aware decision-making. A virtual safety envelope, enforced by\ncalibrated sensor offsets, ensures collision avoidance, while a multithreaded\narchitecture achieves low-latency processing. Enhanced spatial awareness is\nfacilitated by 3D bounding box estimation with Kalman filtering. Experimental\nresults in an indoor testbed demonstrate strong performance, with object\ndetection achieving a mean Average Precision (mAP50) of 0.6, depth estimation\nMean Absolute Error (MAE) of 7.2 cm, only 16 safety envelope breaches across 42\ntrials over approximately 11 minutes, and end-to-end system latency below 1\nsecond. This cloud-supported, high-intelligence framework serves as an\nauxiliary perception and navigation system, complementing state-of-the-art\ndrone autonomy for GPS-denied confined spaces.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eAI\u7684\u611f\u77e5\u7cfb\u7edf\uff0c\u7528\u4e8eGPS\u7f3a\u5931\u7684\u5ba4\u5185\u73af\u5883\u4e2d\u81ea\u4e3b\u56db\u8f74\u98de\u884c\u5668\u5bfc\u822a\uff0c\u7ed3\u5408\u4e91\u8ba1\u7b97\u548c\u5b9a\u5236PCB\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u5bfc\u822a\u3002", "motivation": "\u89e3\u51b3GPS\u7f3a\u5931\u73af\u5883\u4e0b\u56db\u8f74\u98de\u884c\u5668\u7684\u81ea\u4e3b\u5bfc\u822a\u95ee\u9898\uff0c\u63d0\u5347\u5728\u72ed\u5c0f\u7a7a\u95f4\u4e2d\u7684\u611f\u77e5\u548c\u51b3\u7b56\u80fd\u529b\u3002", "method": "\u7cfb\u7edf\u6574\u5408\u4e86YOLOv11\u76ee\u6807\u68c0\u6d4b\u3001Depth Anything V2\u6df1\u5ea6\u4f30\u8ba1\u3001\u5b9a\u5236PCB\uff08\u542bToF\u4f20\u611f\u5668\u548cIMU\uff09\u53ca\u4e91\u7aefLLM\uff0c\u91c7\u7528\u591a\u7ebf\u7a0b\u67b6\u6784\u548c\u865a\u62df\u5b89\u5168\u5305\u7edc\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u76ee\u6807\u68c0\u6d4bmAP50\u4e3a0.6\uff0c\u6df1\u5ea6\u4f30\u8ba1MAE\u4e3a7.2 cm\uff0c42\u6b21\u8bd5\u9a8c\u4e2d\u4ec516\u6b21\u5b89\u5168\u5305\u7edc\u7a81\u7834\uff0c\u7cfb\u7edf\u5ef6\u8fdf\u4f4e\u4e8e1\u79d2\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aGPS\u7f3a\u5931\u73af\u5883\u4e0b\u7684\u65e0\u4eba\u673a\u5bfc\u822a\u63d0\u4f9b\u4e86\u9ad8\u6548\u8f85\u52a9\u611f\u77e5\u7cfb\u7edf\u3002"}}
{"id": "2508.07790", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2508.07790", "abs": "https://arxiv.org/abs/2508.07790", "authors": ["Alessandro Abate", "Thom Badings", "Giuseppe De Giacomo", "Francesco Fabiano"], "title": "Best-Effort Policies for Robust Markov Decision Processes", "comment": null, "summary": "We study the common generalization of Markov decision processes (MDPs) with\nsets of transition probabilities, known as robust MDPs (RMDPs). A standard goal\nin RMDPs is to compute a policy that maximizes the expected return under an\nadversarial choice of the transition probabilities. If the uncertainty in the\nprobabilities is independent between the states, known as s-rectangularity,\nsuch optimal robust policies can be computed efficiently using robust value\niteration. However, there might still be multiple optimal robust policies,\nwhich, while equivalent with respect to the worst-case, reflect different\nexpected returns under non-adversarial choices of the transition probabilities.\nHence, we propose a refined policy selection criterion for RMDPs, drawing\ninspiration from the notions of dominance and best-effort in game theory.\nInstead of seeking a policy that only maximizes the worst-case expected return,\nwe additionally require the policy to achieve a maximal expected return under\ndifferent (i.e., not fully adversarial) transition probabilities. We call such\na policy an optimal robust best-effort (ORBE) policy. We prove that ORBE\npolicies always exist, characterize their structure, and present an algorithm\nto compute them with a small overhead compared to standard robust value\niteration. ORBE policies offer a principled tie-breaker among optimal robust\npolicies. Numerical experiments show the feasibility of our approach.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u9c81\u68d2\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08RMDPs\uff09\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7b56\u7565\u9009\u62e9\u6807\u51c6\u2014\u2014\u6700\u4f18\u9c81\u68d2\u6700\u4f73\u52aa\u529b\uff08ORBE\uff09\u7b56\u7565\uff0c\u4ee5\u5728\u975e\u5b8c\u5168\u5bf9\u6297\u6027\u6982\u7387\u4e0b\u6700\u5927\u5316\u671f\u671b\u56de\u62a5\u3002", "motivation": "\u5728RMDPs\u4e2d\uff0c\u901a\u5e38\u76ee\u6807\u662f\u8ba1\u7b97\u5728\u5bf9\u6297\u6027\u6982\u7387\u4e0b\u6700\u5927\u5316\u671f\u671b\u56de\u62a5\u7684\u7b56\u7565\u3002\u7136\u800c\uff0c\u53ef\u80fd\u5b58\u5728\u591a\u4e2a\u6700\u4f18\u9c81\u68d2\u7b56\u7565\uff0c\u8fd9\u4e9b\u7b56\u7565\u5728\u975e\u5bf9\u6297\u6027\u6982\u7387\u4e0b\u8868\u73b0\u4e0d\u540c\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u7cbe\u7ec6\u7684\u7b56\u7565\u9009\u62e9\u6807\u51c6\u3002", "method": "\u8bba\u6587\u63d0\u51fa\u4e86ORBE\u7b56\u7565\u7684\u6982\u5ff5\uff0c\u7ed3\u5408\u4e86\u535a\u5f08\u8bba\u4e2d\u7684\u4f18\u52bf\u548c\u6700\u4f73\u52aa\u529b\u601d\u60f3\uff0c\u8981\u6c42\u7b56\u7565\u4e0d\u4ec5\u5728\u5bf9\u6297\u6027\u6982\u7387\u4e0b\u6700\u4f18\uff0c\u8fd8\u5728\u975e\u5b8c\u5168\u5bf9\u6297\u6027\u6982\u7387\u4e0b\u8868\u73b0\u6700\u4f73\u3002\u4f5c\u8005\u8bc1\u660e\u4e86ORBE\u7b56\u7565\u7684\u5b58\u5728\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u8ba1\u7b97\u7b97\u6cd5\u3002", "result": "ORBE\u7b56\u7565\u80fd\u591f\u6709\u6548\u6253\u7834\u6700\u4f18\u9c81\u68d2\u7b56\u7565\u4e4b\u95f4\u7684\u5e73\u5c40\uff0c\u6570\u503c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u53ef\u884c\u6027\u3002", "conclusion": "ORBE\u7b56\u7565\u4e3aRMDPs\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u7cbe\u7ec6\u7684\u7b56\u7565\u9009\u62e9\u6807\u51c6\uff0c\u9002\u7528\u4e8e\u5b9e\u9645\u5e94\u7528\u4e2d\u9700\u8981\u6743\u8861\u5bf9\u6297\u6027\u548c\u975e\u5bf9\u6297\u6027\u573a\u666f\u7684\u60c5\u51b5\u3002"}}
{"id": "2508.07917", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07917", "abs": "https://arxiv.org/abs/2508.07917", "authors": ["Jason Lee", "Jiafei Duan", "Haoquan Fang", "Yuquan Deng", "Shuo Liu", "Boyang Li", "Bohan Fang", "Jieyu Zhang", "Yi Ru Wang", "Sangho Lee", "Winson Han", "Wilbert Pumacay", "Angelica Wu", "Rose Hendrix", "Karen Farley", "Eli VanderBilt", "Ali Farhadi", "Dieter Fox", "Ranjay Krishna"], "title": "MolmoAct: Action Reasoning Models that can Reason in Space", "comment": "Appendix on Blogpost: https://allenai.org/blog/molmoact", "summary": "Reasoning is central to purposeful action, yet most robotic foundation models\nmap perception and instructions directly to control, which limits adaptability,\ngeneralization, and semantic grounding. We introduce Action Reasoning Models\n(ARMs), a class of vision-language-action models that integrate perception,\nplanning, and control through a structured three-stage pipeline. Our model,\nMolmoAct, encodes observations and instructions into depth-aware perception\ntokens, generates mid-level spatial plans as editable trajectory traces, and\npredicts precise low-level actions, enabling explainable and steerable\nbehavior. MolmoAct-7B-D achieves strong performance across simulation and\nreal-world settings: 70.5% zero-shot accuracy on SimplerEnv Visual Matching\ntasks, surpassing closed-source Pi-0 and GR00T N1; 86.6% average success on\nLIBERO, including an additional 6.3% gain over ThinkAct on long-horizon tasks;\nand in real-world fine-tuning, an additional 10% (single-arm) and an additional\n22.7% (bimanual) task progression over Pi-0-FAST. It also outperforms baselines\nby an additional 23.3% on out-of-distribution generalization and achieves top\nhuman-preference scores for open-ended instruction following and trajectory\nsteering. Furthermore, we release, for the first time, the MolmoAct Dataset --\na mid-training robot dataset comprising over 10,000 high quality robot\ntrajectories across diverse scenarios and tasks. Training with this dataset\nyields an average 5.5% improvement in general performance over the base model.\nWe release all model weights, training code, our collected dataset, and our\naction reasoning dataset, establishing MolmoAct as both a state-of-the-art\nrobotics foundation model and an open blueprint for building ARMs that\ntransform perception into purposeful action through structured reasoning.\nBlogpost: https://allenai.org/blog/molmoact", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aARMs\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff0c\u901a\u8fc7\u4e09\u9636\u6bb5\u7ed3\u6784\u5316\u6d41\u7a0b\u6574\u5408\u611f\u77e5\u3001\u89c4\u5212\u4e0e\u63a7\u5236\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u7684\u9002\u5e94\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u57fa\u7840\u6a21\u578b\u76f4\u63a5\u5c06\u611f\u77e5\u548c\u6307\u4ee4\u6620\u5c04\u5230\u63a7\u5236\uff0c\u9650\u5236\u4e86\u9002\u5e94\u6027\u3001\u6cdb\u5316\u548c\u8bed\u4e49\u7406\u89e3\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u7ed3\u6784\u5316\u7684\u63a8\u7406\u65b9\u6cd5\u3002", "method": "\u63d0\u51faMolmoAct\u6a21\u578b\uff0c\u901a\u8fc7\u6df1\u5ea6\u611f\u77e5\u6807\u8bb0\u3001\u53ef\u7f16\u8f91\u8f68\u8ff9\u89c4\u5212\u548c\u7cbe\u786e\u52a8\u4f5c\u9884\u6d4b\u7684\u4e09\u9636\u6bb5\u6d41\u7a0b\u5b9e\u73b0\u53ef\u89e3\u91ca\u548c\u53ef\u64cd\u63a7\u7684\u884c\u4e3a\u3002", "result": "MolmoAct-7B-D\u5728\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u96f6\u6837\u672c\u51c6\u786e\u7387\u8fbe70.5%\uff0c\u957f\u65f6\u4efb\u52a1\u6210\u529f\u7387\u63d0\u53476.3%\uff0c\u5e76\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "MolmoAct\u4e0d\u4ec5\u662f\u4e00\u79cd\u5148\u8fdb\u7684\u673a\u5668\u4eba\u57fa\u7840\u6a21\u578b\uff0c\u8fd8\u63d0\u4f9b\u4e86\u6784\u5efaARMs\u7684\u5f00\u653e\u84dd\u56fe\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u63a8\u7406\u5c06\u611f\u77e5\u8f6c\u5316\u4e3a\u6709\u76ee\u7684\u7684\u884c\u4e3a\u3002"}}
{"id": "2508.07834", "categories": ["cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2508.07834", "abs": "https://arxiv.org/abs/2508.07834", "authors": ["Mubaris Nadeem", "Johannes Zenkert", "Lisa Bender", "Christian Weber", "Madjid Fathi"], "title": "KIRETT: Knowledge-Graph-Based Smart Treatment Assistant for Intelligent Rescue Operations", "comment": "LWDA'23, KIRETT project, University of Siegen, Germany", "summary": "Over the years, the need for rescue operations throughout the world has\nincreased rapidly. Demographic changes and the resulting risk of injury or\nhealth disorders form the basis for emergency calls. In such scenarios, first\nresponders are in a rush to reach the patient in need, provide first aid, and\nsave lives. In these situations, they must be able to provide personalized and\noptimized healthcare in the shortest possible time and estimate the patients\ncondition with the help of freshly recorded vital data in an emergency\nsituation. However, in such a timedependent situation, first responders and\nmedical experts cannot fully grasp their knowledge and need assistance and\nrecommendation for further medical treatments. To achieve this, on the spot\ncalculated, evaluated, and processed knowledge must be made available to\nimprove treatments by first responders. The Knowledge Graph presented in this\narticle as a central knowledge representation provides first responders with an\ninnovative knowledge management that enables intelligent treatment\nrecommendations with an artificial intelligence-based pre-recognition of the\nsituation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u548c\u4eba\u5de5\u667a\u80fd\u7684\u521b\u65b0\u77e5\u8bc6\u7ba1\u7406\u7cfb\u7edf\uff0c\u4ee5\u5e2e\u52a9\u6025\u6551\u4eba\u5458\u5728\u7d27\u6025\u60c5\u51b5\u4e0b\u63d0\u4f9b\u4e2a\u6027\u5316\u6cbb\u7597\u5efa\u8bae\u3002", "motivation": "\u5168\u7403\u6551\u63f4\u9700\u6c42\u589e\u52a0\uff0c\u6025\u6551\u4eba\u5458\u9700\u5feb\u901f\u8bc4\u4f30\u60a3\u8005\u72b6\u51b5\u5e76\u63d0\u4f9b\u4f18\u5316\u6cbb\u7597\uff0c\u4f46\u65f6\u95f4\u7d27\u8feb\u9650\u5236\u4e86\u5176\u77e5\u8bc6\u5e94\u7528\u3002", "method": "\u91c7\u7528\u77e5\u8bc6\u56fe\u8c31\u4f5c\u4e3a\u6838\u5fc3\u77e5\u8bc6\u8868\u793a\uff0c\u7ed3\u5408\u4eba\u5de5\u667a\u80fd\u9884\u8bc6\u522b\u60c5\u5883\uff0c\u4e3a\u6025\u6551\u4eba\u5458\u63d0\u4f9b\u667a\u80fd\u6cbb\u7597\u5efa\u8bae\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u5b9e\u65f6\u8ba1\u7b97\u3001\u8bc4\u4f30\u548c\u5904\u7406\u77e5\u8bc6\uff0c\u63d0\u5347\u6025\u6551\u4eba\u5458\u7684\u6cbb\u7597\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "conclusion": "\u77e5\u8bc6\u56fe\u8c31\u548c\u4eba\u5de5\u667a\u80fd\u7684\u7ed3\u5408\u4e3a\u6025\u6551\u573a\u666f\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u77e5\u8bc6\u7ba1\u7406\u89e3\u51b3\u65b9\u6848\uff0c\u4f18\u5316\u4e86\u7d27\u6025\u533b\u7597\u54cd\u5e94\u3002"}}
{"id": "2508.07945", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07945", "abs": "https://arxiv.org/abs/2508.07945", "authors": ["En Yen Puang", "Federico Ceola", "Giulia Pasquale", "Lorenzo Natale"], "title": "PCHands: PCA-based Hand Pose Synergy Representation on Manipulators with N-DoF", "comment": "2025 IEEE-RAS 24th International Conference on Humanoid Robots", "summary": "We consider the problem of learning a common representation for dexterous\nmanipulation across manipulators of different morphologies. To this end, we\npropose PCHands, a novel approach for extracting hand postural synergies from a\nlarge set of manipulators. We define a simplified and unified description\nformat based on anchor positions for manipulators ranging from 2-finger\ngrippers to 5-finger anthropomorphic hands. This enables learning a\nvariable-length latent representation of the manipulator configuration and the\nalignment of the end-effector frame of all manipulators. We show that it is\npossible to extract principal components from this latent representation that\nis universal across manipulators of different structures and degrees of\nfreedom. To evaluate PCHands, we use this compact representation to encode\nobservation and action spaces of control policies for dexterous manipulation\ntasks learned with RL. In terms of learning efficiency and consistency, the\nproposed representation outperforms a baseline that learns the same tasks in\njoint space. We additionally show that PCHands performs robustly in RL from\ndemonstration, when demonstrations are provided from a different manipulator.\nWe further support our results with real-world experiments that involve a\n2-finger gripper and a 4-finger anthropomorphic hand. Code and additional\nmaterial are available at https://hsp-iit.github.io/PCHands/.", "AI": {"tldr": "PCHands\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u8868\u793a\u65b9\u6cd5\uff0c\u7528\u4e8e\u4e0d\u540c\u5f62\u6001\u673a\u68b0\u624b\u7684\u7075\u5de7\u64cd\u4f5c\u5b66\u4e60\uff0c\u57fa\u4e8e\u951a\u70b9\u4f4d\u7f6e\u7edf\u4e00\u63cf\u8ff0\u683c\u5f0f\uff0c\u63d0\u53d6\u4e3b\u6210\u5206\uff0c\u5e76\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u9a8c\u8bc1\u5176\u9ad8\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u89e3\u51b3\u4e0d\u540c\u5f62\u6001\u673a\u68b0\u624b\u5728\u7075\u5de7\u64cd\u4f5c\u4e2d\u901a\u7528\u8868\u793a\u7684\u5b66\u4e60\u95ee\u9898\u3002", "method": "\u63d0\u51faPCHands\u65b9\u6cd5\uff0c\u57fa\u4e8e\u951a\u70b9\u4f4d\u7f6e\u7edf\u4e00\u63cf\u8ff0\u683c\u5f0f\uff0c\u63d0\u53d6\u4e3b\u6210\u5206\u4f5c\u4e3a\u901a\u7528\u8868\u793a\uff0c\u5e76\u7528\u4e8e\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u7684\u89c2\u6d4b\u548c\u52a8\u4f5c\u7a7a\u95f4\u7f16\u7801\u3002", "result": "PCHands\u5728\u5b66\u4e60\u548c\u4e00\u81f4\u6027\u4e0a\u4f18\u4e8e\u5173\u8282\u7a7a\u95f4\u57fa\u7ebf\uff0c\u4e14\u5728\u6f14\u793a\u5b66\u4e60\u4e2d\u4e5f\u8868\u73b0\u9c81\u68d2\u3002", "conclusion": "PCHands\u4e3a\u4e0d\u540c\u5f62\u6001\u673a\u68b0\u624b\u7684\u7075\u5de7\u64cd\u4f5c\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u901a\u7528\u7684\u8868\u793a\u65b9\u6cd5\u3002"}}
{"id": "2508.07932", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07932", "abs": "https://arxiv.org/abs/2508.07932", "authors": ["Yi Zhai", "Zhiqiang Wei", "Ruohan Li", "Keyu Pan", "Shuo Liu", "Lu Zhang", "Jianmin Ji", "Wuyang Zhang", "Yu Zhang", "Yanyong Zhang"], "title": "\\(X\\)-evolve: Solution space evolution powered by large language models", "comment": null, "summary": "While combining large language models (LLMs) with evolutionary algorithms\n(EAs) shows promise for solving complex optimization problems, current\napproaches typically evolve individual solutions, often incurring high LLM call\ncosts. We introduce \\(X\\)-evolve, a paradigm-shifting method that instead\nevolves solution spaces \\(X\\) (sets of individual solutions) - subsets of the\noverall search space \\(S\\). In \\(X\\)-evolve, LLMs generate tunable programs\nwherein certain code snippets, designated as parameters, define a tunable\nsolution space. A score-based search algorithm then efficiently explores this\nparametrically defined space, guided by feedback from objective function\nscores. This strategy enables broader and more efficient exploration, which can\npotentially accelerate convergence at a much lower search cost, requiring up to\ntwo orders of magnitude fewer LLM calls than prior leading methods. We\ndemonstrate \\(X\\)-evolve's efficacy across three distinct hard optimization\nproblems. For the cap set problem, we discover a larger partial admissible set,\nestablishing a new tighter asymptotic lower bound for the cap set constant (\\(C\n\\ge 2.2203\\)). In information theory, we uncover a larger independent set for\nthe 15-vertex cycle graph (\\(\\mathcal{C}_{15}^{\\boxtimes 5}\\), size 19,946),\nthereby raising the known lower bound on its Shannon capacity. Furthermore, for\nthe NP-hard online bin packing problem, we generate heuristics that\nconsistently outperform standard strategies across established benchmarks. By\nevolving solution spaces, our method considerably improves search\neffectiveness, making it possible to tackle high-dimensional problems that were\npreviously computationally prohibitive.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faX-evolve\u65b9\u6cd5\uff0c\u901a\u8fc7\u6f14\u5316\u89e3\u7a7a\u95f4\u800c\u975e\u5355\u4e2a\u89e3\uff0c\u663e\u8457\u51cf\u5c11LLM\u8c03\u7528\u6210\u672c\uff0c\u5e76\u5728\u591a\u4e2a\u4f18\u5316\u95ee\u9898\u4e2d\u53d6\u5f97\u7a81\u7834\u6027\u6210\u679c\u3002", "motivation": "\u5f53\u524d\u7ed3\u5408LLM\u4e0eEA\u7684\u65b9\u6cd5\u901a\u5e38\u6f14\u5316\u5355\u4e2a\u89e3\uff0c\u5bfc\u81f4LLM\u8c03\u7528\u6210\u672c\u9ad8\uff0c\u9650\u5236\u4e86\u89e3\u51b3\u590d\u6742\u4f18\u5316\u95ee\u9898\u7684\u6548\u7387\u3002", "method": "X-evolve\u901a\u8fc7LLM\u751f\u6210\u53ef\u8c03\u7a0b\u5e8f\uff0c\u5b9a\u4e49\u53c2\u6570\u5316\u89e3\u7a7a\u95f4\uff0c\u5e76\u5229\u7528\u57fa\u4e8e\u8bc4\u5206\u7684\u641c\u7d22\u7b97\u6cd5\u9ad8\u6548\u63a2\u7d22\uff0c\u51cf\u5c11LLM\u8c03\u7528\u3002", "result": "\u5728\u4e09\u4e2a\u4f18\u5316\u95ee\u9898\u4e2d\u53d6\u5f97\u663e\u8457\u6210\u679c\uff1acap set\u95ee\u9898\u4e2d\u63d0\u5347\u4e0b\u754c\uff1b\u4fe1\u606f\u8bba\u4e2d\u53d1\u73b0\u66f4\u5927\u72ec\u7acb\u96c6\uff1b\u5728\u7ebf\u88c5\u7bb1\u95ee\u9898\u4e2d\u751f\u6210\u66f4\u4f18\u542f\u53d1\u5f0f\u7b56\u7565\u3002", "conclusion": "X-evolve\u901a\u8fc7\u89e3\u7a7a\u95f4\u6f14\u5316\u663e\u8457\u63d0\u5347\u641c\u7d22\u6548\u7387\uff0c\u89e3\u51b3\u4e86\u4ee5\u5f80\u8ba1\u7b97\u4e0d\u53ef\u884c\u7684\u9ad8\u7ef4\u95ee\u9898\u3002"}}
{"id": "2508.08046", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.08046", "abs": "https://arxiv.org/abs/2508.08046", "authors": ["Fen Liu", "Shenghai Yuan", "Thien-Minh Nguyen", "Wei Meng", "Lihua Xie"], "title": "Aerial Target Encirclement and Interception with Noisy Range Observations", "comment": "The paper has been accepted in Automatica", "summary": "This paper proposes a strategy to encircle and intercept a non-cooperative\naerial point-mass moving target by leveraging noisy range measurements for\nstate estimation. In this approach, the guardians actively ensure the\nobservability of the target by using an anti-synchronization (AS), 3D\n``vibrating string\" trajectory, which enables rapid position and velocity\nestimation based on the Kalman filter. Additionally, a novel anti-target\ncontroller is designed for the guardians to enable adaptive transitions from\nencircling a protected target to encircling, intercepting, and neutralizing a\nhostile target, taking into consideration the input constraints of the\nguardians. Based on the guaranteed uniform observability, the exponentially\nbounded stability of the state estimation error and the convergence of the\nencirclement error are rigorously analyzed. Simulation results and real-world\nUAV experiments are presented to further validate the effectiveness of the\nsystem design.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5229\u7528\u566a\u58f0\u8ddd\u79bb\u6d4b\u91cf\u8fdb\u884c\u72b6\u6001\u4f30\u8ba1\u7684\u7b56\u7565\uff0c\u4ee5\u5305\u56f4\u548c\u62e6\u622a\u975e\u5408\u4f5c\u7a7a\u4e2d\u76ee\u6807\u3002", "motivation": "\u9488\u5bf9\u975e\u5408\u4f5c\u76ee\u6807\u7684\u5feb\u901f\u72b6\u6001\u4f30\u8ba1\u548c\u62e6\u622a\u9700\u6c42\uff0c\u8bbe\u8ba1\u4e00\u79cd\u4e3b\u52a8\u786e\u4fdd\u76ee\u6807\u53ef\u89c2\u6d4b\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u53cd\u540c\u6b65\uff08AS\uff093D\u201c\u632f\u52a8\u5f26\u201d\u8f68\u8ff9\u548c\u5361\u5c14\u66fc\u6ee4\u6ce2\u8fdb\u884c\u72b6\u6001\u4f30\u8ba1\uff0c\u8bbe\u8ba1\u65b0\u578b\u53cd\u76ee\u6807\u63a7\u5236\u5668\u5b9e\u73b0\u81ea\u9002\u5e94\u8fc7\u6e21\u3002", "result": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8bc1\u660e\u4e86\u72b6\u6001\u4f30\u8ba1\u8bef\u5dee\u7684\u6307\u6570\u6709\u754c\u7a33\u5b9a\u6027\u548c\u5305\u56f4\u8bef\u5dee\u7684\u6536\u655b\u6027\u3002", "conclusion": "\u7cfb\u7edf\u8bbe\u8ba1\u5728\u4eff\u771f\u548c\u5b9e\u9645\u65e0\u4eba\u673a\u5b9e\u9a8c\u4e2d\u5747\u8868\u73b0\u51fa\u6709\u6548\u6027\u3002"}}
{"id": "2508.07941", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07941", "abs": "https://arxiv.org/abs/2508.07941", "authors": ["Olivier Poulet", "Fr\u00e9d\u00e9ric Guinand", "Fran\u00e7ois Gu\u00e9rin"], "title": "Deep Reinforcement Learning with anticipatory reward in LSTM for Collision Avoidance of Mobile Robots", "comment": null, "summary": "This article proposes a collision risk anticipation method based on\nshort-term prediction of the agents position. A Long Short-Term Memory (LSTM)\nmodel, trained on past trajectories, is used to estimate the next position of\neach robot. This prediction allows us to define an anticipated collision risk\nby dynamically modulating the reward of a Deep Q-Learning Network (DQN) agent.\nThe approach is tested in a constrained environment, where two robots move\nwithout communication or identifiers. Despite a limited sampling frequency (1\nHz), the results show a significant decrease of the collisions number and a\nstability improvement. The proposed method, which is computationally\ninexpensive, appears particularly attractive for implementation on embedded\nsystems.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u77ed\u671f\u9884\u6d4b\u7684\u78b0\u649e\u98ce\u9669\u9884\u6d4b\u65b9\u6cd5\uff0c\u4f7f\u7528LSTM\u6a21\u578b\u9884\u6d4b\u673a\u5668\u4eba\u4f4d\u7f6e\uff0c\u5e76\u901a\u8fc7\u52a8\u6001\u8c03\u6574DQN\u5956\u52b1\u6765\u51cf\u5c11\u78b0\u649e\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u5728\u65e0\u901a\u4fe1\u6216\u6807\u8bc6\u7684\u53d7\u9650\u73af\u5883\u4e2d\u79fb\u52a8\u65f6\u7684\u78b0\u649e\u95ee\u9898\u3002", "method": "\u4f7f\u7528LSTM\u6a21\u578b\u9884\u6d4b\u673a\u5668\u4eba\u4f4d\u7f6e\uff0c\u7ed3\u5408DQN\u52a8\u6001\u8c03\u6574\u5956\u52b1\u4ee5\u51cf\u5c11\u78b0\u649e\u3002", "result": "\u57281Hz\u91c7\u6837\u9891\u7387\u4e0b\uff0c\u78b0\u649e\u6b21\u6570\u663e\u8457\u51cf\u5c11\uff0c\u7a33\u5b9a\u6027\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u4f4e\uff0c\u9002\u5408\u5d4c\u5165\u5f0f\u7cfb\u7edf\u5b9e\u73b0\u3002"}}
{"id": "2508.08108", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.08108", "abs": "https://arxiv.org/abs/2508.08108", "authors": ["Wei Zhang", "Yinchuan Wang", "Wangtao Lu", "Pengyu Zhang", "Xiang Zhang", "Yue Wang", "Chaoqun Wang"], "title": "Capsizing-Guided Trajectory Optimization for Autonomous Navigation with Rough Terrain", "comment": null, "summary": "It is a challenging task for ground robots to autonomously navigate in harsh\nenvironments due to the presence of non-trivial obstacles and uneven terrain.\nThis requires trajectory planning that balances safety and efficiency. The\nprimary challenge is to generate a feasible trajectory that prevents robot from\ntip-over while ensuring effective navigation. In this paper, we propose a\ncapsizing-aware trajectory planner (CAP) to achieve trajectory planning on the\nuneven terrain. The tip-over stability of the robot on rough terrain is\nanalyzed. Based on the tip-over stability, we define the traversable\norientation, which indicates the safe range of robot orientations. This\norientation is then incorporated into a capsizing-safety constraint for\ntrajectory optimization. We employ a graph-based solver to compute a robust and\nfeasible trajectory while adhering to the capsizing-safety constraint.\nExtensive simulation and real-world experiments validate the effectiveness and\nrobustness of the proposed method. The results demonstrate that CAP outperforms\nexisting state-of-the-art approaches, providing enhanced navigation performance\non uneven terrains.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9632\u503e\u8986\u7684\u8f68\u8ff9\u89c4\u5212\u65b9\u6cd5\uff08CAP\uff09\uff0c\u7528\u4e8e\u5730\u9762\u673a\u5668\u4eba\u5728\u590d\u6742\u5730\u5f62\u4e2d\u7684\u81ea\u4e3b\u5bfc\u822a\uff0c\u7ed3\u5408\u7a33\u5b9a\u6027\u5206\u6790\u548c\u8f68\u8ff9\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bfc\u822a\u6027\u80fd\u3002", "motivation": "\u5730\u9762\u673a\u5668\u4eba\u5728\u590d\u6742\u5730\u5f62\u4e2d\u5bfc\u822a\u65f6\u5bb9\u6613\u503e\u8986\uff0c\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u5e73\u8861\u5b89\u5168\u6027\u4e0e\u6548\u7387\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u8f68\u8ff9\u89c4\u5212\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5206\u6790\u673a\u5668\u4eba\u5728\u4e0d\u5e73\u5730\u5f62\u7684\u503e\u8986\u7a33\u5b9a\u6027\uff0c\u5b9a\u4e49\u53ef\u901a\u884c\u65b9\u5411\uff0c\u5e76\u5c06\u5176\u4f5c\u4e3a\u7ea6\u675f\u6761\u4ef6\u878d\u5165\u8f68\u8ff9\u4f18\u5316\u4e2d\uff0c\u4f7f\u7528\u56fe\u6c42\u89e3\u5668\u751f\u6210\u5b89\u5168\u4e14\u9ad8\u6548\u7684\u8f68\u8ff9\u3002", "result": "\u4eff\u771f\u548c\u5b9e\u9645\u5b9e\u9a8c\u8868\u660e\uff0cCAP\u65b9\u6cd5\u5728\u590d\u6742\u5730\u5f62\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u63d0\u4f9b\u4e86\u66f4\u9ad8\u7684\u5bfc\u822a\u6548\u7387\u548c\u5b89\u5168\u6027\u3002", "conclusion": "CAP\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u503e\u8986\u7a33\u5b9a\u6027\u5206\u6790\u548c\u8f68\u8ff9\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5730\u9762\u673a\u5668\u4eba\u5728\u590d\u6742\u5730\u5f62\u4e2d\u7684\u5bfc\u822a\u80fd\u529b\u3002"}}
{"id": "2508.07950", "categories": ["cs.AI", "cs.CV", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.07950", "abs": "https://arxiv.org/abs/2508.07950", "authors": ["Chen Shen", "Wanqing Zhang", "Kehan Li", "Erwen Huang", "Haitao Bi", "Aiying Fan", "Yiwen Shen", "Hongmei Dong", "Ji Zhang", "Yuming Shao", "Zengjia Liu", "Xinshe Liu", "Tao Li", "Chunxia Yan", "Shuanliang Fan", "Di Wu", "Jianhua Ma", "Bin Cong", "Zhenyuan Wang", "Chunfeng Lian"], "title": "FEAT: A Multi-Agent Forensic AI System with Domain-Adapted Large Language Model for Automated Cause-of-Death Analysis", "comment": "18pages, 6 figures", "summary": "Forensic cause-of-death determination faces systemic challenges, including\nworkforce shortages and diagnostic variability, particularly in high-volume\nsystems like China's medicolegal infrastructure. We introduce FEAT (ForEnsic\nAgenT), a multi-agent AI framework that automates and standardizes death\ninvestigations through a domain-adapted large language model. FEAT's\napplication-oriented architecture integrates: (i) a central Planner for task\ndecomposition, (ii) specialized Local Solvers for evidence analysis, (iii) a\nMemory & Reflection module for iterative refinement, and (iv) a Global Solver\nfor conclusion synthesis. The system employs tool-augmented reasoning,\nhierarchical retrieval-augmented generation, forensic-tuned LLMs, and\nhuman-in-the-loop feedback to ensure legal and medical validity. In evaluations\nacross diverse Chinese case cohorts, FEAT outperformed state-of-the-art AI\nsystems in both long-form autopsy analyses and concise cause-of-death\nconclusions. It demonstrated robust generalization across six geographic\nregions and achieved high expert concordance in blinded validations. Senior\npathologists validated FEAT's outputs as comparable to those of human experts,\nwith improved detection of subtle evidentiary nuances. To our knowledge, FEAT\nis the first LLM-based AI agent system dedicated to forensic medicine, offering\nscalable, consistent death certification while maintaining expert-level rigor.\nBy integrating AI efficiency with human oversight, this work could advance\nequitable access to reliable medicolegal services while addressing critical\ncapacity constraints in forensic systems.", "AI": {"tldr": "FEAT\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53AI\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u6cd5\u533b\u6b7b\u56e0\u9274\u5b9a\uff0c\u901a\u8fc7\u9886\u57df\u9002\u5e94\u7684\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u5347\u6548\u7387\u548c\u4e00\u81f4\u6027\u3002", "motivation": "\u89e3\u51b3\u6cd5\u533b\u6b7b\u56e0\u9274\u5b9a\u4e2d\u7684\u52b3\u52a8\u529b\u77ed\u7f3a\u548c\u8bca\u65ad\u5dee\u5f02\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u9ad8\u8d1f\u8377\u7cfb\u7edf\u5982\u4e2d\u56fd\u7684\u6cd5\u533b\u4f53\u7cfb\u4e2d\u3002", "method": "FEAT\u7ed3\u5408\u4efb\u52a1\u5206\u89e3\u3001\u8bc1\u636e\u5206\u6790\u3001\u8fed\u4ee3\u4f18\u5316\u548c\u7ed3\u8bba\u5408\u6210\u7684\u591a\u6a21\u5757\u67b6\u6784\uff0c\u91c7\u7528\u5de5\u5177\u589e\u5f3a\u63a8\u7406\u548c\u5c42\u6b21\u5316\u68c0\u7d22\u751f\u6210\u6280\u672f\u3002", "result": "FEAT\u5728\u591a\u6837\u5316\u7684\u4e2d\u56fd\u6848\u4f8b\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709AI\u7cfb\u7edf\uff0c\u5177\u6709\u5f3a\u6cdb\u5316\u80fd\u529b\u548c\u9ad8\u4e13\u5bb6\u4e00\u81f4\u6027\u3002", "conclusion": "FEAT\u662f\u9996\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6cd5\u533bAI\u7cfb\u7edf\uff0c\u7ed3\u5408AI\u6548\u7387\u548c\u4eba\u7c7b\u76d1\u7763\uff0c\u63d0\u5347\u6cd5\u533b\u670d\u52a1\u7684\u53ef\u53ca\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2508.08113", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.08113", "abs": "https://arxiv.org/abs/2508.08113", "authors": ["Yinpei Dai", "Jayjun Lee", "Yichi Zhang", "Ziqiao Ma", "Jed Yang", "Amir Zadeh", "Chuan Li", "Nima Fazeli", "Joyce Chai"], "title": "AimBot: A Simple Auxiliary Visual Cue to Enhance Spatial Awareness of Visuomotor Policies", "comment": "CoRL 2025", "summary": "In this paper, we propose AimBot, a lightweight visual augmentation technique\nthat provides explicit spatial cues to improve visuomotor policy learning in\nrobotic manipulation. AimBot overlays shooting lines and scope reticles onto\nmulti-view RGB images, offering auxiliary visual guidance that encodes the\nend-effector's state. The overlays are computed from depth images, camera\nextrinsics, and the current end-effector pose, explicitly conveying spatial\nrelationships between the gripper and objects in the scene. AimBot incurs\nminimal computational overhead (less than 1 ms) and requires no changes to\nmodel architectures, as it simply replaces original RGB images with augmented\ncounterparts. Despite its simplicity, our results show that AimBot consistently\nimproves the performance of various visuomotor policies in both simulation and\nreal-world settings, highlighting the benefits of spatially grounded visual\nfeedback.", "AI": {"tldr": "AimBot\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u89c6\u89c9\u589e\u5f3a\u6280\u672f\uff0c\u901a\u8fc7\u7a7a\u95f4\u63d0\u793a\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u5b66\u4e60\u3002", "motivation": "\u6539\u5584\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u5b66\u4e60\u4e2d\u7684\u7a7a\u95f4\u611f\u77e5\uff0c\u63d0\u4f9b\u66f4\u76f4\u89c2\u7684\u89c6\u89c9\u53cd\u9988\u3002", "method": "\u5728\u591a\u89c6\u89d2RGB\u56fe\u50cf\u4e0a\u53e0\u52a0\u5c04\u51fb\u7ebf\u548c\u7784\u51c6\u955c\u6807\u8bb0\uff0c\u5229\u7528\u6df1\u5ea6\u56fe\u50cf\u3001\u76f8\u673a\u5916\u53c2\u548c\u672b\u7aef\u6267\u884c\u5668\u59ff\u6001\u8ba1\u7b97\u8986\u76d6\u5c42\u3002", "result": "AimBot\u663e\u8457\u63d0\u5347\u4e86\u591a\u79cd\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u5728\u4eff\u771f\u548c\u73b0\u5b9e\u73af\u5883\u4e2d\u7684\u6027\u80fd\uff0c\u8ba1\u7b97\u5f00\u9500\u6781\u4f4e\uff08<1ms\uff09\u3002", "conclusion": "AimBot\u901a\u8fc7\u7a7a\u95f4\u89c6\u89c9\u53cd\u9988\u6709\u6548\u63d0\u5347\u673a\u5668\u4eba\u64cd\u4f5c\u6027\u80fd\uff0c\u4e14\u65e0\u9700\u4fee\u6539\u6a21\u578b\u67b6\u6784\u3002"}}
{"id": "2508.08001", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08001", "abs": "https://arxiv.org/abs/2508.08001", "authors": ["Rui Yao", "Qi Chai", "Jinhai Yao", "Siyuan Li", "Junhao Chen", "Qi Zhang", "Hao Wang"], "title": "Interpreting Fedspeak with Confidence: A LLM-Based Uncertainty-Aware Framework Guided by Monetary Policy Transmission Paths", "comment": "Rui Yao, Qi Chai, and Jinhai Yao contributed equally to this work.\n  Corresponding authors: Qi Zhang (zhang.qi@sjtu.edu.cn) and Hao Wang\n  (haowang@hkust-gz.edu.cn)", "summary": "\"Fedspeak\", the stylized and often nuanced language used by the U.S. Federal\nReserve, encodes implicit policy signals and strategic stances. The Federal\nOpen Market Committee strategically employs Fedspeak as a communication tool to\nshape market expectations and influence both domestic and global economic\nconditions. As such, automatically parsing and interpreting Fedspeak presents a\nhigh-impact challenge, with significant implications for financial forecasting,\nalgorithmic trading, and data-driven policy analysis. In this paper, we propose\nan LLM-based, uncertainty-aware framework for deciphering Fedspeak and\nclassifying its underlying monetary policy stance. Technically, to enrich the\nsemantic and contextual representation of Fedspeak texts, we incorporate\ndomain-specific reasoning grounded in the monetary policy transmission\nmechanism. We further introduce a dynamic uncertainty decoding module to assess\nthe confidence of model predictions, thereby enhancing both classification\naccuracy and model reliability. Experimental results demonstrate that our\nframework achieves state-of-the-art performance on the policy stance analysis\ntask. Moreover, statistical analysis reveals a significant positive correlation\nbetween perceptual uncertainty and model error rates, validating the\neffectiveness of perceptual uncertainty as a diagnostic signal.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u3001\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u6790\u7f8e\u8054\u50a8\u7684\u2018Fedspeak\u2019\u8bed\u8a00\u5e76\u5206\u7c7b\u5176\u8d27\u5e01\u653f\u7b56\u7acb\u573a\uff0c\u7ed3\u5408\u9886\u57df\u7279\u5b9a\u63a8\u7406\u548c\u52a8\u6001\u4e0d\u786e\u5b9a\u6027\u89e3\u7801\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u7c7b\u51c6\u786e\u6027\u548c\u6a21\u578b\u53ef\u9760\u6027\u3002", "motivation": "\u7f8e\u8054\u50a8\u901a\u8fc7\u2018Fedspeak\u2019\u8bed\u8a00\u4f20\u9012\u9690\u542b\u653f\u7b56\u4fe1\u53f7\uff0c\u5bf9\u5e02\u573a\u9884\u671f\u548c\u7ecf\u6d4e\u6761\u4ef6\u6709\u91cd\u8981\u5f71\u54cd\u3002\u81ea\u52a8\u89e3\u6790\u2018Fedspeak\u2019\u5bf9\u91d1\u878d\u9884\u6d4b\u3001\u7b97\u6cd5\u4ea4\u6613\u548c\u653f\u7b56\u5206\u6790\u5177\u6709\u91cd\u5927\u610f\u4e49\u3002", "method": "\u63d0\u51faLLM\u6846\u67b6\uff0c\u7ed3\u5408\u8d27\u5e01\u653f\u7b56\u4f20\u5bfc\u673a\u5236\u7684\u9886\u57df\u7279\u5b9a\u63a8\u7406\uff0c\u5e76\u5f15\u5165\u52a8\u6001\u4e0d\u786e\u5b9a\u6027\u89e3\u7801\u6a21\u5757\u4ee5\u8bc4\u4f30\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u6846\u67b6\u5728\u653f\u7b56\u7acb\u573a\u5206\u6790\u4efb\u52a1\u4e0a\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u4e14\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\u4e0e\u6a21\u578b\u9519\u8bef\u7387\u663e\u8457\u6b63\u76f8\u5173\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86\u2018Fedspeak\u2019\u89e3\u6790\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\uff0c\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\u53ef\u4f5c\u4e3a\u8bca\u65ad\u4fe1\u53f7\u3002"}}
{"id": "2508.08144", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.08144", "abs": "https://arxiv.org/abs/2508.08144", "authors": ["Ganesh Sundaram", "Jonas Ulmen", "Amjad Haider", "Daniel G\u00f6rges"], "title": "COMponent-Aware Pruning for Accelerated Control Tasks in Latent Space Models", "comment": "Submitted in: The 2026 IEEE/SICE International Symposium on System\n  Integration (SII 2026)", "summary": "The rapid growth of resource-constrained mobile platforms, including mobile\nrobots, wearable systems, and Internet-of-Things devices, has increased the\ndemand for computationally efficient neural network controllers (NNCs) that can\noperate within strict hardware limitations. While deep neural networks (DNNs)\ndemonstrate superior performance in control applications, their substantial\ncomputational complexity and memory requirements present significant barriers\nto practical deployment on edge devices. This paper introduces a comprehensive\nmodel compression methodology that leverages component-aware structured pruning\nto determine the optimal pruning magnitude for each pruning group, ensuring a\nbalance between compression and stability for NNC deployment. Our approach is\nrigorously evaluated on Temporal Difference Model Predictive Control (TD-MPC),\na state-of-the-art model-based reinforcement learning algorithm, with a\nsystematic integration of mathematical stability guarantee properties,\nspecifically Lyapunov criteria. The key contribution of this work lies in\nproviding a principled framework for determining the theoretical limits of\nmodel compression while preserving controller stability. Experimental\nvalidation demonstrates that our methodology successfully reduces model\ncomplexity while maintaining requisite control performance and stability\ncharacteristics. Furthermore, our approach establishes a quantitative boundary\nfor safe compression ratios, enabling practitioners to systematically determine\nthe maximum permissible model reduction before violating critical stability\nproperties, thereby facilitating the confident deployment of compressed NNCs in\nresource-limited environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ec4\u4ef6\u611f\u77e5\u7ed3\u6784\u5316\u526a\u679d\u7684\u6a21\u578b\u538b\u7f29\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u8d44\u6e90\u53d7\u9650\u7684\u79fb\u52a8\u5e73\u53f0\u4e0a\u90e8\u7f72\u795e\u7ecf\u7f51\u7edc\u63a7\u5236\u5668\uff08NNCs\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u63a7\u5236\u6027\u80fd\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u968f\u7740\u79fb\u52a8\u673a\u5668\u4eba\u3001\u53ef\u7a7f\u6234\u8bbe\u5907\u548c\u7269\u8054\u7f51\u8bbe\u5907\u7b49\u8d44\u6e90\u53d7\u9650\u5e73\u53f0\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u5bf9\u8ba1\u7b97\u9ad8\u6548\u7684\u795e\u7ecf\u7f51\u7edc\u63a7\u5236\u5668\u7684\u9700\u6c42\u589e\u52a0\uff0c\u4f46\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u9ad8\u8ba1\u7b97\u590d\u6742\u6027\u548c\u5185\u5b58\u9700\u6c42\u9650\u5236\u4e86\u5176\u5b9e\u9645\u90e8\u7f72\u3002", "method": "\u91c7\u7528\u7ec4\u4ef6\u611f\u77e5\u7ed3\u6784\u5316\u526a\u679d\u65b9\u6cd5\uff0c\u7ed3\u5408\u6570\u5b66\u7a33\u5b9a\u6027\u4fdd\u8bc1\uff08\u5982Lyapunov\u51c6\u5219\uff09\uff0c\u5728\u65f6\u95f4\u5dee\u5206\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08TD-MPC\uff09\u7b97\u6cd5\u4e0a\u8fdb\u884c\u7cfb\u7edf\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u663e\u8457\u964d\u4f4e\u6a21\u578b\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u63a7\u5236\u6027\u80fd\u548c\u7a33\u5b9a\u6027\uff0c\u5e76\u786e\u5b9a\u4e86\u5b89\u5168\u538b\u7f29\u6bd4\u7684\u7406\u8bba\u754c\u9650\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6a21\u578b\u538b\u7f29\u63d0\u4f9b\u4e86\u7406\u8bba\u754c\u9650\uff0c\u786e\u4fdd\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u5b89\u5168\u90e8\u7f72\u538b\u7f29\u540e\u7684\u795e\u7ecf\u7f51\u7edc\u63a7\u5236\u5668\u3002"}}
{"id": "2508.08007", "categories": ["cs.AI", "Computing methodologies~Description logics, Computing\n  methodologies~Ontology engineering"], "pdf": "https://arxiv.org/pdf/2508.08007", "abs": "https://arxiv.org/abs/2508.08007", "authors": ["Maurice Funk", "Marvin Grosser", "Carsten Lutz"], "title": "Fitting Description Logic Ontologies to ABox and Query Examples", "comment": "Submitted to the 22nd International Conference on Principles of\n  Knowledge Representation and Reasoning (KR2025), 23 pages", "summary": "We study a fitting problem inspired by ontology-mediated querying: given a\ncollection\n  of positive and negative examples of\n  the form $(\\mathcal{A},q)$ with\n  $\\mathcal{A}$ an ABox and $q$ a Boolean query, we seek\n  an ontology $\\mathcal{O}$ that satisfies $\\mathcal{A} \\cup \\mathcal{O} \\vDash\nq$ for all positive examples and $\\mathcal{A} \\cup \\mathcal{O}\\not\\vDash q$ for\nall negative examples.\n  We consider the description logics $\\mathcal{ALC}$ and $\\mathcal{ALCI}$ as\nontology languages and\n  a range of query languages that\n  includes atomic queries (AQs), conjunctive queries (CQs), and unions thereof\n(UCQs).\n  For all of the resulting fitting problems,\n  we provide\n  effective characterizations and determine the computational complexity\n  of deciding whether a fitting ontology exists. This problem turns out to be\n${\\small CO}NP$ for AQs and full CQs\n  and $2E{\\small XP}T{\\small IME}$-complete for CQs and UCQs.\n  These results hold for both $\\mathcal{ALC}$ and $\\mathcal{ALCI}$.", "AI": {"tldr": "\u7814\u7a76\u57fa\u4e8e\u672c\u4f53\u4ecb\u5bfc\u67e5\u8be2\u7684\u62df\u5408\u95ee\u9898\uff0c\u786e\u5b9a\u662f\u5426\u5b58\u5728\u6ee1\u8db3\u6761\u4ef6\u7684\u672c\u4f53\uff0c\u5e76\u5206\u6790\u5176\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "motivation": "\u89e3\u51b3\u672c\u4f53\u4ecb\u5bfc\u67e5\u8be2\u4e2d\u6b63\u8d1f\u4f8b\u62df\u5408\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u3002", "method": "\u4f7f\u7528\u63cf\u8ff0\u903b\u8f91ALC\u548cALCI\u4f5c\u4e3a\u672c\u4f53\u8bed\u8a00\uff0c\u7ed3\u5408\u591a\u79cd\u67e5\u8be2\u8bed\u8a00\uff08AQs\u3001CQs\u3001UCQs\uff09\uff0c\u5206\u6790\u62df\u5408\u95ee\u9898\u7684\u6709\u6548\u6027\u548c\u590d\u6742\u5ea6\u3002", "result": "AQs\u548c\u5b8c\u6574CQs\u7684\u62df\u5408\u95ee\u9898\u4e3aCONP\u590d\u6742\u5ea6\uff0cCQs\u548cUCQs\u4e3a2EXPTIME\u5b8c\u5168\u3002", "conclusion": "\u7814\u7a76\u4e3a\u4e0d\u540c\u67e5\u8be2\u8bed\u8a00\u4e0b\u7684\u672c\u4f53\u62df\u5408\u95ee\u9898\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\uff0c\u63ed\u793a\u4e86\u5176\u8ba1\u7b97\u590d\u6742\u5ea6\u3002"}}
{"id": "2508.08226", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.08226", "abs": "https://arxiv.org/abs/2508.08226", "authors": ["Haiyue Chen", "Aniket Datar", "Tong Xu", "Francesco Cancelliere", "Harsh Rangwala", "Madhan Balaji Rao", "Daeun Song", "David Eichinger", "Xuesu Xiao"], "title": "Verti-Arena: A Controllable and Standardized Indoor Testbed for Multi-Terrain Off-Road Autonomy", "comment": "6 pages", "summary": "Off-road navigation is an important capability for mobile robots deployed in\nenvironments that are inaccessible or dangerous to humans, such as disaster\nresponse or planetary exploration. Progress is limited due to the lack of a\ncontrollable and standardized real-world testbed for systematic data collection\nand validation. To fill this gap, we introduce Verti-Arena, a reconfigurable\nindoor facility designed specifically for off-road autonomy. By providing a\nrepeatable benchmark environment, Verti-Arena supports reproducible experiments\nacross a variety of vertically challenging terrains and provides precise ground\ntruth measurements through onboard sensors and a motion capture system.\nVerti-Arena also supports consistent data collection and comparative evaluation\nof algorithms in off-road autonomy research. We also develop a web-based\ninterface that enables research groups worldwide to remotely conduct\nstandardized off-road autonomy experiments on Verti-Arena.", "AI": {"tldr": "Verti-Arena\u662f\u4e00\u4e2a\u53ef\u91cd\u6784\u7684\u5ba4\u5185\u8bbe\u65bd\uff0c\u65e8\u5728\u4e3a\u8d8a\u91ce\u81ea\u4e3b\u6027\u7814\u7a76\u63d0\u4f9b\u6807\u51c6\u5316\u6d4b\u8bd5\u73af\u5883\uff0c\u652f\u6301\u53ef\u91cd\u590d\u5b9e\u9a8c\u548c\u8fdc\u7a0b\u534f\u4f5c\u3002", "motivation": "\u8d8a\u91ce\u5bfc\u822a\u5bf9\u79fb\u52a8\u673a\u5668\u4eba\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7f3a\u4e4f\u53ef\u63a7\u4e14\u6807\u51c6\u5316\u7684\u771f\u5b9e\u6d4b\u8bd5\u73af\u5883\u9650\u5236\u4e86\u7814\u7a76\u8fdb\u5c55\u3002", "method": "\u5f00\u53d1Verti-Arena\u8bbe\u65bd\uff0c\u914d\u5907\u4f20\u611f\u5668\u548c\u8fd0\u52a8\u6355\u6349\u7cfb\u7edf\uff0c\u63d0\u4f9b\u7cbe\u786e\u5730\u9762\u771f\u5b9e\u6570\u636e\uff0c\u5e76\u652f\u6301\u8fdc\u7a0b\u5b9e\u9a8c\u3002", "result": "Verti-Arena\u4e3a\u8d8a\u91ce\u81ea\u4e3b\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u53ef\u91cd\u590d\u7684\u57fa\u51c6\u73af\u5883\u548c\u6807\u51c6\u5316\u6570\u636e\u6536\u96c6\u3002", "conclusion": "Verti-Arena\u586b\u8865\u4e86\u8d8a\u91ce\u81ea\u4e3b\u6027\u7814\u7a76\u4e2d\u7684\u6d4b\u8bd5\u73af\u5883\u7a7a\u767d\uff0c\u4fc3\u8fdb\u4e86\u5168\u7403\u534f\u4f5c\u548c\u7b97\u6cd5\u6bd4\u8f83\u3002"}}
{"id": "2508.08053", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08053", "abs": "https://arxiv.org/abs/2508.08053", "authors": ["Runchuan Zhu", "Bowen Jiang", "Lingrui Mei", "Fangkai Yang", "Lu Wang", "Haoxiang Gao", "Fengshuo Bai", "Pu Zhao", "Qingwei Lin", "Saravan Rajmohan", "Dongmei Zhang"], "title": "AdaptFlow: Adaptive Workflow Optimization via Meta-Learning", "comment": null, "summary": "Recent advances in large language models (LLMs) have sparked growing interest\nin agentic workflows, which are structured sequences of LLM invocations\nintended to solve complex tasks. However, existing approaches often rely on\nstatic templates or manually designed workflows, which limit adaptability to\ndiverse tasks and hinder scalability. We propose AdaptFlow, a natural\nlanguage-based meta-learning framework inspired by model-agnostic meta-learning\n(MAML). AdaptFlow learns a generalizable workflow initialization that enables\nrapid subtask-level adaptation. It employs a bi-level optimization scheme: the\ninner loop refines the workflow for a specific subtask using LLM-generated\nfeedback, while the outer loop updates the shared initialization to perform\nwell across tasks. This setup allows AdaptFlow to generalize effectively to\nunseen tasks by adapting the initialized workflow through language-guided\nmodifications. Evaluated across question answering, code generation, and\nmathematical reasoning benchmarks, AdaptFlow consistently outperforms both\nmanually crafted and automatically searched baselines, achieving\nstate-of-the-art results with strong generalization across tasks and models.\nThe source code and data are available at\nhttps://github.com/microsoft/DKI_LLM/tree/AdaptFlow/AdaptFlow.", "AI": {"tldr": "AdaptFlow\u662f\u4e00\u79cd\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u7684\u5143\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u5c42\u4f18\u5316\u5b66\u4e60\u901a\u7528\u5de5\u4f5c\u6d41\u521d\u59cb\u5316\uff0c\u5feb\u901f\u9002\u5e94\u5b50\u4efb\u52a1\uff0c\u5e76\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u9759\u6001\u6a21\u677f\u6216\u624b\u52a8\u8bbe\u8ba1\u7684\u5de5\u4f5c\u6d41\uff0c\u9650\u5236\u4e86\u9002\u5e94\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "method": "\u91c7\u7528\u53cc\u5c42\u4f18\u5316\uff1a\u5185\u5faa\u73af\u901a\u8fc7LLM\u53cd\u9988\u4f18\u5316\u5b50\u4efb\u52a1\u5de5\u4f5c\u6d41\uff0c\u5916\u5faa\u73af\u66f4\u65b0\u5171\u4eab\u521d\u59cb\u5316\u3002", "result": "\u5728\u95ee\u7b54\u3001\u4ee3\u7801\u751f\u6210\u548c\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\u8d85\u8d8a\u57fa\u7ebf\uff0c\u5b9e\u73b0\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "AdaptFlow\u901a\u8fc7\u8bed\u8a00\u5f15\u5bfc\u7684\u4fee\u6539\u5b9e\u73b0\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u9002\u7528\u4e8e\u591a\u6837\u5316\u4efb\u52a1\u3002"}}
{"id": "2508.08240", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08240", "abs": "https://arxiv.org/abs/2508.08240", "authors": ["Kaijun Wang", "Liqin Lu", "Mingyu Liu", "Jianuo Jiang", "Zeju Li", "Bolin Zhang", "Wancai Zheng", "Xinyi Yu", "Hao Chen", "Chunhua Shen"], "title": "ODYSSEY: Open-World Quadrupeds Exploration and Manipulation for Long-Horizon Tasks", "comment": null, "summary": "Language-guided long-horizon mobile manipulation has long been a grand\nchallenge in embodied semantic reasoning, generalizable manipulation, and\nadaptive locomotion. Three fundamental limitations hinder progress: First,\nalthough large language models have improved spatial reasoning and task\nplanning through semantic priors, existing implementations remain confined to\ntabletop scenarios, failing to address the constrained perception and limited\nactuation ranges of mobile platforms. Second, current manipulation strategies\nexhibit insufficient generalization when confronted with the diverse object\nconfigurations encountered in open-world environments. Third, while crucial for\npractical deployment, the dual requirement of maintaining high platform\nmaneuverability alongside precise end-effector control in unstructured settings\nremains understudied.\n  In this work, we present ODYSSEY, a unified mobile manipulation framework for\nagile quadruped robots equipped with manipulators, which seamlessly integrates\nhigh-level task planning with low-level whole-body control. To address the\nchallenge of egocentric perception in language-conditioned tasks, we introduce\na hierarchical planner powered by a vision-language model, enabling\nlong-horizon instruction decomposition and precise action execution. At the\ncontrol level, our novel whole-body policy achieves robust coordination across\nchallenging terrains. We further present the first benchmark for long-horizon\nmobile manipulation, evaluating diverse indoor and outdoor scenarios. Through\nsuccessful sim-to-real transfer, we demonstrate the system's generalization and\nrobustness in real-world deployments, underscoring the practicality of legged\nmanipulators in unstructured environments. Our work advances the feasibility of\ngeneralized robotic assistants capable of complex, dynamic tasks. Our project\npage: https://kaijwang.github.io/odyssey.github.io/", "AI": {"tldr": "ODYSSEY\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u79fb\u52a8\u64cd\u4f5c\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u9ad8\u7ea7\u4efb\u52a1\u89c4\u5212\u548c\u4f4e\u7ea7\u5168\u8eab\u63a7\u5236\uff0c\u89e3\u51b3\u4e86\u8bed\u8a00\u5f15\u5bfc\u7684\u957f\u65f6\u7a0b\u79fb\u52a8\u64cd\u4f5c\u4e2d\u7684\u611f\u77e5\u3001\u6cdb\u5316\u548c\u63a7\u5236\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u79fb\u52a8\u5e73\u53f0\u7684\u611f\u77e5\u8303\u56f4\u3001\u6cdb\u5316\u80fd\u529b\u548c\u63a7\u5236\u7cbe\u5ea6\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0cODYSSEY\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u5f15\u5165\u5206\u5c42\u89c4\u5212\u5668\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4efb\u52a1\u5206\u89e3\uff0c\u7ed3\u5408\u65b0\u578b\u5168\u8eab\u63a7\u5236\u7b56\u7565\u5b9e\u73b0\u534f\u8c03\u64cd\u4f5c\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u8f6c\u79fb\u9a8c\u8bc1\u4e86\u7cfb\u7edf\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "ODYSSEY\u63a8\u52a8\u4e86\u901a\u7528\u673a\u5668\u4eba\u52a9\u624b\u5728\u590d\u6742\u52a8\u6001\u4efb\u52a1\u4e2d\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2508.08075", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08075", "abs": "https://arxiv.org/abs/2508.08075", "authors": ["Meishen He", "Wenjun Ma", "Jiao Wang", "Huijun Yue", "Xiaoma Fan"], "title": "FNBT: Full Negation Belief Transformation for Open-World Information Fusion Based on Dempster-Shafer Theory of Evidence", "comment": null, "summary": "The Dempster-Shafer theory of evidence has been widely applied in the field\nof information fusion under uncertainty. Most existing research focuses on\ncombining evidence within the same frame of discernment. However, in real-world\nscenarios, trained algorithms or data often originate from different regions or\norganizations, where data silos are prevalent. As a result, using different\ndata sources or models to generate basic probability assignments may lead to\nheterogeneous frames, for which traditional fusion methods often yield\nunsatisfactory results. To address this challenge, this study proposes an\nopen-world information fusion method, termed Full Negation Belief\nTransformation (FNBT), based on the Dempster-Shafer theory. More specially, a\ncriterion is introduced to determine whether a given fusion task belongs to the\nopen-world setting. Then, by extending the frames, the method can accommodate\nelements from heterogeneous frames. Finally, a full negation mechanism is\nemployed to transform the mass functions, so that existing combination rules\ncan be applied to the transformed mass functions for such information fusion.\nTheoretically, the proposed method satisfies three desirable properties, which\nare formally proven: mass function invariance, heritability, and essential\nconflict elimination. Empirically, FNBT demonstrates superior performance in\npattern classification tasks on real-world datasets and successfully resolves\nZadeh's counterexample, thereby validating its practical effectiveness.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eDempster-Shafer\u7406\u8bba\u7684\u5f00\u653e\u4e16\u754c\u4fe1\u606f\u878d\u5408\u65b9\u6cd5FNBT\uff0c\u89e3\u51b3\u4e86\u5f02\u6784\u6846\u67b6\u4e0b\u7684\u8bc1\u636e\u878d\u5408\u95ee\u9898\u3002", "motivation": "\u73b0\u5b9e\u573a\u666f\u4e2d\uff0c\u6570\u636e\u6216\u6a21\u578b\u5e38\u6765\u81ea\u4e0d\u540c\u533a\u57df\u6216\u7ec4\u7ec7\uff0c\u5bfc\u81f4\u5f02\u6784\u6846\u67b6\u95ee\u9898\uff0c\u4f20\u7edf\u878d\u5408\u65b9\u6cd5\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u63d0\u51faFNBT\u65b9\u6cd5\uff0c\u901a\u8fc7\u6269\u5c55\u6846\u67b6\u548c\u5168\u5426\u5b9a\u673a\u5236\u8f6c\u6362\u8d28\u91cf\u51fd\u6570\uff0c\u4f7f\u73b0\u6709\u7ec4\u5408\u89c4\u5219\u9002\u7528\u4e8e\u5f02\u6784\u6846\u67b6\u3002", "result": "\u7406\u8bba\u8bc1\u660eFNBT\u6ee1\u8db3\u8d28\u91cf\u51fd\u6570\u4e0d\u53d8\u6027\u3001\u9057\u4f20\u6027\u548c\u51b2\u7a81\u6d88\u9664\u6027\uff1b\u5b9e\u9a8c\u663e\u793a\u5176\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "FNBT\u6709\u6548\u89e3\u51b3\u4e86\u5f02\u6784\u6846\u67b6\u4e0b\u7684\u4fe1\u606f\u878d\u5408\u95ee\u9898\uff0c\u5177\u6709\u7406\u8bba\u548c\u5b9e\u8df5\u4ef7\u503c\u3002"}}
{"id": "2508.08241", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.08241", "abs": "https://arxiv.org/abs/2508.08241", "authors": ["Takara E. Truong", "Qiayuan Liao", "Xiaoyu Huang", "Guy Tevet", "C. Karen Liu", "Koushil Sreenath"], "title": "BeyondMimic: From Motion Tracking to Versatile Humanoid Control via Guided Diffusion", "comment": "9 pages, 1 figure", "summary": "Learning skills from human motions offers a promising path toward\ngeneralizable policies for whole-body humanoid control, yet two key\ncornerstones are missing: (1) a high-quality motion tracking framework that\nfaithfully transforms large-scale kinematic references into robust and\nextremely dynamic motions on real hardware, and (2) a distillation approach\nthat can effectively learn these motion primitives and compose them to solve\ndownstream tasks. We address these gaps with BeyondMimic, the first real-world\nframework to learn from human motions for versatile and naturalistic humanoid\ncontrol via guided diffusion. Our framework provides a motion tracking pipeline\ncapable of challenging skills such as jumping spins, sprinting, and cartwheels\nwith state-of-the-art motion quality. Moving beyond mimicking existing motions\nand synthesize novel ones, we further introduce a unified diffusion policy that\nenables zero-shot task-specific control at test time using simple cost\nfunctions. Deployed on hardware, BeyondMimic performs diverse tasks at test\ntime, including waypoint navigation, joystick teleoperation, and obstacle\navoidance, bridging sim-to-real motion tracking and flexible synthesis of human\nmotion primitives for whole-body control. https://beyondmimic.github.io/.", "AI": {"tldr": "BeyondMimic \u662f\u4e00\u4e2a\u4ece\u4eba\u7c7b\u52a8\u4f5c\u4e2d\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5bfc\u6269\u6563\u6280\u672f\u5b9e\u73b0\u591a\u529f\u80fd\u3001\u81ea\u7136\u7684\u4eba\u5f62\u673a\u5668\u4eba\u63a7\u5236\uff0c\u89e3\u51b3\u4e86\u9ad8\u8d28\u91cf\u52a8\u4f5c\u8ddf\u8e2a\u548c\u52a8\u4f5c\u539f\u8bed\u5b66\u4e60\u7684\u4e24\u4e2a\u5173\u952e\u95ee\u9898\u3002", "motivation": "\u4ece\u4eba\u7c7b\u52a8\u4f5c\u4e2d\u5b66\u4e60\u6280\u80fd\u4e3a\u5168\u8eab\u4eba\u5f62\u673a\u5668\u4eba\u63a7\u5236\u63d0\u4f9b\u4e86\u901a\u7528\u7b56\u7565\u7684\u6f5c\u529b\uff0c\u4f46\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u7684\u52a8\u4f5c\u8ddf\u8e2a\u6846\u67b6\u548c\u6709\u6548\u7684\u52a8\u4f5c\u539f\u8bed\u5b66\u4e60\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa BeyondMimic \u6846\u67b6\uff0c\u5305\u62ec\u4e00\u4e2a\u80fd\u591f\u5904\u7406\u9ad8\u52a8\u6001\u52a8\u4f5c\u7684\u8ddf\u8e2a\u7ba1\u9053\u548c\u4e00\u4e2a\u7edf\u4e00\u7684\u6269\u6563\u7b56\u7565\uff0c\u652f\u6301\u96f6\u6837\u672c\u4efb\u52a1\u63a7\u5236\u3002", "result": "\u5728\u786c\u4ef6\u4e0a\u5b9e\u73b0\u4e86\u591a\u6837\u4efb\u52a1\uff0c\u5982\u5bfc\u822a\u3001\u9065\u64cd\u4f5c\u548c\u907f\u969c\uff0c\u5c55\u793a\u4e86\u9ad8\u8d28\u91cf\u7684\u52a8\u4f5c\u8ddf\u8e2a\u548c\u7075\u6d3b\u7684\u52a8\u4f5c\u5408\u6210\u80fd\u529b\u3002", "conclusion": "BeyondMimic \u586b\u8865\u4e86\u52a8\u4f5c\u8ddf\u8e2a\u548c\u52a8\u4f5c\u539f\u8bed\u5b66\u4e60\u7684\u7a7a\u767d\uff0c\u4e3a\u5168\u8eab\u4eba\u5f62\u673a\u5668\u4eba\u63a7\u5236\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2508.08115", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08115", "abs": "https://arxiv.org/abs/2508.08115", "authors": ["Pranav Pushkar Mishra", "Mohammad Arvan", "Mohan Zalake"], "title": "TeamMedAgents: Enhancing Medical Decision-Making of LLMs Through Structured Teamwork", "comment": "10 pages, 1 figure, 6 tables(2 in main, 4 in appendix)", "summary": "We present TeamMedAgents, a novel multi-agent approach that systematically\nintegrates evidence-based teamwork components from human-human collaboration\ninto medical decision-making with large language models (LLMs). Our approach\nvalidates an organizational psychology teamwork model from human collaboration\nto computational multi-agent medical systems by operationalizing six core\nteamwork components derived from Salas et al.'s \"Big Five\" model: team\nleadership, mutual performance monitoring, team orientation, shared mental\nmodels, closed-loop communication, and mutual trust. We implement and evaluate\nthese components as modular, configurable mechanisms within an adaptive\ncollaboration architecture while assessing the effect of the number of agents\ninvolved based on the task's requirements and domain. Systematic evaluation of\ncomputational implementations of teamwork behaviors across eight medical\nbenchmarks (MedQA, MedMCQA, MMLU-Pro Medical, PubMedQA, DDXPlus, MedBullets,\nPath-VQA, and PMC-VQA) demonstrates consistent improvements across 7 out of 8\nevaluated datasets. Controlled ablation studies conducted on 50 questions per\nconfiguration across 3 independent runs provide mechanistic insights into\nindividual component contributions, revealing optimal teamwork configurations\nthat vary by reasoning task complexity and domain-specific requirements. Our\nablation analyses reveal dataset-specific optimal teamwork configurations,\nindicating that different medical reasoning modalities benefit from distinct\ncollaborative patterns. TeamMedAgents represents an advancement in\ncollaborative AI by providing a systematic translation of established teamwork\ntheories from human collaboration into agentic collaboration, establishing a\nfoundation for evidence-based multi-agent system design in critical\ndecision-making domains.", "AI": {"tldr": "TeamMedAgents\u5c06\u4eba\u7c7b\u56e2\u961f\u5408\u4f5c\u7684\u5fc3\u7406\u5b66\u6a21\u578b\u5e94\u7528\u4e8e\u591a\u667a\u80fd\u4f53\u533b\u7597\u51b3\u7b56\u7cfb\u7edf\uff0c\u901a\u8fc7\u516d\u5927\u6838\u5fc3\u56e2\u961f\u5408\u4f5c\u7ec4\u4ef6\u63d0\u5347LLMs\u7684\u6027\u80fd\uff0c\u5728\u591a\u4e2a\u533b\u7597\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u5c06\u4eba\u7c7b\u56e2\u961f\u5408\u4f5c\u7684\u7406\u8bba\u6a21\u578b\uff08\u5982Salas\u7684\u201cBig Five\u201d\uff09\u8f6c\u5316\u4e3a\u8ba1\u7b97\u6a21\u578b\uff0c\u4ee5\u63d0\u5347\u591a\u667a\u80fd\u4f53\u5728\u533b\u7597\u51b3\u7b56\u4e2d\u7684\u534f\u4f5c\u6548\u679c\u3002", "method": "\u5b9e\u73b0\u5e76\u8bc4\u4f30\u516d\u5927\u56e2\u961f\u5408\u4f5c\u7ec4\u4ef6\uff08\u5982\u56e2\u961f\u9886\u5bfc\u529b\u3001\u5171\u4eab\u5fc3\u667a\u6a21\u578b\u7b49\uff09\uff0c\u901a\u8fc7\u6a21\u5757\u5316\u67b6\u6784\u548c\u6d88\u878d\u5b9e\u9a8c\u5206\u6790\u5404\u7ec4\u4ef6\u8d21\u732e\u3002", "result": "\u57288\u4e2a\u533b\u7597\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c7\u4e2a\u8868\u73b0\u63d0\u5347\uff0c\u6d88\u878d\u5b9e\u9a8c\u63ed\u793a\u4e86\u4e0d\u540c\u4efb\u52a1\u548c\u9886\u57df\u7684\u6700\u4f18\u56e2\u961f\u914d\u7f6e\u3002", "conclusion": "TeamMedAgents\u4e3a\u5173\u952e\u51b3\u7b56\u9886\u57df\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u5c55\u793a\u4e86\u56e2\u961f\u5408\u4f5c\u6a21\u578b\u5728AI\u534f\u4f5c\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.08127", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08127", "abs": "https://arxiv.org/abs/2508.08127", "authors": ["Rui Miao", "Yixin Liu", "Yili Wang", "Xu Shen", "Yue Tan", "Yiwei Dai", "Shirui Pan", "Xin Wang"], "title": "BlindGuard: Safeguarding LLM-based Multi-Agent Systems under Unknown Attacks", "comment": null, "summary": "The security of LLM-based multi-agent systems (MAS) is critically threatened\nby propagation vulnerability, where malicious agents can distort collective\ndecision-making through inter-agent message interactions. While existing\nsupervised defense methods demonstrate promising performance, they may be\nimpractical in real-world scenarios due to their heavy reliance on labeled\nmalicious agents to train a supervised malicious detection model. To enable\npractical and generalizable MAS defenses, in this paper, we propose BlindGuard,\nan unsupervised defense method that learns without requiring any\nattack-specific labels or prior knowledge of malicious behaviors. To this end,\nwe establish a hierarchical agent encoder to capture individual, neighborhood,\nand global interaction patterns of each agent, providing a comprehensive\nunderstanding for malicious agent detection. Meanwhile, we design a\ncorruption-guided detector that consists of directional noise injection and\ncontrastive learning, allowing effective detection model training solely on\nnormal agent behaviors. Extensive experiments show that BlindGuard effectively\ndetects diverse attack types (i.e., prompt injection, memory poisoning, and\ntool attack) across MAS with various communication patterns while maintaining\nsuperior generalizability compared to supervised baselines. The code is\navailable at: https://github.com/MR9812/BlindGuard.", "AI": {"tldr": "BlindGuard\u662f\u4e00\u79cd\u65e0\u76d1\u7763\u9632\u5fa1\u65b9\u6cd5\uff0c\u7528\u4e8e\u68c0\u6d4b\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u6076\u610f\u4ee3\u7406\uff0c\u65e0\u9700\u4f9d\u8d56\u6807\u8bb0\u6570\u636e\u6216\u653b\u51fb\u5148\u9a8c\u77e5\u8bc6\u3002", "motivation": "\u73b0\u6709\u76d1\u7763\u9632\u5fa1\u65b9\u6cd5\u4f9d\u8d56\u6807\u8bb0\u7684\u6076\u610f\u4ee3\u7406\uff0c\u4e0d\u9002\u7528\u4e8e\u5b9e\u9645\u573a\u666f\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u66f4\u901a\u7528\u4e14\u5b9e\u7528\u7684\u9632\u5fa1\u65b9\u6cd5\u3002", "method": "BlindGuard\u901a\u8fc7\u5206\u5c42\u4ee3\u7406\u7f16\u7801\u5668\u6355\u6349\u4e2a\u4f53\u3001\u90bb\u57df\u548c\u5168\u5c40\u4ea4\u4e92\u6a21\u5f0f\uff0c\u5e76\u8bbe\u8ba1\u57fa\u4e8e\u566a\u58f0\u6ce8\u5165\u548c\u5bf9\u6bd4\u5b66\u4e60\u7684\u68c0\u6d4b\u5668\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cBlindGuard\u80fd\u6709\u6548\u68c0\u6d4b\u591a\u79cd\u653b\u51fb\u7c7b\u578b\uff0c\u4e14\u5728\u901a\u7528\u6027\u4e0a\u4f18\u4e8e\u76d1\u7763\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "BlindGuard\u4e3a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u4e14\u901a\u7528\u7684\u65e0\u76d1\u7763\u9632\u5fa1\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.08147", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08147", "abs": "https://arxiv.org/abs/2508.08147", "authors": ["Yunkai Hu", "Tianqiao Zhao", "Meng Yue"], "title": "From Natural Language to Solver-Ready Power System Optimization: An LLM-Assisted, Validation-in-the-Loop Framework", "comment": null, "summary": "This paper introduces a novel Large Language Models (LLMs)-assisted agent\nthat automatically converts natural-language descriptions of power system\noptimization scenarios into compact, solver-ready formulations and generates\ncorresponding solutions. In contrast to approaches that rely solely on LLM to\nproduce solutions directly, the proposed method focuses on discovering a\nmathematically compatible formulation that can be efficiently solved by\noff-the-shelf optimization solvers. Directly using LLMs to produce solutions\noften leads to infeasible or suboptimal results, as these models lack the\nnumerical precision and constraint-handling capabilities of established\noptimization solvers. The pipeline integrates a domain-aware prompt and schema\nwith an LLM, enforces feasibility through systematic validation and iterative\nrepair, and returns both solver-ready models and user-facing results. Using the\nunit commitment problem as a representative case study, the agent produces\noptimal or near-optimal schedules along with the associated objective costs.\nResults demonstrate that coupling the solver with task-specific validation\nsignificantly enhances solution reliability. This work shows that combining AI\nwith established optimization frameworks bridges high-level problem\ndescriptions and executable mathematical models, enabling more efficient\ndecision-making in energy systems", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u4ee3\u7406\uff0c\u5c06\u7535\u529b\u7cfb\u7edf\u4f18\u5316\u573a\u666f\u7684\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u81ea\u52a8\u8f6c\u6362\u4e3a\u53ef\u6c42\u89e3\u7684\u6570\u5b66\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u9a8c\u8bc1\u548c\u8fed\u4ee3\u4fee\u590d\u786e\u4fdd\u53ef\u884c\u6027\u3002", "motivation": "\u76f4\u63a5\u4f7f\u7528LLMs\u751f\u6210\u89e3\u51b3\u65b9\u6848\u5e38\u5bfc\u81f4\u4e0d\u53ef\u884c\u6216\u6b21\u4f18\u7ed3\u679c\uff0c\u56e0\u5176\u7f3a\u4e4f\u6570\u503c\u7cbe\u5ea6\u548c\u7ea6\u675f\u5904\u7406\u80fd\u529b\u3002", "method": "\u7ed3\u5408\u9886\u57df\u611f\u77e5\u63d0\u793a\u4e0eLLM\uff0c\u901a\u8fc7\u7cfb\u7edf\u9a8c\u8bc1\u548c\u8fed\u4ee3\u4fee\u590d\u751f\u6210\u53ef\u6c42\u89e3\u7684\u6570\u5b66\u6a21\u578b\u3002", "result": "\u4ee5\u673a\u7ec4\u7ec4\u5408\u95ee\u9898\u4e3a\u4f8b\uff0c\u4ee3\u7406\u751f\u6210\u4e86\u6700\u4f18\u6216\u63a5\u8fd1\u6700\u4f18\u7684\u8c03\u5ea6\u65b9\u6848\u53ca\u76ee\u6807\u6210\u672c\u3002", "conclusion": "\u7ed3\u5408AI\u4e0e\u4f20\u7edf\u4f18\u5316\u6846\u67b6\uff0c\u80fd\u9ad8\u6548\u8fde\u63a5\u9ad8\u5c42\u95ee\u9898\u63cf\u8ff0\u4e0e\u53ef\u6267\u884c\u6570\u5b66\u6a21\u578b\uff0c\u63d0\u5347\u80fd\u6e90\u7cfb\u7edf\u51b3\u7b56\u6548\u7387\u3002"}}
{"id": "2505.23197", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2505.23197", "abs": "https://arxiv.org/abs/2505.23197", "authors": ["Jatin Kumar Arora", "Shubhendu Bhasin"], "title": "UPP: Unified Path Planner with Adaptive Safety and Optimality", "comment": "8 pages,11 figures", "summary": "We are surrounded by robots helping us perform complex tasks. Robots have a\nwide range of applications, from industrial automation to personalized\nassistance. However, with great technological innovation come significant\nchallenges. One of the major challenges in robotics is path planning. Despite\nadvancements such as graph search, sampling, and potential field methods, most\npath planning algorithms focus either on optimality or on safety. Very little\nresearch addresses both simultaneously. We propose a Unified Path Planner (UPP)\nthat uses modified heuristics and a dynamic safety cost function to balance\nsafety and optimality. The level of safety can be adjusted via tunable\nparameters, trading off against computational complexity. We demonstrate the\nplanner's performance in simulations, showing how parameter variation affects\nresults. UPP is compared with various traditional and safe-optimal planning\nalgorithms across different scenarios. We also validate it on a TurtleBot,\nwhere the robot successfully finds safe and sub-optimal paths.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u8def\u5f84\u89c4\u5212\u5668\uff08UPP\uff09\uff0c\u901a\u8fc7\u6539\u8fdb\u542f\u53d1\u5f0f\u548c\u52a8\u6001\u5b89\u5168\u6210\u672c\u51fd\u6570\uff0c\u540c\u65f6\u517c\u987e\u5b89\u5168\u6027\u548c\u6700\u4f18\u6027\u3002", "motivation": "\u73b0\u6709\u8def\u5f84\u89c4\u5212\u7b97\u6cd5\u591a\u4e13\u6ce8\u4e8e\u6700\u4f18\u6027\u6216\u5b89\u5168\u6027\uff0c\u7f3a\u4e4f\u540c\u65f6\u89e3\u51b3\u4e24\u8005\u7684\u7814\u7a76\u3002", "method": "\u4f7f\u7528\u6539\u8fdb\u7684\u542f\u53d1\u5f0f\u548c\u52a8\u6001\u5b89\u5168\u6210\u672c\u51fd\u6570\uff0c\u901a\u8fc7\u53ef\u8c03\u53c2\u6570\u5e73\u8861\u5b89\u5168\u6027\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "result": "\u4eff\u771f\u548c\u5b9e\u9645\u6d4b\u8bd5\uff08TurtleBot\uff09\u8868\u660e\uff0cUPP\u80fd\u5728\u4e0d\u540c\u573a\u666f\u4e2d\u5b9e\u73b0\u5b89\u5168\u4e14\u63a5\u8fd1\u6700\u4f18\u7684\u8def\u5f84\u89c4\u5212\u3002", "conclusion": "UPP\u4e3a\u8def\u5f84\u89c4\u5212\u63d0\u4f9b\u4e86\u4e00\u79cd\u517c\u987e\u5b89\u5168\u6027\u548c\u6700\u4f18\u6027\u7684\u65b0\u65b9\u6cd5\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
