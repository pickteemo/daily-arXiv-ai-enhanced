{"id": "2507.12489", "categories": ["cs.RO", "cs.CV", "cs.GR", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.12489", "abs": "https://arxiv.org/abs/2507.12489", "authors": ["Richard Marcus", "Marc Stamminger"], "title": "Physically Based Neural LiDAR Resimulation", "comment": "Accepted at ITSC 2025, Gold Coast Australia", "summary": "Methods for Novel View Synthesis (NVS) have recently found traction in the\nfield of LiDAR simulation and large-scale 3D scene reconstruction. While\nsolutions for faster rendering or handling dynamic scenes have been proposed,\nLiDAR specific effects remain insufficiently addressed. By explicitly modeling\nsensor characteristics such as rolling shutter, laser power variations, and\nintensity falloff, our method achieves more accurate LiDAR simulation compared\nto existing techniques. We demonstrate the effectiveness of our approach\nthrough quantitative and qualitative comparisons with state-of-the-art methods,\nas well as ablation studies that highlight the importance of each sensor model\ncomponent. Beyond that, we show that our approach exhibits advanced\nresimulation capabilities, such as generating high resolution LiDAR scans in\nthe camera perspective.\n  Our code and the resulting dataset are available at\nhttps://github.com/richardmarcus/PBNLiDAR.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9LiDAR\u4eff\u771f\u7684\u65b0\u89c6\u89d2\u5408\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u4f20\u611f\u5668\u7279\u6027\uff08\u5982\u6eda\u52a8\u5feb\u95e8\u3001\u6fc0\u5149\u529f\u7387\u53d8\u5316\u3001\u5f3a\u5ea6\u8870\u51cf\uff09\uff0c\u5b9e\u73b0\u4e86\u6bd4\u73b0\u6709\u6280\u672f\u66f4\u51c6\u786e\u7684LiDAR\u4eff\u771f\uff0c\u5e76\u5c55\u793a\u4e86\u9ad8\u5206\u8fa8\u7387LiDAR\u626b\u63cf\u751f\u6210\u7b49\u5148\u8fdb\u7684\u91cd\u4eff\u771f\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u65b0\u89c6\u89d2\u5408\u6210\u65b9\u6cd5\u5728LiDAR\u4eff\u771f\u548c\u5927\u89c4\u6a213D\u573a\u666f\u91cd\u5efa\u4e2d\u7f3a\u4e4f\u5bf9LiDAR\u7279\u5b9a\u6548\u5e94\u7684\u5145\u5206\u5904\u7406\uff0c\u867d\u7136\u6709\u5feb\u901f\u6e32\u67d3\u548c\u52a8\u6001\u573a\u666f\u5904\u7406\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46LiDAR\u4f20\u611f\u5668\u7684\u72ec\u7279\u7279\u6027\u5efa\u6a21\u4ecd\u7136\u4e0d\u8db3\u3002", "method": "\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21LiDAR\u4f20\u611f\u5668\u7684\u7279\u6027\uff0c\u5305\u62ec\u6eda\u52a8\u5feb\u95e8\u6548\u5e94\u3001\u6fc0\u5149\u529f\u7387\u53d8\u5316\u548c\u5f3a\u5ea6\u8870\u51cf\u7b49\u5173\u952e\u56e0\u7d20\uff0c\u5f00\u53d1\u51fa\u66f4\u51c6\u786e\u7684LiDAR\u4eff\u771f\u65b9\u6cd5\u3002", "result": "\u901a\u8fc7\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u5b9a\u91cf\u548c\u5b9a\u6027\u6bd4\u8f83\uff0c\u4ee5\u53ca\u7a81\u51fa\u6bcf\u4e2a\u4f20\u611f\u5668\u6a21\u578b\u7ec4\u4ef6\u91cd\u8981\u6027\u7684\u6d88\u878d\u7814\u7a76\uff0c\u8bc1\u660e\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u5c55\u793a\u4e86\u5148\u8fdb\u7684\u91cd\u4eff\u771f\u80fd\u529b\uff0c\u5982\u5728\u76f8\u673a\u89c6\u89d2\u4e0b\u751f\u6210\u9ad8\u5206\u8fa8\u7387LiDAR\u626b\u63cf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u51c6\u786e\u5efa\u6a21LiDAR\u4f20\u611f\u5668\u7279\u6027\uff0c\u5728LiDAR\u4eff\u771f\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\uff0c\u4e3aLiDAR\u4eff\u771f\u548c3D\u573a\u666f\u91cd\u5efa\u9886\u57df\u63d0\u4f9b\u4e86\u66f4\u7cbe\u786e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u516c\u5f00\u53ef\u7528\u3002"}}
{"id": "2507.12496", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12496", "abs": "https://arxiv.org/abs/2507.12496", "authors": ["Yucen Wang", "Rui Yu", "Shenghua Wan", "Le Gan", "De-Chuan Zhan"], "title": "FOUNDER: Grounding Foundation Models in World Models for Open-Ended Embodied Decision Making", "comment": "Accepted by Forty-Second International Conference on Machine Learning\n  (ICML 2025)", "summary": "Foundation Models (FMs) and World Models (WMs) offer complementary strengths\nin task generalization at different levels. In this work, we propose FOUNDER, a\nframework that integrates the generalizable knowledge embedded in FMs with the\ndynamic modeling capabilities of WMs to enable open-ended task solving in\nembodied environments in a reward-free manner. We learn a mapping function that\ngrounds FM representations in the WM state space, effectively inferring the\nagent's physical states in the world simulator from external observations. This\nmapping enables the learning of a goal-conditioned policy through imagination\nduring behavior learning, with the mapped task serving as the goal state. Our\nmethod leverages the predicted temporal distance to the goal state as an\ninformative reward signal. FOUNDER demonstrates superior performance on various\nmulti-task offline visual control benchmarks, excelling in capturing the\ndeep-level semantics of tasks specified by text or videos, particularly in\nscenarios involving complex observations or domain gaps where prior methods\nstruggle. The consistency of our learned reward function with the ground-truth\nreward is also empirically validated. Our project website is\nhttps://sites.google.com/view/founder-rl.", "AI": {"tldr": "FOUNDER\u6846\u67b6\u7ed3\u5408\u57fa\u7840\u6a21\u578b\uff08FMs\uff09\u548c\u4e16\u754c\u6a21\u578b\uff08WMs\uff09\u7684\u4f18\u52bf\uff0c\u901a\u8fc7\u65e0\u5956\u52b1\u65b9\u5f0f\u5728\u5177\u8eab\u73af\u5883\u4e2d\u5b9e\u73b0\u5f00\u653e\u4efb\u52a1\u89e3\u51b3\u3002", "motivation": "\u5229\u7528FMs\u7684\u901a\u7528\u77e5\u8bc6\u4e0eWMs\u7684\u52a8\u6001\u5efa\u6a21\u80fd\u529b\uff0c\u89e3\u51b3\u590d\u6742\u89c2\u5bdf\u6216\u9886\u57df\u5dee\u8ddd\u4e0b\u7684\u4efb\u52a1\u6cdb\u5316\u95ee\u9898\u3002", "method": "\u5b66\u4e60\u6620\u5c04\u51fd\u6570\u5c06FM\u8868\u793a\u5d4c\u5165WM\u72b6\u6001\u7a7a\u95f4\uff0c\u901a\u8fc7\u60f3\u8c61\u5b66\u4e60\u76ee\u6807\u6761\u4ef6\u7b56\u7565\uff0c\u5e76\u5229\u7528\u9884\u6d4b\u7684\u65f6\u95f4\u8ddd\u79bb\u4f5c\u4e3a\u5956\u52b1\u4fe1\u53f7\u3002", "result": "FOUNDER\u5728\u591a\u4efb\u52a1\u79bb\u7ebf\u89c6\u89c9\u63a7\u5236\u57fa\u51c6\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u5c24\u5176\u5728\u590d\u6742\u89c2\u5bdf\u6216\u9886\u57df\u5dee\u8ddd\u573a\u666f\u4e2d\u3002", "conclusion": "FOUNDER\u901a\u8fc7\u7ed3\u5408FMs\u548cWMs\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u4efb\u52a1\u89e3\u51b3\u548c\u5956\u52b1\u4e00\u81f4\u6027\u9a8c\u8bc1\u3002"}}
{"id": "2507.12499", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12499", "abs": "https://arxiv.org/abs/2507.12499", "authors": ["Yuhang Lu", "Jiadong Tu", "Yuexin Ma", "Xinge Zhu"], "title": "ReAL-AD: Towards Human-Like Reasoning in End-to-End Autonomous Driving", "comment": "Accepted by ICCV2025", "summary": "End-to-end autonomous driving has emerged as a promising approach to unify\nperception, prediction, and planning within a single framework, reducing\ninformation loss and improving adaptability. However, existing methods often\nrely on fixed and sparse trajectory supervision, limiting their ability to\ncapture the hierarchical reasoning process that human drivers naturally employ.\nTo bridge this gap, we propose ReAL-AD, a Reasoning-Augmented Learning\nframework that structures decision-making in autonomous driving based on the\nthree-tier human cognitive model: Driving Strategy, Driving Decision, and\nDriving Operation, where Vision-Language Models (VLMs) are incorporated to\nenhance situational awareness and structured reasoning across these levels.\nSpecifically, we introduce: (1) the Strategic Reasoning Injector, which\nformulates high-level driving strategies by interpreting complex traffic\ncontexts from VLM-generated insights; (2) the Tactical Reasoning Integrator,\nwhich refines strategic intent into interpretable tactical choices such as lane\nchanges, overtaking, and speed adjustments; and (3) the Hierarchical Trajectory\nDecoder, which progressively translates tactical decisions into precise control\nactions for smooth and human-like trajectory execution. Extensive evaluations\nshow that integrating our framework improves planning accuracy and safety by\nover 30%, making end-to-end autonomous driving more interpretable and aligned\nwith human-like hierarchical reasoning. The project page can be found at:\n\\href{https://4dvlab.github.io/project_page/realad}{\\texttt{4dvlab.github.io/project\\_page/realad}}", "AI": {"tldr": "ReAL-AD\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4eba\u7c7b\u8ba4\u77e5\u6a21\u578b\u7684\u4e09\u5c42\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7684\u51b3\u7b56\u80fd\u529b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u89c4\u5212\u51c6\u786e\u6027\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u73b0\u6709\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u65b9\u6cd5\u4f9d\u8d56\u56fa\u5b9a\u7a00\u758f\u7684\u8f68\u8ff9\u76d1\u7763\uff0c\u65e0\u6cd5\u6a21\u62df\u4eba\u7c7b\u9a7e\u9a76\u5458\u7684\u5206\u5c42\u63a8\u7406\u8fc7\u7a0b\uff0c\u9650\u5236\u4e86\u5176\u9002\u5e94\u6027\u548c\u8868\u73b0\u3002", "method": "ReAL-AD\u6846\u67b6\u5305\u542b\u4e09\u5c42\uff1a\u6218\u7565\u63a8\u7406\u6ce8\u5165\u5668\uff08\u751f\u6210\u9ad8\u5c42\u7b56\u7565\uff09\u3001\u6218\u672f\u63a8\u7406\u6574\u5408\u5668\uff08\u7ec6\u5316\u6218\u672f\u9009\u62e9\uff09\u548c\u5206\u5c42\u8f68\u8ff9\u89e3\u7801\u5668\uff08\u751f\u6210\u7cbe\u786e\u63a7\u5236\u52a8\u4f5c\uff09\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5c06\u89c4\u5212\u51c6\u786e\u6027\u548c\u5b89\u5168\u6027\u63d0\u9ad8\u4e8630%\u4ee5\u4e0a\uff0c\u4f7f\u81ea\u52a8\u9a7e\u9a76\u66f4\u63a5\u8fd1\u4eba\u7c7b\u63a8\u7406\u6a21\u5f0f\u3002", "conclusion": "ReAL-AD\u901a\u8fc7\u5206\u5c42\u63a8\u7406\u548cVLM\u7684\u7ed3\u5408\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u7684\u9002\u5e94\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2507.12644", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12644", "abs": "https://arxiv.org/abs/2507.12644", "authors": ["George Jiayuan Gao", "Tianyu Li", "Junyao Shi", "Yihan Li", "Zizhe Zhang", "Nadia Figueroa", "Dinesh Jayaraman"], "title": "VLMgineer: Vision Language Models as Robotic Toolsmiths", "comment": "Project Website: https://vlmgineer.github.io/release", "summary": "Tool design and use reflect the ability to understand and manipulate the\nphysical world through creativity, planning, and foresight. As such, these\ncapabilities are often regarded as measurable indicators of intelligence across\nbiological species. While much of today's research on robotic intelligence\nfocuses on generating better controllers, inventing smarter tools offers a\ncomplementary form of physical intelligence: shifting the onus of\nproblem-solving onto the tool's design. Given the vast and impressive\ncommon-sense, reasoning, and creative capabilities of today's foundation\nmodels, we investigate whether these models can provide useful priors to\nautomatically design and effectively wield such tools? We present VLMgineer, a\nframework that harnesses the code generation abilities of vision language\nmodels (VLMs) together with evolutionary search to iteratively co-design\nphysical tools and the action plans that operate them to perform a task. We\nevaluate VLMgineer on a diverse new benchmark of everyday manipulation\nscenarios that demand creative tool design and use. Across this suite,\nVLMgineer consistently discovers tools and policies that solve tasks more\neffectively and innovatively, transforming challenging robotics problems into\nstraightforward executions. It also outperforms VLM-generated designs from\nhuman specifications and existing human-crafted tools for everyday tasks. To\nfacilitate future research on automated tool invention, we will release our\nbenchmark and code.", "AI": {"tldr": "VLMgineer\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u8fdb\u5316\u641c\u7d22\u5171\u540c\u8bbe\u8ba1\u5de5\u5177\u548c\u64cd\u4f5c\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4efb\u52a1\u89e3\u51b3\u6548\u7387\u548c\u521b\u65b0\u6027\u3002", "motivation": "\u63a2\u7d22\u57fa\u7840\u6a21\u578b\u662f\u5426\u80fd\u901a\u8fc7\u5de5\u5177\u8bbe\u8ba1\u548c\u4f7f\u7528\u7684\u81ea\u52a8\u5316\uff0c\u63d0\u5347\u7269\u7406\u667a\u80fd\uff0c\u89e3\u51b3\u673a\u5668\u4eba\u5b66\u4e2d\u7684\u590d\u6742\u95ee\u9898\u3002", "method": "\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u4ee3\u7801\u751f\u6210\u80fd\u529b\u548c\u8fdb\u5316\u641c\u7d22\uff0c\u8fed\u4ee3\u8bbe\u8ba1\u5de5\u5177\u548c\u64cd\u4f5c\u7b56\u7565\u3002", "result": "VLMgineer\u5728\u591a\u6837\u5316\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f18\u4e8e\u4eba\u7c7b\u6307\u5b9a\u8bbe\u8ba1\u548c\u73b0\u6709\u624b\u5de5\u5de5\u5177\u3002", "conclusion": "VLMgineer\u4e3a\u81ea\u52a8\u5316\u5de5\u5177\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u5e76\u516c\u5f00\u4e86\u57fa\u51c6\u548c\u4ee3\u7801\u4ee5\u4fc3\u8fdb\u672a\u6765\u7814\u7a76\u3002"}}
{"id": "2507.12484", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.12484", "abs": "https://arxiv.org/abs/2507.12484", "authors": ["Jaros\u0142aw A. Chudziak", "Adam Kostka"], "title": "AI-Powered Math Tutoring: Platform for Personalized and Adaptive Education", "comment": "8 pages, 5 figures", "summary": "The growing ubiquity of artificial intelligence (AI), in particular large\nlanguage models (LLMs), has profoundly altered the way in which learners gain\nknowledge and interact with learning material, with many claiming that AI\npositively influences their learning achievements. Despite this advancement,\ncurrent AI tutoring systems face limitations associated with their reactive\nnature, often providing direct answers without encouraging deep reflection or\nincorporating structured pedagogical tools and strategies. This limitation is\nmost apparent in the field of mathematics, in which AI tutoring systems remain\nunderdeveloped. This research addresses the question: How can AI tutoring\nsystems move beyond providing reactive assistance to enable structured,\nindividualized, and tool-assisted learning experiences? We introduce a novel\nmulti-agent AI tutoring platform that combines adaptive and personalized\nfeedback, structured course generation, and textbook knowledge retrieval to\nenable modular, tool-assisted learning processes. This system allows students\nto learn new topics while identifying and targeting their weaknesses, revise\nfor exams effectively, and practice on an unlimited number of personalized\nexercises. This article contributes to the field of artificial intelligence in\neducation by introducing a novel platform that brings together pedagogical\nagents and AI-driven components, augmenting the field with modular and\neffective systems for teaching mathematics.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u591a\u4ee3\u7406AI\u8f85\u5bfc\u5e73\u53f0\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709AI\u8f85\u5bfc\u7cfb\u7edf\u7684\u88ab\u52a8\u6027\uff0c\u901a\u8fc7\u4e2a\u6027\u5316\u53cd\u9988\u548c\u7ed3\u6784\u5316\u8bfe\u7a0b\u751f\u6210\u63d0\u5347\u6570\u5b66\u5b66\u4e60\u6548\u679c\u3002", "motivation": "\u5f53\u524dAI\u8f85\u5bfc\u7cfb\u7edf\u591a\u4e3a\u88ab\u52a8\u54cd\u5e94\uff0c\u7f3a\u4e4f\u6df1\u5ea6\u53cd\u601d\u548c\u7ed3\u6784\u5316\u6559\u5b66\u5de5\u5177\uff0c\u5c24\u5176\u5728\u6570\u5b66\u9886\u57df\u8868\u73b0\u4e0d\u8db3\u3002", "method": "\u5f15\u5165\u591a\u4ee3\u7406AI\u8f85\u5bfc\u5e73\u53f0\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u53cd\u9988\u3001\u7ed3\u6784\u5316\u8bfe\u7a0b\u751f\u6210\u548c\u6559\u6750\u77e5\u8bc6\u68c0\u7d22\uff0c\u652f\u6301\u6a21\u5757\u5316\u5b66\u4e60\u3002", "result": "\u7cfb\u7edf\u5e2e\u52a9\u5b66\u751f\u9488\u5bf9\u6027\u5b66\u4e60\u3001\u9ad8\u6548\u590d\u4e60\uff0c\u5e76\u63d0\u4f9b\u65e0\u9650\u4e2a\u6027\u5316\u7ec3\u4e60\u3002", "conclusion": "\u8be5\u5e73\u53f0\u4e3aAI\u6559\u80b2\u9886\u57df\u8d21\u732e\u4e86\u6a21\u5757\u5316\u4e14\u9ad8\u6548\u7684\u6570\u5b66\u6559\u5b66\u7cfb\u7edf\u3002"}}
{"id": "2507.12716", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.12716", "abs": "https://arxiv.org/abs/2507.12716", "authors": ["Nathaniel Rose", "Hannah Chuang", "Manuel A Andrade-Rodriguez", "Rishi Parashar", "Dani Or", "Parikshit Maini"], "title": "MoistureMapper: An Autonomous Mobile Robot for High-Resolution Soil Moisture Mapping at Scale", "comment": "Accepted by 2025 IEEE 21st International Conference on Automation\n  Science and Engineering. 8 pages, 10 figures, 2 tables", "summary": "Soil moisture is a quantity of interest in many application areas including\nagriculture and climate modeling. Existing methods are not suitable for scale\napplications due to large deployment costs in high-resolution sensing\napplications such as for variable irrigation. In this work, we design, build\nand field deploy an autonomous mobile robot, MoistureMapper, for soil moisture\nsensing. The robot is equipped with Time Domain Reflectometry (TDR) sensors and\na direct push drill mechanism for deploying the sensor to measure volumetric\nwater content in the soil. Additionally, we implement and evaluate multiple\nadaptive sampling strategies based on a Gaussian Process based modeling to\nbuild a spatial mapping of moisture distribution in the soil. We present\nresults from large scale computational simulations and proof-of-concept\ndeployment on the field. The adaptive sampling approach outperforms a greedy\nbenchmark approach and results in up to 30\\% reduction in travel distance and\n5\\% reduction in variance in the reconstructed moisture maps. Link to video\nshowing field experiments: https://youtu.be/S4bJ4tRzObg", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u4e3b\u79fb\u52a8\u673a\u5668\u4ebaMoistureMapper\uff0c\u7528\u4e8e\u571f\u58e4\u6e7f\u5ea6\u4f20\u611f\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u91c7\u6837\u7b56\u7565\u4f18\u5316\u4e86\u6d4b\u91cf\u6548\u7387\u548c\u7cbe\u5ea6\u3002", "motivation": "\u571f\u58e4\u6e7f\u5ea6\u5728\u519c\u4e1a\u548c\u6c14\u5019\u5efa\u6a21\u4e2d\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u6210\u672c\u9ad8\u4e14\u4e0d\u9002\u5408\u5927\u89c4\u6a21\u5e94\u7528\u3002", "method": "\u8bbe\u8ba1\u5e76\u90e8\u7f72\u4e86\u914d\u5907TDR\u4f20\u611f\u5668\u548c\u76f4\u63a5\u63a8\u8fdb\u94bb\u63a2\u673a\u5236\u7684\u673a\u5668\u4eba\uff0c\u7ed3\u5408\u9ad8\u65af\u8fc7\u7a0b\u5efa\u6a21\u7684\u81ea\u9002\u5e94\u91c7\u6837\u7b56\u7565\u3002", "result": "\u81ea\u9002\u5e94\u91c7\u6837\u7b56\u7565\u5728\u8ba1\u7b97\u6a21\u62df\u548c\u5b9e\u5730\u90e8\u7f72\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u51cf\u5c11\u4e8630%\u7684\u79fb\u52a8\u8ddd\u79bb\u548c5%\u7684\u6e7f\u5ea6\u56fe\u65b9\u5dee\u3002", "conclusion": "MoistureMapper\u53ca\u5176\u81ea\u9002\u5e94\u91c7\u6837\u7b56\u7565\u4e3a\u5927\u89c4\u6a21\u571f\u58e4\u6e7f\u5ea6\u6d4b\u91cf\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u7ecf\u6d4e\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.12494", "categories": ["cs.AI", "cs.GT", "cs.MA", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.12494", "abs": "https://arxiv.org/abs/2507.12494", "authors": ["Dustin Holley", "Jovin D'sa", "Hossein Nourkhiz Mahjoub", "Gibran Ali"], "title": "MR-LDM -- The Merge-Reactive Longitudinal Decision Model: Game Theoretic Human Decision Modeling for Interactive Sim Agents", "comment": "8 pages", "summary": "Enhancing simulation environments to replicate real-world driver behavior,\ni.e., more humanlike sim agents, is essential for developing autonomous vehicle\ntechnology. In the context of highway merging, previous works have studied the\noperational-level yielding dynamics of lag vehicles in response to a merging\ncar at highway on-ramps. Other works focusing on tactical decision modeling\ngenerally consider limited action sets or utilize payoff functions with large\nparameter sets and limited payoff bounds. In this work, we aim to improve the\nsimulation of the highway merge scenario by targeting a game theoretic model\nfor tactical decision-making with improved payoff functions and lag actions. We\ncouple this with an underlying dynamics model to have a unified decision and\ndynamics model that can capture merging interactions and simulate more\nrealistic interactions in an explainable and interpretable fashion. The\nproposed model demonstrated good reproducibility of complex interactions when\nvalidated on a real-world dataset. The model was finally integrated into a high\nfidelity simulation environment and confirmed to have adequate computation time\nefficiency for use in large-scale simulations to support autonomous vehicle\ndevelopment.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u535a\u5f08\u8bba\u7684\u9ad8\u901f\u516c\u8def\u5408\u6d41\u573a\u666f\u6218\u672f\u51b3\u7b56\u6a21\u578b\uff0c\u7ed3\u5408\u6539\u8fdb\u7684\u6536\u76ca\u51fd\u6570\u548c\u6ede\u540e\u52a8\u4f5c\uff0c\u4ee5\u66f4\u771f\u5b9e\u5730\u6a21\u62df\u9a7e\u9a76\u5458\u884c\u4e3a\u3002", "motivation": "\u63d0\u5347\u6a21\u62df\u73af\u5883\u4e2d\u9a7e\u9a76\u5458\u884c\u4e3a\u7684\u771f\u5b9e\u6027\u5bf9\u81ea\u52a8\u9a7e\u9a76\u6280\u672f\u53d1\u5c55\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u901f\u516c\u8def\u5408\u6d41\u573a\u666f\u4e2d\u3002", "method": "\u91c7\u7528\u535a\u5f08\u8bba\u6a21\u578b\u6539\u8fdb\u6218\u672f\u51b3\u7b56\uff0c\u7ed3\u5408\u6536\u76ca\u51fd\u6570\u548c\u6ede\u540e\u52a8\u4f5c\uff0c\u5e76\u4e0e\u5e95\u5c42\u52a8\u529b\u5b66\u6a21\u578b\u8026\u5408\uff0c\u5f62\u6210\u7edf\u4e00\u7684\u51b3\u7b56\u4e0e\u52a8\u529b\u5b66\u6a21\u578b\u3002", "result": "\u6a21\u578b\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u590d\u6742\u4ea4\u4e92\u7684\u53ef\u91cd\u73b0\u6027\uff0c\u5e76\u5728\u9ad8\u4fdd\u771f\u6a21\u62df\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8db3\u591f\u7684\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u8be5\u6a21\u578b\u80fd\u591f\u4ee5\u53ef\u89e3\u91ca\u548c\u53ef\u89e3\u91ca\u7684\u65b9\u5f0f\u6a21\u62df\u66f4\u771f\u5b9e\u7684\u4ea4\u4e92\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u81ea\u52a8\u9a7e\u9a76\u5f00\u53d1\u6a21\u62df\u3002"}}
{"id": "2507.12731", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.12731", "abs": "https://arxiv.org/abs/2507.12731", "authors": ["Nathaniel Rose", "Arif Ahmed", "Emanuel Gutierrez-Cornejo", "Parikshit Maini"], "title": "Learning to Predict Mobile Robot Stability in Off-Road Environments", "comment": "Nathaniel Rose and Arif Ahmed contributed equally to this work.\n  Accepted poster for RSS 2025 Workshop on Resilient Off-road Autonomous\n  Robotics. 8 pages, 8 figures, 1 table", "summary": "Navigating in off-road environments for wheeled mobile robots is challenging\ndue to dynamic and rugged terrain. Traditional physics-based stability metrics,\nsuch as Static Stability Margin (SSM) or Zero Moment Point (ZMP) require\nknowledge of contact forces, terrain geometry, and the robot's precise\ncenter-of-mass that are difficult to measure accurately in real-world field\nconditions. In this work, we propose a learning-based approach to estimate\nrobot platform stability directly from proprioceptive data using a lightweight\nneural network, IMUnet. Our method enables data-driven inference of robot\nstability without requiring an explicit terrain model or force sensing.\n  We also develop a novel vision-based ArUco tracking method to compute a\nscalar score to quantify robot platform stability called C3 score. The score\ncaptures image-space perturbations over time as a proxy for physical\ninstability and is used as a training signal for the neural network based\nmodel. As a pilot study, we evaluate our approach on data collected across\nmultiple terrain types and speeds and demonstrate generalization to previously\nunseen conditions. These initial results highlight the potential of using IMU\nand robot velocity as inputs to estimate platform stability. The proposed\nmethod finds application in gating robot tasks such as precision actuation and\nsensing, especially for mobile manipulation tasks in agricultural and space\napplications. Our learning method also provides a supervision mechanism for\nperception based traversability estimation and planning.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7f51\u7edcIMUnet\uff0c\u76f4\u63a5\u4ece\u672c\u4f53\u611f\u53d7\u6570\u636e\u4f30\u8ba1\u673a\u5668\u4eba\u5e73\u53f0\u7a33\u5b9a\u6027\uff0c\u65e0\u9700\u5730\u5f62\u6a21\u578b\u6216\u529b\u4f20\u611f\u3002\u540c\u65f6\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u7684ArUco\u8ddf\u8e2a\u65b9\u6cd5\uff0c\u7528\u4e8e\u91cf\u5316\u7a33\u5b9a\u6027\uff08C3\u5206\u6570\uff09\u3002", "motivation": "\u4f20\u7edf\u7269\u7406\u7a33\u5b9a\u6027\u6307\u6807\uff08\u5982SSM\u6216ZMP\uff09\u9700\u8981\u96be\u4ee5\u51c6\u786e\u6d4b\u91cf\u7684\u63a5\u89e6\u529b\u3001\u5730\u5f62\u51e0\u4f55\u548c\u673a\u5668\u4eba\u8d28\u5fc3\u4fe1\u606f\uff0c\u9650\u5236\u4e86\u5728\u52a8\u6001\u5d0e\u5c96\u5730\u5f62\u4e2d\u7684\u5bfc\u822a\u80fd\u529b\u3002", "method": "\u4f7f\u7528IMUnet\u4ece\u672c\u4f53\u611f\u53d7\u6570\u636e\u5b66\u4e60\u7a33\u5b9a\u6027\uff0c\u5e76\u5f00\u53d1\u57fa\u4e8e\u89c6\u89c9\u7684C3\u5206\u6570\u4f5c\u4e3a\u8bad\u7ec3\u4fe1\u53f7\u3002", "result": "\u5728\u591a\u5730\u5f62\u548c\u901f\u5ea6\u6570\u636e\u4e0a\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5c55\u793a\u4e86IMU\u548c\u901f\u5ea6\u8f93\u5165\u4f30\u8ba1\u7a33\u5b9a\u6027\u7684\u6f5c\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u519c\u4e1a\u548c\u592a\u7a7a\u5e94\u7528\u4e2d\u7684\u79fb\u52a8\u64cd\u4f5c\u4efb\u52a1\uff0c\u5e76\u4e3a\u611f\u77e5\u9a71\u52a8\u7684\u53ef\u901a\u884c\u6027\u4f30\u8ba1\u548c\u89c4\u5212\u63d0\u4f9b\u4e86\u76d1\u7763\u673a\u5236\u3002"}}
{"id": "2507.12599", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12599", "abs": "https://arxiv.org/abs/2507.12599", "authors": ["L\u00e9o Sauli\u00e8res"], "title": "A Survey of Explainable Reinforcement Learning: Targets, Methods and Needs", "comment": "69 pages, 19 figures", "summary": "The success of recent Artificial Intelligence (AI) models has been\naccompanied by the opacity of their internal mechanisms, due notably to the use\nof deep neural networks. In order to understand these internal mechanisms and\nexplain the output of these AI models, a set of methods have been proposed,\ngrouped under the domain of eXplainable AI (XAI). This paper focuses on a\nsub-domain of XAI, called eXplainable Reinforcement Learning (XRL), which aims\nto explain the actions of an agent that has learned by reinforcement learning.\nWe propose an intuitive taxonomy based on two questions \"What\" and \"How\". The\nfirst question focuses on the target that the method explains, while the second\nrelates to the way the explanation is provided. We use this taxonomy to provide\na state-of-the-art review of over 250 papers. In addition, we present a set of\ndomains close to XRL, which we believe should get attention from the community.\nFinally, we identify some needs for the field of XRL.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u201cWhat\u201d\u548c\u201cHow\u201d\u95ee\u9898\u7684\u76f4\u89c2\u5206\u7c7b\u6cd5\uff0c\u7528\u4e8e\u68b3\u7406\u53ef\u89e3\u91ca\u5f3a\u5316\u5b66\u4e60\uff08XRL\uff09\u9886\u57df\u7684\u7814\u7a76\uff0c\u5e76\u7efc\u8ff0\u4e86250\u591a\u7bc7\u8bba\u6587\u3002", "motivation": "\u7531\u4e8e\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u7684\u5185\u90e8\u673a\u5236\u4e0d\u900f\u660e\uff0c\u7406\u89e3AI\u6a21\u578b\u7684\u8f93\u51fa\u6210\u4e3a\u6311\u6218\uff0cXRL\u65e8\u5728\u89e3\u91ca\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u7684\u884c\u4e3a\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u201cWhat\u201d\uff08\u89e3\u91ca\u76ee\u6807\uff09\u548c\u201cHow\u201d\uff08\u89e3\u91ca\u65b9\u5f0f\uff09\u7684\u5206\u7c7b\u6cd5\uff0c\u5e76\u7528\u4e8e\u7efc\u8ff0XRL\u9886\u57df\u7684\u7814\u7a76\u3002", "result": "\u901a\u8fc7\u5206\u7c7b\u6cd5\u68b3\u7406\u4e86250\u591a\u7bc7\u8bba\u6587\uff0c\u5e76\u6307\u51fa\u4e86XRL\u9886\u57df\u7684\u76f8\u5173\u7814\u7a76\u65b9\u5411\u548c\u9700\u6c42\u3002", "conclusion": "XRL\u9886\u57df\u9700\u8981\u66f4\u591a\u5173\u6ce8\uff0c\u5206\u7c7b\u6cd5\u4e3a\u7814\u7a76\u63d0\u4f9b\u4e86\u7cfb\u7edf\u5316\u7684\u89c6\u89d2\u3002"}}
{"id": "2507.12744", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.12744", "abs": "https://arxiv.org/abs/2507.12744", "authors": ["Cheng Liu", "Fan Zhu", "Yaoyu Zhuang Zhinan Chen Jiefeng Tang"], "title": "ASC-SW: Atrous strip convolution network with sliding windows for visual-assisted map navigation", "comment": null, "summary": "With the rapid development of lightweight visual neural network\narchitectures, traditional high-performance vision models have undergone\nsignificant compression, greatly improving their computational efficiency and\nenergy consumption ratio. This makes them feasible for deployment on\nresource-constrained edge computing devices. We propose a visual-assisted\nnavigation framework called Atrous Strip Convolution-Sliding Window (ASC-SW),\nwhich leverages a depth camera and a lightweight visual neural network to\nassist map-based mobile robot navigation. This framework compensates for the\ninability of traditional light detection and range (LiDAR) sensors to detect\nground-level obstacles such as ground-level wires. We introduce a lightweight\nand efficient segmentation model, Atrous Strip Convolution Network (ASCnet),\nfor detecting deformable linear objects (DLOs). MobileNetV2 is used as the\nbackbone network, and Atrous Strip Convolution Spatial Pyramid Pooling (ASCSPP)\nis designed to extract DLO features more effectively. Atrous Strip Convolution\nis integrated into ASCSPP to accurately identify the linear structure of DLOs\nwith low computational cost. Additionally, a Sliding Window (SW)\npost-processing module is proposed to denoise the output in complex\nenvironments, improving recognition accuracy. Our method strikes a balance\nbetween inference speed and segmentation performance. It achieves a mean\nIntersection over Union (Miou) score of 75.3% on a self-built dataset and\nreaches 9.3 FPS inference speed on the Jetson Orin Nano edge device. Overall,\nour approach outperforms existing DLO detection models and has been\nsuccessfully validated on a physical robotic platform.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u89c6\u89c9\u8f85\u52a9\u5bfc\u822a\u6846\u67b6ASC-SW\uff0c\u7ed3\u5408\u6df1\u5ea6\u76f8\u673a\u548c\u8f7b\u91cf\u7ea7\u89c6\u89c9\u795e\u7ecf\u7f51\u7edc\uff0c\u7528\u4e8e\u79fb\u52a8\u673a\u5668\u4eba\u5bfc\u822a\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfLiDAR\u65e0\u6cd5\u68c0\u6d4b\u5730\u9762\u969c\u788d\u7269\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edfLiDAR\u4f20\u611f\u5668\u65e0\u6cd5\u68c0\u6d4b\u5730\u9762\u969c\u788d\u7269\uff08\u5982\u7535\u7ebf\uff09\uff0c\u800c\u8f7b\u91cf\u7ea7\u89c6\u89c9\u795e\u7ecf\u7f51\u7edc\u7684\u53d1\u5c55\u4e3a\u8fb9\u7f18\u8ba1\u7b97\u8bbe\u5907\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u8bbe\u8ba1\u4e86\u8f7b\u91cf\u9ad8\u6548\u7684ASCnet\u5206\u5272\u6a21\u578b\uff0c\u91c7\u7528MobileNetV2\u4f5c\u4e3a\u4e3b\u5e72\u7f51\u7edc\uff0c\u7ed3\u5408ASCSPP\u6a21\u5757\u63d0\u53d6DLO\u7279\u5f81\uff0c\u5e76\u5f15\u5165\u6ed1\u52a8\u7a97\u53e3\uff08SW\uff09\u540e\u5904\u7406\u6a21\u5757\u63d0\u5347\u8bc6\u522b\u7cbe\u5ea6\u3002", "result": "\u5728\u81ea\u5efa\u6570\u636e\u96c6\u4e0a\u8fbe\u523075.3%\u7684Miou\uff0c\u8fb9\u7f18\u8bbe\u5907Jetson Orin Nano\u4e0a\u5b9e\u73b09.3 FPS\u7684\u63a8\u7406\u901f\u5ea6\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709DLO\u68c0\u6d4b\u6a21\u578b\u3002", "conclusion": "ASC-SW\u6846\u67b6\u5728\u63a8\u7406\u901f\u5ea6\u548c\u5206\u5272\u6027\u80fd\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u6210\u529f\u5e94\u7528\u4e8e\u7269\u7406\u673a\u5668\u4eba\u5e73\u53f0\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2507.12666", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12666", "abs": "https://arxiv.org/abs/2507.12666", "authors": ["Alex Zook", "Josef Spjut", "Jonathan Tremblay"], "title": "Fly, Fail, Fix: Iterative Game Repair with Reinforcement Learning and Large Multimodal Models", "comment": "Published at Reinforcement Learning and Video Games workshop\n  https://sites.google.com/view/rlvg-workshop-2025/home", "summary": "Game design hinges on understanding how static rules and content translate\ninto dynamic player behavior - something modern generative systems that inspect\nonly a game's code or assets struggle to capture. We present an automated\ndesign iteration framework that closes this gap by pairing a reinforcement\nlearning (RL) agent, which playtests the game, with a large multimodal model\n(LMM), which revises the game based on what the agent does. In each loop the RL\nplayer completes several episodes, producing (i) numerical play metrics and/or\n(ii) a compact image strip summarising recent video frames. The LMM designer\nreceives a gameplay goal and the current game configuration, analyses the play\ntraces, and edits the configuration to steer future behaviour toward the goal.\nWe demonstrate results that LMMs can reason over behavioral traces supplied by\nRL agents to iteratively refine game mechanics, pointing toward practical,\nscalable tools for AI-assisted game design.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u4ee3\u7406\u548c\u591a\u6a21\u6001\u6a21\u578b\uff08LMM\uff09\u7684\u81ea\u52a8\u5316\u6e38\u620f\u8bbe\u8ba1\u8fed\u4ee3\u6846\u67b6\uff0c\u901a\u8fc7RL\u4ee3\u7406\u6d4b\u8bd5\u6e38\u620f\u884c\u4e3a\uff0cLMM\u6839\u636e\u6d4b\u8bd5\u7ed3\u679c\u8c03\u6574\u6e38\u620f\u8bbe\u8ba1\u3002", "motivation": "\u73b0\u4ee3\u751f\u6210\u7cfb\u7edf\u4ec5\u5206\u6790\u6e38\u620f\u4ee3\u7801\u6216\u8d44\u6e90\uff0c\u96be\u4ee5\u6355\u6349\u9759\u6001\u89c4\u5219\u4e0e\u52a8\u6001\u73a9\u5bb6\u884c\u4e3a\u7684\u5173\u7cfb\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u81ea\u52a8\u8fed\u4ee3\u8bbe\u8ba1\u7684\u65b9\u6cd5\u3002", "method": "\u6846\u67b6\u4e2dRL\u4ee3\u7406\u901a\u8fc7\u591a\u6b21\u6e38\u620f\u6d4b\u8bd5\u751f\u6210\u6570\u503c\u6307\u6807\u6216\u56fe\u50cf\u6458\u8981\uff0cLMM\u6839\u636e\u8fd9\u4e9b\u6570\u636e\u548c\u76ee\u6807\u8c03\u6574\u6e38\u620f\u914d\u7f6e\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cLMM\u80fd\u57fa\u4e8eRL\u4ee3\u7406\u7684\u884c\u4e3a\u8f68\u8ff9\u8fed\u4ee3\u4f18\u5316\u6e38\u620f\u673a\u5236\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aAI\u8f85\u52a9\u6e38\u620f\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5b9e\u7528\u4e14\u53ef\u6269\u5c55\u7684\u5de5\u5177\u3002"}}
{"id": "2507.12751", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.12751", "abs": "https://arxiv.org/abs/2507.12751", "authors": ["Yasser G. Alqaham", "Jing Cheng", "Zhenyu Gan"], "title": "Refining Motion for Peak Performance: Identifying Optimal Gait Parameters for Energy-Efficient Quadrupedal Bounding", "comment": "Published in the ACC 2025 Conference proceedings", "summary": "Energy efficiency is a critical factor in the performance and autonomy of\nquadrupedal robots. While previous research has focused on mechanical design\nand actuation improvements, the impact of gait parameters on energetics has\nbeen less explored. In this paper, we hypothesize that gait parameters,\nspecifically duty factor, phase shift, and stride duration, are key\ndeterminants of energy consumption in quadrupedal locomotion. To test this\nhypothesis, we modeled the Unitree A1 quadrupedal robot and developed a\nlocomotion controller capable of independently adjusting these gait parameters.\nSimulations of bounding gaits were conducted in Gazebo across a range of gait\nparameters at three different speeds: low, medium, and high. Experimental tests\nwere also performed to validate the simulation results. The findings\ndemonstrate that optimizing gait parameters can lead to significant reductions\nin energy consumption, enhancing the overall efficiency of quadrupedal\nlocomotion. This work contributes to the advancement of energy-efficient\ncontrol strategies for legged robots, offering insights directly applicable to\ncommercially available platforms.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u6b65\u6001\u53c2\u6570\u5bf9\u56db\u8db3\u673a\u5668\u4eba\u80fd\u91cf\u6548\u7387\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u4f18\u5316\u8fd9\u4e9b\u53c2\u6570\u53ef\u663e\u8457\u964d\u4f4e\u80fd\u8017\u3002", "motivation": "\u5c3d\u7ba1\u673a\u68b0\u8bbe\u8ba1\u548c\u9a71\u52a8\u6539\u8fdb\u5df2\u6709\u7814\u7a76\uff0c\u4f46\u6b65\u6001\u53c2\u6570\u5bf9\u80fd\u91cf\u6d88\u8017\u7684\u5f71\u54cd\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u901a\u8fc7\u5efa\u6a21Unitree A1\u673a\u5668\u4eba\u5e76\u5f00\u53d1\u63a7\u5236\u5668\u8c03\u6574\u6b65\u6001\u53c2\u6570\uff0c\u5728Gazebo\u4e2d\u8fdb\u884c\u4eff\u771f\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u4f18\u5316\u6b65\u6001\u53c2\u6570\u53ef\u663e\u8457\u51cf\u5c11\u80fd\u91cf\u6d88\u8017\uff0c\u63d0\u5347\u56db\u8db3\u673a\u5668\u4eba\u8fd0\u52a8\u6548\u7387\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u56db\u8db3\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u8282\u80fd\u63a7\u5236\u7b56\u7565\uff0c\u53ef\u76f4\u63a5\u5e94\u7528\u4e8e\u5546\u4e1a\u5316\u5e73\u53f0\u3002"}}
{"id": "2507.12691", "categories": ["cs.AI", "cs.LG", "I.2.7; K.4.1"], "pdf": "https://arxiv.org/pdf/2507.12691", "abs": "https://arxiv.org/abs/2507.12691", "authors": ["Avi Parrack", "Carlo Leonardo Attubato", "Stefan Heimersheim"], "title": "Benchmarking Deception Probes via Black-to-White Performance Boosts", "comment": "Preprint. 37 pages, 10 figures, 7 tables", "summary": "AI assistants will occasionally respond deceptively to user queries.\nRecently, linear classifiers (called \"deception probes\") have been trained to\ndistinguish the internal activations of a language model during deceptive\nversus honest responses. However, it's unclear how effective these probes are\nat detecting deception in practice, nor whether such probes are resistant to\nsimple counter strategies from a deceptive assistant who wishes to evade\ndetection. In this paper, we compare white-box monitoring (where the monitor\nhas access to token-level probe activations) to black-box monitoring (without\nsuch access). We benchmark deception probes by the extent to which the white\nbox monitor outperforms the black-box monitor, i.e. the black-to-white\nperformance boost. We find weak but encouraging black-to-white performance\nboosts from existing deception probes.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86AI\u52a9\u624b\u6b3a\u9a97\u6027\u56de\u5e94\u7684\u68c0\u6d4b\u65b9\u6cd5\uff0c\u6bd4\u8f83\u4e86\u767d\u76d2\u4e0e\u9ed1\u76d2\u76d1\u63a7\u7684\u6548\u679c\uff0c\u53d1\u73b0\u73b0\u6709\u6b3a\u9a97\u63a2\u9488\u6548\u679c\u6709\u9650\u4f46\u4ecd\u6709\u6f5c\u529b\u3002", "motivation": "\u7814\u7a76AI\u52a9\u624b\u6b3a\u9a97\u6027\u56de\u5e94\u7684\u68c0\u6d4b\u65b9\u6cd5\uff0c\u8bc4\u4f30\u73b0\u6709\u6b3a\u9a97\u63a2\u9488\u7684\u5b9e\u7528\u6027\u53ca\u5176\u5bf9\u6297\u7b56\u7565\u7684\u9c81\u68d2\u6027\u3002", "method": "\u6bd4\u8f83\u767d\u76d2\u76d1\u63a7\uff08\u8bbf\u95ee\u63a2\u9488\u6fc0\u6d3b\uff09\u4e0e\u9ed1\u76d2\u76d1\u63a7\uff08\u65e0\u8bbf\u95ee\uff09\uff0c\u901a\u8fc7\u9ed1\u76d2\u5230\u767d\u76d2\u7684\u6027\u80fd\u63d0\u5347\u8bc4\u4f30\u6b3a\u9a97\u63a2\u9488\u6548\u679c\u3002", "result": "\u73b0\u6709\u6b3a\u9a97\u63a2\u9488\u8868\u73b0\u51fa\u5fae\u5f31\u4f46\u6709\u5e0c\u671b\u7684\u9ed1\u76d2\u5230\u767d\u76d2\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u6b3a\u9a97\u63a2\u9488\u5728\u68c0\u6d4bAI\u52a9\u624b\u6b3a\u9a97\u6027\u56de\u5e94\u65b9\u9762\u6709\u4e00\u5b9a\u6f5c\u529b\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u3002"}}
{"id": "2507.12753", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.12753", "abs": "https://arxiv.org/abs/2507.12753", "authors": ["Fujing Xie", "S\u00f6ren Schwertfeger", "Hermann Blum"], "title": "osmAG-LLM: Zero-Shot Open-Vocabulary Object Navigation via Semantic Maps and Large Language Models Reasoning", "comment": null, "summary": "Recent open-vocabulary robot mapping methods enrich dense geometric maps with\npre-trained visual-language features, achieving a high level of detail and\nguiding robots to find objects specified by open-vocabulary language queries.\nWhile the issue of scalability for such approaches has received some attention,\nanother fundamental problem is that high-detail object mapping quickly becomes\noutdated, as objects get moved around a lot. In this work, we develop a mapping\nand navigation system for object-goal navigation that, from the ground up,\nconsiders the possibilities that a queried object can have moved, or may not be\nmapped at all. Instead of striving for high-fidelity mapping detail, we\nconsider that the main purpose of a map is to provide environment grounding and\ncontext, which we combine with the semantic priors of LLMs to reason about\nobject locations and deploy an active, online approach to navigate to the\nobjects. Through simulated and real-world experiments we find that our approach\ntends to have higher retrieval success at shorter path lengths for static\nobjects and by far outperforms prior approaches in cases of dynamic or unmapped\nobject queries. We provide our code and dataset at:\nhttps://anonymous.4open.science/r/osmAG-LLM.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u52a8\u6001\u6216\u672a\u6620\u5c04\u5bf9\u8c61\u7684\u5bfc\u822a\u7cfb\u7edf\uff0c\u7ed3\u5408\u73af\u5883\u5730\u56fe\u4e0eLLM\u8bed\u4e49\u5148\u9a8c\uff0c\u4f18\u5316\u5bf9\u8c61\u68c0\u7d22\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709\u9ad8\u7ec6\u8282\u5bf9\u8c61\u5730\u56fe\u6613\u8fc7\u65f6\uff0c\u5bf9\u8c61\u79fb\u52a8\u9891\u7e41\uff0c\u9700\u4e00\u79cd\u80fd\u9002\u5e94\u52a8\u6001\u73af\u5883\u7684\u5bfc\u822a\u65b9\u6cd5\u3002", "method": "\u5f00\u53d1\u4e86\u7ed3\u5408\u73af\u5883\u5730\u56fe\u4e0eLLM\u8bed\u4e49\u5148\u9a8c\u7684\u4e3b\u52a8\u5728\u7ebf\u5bfc\u822a\u7cfb\u7edf\uff0c\u4f18\u5316\u5bf9\u8c61\u5b9a\u4f4d\u4e0e\u8def\u5f84\u89c4\u5212\u3002", "result": "\u5728\u9759\u6001\u548c\u52a8\u6001\u5bf9\u8c61\u67e5\u8be2\u4e2d\uff0c\u7cfb\u7edf\u68c0\u7d22\u6210\u529f\u7387\u66f4\u9ad8\uff0c\u8def\u5f84\u66f4\u77ed\uff0c\u5c24\u5176\u5728\u52a8\u6001\u5bf9\u8c61\u4e0a\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u7cfb\u7edf\u901a\u8fc7\u73af\u5883\u5730\u56fe\u4e0eLLM\u7ed3\u5408\uff0c\u6709\u6548\u63d0\u5347\u52a8\u6001\u5bf9\u8c61\u5bfc\u822a\u6027\u80fd\uff0c\u4e3a\u673a\u5668\u4eba\u5bfc\u822a\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.12801", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.12801", "abs": "https://arxiv.org/abs/2507.12801", "authors": ["Sosui Moribe", "Taketoshi Ushiama"], "title": "Imitating Mistakes in a Learning Companion AI Agent for Online Peer Learning", "comment": "This is the preprint version of the paper published in IMCOM 2025,\n  IEEE Xplore (DOI: 10.1109/IMCOM64595.2025.10857528)", "summary": "In recent years, peer learning has gained attention as a method that promotes\nspontaneous thinking among learners, and its effectiveness has been confirmed\nby numerous studies. This study aims to develop an AI Agent as a learning\ncompanion that enables peer learning anytime and anywhere. However, peer\nlearning between humans has various limitations, and it is not always\neffective. Effective peer learning requires companions at the same proficiency\nlevels. In this study, we assume that a learner's peers with the same\nproficiency level as the learner make the same mistakes as the learner does and\nfocus on English composition as a specific example to validate this approach.", "AI": {"tldr": "\u7814\u7a76\u5f00\u53d1AI\u5b66\u4e60\u4f34\u4fa3\u4ee5\u652f\u6301\u540c\u4f34\u5b66\u4e60\uff0c\u9a8c\u8bc1\u540c\u6c34\u5e73\u540c\u4f34\u5728\u82f1\u8bed\u5199\u4f5c\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u540c\u4f34\u5b66\u4e60\u867d\u6709\u6548\u4f46\u5b58\u5728\u9650\u5236\uff0c\u9700\u540c\u6c34\u5e73\u540c\u4f34\uff0cAI\u53ef\u7a81\u7834\u65f6\u7a7a\u9650\u5236\u3002", "method": "\u5047\u8bbe\u540c\u6c34\u5e73\u540c\u4f34\u4f1a\u72af\u76f8\u540c\u9519\u8bef\uff0c\u4ee5\u82f1\u8bed\u5199\u4f5c\u4e3a\u4f8b\u9a8c\u8bc1\u3002", "result": "\u672a\u660e\u786e\u63d0\u53ca\u5177\u4f53\u7ed3\u679c\u3002", "conclusion": "AI\u5b66\u4e60\u4f34\u4fa3\u6709\u671b\u89e3\u51b3\u540c\u4f34\u5b66\u4e60\u7684\u9650\u5236\u3002"}}
{"id": "2507.12800", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.12800", "abs": "https://arxiv.org/abs/2507.12800", "authors": ["Jikai Wang", "Yunqi Cheng", "Zonghai Chen"], "title": "FFI-VTR: Lightweight and Robust Visual Teach and Repeat Navigation based on Feature Flow Indicator and Probabilistic Motion Planning", "comment": null, "summary": "Though visual and repeat navigation is a convenient solution for mobile robot\nself-navigation, achieving balance between efficiency and robustness in task\nenvironment still remains challenges. In this paper, we propose a novel visual\nand repeat robotic autonomous navigation method that requires no accurate\nlocalization and dense reconstruction modules, which makes our system featured\nby lightweight and robustness. Firstly, feature flow is introduced and we\ndevelop a qualitative mapping between feature flow and robot's motion, in which\nfeature flow is defined as pixel location bias between matched features. Based\non the mapping model, the map outputted by the teaching phase is represented as\na keyframe graph, in which the feature flow on the edge encodes the relative\nmotion between adjacent keyframes. Secondly, the visual repeating navigation is\nessentially modeled as a feature flow minimization problem between current\nobservation and the map keyframe. To drive the robot to consistently reduce the\nfeature flow between current frame and map keyframes without accurate\nlocalization, a probabilistic motion planning is developed based on our\nqualitative feature flow-motion mapping indicator. Extensive experiments using\nour mobile platform demonstrates that our proposed method is lightweight,\nrobust, and superior to baselines. The source code has been made public at\nhttps://github.com/wangjks/FFI-VTR to benefit the community.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u4e14\u9c81\u68d2\u7684\u89c6\u89c9\u91cd\u590d\u81ea\u4e3b\u5bfc\u822a\u65b9\u6cd5\uff0c\u65e0\u9700\u7cbe\u786e\u5b9a\u4f4d\u548c\u5bc6\u96c6\u91cd\u5efa\u6a21\u5757\uff0c\u901a\u8fc7\u7279\u5f81\u6d41\u6700\u5c0f\u5316\u5b9e\u73b0\u5bfc\u822a\u3002", "motivation": "\u89e3\u51b3\u79fb\u52a8\u673a\u5668\u4eba\u5728\u89c6\u89c9\u91cd\u590d\u5bfc\u822a\u4e2d\u6548\u7387\u4e0e\u9c81\u68d2\u6027\u96be\u4ee5\u5e73\u8861\u7684\u95ee\u9898\u3002", "method": "\u5f15\u5165\u7279\u5f81\u6d41\u5e76\u5efa\u7acb\u5176\u4e0e\u673a\u5668\u4eba\u8fd0\u52a8\u7684\u5b9a\u6027\u6620\u5c04\uff0c\u5c06\u5730\u56fe\u8868\u793a\u4e3a\u5173\u952e\u5e27\u56fe\uff0c\u5bfc\u822a\u5efa\u6a21\u4e3a\u7279\u5f81\u6d41\u6700\u5c0f\u5316\u95ee\u9898\uff0c\u5f00\u53d1\u6982\u7387\u8fd0\u52a8\u89c4\u5212\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u8f7b\u91cf\u3001\u9c81\u68d2\u4e14\u4f18\u4e8e\u57fa\u7ebf\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u65e0\u9700\u7cbe\u786e\u5b9a\u4f4d\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u9c81\u68d2\u7684\u5bfc\u822a\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.12806", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.12806", "abs": "https://arxiv.org/abs/2507.12806", "authors": ["Zhiwei Liu", "Jielin Qiu", "Shiyu Wang", "Jianguo Zhang", "Zuxin Liu", "Roshan Ram", "Haolin Chen", "Weiran Yao", "Huan Wang", "Shelby Heinecke", "Silvio Savarese", "Caiming Xiong"], "title": "MCPEval: Automatic MCP-based Deep Evaluation for AI Agent Models", "comment": "https://github.com/SalesforceAIResearch/MCPEval", "summary": "The rapid rise of Large Language Models (LLMs)-based intelligent agents\nunderscores the need for robust, scalable evaluation frameworks. Existing\nmethods rely on static benchmarks and labor-intensive data collection, limiting\npractical assessment. We introduce \\oursystemname, an open-source Model Context\nProtocol (MCP)-based framework that automates end-to-end task generation and\ndeep evaluation of LLM agents across diverse domains. MCPEval standardizes\nmetrics, seamlessly integrates with native agent tools, and eliminates manual\neffort in building evaluation pipelines. Empirical results across five\nreal-world domains show its effectiveness in revealing nuanced, domain-specific\nperformance. We publicly release MCPEval\nhttps://github.com/SalesforceAIResearch/MCPEval to promote reproducible and\nstandardized LLM agent evaluation.", "AI": {"tldr": "MCPEval\u662f\u4e00\u4e2a\u57fa\u4e8e\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\uff08MCP\uff09\u7684\u5f00\u6e90\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u751f\u6210\u4efb\u52a1\u548c\u6df1\u5ea6\u8bc4\u4f30LLM\u667a\u80fd\u4ee3\u7406\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u9759\u6001\u57fa\u51c6\u548c\u4eba\u5de5\u6570\u636e\u6536\u96c6\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u4f9d\u8d56\u9759\u6001\u57fa\u51c6\u548c\u4eba\u5de5\u6570\u636e\u6536\u96c6\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u8bc4\u4f30\u7684\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002", "method": "MCPEval\u901a\u8fc7MCP\u534f\u8bae\u81ea\u52a8\u5316\u751f\u6210\u4efb\u52a1\u548c\u8bc4\u4f30\u6d41\u7a0b\uff0c\u6807\u51c6\u5316\u6307\u6807\u5e76\u96c6\u6210\u539f\u751f\u4ee3\u7406\u5de5\u5177\u3002", "result": "\u5728\u4e94\u4e2a\u5b9e\u9645\u9886\u57df\u4e2d\u7684\u5b9e\u8bc1\u7ed3\u679c\u8868\u660e\uff0cMCPEval\u80fd\u6709\u6548\u63ed\u793a\u9886\u57df\u7279\u5b9a\u7684\u6027\u80fd\u5dee\u5f02\u3002", "conclusion": "MCPEval\u4e3aLLM\u4ee3\u7406\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u91cd\u590d\u548c\u6807\u51c6\u5316\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.12846", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12846", "abs": "https://arxiv.org/abs/2507.12846", "authors": ["Muhammad Fadhil Ginting", "Dong-Ki Kim", "Xiangyun Meng", "Andrzej Reinke", "Bandi Jai Krishna", "Navid Kayhani", "Oriana Peltzer", "David D. Fan", "Amirreza Shaban", "Sung-Kyun Kim", "Mykel J. Kochenderfer", "Ali-akbar Agha-mohammadi", "Shayegan Omidshafiei"], "title": "Enter the Mind Palace: Reasoning and Planning for Long-term Active Embodied Question Answering", "comment": null, "summary": "As robots become increasingly capable of operating over extended periods --\nspanning days, weeks, and even months -- they are expected to accumulate\nknowledge of their environments and leverage this experience to assist humans\nmore effectively. This paper studies the problem of Long-term Active Embodied\nQuestion Answering (LA-EQA), a new task in which a robot must both recall past\nexperiences and actively explore its environment to answer complex,\ntemporally-grounded questions. Unlike traditional EQA settings, which typically\nfocus either on understanding the present environment alone or on recalling a\nsingle past observation, LA-EQA challenges an agent to reason over past,\npresent, and possible future states, deciding when to explore, when to consult\nits memory, and when to stop gathering observations and provide a final answer.\nStandard EQA approaches based on large models struggle in this setting due to\nlimited context windows, absence of persistent memory, and an inability to\ncombine memory recall with active exploration. To address this, we propose a\nstructured memory system for robots, inspired by the mind palace method from\ncognitive science. Our method encodes episodic experiences as scene-graph-based\nworld instances, forming a reasoning and planning algorithm that enables\ntargeted memory retrieval and guided navigation. To balance the\nexploration-recall trade-off, we introduce value-of-information-based stopping\ncriteria that determines when the agent has gathered sufficient information. We\nevaluate our method on real-world experiments and introduce a new benchmark\nthat spans popular simulation environments and actual industrial sites. Our\napproach significantly outperforms state-of-the-art baselines, yielding\nsubstantial gains in both answer accuracy and exploration efficiency.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u957f\u671f\u4e3b\u52a8\u5177\u8eab\u95ee\u7b54\uff08LA-EQA\uff09\u4efb\u52a1\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u8bb0\u5fc6\u7cfb\u7edf\u548c\u57fa\u4e8e\u4fe1\u606f\u4ef7\u503c\u7684\u505c\u6b62\u6807\u51c6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u95ee\u7b54\u80fd\u529b\u548c\u63a2\u7d22\u6548\u7387\u3002", "motivation": "\u968f\u7740\u673a\u5668\u4eba\u8fd0\u884c\u65f6\u95f4\u7684\u5ef6\u957f\uff0c\u5982\u4f55\u6709\u6548\u5229\u7528\u7d2f\u79ef\u7684\u73af\u5883\u77e5\u8bc6\u8f85\u52a9\u4eba\u7c7b\u6210\u4e3a\u5173\u952e\u95ee\u9898\u3002\u4f20\u7edfEQA\u65b9\u6cd5\u56e0\u4e0a\u4e0b\u6587\u7a97\u53e3\u6709\u9650\u3001\u7f3a\u4e4f\u6301\u4e45\u8bb0\u5fc6\u7b49\u95ee\u9898\uff0c\u96be\u4ee5\u5e94\u5bf9\u957f\u671f\u4efb\u52a1\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53d7\u8ba4\u77e5\u79d1\u5b66\u542f\u53d1\u7684\u7ed3\u6784\u5316\u8bb0\u5fc6\u7cfb\u7edf\uff0c\u5c06\u7ecf\u9a8c\u7f16\u7801\u4e3a\u573a\u666f\u56fe\u4e16\u754c\u5b9e\u4f8b\uff0c\u5e76\u8bbe\u8ba1\u57fa\u4e8e\u4fe1\u606f\u4ef7\u503c\u7684\u505c\u6b62\u6807\u51c6\u4ee5\u5e73\u8861\u63a2\u7d22\u4e0e\u8bb0\u5fc6\u68c0\u7d22\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u548c\u65b0\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u5728\u7b54\u6848\u51c6\u786e\u6027\u548c\u63a2\u7d22\u6548\u7387\u4e0a\u5747\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u7ed3\u6784\u5316\u8bb0\u5fc6\u7cfb\u7edf\u548c\u4fe1\u606f\u4ef7\u503c\u9a71\u52a8\u7684\u51b3\u7b56\u673a\u5236\u4e3a\u957f\u671f\u673a\u5668\u4eba\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u5177\u8eab\u95ee\u7b54\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2507.12820", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.12820", "abs": "https://arxiv.org/abs/2507.12820", "authors": ["Shiquan Wang", "Ruiyu Fang", "Zhongjiang He", "Shuangyong Song", "Yongxiang Li"], "title": "Emotional Support with LLM-based Empathetic Dialogue Generation", "comment": null, "summary": "Emotional Support Conversation (ESC) aims to provide empathetic and effective\nemotional assistance through dialogue, addressing the growing demand for mental\nhealth support. This paper presents our solution for the NLPCC 2025 Task 8 ESC\nevaluation, where we leverage large-scale language models enhanced by prompt\nengineering and finetuning techniques. We explore both parameter-efficient\nLow-Rank Adaptation and full-parameter fine-tuning strategies to improve the\nmodel's ability to generate supportive and contextually appropriate responses.\nOur best model ranked second in the competition, highlighting the potential of\ncombining LLMs with effective adaptation methods for ESC tasks. Future work\nwill focus on further enhancing emotional understanding and response\npersonalization to build more practical and reliable emotional support systems.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u5229\u7528\u5927\u89c4\u6a21\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\u63d0\u793a\u5de5\u7a0b\u548c\u5fae\u8c03\u6280\u672f\uff0c\u63d0\u5347\u60c5\u611f\u652f\u6301\u5bf9\u8bdd\uff08ESC\uff09\u4efb\u52a1\u8868\u73b0\u7684\u65b9\u6cd5\uff0c\u5e76\u5728NLPCC 2025 Task 8\u4e2d\u53d6\u5f97\u7b2c\u4e8c\u540d\u3002", "motivation": "\u6ee1\u8db3\u5fc3\u7406\u5065\u5eb7\u652f\u6301\u7684\u9700\u6c42\uff0c\u63d0\u4f9b\u66f4\u5177\u540c\u7406\u5fc3\u548c\u6709\u6548\u7684\u60c5\u611f\u652f\u6301\u5bf9\u8bdd\u3002", "method": "\u7ed3\u5408\u63d0\u793a\u5de5\u7a0b\u548c\u5fae\u8c03\u6280\u672f\uff0c\u63a2\u7d22\u53c2\u6570\u9ad8\u6548\u7684\u4f4e\u79e9\u9002\u5e94\u548c\u5168\u53c2\u6570\u5fae\u8c03\u7b56\u7565\u3002", "result": "\u6700\u4f73\u6a21\u578b\u5728\u7ade\u8d5b\u4e2d\u6392\u540d\u7b2c\u4e8c\uff0c\u9a8c\u8bc1\u4e86LLMs\u7ed3\u5408\u9002\u5e94\u65b9\u6cd5\u7684\u6f5c\u529b\u3002", "conclusion": "\u672a\u6765\u5de5\u4f5c\u5c06\u96c6\u4e2d\u4e8e\u63d0\u5347\u60c5\u611f\u7406\u89e3\u548c\u54cd\u5e94\u4e2a\u6027\u5316\uff0c\u4ee5\u6784\u5efa\u66f4\u5b9e\u7528\u53ef\u9760\u7684\u60c5\u611f\u652f\u6301\u7cfb\u7edf\u3002"}}
{"id": "2507.12855", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.12855", "abs": "https://arxiv.org/abs/2507.12855", "authors": ["Rahel Rickenbach", "Bruce Lee", "Ren\u00e9 Zurbr\u00fcgg", "Carmen Amo Alonso", "Melanie N. Zeilinger"], "title": "DEMONSTRATE: Zero-shot Language to Robotic Control via Multi-task Demonstration Learning", "comment": null, "summary": "The integration of large language models (LLMs) with control systems has\ndemonstrated significant potential in various settings, such as task completion\nwith a robotic manipulator. A main reason for this success is the ability of\nLLMs to perform in-context learning, which, however, strongly relies on the\ndesign of task examples, closely related to the target tasks. Consequently,\nemploying LLMs to formulate optimal control problems often requires task\nexamples that contain explicit mathematical expressions, designed by trained\nengineers. Furthermore, there is often no principled way to evaluate for\nhallucination before task execution. To address these challenges, we propose\nDEMONSTRATE, a novel methodology that avoids the use of LLMs for complex\noptimization problem generations, and instead only relies on the embedding\nrepresentations of task descriptions. To do this, we leverage tools from\ninverse optimal control to replace in-context prompt examples with task\ndemonstrations, as well as the concept of multitask learning, which ensures\ntarget and example task similarity by construction. Given the fact that\nhardware demonstrations can easily be collected using teleoperation or guidance\nof the robot, our approach significantly reduces the reliance on engineering\nexpertise for designing in-context examples. Furthermore, the enforced\nmultitask structure enables learning from few demonstrations and assessment of\nhallucinations prior to task execution. We demonstrate the effectiveness of our\nmethod through simulation and hardware experiments involving a robotic arm\ntasked with tabletop manipulation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDEMONSTRATE\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u4efb\u52a1\u63cf\u8ff0\u7684\u5d4c\u5165\u8868\u793a\u548c\u9006\u6700\u4f18\u63a7\u5236\u5de5\u5177\uff0c\u51cf\u5c11\u5bf9LLMs\u751f\u6210\u590d\u6742\u4f18\u5316\u95ee\u9898\u7684\u4f9d\u8d56\uff0c\u540c\u65f6\u901a\u8fc7\u591a\u4efb\u52a1\u5b66\u4e60\u786e\u4fdd\u4efb\u52a1\u76f8\u4f3c\u6027\u3002", "motivation": "\u5f53\u524dLLMs\u5728\u63a7\u5236\u7cfb\u7edf\u4e2d\u4f9d\u8d56\u4e0a\u4e0b\u6587\u5b66\u4e60\uff0c\u9700\u8981\u8bbe\u8ba1\u590d\u6742\u7684\u4efb\u52a1\u793a\u4f8b\uff0c\u4e14\u7f3a\u4e4f\u8bc4\u4f30\u5e7b\u89c9\u7684\u65b9\u6cd5\u3002DEMONSTRATE\u65e8\u5728\u51cf\u5c11\u5bf9\u5de5\u7a0b\u4e13\u4e1a\u77e5\u8bc6\u7684\u9700\u6c42\uff0c\u5e76\u63d0\u524d\u8bc4\u4f30\u5e7b\u89c9\u3002", "method": "\u5229\u7528\u9006\u6700\u4f18\u63a7\u5236\u5de5\u5177\u548c\u591a\u4efb\u52a1\u5b66\u4e60\uff0c\u7528\u4efb\u52a1\u6f14\u793a\u66ff\u4ee3\u4e0a\u4e0b\u6587\u63d0\u793a\u793a\u4f8b\uff0c\u901a\u8fc7\u786c\u4ef6\u6f14\u793a\uff08\u5982\u9065\u64cd\u4f5c\uff09\u6536\u96c6\u6570\u636e\u3002", "result": "\u5728\u6a21\u62df\u548c\u786c\u4ef6\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u6709\u6548\u51cf\u5c11\u4e86LLMs\u7684\u4f9d\u8d56\uff0c\u5e76\u80fd\u5728\u4efb\u52a1\u6267\u884c\u524d\u8bc4\u4f30\u5e7b\u89c9\u3002", "conclusion": "DEMONSTRATE\u65b9\u6cd5\u901a\u8fc7\u4efb\u52a1\u6f14\u793a\u548c\u591a\u4efb\u52a1\u5b66\u4e60\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5bf9\u5de5\u7a0b\u8bbe\u8ba1\u7684\u4f9d\u8d56\uff0c\u5e76\u63d0\u9ad8\u4e86\u4efb\u52a1\u6267\u884c\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2507.12821", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12821", "abs": "https://arxiv.org/abs/2507.12821", "authors": ["Lance Ying", "Katherine M. Collins", "Prafull Sharma", "Cedric Colas", "Kaiya Ivy Zhao", "Adrian Weller", "Zenna Tavares", "Phillip Isola", "Samuel J. Gershman", "Jacob D. Andreas", "Thomas L. Griffiths", "Francois Chollet", "Kelsey R. Allen", "Joshua B. Tenenbaum"], "title": "Assessing adaptive world models in machines with novel games", "comment": "17 pages, 4 figures", "summary": "Human intelligence exhibits a remarkable capacity for rapid adaptation and\neffective problem-solving in novel and unfamiliar contexts. We argue that this\nprofound adaptability is fundamentally linked to the efficient construction and\nrefinement of internal representations of the environment, commonly referred to\nas world models, and we refer to this adaptation mechanism as world model\ninduction. However, current understanding and evaluation of world models in\nartificial intelligence (AI) remains narrow, often focusing on static\nrepresentations learned from training on a massive corpora of data, instead of\nthe efficiency and efficacy of models in learning these representations through\ninteraction and exploration within a novel environment. In this Perspective, we\nprovide a view of world model induction drawing on decades of research in\ncognitive science on how humans learn and adapt so efficiently; we then call\nfor a new evaluation framework for assessing adaptive world models in AI.\nConcretely, we propose a new benchmarking paradigm based on suites of carefully\ndesigned games with genuine, deep and continually refreshing novelty in the\nunderlying game structures -- we refer to this kind of games as novel games. We\ndetail key desiderata for constructing these games and propose appropriate\nmetrics to explicitly challenge and evaluate the agent's ability for rapid\nworld model induction. We hope that this new evaluation framework will inspire\nfuture evaluation efforts on world models in AI and provide a crucial step\ntowards developing AI systems capable of the human-like rapid adaptation and\nrobust generalization -- a critical component of artificial general\nintelligence.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4eba\u7c7b\u667a\u80fd\u7684\u5feb\u901f\u9002\u5e94\u80fd\u529b\u4e0e\u9ad8\u6548\u6784\u5efa\u548c\u4f18\u5316\u4e16\u754c\u6a21\u578b\u76f8\u5173\uff0c\u547c\u5401\u5728AI\u4e2d\u5efa\u7acb\u65b0\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u8bbe\u8ba1\u65b0\u9896\u6e38\u620f\u6765\u6d4b\u8bd5\u4e16\u754c\u6a21\u578b\u8bf1\u5bfc\u80fd\u529b\u3002", "motivation": "\u5f53\u524dAI\u5bf9\u4e16\u754c\u6a21\u578b\u7684\u7406\u89e3\u548c\u8bc4\u4f30\u8fc7\u4e8e\u9759\u6001\uff0c\u672a\u80fd\u4f53\u73b0\u4eba\u7c7b\u901a\u8fc7\u4e92\u52a8\u548c\u63a2\u7d22\u9ad8\u6548\u5b66\u4e60\u7684\u80fd\u529b\u3002", "method": "\u501f\u9274\u8ba4\u77e5\u79d1\u5b66\u7814\u7a76\uff0c\u63d0\u51fa\u57fa\u4e8e\u65b0\u9896\u6e38\u620f\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u8bbe\u8ba1\u5177\u6709\u6301\u7eed\u65b0\u5947\u7684\u6e38\u620f\u5e76\u5236\u5b9a\u76f8\u5e94\u6307\u6807\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u8303\u5f0f\uff0c\u65e8\u5728\u6311\u6218\u548c\u8bc4\u4f30AI\u4ee3\u7406\u7684\u5feb\u901f\u4e16\u754c\u6a21\u578b\u8bf1\u5bfc\u80fd\u529b\u3002", "conclusion": "\u5e0c\u671b\u8be5\u6846\u67b6\u80fd\u63a8\u52a8AI\u4e16\u754c\u6a21\u578b\u8bc4\u4f30\uff0c\u52a9\u529b\u5b9e\u73b0\u7c7b\u4eba\u5feb\u901f\u9002\u5e94\u548c\u9c81\u68d2\u6cdb\u5316\u7684AGI\u53d1\u5c55\u3002"}}
{"id": "2507.12911", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12911", "abs": "https://arxiv.org/abs/2507.12911", "authors": ["Hayeon Oh"], "title": "LaViPlan : Language-Guided Visual Path Planning with RLVR", "comment": "11 pages, 6 figures", "summary": "Out-of-distribution (OOD) scenarios in autonomous driving refer to situations\nthat deviate from the training domain, often leading to unexpected and\npotentially hazardous behavior from planners that lack prior exposure to such\ncases. Recently, Vision-Language Models (VLMs) have been introduced into\nautonomous driving research for their promising generalization capabilities in\nOOD settings. Early studies demonstrated that VLMs could recognize OOD\nscenarios and generate user-level decisions such as \"go straight\" or \"turn\nright.\" However, a new challenge has emerged due to the misalignment between\nthe VLM's high-level decisions or visual reasoning expressed in language, and\nthe low-level predicted trajectories interpreted as actions. In this paper, we\npropose LaViPlan, a framework that leverages Reinforcement Learning with\nVerifiable Rewards (RLVR) to optimize VLMs using planning-oriented metrics.\nThis approach addresses the vision-language-action misalignment observed in\nexisting VLMs fine-tuned via supervised learning, which can recognize driving\nscenarios but often produce context-unaware decisions. Experimental results\ndemonstrate that our method improves situational awareness and decision-making\nunder OOD conditions, highlighting its potential to mitigate the misalignment\nissue. This work introduces a promising post-training paradigm for VLM agents\nin the context of autonomous driving.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faLaViPlan\u6846\u67b6\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u4f18\u5316\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\uff0c\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u4e2d\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u4e0d\u5bf9\u9f50\u95ee\u9898\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u4e2d\uff0c\u5206\u5e03\u5916\uff08OOD\uff09\u573a\u666f\u53ef\u80fd\u5bfc\u81f4\u5371\u9669\u884c\u4e3a\uff0c\u73b0\u6709VLM\u867d\u80fd\u8bc6\u522b\u573a\u666f\u4f46\u51b3\u7b56\u7f3a\u4e4f\u4e0a\u4e0b\u6587\u611f\u77e5\u3002", "method": "\u91c7\u7528\u5e26\u53ef\u9a8c\u8bc1\u5956\u52b1\u7684\u5f3a\u5316\u5b66\u4e60\uff08RLVR\uff09\u4f18\u5316VLM\uff0c\u4ee5\u89c4\u5212\u4e3a\u5bfc\u5411\u7684\u6307\u6807\u89e3\u51b3\u4e0d\u5bf9\u9f50\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u63d0\u5347\u4e86OOD\u6761\u4ef6\u4e0b\u7684\u60c5\u5883\u611f\u77e5\u548c\u51b3\u7b56\u80fd\u529b\u3002", "conclusion": "LaViPlan\u4e3a\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684VLM\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u540e\u8bad\u7ec3\u8303\u5f0f\u3002"}}
{"id": "2507.12862", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.12862", "abs": "https://arxiv.org/abs/2507.12862", "authors": ["Hussein Abbass", "Taylan Akay", "Harrison Tolley"], "title": "Information-Theoretic Aggregation of Ethical Attributes in Simulated-Command", "comment": null, "summary": "In the age of AI, human commanders need to use the computational powers\navailable in today's environment to simulate a very large number of scenarios.\nWithin each scenario, situations occur where different decision design options\ncould have ethical consequences. Making these decisions reliant on human\njudgement is both counter-productive to the aim of exploring very large number\nof scenarios in a timely manner and infeasible when considering the workload\nneeded to involve humans in each of these choices. In this paper, we move human\njudgement outside the simulation decision cycle. Basically, the human will\ndesign the ethical metric space, leaving it to the simulated environment to\nexplore the space. When the simulation completes its testing cycles, the\ntesting environment will come back to the human commander with a few options to\nselect from. The human commander will then exercise human-judgement to select\nthe most appropriate course of action, which will then get executed\naccordingly. We assume that the problem of designing metrics that are\nsufficiently granular to assess the ethical implications of decisions is\nsolved. Subsequently, the fundamental problem we look at in this paper is how\nto weight ethical decisions during the running of these simulations; that is,\nhow to dynamically weight the ethical attributes when agents are faced with\ndecision options with ethical implications during generative simulations. The\nmulti-criteria decision making literature has started to look at nearby\nproblems, where the concept of entropy has been used to determine the weights\nduring aggregation. We draw from that literature different approaches to\nautomatically calculate the weights for ethical attributes during\nsimulation-based testing and evaluation.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e00\u79cd\u65b9\u6cd5\uff0c\u5c06\u4eba\u7c7b\u5224\u65ad\u4ece\u6a21\u62df\u51b3\u7b56\u5faa\u73af\u4e2d\u79fb\u51fa\uff0c\u8bbe\u8ba1\u4f26\u7406\u5ea6\u91cf\u7a7a\u95f4\uff0c\u7531\u6a21\u62df\u73af\u5883\u63a2\u7d22\uff0c\u6700\u540e\u4eba\u7c7b\u6307\u6325\u5b98\u4ece\u5c11\u6570\u9009\u9879\u4e2d\u9009\u62e9\u6700\u4f73\u884c\u52a8\u3002", "motivation": "\u5728AI\u65f6\u4ee3\uff0c\u4eba\u7c7b\u6307\u6325\u5b98\u9700\u5229\u7528\u8ba1\u7b97\u80fd\u529b\u6a21\u62df\u5927\u91cf\u573a\u666f\uff0c\u4f46\u4f9d\u8d56\u4eba\u7c7b\u5224\u65ad\u6bcf\u4e2a\u51b3\u7b56\u65e2\u4f4e\u6548\u53c8\u4e0d\u53ef\u884c\u3002", "method": "\u4eba\u7c7b\u8bbe\u8ba1\u4f26\u7406\u5ea6\u91cf\u7a7a\u95f4\uff0c\u6a21\u62df\u73af\u5883\u63a2\u7d22\u8be5\u7a7a\u95f4\u5e76\u751f\u6210\u9009\u9879\uff0c\u4eba\u7c7b\u6307\u6325\u5b98\u4ece\u4e2d\u9009\u62e9\u6700\u4f73\u884c\u52a8\u3002", "result": "\u901a\u8fc7\u52a8\u6001\u52a0\u6743\u4f26\u7406\u5c5e\u6027\uff0c\u6a21\u62df\u73af\u5883\u80fd\u9ad8\u6548\u751f\u6210\u9009\u9879\uff0c\u4eba\u7c7b\u6307\u6325\u5b98\u6700\u7ec8\u505a\u51fa\u51b3\u7b56\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u6a21\u62df\u4e2d\u4f26\u7406\u51b3\u7b56\u7684\u6548\u7387\u548c\u53ef\u884c\u6027\u95ee\u9898\u3002"}}
{"id": "2507.12920", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.12920", "abs": "https://arxiv.org/abs/2507.12920", "authors": ["Zichao Shu", "Shitao Bei", "Jicheng Dai", "Lijun Li", "Zetao Chen"], "title": "MoCap2GT: A High-Precision Ground Truth Estimator for SLAM Benchmarking Based on Motion Capture and IMU Fusion", "comment": null, "summary": "Marker-based optical motion capture (MoCap) systems are widely used to\nprovide ground truth (GT) trajectories for benchmarking SLAM algorithms.\nHowever, the accuracy of MoCap-based GT trajectories is mainly affected by two\nfactors: spatiotemporal calibration errors between the MoCap system and the\ndevice under test (DUT), and inherent MoCap jitter. Consequently, existing\nbenchmarks focus primarily on absolute translation error, as accurate\nassessment of rotation and inter-frame errors remains challenging, hindering\nthorough SLAM evaluation. This paper proposes MoCap2GT, a joint optimization\napproach that integrates MoCap data and inertial measurement unit (IMU)\nmeasurements from the DUT for generating high-precision GT trajectories.\nMoCap2GT includes a robust state initializer to ensure global convergence,\nintroduces a higher-order B-spline pose parameterization on the SE(3) manifold\nwith variable time offset to effectively model MoCap factors, and employs a\ndegeneracy-aware measurement rejection strategy to enhance estimation accuracy.\nExperimental results demonstrate that MoCap2GT outperforms existing methods and\nsignificantly contributes to precise SLAM benchmarking. The source code is\navailable at https://anonymous.4open.science/r/mocap2gt (temporarily hosted\nanonymously for double-blind review).", "AI": {"tldr": "MoCap2GT\u662f\u4e00\u79cd\u8054\u5408\u4f18\u5316\u65b9\u6cd5\uff0c\u7ed3\u5408MoCap\u6570\u636e\u548cIMU\u6d4b\u91cf\uff0c\u751f\u6210\u9ad8\u7cbe\u5ea6\u5730\u9762\u771f\u5b9e\u8f68\u8ff9\uff0c\u7528\u4e8eSLAM\u7b97\u6cd5\u7684\u7cbe\u786e\u8bc4\u4f30\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eMoCap\u7684\u5730\u9762\u771f\u5b9e\u8f68\u8ff9\u5b58\u5728\u65f6\u7a7a\u6821\u51c6\u8bef\u5dee\u548c\u56fa\u6709\u6296\u52a8\u95ee\u9898\uff0c\u9650\u5236\u4e86SLAM\u7b97\u6cd5\u7684\u5168\u9762\u8bc4\u4f30\u3002", "method": "\u63d0\u51faMoCap2GT\uff0c\u5305\u62ec\u9c81\u68d2\u72b6\u6001\u521d\u59cb\u5316\u5668\u3001\u9ad8\u9636B\u6837\u6761\u59ff\u6001\u53c2\u6570\u5316\u548c\u9000\u5316\u611f\u77e5\u6d4b\u91cf\u62d2\u7edd\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u8868\u660eMoCap2GT\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86SLAM\u8bc4\u4f30\u7684\u7cbe\u5ea6\u3002", "conclusion": "MoCap2GT\u4e3aSLAM\u57fa\u51c6\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u9ad8\u7cbe\u5ea6\u7684\u5730\u9762\u771f\u5b9e\u8f68\u8ff9\uff0c\u4ee3\u7801\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.12872", "categories": ["cs.AI", "cs.CR", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.12872", "abs": "https://arxiv.org/abs/2507.12872", "authors": ["Rishane Dassanayake", "Mario Demetroudi", "James Walpole", "Lindley Lentati", "Jason R. Brown", "Edward James Young"], "title": "Manipulation Attacks by Misaligned AI: Risk Analysis and Safety Case Framework", "comment": "24 pages (14 pages main text, 4 pages bibliography, 6 pages\n  appendices), 3 figures", "summary": "Frontier AI systems are rapidly advancing in their capabilities to persuade,\ndeceive, and influence human behaviour, with current models already\ndemonstrating human-level persuasion and strategic deception in specific\ncontexts. Humans are often the weakest link in cybersecurity systems, and a\nmisaligned AI system deployed internally within a frontier company may seek to\nundermine human oversight by manipulating employees. Despite this growing\nthreat, manipulation attacks have received little attention, and no systematic\nframework exists for assessing and mitigating these risks. To address this, we\nprovide a detailed explanation of why manipulation attacks are a significant\nthreat and could lead to catastrophic outcomes. Additionally, we present a\nsafety case framework for manipulation risk, structured around three core lines\nof argument: inability, control, and trustworthiness. For each argument, we\nspecify evidence requirements, evaluation methodologies, and implementation\nconsiderations for direct application by AI companies. This paper provides the\nfirst systematic methodology for integrating manipulation risk into AI safety\ngovernance, offering AI companies a concrete foundation to assess and mitigate\nthese threats before deployment.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u524d\u6cbfAI\u7cfb\u7edf\u5728\u8bf4\u670d\u3001\u6b3a\u9a97\u548c\u5f71\u54cd\u4eba\u7c7b\u884c\u4e3a\u65b9\u9762\u7684\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7cfb\u7edf\u6027\u6846\u67b6\u6765\u8bc4\u4f30\u548c\u51cf\u8f7b\u64cd\u7eb5\u98ce\u9669\u3002", "motivation": "\u968f\u7740AI\u7cfb\u7edf\u80fd\u529b\u7684\u63d0\u5347\uff0c\u64cd\u7eb5\u653b\u51fb\u5bf9\u4eba\u7c7b\u76d1\u7763\u7684\u5a01\u80c1\u65e5\u76ca\u4e25\u91cd\uff0c\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u6027\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u56f4\u7ed5\u4e09\u4e2a\u6838\u5fc3\u8bba\u70b9\uff08\u65e0\u80fd\u3001\u63a7\u5236\u548c\u53ef\u4fe1\u5ea6\uff09\u7684\u5b89\u5168\u6848\u4f8b\u6846\u67b6\uff0c\u5e76\u8be6\u7ec6\u8bf4\u660e\u4e86\u8bc1\u636e\u8981\u6c42\u3001\u8bc4\u4f30\u65b9\u6cd5\u548c\u5b9e\u65bd\u8003\u8651\u3002", "result": "\u4e3aAI\u516c\u53f8\u63d0\u4f9b\u4e86\u9996\u4e2a\u7cfb\u7edf\u6027\u65b9\u6cd5\uff0c\u5c06\u64cd\u7eb5\u98ce\u9669\u7eb3\u5165AI\u5b89\u5168\u6cbb\u7406\uff0c\u4ee5\u8bc4\u4f30\u548c\u51cf\u8f7b\u5a01\u80c1\u3002", "conclusion": "\u672c\u6587\u4e3aAI\u516c\u53f8\u63d0\u4f9b\u4e86\u8bc4\u4f30\u548c\u51cf\u8f7b\u64cd\u7eb5\u98ce\u9669\u7684\u5177\u4f53\u57fa\u7840\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002"}}
{"id": "2507.12977", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.12977", "abs": "https://arxiv.org/abs/2507.12977", "authors": ["Giwon Lee", "Daehee Park", "Jaewoo Jeong", "Kuk-Jin Yoon"], "title": "Non-differentiable Reward Optimization for Diffusion-based Autonomous Motion Planning", "comment": "Accepted at IROS 2025", "summary": "Safe and effective motion planning is crucial for autonomous robots.\nDiffusion models excel at capturing complex agent interactions, a fundamental\naspect of decision-making in dynamic environments. Recent studies have\nsuccessfully applied diffusion models to motion planning, demonstrating their\ncompetence in handling complex scenarios and accurately predicting multi-modal\nfuture trajectories. Despite their effectiveness, diffusion models have\nlimitations in training objectives, as they approximate data distributions\nrather than explicitly capturing the underlying decision-making dynamics.\nHowever, the crux of motion planning lies in non-differentiable downstream\nobjectives, such as safety (collision avoidance) and effectiveness\n(goal-reaching), which conventional learning algorithms cannot directly\noptimize. In this paper, we propose a reinforcement learning-based training\nscheme for diffusion motion planning models, enabling them to effectively learn\nnon-differentiable objectives that explicitly measure safety and effectiveness.\nSpecifically, we introduce a reward-weighted dynamic thresholding algorithm to\nshape a dense reward signal, facilitating more effective training and\noutperforming models trained with differentiable objectives. State-of-the-art\nperformance on pedestrian datasets (CrowdNav, ETH-UCY) compared to various\nbaselines demonstrates the versatility of our approach for safe and effective\nmotion planning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u6269\u6563\u8fd0\u52a8\u89c4\u5212\u6a21\u578b\u8bad\u7ec3\u65b9\u6848\uff0c\u4ee5\u4f18\u5316\u975e\u53ef\u5fae\u76ee\u6807\uff08\u5982\u5b89\u5168\u6027\u548c\u6709\u6548\u6027\uff09\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u5728\u8fd0\u52a8\u89c4\u5212\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u65e0\u6cd5\u76f4\u63a5\u4f18\u5316\u975e\u53ef\u5fae\u76ee\u6807\uff08\u5982\u78b0\u649e\u907f\u514d\u548c\u76ee\u6807\u8fbe\u6210\uff09\uff0c\u56e0\u6b64\u9700\u8981\u6539\u8fdb\u8bad\u7ec3\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6269\u6563\u6a21\u578b\uff0c\u5f15\u5165\u5956\u52b1\u52a0\u6743\u52a8\u6001\u9608\u503c\u7b97\u6cd5\u4ee5\u4f18\u5316\u975e\u53ef\u5fae\u76ee\u6807\u3002", "result": "\u5728\u884c\u4eba\u6570\u636e\u96c6\uff08CrowdNav, ETH-UCY\uff09\u4e0a\u5b9e\u73b0\u4e86\u6700\u4f18\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u8fd0\u52a8\u89c4\u5212\u7684\u5b89\u5168\u6027\u548c\u6709\u6548\u6027\uff0c\u9002\u7528\u4e8e\u590d\u6742\u52a8\u6001\u73af\u5883\u3002"}}
{"id": "2507.12885", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.12885", "abs": "https://arxiv.org/abs/2507.12885", "authors": ["Jian Yao", "Ran Cheng", "Kay Chen Tan"], "title": "VAR-MATH: Probing True Mathematical Reasoning in Large Language Models via Symbolic Multi-Instance Benchmarks", "comment": null, "summary": "Recent advances in reinforcement learning (RL) have led to substantial\nimprovements in the mathematical reasoning abilities of large language models\n(LLMs), as measured by standard benchmarks. However, these gains often persist\neven when models are trained with flawed signals, such as random or inverted\nrewards, raising a fundamental question: do such improvements reflect true\nreasoning, or are they merely artifacts of overfitting to benchmark-specific\npatterns? To address this question, we take an evaluation-centric perspective\nand identify two critical shortcomings in existing protocols. First,\n\\emph{benchmark contamination} arises from the public availability of test\nproblems, increasing the risk of data leakage. Second, \\emph{evaluation\nfragility} stems from the reliance on single-instance assessments, which are\nhighly sensitive to stochastic outputs and fail to capture reasoning\nconsistency. To overcome these limitations, we introduce {VAR-MATH}, a symbolic\nevaluation framework designed to probe genuine reasoning ability. By converting\nfixed numerical problems into symbolic templates and requiring models to solve\nmultiple instantiations of each, VAR-MATH enforces consistent reasoning across\nstructurally equivalent variants, thereby mitigating contamination and\nimproving evaluation robustness. We apply VAR-MATH to transform two popular\nbenchmarks, AMC23 and AIME24, into their symbolic counterparts, VAR-AMC23 and\nVAR-AIME24. Experimental results reveal substantial performance drops for\nRL-trained models on the variabilized versions, especially for smaller models,\nwith average declines of 48.0\\% on AMC23 and 58.3\\% on AIME24. These findings\nsuggest that many existing RL methods rely on superficial heuristics and fail\nto generalize beyond specific numerical forms. Overall, VAR-MATH offers a\nprincipled, contamination-resistant evaluation paradigm for mathematical\nreasoning.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u5728\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u6570\u5b66\u63a8\u7406\u80fd\u529b\u65f6\u53ef\u80fd\u5b58\u5728\u7684\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7b26\u53f7\u5316\u8bc4\u4f30\u6846\u67b6VAR-MATH\u4ee5\u89e3\u51b3\u73b0\u6709\u8bc4\u4f30\u534f\u8bae\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709RL\u65b9\u6cd5\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u63d0\u5347\u53ef\u80fd\u4ec5\u662f\u9488\u5bf9\u7279\u5b9a\u57fa\u51c6\u6d4b\u8bd5\u7684\u8fc7\u62df\u5408\u7ed3\u679c\uff0c\u800c\u975e\u771f\u6b63\u7684\u63a8\u7406\u80fd\u529b\u63d0\u5347\u3002", "method": "\u63d0\u51faVAR-MATH\u6846\u67b6\uff0c\u5c06\u56fa\u5b9a\u6570\u503c\u95ee\u9898\u8f6c\u6362\u4e3a\u7b26\u53f7\u6a21\u677f\uff0c\u8981\u6c42\u6a21\u578b\u89e3\u51b3\u591a\u4e2a\u53d8\u4f53\u4ee5\u9a8c\u8bc1\u63a8\u7406\u4e00\u81f4\u6027\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cRL\u8bad\u7ec3\u6a21\u578b\u5728\u7b26\u53f7\u5316\u7248\u672c\uff08VAR-AMC23\u548cVAR-AIME24\uff09\u4e0a\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u8868\u660e\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u8868\u9762\u542f\u53d1\u5f0f\u3002", "conclusion": "VAR-MATH\u63d0\u4f9b\u4e86\u4e00\u79cd\u6297\u6c61\u67d3\u3001\u66f4\u7a33\u5065\u7684\u6570\u5b66\u63a8\u7406\u8bc4\u4f30\u8303\u5f0f\u3002"}}
{"id": "2507.12986", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.12986", "abs": "https://arxiv.org/abs/2507.12986", "authors": ["Sepeedeh Shahbeigi", "Nawshin Mannan Proma", "Victoria Hodge", "Richard Hawkins", "Boda Li", "Valentina Donzella"], "title": "Robustness Requirement Coverage using a Situation Coverage Approach for Vision-based AI Systems", "comment": "4 pages, 1 figure", "summary": "AI-based robots and vehicles are expected to operate safely in complex and\ndynamic environments, even in the presence of component degradation. In such\nsystems, perception relies on sensors such as cameras to capture environmental\ndata, which is then processed by AI models to support decision-making. However,\ndegradation in sensor performance directly impacts input data quality and can\nimpair AI inference. Specifying safety requirements for all possible sensor\ndegradation scenarios leads to unmanageable complexity and inevitable gaps. In\nthis position paper, we present a novel framework that integrates camera noise\nfactor identification with situation coverage analysis to systematically elicit\nrobustness-related safety requirements for AI-based perception systems. We\nfocus specifically on camera degradation in the automotive domain. Building on\nan existing framework for identifying degradation modes, we propose involving\ndomain, sensor, and safety experts, and incorporating Operational Design Domain\nspecifications to extend the degradation model by incorporating noise factors\nrelevant to AI performance. Situation coverage analysis is then applied to\nidentify representative operational contexts. This work marks an initial step\ntoward integrating noise factor analysis and situational coverage to support\nprincipled formulation and completeness assessment of robustness requirements\nfor camera-based AI perception.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u76f8\u673a\u566a\u58f0\u56e0\u5b50\u8bc6\u522b\u4e0e\u60c5\u5883\u8986\u76d6\u5206\u6790\u7684\u65b0\u6846\u67b6\uff0c\u4ee5\u7cfb\u7edf\u6027\u63d0\u53d6\u57fa\u4e8eAI\u7684\u611f\u77e5\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u5b89\u5168\u9700\u6c42\u3002", "motivation": "\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\uff0c\u4f20\u611f\u5668\u6027\u80fd\u9000\u5316\u76f4\u63a5\u5f71\u54cdAI\u63a8\u7406\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5168\u9762\u8986\u76d6\u6240\u6709\u9000\u5316\u573a\u666f\u7684\u5b89\u5168\u9700\u6c42\u3002", "method": "\u6574\u5408\u76f8\u673a\u566a\u58f0\u56e0\u5b50\u8bc6\u522b\u4e0e\u60c5\u5883\u8986\u76d6\u5206\u6790\uff0c\u7ed3\u5408\u9886\u57df\u3001\u4f20\u611f\u5668\u548c\u5b89\u5168\u4e13\u5bb6\u77e5\u8bc6\uff0c\u6269\u5c55\u9000\u5316\u6a21\u578b\u5e76\u8bc6\u522b\u4ee3\u8868\u6027\u64cd\u4f5c\u60c5\u5883\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u6027\u65b9\u6cd5\uff0c\u652f\u6301\u76f8\u673a\u9000\u5316\u573a\u666f\u4e0bAI\u611f\u77e5\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u9700\u6c42\u5236\u5b9a\u4e0e\u5b8c\u6574\u6027\u8bc4\u4f30\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u76f8\u673a\u9000\u5316\u573a\u666f\u4e0b\u7684AI\u611f\u77e5\u7cfb\u7edf\u9c81\u68d2\u6027\u9700\u6c42\u63d0\u4f9b\u4e86\u521d\u6b65\u89e3\u51b3\u65b9\u6848\uff0c\u672a\u6765\u53ef\u8fdb\u4e00\u6b65\u6269\u5c55\u3002"}}
{"id": "2507.12989", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.12989", "abs": "https://arxiv.org/abs/2507.12989", "authors": ["Lyris Xu", "Fabio Aurelio D'Asaro", "Luke Dickens"], "title": "A Translation of Probabilistic Event Calculus into Markov Decision Processes", "comment": null, "summary": "Probabilistic Event Calculus (PEC) is a logical framework for reasoning about\nactions and their effects in uncertain environments, which enables the\nrepresentation of probabilistic narratives and computation of temporal\nprojections. The PEC formalism offers significant advantages in\ninterpretability and expressiveness for narrative reasoning. However, it lacks\nmechanisms for goal-directed reasoning. This paper bridges this gap by\ndeveloping a formal translation of PEC domains into Markov Decision Processes\n(MDPs), introducing the concept of \"action-taking situations\" to preserve PEC's\nflexible action semantics. The resulting PEC-MDP formalism enables the\nextensive collection of algorithms and theoretical tools developed for MDPs to\nbe applied to PEC's interpretable narrative domains. We demonstrate how the\ntranslation supports both temporal reasoning tasks and objective-driven\nplanning, with methods for mapping learned policies back into human-readable\nPEC representations, maintaining interpretability while extending PEC's\ncapabilities.", "AI": {"tldr": "\u5c06\u6982\u7387\u4e8b\u4ef6\u6f14\u7b97\uff08PEC\uff09\u8f6c\u5316\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDP\uff09\uff0c\u4ee5\u652f\u6301\u76ee\u6807\u5bfc\u5411\u63a8\u7406\uff0c\u540c\u65f6\u4fdd\u6301PEC\u7684\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "PEC\u5728\u4e0d\u786e\u5b9a\u73af\u5883\u4e2d\u5177\u6709\u4f18\u52bf\uff0c\u4f46\u7f3a\u4e4f\u76ee\u6807\u5bfc\u5411\u63a8\u7406\u673a\u5236\uff0c\u9700\u7ed3\u5408MDP\u7684\u7b97\u6cd5\u548c\u5de5\u5177\u6269\u5c55\u5176\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u201c\u52a8\u4f5c\u6267\u884c\u60c5\u5883\u201d\u6982\u5ff5\uff0c\u5c06PEC\u9886\u57df\u5f62\u5f0f\u5316\u8f6c\u5316\u4e3aMDP\uff0c\u4fdd\u7559PEC\u7684\u7075\u6d3b\u52a8\u4f5c\u8bed\u4e49\u3002", "result": "PEC-MDP\u5f62\u5f0f\u5316\u652f\u6301\u65f6\u95f4\u63a8\u7406\u548c\u76ee\u6807\u9a71\u52a8\u89c4\u5212\uff0c\u5e76\u80fd\u5c06\u5b66\u4e60\u7b56\u7565\u6620\u5c04\u56de\u53ef\u8bfb\u7684PEC\u8868\u793a\u3002", "conclusion": "PEC-MDP\u7ed3\u5408\u4e86PEC\u7684\u53ef\u89e3\u91ca\u6027\u548cMDP\u7684\u7b97\u6cd5\u4f18\u52bf\uff0c\u6269\u5c55\u4e86PEC\u7684\u5e94\u7528\u8303\u56f4\u3002"}}
{"id": "2507.13019", "categories": ["cs.RO", "cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.13019", "abs": "https://arxiv.org/abs/2507.13019", "authors": ["Liuyi Wang", "Xinyuan Xia", "Hui Zhao", "Hanqing Wang", "Tai Wang", "Yilun Chen", "Chengju Liu", "Qijun Chen", "Jiangmiao Pang"], "title": "Rethinking the Embodied Gap in Vision-and-Language Navigation: A Holistic Study of Physical and Visual Disparities", "comment": "Accepted by ICCV 2025", "summary": "Recent Vision-and-Language Navigation (VLN) advancements are promising, but\ntheir idealized assumptions about robot movement and control fail to reflect\nphysically embodied deployment challenges. To bridge this gap, we introduce\nVLN-PE, a physically realistic VLN platform supporting humanoid, quadruped, and\nwheeled robots. For the first time, we systematically evaluate several\nego-centric VLN methods in physical robotic settings across different technical\npipelines, including classification models for single-step discrete action\nprediction, a diffusion model for dense waypoint prediction, and a train-free,\nmap-based large language model (LLM) integrated with path planning. Our results\nreveal significant performance degradation due to limited robot observation\nspace, environmental lighting variations, and physical challenges like\ncollisions and falls. This also exposes locomotion constraints for legged\nrobots in complex environments. VLN-PE is highly extensible, allowing seamless\nintegration of new scenes beyond MP3D, thereby enabling more comprehensive VLN\nevaluation. Despite the weak generalization of current models in physical\ndeployment, VLN-PE provides a new pathway for improving cross-embodiment's\noverall adaptability. We hope our findings and tools inspire the community to\nrethink VLN limitations and advance robust, practical VLN models. The code is\navailable at https://crystalsixone.github.io/vln_pe.github.io/.", "AI": {"tldr": "VLN-PE\u662f\u4e00\u4e2a\u7269\u7406\u73b0\u5b9e\u7684\u89c6\u89c9\u4e0e\u8bed\u8a00\u5bfc\u822a\u5e73\u53f0\uff0c\u652f\u6301\u4eba\u5f62\u3001\u56db\u8db3\u548c\u8f6e\u5f0f\u673a\u5668\u4eba\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u7269\u7406\u90e8\u7f72\u4e2d\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u89c6\u89c9\u4e0e\u8bed\u8a00\u5bfc\u822a\uff08VLN\uff09\u65b9\u6cd5\u5728\u7269\u7406\u673a\u5668\u4eba\u90e8\u7f72\u4e2d\u7684\u7406\u60f3\u5316\u5047\u8bbe\u95ee\u9898\u3002", "method": "\u5f15\u5165VLN-PE\u5e73\u53f0\uff0c\u8bc4\u4f30\u591a\u79cdVLN\u65b9\u6cd5\uff0c\u5305\u62ec\u5206\u7c7b\u6a21\u578b\u3001\u6269\u6563\u6a21\u578b\u548c\u57fa\u4e8eLLM\u7684\u65e0\u8bad\u7ec3\u8def\u5f84\u89c4\u5212\u3002", "result": "\u53d1\u73b0\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u4e3b\u8981\u6e90\u4e8e\u673a\u5668\u4eba\u89c2\u5bdf\u7a7a\u95f4\u9650\u5236\u3001\u73af\u5883\u5149\u7167\u53d8\u5316\u53ca\u7269\u7406\u6311\u6218\uff08\u5982\u78b0\u649e\u548c\u8dcc\u5012\uff09\u3002", "conclusion": "VLN-PE\u4e3a\u6539\u8fdb\u8de8\u5177\u8eab\u9002\u5e94\u6027\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\uff0c\u5e76\u63a8\u52a8\u66f4\u9c81\u68d2\u3001\u5b9e\u7528\u7684VLN\u6a21\u578b\u53d1\u5c55\u3002"}}
{"id": "2507.13007", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13007", "abs": "https://arxiv.org/abs/2507.13007", "authors": ["Roger Xavier Lera-Leri", "Filippo Bistaffa", "Athina Georgara", "Juan Antonio Rodriguez-Aguilar"], "title": "Exploiting Constraint Reasoning to Build Graphical Explanations for Mixed-Integer Linear Programming", "comment": "To appear in Lecture Notes in Artificial Intelligence", "summary": "Following the recent push for trustworthy AI, there has been an increasing\ninterest in developing contrastive explanation techniques for optimisation,\nespecially concerning the solution of specific decision-making processes\nformalised as MILPs. Along these lines, we propose X-MILP, a domain-agnostic\napproach for building contrastive explanations for MILPs based on constraint\nreasoning techniques. First, we show how to encode the queries a user makes\nabout the solution of an MILP problem as additional constraints. Then, we\ndetermine the reasons that constitute the answer to the user's query by\ncomputing the Irreducible Infeasible Subsystem (IIS) of the newly obtained set\nof constraints. Finally, we represent our explanation as a \"graph of reasons\"\nconstructed from the IIS, which helps the user understand the structure among\nthe reasons that answer their query. We test our method on instances of\nwell-known optimisation problems to evaluate the empirical hardness of\ncomputing explanations.", "AI": {"tldr": "X-MILP\u662f\u4e00\u79cd\u57fa\u4e8e\u7ea6\u675f\u63a8\u7406\u6280\u672f\u7684\u9886\u57df\u65e0\u5173\u65b9\u6cd5\uff0c\u7528\u4e8e\u4e3a\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\uff08MILP\uff09\u6784\u5efa\u5bf9\u6bd4\u89e3\u91ca\u3002", "motivation": "\u968f\u7740\u5bf9\u53ef\u4fe1AI\u7684\u9700\u6c42\u589e\u52a0\uff0c\u5f00\u53d1\u9488\u5bf9\u4f18\u5316\u95ee\u9898\u7684\u5bf9\u6bd4\u89e3\u91ca\u6280\u672f\u53d8\u5f97\u91cd\u8981\uff0c\u5c24\u5176\u662f\u9488\u5bf9MILP\u7684\u51b3\u7b56\u8fc7\u7a0b\u3002", "method": "\u5c06\u7528\u6237\u67e5\u8be2\u7f16\u7801\u4e3a\u989d\u5916\u7ea6\u675f\uff0c\u901a\u8fc7\u8ba1\u7b97\u4e0d\u53ef\u7ea6\u4e0d\u53ef\u884c\u5b50\u7cfb\u7edf\uff08IIS\uff09\u751f\u6210\u89e3\u91ca\uff0c\u5e76\u4ee5\u201c\u539f\u56e0\u56fe\u201d\u5f62\u5f0f\u5448\u73b0\u3002", "result": "\u5728\u7ecf\u5178\u4f18\u5316\u95ee\u9898\u4e0a\u6d4b\u8bd5\uff0c\u9a8c\u8bc1\u4e86\u8ba1\u7b97\u89e3\u91ca\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "X-MILP\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u5e2e\u52a9\u7528\u6237\u7406\u89e3MILP\u89e3\u51b3\u65b9\u6848\u80cc\u540e\u7684\u539f\u56e0\u3002"}}
{"id": "2507.13041", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.13041", "abs": "https://arxiv.org/abs/2507.13041", "authors": ["Julien Wacquez", "Elisabetta Zibetti", "Joffrey Becker", "Lorenzo Aloe", "Fabio Amadio", "Salvatore Anzalone", "Lola Ca\u00f1amero", "Serena Ivaldi"], "title": "What Can Robots Teach Us About Trust and Reliance? An interdisciplinary dialogue between Social Sciences and Social Robotics", "comment": null, "summary": "As robots find their way into more and more aspects of everyday life,\nquestions around trust are becoming increasingly important. What does it mean\nto trust a robot? And how should we think about trust in relationships that\ninvolve both humans and non-human agents? While the field of Human-Robot\nInteraction (HRI) has made trust a central topic, the concept is often\napproached in fragmented ways. At the same time, established work in sociology,\nwhere trust has long been a key theme, is rarely brought into conversation with\ndevelopments in robotics. This article argues that we need a more\ninterdisciplinary approach. By drawing on insights from both social sciences\nand social robotics, we explore how trust is shaped, tested and made visible.\nOur goal is to open up a dialogue between disciplines and help build a more\ngrounded and adaptable framework for understanding trust in the evolving world\nof human-robot interaction.", "AI": {"tldr": "\u8bba\u6587\u4e3b\u5f20\u901a\u8fc7\u8de8\u5b66\u79d1\u65b9\u6cd5\uff08\u793e\u4f1a\u5b66\u4e0e\u673a\u5668\u4eba\u5b66\uff09\u7814\u7a76\u4eba\u673a\u4ea4\u4e92\u4e2d\u7684\u4fe1\u4efb\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u673a\u5668\u4eba\u8fdb\u5165\u65e5\u5e38\u751f\u6d3b\uff0c\u4fe1\u4efb\u95ee\u9898\u65e5\u76ca\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u7cfb\u7edf\u6027\uff0c\u4e14\u793e\u4f1a\u5b66\u4e0e\u673a\u5668\u4eba\u5b66\u9886\u57df\u7f3a\u4e4f\u5bf9\u8bdd\u3002", "method": "\u7ed3\u5408\u793e\u4f1a\u5b66\u4e0e\u793e\u4ea4\u673a\u5668\u4eba\u7684\u89c1\u89e3\uff0c\u63a2\u8ba8\u4fe1\u4efb\u7684\u5f62\u6210\u3001\u6d4b\u8bd5\u4e0e\u8868\u73b0\u3002", "result": "\u63d0\u51fa\u8de8\u5b66\u79d1\u5bf9\u8bdd\u7684\u5fc5\u8981\u6027\uff0c\u4ee5\u6784\u5efa\u66f4\u624e\u5b9e\u3001\u9002\u5e94\u6027\u5f3a\u7684\u4fe1\u4efb\u6846\u67b6\u3002", "conclusion": "\u9700\u901a\u8fc7\u5b66\u79d1\u4ea4\u53c9\u63a8\u52a8\u5bf9\u4eba\u673a\u4fe1\u4efb\u7684\u66f4\u6df1\u5165\u7406\u89e3\u3002"}}
{"id": "2507.13112", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13112", "abs": "https://arxiv.org/abs/2507.13112", "authors": ["Junseong Lee", "Jaegwan Cho", "Yoonju Cho", "Seoyoon Choi", "Yejin Shin"], "title": "Prediction of Highway Traffic Flow Based on Artificial Intelligence Algorithms Using California Traffic Data", "comment": null, "summary": "The study \"Prediction of Highway Traffic Flow Based on Artificial\nIntelligence Algorithms Using California Traffic Data\" presents a machine\nlearning-based traffic flow prediction model to address global traffic\ncongestion issues. The research utilized 30-second interval traffic data from\nCalifornia Highway 78 over a five-month period from July to November 2022,\nanalyzing a 7.24 km westbound section connecting \"Melrose Dr\" and \"El-Camino\nReal\" in the San Diego area. The study employed Multiple Linear Regression\n(MLR) and Random Forest (RF) algorithms, analyzing data collection intervals\nranging from 30 seconds to 15 minutes. Using R^2, MAE, and RMSE as performance\nmetrics, the analysis revealed that both MLR and RF models performed optimally\nwith 10-minute data collection intervals. These findings are expected to\ncontribute to future traffic congestion solutions and efficient traffic\nmanagement.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5229\u7528\u4eba\u5de5\u667a\u80fd\u7b97\u6cd5\u9884\u6d4b\u9ad8\u901f\u516c\u8def\u4ea4\u901a\u6d41\u91cf\uff0c\u57fa\u4e8e\u52a0\u5dde\u4ea4\u901a\u6570\u636e\uff0c\u53d1\u73b010\u5206\u949f\u6570\u636e\u91c7\u96c6\u95f4\u9694\u4e0bMLR\u548cRF\u6a21\u578b\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u89e3\u51b3\u5168\u7403\u4ea4\u901a\u62e5\u5835\u95ee\u9898\uff0c\u63d0\u51fa\u9ad8\u6548\u7684\u4ea4\u901a\u6d41\u91cf\u9884\u6d4b\u6a21\u578b\u3002", "method": "\u4f7f\u7528\u52a0\u5dde78\u53f7\u9ad8\u901f\u516c\u8def5\u4e2a\u6708\u768430\u79d2\u95f4\u9694\u6570\u636e\uff0c\u91c7\u7528MLR\u548cRF\u7b97\u6cd5\uff0c\u5206\u6790\u4e0d\u540c\u6570\u636e\u91c7\u96c6\u95f4\u9694\uff0830\u79d2\u81f315\u5206\u949f\uff09\u3002", "result": "MLR\u548cRF\u6a21\u578b\u572810\u5206\u949f\u6570\u636e\u91c7\u96c6\u95f4\u9694\u4e0b\u8868\u73b0\u6700\u4f18\uff0c\u6027\u80fd\u6307\u6807\u4e3aR^2\u3001MAE\u548cRMSE\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u6709\u52a9\u4e8e\u672a\u6765\u4ea4\u901a\u62e5\u5835\u89e3\u51b3\u65b9\u6848\u548c\u9ad8\u6548\u4ea4\u901a\u7ba1\u7406\u3002"}}
{"id": "2507.13053", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.13053", "abs": "https://arxiv.org/abs/2507.13053", "authors": ["Sanjeev Ramkumar Sudha", "Joel Jose", "Erlend M. Coates"], "title": "Efficient Online Learning and Adaptive Planning for Robotic Information Gathering Based on Streaming Data", "comment": null, "summary": "Robotic information gathering (RIG) techniques refer to methods where mobile\nrobots are used to acquire data about the physical environment with a suite of\nsensors. Informative planning is an important part of RIG where the goal is to\nfind sequences of actions or paths that maximize efficiency or the quality of\ninformation collected. Many existing solutions solve this problem by assuming\nthat the environment is known in advance. However, real environments could be\nunknown or time-varying, and adaptive informative planning remains an active\narea of research. Adaptive planning and incremental online mapping are required\nfor mapping initially unknown or varying spatial fields. Gaussian process (GP)\nregression is a widely used technique in RIG for mapping continuous spatial\nfields. However, it falls short in many applications as its real-time\nperformance does not scale well to large datasets. To address these challenges,\nthis paper proposes an efficient adaptive informative planning approach for\nmapping continuous scalar fields with GPs with streaming sparse GPs. Simulation\nexperiments are performed with a synthetic dataset and compared against\nexisting benchmarks. Finally, it is also verified with a real-world dataset to\nfurther validate the efficacy of the proposed method. Results show that our\nmethod achieves similar mapping accuracy to the baselines while reducing\ncomputational complexity for longer missions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9ad8\u65af\u8fc7\u7a0b\u7684\u6d41\u5f0f\u7a00\u758f\u81ea\u9002\u5e94\u4fe1\u606f\u89c4\u5212\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u6548\u6620\u5c04\u672a\u77e5\u6216\u65f6\u53d8\u73af\u5883\u4e2d\u7684\u8fde\u7eed\u6807\u91cf\u573a\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5047\u8bbe\u73af\u5883\u5df2\u77e5\uff0c\u4f46\u5b9e\u9645\u73af\u5883\u53ef\u80fd\u672a\u77e5\u6216\u65f6\u53d8\uff0c\u4e14\u9ad8\u65af\u8fc7\u7a0b\u5728\u5927\u6570\u636e\u96c6\u4e0a\u5b9e\u65f6\u6027\u80fd\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u6d41\u5f0f\u7a00\u758f\u9ad8\u65af\u8fc7\u7a0b\u8fdb\u884c\u81ea\u9002\u5e94\u89c4\u5212\u548c\u589e\u91cf\u5728\u7ebf\u6620\u5c04\u3002", "result": "\u4eff\u771f\u548c\u771f\u5b9e\u6570\u636e\u96c6\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u957f\u4efb\u52a1\u4e2d\u9ad8\u6548\u4e14\u9002\u7528\u4e8e\u672a\u77e5\u6216\u65f6\u53d8\u73af\u5883\u3002"}}
{"id": "2507.13142", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.13142", "abs": "https://arxiv.org/abs/2507.13142", "authors": ["Ahmed Bahloul", "Simon Malberg"], "title": "From Roots to Rewards: Dynamic Tree Reasoning with RL", "comment": null, "summary": "Modern language models address complex questions through chain-of-thought\n(CoT) reasoning (Wei et al., 2023) and retrieval augmentation (Lewis et al.,\n2021), yet struggle with error propagation and knowledge integration.\nTree-structured reasoning methods, particularly the Probabilistic\nTree-of-Thought (ProbTree)(Cao et al., 2023) framework, mitigate these issues\nby decomposing questions into hierarchical structures and selecting answers\nthrough confidence-weighted aggregation of parametric and retrieved knowledge\n(Yao et al., 2023). However, ProbTree's static implementation introduces two\nkey limitations: (1) the reasoning tree is fixed during the initial\nconstruction phase, preventing dynamic adaptation to intermediate results, and\n(2) each node requires exhaustive evaluation of all possible solution\nstrategies, creating computational inefficiency. We present a dynamic\nreinforcement learning (Sutton and Barto, 2018) framework that transforms\ntree-based reasoning into an adaptive process. Our approach incrementally\nconstructs the reasoning tree based on real-time confidence estimates, while\nlearning optimal policies for action selection (decomposition, retrieval, or\naggregation). This maintains ProbTree's probabilistic rigor while improving\nboth solution quality and computational efficiency through selective expansion\nand focused resource allocation. The work establishes a new paradigm for\ntreestructured reasoning that balances the reliability of probabilistic\nframeworks with the flexibility required for real-world question answering\nsystems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u6539\u8fdb\u6811\u72b6\u63a8\u7406\u65b9\u6cd5\uff0c\u89e3\u51b3\u9759\u6001\u6811\u7ed3\u6784\u7684\u5c40\u9650\u6027\uff0c\u63d0\u5347\u8ba1\u7b97\u6548\u7387\u548c\u89e3\u7b54\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u6811\u72b6\u63a8\u7406\u65b9\u6cd5\uff08\u5982ProbTree\uff09\u5b58\u5728\u9759\u6001\u6811\u7ed3\u6784\u548c\u8ba1\u7b97\u6548\u7387\u4f4e\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u5176\u5728\u5b9e\u9645\u95ee\u7b54\u7cfb\u7edf\u4e2d\u7684\u9002\u5e94\u6027\u3002", "method": "\u91c7\u7528\u52a8\u6001\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5b9e\u65f6\u6784\u5efa\u63a8\u7406\u6811\uff0c\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u548c\u7b56\u7565\u5b66\u4e60\u4f18\u5316\u5206\u89e3\u3001\u68c0\u7d22\u548c\u805a\u5408\u64cd\u4f5c\u3002", "result": "\u52a8\u6001\u6846\u67b6\u5728\u4fdd\u6301\u6982\u7387\u4e25\u8c28\u6027\u7684\u540c\u65f6\uff0c\u63d0\u9ad8\u4e86\u89e3\u7b54\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6811\u72b6\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u5e73\u8861\u4e86\u6982\u7387\u6846\u67b6\u7684\u53ef\u9760\u6027\u548c\u5b9e\u9645\u95ee\u7b54\u7cfb\u7edf\u7684\u7075\u6d3b\u6027\u3002"}}
{"id": "2507.13088", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.13088", "abs": "https://arxiv.org/abs/2507.13088", "authors": ["Rahel Rickenbach", "Alan A. Lahoud", "Erik Schaffernicht", "Melanie N. Zeilinger", "Johannes A. Stork"], "title": "ZipMPC: Compressed Context-Dependent MPC Cost via Imitation Learning", "comment": null, "summary": "The computational burden of model predictive control (MPC) limits its\napplication on real-time systems, such as robots, and often requires the use of\nshort prediction horizons. This not only affects the control performance, but\nalso increases the difficulty of designing MPC cost functions that reflect the\ndesired long-term objective. This paper proposes ZipMPC, a method that imitates\na long-horizon MPC behaviour by learning a compressed and context-dependent\ncost function for a short-horizon MPC. It improves performance over alternative\nmethods, such as approximate explicit MPC and automatic cost parameter tuning,\nin particular in terms of i) optimizing the long term objective; ii)\nmaintaining computational costs comparable to a short-horizon MPC; iii)\nensuring constraint satisfaction; and iv) generalizing control behaviour to\nenvironments not observed during training. For this purpose, ZipMPC leverages\nthe concept of differentiable MPC with neural networks to propagate gradients\nof the imitation loss through the MPC optimization. We validate our proposed\nmethod in simulation and real-world experiments on autonomous racing. ZipMPC\nconsistently completes laps faster than selected baselines, achieving lap times\nclose to the long-horizon MPC baseline. In challenging scenarios where the\nshort-horizon MPC baseline fails to complete a lap, ZipMPC is able to do so. In\nparticular, these performance gains are also observed on tracks unseen during\ntraining.", "AI": {"tldr": "ZipMPC\u901a\u8fc7\u4e3a\u77ed\u65f6\u57dfMPC\u5b66\u4e60\u538b\u7f29\u4e14\u4e0a\u4e0b\u6587\u76f8\u5173\u7684\u6210\u672c\u51fd\u6570\uff0c\u6a21\u4eff\u957f\u65f6\u57dfMPC\u884c\u4e3a\uff0c\u63d0\u5347\u6027\u80fd\u5e76\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u89e3\u51b3MPC\u5728\u5b9e\u65f6\u7cfb\u7edf\uff08\u5982\u673a\u5668\u4eba\uff09\u4e2d\u56e0\u8ba1\u7b97\u8d1f\u62c5\u800c\u53ea\u80fd\u4f7f\u7528\u77ed\u9884\u6d4b\u65f6\u57df\u7684\u95ee\u9898\uff0c\u540c\u65f6\u4f18\u5316\u957f\u671f\u76ee\u6807\u548c\u6ee1\u8db3\u7ea6\u675f\u3002", "method": "\u5229\u7528\u53ef\u5fae\u5206MPC\u548c\u795e\u7ecf\u7f51\u7edc\u4f20\u64ad\u68af\u5ea6\uff0c\u5b66\u4e60\u538b\u7f29\u7684\u6210\u672c\u51fd\u6570\uff0c\u6a21\u4eff\u957f\u65f6\u57dfMPC\u884c\u4e3a\u3002", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u8d5b\u8f66\u5b9e\u9a8c\u4e2d\uff0cZipMPC\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5b8c\u6210\u5708\u901f\u63a5\u8fd1\u957f\u65f6\u57dfMPC\uff0c\u4e14\u80fd\u6cdb\u5316\u5230\u672a\u8bad\u7ec3\u73af\u5883\u3002", "conclusion": "ZipMPC\u5728\u4fdd\u6301\u77ed\u65f6\u57dfMPC\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u63a5\u8fd1\u957f\u65f6\u57dfMPC\u7684\u6027\u80fd\uff0c\u5e76\u5177\u5907\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.13175", "categories": ["cs.AI", "68T27, 03B42 68T27, 03B4268T27, 03B42 68T27, 03B42 68T27, 03B42\n  68T27, 03B42 68T27, 03B42 68T27, 03B4268T27, 03B42", "I.2.0; I.2.9; K.4.1"], "pdf": "https://arxiv.org/pdf/2507.13175", "abs": "https://arxiv.org/abs/2507.13175", "authors": ["Matthew E. Brophy"], "title": "Black Box Deployed -- Functional Criteria for Artificial Moral Agents in the LLM Era", "comment": "42 pages. Supplementary material included at end of article", "summary": "The advancement of powerful yet opaque large language models (LLMs)\nnecessitates a fundamental revision of the philosophical criteria used to\nevaluate artificial moral agents (AMAs). Pre-LLM frameworks often relied on the\nassumption of transparent architectures, which LLMs defy due to their\nstochastic outputs and opaque internal states. This paper argues that\ntraditional ethical criteria are pragmatically obsolete for LLMs due to this\nmismatch. Engaging with core themes in the philosophy of technology, this paper\nproffers a revised set of ten functional criteria to evaluate LLM-based\nartificial moral agents: moral concordance, context sensitivity, normative\nintegrity, metaethical awareness, system resilience, trustworthiness,\ncorrigibility, partial transparency, functional autonomy, and moral\nimagination. These guideposts, applied to what we term \"SMA-LLS\" (Simulating\nMoral Agency through Large Language Systems), aim to steer AMAs toward greater\nalignment and beneficial societal integration in the coming years. We\nillustrate these criteria using hypothetical scenarios involving an autonomous\npublic bus (APB) to demonstrate their practical applicability in morally\nsalient contexts.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4f20\u7edf\u4f26\u7406\u6807\u51c6\u4e0d\u9002\u7528\u4e8e\u4e0d\u900f\u660e\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u5e76\u63d0\u51fa\u4e86\u5341\u9879\u65b0\u529f\u80fd\u6807\u51c6\u6765\u8bc4\u4f30\u57fa\u4e8eLLM\u7684\u4eba\u5de5\u9053\u5fb7\u4ee3\u7406\uff08AMAs\uff09\u3002", "motivation": "\u7531\u4e8eLLMs\u7684\u968f\u673a\u8f93\u51fa\u548c\u4e0d\u900f\u660e\u5185\u90e8\u72b6\u6001\uff0c\u4f20\u7edf\u4f26\u7406\u6807\u51c6\u5931\u6548\uff0c\u9700\u8981\u65b0\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u901a\u8fc7\u54f2\u5b66\u4e0e\u6280\u672f\u7ed3\u5408\uff0c\u63d0\u51fa\u5341\u9879\u529f\u80fd\u6807\u51c6\uff0c\u5e76\u4ee5\u81ea\u52a8\u9a7e\u9a76\u516c\u4ea4\u8f66\uff08APB\uff09\u4e3a\u4f8b\u8bf4\u660e\u5176\u5e94\u7528\u3002", "result": "\u65b0\u6807\u51c6\uff08\u5982\u9053\u5fb7\u4e00\u81f4\u6027\u3001\u4e0a\u4e0b\u6587\u654f\u611f\u6027\u7b49\uff09\u65e8\u5728\u63d0\u5347LLM\u7684\u9053\u5fb7\u4ee3\u7406\u80fd\u529b\u4e0e\u793e\u4f1a\u878d\u5408\u3002", "conclusion": "\u4fee\u8ba2\u540e\u7684\u6807\u51c6\u6709\u52a9\u4e8eLLM\u66f4\u597d\u5730\u6a21\u62df\u9053\u5fb7\u4ee3\u7406\uff0c\u4fc3\u8fdb\u793e\u4f1a\u6574\u5408\u3002"}}
{"id": "2507.13097", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13097", "abs": "https://arxiv.org/abs/2507.13097", "authors": ["Adithyavairavan Murali", "Balakumar Sundaralingam", "Yu-Wei Chao", "Wentao Yuan", "Jun Yamada", "Mark Carlson", "Fabio Ramos", "Stan Birchfield", "Dieter Fox", "Clemens Eppner"], "title": "GraspGen: A Diffusion-based Framework for 6-DOF Grasping with On-Generator Training", "comment": null, "summary": "Grasping is a fundamental robot skill, yet despite significant research\nadvancements, learning-based 6-DOF grasping approaches are still not turnkey\nand struggle to generalize across different embodiments and in-the-wild\nsettings. We build upon the recent success on modeling the object-centric grasp\ngeneration process as an iterative diffusion process. Our proposed framework,\nGraspGen, consists of a DiffusionTransformer architecture that enhances grasp\ngeneration, paired with an efficient discriminator to score and filter sampled\ngrasps. We introduce a novel and performant on-generator training recipe for\nthe discriminator. To scale GraspGen to both objects and grippers, we release a\nnew simulated dataset consisting of over 53 million grasps. We demonstrate that\nGraspGen outperforms prior methods in simulations with singulated objects\nacross different grippers, achieves state-of-the-art performance on the\nFetchBench grasping benchmark, and performs well on a real robot with noisy\nvisual observations.", "AI": {"tldr": "GraspGen\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u53d8\u6362\u5668\u67b6\u6784\u76846-DOF\u6293\u53d6\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u9ad8\u6548\u5224\u522b\u5668\u8bc4\u5206\u548c\u8fc7\u6ee4\u6293\u53d6\u6837\u672c\uff0c\u7ed3\u5408\u65b0\u578b\u8bad\u7ec3\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6293\u53d6\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u6293\u53d6\u662f\u673a\u5668\u4eba\u7684\u57fa\u672c\u6280\u80fd\uff0c\u4f46\u73b0\u6709\u5b66\u4e60\u578b6-DOF\u6293\u53d6\u65b9\u6cd5\u4ecd\u96be\u4ee5\u6cdb\u5316\u5230\u4e0d\u540c\u673a\u68b0\u81c2\u548c\u5b9e\u9645\u573a\u666f\u3002", "method": "\u91c7\u7528\u6269\u6563\u53d8\u6362\u5668\u67b6\u6784\u751f\u6210\u6293\u53d6\uff0c\u7ed3\u5408\u9ad8\u6548\u5224\u522b\u5668\u8bc4\u5206\u548c\u8fc7\u6ee4\uff0c\u5e76\u5f15\u5165\u65b0\u578b\u8bad\u7ec3\u65b9\u6cd5\u3002", "result": "\u5728\u4eff\u771f\u548c\u5b9e\u9645\u673a\u5668\u4eba\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728FetchBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u4f18\u3002", "conclusion": "GraspGen\u4e3a6-DOF\u6293\u53d6\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u4e14\u6cdb\u5316\u80fd\u529b\u5f3a\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.13208", "categories": ["cs.AI", "cs.LO", "math.LO", "03B70 (Primary) 68T37, 68T27, 68Q42, 03B40, 68V15 (Secondary)", "F.4.1; I.2.3"], "pdf": "https://arxiv.org/pdf/2507.13208", "abs": "https://arxiv.org/abs/2507.13208", "authors": ["Besik Dundua", "Temur Kutsia"], "title": "Higher-Order Pattern Unification Modulo Similarity Relations", "comment": "23 pages", "summary": "The combination of higher-order theories and fuzzy logic can be useful in\ndecision-making tasks that involve reasoning across abstract functions and\npredicates, where exact matches are often rare or unnecessary. Developing\nefficient reasoning and computational techniques for such a combined formalism\npresents a significant challenge. In this paper, we adopt a more\nstraightforward approach aiming at integrating two well-established and\ncomputationally well-behaved components: higher-order patterns on one side and\nfuzzy equivalences expressed through similarity relations based on minimum\nT-norm on the other. We propose a unification algorithm for higher-order\npatterns modulo these similarity relations and prove its termination,\nsoundness, and completeness. This unification problem, like its crisp\ncounterpart, is unitary. The algorithm computes a most general unifier with the\nhighest degree of approximation when the given terms are unifiable.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u9ad8\u9636\u6a21\u5f0f\u548c\u6a21\u7cca\u7b49\u4ef7\u5173\u7cfb\u7684\u7edf\u4e00\u7b97\u6cd5\uff0c\u7528\u4e8e\u51b3\u7b56\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\uff0c\u8bc1\u660e\u4e86\u5176\u7ec8\u6b62\u6027\u3001\u5065\u5168\u6027\u548c\u5b8c\u5907\u6027\u3002", "motivation": "\u5728\u6d89\u53ca\u62bd\u8c61\u51fd\u6570\u548c\u8c13\u8bcd\u7684\u51b3\u7b56\u4efb\u52a1\u4e2d\uff0c\u9ad8\u9636\u7406\u8bba\u4e0e\u6a21\u7cca\u903b\u8f91\u7684\u7ed3\u5408\u53ef\u4ee5\u63d0\u5347\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u6548\u7387\u4e0d\u8db3\u3002", "method": "\u91c7\u7528\u9ad8\u9636\u6a21\u5f0f\u548c\u57fa\u4e8e\u6700\u5c0fT-\u8303\u6570\u7684\u6a21\u7cca\u7b49\u4ef7\u5173\u7cfb\uff0c\u63d0\u51fa\u7edf\u4e00\u7b97\u6cd5\u5e76\u8bc1\u660e\u5176\u6027\u8d28\u3002", "result": "\u7b97\u6cd5\u80fd\u8ba1\u7b97\u6700\u4e00\u822c\u7edf\u4e00\u5b50\uff0c\u5e76\u5728\u53ef\u7edf\u4e00\u65f6\u63d0\u4f9b\u6700\u9ad8\u8fd1\u4f3c\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u9ad8\u9636\u6a21\u7cca\u63a8\u7406\u63d0\u4f9b\u4e86\u9ad8\u6548\u4e14\u53ef\u9760\u7684\u8ba1\u7b97\u5de5\u5177\u3002"}}
{"id": "2507.13171", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.13171", "abs": "https://arxiv.org/abs/2507.13171", "authors": ["Suzie Kim", "Hye-Bin Shin", "Seong-Whan Lee"], "title": "Aligning Humans and Robots via Reinforcement Learning from Implicit Human Feedback", "comment": null, "summary": "Conventional reinforcement learning (RL) ap proaches often struggle to learn\neffective policies under sparse reward conditions, necessitating the manual\ndesign of complex, task-specific reward functions. To address this limitation,\nrein forcement learning from human feedback (RLHF) has emerged as a promising\nstrategy that complements hand-crafted rewards with human-derived evaluation\nsignals. However, most existing RLHF methods depend on explicit feedback\nmechanisms such as button presses or preference labels, which disrupt the\nnatural interaction process and impose a substantial cognitive load on the\nuser. We propose a novel reinforcement learning from implicit human feedback\n(RLIHF) framework that utilizes non-invasive electroencephalography (EEG)\nsignals, specifically error-related potentials (ErrPs), to provide continuous,\nimplicit feedback without requiring explicit user intervention. The proposed\nmethod adopts a pre-trained decoder to transform raw EEG signals into\nprobabilistic reward components, en abling effective policy learning even in\nthe presence of sparse external rewards. We evaluate our approach in a\nsimulation environment built on the MuJoCo physics engine, using a Kinova Gen2\nrobotic arm to perform a complex pick-and-place task that requires avoiding\nobstacles while manipulating target objects. The results show that agents\ntrained with decoded EEG feedback achieve performance comparable to those\ntrained with dense, manually designed rewards. These findings validate the\npotential of using implicit neural feedback for scalable and human-aligned\nreinforcement learning in interactive robotics.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9690\u5f0f\u4eba\u7c7b\u53cd\u9988\uff08RLIHF\uff09\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u8111\u7535\u56fe\uff08EEG\uff09\u4fe1\u53f7\u63d0\u4f9b\u8fde\u7eed\u53cd\u9988\uff0c\u65e0\u9700\u7528\u6237\u663e\u5f0f\u5e72\u9884\uff0c\u89e3\u51b3\u4e86\u7a00\u758f\u5956\u52b1\u4e0b\u7684\u7b56\u7565\u5b66\u4e60\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u5728\u7a00\u758f\u5956\u52b1\u6761\u4ef6\u4e0b\u6548\u679c\u4e0d\u4f73\uff0c\u4e14\u4f9d\u8d56\u4eba\u5de5\u8bbe\u8ba1\u7684\u590d\u6742\u5956\u52b1\u51fd\u6570\u3002\u73b0\u6709\u7684\u4eba\u7c7b\u53cd\u9988\u5f3a\u5316\u5b66\u4e60\uff08RLHF\uff09\u65b9\u6cd5\u9700\u8981\u663e\u5f0f\u53cd\u9988\uff0c\u5e72\u6270\u81ea\u7136\u4ea4\u4e92\u5e76\u589e\u52a0\u7528\u6237\u8ba4\u77e5\u8d1f\u62c5\u3002", "method": "\u63d0\u51faRLIHF\u6846\u67b6\uff0c\u901a\u8fc7\u975e\u4fb5\u5165\u5f0fEEG\u4fe1\u53f7\uff08\u7279\u522b\u662f\u9519\u8bef\u76f8\u5173\u7535\u4f4dErrPs\uff09\u63d0\u4f9b\u9690\u5f0f\u53cd\u9988\uff0c\u4f7f\u7528\u9884\u8bad\u7ec3\u89e3\u7801\u5668\u5c06EEG\u4fe1\u53f7\u8f6c\u5316\u4e3a\u6982\u7387\u5956\u52b1\u7ec4\u4ef6\u3002", "result": "\u5728MuJoCo\u4eff\u771f\u73af\u5883\u4e2d\uff0c\u4f7f\u7528Kinova Gen2\u673a\u68b0\u81c2\u8fdb\u884c\u590d\u6742\u6293\u53d6\u4efb\u52a1\uff0c\u7ed3\u679c\u663e\u793a\u57fa\u4e8eEEG\u53cd\u9988\u7684\u4ee3\u7406\u6027\u80fd\u4e0e\u5bc6\u96c6\u4eba\u5de5\u5956\u52b1\u76f8\u5f53\u3002", "conclusion": "\u9690\u5f0f\u795e\u7ecf\u53cd\u9988\u4e3a\u4ea4\u4e92\u5f0f\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u4e0e\u4eba\u7c7b\u5bf9\u9f50\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u3002"}}
{"id": "2507.13302", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.13302", "abs": "https://arxiv.org/abs/2507.13302", "authors": ["Carlos Arriaga", "Gonzalo Mart\u00ednez", "Eneko Sendin", "Javier Conde", "Pedro Reviriego"], "title": "The Generative Energy Arena (GEA): Incorporating Energy Awareness in Large Language Model (LLM) Human Evaluations", "comment": null, "summary": "The evaluation of large language models is a complex task, in which several\napproaches have been proposed. The most common is the use of automated\nbenchmarks in which LLMs have to answer multiple-choice questions of different\ntopics. However, this method has certain limitations, being the most\nconcerning, the poor correlation with the humans. An alternative approach, is\nto have humans evaluate the LLMs. This poses scalability issues as there is a\nlarge and growing number of models to evaluate making it impractical (and\ncostly) to run traditional studies based on recruiting a number of evaluators\nand having them rank the responses of the models. An alternative approach is\nthe use of public arenas, such as the popular LM arena, on which any user can\nfreely evaluate models on any question and rank the responses of two models.\nThe results are then elaborated into a model ranking. An increasingly important\naspect of LLMs is their energy consumption and, therefore, evaluating how\nenergy awareness influences the decisions of humans in selecting a model is of\ninterest. In this paper, we present GEA, the Generative Energy Arena, an arena\nthat incorporates information on the energy consumption of the model in the\nevaluation process. Preliminary results obtained with GEA are also presented,\nshowing that for most questions, when users are aware of the energy\nconsumption, they favor smaller and more energy efficient models. This suggests\nthat for most user interactions, the extra cost and energy incurred by the more\ncomplex and top-performing models do not provide an increase in the perceived\nquality of the responses that justifies their use.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u6cd5GEA\uff08Generative Energy Arena\uff09\uff0c\u901a\u8fc7\u516c\u5f00\u7ade\u6280\u573a\u7ed3\u5408\u80fd\u6e90\u6d88\u8017\u4fe1\u606f\uff0c\u53d1\u73b0\u7528\u6237\u66f4\u503e\u5411\u4e8e\u9009\u62e9\u80fd\u6e90\u6548\u7387\u66f4\u9ad8\u7684\u5c0f\u578b\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u65b9\u6cd5\uff08\u5982\u81ea\u52a8\u5316\u57fa\u51c6\u6d4b\u8bd5\u6216\u4eba\u5de5\u8bc4\u4f30\uff09\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5982\u4e0e\u4eba\u7c7b\u8bc4\u4ef7\u76f8\u5173\u6027\u4f4e\u6216\u6269\u5c55\u6027\u95ee\u9898\u3002\u80fd\u6e90\u6d88\u8017\u6210\u4e3a\u6a21\u578b\u9009\u62e9\u7684\u91cd\u8981\u56e0\u7d20\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u80fd\u6e90\u610f\u8bc6\u5982\u4f55\u5f71\u54cd\u7528\u6237\u9009\u62e9\u3002", "method": "\u63d0\u51faGEA\uff08Generative Energy Arena\uff09\uff0c\u5728\u516c\u5f00\u7ade\u6280\u573a\u4e2d\u5c55\u793a\u6a21\u578b\u7684\u80fd\u6e90\u6d88\u8017\u4fe1\u606f\uff0c\u8ba9\u7528\u6237\u57fa\u4e8e\u6b64\u8bc4\u4f30\u548c\u6392\u540d\u6a21\u578b\u3002", "result": "\u521d\u6b65\u7ed3\u679c\u663e\u793a\uff0c\u7528\u6237\u5728\u4e86\u89e3\u80fd\u6e90\u6d88\u8017\u540e\uff0c\u66f4\u503e\u5411\u4e8e\u9009\u62e9\u5c0f\u578b\u4e14\u80fd\u6e90\u6548\u7387\u66f4\u9ad8\u7684\u6a21\u578b\uff0c\u8868\u660e\u9ad8\u6027\u80fd\u6a21\u578b\u7684\u989d\u5916\u6210\u672c\u548c\u80fd\u6e90\u6d88\u8017\u5e76\u672a\u663e\u8457\u63d0\u5347\u7528\u6237\u4f53\u9a8c\u3002", "conclusion": "GEA\u63d0\u4f9b\u4e86\u4e00\u79cd\u7ed3\u5408\u80fd\u6e90\u6d88\u8017\u7684\u6a21\u578b\u8bc4\u4f30\u65b9\u6cd5\uff0c\u8868\u660e\u80fd\u6e90\u6548\u7387\u53ef\u80fd\u6210\u4e3a\u672a\u6765\u6a21\u578b\u9009\u62e9\u7684\u91cd\u8981\u6807\u51c6\u3002"}}
{"id": "2507.13200", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.13200", "abs": "https://arxiv.org/abs/2507.13200", "authors": ["Marina Y. Aoyama", "Sethu Vijayakumar", "Tetsuya Narita"], "title": "Few-shot transfer of tool-use skills using human demonstrations with proximity and tactile sensing", "comment": "8 pages, 9 figures, IEEE Robotics and Automation Letters", "summary": "Tools extend the manipulation abilities of robots, much like they do for\nhumans. Despite human expertise in tool manipulation, teaching robots these\nskills faces challenges. The complexity arises from the interplay of two\nsimultaneous points of contact: one between the robot and the tool, and another\nbetween the tool and the environment. Tactile and proximity sensors play a\ncrucial role in identifying these complex contacts. However, learning tool\nmanipulation using these sensors remains challenging due to limited real-world\ndata and the large sim-to-real gap. To address this, we propose a few-shot\ntool-use skill transfer framework using multimodal sensing. The framework\ninvolves pre-training the base policy to capture contact states common in\ntool-use skills in simulation and fine-tuning it with human demonstrations\ncollected in the real-world target domain to bridge the domain gap. We validate\nthat this framework enables teaching surface-following tasks using tools with\ndiverse physical and geometric properties with a small number of demonstrations\non the Franka Emika robot arm. Our analysis suggests that the robot acquires\nnew tool-use skills by transferring the ability to recognise tool-environment\ncontact relationships from pre-trained to fine-tuned policies. Additionally,\ncombining proximity and tactile sensors enhances the identification of contact\nstates and environmental geometry.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u611f\u77e5\u7684\u5c0f\u6837\u672c\u5de5\u5177\u4f7f\u7528\u6280\u80fd\u8fc1\u79fb\u6846\u67b6\uff0c\u901a\u8fc7\u4eff\u771f\u9884\u8bad\u7ec3\u548c\u73b0\u5b9e\u6f14\u793a\u5fae\u8c03\uff0c\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u5de5\u5177\u64cd\u4f5c\u7684\u590d\u6742\u63a5\u89e6\u95ee\u9898\u3002", "motivation": "\u4eba\u7c7b\u64c5\u957f\u5de5\u5177\u64cd\u4f5c\uff0c\u4f46\u673a\u5668\u4eba\u5b66\u4e60\u6b64\u7c7b\u6280\u80fd\u9762\u4e34\u590d\u6742\u63a5\u89e6\u548c\u73b0\u5b9e\u6570\u636e\u4e0d\u8db3\u7684\u6311\u6218\u3002", "method": "\u6846\u67b6\u5305\u62ec\u4eff\u771f\u9884\u8bad\u7ec3\u57fa\u7840\u7b56\u7565\u4ee5\u6355\u6349\u5de5\u5177\u4f7f\u7528\u4e2d\u7684\u5e38\u89c1\u63a5\u89e6\u72b6\u6001\uff0c\u5e76\u901a\u8fc7\u73b0\u5b9e\u6f14\u793a\u5fae\u8c03\u4ee5\u5f25\u5408\u9886\u57df\u5dee\u8ddd\u3002", "result": "\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u4ee5\u5c11\u91cf\u6f14\u793a\u6559\u4f1a\u673a\u5668\u4eba\u4f7f\u7528\u4e0d\u540c\u7269\u7406\u548c\u51e0\u4f55\u5c5e\u6027\u7684\u5de5\u5177\u5b8c\u6210\u8868\u9762\u8ddf\u968f\u4efb\u52a1\u3002", "conclusion": "\u673a\u5668\u4eba\u901a\u8fc7\u8fc1\u79fb\u9884\u8bad\u7ec3\u7b56\u7565\u4e2d\u7684\u5de5\u5177-\u73af\u5883\u63a5\u89e6\u8bc6\u522b\u80fd\u529b\u5b66\u4e60\u65b0\u6280\u80fd\uff0c\u7ed3\u5408\u63a5\u8fd1\u548c\u89e6\u89c9\u4f20\u611f\u5668\u53ef\u63d0\u5347\u63a5\u89e6\u72b6\u6001\u548c\u73af\u5883\u51e0\u4f55\u8bc6\u522b\u3002"}}
{"id": "2507.13337", "categories": ["cs.AI", "cs.CC", "math.LO"], "pdf": "https://arxiv.org/pdf/2507.13337", "abs": "https://arxiv.org/abs/2507.13337", "authors": ["Gal Beniamini", "Yuval Dor", "Alon Vinnikov", "Shir Granot Peled", "Or Weinstein", "Or Sharir", "Noam Wies", "Tomer Nussbaum", "Ido Ben Shaul", "Tomer Zekharya", "Yoav Levine", "Shai Shalev-Shwartz", "Amnon Shashua"], "title": "FormulaOne: Measuring the Depth of Algorithmic Reasoning Beyond Competitive Programming", "comment": null, "summary": "Frontier AI models demonstrate formidable breadth of knowledge. But how close\nare they to true human -- or superhuman -- expertise? Genuine experts can\ntackle the hardest problems and push the boundaries of scientific\nunderstanding. To illuminate the limits of frontier model capabilities, we turn\naway from contrived competitive programming puzzles, and instead focus on\nreal-life research problems.\n  We construct FormulaOne, a benchmark that lies at the intersection of graph\ntheory, logic, and algorithms, all well within the training distribution of\nfrontier models. Our problems are incredibly demanding, requiring an array of\nreasoning steps. The dataset has three key properties. First, it is of\ncommercial interest and relates to practical large-scale optimisation problems,\nsuch as those arising in routing, scheduling, and network design. Second, it is\ngenerated from the highly expressive framework of Monadic Second-Order (MSO)\nlogic on graphs, paving the way toward automatic problem generation at scale;\nideal for building RL environments. Third, many of our problems are intimately\nrelated to the frontier of theoretical computer science, and to central\nconjectures therein, such as the Strong Exponential Time Hypothesis (SETH). As\nsuch, any significant algorithmic progress on our dataset, beyond known\nresults, could carry profound theoretical implications.\n  Remarkably, state-of-the-art models like OpenAI's o3 fail entirely on\nFormulaOne, solving less than 1% of the questions, even when given 10 attempts\nand explanatory fewshot examples -- highlighting how far they remain from\nexpert-level understanding in some domains. To support further research, we\nadditionally curate FormulaOne-Warmup, offering a set of simpler tasks, from\nthe same distribution. We release the full corpus along with a comprehensive\nevaluation framework.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86FormulaOne\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u524d\u6cbfAI\u6a21\u578b\u5728\u590d\u6742\u7814\u7a76\u95ee\u9898\u4e0a\u7684\u8868\u73b0\uff0c\u7ed3\u679c\u663e\u793a\u5f53\u524d\u6a21\u578b\u8868\u73b0\u6781\u5dee\u3002", "motivation": "\u63a2\u8ba8\u524d\u6cbfAI\u6a21\u578b\u662f\u5426\u5177\u5907\u771f\u6b63\u4e13\u5bb6\u6216\u8d85\u4eba\u7c7b\u7684\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5728\u89e3\u51b3\u5b9e\u9645\u7814\u7a76\u95ee\u9898\u4e0a\u7684\u8868\u73b0\u3002", "method": "\u6784\u5efaFormulaOne\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7ed3\u5408\u56fe\u8bba\u3001\u903b\u8f91\u548c\u7b97\u6cd5\uff0c\u751f\u6210\u9ad8\u96be\u5ea6\u95ee\u9898\uff0c\u5e76\u8bc4\u4f30\u6a21\u578b\u8868\u73b0\u3002", "result": "\u524d\u6cbf\u6a21\u578b\u5982OpenAI\u7684o3\u5728FormulaOne\u4e0a\u8868\u73b0\u6781\u5dee\uff0c\u89e3\u51b3\u7387\u4f4e\u4e8e1%\u3002", "conclusion": "\u5f53\u524dAI\u6a21\u578b\u5728\u590d\u6742\u7814\u7a76\u95ee\u9898\u4e0a\u4ecd\u8fdc\u672a\u8fbe\u5230\u4e13\u5bb6\u6c34\u5e73\uff0cFormulaOne\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2507.13225", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.13225", "abs": "https://arxiv.org/abs/2507.13225", "authors": ["Manas Sashank Juvvi", "Tushar Dilip Kurne", "Vaishnavi J", "Shishir Kolathaya", "Pushpak Jagtap"], "title": "Signal Temporal Logic Compliant Co-design of Planning and Control", "comment": null, "summary": "This work presents a novel co-design strategy that integrates trajectory\nplanning and control to handle STL-based tasks in autonomous robots. The method\nconsists of two phases: $(i)$ learning spatio-temporal motion primitives to\nencapsulate the inherent robot-specific constraints and $(ii)$ constructing an\nSTL-compliant motion plan from these primitives. Initially, we employ\nreinforcement learning to construct a library of control policies that perform\ntrajectories described by the motion primitives. Then, we map motion primitives\nto spatio-temporal characteristics. Subsequently, we present a sampling-based\nSTL-compliant motion planning strategy tailored to meet the STL specification.\nThe proposed model-free approach, which generates feasible STL-compliant motion\nplans across various environments, is validated on differential-drive and\nquadruped robots across various STL specifications. Demonstration videos are\navailable at https://tinyurl.com/m6zp7rsm.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u534f\u540c\u8bbe\u8ba1\u7b56\u7565\uff0c\u7ed3\u5408\u8f68\u8ff9\u89c4\u5212\u4e0e\u63a7\u5236\uff0c\u5904\u7406\u81ea\u4e3b\u673a\u5668\u4eba\u4e2d\u57fa\u4e8eSTL\u7684\u4efb\u52a1\u3002", "motivation": "\u89e3\u51b3\u81ea\u4e3b\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u6ee1\u8db3STL\uff08\u65f6\u7a7a\u903b\u8f91\uff09\u4efb\u52a1\u7684\u9700\u6c42\uff0c\u540c\u65f6\u8003\u8651\u673a\u5668\u4eba\u81ea\u8eab\u7684\u8fd0\u52a8\u7ea6\u675f\u3002", "method": "\u5206\u4e3a\u4e24\u9636\u6bb5\uff1a1) \u5b66\u4e60\u65f6\u7a7a\u8fd0\u52a8\u57fa\u5143\u4ee5\u5c01\u88c5\u673a\u5668\u4eba\u7ea6\u675f\uff1b2) \u4ece\u57fa\u5143\u6784\u5efa\u7b26\u5408STL\u7684\u8fd0\u52a8\u8ba1\u5212\u3002\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u751f\u6210\u63a7\u5236\u7b56\u7565\u5e93\uff0c\u5e76\u901a\u8fc7\u91c7\u6837\u65b9\u6cd5\u5b9e\u73b0STL\u5408\u89c4\u7684\u8fd0\u52a8\u89c4\u5212\u3002", "result": "\u5728\u5dee\u901f\u9a71\u52a8\u548c\u56db\u8db3\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u751f\u6210\u7b26\u5408STL\u7684\u53ef\u884c\u8fd0\u52a8\u8ba1\u5212\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u81ea\u4e3b\u673a\u5668\u4eba\u5728\u591a\u6837\u5316\u73af\u5883\u4e2d\u6267\u884cSTL\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u65e0\u6a21\u578b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.13277", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13277", "abs": "https://arxiv.org/abs/2507.13277", "authors": ["Emma M. A. Harrison"], "title": "Evaluating Reinforcement Learning Algorithms for Navigation in Simulated Robotic Quadrupeds: A Comparative Study Inspired by Guide Dog Behaviour", "comment": null, "summary": "Robots are increasingly integrated across industries, particularly in\nhealthcare. However, many valuable applications for quadrupedal robots remain\noverlooked. This research explores the effectiveness of three reinforcement\nlearning algorithms in training a simulated quadruped robot for autonomous\nnavigation and obstacle avoidance. The goal is to develop a robotic guide dog\nsimulation capable of path following and obstacle avoidance, with long-term\npotential for real-world assistance to guide dogs and visually impaired\nindividuals. It also seeks to expand research into medical 'pets', including\nrobotic guide and alert dogs.\n  A comparative analysis of thirteen related research papers shaped key\nevaluation criteria, including collision detection, pathfinding algorithms,\nsensor usage, robot type, and simulation platforms. The study focuses on sensor\ninputs, collision frequency, reward signals, and learning progression to\ndetermine which algorithm best supports robotic navigation in complex\nenvironments.\n  Custom-made environments were used to ensure fair evaluation of all three\nalgorithms under controlled conditions, allowing consistent data collection.\nResults show that Proximal Policy Optimization (PPO) outperformed Deep\nQ-Network (DQN) and Q-learning across all metrics, particularly in average and\nmedian steps to goal per episode.\n  By analysing these results, this study contributes to robotic navigation, AI\nand medical robotics, offering insights into the feasibility of AI-driven\nquadruped mobility and its role in assistive robotics.", "AI": {"tldr": "\u7814\u7a76\u6bd4\u8f83\u4e86\u4e09\u79cd\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u5728\u56db\u8db3\u673a\u5668\u4eba\u81ea\u4e3b\u5bfc\u822a\u548c\u907f\u969c\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0PPO\u7b97\u6cd5\u4f18\u4e8eDQN\u548cQ-learning\uff0c\u4e3a\u8f85\u52a9\u673a\u5668\u4eba\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "motivation": "\u63a2\u7d22\u56db\u8db3\u673a\u5668\u4eba\u5728\u533b\u7597\u548c\u8f85\u52a9\u9886\u57df\u7684\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u4f5c\u4e3a\u5bfc\u76f2\u72ac\u7684\u6a21\u62df\uff0c\u4ee5\u5e2e\u52a9\u89c6\u969c\u4eba\u58eb\u3002", "method": "\u901a\u8fc7\u81ea\u5b9a\u4e49\u73af\u5883\u548c\u5bf9\u6bd4\u5206\u6790\uff0c\u8bc4\u4f30PPO\u3001DQN\u548cQ-learning\u5728\u5bfc\u822a\u548c\u907f\u969c\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "PPO\u5728\u6240\u6709\u6307\u6807\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u7279\u522b\u662f\u5728\u5e73\u5747\u548c\u4e2d\u95f4\u6b65\u9aa4\u5230\u76ee\u6807\u7684\u6548\u7387\u4e0a\u3002", "conclusion": "\u7814\u7a76\u4e3aAI\u9a71\u52a8\u7684\u56db\u8db3\u673a\u5668\u4eba\u79fb\u52a8\u6027\u548c\u8f85\u52a9\u673a\u5668\u4eba\u9886\u57df\u63d0\u4f9b\u4e86\u53ef\u884c\u6027\u89c1\u89e3\u3002"}}
{"id": "2507.13340", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.13340", "abs": "https://arxiv.org/abs/2507.13340", "authors": ["Yiqi Wang", "Mrinal Verghese", "Jeff Schneider"], "title": "Latent Policy Steering with Embodiment-Agnostic Pretrained World Models", "comment": null, "summary": "Learning visuomotor policies via imitation has proven effective across a wide\nrange of robotic domains. However, the performance of these policies is heavily\ndependent on the number of training demonstrations, which requires expensive\ndata collection in the real world. In this work, we aim to reduce data\ncollection efforts when learning visuomotor robot policies by leveraging\nexisting or cost-effective data from a wide range of embodiments, such as\npublic robot datasets and the datasets of humans playing with objects (human\ndata from play). Our approach leverages two key insights. First, we use optic\nflow as an embodiment-agnostic action representation to train a World Model\n(WM) across multi-embodiment datasets, and finetune it on a small amount of\nrobot data from the target embodiment. Second, we develop a method, Latent\nPolicy Steering (LPS), to improve the output of a behavior-cloned policy by\nsearching in the latent space of the WM for better action sequences. In real\nworld experiments, we observe significant improvements in the performance of\npolicies trained with a small amount of data (over 50% relative improvement\nwith 30 demonstrations and over 20% relative improvement with 50\ndemonstrations) by combining the policy with a WM pretrained on two thousand\nepisodes sampled from the existing Open X-embodiment dataset across different\nrobots or a cost-effective human dataset from play.", "AI": {"tldr": "\u901a\u8fc7\u6a21\u4eff\u5b66\u4e60\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\uff0c\u4f46\u4f9d\u8d56\u5927\u91cf\u8bad\u7ec3\u6570\u636e\u3002\u672c\u6587\u63d0\u51fa\u5229\u7528\u591a\u4f53\u73b0\u6570\u636e\uff08\u5982\u516c\u5f00\u673a\u5668\u4eba\u6570\u636e\u96c6\u548c\u4eba\u7c7b\u73a9\u800d\u6570\u636e\uff09\u51cf\u5c11\u6570\u636e\u6536\u96c6\uff0c\u7ed3\u5408\u5149\u6d41\u548c\u4e16\u754c\u6a21\u578b\uff08WM\uff09\u4f18\u5316\u7b56\u7565\u3002", "motivation": "\u51cf\u5c11\u5b66\u4e60\u89c6\u89c9\u8fd0\u52a8\u673a\u5668\u4eba\u7b56\u7565\u65f6\u7684\u6570\u636e\u6536\u96c6\u6210\u672c\uff0c\u5229\u7528\u73b0\u6709\u6216\u4f4e\u6210\u672c\u7684\u591a\u4f53\u73b0\u6570\u636e\u3002", "method": "\u4f7f\u7528\u5149\u6d41\u4f5c\u4e3a\u4f53\u73b0\u65e0\u5173\u7684\u52a8\u4f5c\u8868\u793a\u8bad\u7ec3\u4e16\u754c\u6a21\u578b\uff08WM\uff09\uff0c\u5e76\u7ed3\u5408\u6f5c\u5728\u7b56\u7565\u5f15\u5bfc\uff08LPS\uff09\u4f18\u5316\u884c\u4e3a\u514b\u9686\u7b56\u7565\u3002", "result": "\u5728\u5c11\u91cf\u6570\u636e\u4e0b\u663e\u8457\u63d0\u5347\u7b56\u7565\u6027\u80fd\uff0830\u6b21\u6f14\u793a\u63d0\u534750%\uff0c50\u6b21\u6f14\u793a\u63d0\u534720%\uff09\u3002", "conclusion": "\u7ed3\u5408\u591a\u4f53\u73b0\u6570\u636e\u548cWM\u9884\u8bad\u7ec3\uff0c\u53ef\u9ad8\u6548\u63d0\u5347\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\u6027\u80fd\u3002"}}
