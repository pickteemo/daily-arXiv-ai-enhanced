{"id": "2507.10562", "categories": ["cs.AI", "cs.CR", "cs.DB", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10562", "abs": "https://arxiv.org/abs/2507.10562", "authors": ["Hari Masoor"], "title": "SAMEP: A Secure Protocol for Persistent Context Sharing Across AI Agents", "comment": "7 pages, 4 figures, 3 implementation examples. Original work\n  submitted as a preprint", "summary": "Current AI agent architectures suffer from ephemeral memory limitations,\npreventing effective collaboration and knowledge sharing across sessions and\nagent boundaries. We introduce SAMEP (Secure Agent Memory Exchange Protocol), a\nnovel framework that enables persistent, secure, and semantically searchable\nmemory sharing among AI agents. Our protocol addresses three critical\nchallenges: (1) persistent context preservation across agent sessions, (2)\nsecure multi-agent collaboration with fine-grained access control, and (3)\nefficient semantic discovery of relevant historical context. SAMEP implements a\ndistributed memory repository with vector-based semantic search, cryptographic\naccess controls (AES-256-GCM), and standardized APIs compatible with existing\nagent communication protocols (MCP, A2A). We demonstrate SAMEP's effectiveness\nacross diverse domains including multi-agent software development, healthcare\nAI with HIPAA compliance, and multi-modal processing pipelines. Experimental\nresults show 73% reduction in redundant computations, 89% improvement in\ncontext relevance scores, and complete compliance with regulatory requirements\nincluding audit trail generation. SAMEP enables a new paradigm of persistent,\ncollaborative AI agent ecosystems while maintaining security and privacy\nguarantees.", "AI": {"tldr": "SAMEP\u662f\u4e00\u79cd\u65b0\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u6301\u4e45\u5316\u3001\u5b89\u5168\u4e14\u53ef\u8bed\u4e49\u641c\u7d22\u7684\u8bb0\u5fc6\u5171\u4eab\u534f\u8bae\uff0c\u89e3\u51b3AI\u4ee3\u7406\u7684\u77ed\u6682\u8bb0\u5fc6\u9650\u5236\u95ee\u9898\u3002", "motivation": "\u5f53\u524dAI\u4ee3\u7406\u67b6\u6784\u5b58\u5728\u8bb0\u5fc6\u77ed\u6682\u6027\u95ee\u9898\uff0c\u963b\u788d\u4e86\u8de8\u4f1a\u8bdd\u548c\u4ee3\u7406\u8fb9\u754c\u7684\u6709\u6548\u534f\u4f5c\u4e0e\u77e5\u8bc6\u5171\u4eab\u3002", "method": "SAMEP\u91c7\u7528\u5206\u5e03\u5f0f\u8bb0\u5fc6\u5b58\u50a8\u5e93\uff0c\u7ed3\u5408\u5411\u91cf\u8bed\u4e49\u641c\u7d22\u3001\u52a0\u5bc6\u8bbf\u95ee\u63a7\u5236\u548c\u6807\u51c6\u5316API\uff0c\u652f\u6301\u591a\u4ee3\u7406\u534f\u4f5c\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0cSAMEP\u51cf\u5c11\u4e8673%\u7684\u5197\u4f59\u8ba1\u7b97\uff0c\u63d0\u5347\u4e8689%\u7684\u4e0a\u4e0b\u6587\u76f8\u5173\u6027\uff0c\u5e76\u5b8c\u5168\u7b26\u5408\u6cd5\u89c4\u8981\u6c42\u3002", "conclusion": "SAMEP\u4e3a\u6301\u4e45\u5316\u3001\u534f\u4f5c\u5f0fAI\u4ee3\u7406\u751f\u6001\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b89\u5168\u4e14\u9690\u79c1\u4fdd\u969c\u7684\u65b0\u8303\u5f0f\u3002"}}
{"id": "2507.10602", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10602", "abs": "https://arxiv.org/abs/2507.10602", "authors": ["Maximilian St\u00f6lzle", "T. Konstantin Rusch", "Zach J. Patterson", "Rodrigo P\u00e9rez-Dattari", "Francesco Stella", "Josie Hughes", "Cosimo Della Santina", "Daniela Rus"], "title": "Learning to Move in Rhythm: Task-Conditioned Motion Policies with Orbital Stability Guarantees", "comment": "73 pages", "summary": "Learning from demonstration provides a sample-efficient approach to acquiring\ncomplex behaviors, enabling robots to move robustly, compliantly, and with\nfluidity. In this context, Dynamic Motion Primitives offer built - in stability\nand robustness to disturbances but often struggle to capture complex periodic\nbehaviors. Moreover, they are limited in their ability to interpolate between\ndifferent tasks. These shortcomings substantially narrow their applicability,\nexcluding a wide class of practically meaningful tasks such as locomotion and\nrhythmic tool use. In this work, we introduce Orbitally Stable Motion\nPrimitives (OSMPs) - a framework that combines a learned diffeomorphic encoder\nwith a supercritical Hopf bifurcation in latent space, enabling the accurate\nacquisition of periodic motions from demonstrations while ensuring formal\nguarantees of orbital stability and transverse contraction. Furthermore, by\nconditioning the bijective encoder on the task, we enable a single learned\npolicy to represent multiple motion objectives, yielding consistent zero-shot\ngeneralization to unseen motion objectives within the training distribution. We\nvalidate the proposed approach through extensive simulation and real-world\nexperiments across a diverse range of robotic platforms - from collaborative\narms and soft manipulators to a bio-inspired rigid-soft turtle robot -\ndemonstrating its versatility and effectiveness in consistently outperforming\nstate-of-the-art baselines such as diffusion policies, among others.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aOSMPs\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u5b66\u4e60\u5230\u7684\u5fae\u5206\u540c\u80da\u7f16\u7801\u5668\u548c\u8d85\u4e34\u754cHopf\u5206\u5c94\uff0c\u89e3\u51b3\u4e86\u52a8\u6001\u8fd0\u52a8\u57fa\u5143\u5728\u6355\u83b7\u5468\u671f\u6027\u884c\u4e3a\u548c\u4efb\u52a1\u63d2\u503c\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u8f68\u9053\u7a33\u5b9a\u6027\u548c\u6a2a\u5411\u6536\u7f29\u7684\u6b63\u5f0f\u4fdd\u8bc1\u3002", "motivation": "\u52a8\u6001\u8fd0\u52a8\u57fa\u5143\uff08DMPs\uff09\u5728\u7a33\u5b9a\u6027\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u590d\u6742\u5468\u671f\u6027\u884c\u4e3a\u548c\u4efb\u52a1\u63d2\u503c\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u8303\u56f4\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86OSMPs\u6846\u67b6\u3002", "method": "OSMPs\u7ed3\u5408\u4e86\u5b66\u4e60\u5230\u7684\u5fae\u5206\u540c\u80da\u7f16\u7801\u5668\u548c\u8d85\u4e34\u754cHopf\u5206\u5c94\uff0c\u901a\u8fc7\u4efb\u52a1\u6761\u4ef6\u5316\u7684\u53cc\u5c04\u7f16\u7801\u5668\uff0c\u5b9e\u73b0\u4e86\u5bf9\u5468\u671f\u6027\u8fd0\u52a8\u7684\u51c6\u786e\u5b66\u4e60\uff0c\u5e76\u652f\u6301\u591a\u4efb\u52a1\u8868\u793a\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0cOSMPs\u5728\u591a\u79cd\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\uff08\u5982\u534f\u4f5c\u81c2\u3001\u8f6f\u4f53\u673a\u68b0\u624b\u548c\u4eff\u751f\u9f9f\u673a\u5668\u4eba\uff09\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff08\u5982\u6269\u6563\u7b56\u7565\uff09\u3002", "conclusion": "OSMPs\u6846\u67b6\u5728\u5468\u671f\u6027\u8fd0\u52a8\u5b66\u4e60\u548c\u4efb\u52a1\u6cdb\u5316\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u673a\u5668\u4eba\u5b66\u4e60\u590d\u6742\u884c\u4e3a\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u548c\u7a33\u5b9a\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.10566", "categories": ["cs.AI", "cs.GT", "cs.LG", "cs.MA", "cs.NE", "68T07, 68T40, 91A20", "I.2.6; I.2.11; I.2.4"], "pdf": "https://arxiv.org/pdf/2507.10566", "abs": "https://arxiv.org/abs/2507.10566", "authors": ["Hung Ming Liu"], "title": "AI Mother Tongue: Self-Emergent Communication in MARL via Endogenous Symbol Systems", "comment": "30 pages, 4 figures", "summary": "In Decentralized Multi-Agent Reinforcement Learning (MARL), the development\nof Emergent Communication has long been constrained by the ``Joint Exploration\nDilemma'', leading agents to fall into a ``Communication Vacuum Equilibrium'' .\nTraditional methods address this by introducing inductive biases to facilitate\ncommunication emergence . This study fundamentally questions whether such\nartificial inductive biases are, in fact, over-engineering. Through experiments\nwith the ``AI Mother Tongue'' (AIM) framework, based on a Vector Quantized\nVariational Autoencoder (VQ-VAE), we demonstrate that when agents possess an\nendogenous symbol system, their neural representations naturally exhibit\nspontaneous semantic compression and Nash equilibrium-driven semantic\nconvergence, achieving effective symbolic communication without external\ninductive biases. This aligns with recent neuroscience findings suggesting that\nthe human brain does not directly use human language for internal thought , and\nresonates with research on ``soft thinking'' capabilities in Large Language\nModels (LLMs) . Compared to traditional explicit communication methods, AIM\ndemonstrates stronger generality and efficiency. The interpretable analysis\ntoolkit developed in this study confirms that symbol usage exhibits a\nsignificant power-law distribution, leading to three major theoretical\ninsights: the ``Neural Communication Hypothesis'', the ``Tool-First\nPrinciple'', and the ``Semantic Interpretability Paradigm''. Future research\nwill explore the integration of Hierarchical Quantized Variational Autoencoders\n(HQ-VAE) to enhance AIM's complex expressive capabilities and investigate the\npotential for ``Reinforcement Learning (RL) Low-Level Pre-training''. This\ndiscovery offers new avenues for bridging symbolism and connectionism.", "AI": {"tldr": "\u7814\u7a76\u8d28\u7591\u4f20\u7edf\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u5f15\u5165\u4eba\u5de5\u5f52\u7eb3\u504f\u7f6e\u7684\u5fc5\u8981\u6027\uff0c\u63d0\u51fa\u57fa\u4e8eVQ-VAE\u7684AIM\u6846\u67b6\uff0c\u8bc1\u660e\u667a\u80fd\u4f53\u5185\u751f\u7b26\u53f7\u7cfb\u7edf\u53ef\u5b9e\u73b0\u81ea\u53d1\u8bed\u4e49\u538b\u7f29\u4e0e\u7eb3\u4ec0\u5747\u8861\u9a71\u52a8\u7684\u8bed\u4e49\u6536\u655b\uff0c\u65e0\u9700\u5916\u90e8\u504f\u7f6e\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u901a\u8fc7\u4eba\u5de5\u5f52\u7eb3\u504f\u7f6e\u4fc3\u8fdb\u901a\u4fe1\uff0c\u4f46\u53ef\u80fd\u8fc7\u5ea6\u8bbe\u8ba1\u3002\u7814\u7a76\u63a2\u7d22\u662f\u5426\u667a\u80fd\u4f53\u5185\u751f\u7b26\u53f7\u7cfb\u7edf\u8db3\u4ee5\u5b9e\u73b0\u9ad8\u6548\u901a\u4fe1\u3002", "method": "\u91c7\u7528\u57fa\u4e8eVQ-VAE\u7684AIM\u6846\u67b6\uff0c\u5206\u6790\u667a\u80fd\u4f53\u7684\u795e\u7ecf\u8868\u5f81\u4e0e\u7b26\u53f7\u7cfb\u7edf\uff0c\u5f00\u53d1\u53ef\u89e3\u91ca\u6027\u5de5\u5177\u5305\u9a8c\u8bc1\u7b26\u53f7\u4f7f\u7528\u89c4\u5f8b\u3002", "result": "AIM\u6846\u67b6\u5b9e\u73b0\u81ea\u53d1\u8bed\u4e49\u538b\u7f29\u4e0e\u8bed\u4e49\u6536\u655b\uff0c\u7b26\u53f7\u4f7f\u7528\u5448\u73b0\u5e42\u5f8b\u5206\u5e03\uff0c\u63d0\u51fa\u4e09\u9879\u7406\u8bba\u89c1\u89e3\uff08\u795e\u7ecf\u901a\u4fe1\u5047\u8bf4\u3001\u5de5\u5177\u4f18\u5148\u539f\u5219\u3001\u8bed\u4e49\u53ef\u89e3\u91ca\u8303\u5f0f\uff09\u3002", "conclusion": "\u5185\u751f\u7b26\u53f7\u7cfb\u7edf\u53ef\u66ff\u4ee3\u4eba\u5de5\u504f\u7f6e\uff0c\u672a\u6765\u5c06\u63a2\u7d22HQ-VAE\u589e\u5f3a\u8868\u8fbe\u529b\u4e0eRL\u4f4e\u5c42\u9884\u8bad\u7ec3\uff0c\u4e3a\u7b26\u53f7\u4e3b\u4e49\u4e0e\u8fde\u63a5\u4e3b\u4e49\u67b6\u6865\u3002"}}
{"id": "2507.10672", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10672", "abs": "https://arxiv.org/abs/2507.10672", "authors": ["Muhayy Ud Din", "Waseem Akram", "Lyes Saad Saoud", "Jan Rosell", "Irfan Hussain"], "title": "Vision Language Action Models in Robotic Manipulation: A Systematic Review", "comment": "submitted to annual review in control", "summary": "Vision Language Action (VLA) models represent a transformative shift in\nrobotics, with the aim of unifying visual perception, natural language\nunderstanding, and embodied control within a single learning framework. This\nreview presents a comprehensive and forward-looking synthesis of the VLA\nparadigm, with a particular emphasis on robotic manipulation and\ninstruction-driven autonomy. We comprehensively analyze 102 VLA models, 26\nfoundational datasets, and 12 simulation platforms that collectively shape the\ndevelopment and evaluation of VLAs models. These models are categorized into\nkey architectural paradigms, each reflecting distinct strategies for\nintegrating vision, language, and control in robotic systems. Foundational\ndatasets are evaluated using a novel criterion based on task complexity,\nvariety of modalities, and dataset scale, allowing a comparative analysis of\ntheir suitability for generalist policy learning. We introduce a\ntwo-dimensional characterization framework that organizes these datasets based\non semantic richness and multimodal alignment, showing underexplored regions in\nthe current data landscape. Simulation environments are evaluated for their\neffectiveness in generating large-scale data, as well as their ability to\nfacilitate transfer from simulation to real-world settings and the variety of\nsupported tasks. Using both academic and industrial contributions, we recognize\nongoing challenges and outline strategic directions such as scalable\npretraining protocols, modular architectural design, and robust multimodal\nalignment strategies. This review serves as both a technical reference and a\nconceptual roadmap for advancing embodiment and robotic control, providing\ninsights that span from dataset generation to real world deployment of\ngeneralist robotic agents.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86\u89c6\u89c9\u8bed\u8a00\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u5728\u673a\u5668\u4eba\u9886\u57df\u7684\u5e94\u7528\uff0c\u5206\u6790\u4e86102\u4e2a\u6a21\u578b\u300126\u4e2a\u6570\u636e\u96c6\u548c12\u4e2a\u4eff\u771f\u5e73\u53f0\uff0c\u63d0\u51fa\u4e86\u5206\u7c7b\u6846\u67b6\u548c\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u7edf\u4e00\u89c6\u89c9\u611f\u77e5\u3001\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u548c\u673a\u5668\u4eba\u63a7\u5236\uff0c\u63a8\u52a8\u901a\u7528\u673a\u5668\u4eba\u4ee3\u7406\u7684\u53d1\u5c55\u3002", "method": "\u901a\u8fc7\u5206\u7c7b\u67b6\u6784\u8303\u5f0f\u3001\u8bc4\u4f30\u6570\u636e\u96c6\u548c\u4eff\u771f\u5e73\u53f0\uff0c\u63d0\u51fa\u4e8c\u7ef4\u8868\u5f81\u6846\u67b6\u3002", "result": "\u53d1\u73b0\u5f53\u524d\u6570\u636e\u96c6\u7684\u4e0d\u8db3\uff0c\u63d0\u51fa\u672a\u6765\u7814\u7a76\u65b9\u5411\u5982\u53ef\u6269\u5c55\u9884\u8bad\u7ec3\u534f\u8bae\u548c\u6a21\u5757\u5316\u8bbe\u8ba1\u3002", "conclusion": "\u672c\u6587\u4e3aVLA\u6a21\u578b\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u6280\u672f\u53c2\u8003\u548c\u6982\u5ff5\u8def\u7ebf\u56fe\uff0c\u4ece\u6570\u636e\u96c6\u751f\u6210\u5230\u5b9e\u9645\u90e8\u7f72\u3002"}}
{"id": "2507.10571", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.10571", "abs": "https://arxiv.org/abs/2507.10571", "authors": ["Konstantinos I. Roumeliotis", "Ranjan Sapkota", "Manoj Karkee", "Nikolaos D. Tselikas"], "title": "Orchestrator-Agent Trust: A Modular Agentic AI Visual Classification System with Trust-Aware Orchestration and RAG-Based Reasoning", "comment": null, "summary": "Modern Artificial Intelligence (AI) increasingly relies on multi-agent\narchitectures that blend visual and language understanding. Yet, a pressing\nchallenge remains: How can we trust these agents especially in zero-shot\nsettings with no fine-tuning? We introduce a novel modular Agentic AI visual\nclassification framework that integrates generalist multimodal agents with a\nnon-visual reasoning orchestrator and a Retrieval-Augmented Generation (RAG)\nmodule. Applied to apple leaf disease diagnosis, we benchmark three\nconfigurations: (I) zero-shot with confidence-based orchestration, (II)\nfine-tuned agents with improved performance, and (III) trust-calibrated\norchestration enhanced by CLIP-based image retrieval and re-evaluation loops.\nUsing confidence calibration metrics (ECE, OCR, CCC), the orchestrator\nmodulates trust across agents. Our results demonstrate a 77.94\\% accuracy\nimprovement in the zero-shot setting using trust-aware orchestration and RAG,\nachieving 85.63\\% overall. GPT-4o showed better calibration, while Qwen-2.5-VL\ndisplayed overconfidence. Furthermore, image-RAG grounded predictions with\nvisually similar cases, enabling correction of agent overconfidence via\niterative re-evaluation. The proposed system separates perception (vision\nagents) from meta-reasoning (orchestrator), enabling scalable and interpretable\nmulti-agent AI. This blueprint is extensible to diagnostics, biology, and other\ntrust-critical domains. All models, prompts, results, and system components\nincluding the complete software source code are openly released to support\nreproducibility, transparency, and community benchmarking at Github:\nhttps://github.com/Applied-AI-Research-Lab/Orchestrator-Agent-Trust", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u5757\u5316\u7684\u591a\u667a\u80fd\u4f53AI\u89c6\u89c9\u5206\u7c7b\u6846\u67b6\uff0c\u901a\u8fc7\u4fe1\u4efb\u611f\u77e5\u7684\u534f\u8c03\u548cRAG\u6a21\u5757\uff0c\u663e\u8457\u63d0\u5347\u4e86\u96f6\u6837\u672c\u573a\u666f\u4e0b\u7684\u51c6\u786e\u7387\u3002", "motivation": "\u89e3\u51b3\u591a\u667a\u80fd\u4f53AI\u5728\u96f6\u6837\u672c\u573a\u666f\u4e0b\u7684\u4fe1\u4efb\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u89c6\u89c9\u548c\u8bed\u8a00\u7406\u89e3\u7684\u878d\u5408\u4e2d\u3002", "method": "\u7ed3\u5408\u901a\u7528\u591a\u6a21\u6001\u667a\u80fd\u4f53\u3001\u975e\u89c6\u89c9\u63a8\u7406\u534f\u8c03\u5668\u548cRAG\u6a21\u5757\uff0c\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u6821\u51c6\u548c\u56fe\u50cf\u68c0\u7d22\u4f18\u5316\u4fe1\u4efb\u5206\u914d\u3002", "result": "\u96f6\u6837\u672c\u573a\u666f\u4e0b\u51c6\u786e\u7387\u63d0\u534777.94%\uff0c\u603b\u4f53\u8fbe\u523085.63%\uff0c\u5e76\u5c55\u793a\u4e86\u4e0d\u540c\u6a21\u578b\u7684\u6821\u51c6\u8868\u73b0\u3002", "conclusion": "\u8be5\u6846\u67b6\u53ef\u6269\u5c55\u81f3\u8bca\u65ad\u3001\u751f\u7269\u5b66\u7b49\u4fe1\u4efb\u5173\u952e\u9886\u57df\uff0c\u6240\u6709\u8d44\u6e90\u5f00\u6e90\u4ee5\u652f\u6301\u590d\u73b0\u548c\u793e\u533a\u57fa\u51c6\u6d4b\u8bd5\u3002"}}
{"id": "2507.10694", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10694", "abs": "https://arxiv.org/abs/2507.10694", "authors": ["Francesco Fuentes", "Serigne Diagne", "Zachary Kingston", "Laura H. Blumenschein"], "title": "Exteroception through Proprioception Sensing through Improved Contact Modeling for Soft Growing Robots", "comment": "22 pages, 21 figures, submitted to journal for potential publication", "summary": "Passive deformation due to compliance is a commonly used benefit of soft\nrobots, providing opportunities to achieve robust actuation with few active\ndegrees of freedom. Soft growing robots in particular have shown promise in\nnavigation of unstructured environments due to their passive deformation. If\ntheir collisions and subsequent deformations can be better understood, soft\nrobots could be used to understand the structure of the environment from direct\ntactile measurements. In this work, we propose the use of soft growing robots\nas mapping and exploration tools. We do this by first characterizing collision\nbehavior during discrete turns, then leveraging this model to develop a\ngeometry-based simulator that models robot trajectories in 2D environments.\nFinally, we demonstrate the model and simulator validity by mapping unknown\nenvironments using Monte Carlo sampling to estimate the optimal next deployment\ngiven current knowledge. Over both uniform and non-uniform environments, this\nselection method rapidly approaches ideal actions, showing the potential for\nsoft growing robots in unstructured environment exploration and mapping.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5229\u7528\u8f6f\u4f53\u751f\u957f\u673a\u5668\u4eba\u4f5c\u4e3a\u73af\u5883\u63a2\u7d22\u548c\u5730\u56fe\u6784\u5efa\u5de5\u5177\uff0c\u901a\u8fc7\u78b0\u649e\u884c\u4e3a\u5efa\u6a21\u548c\u51e0\u4f55\u6a21\u62df\u5668\u5f00\u53d1\uff0c\u7ed3\u5408\u8499\u7279\u5361\u6d1b\u91c7\u6837\u4f18\u5316\u90e8\u7f72\u7b56\u7565\u3002", "motivation": "\u8f6f\u4f53\u673a\u5668\u4eba\u56e0\u5176\u88ab\u52a8\u53d8\u5f62\u7279\u6027\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u7406\u89e3\u5176\u78b0\u649e\u548c\u53d8\u5f62\u884c\u4e3a\u4ee5\u5229\u7528\u89e6\u89c9\u6d4b\u91cf\u73af\u5883\u7ed3\u6784\u3002", "method": "1. \u5206\u6790\u79bb\u6563\u8f6c\u5411\u65f6\u7684\u78b0\u649e\u884c\u4e3a\uff1b2. \u5f00\u53d1\u57fa\u4e8e\u51e0\u4f55\u76842D\u73af\u5883\u673a\u5668\u4eba\u8f68\u8ff9\u6a21\u62df\u5668\uff1b3. \u4f7f\u7528\u8499\u7279\u5361\u6d1b\u91c7\u6837\u4f30\u8ba1\u6700\u4f18\u90e8\u7f72\u7b56\u7565\u3002", "result": "\u5728\u5747\u5300\u548c\u975e\u5747\u5300\u73af\u5883\u4e2d\uff0c\u8be5\u65b9\u6cd5\u80fd\u5feb\u901f\u63a5\u8fd1\u7406\u60f3\u52a8\u4f5c\uff0c\u9a8c\u8bc1\u4e86\u8f6f\u4f53\u751f\u957f\u673a\u5668\u4eba\u5728\u73af\u5883\u63a2\u7d22\u548c\u5730\u56fe\u6784\u5efa\u4e2d\u7684\u6f5c\u529b\u3002", "conclusion": "\u8f6f\u4f53\u751f\u957f\u673a\u5668\u4eba\u53ef\u4f5c\u4e3a\u9ad8\u6548\u7684\u73af\u5883\u63a2\u7d22\u5de5\u5177\uff0c\u5176\u78b0\u649e\u6a21\u578b\u548c\u6a21\u62df\u5668\u4e3a\u672a\u6765\u5e94\u7528\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.10624", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10624", "abs": "https://arxiv.org/abs/2507.10624", "authors": ["Zheng Zhang"], "title": "Comprehension Without Competence: Architectural Limits of LLMs in Symbolic Computation and Reasoning", "comment": "Substantial change to previous version (experiments, theorem,\n  analysis and related work); currently under review at TMLR", "summary": "Large Language Models (LLMs) display striking surface fluency yet\nsystematically fail at tasks requiring symbolic reasoning, arithmetic accuracy,\nand logical consistency. This paper offers a structural diagnosis of such\nfailures, revealing a persistent gap between \\textit{comprehension} and\n\\textit{competence}. Through controlled experiments and architectural analysis,\nwe demonstrate that LLMs often articulate correct principles without reliably\napplying them--a failure rooted not in knowledge access, but in computational\nexecution. We term this phenomenon the computational \\textit{split-brain\nsyndrome}, where instruction and action pathways are geometrically and\nfunctionally dissociated. This core limitation recurs across domains, from\nmathematical operations to relational inferences, and explains why model\nbehavior remains brittle even under idealized prompting. We argue that LLMs\nfunction as powerful pattern completion engines, but lack the architectural\nscaffolding for principled, compositional reasoning. Our findings delineate the\nboundary of current LLM capabilities and motivate future models with\nmetacognitive control, principle lifting, and structurally grounded execution.\nThis diagnosis also clarifies why mechanistic interpretability findings may\nreflect training-specific pattern coordination rather than universal\ncomputational principles, and why the geometric separation between instruction\nand execution pathways suggests limitations in neural introspection and\nmechanistic analysis.", "AI": {"tldr": "LLMs\u8868\u9762\u6d41\u7545\u4f46\u7b26\u53f7\u63a8\u7406\u3001\u7b97\u672f\u51c6\u786e\u6027\u548c\u903b\u8f91\u4e00\u81f4\u6027\u4efb\u52a1\u8868\u73b0\u4e0d\u4f73\uff0c\u7814\u7a76\u53d1\u73b0\u5176\u6838\u5fc3\u95ee\u9898\u662f\u8ba1\u7b97\u6267\u884c\u4e2d\u7684\u201c\u5206\u88c2\u8111\u7efc\u5408\u5f81\u201d\uff0c\u5373\u7406\u89e3\u4e0e\u80fd\u529b\u8131\u8282\u3002", "motivation": "\u63ed\u793aLLMs\u5728\u4efb\u52a1\u5931\u8d25\u4e2d\u7684\u7ed3\u6784\u6027\u539f\u56e0\uff0c\u63a2\u8ba8\u5176\u7406\u89e3\u4e0e\u6267\u884c\u4e4b\u95f4\u7684\u9e3f\u6c9f\u3002", "method": "\u901a\u8fc7\u63a7\u5236\u5b9e\u9a8c\u548c\u67b6\u6784\u5206\u6790\uff0c\u7814\u7a76LLMs\u5728\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u53ca\u5176\u8ba1\u7b97\u6267\u884c\u95ee\u9898\u3002", "result": "LLMs\u80fd\u8868\u8fbe\u6b63\u786e\u539f\u5219\u4f46\u65e0\u6cd5\u53ef\u9760\u5e94\u7528\uff0c\u6839\u6e90\u5728\u4e8e\u8ba1\u7b97\u6267\u884c\u800c\u975e\u77e5\u8bc6\u83b7\u53d6\uff0c\u8868\u73b0\u4e3a\u201c\u5206\u88c2\u8111\u7efc\u5408\u5f81\u201d\u3002", "conclusion": "LLMs\u662f\u5f3a\u5927\u7684\u6a21\u5f0f\u5b8c\u6210\u5f15\u64ce\uff0c\u4f46\u7f3a\u4e4f\u7ed3\u6784\u5316\u63a8\u7406\u80fd\u529b\uff0c\u672a\u6765\u9700\u6539\u8fdb\u5143\u8ba4\u77e5\u63a7\u5236\u548c\u6267\u884c\u67b6\u6784\u3002"}}
{"id": "2507.10749", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10749", "abs": "https://arxiv.org/abs/2507.10749", "authors": ["Benjamin Stoler", "Juliet Yang", "Jonathan Francis", "Jean Oh"], "title": "RCG: Safety-Critical Scenario Generation for Robust Autonomous Driving via Real-World Crash Grounding", "comment": null, "summary": "Safety-critical scenarios are essential for training and evaluating\nautonomous driving (AD) systems, yet remain extremely rare in real-world\ndriving datasets. To address this, we propose Real-world Crash Grounding (RCG),\na scenario generation framework that integrates crash-informed semantics into\nadversarial perturbation pipelines. We construct a safety-aware behavior\nrepresentation through contrastive pre-training on large-scale driving logs,\nfollowed by fine-tuning on a small, crash-rich dataset with approximate\ntrajectory annotations extracted from video. This embedding captures semantic\nstructure aligned with real-world accident behaviors and supports selection of\nadversary trajectories that are both high-risk and behaviorally realistic. We\nincorporate the resulting selection mechanism into two prior scenario\ngeneration pipelines, replacing their handcrafted scoring objectives with an\nembedding-based criterion. Experimental results show that ego agents trained\nagainst these generated scenarios achieve consistently higher downstream\nsuccess rates, with an average improvement of 9.2% across seven evaluation\nsettings. Qualitative and quantitative analyses further demonstrate that our\napproach produces more plausible and nuanced adversary behaviors, enabling more\neffective and realistic stress testing of AD systems. Code and tools will be\nreleased publicly.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faReal-world Crash Grounding (RCG)\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u78b0\u649e\u8bed\u4e49\u548c\u5bf9\u6297\u6270\u52a8\u751f\u6210\u5b89\u5168\u5173\u952e\u573a\u666f\uff0c\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u8bad\u7ec3\u6548\u679c\u3002", "motivation": "\u73b0\u5b9e\u9a7e\u9a76\u6570\u636e\u4e2d\u5b89\u5168\u5173\u952e\u573a\u666f\u7a00\u7f3a\uff0c\u96be\u4ee5\u6709\u6548\u8bad\u7ec3\u548c\u8bc4\u4f30\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u3002", "method": "\u901a\u8fc7\u5bf9\u6bd4\u9884\u8bad\u7ec3\u6784\u5efa\u5b89\u5168\u611f\u77e5\u884c\u4e3a\u8868\u793a\uff0c\u7ed3\u5408\u5c0f\u89c4\u6a21\u78b0\u649e\u6570\u636e\u96c6\u5fae\u8c03\uff0c\u5d4c\u5165\u8bed\u4e49\u7ed3\u6784\u4ee5\u751f\u6210\u9ad8\u98ce\u9669\u4e14\u884c\u4e3a\u771f\u5b9e\u7684\u5bf9\u6297\u8f68\u8ff9\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c\u4f7f\u7528\u751f\u6210\u573a\u666f\u8bad\u7ec3\u7684\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4e0b\u6e38\u6210\u529f\u7387\u5e73\u5747\u63d0\u53479.2%\uff0c\u4e14\u751f\u6210\u7684\u5bf9\u6297\u884c\u4e3a\u66f4\u771f\u5b9e\u6709\u6548\u3002", "conclusion": "RCG\u6846\u67b6\u80fd\u751f\u6210\u66f4\u771f\u5b9e\u7684\u9ad8\u98ce\u9669\u573a\u666f\uff0c\u663e\u8457\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u538b\u529b\u6d4b\u8bd5\u6548\u679c\u3002"}}
{"id": "2507.10630", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.10630", "abs": "https://arxiv.org/abs/2507.10630", "authors": ["Ye Yang", "Xue Xiao", "Ping Yin", "Taotao Xie"], "title": "Enhancing the Capabilities of Large Language Models for API calls through Knowledge Graphs", "comment": null, "summary": "API calls by large language models (LLMs) offer a cutting-edge approach for\ndata analysis. However, their ability to effectively utilize tools via API\ncalls remains underexplored in knowledge-intensive domains like meteorology.\nThis paper introduces KG2data, a system that integrates knowledge graphs, LLMs,\nReAct agents, and tool-use technologies to enable intelligent data acquisition\nand query handling in the meteorological field. Using a virtual API, we\nevaluate API call accuracy across three metrics: name recognition failure,\nhallucination failure, and call correctness. KG2data achieves superior\nperformance (1.43%, 0%, 88.57%) compared to RAG2data (16%, 10%, 72.14%) and\nchat2data (7.14%, 8.57%, 71.43%). KG2data differs from typical LLM-based\nsystems by addressing their limited access to domain-specific knowledge, which\nhampers performance on complex or terminology-rich queries. By using a\nknowledge graph as persistent memory, our system enhances content retrieval,\ncomplex query handling, domain-specific reasoning, semantic relationship\nresolution, and heterogeneous data integration. It also mitigates the high cost\nof fine-tuning LLMs, making the system more adaptable to evolving domain\nknowledge and API structures. In summary, KG2data provides a novel solution for\nintelligent, knowledge-based question answering and data analysis in domains\nwith high knowledge demands.", "AI": {"tldr": "KG2data\u7cfb\u7edf\u7ed3\u5408\u77e5\u8bc6\u56fe\u8c31\u3001LLM\u3001ReAct\u4ee3\u7406\u548c\u5de5\u5177\u4f7f\u7528\u6280\u672f\uff0c\u63d0\u5347\u6c14\u8c61\u9886\u57df\u6570\u636e\u67e5\u8be2\u80fd\u529b\uff0c\u6027\u80fd\u4f18\u4e8eRAG2data\u548cchat2data\u3002", "motivation": "\u63a2\u7d22LLM\u901a\u8fc7API\u8c03\u7528\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u9886\u57df\uff08\u5982\u6c14\u8c61\u5b66\uff09\u4e2d\u7684\u5de5\u5177\u4f7f\u7528\u80fd\u529b\uff0c\u89e3\u51b3\u73b0\u6709\u7cfb\u7edf\u5728\u590d\u6742\u67e5\u8be2\u548c\u672f\u8bed\u4e30\u5bcc\u573a\u666f\u4e0b\u7684\u5c40\u9650\u6027\u3002", "method": "\u96c6\u6210\u77e5\u8bc6\u56fe\u8c31\u3001LLM\u3001ReAct\u4ee3\u7406\u548c\u865a\u62dfAPI\uff0c\u8bc4\u4f30API\u8c03\u7528\u7684\u51c6\u786e\u6027\uff08\u540d\u79f0\u8bc6\u522b\u5931\u8d25\u3001\u5e7b\u89c9\u5931\u8d25\u3001\u8c03\u7528\u6b63\u786e\u6027\uff09\u3002", "result": "KG2data\u5728\u540d\u79f0\u8bc6\u522b\u5931\u8d25\uff081.43%\uff09\u3001\u5e7b\u89c9\u5931\u8d25\uff080%\uff09\u548c\u8c03\u7528\u6b63\u786e\u6027\uff0888.57%\uff09\u4e0a\u8868\u73b0\u4f18\u4e8e\u5bf9\u6bd4\u7cfb\u7edf\u3002", "conclusion": "KG2data\u901a\u8fc7\u77e5\u8bc6\u56fe\u8c31\u89e3\u51b3\u4e86LLM\u5728\u9886\u57df\u77e5\u8bc6\u8bbf\u95ee\u4e0a\u7684\u9650\u5236\uff0c\u4e3a\u9ad8\u77e5\u8bc6\u9700\u6c42\u9886\u57df\u63d0\u4f9b\u4e86\u667a\u80fd\u95ee\u7b54\u548c\u6570\u636e\u5206\u6790\u7684\u65b0\u65b9\u6848\u3002"}}
{"id": "2507.10776", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10776", "abs": "https://arxiv.org/abs/2507.10776", "authors": ["Howard H. Qian", "Yiting Chen", "Gaotian Wang", "Podshara Chanrungmaneekul", "Kaiyu Hang"], "title": "rt-RISeg: Real-Time Model-Free Robot Interactive Segmentation for Active Instance-Level Object Understanding", "comment": "8 pages, IROS 2025, Interactive Perception, Segmentation, Robotics,\n  Computer Vision", "summary": "Successful execution of dexterous robotic manipulation tasks in new\nenvironments, such as grasping, depends on the ability to proficiently segment\nunseen objects from the background and other objects. Previous works in unseen\nobject instance segmentation (UOIS) train models on large-scale datasets, which\noften leads to overfitting on static visual features. This dependency results\nin poor generalization performance when confronted with out-of-distribution\nscenarios. To address this limitation, we rethink the task of UOIS based on the\nprinciple that vision is inherently interactive and occurs over time. We\npropose a novel real-time interactive perception framework, rt-RISeg, that\ncontinuously segments unseen objects by robot interactions and analysis of a\ndesigned body frame-invariant feature (BFIF). We demonstrate that the relative\nrotational and linear velocities of randomly sampled body frames, resulting\nfrom selected robot interactions, can be used to identify objects without any\nlearned segmentation model. This fully self-contained segmentation pipeline\ngenerates and updates object segmentation masks throughout each robot\ninteraction without the need to wait for an action to finish. We showcase the\neffectiveness of our proposed interactive perception method by achieving an\naverage object segmentation accuracy rate 27.5% greater than state-of-the-art\nUOIS methods. Furthermore, although rt-RISeg is a standalone framework, we show\nthat the autonomously generated segmentation masks can be used as prompts to\nvision foundation models for significantly improved performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u65f6\u4ea4\u4e92\u611f\u77e5\u6846\u67b6rt-RISeg\uff0c\u901a\u8fc7\u673a\u5668\u4eba\u4ea4\u4e92\u548c\u8bbe\u8ba1\u7684\u4f53\u5e27\u4e0d\u53d8\u7279\u5f81\uff08BFIF\uff09\u5b9e\u73b0\u672a\u89c1\u7269\u4f53\u7684\u8fde\u7eed\u5206\u5272\uff0c\u65e0\u9700\u4f9d\u8d56\u5b66\u4e60\u6a21\u578b\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u672a\u89c1\u7269\u4f53\u5b9e\u4f8b\u5206\u5272\uff08UOIS\uff09\u65b9\u6cd5\u56e0\u4f9d\u8d56\u5927\u89c4\u6a21\u6570\u636e\u96c6\u800c\u5bfc\u81f4\u7684\u8fc7\u62df\u5408\u548c\u6cdb\u5316\u6027\u80fd\u5dee\u7684\u95ee\u9898\u3002", "method": "\u57fa\u4e8e\u4ea4\u4e92\u89c6\u89c9\u539f\u7406\uff0c\u5229\u7528\u673a\u5668\u4eba\u4ea4\u4e92\u4ea7\u751f\u7684\u76f8\u5bf9\u65cb\u8f6c\u548c\u7ebf\u6027\u901f\u5ea6\u8bbe\u8ba1\u4f53\u5e27\u4e0d\u53d8\u7279\u5f81\uff08BFIF\uff09\uff0c\u5b9e\u65f6\u5206\u5272\u7269\u4f53\u3002", "result": "\u5e73\u5747\u5206\u5272\u51c6\u786e\u7387\u6bd4\u73b0\u6709UOIS\u65b9\u6cd5\u9ad827.5%\uff0c\u5e76\u53ef\u4f5c\u4e3a\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u63d0\u793a\u8fdb\u4e00\u6b65\u63d0\u5347\u6027\u80fd\u3002", "conclusion": "rt-RISeg\u901a\u8fc7\u4ea4\u4e92\u611f\u77e5\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u81ea\u7ed9\u81ea\u8db3\u7684\u5206\u5272\uff0c\u4e3a\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.10644", "categories": ["cs.AI", "cs.CL", "cs.CR", "cs.HC", "cs.MA", "I.2.11; I.2.7; C.2.4; K.6.5; I.2.4"], "pdf": "https://arxiv.org/pdf/2507.10644", "abs": "https://arxiv.org/abs/2507.10644", "authors": ["Tatiana Petrova", "Aleksandr Puzikov", "Boris Bliznukov", "Radu State"], "title": "From Semantic Web and MAS to Agentic AI: A Unified Narrative of the Web of Agents", "comment": "33 pages, 9 figures, 8 tables", "summary": "The concept of the Web of Agents (WoA), which transforms the static,\ndocument-centric Web into an environment of autonomous agents acting on users'\nbehalf, has attracted growing interest as large language models (LLMs) become\nmore capable. However, research in this area is still fragmented across\ndifferent communities. Contemporary surveys catalog the latest LLM-powered\nframeworks, while the rich histories of Multi-Agent Systems (MAS) and the\nSemantic Web are often treated as separate, legacy domains. This fragmentation\nobscures the intellectual lineage of modern systems and hinders a holistic\nunderstanding of the field's trajectory. We present the first comprehensive\nevolutionary overview of the WoA. We show that modern protocols like A2A and\nthe MCP, are direct evolutionary responses to the well-documented limitations\nof earlier standards like FIPA standards and OWL-based semantic agents. To\nsystematize this analysis, we introduce a four-axis taxonomy (semantic\nfoundation, communication paradigm, locus of intelligence, discovery\nmechanism). This framework provides a unified analytical lens for comparing\nagent architectures across all generations, revealing a clear line of descent\nwhere others have seen a disconnect. Our analysis identifies a paradigm shift\nin the 'locus of intelligence': from being encoded in external data (Semantic\nWeb) or the platform (MAS) to being embedded within the agent's core model\n(LLM). This shift is foundational to modern Agentic AI, enabling the scalable\nand adaptive systems the WoA has long envisioned. We conclude that while new\nprotocols are essential, they are insufficient for building a robust, open,\ntrustworthy ecosystem. Finally, we argue that the next research frontier lies\nin solving persistent socio-technical challenges, and we map out a new agenda\nfocused on decentralized identity, economic models, security, and governance\nfor the emerging WoA.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u5168\u9762\u7684Web of Agents (WoA)\u6f14\u5316\u6982\u8ff0\uff0c\u63ed\u793a\u4e86\u73b0\u4ee3\u534f\u8bae\u4e0e\u65e9\u671f\u6807\u51c6\u7684\u76f4\u63a5\u8054\u7cfb\uff0c\u5e76\u901a\u8fc7\u56db\u8f74\u5206\u7c7b\u6cd5\u7edf\u4e00\u5206\u6790\u6846\u67b6\uff0c\u6307\u51fa\u667a\u80fd\u6838\u5fc3\u4ece\u5916\u90e8\u6570\u636e\u8f6c\u5411\u4ee3\u7406\u5185\u90e8\u6a21\u578b\u7684\u8303\u5f0f\u8f6c\u53d8\u3002", "motivation": "\u7814\u7a76WoA\u9886\u57df\u7684\u788e\u7247\u5316\u95ee\u9898\uff0c\u6574\u5408\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08MAS\uff09\u548c\u8bed\u4e49Web\u7684\u5386\u53f2\uff0c\u4ee5\u63d0\u4f9b\u5bf9\u9886\u57df\u53d1\u5c55\u7684\u6574\u4f53\u7406\u89e3\u3002", "method": "\u5f15\u5165\u56db\u8f74\u5206\u7c7b\u6cd5\uff08\u8bed\u4e49\u57fa\u7840\u3001\u901a\u4fe1\u8303\u5f0f\u3001\u667a\u80fd\u6838\u5fc3\u3001\u53d1\u73b0\u673a\u5236\uff09\u4f5c\u4e3a\u7edf\u4e00\u5206\u6790\u6846\u67b6\uff0c\u6bd4\u8f83\u4e0d\u540c\u4e16\u4ee3\u7684\u4ee3\u7406\u67b6\u6784\u3002", "result": "\u63ed\u793a\u4e86\u667a\u80fd\u6838\u5fc3\u4ece\u5916\u90e8\u6570\u636e\u6216\u5e73\u53f0\u8f6c\u5411\u4ee3\u7406\u5185\u90e8\u6a21\u578b\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u4e3a\u73b0\u4ee3Agentic AI\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "\u65b0\u534f\u8bae\u867d\u91cd\u8981\uff0c\u4f46\u4e0d\u8db3\u4ee5\u6784\u5efa\u7a33\u5065\u3001\u5f00\u653e\u3001\u53ef\u4fe1\u7684\u751f\u6001\u7cfb\u7edf\uff1b\u672a\u6765\u7814\u7a76\u5e94\u805a\u7126\u4e8e\u53bb\u4e2d\u5fc3\u5316\u8eab\u4efd\u3001\u7ecf\u6d4e\u6a21\u578b\u3001\u5b89\u5168\u548c\u6cbb\u7406\u7b49\u793e\u4f1a\u6280\u672f\u6311\u6218\u3002"}}
{"id": "2507.10814", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10814", "abs": "https://arxiv.org/abs/2507.10814", "authors": ["Huiyi Wang", "Fahim Shahriar", "Alireza Azimi", "Gautham Vasan", "Rupam Mahmood", "Colin Bellinger"], "title": "Versatile and Generalizable Manipulation via Goal-Conditioned Reinforcement Learning with Grounded Object Detection", "comment": "8 pages, 4 figures, 3 tables", "summary": "General-purpose robotic manipulation, including reach and grasp, is essential\nfor deployment into households and workspaces involving diverse and evolving\ntasks. Recent advances propose using large pre-trained models, such as Large\nLanguage Models and object detectors, to boost robotic perception in\nreinforcement learning. These models, trained on large datasets via\nself-supervised learning, can process text prompts and identify diverse objects\nin scenes, an invaluable skill in RL where learning object interaction is\nresource-intensive. This study demonstrates how to integrate such models into\nGoal-Conditioned Reinforcement Learning to enable general and versatile robotic\nreach and grasp capabilities. We use a pre-trained object detection model to\nenable the agent to identify the object from a text prompt and generate a mask\nfor goal conditioning. Mask-based goal conditioning provides object-agnostic\ncues, improving feature sharing and generalization. The effectiveness of the\nproposed framework is demonstrated in a simulated reach-and-grasp task, where\nthe mask-based goal conditioning consistently maintains a $\\sim$90\\% success\nrate in grasping both in and out-of-distribution objects, while also ensuring\nfaster convergence to higher returns.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u9884\u8bad\u7ec3\u5927\u6a21\u578b\uff08\u5982\u8bed\u8a00\u6a21\u578b\u548c\u7269\u4f53\u68c0\u6d4b\u5668\uff09\u878d\u5165\u76ee\u6807\u6761\u4ef6\u5f3a\u5316\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347\u673a\u5668\u4eba\u6293\u53d6\u7684\u901a\u7528\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u901a\u7528\u673a\u5668\u4eba\u64cd\u4f5c\uff08\u5982\u6293\u53d6\uff09\u5728\u5bb6\u5ead\u548c\u5de5\u4f5c\u573a\u666f\u4e2d\u9700\u6c42\u5e7f\u6cdb\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u5b66\u4e60\u7269\u4f53\u4ea4\u4e92\u6210\u672c\u9ad8\u3002\u9884\u8bad\u7ec3\u5927\u6a21\u578b\u80fd\u9ad8\u6548\u5904\u7406\u6587\u672c\u63d0\u793a\u548c\u8bc6\u522b\u7269\u4f53\uff0c\u4e3a\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002", "method": "\u4f7f\u7528\u9884\u8bad\u7ec3\u7269\u4f53\u68c0\u6d4b\u6a21\u578b\uff0c\u901a\u8fc7\u6587\u672c\u63d0\u793a\u8bc6\u522b\u7269\u4f53\u5e76\u751f\u6210\u63a9\u7801\uff0c\u7528\u4e8e\u76ee\u6807\u6761\u4ef6\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u76ee\u6807\u8bbe\u5b9a\u3002\u63a9\u7801\u63d0\u4f9b\u7269\u4f53\u65e0\u5173\u7684\u63d0\u793a\uff0c\u63d0\u5347\u7279\u5f81\u5171\u4eab\u548c\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u5728\u6a21\u62df\u6293\u53d6\u4efb\u52a1\u4e2d\uff0c\u63a9\u7801\u76ee\u6807\u6761\u4ef6\u65b9\u6cd5\u5b9e\u73b0\u4e86\u7ea690%\u7684\u6210\u529f\u7387\uff0c\u4e14\u5bf9\u5206\u5e03\u5185\u5916\u7269\u4f53\u5747\u6709\u6548\uff0c\u540c\u65f6\u6536\u655b\u66f4\u5feb\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u9884\u8bad\u7ec3\u6a21\u578b\u548c\u76ee\u6807\u6761\u4ef6\u5f3a\u5316\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u6293\u53d6\u7684\u901a\u7528\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2507.10740", "categories": ["cs.AI", "cs.NE", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.10740", "abs": "https://arxiv.org/abs/2507.10740", "authors": ["Maziar Kanani", "Sean O Leary", "James McDermott"], "title": "Parsing Musical Structure to Enable Meaningful Variations", "comment": null, "summary": "This paper presents a novel rule-based approach for generating music by\nvarying existing tunes. We parse each tune to find the Pathway Assembly (PA) [\n1], that is a structure representing all repetitions in the tune. The Sequitur\nalgorithm [2 ] is used for this. The result is a grammar. We then carry out\nmutation on the grammar, rather than on a tune directly. There are potentially\n19 types of mutations such as adding, removing, swapping or reversing parts of\nthe grammar that can be applied to the grammars. The system employs one of the\nmutations randomly in this step to automatically manipulate the grammar.\nFollowing the mutation, we need to expand the grammar which returns a new tune.\nThe output after 1 or more mutations will be a new tune related to the original\ntune. Our study examines how tunes change gradually over the course of multiple\nmutations. Edit distances, structural complexity and length of the tunes are\nused to show how a tune is changed after multiple mutations. In addition, the\nsize of effect of each mutation type is analyzed. As a final point, we review\nthe musical aspect of the output tunes. It should be noted that the study only\nfocused on generating new pitch sequences. The study is based on an Irish\ntraditional tune dataset and a list of integers has been used to represent each\ntune's pitch values.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u89c4\u5219\u7684\u97f3\u4e50\u751f\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u53d8\u5f02\u73b0\u6709\u66f2\u8c03\u7684\u8bed\u6cd5\u7ed3\u6784\u751f\u6210\u65b0\u66f2\u8c03\uff0c\u5206\u6790\u53d8\u5f02\u5bf9\u66f2\u8c03\u7684\u5f71\u54cd\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u8bed\u6cd5\u53d8\u5f02\u751f\u6210\u4e0e\u539f\u66f2\u8c03\u76f8\u5173\u7684\u65b0\u97f3\u4e50\uff0c\u5e76\u91cf\u5316\u53d8\u5f02\u5bf9\u66f2\u8c03\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528Sequitur\u7b97\u6cd5\u89e3\u6790\u66f2\u8c03\u4e3a\u8bed\u6cd5\u7ed3\u6784\uff08PA\uff09\uff0c\u968f\u673a\u5e94\u752819\u79cd\u53d8\u5f02\u7c7b\u578b\uff08\u5982\u6dfb\u52a0\u3001\u5220\u9664\u3001\u4ea4\u6362\u7b49\uff09\uff0c\u518d\u6269\u5c55\u8bed\u6cd5\u751f\u6210\u65b0\u66f2\u8c03\u3002", "result": "\u901a\u8fc7\u7f16\u8f91\u8ddd\u79bb\u3001\u7ed3\u6784\u590d\u6742\u5ea6\u548c\u957f\u5ea6\u5206\u6790\u53d8\u5f02\u6548\u679c\uff0c\u5e76\u8bc4\u4f30\u6bcf\u79cd\u53d8\u5f02\u7c7b\u578b\u7684\u5f71\u54cd\u5927\u5c0f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u751f\u6210\u4e0e\u539f\u66f2\u8c03\u76f8\u5173\u7684\u65b0\u97f3\u4e50\uff0c\u4f46\u4ec5\u5173\u6ce8\u97f3\u9ad8\u5e8f\u5217\u751f\u6210\uff0c\u672a\u6d89\u53ca\u5176\u4ed6\u97f3\u4e50\u5143\u7d20\u3002"}}
{"id": "2507.10878", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10878", "abs": "https://arxiv.org/abs/2507.10878", "authors": ["Savva Morozov", "Tobia Marcucci", "Bernhard Paus Graesdal", "Alexandre Amice", "Pablo A. Parrilo", "Russ Tedrake"], "title": "Mixed Discrete and Continuous Planning using Shortest Walks in Graphs of Convex Sets", "comment": "10 pages", "summary": "We study the Shortest-Walk Problem (SWP) in a Graph of Convex Sets (GCS). A\nGCS is a graph where each vertex is paired with a convex program, and each edge\ncouples adjacent programs via additional costs and constraints. A walk in a GCS\nis a sequence of vertices connected by edges, where vertices may be repeated.\nThe length of a walk is given by the cumulative optimal value of the\ncorresponding convex programs. To solve the SWP in GCS, we first synthesize a\npiecewise-quadratic lower bound on the problem's cost-to-go function using\nsemidefinite programming. Then we use this lower bound to guide an\nincremental-search algorithm that yields an approximate shortest walk. We show\nthat the SWP in GCS is a natural language for many mixed discrete-continuous\nplanning problems in robotics, unifying problems that typically require\nspecialized solutions while delivering high performance and computational\nefficiency. We demonstrate this through experiments in collision-free motion\nplanning, skill chaining, and optimal control of hybrid systems.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5728\u51f8\u96c6\u56fe\uff08GCS\uff09\u4e2d\u7684\u6700\u77ed\u8def\u5f84\u95ee\u9898\uff08SWP\uff09\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u534a\u5b9a\u89c4\u5212\u548c\u589e\u91cf\u641c\u7d22\u7684\u8fd1\u4f3c\u89e3\u6cd5\uff0c\u5e76\u5728\u673a\u5668\u4eba\u6df7\u5408\u79bb\u6563-\u8fde\u7eed\u89c4\u5212\u95ee\u9898\u4e2d\u9a8c\u8bc1\u4e86\u5176\u9ad8\u6548\u6027\u3002", "motivation": "\u51f8\u96c6\u56fe\uff08GCS\uff09\u4e3a\u6df7\u5408\u79bb\u6563-\u8fde\u7eed\u89c4\u5212\u95ee\u9898\u63d0\u4f9b\u4e86\u7edf\u4e00\u6846\u67b6\uff0c\u4f46\u7f3a\u4e4f\u9ad8\u6548\u7684\u6c42\u89e3\u65b9\u6cd5\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76SWP\u5728GCS\u4e2d\u7684\u89e3\u6cd5\u3002", "method": "\u901a\u8fc7\u534a\u5b9a\u89c4\u5212\u5408\u6210\u6210\u672c\u51fd\u6570\u7684\u4e8c\u6b21\u4e0b\u754c\uff0c\u5e76\u5229\u7528\u589e\u91cf\u641c\u7d22\u7b97\u6cd5\u8fd1\u4f3c\u6c42\u89e3\u6700\u77ed\u8def\u5f84\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u78b0\u649e\u907f\u514d\u8fd0\u52a8\u89c4\u5212\u3001\u6280\u80fd\u94fe\u548c\u6df7\u5408\u7cfb\u7edf\u6700\u4f18\u63a7\u5236\u7b49\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u9ad8\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "SWP\u5728GCS\u4e2d\u4e3a\u673a\u5668\u4eba\u6df7\u5408\u89c4\u5212\u95ee\u9898\u63d0\u4f9b\u4e86\u901a\u7528\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.10750", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10750", "abs": "https://arxiv.org/abs/2507.10750", "authors": ["Pandu Devarakota", "Nicolas Tsesmetzis", "Faruk O. Alpak", "Apurva Gala", "Detlef Hohl"], "title": "AI and the Net-Zero Journey: Energy Demand, Emissions, and the Potential for Transition", "comment": "Technical article to be submitted to Data Centric Engineering Journal", "summary": "Thanks to the availability of massive amounts of data, computing resources,\nand advanced algorithms, AI has entered nearly every sector. This has sparked\nsignificant investment and interest, particularly in building data centers with\nthe necessary hardware and software to develop and operate AI models and\nAI-based workflows. In this technical review article, we present energy\nconsumption scenarios of data centers and impact on GHG emissions, considering\nboth near-term projections (up to 2030) and long-term outlook (2035 and\nbeyond). We address the quintessential question of whether AI will have a net\npositive, neutral, or negative impact on CO2 emissions by 2035. Additionally,\nwe discuss AI's potential to automate, create efficient and disruptive\nworkflows across various fields related to energy production, supply and\nconsumption. In the near-term scenario, the growing demand for AI will likely\nstrain computing resources, lead to increase in electricity consumption and\ntherefore associated CO2 emissions. This is due to the power-hungry nature of\nbig data centers and the requirements for training and running of large and\ncomplex AI models, as well as the penetration of AI assistant search and\napplications for public use. However, the long-term outlook could be more\npromising. AI has the potential to be a game-changer in CO2 reduction. Its\nability to further automate and optimize processes across industries, from\nenergy production to logistics, could significantly decrease our carbon\nfootprint. This positive impact is anticipated to outweigh the initial\nemissions bump, creating value for businesses and society in areas where\ntraditional solutions have fallen short. In essence, AI might cause some\ninitial growing pains for the environment, but it has the potential to support\nclimate mitigation efforts.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86AI\u6570\u636e\u4e2d\u5fc3\u5bf9\u80fd\u6e90\u6d88\u8017\u548c\u6e29\u5ba4\u6c14\u4f53\u6392\u653e\u7684\u5f71\u54cd\uff0c\u5206\u6790\u4e86\u77ed\u671f\uff08\u81f32030\u5e74\uff09\u548c\u957f\u671f\uff082035\u5e74\u540e\uff09\u7684\u78b3\u6392\u653e\u8d8b\u52bf\uff0c\u5e76\u8bc4\u4f30AI\u5bf9CO2\u6392\u653e\u7684\u51c0\u5f71\u54cd\u3002", "motivation": "\u968f\u7740AI\u6280\u672f\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u6570\u636e\u4e2d\u5fc3\u7684\u80fd\u6e90\u6d88\u8017\u548c\u78b3\u6392\u653e\u95ee\u9898\u65e5\u76ca\u7a81\u51fa\u3002\u672c\u6587\u65e8\u5728\u8bc4\u4f30AI\u5bf9\u73af\u5883\u7684\u77ed\u671f\u548c\u957f\u671f\u5f71\u54cd\uff0c\u63a2\u8ba8\u5176\u662f\u5426\u80fd\u4e3a\u51cf\u6392\u505a\u51fa\u8d21\u732e\u3002", "method": "\u901a\u8fc7\u5206\u6790\u6570\u636e\u4e2d\u5fc3\u7684\u80fd\u6e90\u6d88\u8017\u548c\u78b3\u6392\u653e\u8d8b\u52bf\uff0c\u7ed3\u5408AI\u5728\u80fd\u6e90\u751f\u4ea7\u3001\u4f9b\u5e94\u548c\u6d88\u8d39\u9886\u57df\u7684\u4f18\u5316\u6f5c\u529b\uff0c\u8bc4\u4f30\u5176\u51c0\u73af\u5883\u5f71\u54cd\u3002", "result": "\u77ed\u671f\u5185\uff0cAI\u9700\u6c42\u589e\u957f\u53ef\u80fd\u5bfc\u81f4\u78b3\u6392\u653e\u589e\u52a0\uff1b\u4f46\u957f\u671f\u6765\u770b\uff0cAI\u7684\u4f18\u5316\u80fd\u529b\u6709\u671b\u663e\u8457\u51cf\u5c11\u78b3\u8db3\u8ff9\uff0c\u62b5\u6d88\u521d\u671f\u6392\u653e\u3002", "conclusion": "AI\u521d\u671f\u53ef\u80fd\u5bf9\u73af\u5883\u9020\u6210\u538b\u529b\uff0c\u4f46\u957f\u671f\u6709\u671b\u6210\u4e3a\u51cf\u6392\u7684\u91cd\u8981\u5de5\u5177\uff0c\u4e3a\u6c14\u5019\u7f13\u89e3\u63d0\u4f9b\u652f\u6301\u3002"}}
{"id": "2507.10899", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10899", "abs": "https://arxiv.org/abs/2507.10899", "authors": ["Wang Zhicheng", "Satoshi Yagi", "Satoshi Yamamori", "Jun Morimoto"], "title": "Object-Centric Mobile Manipulation through SAM2-Guided Perception and Imitation Learning", "comment": null, "summary": "Imitation learning for mobile manipulation is a key challenge in the field of\nrobotic manipulation. However, current mobile manipulation frameworks typically\ndecouple navigation and manipulation, executing manipulation only after\nreaching a certain location. This can lead to performance degradation when\nnavigation is imprecise, especially due to misalignment in approach angles. To\nenable a mobile manipulator to perform the same task from diverse orientations,\nan essential capability for building general-purpose robotic models, we propose\nan object-centric method based on SAM2, a foundation model towards solving\npromptable visual segmentation in images, which incorporates manipulation\norientation information into our model. Our approach enables consistent\nunderstanding of the same task from different orientations. We deploy the model\non a custom-built mobile manipulator and evaluate it on a pick-and-place task\nunder varied orientation angles. Compared to Action Chunking Transformer, our\nmodel maintains superior generalization when trained with demonstrations from\nvaried approach angles. This work significantly enhances the generalization and\nrobustness of imitation learning-based mobile manipulation systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eSAM2\u7684\u5bf9\u8c61\u4e2d\u5fc3\u65b9\u6cd5\uff0c\u7528\u4e8e\u63d0\u5347\u79fb\u52a8\u673a\u68b0\u81c2\u5728\u4e0d\u540c\u65b9\u5411\u4e0b\u6267\u884c\u4efb\u52a1\u7684\u80fd\u529b\uff0c\u663e\u8457\u589e\u5f3a\u4e86\u6a21\u4eff\u5b66\u4e60\u7cfb\u7edf\u7684\u6cdb\u5316\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u5f53\u524d\u79fb\u52a8\u673a\u68b0\u81c2\u6846\u67b6\u901a\u5e38\u5c06\u5bfc\u822a\u548c\u64cd\u4f5c\u89e3\u8026\uff0c\u5bfc\u81f4\u5bfc\u822a\u4e0d\u7cbe\u786e\u65f6\u6027\u80fd\u4e0b\u964d\uff0c\u5c24\u5176\u662f\u5728\u89d2\u5ea6\u4e0d\u5bf9\u9f50\u7684\u60c5\u51b5\u4e0b\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u4f7f\u673a\u68b0\u81c2\u80fd\u4ece\u4e0d\u540c\u65b9\u5411\u6267\u884c\u4efb\u52a1\u3002", "method": "\u91c7\u7528\u57fa\u4e8eSAM2\u7684\u5bf9\u8c61\u4e2d\u5fc3\u65b9\u6cd5\uff0c\u5c06\u64cd\u4f5c\u65b9\u5411\u4fe1\u606f\u878d\u5165\u6a21\u578b\uff0c\u5b9e\u73b0\u5bf9\u540c\u4e00\u4efb\u52a1\u5728\u4e0d\u540c\u65b9\u5411\u4e0b\u7684\u4e00\u81f4\u6027\u7406\u89e3\u3002", "result": "\u5728\u81ea\u5b9a\u4e49\u79fb\u52a8\u673a\u68b0\u81c2\u4e0a\u90e8\u7f72\u6a21\u578b\uff0c\u5e76\u5728\u4e0d\u540c\u89d2\u5ea6\u4e0b\u8fdb\u884c\u62fe\u53d6\u653e\u7f6e\u4efb\u52a1\u6d4b\u8bd5\uff0c\u76f8\u6bd4Action Chunking Transformer\uff0c\u6a21\u578b\u5728\u591a\u6837\u5316\u89d2\u5ea6\u8bad\u7ec3\u4e0b\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u6a21\u4eff\u5b66\u4e60\u5728\u79fb\u52a8\u673a\u68b0\u81c2\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.10758", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10758", "abs": "https://arxiv.org/abs/2507.10758", "authors": ["Nikesh Prajapati", "Bimal Karki", "Saroj Gopali", "Akbar Siami Namin"], "title": "IoT Malware Network Traffic Detection using Deep Learning and GraphSAGE Models", "comment": null, "summary": "This paper intends to detect IoT malicious attacks through deep learning\nmodels and demonstrates a comprehensive evaluation of the deep learning and\ngraph-based models regarding malicious network traffic detection. The models\nparticularly are based on GraphSAGE, Bidirectional encoder representations from\ntransformers (BERT), Temporal Convolutional Network (TCN) as well as Multi-Head\nAttention, together with Bidirectional Long Short-Term Memory (BI-LSTM)\nMulti-Head Attention and BI-LSTM and LSTM models. The chosen models\ndemonstrated great performance to model temporal patterns and detect feature\nsignificance. The observed performance are mainly due to the fact that IoT\nsystem traffic patterns are both sequential and diverse, leaving a rich set of\ntemporal patterns for the models to learn. Experimental results showed that\nBERT maintained the best performance. It achieved 99.94% accuracy rate\nalongside high precision and recall, F1-score and AUC-ROC score of 99.99% which\ndemonstrates its capabilities through temporal dependency capture. The\nMulti-Head Attention offered promising results by providing good detection\ncapabilities with interpretable results. On the other side, the Multi-Head\nAttention model required significant processing time like BI-LSTM variants. The\nGraphSAGE model achieved good accuracy while requiring the shortest training\ntime but yielded the lowest accuracy, precision, and F1 score compared to the\nother models", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u68c0\u6d4bIoT\u6076\u610f\u653b\u51fb\uff0c\u8bc4\u4f30\u4e86\u591a\u79cd\u6a21\u578b\uff08\u5982GraphSAGE\u3001BERT\u3001TCN\u7b49\uff09\u5728\u6076\u610f\u6d41\u91cf\u68c0\u6d4b\u4e2d\u7684\u8868\u73b0\uff0c\u5176\u4e2dBERT\u8868\u73b0\u6700\u4f73\u3002", "motivation": "IoT\u7cfb\u7edf\u6d41\u91cf\u6a21\u5f0f\u5177\u6709\u65f6\u5e8f\u6027\u548c\u591a\u6837\u6027\uff0c\u4e3a\u6a21\u578b\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u6570\u636e\uff0c\u56e0\u6b64\u9700\u8981\u9ad8\u6548\u4e14\u51c6\u786e\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u4e86GraphSAGE\u3001BERT\u3001TCN\u3001Multi-Head Attention\u53caBI-LSTM\u7b49\u6a21\u578b\uff0c\u8bc4\u4f30\u5176\u5728\u6076\u610f\u6d41\u91cf\u68c0\u6d4b\u4e2d\u7684\u6027\u80fd\u3002", "result": "BERT\u8868\u73b0\u6700\u4f18\uff0c\u51c6\u786e\u7387\u8fbe99.94%\uff0c\u5176\u4ed6\u6a21\u578b\u5982Multi-Head Attention\u548cGraphSAGE\u5404\u6709\u4f18\u52a3\u3002", "conclusion": "BERT\u5728\u6355\u83b7\u65f6\u5e8f\u4f9d\u8d56\u65b9\u9762\u8868\u73b0\u5353\u8d8a\uff0c\u4f46\u9700\u6743\u8861\u6a21\u578b\u6027\u80fd\u4e0e\u8ba1\u7b97\u6210\u672c\u3002"}}
{"id": "2507.10914", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.10914", "abs": "https://arxiv.org/abs/2507.10914", "authors": ["James A. Preiss", "Fengze Xie", "Yiheng Lin", "Adam Wierman", "Yisong Yue"], "title": "Fast Non-Episodic Adaptive Tuning of Robot Controllers with Online Policy Optimization", "comment": "11 pages, 9 figures", "summary": "We study online algorithms to tune the parameters of a robot controller in a\nsetting where the dynamics, policy class, and optimality objective are all\ntime-varying. The system follows a single trajectory without episodes or state\nresets, and the time-varying information is not known in advance. Focusing on\nnonlinear geometric quadrotor controllers as a test case, we propose a\npractical implementation of a single-trajectory model-based online policy\noptimization algorithm, M-GAPS,along with reparameterizations of the quadrotor\nstate space and policy class to improve the optimization landscape. In hardware\nexperiments,we compare to model-based and model-free baselines that impose\nartificial episodes. We show that M-GAPS finds near-optimal parameters more\nquickly, especially when the episode length is not favorable. We also show that\nM-GAPS rapidly adapts to heavy unmodeled wind and payload disturbances, and\nachieves similar strong improvement on a 1:6-scale Ackermann-steered car. Our\nresults demonstrate the hardware practicality of this emerging class of online\npolicy optimization that offers significantly more flexibility than classic\nadaptive control, while being more stable and data-efficient than model-free\nreinforcement learning.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aM-GAPS\u7684\u5355\u8f68\u8ff9\u5728\u7ebf\u7b56\u7565\u4f18\u5316\u7b97\u6cd5\uff0c\u7528\u4e8e\u52a8\u6001\u8c03\u6574\u673a\u5668\u4eba\u63a7\u5236\u5668\u53c2\u6570\uff0c\u9002\u5e94\u65f6\u53d8\u73af\u5883\uff0c\u5e76\u5728\u786c\u4ef6\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u89e3\u51b3\u673a\u5668\u4eba\u63a7\u5236\u5668\u53c2\u6570\u5728\u52a8\u6001\u53d8\u5316\u73af\u5883\u4e2d\u7684\u5728\u7ebf\u4f18\u5316\u95ee\u9898\uff0c\u907f\u514d\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u91cd\u7f6e\u72b6\u6001\u6216\u9884\u5148\u77e5\u9053\u65f6\u53d8\u4fe1\u606f\u7684\u9650\u5236\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u63d0\u51faM-GAPS\u7b97\u6cd5\uff0c\u91cd\u65b0\u53c2\u6570\u5316\u56db\u65cb\u7ffc\u72b6\u6001\u7a7a\u95f4\u548c\u7b56\u7565\u7c7b\u4ee5\u4f18\u5316\u641c\u7d22\u7a7a\u95f4\uff0c\u5e76\u5728\u786c\u4ef6\u5b9e\u9a8c\u4e2d\u4e0e\u57fa\u4e8e\u6a21\u578b\u548c\u65e0\u6a21\u578b\u7684\u57fa\u7ebf\u65b9\u6cd5\u5bf9\u6bd4\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0cM-GAPS\u80fd\u66f4\u5feb\u627e\u5230\u63a5\u8fd1\u6700\u4f18\u7684\u53c2\u6570\uff0c\u9002\u5e94\u672a\u5efa\u6a21\u7684\u98ce\u548c\u8d1f\u8f7d\u6270\u52a8\uff0c\u5e76\u5728\u4e0d\u540c\u5e73\u53f0\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u7ed3\u8bba\u662fM-GAPS\u5728\u7075\u6d3b\u6027\u548c\u6570\u636e\u6548\u7387\u4e0a\u4f18\u4e8e\u7ecf\u5178\u81ea\u9002\u5e94\u63a7\u5236\u548c\u6a21\u578b\u65e0\u5173\u5f3a\u5316\u5b66\u4e60\uff0c\u5177\u6709\u5b9e\u9645\u786c\u4ef6\u5e94\u7528\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.10761", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.10761", "abs": "https://arxiv.org/abs/2507.10761", "authors": ["Tyler King", "Nikolos Gurney", "John H. Miller", "Volkan Ustun"], "title": "Detecting AI Assistance in Abstract Complex Tasks", "comment": "Accepted to HCII 2025", "summary": "Detecting assistance from artificial intelligence is increasingly important\nas they become ubiquitous across complex tasks such as text generation, medical\ndiagnosis, and autonomous driving. Aid detection is challenging for humans,\nespecially when looking at abstract task data. Artificial neural networks excel\nat classification thanks to their ability to quickly learn from and process\nlarge amounts of data -- assuming appropriate preprocessing. We posit detecting\nhelp from AI as a classification task for such models. Much of the research in\nthis space examines the classification of complex but concrete data classes,\nsuch as images. Many AI assistance detection scenarios, however, result in data\nthat is not machine learning-friendly. We demonstrate that common models can\neffectively classify such data when it is appropriately preprocessed. To do so,\nwe construct four distinct neural network-friendly image formulations along\nwith an additional time-series formulation that explicitly encodes the\nexploration/exploitation of users, which allows for generalizability to other\nabstract tasks. We benchmark the quality of each image formulation across three\nclassical deep learning architectures, along with a parallel CNN-RNN\narchitecture that leverages the additional time series to maximize testing\nperformance, showcasing the importance of encoding temporal and spatial\nquantities for detecting AI aid in abstract tasks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u9884\u5904\u7406\u548c\u795e\u7ecf\u7f51\u7edc\u5206\u7c7b\u68c0\u6d4bAI\u8f85\u52a9\u7684\u65b9\u6cd5\uff0c\u91cd\u70b9\u5904\u7406\u62bd\u8c61\u4efb\u52a1\u6570\u636e\uff0c\u5e76\u5c55\u793a\u4e86\u591a\u79cd\u6570\u636e\u8868\u793a\u5f62\u5f0f\u7684\u6709\u6548\u6027\u3002", "motivation": "\u968f\u7740AI\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u666e\u53ca\uff0c\u68c0\u6d4bAI\u8f85\u52a9\u53d8\u5f97\u91cd\u8981\uff0c\u4f46\u62bd\u8c61\u4efb\u52a1\u6570\u636e\u5bf9\u4f20\u7edf\u65b9\u6cd5\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u6784\u5efa\u56db\u79cd\u795e\u7ecf\u7f51\u7edc\u53cb\u597d\u7684\u56fe\u50cf\u8868\u793a\u5f62\u5f0f\u53ca\u4e00\u79cd\u65f6\u95f4\u5e8f\u5217\u8868\u793a\u5f62\u5f0f\uff0c\u6d4b\u8bd5\u4e0d\u540c\u6df1\u5ea6\u5b66\u4e60\u67b6\u6784\u7684\u5206\u7c7b\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u9002\u5f53\u9884\u5904\u7406\u7684\u6570\u636e\u548c\u7ed3\u5408\u65f6\u7a7a\u4fe1\u606f\u7684\u67b6\u6784\u80fd\u6709\u6548\u68c0\u6d4bAI\u8f85\u52a9\u3002", "conclusion": "\u7f16\u7801\u65f6\u7a7a\u4fe1\u606f\u5bf9\u68c0\u6d4b\u62bd\u8c61\u4efb\u52a1\u4e2d\u7684AI\u8f85\u52a9\u81f3\u5173\u91cd\u8981\uff0c\u4e14\u9884\u5904\u7406\u548c\u67b6\u6784\u9009\u62e9\u663e\u8457\u5f71\u54cd\u6027\u80fd\u3002"}}
{"id": "2507.10950", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10950", "abs": "https://arxiv.org/abs/2507.10950", "authors": ["Zhiwei Wu", "Jiahao Luo", "Siyi Wei", "Jinhui Zhang"], "title": "Unified Modeling and Structural Optimization of Multi-magnet Embedded Soft Continuum Robots for Enhanced Kinematic Performances", "comment": null, "summary": "This paper presents a unified modeling and optimization framework to enhance\nthe kinematic performance of multi-magnet embedded soft continuum robots\n(MeSCRs). To this end, we establish a differentiable system formulation based\non an extended pseudo-rigid-body model. This formulation enables analysis of\nthe equilibrium well-posedness and the geometry of the induced configuration\nunder magnetic actuation. In particular, we show that the maximum controllable\ndegrees of freedom of a MeSCR equal twice the number of embedded magnets. We\nsubsequently develop a structural optimization framework based on differential\ngeometry that links classical kinematic measures (e.g., manipulability and\ndexterity) to the configuration of embedded magnets. The resulting optimization\ncondition reveals that improving local performance requires structurally\nmodulating the spectrum of the configuration space metric to counteract its\ndistortion. Closed-form solutions for optimal magnet configurations are derived\nunder representative conditions, and a gradient-based numerical method is\nproposed for general design scenarios. Simulation studies validate the\neffectiveness of the proposed framework.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u5efa\u6a21\u4e0e\u4f18\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u591a\u78c1\u4f53\u5d4c\u5165\u5f0f\u8f6f\u8fde\u7eed\u4f53\u673a\u5668\u4eba\uff08MeSCRs\uff09\u7684\u8fd0\u52a8\u6027\u80fd\u3002", "motivation": "\u901a\u8fc7\u5efa\u7acb\u53ef\u5fae\u7cfb\u7edf\u516c\u5f0f\uff0c\u5206\u6790\u78c1\u9a71\u52a8\u4e0b\u7684\u5e73\u8861\u9002\u5b9a\u6027\u548c\u8bf1\u5bfc\u6784\u578b\u51e0\u4f55\uff0c\u4ee5\u4f18\u5316\u673a\u5668\u4eba\u7684\u8fd0\u52a8\u6027\u80fd\u3002", "method": "\u57fa\u4e8e\u6269\u5c55\u4f2a\u521a\u4f53\u6a21\u578b\u5efa\u7acb\u53ef\u5fae\u7cfb\u7edf\u516c\u5f0f\uff0c\u5e76\u5f00\u53d1\u57fa\u4e8e\u5fae\u5206\u51e0\u4f55\u7684\u7ed3\u6784\u4f18\u5316\u6846\u67b6\uff0c\u5c06\u7ecf\u5178\u8fd0\u52a8\u5b66\u6307\u6807\u4e0e\u78c1\u4f53\u914d\u7f6e\u5173\u8054\u3002", "result": "\u6700\u5927\u53ef\u63a7\u81ea\u7531\u5ea6\u7b49\u4e8e\u5d4c\u5165\u78c1\u4f53\u6570\u91cf\u7684\u4e24\u500d\uff0c\u4f18\u5316\u6761\u4ef6\u63ed\u793a\u4e86\u5c40\u90e8\u6027\u80fd\u63d0\u5347\u9700\u901a\u8fc7\u8c03\u5236\u6784\u578b\u7a7a\u95f4\u5ea6\u91cf\u8c31\u6765\u62b5\u6d88\u5176\u626d\u66f2\u3002", "conclusion": "\u4eff\u771f\u9a8c\u8bc1\u4e86\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u95ed\u5f0f\u89e3\u548c\u68af\u5ea6\u6570\u503c\u65b9\u6cd5\u4e3a\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5b9e\u7528\u5de5\u5177\u3002"}}
{"id": "2507.10798", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10798", "abs": "https://arxiv.org/abs/2507.10798", "authors": ["Asim H. Gazi", "Bhanu T. Gullapalli", "Daiqi Gao", "Benjamin M. Marlin", "Vivek Shetty", "Susan A. Murphy"], "title": "Uncertainty-Informed Scheduling of Decision Points for Intelligent Mobile Health Interventions", "comment": "4 pages, 3 figures", "summary": "Timely decision making is critical to the effectiveness of mobile health\n(mHealth) interventions. At predefined timepoints called \"decision points,\"\nintelligent mHealth systems such as just-in-time adaptive interventions\n(JITAIs) estimate an individual's biobehavioral context from sensor or survey\ndata and determine whether and how to intervene. For interventions targeting\nhabitual behavior (e.g., oral hygiene), effectiveness often hinges on\ndelivering support shortly before the target behavior is likely to occur.\nCurrent practice schedules decision points at a fixed interval (e.g., one hour)\nbefore user-provided behavior times, and the fixed interval is kept the same\nfor all individuals. However, this one-size-fits-all approach performs poorly\nfor individuals with irregular routines, often scheduling decision points after\nthe target behavior has already occurred, rendering interventions ineffective.\nIn this paper, we propose SigmaScheduling, a method to dynamically schedule\ndecision points based on uncertainty in predicted behavior times. When behavior\ntiming is more predictable, SigmaScheduling schedules decision points closer to\nthe predicted behavior time; when timing is less certain, SigmaScheduling\nschedules decision points earlier, increasing the likelihood of timely\nintervention. We evaluated SigmaScheduling using real-world data from 68\nparticipants in a 10-week trial of Oralytics, a JITAI designed to improve daily\ntoothbrushing. SigmaScheduling increased the likelihood that decision points\npreceded brushing events in at least 70% of cases, preserving opportunities to\nintervene and impact behavior. Our results indicate that SigmaScheduling can\nadvance precision mHealth, particularly for JITAIs targeting time-sensitive,\nhabitual behaviors such as oral hygiene or dietary habits.", "AI": {"tldr": "SigmaScheduling\u52a8\u6001\u8c03\u6574\u51b3\u7b56\u70b9\u65f6\u95f4\uff0c\u63d0\u9ad8\u79fb\u52a8\u5065\u5eb7\u5e72\u9884\u7684\u53ca\u65f6\u6027\u3002", "motivation": "\u56fa\u5b9a\u95f4\u9694\u7684\u51b3\u7b56\u70b9\u8c03\u5ea6\u65b9\u6cd5\u5bf9\u4e60\u60ef\u6027\u884c\u4e3a\u5e72\u9884\u6548\u679c\u4e0d\u4f73\uff0c\u5c24\u5176\u662f\u5bf9\u4f5c\u606f\u4e0d\u89c4\u5f8b\u7684\u7528\u6237\u3002", "method": "\u63d0\u51faSigmaScheduling\u65b9\u6cd5\uff0c\u6839\u636e\u884c\u4e3a\u65f6\u95f4\u9884\u6d4b\u7684\u4e0d\u786e\u5b9a\u6027\u52a8\u6001\u8c03\u6574\u51b3\u7b56\u70b9\u3002", "result": "\u572868\u540d\u53c2\u4e0e\u8005\u7684\u8bd5\u9a8c\u4e2d\uff0cSigmaScheduling\u572870%\u4ee5\u4e0a\u7684\u60c5\u51b5\u4e0b\u786e\u4fdd\u51b3\u7b56\u70b9\u65e9\u4e8e\u76ee\u6807\u884c\u4e3a\u3002", "conclusion": "SigmaScheduling\u80fd\u63d0\u5347\u7cbe\u51c6\u79fb\u52a8\u5065\u5eb7\u5e72\u9884\u7684\u6548\u679c\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u65f6\u95f4\u654f\u611f\u7684\u4e60\u60ef\u6027\u884c\u4e3a\u3002"}}
{"id": "2507.10960", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10960", "abs": "https://arxiv.org/abs/2507.10960", "authors": ["He Zhu", "Ryo Miyoshi", "Yuki Okafuji"], "title": "Whom to Respond To? A Transformer-Based Model for Multi-Party Social Robot Interaction", "comment": null, "summary": "Prior human-robot interaction (HRI) research has primarily focused on\nsingle-user interactions, where robots do not need to consider the timing or\nrecipient of their responses. However, in multi-party interactions, such as at\nmalls and hospitals, social robots must understand the context and decide both\nwhen and to whom they should respond. In this paper, we propose a\nTransformer-based multi-task learning framework to improve the decision-making\nprocess of social robots, particularly in multi-user environments. Considering\nthe characteristics of HRI, we propose two novel loss functions: one that\nenforces constraints on active speakers to improve scene modeling, and another\nthat guides response selection towards utterances specifically directed at the\nrobot. Additionally, we construct a novel multi-party HRI dataset that captures\nreal-world complexities, such as gaze misalignment. Experimental results\ndemonstrate that our model achieves state-of-the-art performance in respond\ndecisions, outperforming existing heuristic-based and single-task approaches.\nOur findings contribute to the development of socially intelligent social\nrobots capable of engaging in natural and context-aware multi-party\ninteractions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u793e\u4ea4\u673a\u5668\u4eba\u5728\u591a\u7528\u6237\u73af\u5883\u4e2d\u7684\u51b3\u7b56\u80fd\u529b\uff0c\u901a\u8fc7\u4e24\u79cd\u65b0\u7684\u635f\u5931\u51fd\u6570\u548c\u65b0\u7684\u6570\u636e\u96c6\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u54cd\u5e94\u51b3\u7b56\u6027\u80fd\u3002", "motivation": "\u591a\u7528\u6237\u73af\u5883\u4e2d\uff0c\u793e\u4ea4\u673a\u5668\u4eba\u9700\u8981\u7406\u89e3\u4e0a\u4e0b\u6587\u5e76\u51b3\u5b9a\u4f55\u65f6\u53ca\u5411\u8c01\u54cd\u5e94\uff0c\u800c\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u5355\u7528\u6237\u4ea4\u4e92\u3002", "method": "\u91c7\u7528Transformer\u591a\u4efb\u52a1\u5b66\u4e60\u6846\u67b6\uff0c\u63d0\u51fa\u4e24\u79cd\u65b0\u635f\u5931\u51fd\u6570\uff1a\u4e00\u79cd\u7ea6\u675f\u4e3b\u52a8\u8bf4\u8bdd\u8005\u4ee5\u6539\u8fdb\u573a\u666f\u5efa\u6a21\uff0c\u53e6\u4e00\u79cd\u5f15\u5bfc\u54cd\u5e94\u9009\u62e9\u9488\u5bf9\u673a\u5668\u4eba\u7684\u8bdd\u8bed\u3002\u6784\u5efa\u4e86\u65b0\u7684\u591a\u7528\u6237HRI\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728\u54cd\u5e94\u51b3\u7b56\u4e0a\u4f18\u4e8e\u73b0\u6709\u542f\u53d1\u5f0f\u548c\u5355\u4efb\u52a1\u65b9\u6cd5\uff0c\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u4e3a\u5f00\u53d1\u5177\u6709\u793e\u4ea4\u667a\u80fd\u7684\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u652f\u6301\uff0c\u4f7f\u5176\u80fd\u591f\u8fdb\u884c\u81ea\u7136\u4e14\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u591a\u7528\u6237\u4ea4\u4e92\u3002"}}
{"id": "2507.10803", "categories": ["cs.AI", "cs.CL", "cs.ET", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.10803", "abs": "https://arxiv.org/abs/2507.10803", "authors": ["JaMor Hairston", "Ritvik Ranjan", "Sahithi Lakamana", "Anthony Spadaro", "Selen Bozkurt", "Jeanmarie Perrone", "Abeed Sarker"], "title": "Automated Thematic Analyses Using LLMs: Xylazine Wound Management Social Media Chatter Use Case", "comment": "Pages: 19, Abstract word count: 151 words, Manuscript word count:\n  2185 words, References: 14, Figures: 3, Tables: 2", "summary": "Background Large language models (LLMs) face challenges in inductive thematic\nanalysis, a task requiring deep interpretive and domain-specific expertise. We\nevaluated the feasibility of using LLMs to replicate expert-driven thematic\nanalysis of social media data. Methods Using two temporally non-intersecting\nReddit datasets on xylazine (n=286 and n=686, for model optimization and\nvalidation, respectively) with twelve expert-derived themes, we evaluated five\nLLMs against expert coding. We modeled the task as a series of binary\nclassifications, rather than a single, multi-label classification, employing\nzero-, single-, and few-shot prompting strategies and measuring performance via\naccuracy, precision, recall, and F1-score. Results On the validation set,\nGPT-4o with two-shot prompting performed best (accuracy: 90.9%; F1-score:\n0.71). For high-prevalence themes, model-derived thematic distributions closely\nmirrored expert classifications (e.g., xylazine use: 13.6% vs. 17.8%; MOUD use:\n16.5% vs. 17.8%). Conclusions Our findings suggest that few-shot LLM-based\napproaches can automate thematic analyses, offering a scalable supplement for\nqualitative research. Keywords: thematic analysis, large language models,\nnatural language processing, qualitative analysis, social media, prompt\nengineering, public health", "AI": {"tldr": "\u7814\u7a76\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4e3b\u9898\u5206\u6790\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0GPT-4o\u5728\u5c11\u91cf\u6837\u672c\u63d0\u793a\u4e0b\u8868\u73b0\u6700\u4f73\uff0c\u53ef\u4f5c\u4e3a\u5b9a\u6027\u7814\u7a76\u7684\u8865\u5145\u5de5\u5177\u3002", "motivation": "\u63a2\u8ba8LLMs\u5728\u9700\u8981\u6df1\u5ea6\u89e3\u91ca\u548c\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u7684\u4e3b\u9898\u5206\u6790\u4efb\u52a1\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u4ee5\u66ff\u4ee3\u6216\u8865\u5145\u4e13\u5bb6\u5206\u6790\u3002", "method": "\u4f7f\u7528\u4e24\u4e2aReddit\u6570\u636e\u96c6\uff0c\u5c06\u4efb\u52a1\u5efa\u6a21\u4e3a\u4e00\u7cfb\u5217\u4e8c\u5143\u5206\u7c7b\uff0c\u91c7\u7528\u96f6\u6837\u672c\u3001\u5355\u6837\u672c\u548c\u5c11\u91cf\u6837\u672c\u63d0\u793a\u7b56\u7565\uff0c\u8bc4\u4f30LLMs\u7684\u8868\u73b0\u3002", "result": "GPT-4o\u5728\u5c11\u91cf\u6837\u672c\u63d0\u793a\u4e0b\u8868\u73b0\u6700\u4f73\uff08\u51c6\u786e\u738790.9%\uff0cF1\u5206\u65700.71\uff09\uff0c\u9ad8\u6d41\u884c\u4e3b\u9898\u7684\u5206\u5e03\u4e0e\u4e13\u5bb6\u5206\u7c7b\u63a5\u8fd1\u3002", "conclusion": "\u5c11\u91cf\u6837\u672cLLM\u65b9\u6cd5\u53ef\u81ea\u52a8\u5316\u4e3b\u9898\u5206\u6790\uff0c\u4e3a\u5b9a\u6027\u7814\u7a76\u63d0\u4f9b\u53ef\u6269\u5c55\u7684\u8865\u5145\u3002"}}
{"id": "2507.10961", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10961", "abs": "https://arxiv.org/abs/2507.10961", "authors": ["Joohwan Seo", "Arvind Kruthiventy", "Soomi Lee", "Megan Teng", "Xiang Zhang", "Seoyeon Choi", "Jongeun Choi", "Roberto Horowitz"], "title": "EquiContact: A Hierarchical SE(3) Vision-to-Force Equivariant Policy for Spatially Generalizable Contact-rich Tasks", "comment": "Submitted to RA-L", "summary": "This paper presents a framework for learning vision-based robotic policies\nfor contact-rich manipulation tasks that generalize spatially across task\nconfigurations. We focus on achieving robust spatial generalization of the\npolicy for the peg-in-hole (PiH) task trained from a small number of\ndemonstrations. We propose EquiContact, a hierarchical policy composed of a\nhigh-level vision planner (Diffusion Equivariant Descriptor Field, Diff-EDF)\nand a novel low-level compliant visuomotor policy (Geometric Compliant ACT,\nG-CompACT). G-CompACT operates using only localized observations (geometrically\nconsistent error vectors (GCEV), force-torque readings, and wrist-mounted RGB\nimages) and produces actions defined in the end-effector frame. Through these\ndesign choices, we show that the entire EquiContact pipeline is\nSE(3)-equivariant, from perception to force control. We also outline three key\ncomponents for spatially generalizable contact-rich policies: compliance,\nlocalized policies, and induced equivariance. Real-world experiments on PiH\ntasks demonstrate a near-perfect success rate and robust generalization to\nunseen spatial configurations, validating the proposed framework and\nprinciples. The experimental videos can be found on the project website:\nhttps://sites.google.com/berkeley.edu/equicontact", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEquiContact\u7684\u5206\u5c42\u6846\u67b6\uff0c\u7528\u4e8e\u5b66\u4e60\u57fa\u4e8e\u89c6\u89c9\u7684\u673a\u5668\u4eba\u7b56\u7565\uff0c\u5728\u63a5\u89e6\u5bc6\u96c6\u578b\u4efb\u52a1\u4e2d\u5b9e\u73b0\u7a7a\u95f4\u6cdb\u5316\u3002", "motivation": "\u89e3\u51b3peg-in-hole\u4efb\u52a1\u4e2d\u4ece\u5c11\u91cf\u6f14\u793a\u8bad\u7ec3\u7684\u7b56\u7565\u5728\u7a7a\u95f4\u914d\u7f6e\u4e0a\u7684\u9c81\u68d2\u6cdb\u5316\u95ee\u9898\u3002", "method": "\u91c7\u7528\u5206\u5c42\u7b56\u7565\uff0c\u5305\u62ec\u9ad8\u5c42\u89c6\u89c9\u89c4\u5212\u5668\uff08Diff-EDF\uff09\u548c\u4f4e\u5c42\u987a\u5e94\u6027\u89c6\u89c9\u8fd0\u52a8\u7b56\u7565\uff08G-CompACT\uff09\uff0c\u5229\u7528\u5c40\u90e8\u89c2\u6d4b\u548cSE(3)-\u7b49\u53d8\u6027\u8bbe\u8ba1\u3002", "result": "\u5728\u771f\u5b9ePiH\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u63a5\u8fd1\u5b8c\u7f8e\u7684\u6210\u529f\u7387\uff0c\u5e76\u5bf9\u672a\u89c1\u7a7a\u95f4\u914d\u7f6e\u8868\u73b0\u51fa\u9c81\u68d2\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "EquiContact\u6846\u67b6\u901a\u8fc7\u987a\u5e94\u6027\u3001\u5c40\u90e8\u7b56\u7565\u548c\u7b49\u53d8\u6027\u8bbe\u8ba1\uff0c\u6709\u6548\u63d0\u5347\u4e86\u63a5\u89e6\u5bc6\u96c6\u578b\u4efb\u52a1\u7684\u7a7a\u95f4\u6cdb\u5316\u6027\u80fd\u3002"}}
{"id": "2507.10831", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10831", "abs": "https://arxiv.org/abs/2507.10831", "authors": ["Yilin Xia", "Heng Zheng", "Shawn Bowers", "Bertram Lud\u00e4scher"], "title": "AF-XRAY: Visual Explanation and Resolution of Ambiguity in Legal Argumentation Frameworks", "comment": "International Conference on Artificial Intelligence and Law (ICAIL),\n  June 16-20, 2025. Chicago, IL, USA", "summary": "Argumentation frameworks (AFs) provide formal approaches for legal reasoning,\nbut identifying sources of ambiguity and explaining argument acceptance remains\nchallenging for non-experts. We present AF-XRAY, an open-source toolkit for\nexploring, analyzing, and visualizing abstract AFs in legal reasoning. AF-XRAY\nintroduces: (i) layered visualizations based on game-theoretic argument length\nrevealing well-founded derivation structures; (ii) classification of attack\nedges by semantic roles (primary, secondary, blunders); (iii) overlay\nvisualizations of alternative 2-valued solutions on ambiguous 3-valued grounded\nsemantics; and (iv) identification of critical attack sets whose suspension\nresolves undecided arguments. Through systematic generation of critical attack\nsets, AF-XRAY transforms ambiguous scenarios into grounded solutions, enabling\nusers to pinpoint specific causes of ambiguity and explore alternative\nresolutions. We use real-world legal cases (e.g., Wild Animals as modeled by\nBench-Capon) to show that our tool supports teleological legal reasoning by\nrevealing how different assumptions lead to different justified conclusions.", "AI": {"tldr": "AF-XRAY\u662f\u4e00\u4e2a\u5f00\u6e90\u5de5\u5177\u5305\uff0c\u7528\u4e8e\u63a2\u7d22\u3001\u5206\u6790\u548c\u53ef\u89c6\u5316\u6cd5\u5f8b\u63a8\u7406\u4e2d\u7684\u62bd\u8c61\u8bba\u8bc1\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u53ef\u89c6\u5316\u548c\u5206\u7c7b\u653b\u51fb\u8fb9\u7b49\u529f\u80fd\uff0c\u5e2e\u52a9\u975e\u4e13\u5bb6\u7406\u89e3\u8bba\u8bc1\u63a5\u53d7\u548c\u89e3\u51b3\u6a21\u7cca\u6027\u3002", "motivation": "\u6cd5\u5f8b\u63a8\u7406\u4e2d\u7684\u8bba\u8bc1\u6846\u67b6\uff08AFs\uff09\u5b58\u5728\u6a21\u7cca\u6027\u548c\u89e3\u91ca\u56f0\u96be\u7684\u95ee\u9898\uff0c\u975e\u4e13\u5bb6\u96be\u4ee5\u7406\u89e3\u3002AF-XRAY\u65e8\u5728\u901a\u8fc7\u53ef\u89c6\u5316\u5de5\u5177\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "AF-XRAY\u63d0\u4f9b\u5206\u5c42\u53ef\u89c6\u5316\u3001\u653b\u51fb\u8fb9\u5206\u7c7b\u3001\u8986\u76d6\u53ef\u89c6\u5316\u6a21\u7cca\u8bed\u4e49\u89e3\u51b3\u65b9\u6848\uff0c\u4ee5\u53ca\u8bc6\u522b\u5173\u952e\u653b\u51fb\u96c6\u4ee5\u89e3\u51b3\u672a\u51b3\u8bba\u8bc1\u3002", "result": "AF-XRAY\u80fd\u591f\u5c06\u6a21\u7cca\u573a\u666f\u8f6c\u5316\u4e3a\u660e\u786e\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5e2e\u52a9\u7528\u6237\u8bc6\u522b\u6a21\u7cca\u539f\u56e0\u5e76\u63a2\u7d22\u66ff\u4ee3\u65b9\u6848\u3002\u901a\u8fc7\u5b9e\u9645\u6cd5\u5f8b\u6848\u4f8b\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "AF-XRAY\u901a\u8fc7\u7cfb\u7edf\u5316\u65b9\u6cd5\u89e3\u51b3\u4e86\u6cd5\u5f8b\u63a8\u7406\u4e2d\u7684\u6a21\u7cca\u6027\u95ee\u9898\uff0c\u652f\u6301\u7528\u6237\u901a\u8fc7\u4e0d\u540c\u5047\u8bbe\u5f97\u51fa\u4e0d\u540c\u7ed3\u8bba\uff0c\u63d0\u5347\u4e86\u6cd5\u5f8b\u63a8\u7406\u7684\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2507.10968", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.10968", "abs": "https://arxiv.org/abs/2507.10968", "authors": ["Toktam Mohammadnejad", "Jovin D'sa", "Behdad Chalaki", "Hossein Nourkhiz Mahjoub", "Ehsan Moradi-Pari"], "title": "SMART-Merge Planner: A Safe Merging and Real-Time Motion Planner for Autonomous Highway On-Ramp Merging", "comment": "Accepted at IEEE ITSC 2025", "summary": "Merging onto a highway is a complex driving task that requires identifying a\nsafe gap, adjusting speed, often interactions to create a merging gap, and\ncompleting the merge maneuver within a limited time window while maintaining\nsafety and driving comfort. In this paper, we introduce a Safe Merging and\nReal-Time Merge (SMART-Merge) planner, a lattice-based motion planner designed\nto facilitate safe and comfortable forced merging. By deliberately adapting\ncost terms to the unique challenges of forced merging and introducing a desired\nspeed heuristic, SMART-Merge planner enables the ego vehicle to merge\nsuccessfully while minimizing the merge time. We verify the efficiency and\neffectiveness of the proposed merge planner through high-fidelity CarMaker\nsimulations on hundreds of highway merge scenarios. Our proposed planner\nachieves the success rate of 100% as well as completes the merge maneuver in\nthe shortest amount of time compared with the baselines, demonstrating our\nplanner's capability to handle complex forced merge tasks and provide a\nreliable and robust solution for autonomous highway merge. The simulation\nresult videos are available at\nhttps://sites.google.com/view/smart-merge-planner/home.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6805\u683c\u7684SMART-Merge\u89c4\u5212\u5668\uff0c\u7528\u4e8e\u5b89\u5168\u9ad8\u6548\u7684\u5f3a\u5236\u5e76\u9053\u4efb\u52a1\u3002", "motivation": "\u9ad8\u901f\u516c\u8def\u5e76\u9053\u662f\u4e00\u9879\u590d\u6742\u7684\u9a7e\u9a76\u4efb\u52a1\uff0c\u9700\u8981\u8bc6\u522b\u5b89\u5168\u95f4\u9699\u3001\u8c03\u6574\u901f\u5ea6\uff0c\u5e76\u5728\u6709\u9650\u65f6\u95f4\u5185\u5b8c\u6210\u5e76\u9053\uff0c\u540c\u65f6\u4fdd\u8bc1\u5b89\u5168\u548c\u8212\u9002\u6027\u3002", "method": "\u901a\u8fc7\u8c03\u6574\u6210\u672c\u51fd\u6570\u4ee5\u9002\u5e94\u5f3a\u5236\u5e76\u9053\u7684\u72ec\u7279\u6311\u6218\uff0c\u5e76\u5f15\u5165\u671f\u671b\u901f\u5ea6\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0cSMART-Merge\u89c4\u5212\u5668\u5b9e\u73b0\u4e86\u5feb\u901f\u4e14\u5b89\u5168\u7684\u5e76\u9053\u3002", "result": "\u5728\u9ad8\u4fdd\u771fCarMaker\u6a21\u62df\u4e2d\uff0c\u8be5\u89c4\u5212\u5668\u5728\u6570\u767e\u79cd\u9ad8\u901f\u5e76\u9053\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86100%\u7684\u6210\u529f\u7387\uff0c\u5e76\u9053\u65f6\u95f4\u6700\u77ed\u3002", "conclusion": "SMART-Merge\u89c4\u5212\u5668\u80fd\u591f\u53ef\u9760\u4e14\u7a33\u5065\u5730\u5904\u7406\u590d\u6742\u7684\u5f3a\u5236\u5e76\u9053\u4efb\u52a1\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u9ad8\u901f\u5e76\u9053\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.10894", "categories": ["cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10894", "abs": "https://arxiv.org/abs/2507.10894", "authors": ["Zongtao He", "Liuyi Wang", "Lu Chen", "Chengju Liu", "Qijun Chen"], "title": "NavComposer: Composing Language Instructions for Navigation Trajectories through Action-Scene-Object Modularization", "comment": null, "summary": "Language-guided navigation is a cornerstone of embodied AI, enabling agents\nto interpret language instructions and navigate complex environments. However,\nexpert-provided instructions are limited in quantity, while synthesized\nannotations often lack quality, making them insufficient for large-scale\nresearch. To address this, we propose NavComposer, a novel framework for\nautomatically generating high-quality navigation instructions. NavComposer\nexplicitly decomposes semantic entities such as actions, scenes, and objects,\nand recomposes them into natural language instructions. Its modular\narchitecture allows flexible integration of state-of-the-art techniques, while\nthe explicit use of semantic entities enhances both the richness and accuracy\nof instructions. Moreover, it operates in a data-agnostic manner, supporting\nadaptation to diverse navigation trajectories without domain-specific training.\nComplementing NavComposer, we introduce NavInstrCritic, a comprehensive\nannotation-free evaluation system that assesses navigation instructions on\nthree dimensions: contrastive matching, semantic consistency, and linguistic\ndiversity. NavInstrCritic provides a holistic evaluation of instruction\nquality, addressing limitations of traditional metrics that rely heavily on\nexpert annotations. By decoupling instruction generation and evaluation from\nspecific navigation agents, our method enables more scalable and generalizable\nresearch. Extensive experiments provide direct and practical evidence for the\neffectiveness of our method.", "AI": {"tldr": "NavComposer\u662f\u4e00\u4e2a\u81ea\u52a8\u751f\u6210\u9ad8\u8d28\u91cf\u5bfc\u822a\u6307\u4ee4\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u89e3\u548c\u91cd\u7ec4\u8bed\u4e49\u5b9e\u4f53\uff08\u5982\u52a8\u4f5c\u3001\u573a\u666f\u548c\u5bf9\u8c61\uff09\u6765\u751f\u6210\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u3002NavInstrCritic\u662f\u4e00\u4e2a\u65e0\u9700\u6807\u6ce8\u7684\u8bc4\u4f30\u7cfb\u7edf\uff0c\u4ece\u5bf9\u6bd4\u5339\u914d\u3001\u8bed\u4e49\u4e00\u81f4\u6027\u548c\u8bed\u8a00\u591a\u6837\u6027\u4e09\u4e2a\u7ef4\u5ea6\u8bc4\u4f30\u6307\u4ee4\u8d28\u91cf\u3002", "motivation": "\u89e3\u51b3\u4e13\u5bb6\u63d0\u4f9b\u7684\u5bfc\u822a\u6307\u4ee4\u6570\u91cf\u6709\u9650\u4e14\u5408\u6210\u6ce8\u91ca\u8d28\u91cf\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u4ee5\u652f\u6301\u5927\u89c4\u6a21\u7814\u7a76\u3002", "method": "NavComposer\u5206\u89e3\u8bed\u4e49\u5b9e\u4f53\u5e76\u91cd\u7ec4\u4e3a\u6307\u4ee4\uff0c\u652f\u6301\u6570\u636e\u65e0\u5173\u7684\u9002\u5e94\uff1bNavInstrCritic\u63d0\u4f9b\u65e0\u6807\u6ce8\u7684\u6307\u4ee4\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u6709\u6548\uff0c\u652f\u6301\u66f4\u53ef\u6269\u5c55\u548c\u901a\u7528\u7684\u7814\u7a76\u3002", "conclusion": "NavComposer\u548cNavInstrCritic\u4e3a\u8bed\u8a00\u5bfc\u822a\u7814\u7a76\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u6307\u4ee4\u751f\u6210\u548c\u8bc4\u4f30\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.10991", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10991", "abs": "https://arxiv.org/abs/2507.10991", "authors": ["Abhimanyu Bhowmik", "Mohit Singh", "Madhushree Sannigrahi", "Martin Ludvigsen", "Kostas Alexis"], "title": "Uncertainty Aware Mapping for Vision-Based Underwater Robots", "comment": "Presented at the 2025 IEEE ICRA Workshop on Field Robotics", "summary": "Vision-based underwater robots can be useful in inspecting and exploring\nconfined spaces where traditional sensors and preplanned paths cannot be\nfollowed. Sensor noise and situational change can cause significant uncertainty\nin environmental representation. Thus, this paper explores how to represent\nmapping inconsistency in vision-based sensing and incorporate depth estimation\nconfidence into the mapping framework. The scene depth and the confidence are\nestimated using the RAFT-Stereo model and are integrated into a voxel-based\nmapping framework, Voxblox. Improvements in the existing Voxblox weight\ncalculation and update mechanism are also proposed. Finally, a qualitative\nanalysis of the proposed method is performed in a confined pool and in a pier\nin the Trondheim fjord. Experiments using an underwater robot demonstrated the\nchange in uncertainty in the visualization.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5728\u57fa\u4e8e\u89c6\u89c9\u7684\u6c34\u4e0b\u673a\u5668\u4eba\u4e2d\u8868\u793a\u5730\u56fe\u4e0d\u4e00\u81f4\u6027\uff0c\u5e76\u5c06\u6df1\u5ea6\u4f30\u8ba1\u7f6e\u4fe1\u5ea6\u878d\u5165\u4f53\u7d20\u5730\u56fe\u6846\u67b6\u4e2d\uff0c\u6539\u8fdb\u4e86Voxblox\u7684\u6743\u91cd\u8ba1\u7b97\u548c\u66f4\u65b0\u673a\u5236\u3002", "motivation": "\u4f20\u7edf\u4f20\u611f\u5668\u548c\u9884\u89c4\u5212\u8def\u5f84\u5728\u72ed\u7a84\u7a7a\u95f4\u4e2d\u7684\u5c40\u9650\u6027\u4fc3\u4f7f\u7814\u7a76\u5982\u4f55\u5229\u7528\u89c6\u89c9\u611f\u77e5\u5904\u7406\u73af\u5883\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u4f7f\u7528RAFT-Stereo\u6a21\u578b\u4f30\u8ba1\u573a\u666f\u6df1\u5ea6\u548c\u7f6e\u4fe1\u5ea6\uff0c\u5e76\u5c06\u5176\u96c6\u6210\u5230Voxblox\u4f53\u7d20\u5730\u56fe\u6846\u67b6\u4e2d\uff0c\u540c\u65f6\u6539\u8fdb\u4e86\u6743\u91cd\u8ba1\u7b97\u548c\u66f4\u65b0\u673a\u5236\u3002", "result": "\u5728\u53d7\u9650\u6c34\u6c60\u548c\u7279\u9686\u8d6b\u59c6\u5ce1\u6e7e\u7801\u5934\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6c34\u4e0b\u673a\u5668\u4eba\u80fd\u591f\u53ef\u89c6\u5316\u4e0d\u786e\u5b9a\u6027\u53d8\u5316\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u89c6\u89c9\u6c34\u4e0b\u673a\u5668\u4eba\u5728\u72ed\u7a84\u7a7a\u95f4\u4e2d\u7684\u73af\u5883\u8868\u793a\u80fd\u529b\u3002"}}
{"id": "2507.10911", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10911", "abs": "https://arxiv.org/abs/2507.10911", "authors": ["Yicong Wu", "Ting Chen", "Irit Hochberg", "Zhoujian Sun", "Ruth Edry", "Zhengxing Huang", "Mor Peleg"], "title": "Lessons Learned from Evaluation of LLM based Multi-agents in Safer Therapy Recommendation", "comment": null, "summary": "Therapy recommendation for chronic patients with multimorbidity is\nchallenging due to risks of treatment conflicts. Existing decision support\nsystems face scalability limitations. Inspired by the way in which general\npractitioners (GP) manage multimorbidity patients, occasionally convening\nmultidisciplinary team (MDT) collaboration, this study investigated the\nfeasibility and value of using a Large Language Model (LLM)-based multi-agent\nsystem (MAS) for safer therapy recommendations. We designed a single agent and\na MAS framework simulating MDT decision-making by enabling discussion among LLM\nagents to resolve medical conflicts. The systems were evaluated on therapy\nplanning tasks for multimorbidity patients using benchmark cases. We compared\nMAS performance with single-agent approaches and real-world benchmarks. An\nimportant contribution of our study is the definition of evaluation metrics\nthat go beyond the technical precision and recall and allow the inspection of\nclinical goals met and medication burden of the proposed advices to a gold\nstandard benchmark. Our results show that with current LLMs, a single agent GP\nperforms as well as MDTs. The best-scoring models provide correct\nrecommendations that address all clinical goals, yet the advices are\nincomplete. Some models also present unnecessary medications, resulting in\nunnecessary conflicts between medication and conditions or drug-drug\ninteractions.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u4f7f\u7528\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u591a\u4ee3\u7406\u7cfb\u7edf\uff08MAS\uff09\u4e3a\u591a\u75c5\u75c7\u6162\u6027\u60a3\u8005\u63d0\u4f9b\u66f4\u5b89\u5168\u7684\u6cbb\u7597\u5efa\u8bae\u7684\u53ef\u884c\u6027\uff0c\u53d1\u73b0\u5355\u4ee3\u7406\u7cfb\u7edf\u8868\u73b0\u4e0e\u591a\u5b66\u79d1\u56e2\u961f\uff08MDT\uff09\u76f8\u5f53\uff0c\u4f46\u5efa\u8bae\u4ecd\u5b58\u5728\u4e0d\u5b8c\u6574\u548c\u4e0d\u5fc5\u8981\u7684\u836f\u7269\u95ee\u9898\u3002", "motivation": "\u591a\u75c5\u75c7\u60a3\u8005\u7684\u6cbb\u7597\u5efa\u8bae\u56e0\u6cbb\u7597\u51b2\u7a81\u98ce\u9669\u800c\u590d\u6742\u5316\uff0c\u73b0\u6709\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u53ef\u6269\u5c55\u6027\u4e0d\u8db3\uff0c\u7814\u7a76\u53d7\u5168\u79d1\u533b\u751f\uff08GP\uff09\u548c\u591a\u5b66\u79d1\u56e2\u961f\uff08MDT\uff09\u534f\u4f5c\u542f\u53d1\uff0c\u63a2\u7d22LLM-MAS\u7684\u6f5c\u529b\u3002", "method": "\u8bbe\u8ba1\u5355\u4ee3\u7406\u548cMAS\u6846\u67b6\u6a21\u62dfMDT\u51b3\u7b56\uff0c\u901a\u8fc7LLM\u4ee3\u7406\u8ba8\u8bba\u89e3\u51b3\u533b\u7597\u51b2\u7a81\uff0c\u5728\u591a\u75c5\u75c7\u60a3\u8005\u6cbb\u7597\u4efb\u52a1\u4e2d\u8bc4\u4f30\u6027\u80fd\uff0c\u5e76\u4e0e\u5355\u4ee3\u7406\u65b9\u6cd5\u548c\u771f\u5b9e\u57fa\u51c6\u5bf9\u6bd4\u3002", "result": "\u5f53\u524dLLM\u4e0b\u5355\u4ee3\u7406GP\u8868\u73b0\u4e0eMDT\u76f8\u5f53\uff0c\u6700\u4f73\u6a21\u578b\u80fd\u63d0\u4f9b\u6ee1\u8db3\u4e34\u5e8a\u76ee\u6807\u7684\u6b63\u786e\u5efa\u8bae\uff0c\u4f46\u5efa\u8bae\u4e0d\u5b8c\u6574\u4e14\u90e8\u5206\u6a21\u578b\u5b58\u5728\u4e0d\u5fc5\u8981\u7684\u836f\u7269\u51b2\u7a81\u3002", "conclusion": "LLM-MAS\u5728\u591a\u75c5\u75c7\u6cbb\u7597\u5efa\u8bae\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u4ee5\u51cf\u5c11\u4e0d\u5b8c\u6574\u5efa\u8bae\u548c\u4e0d\u5fc5\u8981\u7684\u836f\u7269\u51b2\u7a81\u3002"}}
{"id": "2507.11000", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11000", "abs": "https://arxiv.org/abs/2507.11000", "authors": ["Minwoo Cho", "Jaehwi Jang", "Daehyung Park"], "title": "ILCL: Inverse Logic-Constraint Learning from Temporally Constrained Demonstrations", "comment": "8 pages, 6 figures", "summary": "We aim to solve the problem of temporal-constraint learning from\ndemonstrations to reproduce demonstration-like logic-constrained behaviors.\nLearning logic constraints is challenging due to the combinatorially large\nspace of possible specifications and the ill-posed nature of non-Markovian\nconstraints. To figure it out, we introduce a novel temporal-constraint\nlearning method, which we call inverse logic-constraint learning (ILCL). Our\nmethod frames ICL as a two-player zero-sum game between 1) a genetic\nalgorithm-based temporal-logic mining (GA-TL-Mining) and 2) logic-constrained\nreinforcement learning (Logic-CRL). GA-TL-Mining efficiently constructs syntax\ntrees for parameterized truncated linear temporal logic (TLTL) without\npredefined templates. Subsequently, Logic-CRL finds a policy that maximizes\ntask rewards under the constructed TLTL constraints via a novel constraint\nredistribution scheme. Our evaluations show ILCL outperforms state-of-the-art\nbaselines in learning and transferring TL constraints on four temporally\nconstrained tasks. We also demonstrate successful transfer to real-world\npeg-in-shallow-hole tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aILCL\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u9057\u4f20\u7b97\u6cd5\u548c\u903b\u8f91\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\u7684\u535a\u5f08\uff0c\u9ad8\u6548\u5b66\u4e60\u65f6\u95f4\u7ea6\u675f\u903b\u8f91\u3002", "motivation": "\u89e3\u51b3\u4ece\u6f14\u793a\u4e2d\u5b66\u4e60\u65f6\u95f4\u7ea6\u675f\u903b\u8f91\u7684\u95ee\u9898\uff0c\u514b\u670d\u7ec4\u5408\u7a7a\u95f4\u5927\u548c\u975e\u9a6c\u5c14\u53ef\u592b\u7ea6\u675f\u7684\u6311\u6218\u3002", "method": "\u7ed3\u5408\u9057\u4f20\u7b97\u6cd5\u7684\u65f6\u95f4\u903b\u8f91\u6316\u6398\uff08GA-TL-Mining\uff09\u548c\u903b\u8f91\u7ea6\u675f\u5f3a\u5316\u5b66\u4e60\uff08Logic-CRL\uff09\uff0c\u901a\u8fc7\u96f6\u548c\u535a\u5f08\u6846\u67b6\u5b66\u4e60TLTL\u7ea6\u675f\u3002", "result": "\u5728\u56db\u4e2a\u65f6\u95f4\u7ea6\u675f\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u6210\u529f\u8fc1\u79fb\u5230\u771f\u5b9e\u4e16\u754c\u7684\u4efb\u52a1\u3002", "conclusion": "ILCL\u65b9\u6cd5\u5728\u5b66\u4e60\u548c\u8fc1\u79fb\u65f6\u95f4\u7ea6\u675f\u903b\u8f91\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.10923", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10923", "abs": "https://arxiv.org/abs/2507.10923", "authors": ["Yuhao Wang", "Keyan Ding", "Kehua Feng", "Zeyuan Wang", "Ming Qin", "Xiaotong Li", "Qiang Zhang", "Huajun Chen"], "title": "Enhancing Safe and Controllable Protein Generation via Knowledge Preference Optimization", "comment": "Accepted at ACL 2025 (Main Conference)", "summary": "Protein language models have emerged as powerful tools for sequence\ngeneration, offering substantial advantages in functional optimization and\ndenovo design. However, these models also present significant risks of\ngenerating harmful protein sequences, such as those that enhance viral\ntransmissibility or evade immune responses. These concerns underscore critical\nbiosafety and ethical challenges. To address these issues, we propose a\nKnowledge-guided Preference Optimization (KPO) framework that integrates prior\nknowledge via a Protein Safety Knowledge Graph. This framework utilizes an\nefficient graph pruning strategy to identify preferred sequences and employs\nreinforcement learning to minimize the risk of generating harmful proteins.\nExperimental results demonstrate that KPO effectively reduces the likelihood of\nproducing hazardous sequences while maintaining high functionality, offering a\nrobust safety assurance framework for applying generative models in\nbiotechnology.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u77e5\u8bc6\u5f15\u5bfc\u7684\u504f\u597d\u4f18\u5316\uff08KPO\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u86cb\u767d\u8d28\u5b89\u5168\u77e5\u8bc6\u56fe\u8c31\u6574\u5408\u5148\u9a8c\u77e5\u8bc6\uff0c\u51cf\u5c11\u751f\u6210\u6709\u5bb3\u86cb\u767d\u8d28\u5e8f\u5217\u7684\u98ce\u9669\u3002", "motivation": "\u86cb\u767d\u8d28\u8bed\u8a00\u6a21\u578b\u5728\u529f\u80fd\u4f18\u5316\u548c\u8bbe\u8ba1\u4e2d\u6709\u4f18\u52bf\uff0c\u4f46\u53ef\u80fd\u751f\u6210\u6709\u5bb3\u5e8f\u5217\uff0c\u5e26\u6765\u751f\u7269\u5b89\u5168\u548c\u4f26\u7406\u6311\u6218\u3002", "method": "\u7ed3\u5408\u86cb\u767d\u8d28\u5b89\u5168\u77e5\u8bc6\u56fe\u8c31\uff0c\u91c7\u7528\u56fe\u526a\u679d\u7b56\u7565\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u4f18\u5316\u5e8f\u5217\u751f\u6210\u3002", "result": "KPO\u663e\u8457\u964d\u4f4e\u6709\u5bb3\u5e8f\u5217\u751f\u6210\u6982\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u529f\u80fd\u6027\u3002", "conclusion": "KPO\u4e3a\u751f\u7269\u6280\u672f\u4e2d\u7684\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u5b89\u5168\u4fdd\u8bc1\u6846\u67b6\u3002"}}
{"id": "2507.11001", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11001", "abs": "https://arxiv.org/abs/2507.11001", "authors": ["Yanbo Wang", "Zipeng Fang", "Lei Zhao", "Weidong Chen"], "title": "Learning to Tune Like an Expert: Interpretable and Scene-Aware Navigation via MLLM Reasoning and CVAE-Based Adaptation", "comment": null, "summary": "Service robots are increasingly deployed in diverse and dynamic environments,\nwhere both physical layouts and social contexts change over time and across\nlocations. In these unstructured settings, conventional navigation systems that\nrely on fixed parameters often fail to generalize across scenarios, resulting\nin degraded performance and reduced social acceptance. Although recent\napproaches have leveraged reinforcement learning to enhance traditional\nplanners, these methods often fail in real-world deployments due to poor\ngeneralization and limited simulation diversity, which hampers effective\nsim-to-real transfer. To tackle these issues, we present LE-Nav, an\ninterpretable and scene-aware navigation framework that leverages multi-modal\nlarge language model reasoning and conditional variational autoencoders to\nadaptively tune planner hyperparameters. To achieve zero-shot scene\nunderstanding, we utilize one-shot exemplars and chain-of-thought prompting\nstrategies. Additionally, a conditional variational autoencoder captures the\nmapping between natural language instructions and navigation hyperparameters,\nenabling expert-level tuning. Experiments show that LE-Nav can generate\nhyperparameters achieving human-level tuning across diverse planners and\nscenarios. Real-world navigation trials and a user study on a smart wheelchair\nplatform demonstrate that it outperforms state-of-the-art methods on\nquantitative metrics such as success rate, efficiency, safety, and comfort,\nwhile receiving higher subjective scores for perceived safety and social\nacceptance. Code is available at https://github.com/Cavendish518/LE-Nav.", "AI": {"tldr": "LE-Nav\u662f\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u548c\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668\u7684\u5bfc\u822a\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u8c03\u6574\u89c4\u5212\u5668\u8d85\u53c2\u6570\uff0c\u5b9e\u73b0\u96f6\u6837\u672c\u573a\u666f\u7406\u89e3\u548c\u4e13\u5bb6\u7ea7\u8c03\u53c2\uff0c\u663e\u8457\u63d0\u5347\u5bfc\u822a\u6027\u80fd\u548c\u793e\u4f1a\u63a5\u53d7\u5ea6\u3002", "motivation": "\u4f20\u7edf\u5bfc\u822a\u7cfb\u7edf\u5728\u52a8\u6001\u548c\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u56e0\u6cdb\u5316\u80fd\u529b\u5dee\u548c\u4eff\u771f\u591a\u6837\u6027\u4e0d\u8db3\u800c\u96be\u4ee5\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u548c\u6761\u4ef6\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff0c\u7ed3\u5408\u5355\u6837\u672c\u793a\u4f8b\u548c\u601d\u7ef4\u94fe\u63d0\u793a\u7b56\u7565\uff0c\u5b9e\u73b0\u81ea\u9002\u5e94\u8d85\u53c2\u6570\u8c03\u4f18\u3002", "result": "\u5b9e\u9a8c\u8868\u660eLE-Nav\u5728\u591a\u79cd\u89c4\u5212\u5668\u548c\u573a\u666f\u4e2d\u8fbe\u5230\u4eba\u7c7b\u8c03\u53c2\u6c34\u5e73\uff0c\u5b9e\u9645\u5bfc\u822a\u6d4b\u8bd5\u548c\u7528\u6237\u7814\u7a76\u663e\u793a\u5176\u5728\u6210\u529f\u7387\u3001\u6548\u7387\u3001\u5b89\u5168\u6027\u548c\u8212\u9002\u5ea6\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "LE-Nav\u901a\u8fc7\u573a\u666f\u611f\u77e5\u548c\u81ea\u9002\u5e94\u8c03\u53c2\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5bfc\u822a\u7cfb\u7edf\u7684\u6027\u80fd\u548c\u7528\u6237\u63a5\u53d7\u5ea6\uff0c\u4e3a\u52a8\u6001\u73af\u5883\u4e2d\u7684\u670d\u52a1\u673a\u5668\u4eba\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.10993", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10993", "abs": "https://arxiv.org/abs/2507.10993", "authors": ["Emir Durakovic", "Min-Hong Shih"], "title": "Modeling Habitat Shifts: Integrating Convolutional Neural Networks and Tabular Data for Species Migration Prediction", "comment": "This paper uses a lightly modified version of the AAAI 2025 LaTeX\n  style for formatting consistency. It is not a submission to AAAI and does not\n  include any AAAI-specific headers, footers, or metadata", "summary": "Due to climate-induced changes, many habitats are experiencing range shifts\naway from their traditional geographic locations (Piguet, 2011). We propose a\nsolution to accurately model whether bird species are present in a specific\nhabitat through the combination of Convolutional Neural Networks (CNNs)\n(O'Shea, 2015) and tabular data. Our approach makes use of satellite imagery\nand environmental features (e.g., temperature, precipitation, elevation) to\npredict bird presence across various climates. The CNN model captures spatial\ncharacteristics of landscapes such as forestation, water bodies, and\nurbanization, whereas the tabular method uses ecological and geographic data.\nBoth systems predict the distribution of birds with an average accuracy of 85%,\noffering a scalable but reliable method to understand bird migration.", "AI": {"tldr": "\u7ed3\u5408\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u548c\u8868\u683c\u6570\u636e\uff0c\u51c6\u786e\u9884\u6d4b\u9e1f\u7c7b\u5728\u7279\u5b9a\u6816\u606f\u5730\u7684\u5b58\u5728\u3002", "motivation": "\u7531\u4e8e\u6c14\u5019\u53d8\u5316\u5bfc\u81f4\u6816\u606f\u5730\u8303\u56f4\u53d8\u5316\uff0c\u9700\u8981\u4e00\u79cd\u53ef\u9760\u7684\u65b9\u6cd5\u6765\u9884\u6d4b\u9e1f\u7c7b\u5206\u5e03\u3002", "method": "\u5229\u7528\u536b\u661f\u56fe\u50cf\u548c\u73af\u5883\u7279\u5f81\uff08\u5982\u6e29\u5ea6\u3001\u964d\u6c34\u3001\u6d77\u62d4\uff09\uff0c\u7ed3\u5408CNN\u548c\u8868\u683c\u6570\u636e\u5efa\u6a21\u3002", "result": "\u6a21\u578b\u9884\u6d4b\u9e1f\u7c7b\u5206\u5e03\u7684\u51c6\u786e\u7387\u8fbe\u523085%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u7406\u89e3\u9e1f\u7c7b\u8fc1\u5f99\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.11006", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11006", "abs": "https://arxiv.org/abs/2507.11006", "authors": ["Ashutosh Mishra", "Shreya Santra", "Hazal Gozbasi", "Kentaro Uno", "Kazuya Yoshida"], "title": "Enhancing Autonomous Manipulator Control with Human-in-loop for Uncertain Assembly Environments", "comment": "6 pages, 7 figures. Manuscript accepted at the 2025 IEEE 21st\n  International Conference on Automation Science and Engineering (CASE 2025)", "summary": "This study presents an advanced approach to enhance robotic manipulation in\nuncertain and challenging environments, with a focus on autonomous operations\naugmented by human-in-the-loop (HITL) control for lunar missions. By\nintegrating human decision-making with autonomous robotic functions, the\nresearch improves task reliability and efficiency for space applications. The\nkey task addressed is the autonomous deployment of flexible solar panels using\nan extendable ladder-like structure and a robotic manipulator with real-time\nfeedback for precision. The manipulator relays position and force-torque data,\nenabling dynamic error detection and adaptive control during deployment. To\nmitigate the effects of sinkage, variable payload, and low-lighting conditions,\nefficient motion planning strategies are employed, supplemented by human\ncontrol that allows operators to intervene in ambiguous scenarios. Digital twin\nsimulation enhances system robustness by enabling continuous feedback,\niterative task refinement, and seamless integration with the deployment\npipeline. The system has been tested to validate its performance in simulated\nlunar conditions and ensure reliability in extreme lighting, variable terrain,\nchanging payloads, and sensor limitations.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u81ea\u4e3b\u673a\u5668\u4eba\u548c\u4eba\u7c7b\u63a7\u5236\u7684\u5148\u8fdb\u65b9\u6cd5\uff0c\u7528\u4e8e\u6708\u7403\u4efb\u52a1\u4e2d\u4e0d\u786e\u5b9a\u73af\u5883\u4e0b\u7684\u673a\u5668\u4eba\u64cd\u4f5c\uff0c\u91cd\u70b9\u662f\u901a\u8fc7\u5b9e\u65f6\u53cd\u9988\u548c\u6570\u5b57\u5b6a\u751f\u6280\u672f\u63d0\u9ad8\u4efb\u52a1\u53ef\u9760\u6027\u3002", "motivation": "\u89e3\u51b3\u6708\u7403\u4efb\u52a1\u4e2d\u590d\u6742\u73af\u5883\u4e0b\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u6311\u6218\uff0c\u5982\u5730\u5f62\u53d8\u5316\u3001\u5149\u7167\u4e0d\u8db3\u548c\u4f20\u611f\u5668\u9650\u5236\uff0c\u901a\u8fc7\u7ed3\u5408\u4eba\u7c7b\u51b3\u7b56\u548c\u81ea\u4e3b\u529f\u80fd\u63d0\u9ad8\u4efb\u52a1\u6548\u7387\u3002", "method": "\u91c7\u7528\u53ef\u6269\u5c55\u7684\u68af\u72b6\u7ed3\u6784\u548c\u673a\u5668\u4eba\u64cd\u7eb5\u5668\uff0c\u7ed3\u5408\u5b9e\u65f6\u53cd\u9988\u548c\u52a8\u6001\u8bef\u5dee\u68c0\u6d4b\uff0c\u8f85\u4ee5\u9ad8\u6548\u8fd0\u52a8\u89c4\u5212\u548c\u4eba\u7c7b\u5e72\u9884\uff0c\u6570\u5b57\u5b6a\u751f\u6a21\u62df\u7528\u4e8e\u4efb\u52a1\u4f18\u5316\u3002", "result": "\u7cfb\u7edf\u5728\u6a21\u62df\u6708\u7403\u6761\u4ef6\u4e0b\u9a8c\u8bc1\u4e86\u5176\u6027\u80fd\uff0c\u80fd\u591f\u5e94\u5bf9\u6781\u7aef\u5149\u7167\u3001\u5730\u5f62\u53d8\u5316\u548c\u4f20\u611f\u5668\u9650\u5236\uff0c\u63d0\u9ad8\u4e86\u4efb\u52a1\u53ef\u9760\u6027\u3002", "conclusion": "\u7ed3\u5408\u4eba\u7c7b\u63a7\u5236\u548c\u81ea\u4e3b\u6280\u672f\u7684\u7cfb\u7edf\u5728\u590d\u6742\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u672a\u6765\u7a7a\u95f4\u4efb\u52a1\u63d0\u4f9b\u4e86\u53ef\u9760\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.11060", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11060", "abs": "https://arxiv.org/abs/2507.11060", "authors": ["Yilmazcan Ozyurt", "Tunaberk Almaci", "Stefan Feuerriegel", "Mrinmaya Sachan"], "title": "Personalized Exercise Recommendation with Semantically-Grounded Knowledge Tracing", "comment": null, "summary": "We introduce ExRec, a general framework for personalized exercise\nrecommendation with semantically-grounded knowledge tracing. Our method builds\non the observation that existing exercise recommendation approaches simulate\nstudent performance via knowledge tracing (KT) but they often overlook two key\naspects: (a) the semantic content of questions and (b) the sequential,\nstructured progression of student learning. To address this, our ExRec presents\nan end-to-end pipeline, from annotating the KCs of questions and learning their\nsemantic representations to training KT models and optimizing several\nreinforcement learning (RL) methods. Moreover, we improve standard\nQ-learning-based continuous RL methods via a tailored model-based value\nestimation (MVE) approach that directly leverages the components of KT model in\nestimating cumulative knowledge improvement. We validate the effectiveness of\nour ExRec using various RL methods across four real-world tasks with different\neducational goals in online math learning. We further show that ExRec\ngeneralizes robustly to new, unseen questions and that it produces\ninterpretable student learning trajectories. Together, our findings highlight\nthe promise of KT-guided RL for effective personalization in education.", "AI": {"tldr": "ExRec\u662f\u4e00\u4e2a\u7ed3\u5408\u8bed\u4e49\u77e5\u8bc6\u8ffd\u8e2a\u7684\u4e2a\u6027\u5316\u4e60\u9898\u63a8\u8350\u6846\u67b6\uff0c\u901a\u8fc7\u6539\u8fdb\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4f18\u5316\u5b66\u4e60\u8def\u5f84\u3002", "motivation": "\u73b0\u6709\u4e60\u9898\u63a8\u8350\u65b9\u6cd5\u5ffd\u89c6\u95ee\u9898\u7684\u8bed\u4e49\u5185\u5bb9\u548c\u5b66\u4e60\u987a\u5e8f\uff0cExRec\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u7aef\u5230\u7aef\u6d41\u7a0b\uff0c\u5305\u62ec\u6807\u6ce8\u77e5\u8bc6\u70b9\u3001\u5b66\u4e60\u8bed\u4e49\u8868\u793a\u3001\u8bad\u7ec3\u77e5\u8bc6\u8ffd\u8e2a\u6a21\u578b\u53ca\u4f18\u5316\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u3002", "result": "\u5728\u591a\u4e2a\u5b9e\u9645\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u6709\u6548\u6027\uff0c\u80fd\u6cdb\u5316\u5230\u65b0\u95ee\u9898\u5e76\u751f\u6210\u53ef\u89e3\u91ca\u7684\u5b66\u4e60\u8f68\u8ff9\u3002", "conclusion": "\u77e5\u8bc6\u8ffd\u8e2a\u5f15\u5bfc\u7684\u5f3a\u5316\u5b66\u4e60\u5728\u6559\u80b2\u4e2a\u6027\u5316\u4e2d\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2507.11069", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11069", "abs": "https://arxiv.org/abs/2507.11069", "authors": ["Jeongyun Kim", "Seunghoon Jeong", "Giseop Kim", "Myung-Hwan Jeon", "Eunji Jun", "Ayoung Kim"], "title": "TRAN-D: 2D Gaussian Splatting-based Sparse-view Transparent Object Depth Reconstruction via Physics Simulation for Scene Update", "comment": null, "summary": "Understanding the 3D geometry of transparent objects from RGB images is\nchallenging due to their inherent physical properties, such as reflection and\nrefraction. To address these difficulties, especially in scenarios with sparse\nviews and dynamic environments, we introduce TRAN-D, a novel 2D Gaussian\nSplatting-based depth reconstruction method for transparent objects. Our key\ninsight lies in separating transparent objects from the background, enabling\nfocused optimization of Gaussians corresponding to the object. We mitigate\nartifacts with an object-aware loss that places Gaussians in obscured regions,\nensuring coverage of invisible surfaces while reducing overfitting.\nFurthermore, we incorporate a physics-based simulation that refines the\nreconstruction in just a few seconds, effectively handling object removal and\nchain-reaction movement of remaining objects without the need for rescanning.\nTRAN-D is evaluated on both synthetic and real-world sequences, and it\nconsistently demonstrated robust improvements over existing GS-based\nstate-of-the-art methods. In comparison with baselines, TRAN-D reduces the mean\nabsolute error by over 39% for the synthetic TRansPose sequences. Furthermore,\ndespite being updated using only one image, TRAN-D reaches a {\\delta} < 2.5 cm\naccuracy of 48.46%, over 1.5 times that of baselines, which uses six images.\nCode and more results are available at https://jeongyun0609.github.io/TRAN-D/.", "AI": {"tldr": "TRAN-D\u662f\u4e00\u79cd\u57fa\u4e8e2D\u9ad8\u65af\u6cfc\u6e85\u7684\u900f\u660e\u7269\u4f53\u6df1\u5ea6\u91cd\u5efa\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u79bb\u900f\u660e\u7269\u4f53\u4e0e\u80cc\u666f\u5e76\u4f18\u5316\u9ad8\u65af\u5206\u5e03\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7a00\u758f\u89c6\u56fe\u548c\u52a8\u6001\u73af\u5883\u4e0b\u76843D\u51e0\u4f55\u91cd\u5efa\u6548\u679c\u3002", "motivation": "\u900f\u660e\u7269\u4f53\u76843D\u51e0\u4f55\u91cd\u5efa\u56e0\u53cd\u5c04\u548c\u6298\u5c04\u7b49\u7269\u7406\u7279\u6027\u800c\u6781\u5177\u6311\u6218\u6027\uff0c\u5c24\u5176\u5728\u7a00\u758f\u89c6\u56fe\u548c\u52a8\u6001\u73af\u5883\u4e2d\u3002", "method": "TRAN-D\u901a\u8fc7\u5206\u79bb\u900f\u660e\u7269\u4f53\u4e0e\u80cc\u666f\uff0c\u4f18\u5316\u5bf9\u5e94\u9ad8\u65af\u5206\u5e03\uff0c\u5e76\u5f15\u5165\u7269\u4f53\u611f\u77e5\u635f\u5931\u548c\u57fa\u4e8e\u7269\u7406\u7684\u6a21\u62df\uff0c\u51cf\u5c11\u4f2a\u5f71\u5e76\u63d0\u5347\u91cd\u5efa\u6548\u7387\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u5e8f\u5217\u4e2d\uff0cTRAN-D\u7684\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u964d\u4f4e39%\uff0c\u5355\u56fe\u50cf\u66f4\u65b0\u7684\u7cbe\u5ea6\u8fbe48.46%\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "TRAN-D\u5728\u900f\u660e\u7269\u4f53\u91cd\u5efa\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728\u7a00\u758f\u89c6\u56fe\u548c\u52a8\u6001\u73af\u5883\u4e0b\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.11079", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11079", "abs": "https://arxiv.org/abs/2507.11079", "authors": ["Li Wang", "Qizhen Wu", "Lei Chen"], "title": "Tactical Decision for Multi-UGV Confrontation with a Vision-Language Model-Based Commander", "comment": null, "summary": "In multiple unmanned ground vehicle confrontations, autonomously evolving\nmulti-agent tactical decisions from situational awareness remain a significant\nchallenge. Traditional handcraft rule-based methods become vulnerable in the\ncomplicated and transient battlefield environment, and current reinforcement\nlearning methods mainly focus on action manipulation instead of strategic\ndecisions due to lack of interpretability. Here, we propose a vision-language\nmodel-based commander to address the issue of intelligent\nperception-to-decision reasoning in autonomous confrontations. Our method\nintegrates a vision language model for scene understanding and a lightweight\nlarge language model for strategic reasoning, achieving unified perception and\ndecision within a shared semantic space, with strong adaptability and\ninterpretability. Unlike rule-based search and reinforcement learning methods,\nthe combination of the two modules establishes a full-chain process, reflecting\nthe cognitive process of human commanders. Simulation and ablation experiments\nvalidate that the proposed approach achieves a win rate of over 80% compared\nwith baseline models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u6307\u6325\u5b98\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u65e0\u4eba\u5730\u9762\u8f66\u8f86\u5bf9\u6297\u4e2d\u7684\u667a\u80fd\u611f\u77e5\u5230\u51b3\u7b56\u63a8\u7406\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u624b\u5de5\u89c4\u5219\u65b9\u6cd5\u5728\u590d\u6742\u6218\u573a\u73af\u5883\u4e2d\u8868\u73b0\u8106\u5f31\uff0c\u800c\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\uff0c\u4e3b\u8981\u5173\u6ce8\u52a8\u4f5c\u800c\u975e\u6218\u7565\u51b3\u7b56\u3002", "method": "\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u573a\u666f\u7406\u89e3\u548c\u8f7b\u91cf\u7ea7\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u6218\u7565\u63a8\u7406\uff0c\u5b9e\u73b0\u611f\u77e5\u4e0e\u51b3\u7b56\u7684\u7edf\u4e00\u3002", "result": "\u4eff\u771f\u548c\u6d88\u878d\u5b9e\u9a8c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u57fa\u7ebf\u6a21\u578b\u80dc\u7387\u8d85\u8fc780%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u6307\u6325\u5b98\u7684\u8ba4\u77e5\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u9002\u5e94\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2507.11076", "categories": ["cs.RO", "cs.NA", "math.DG", "math.DS", "math.GR", "math.NA"], "pdf": "https://arxiv.org/pdf/2507.11076", "abs": "https://arxiv.org/abs/2507.11076", "authors": ["Andreas Mueller", "Shivesh Kumar"], "title": "Closed Form Time Derivatives of the Equations of Motion of Rigid Body Systems", "comment": null, "summary": "Derivatives of equations of motion(EOM) describing the dynamics of rigid body\nsystems are becoming increasingly relevant for the robotics community and find\nmany applications in design and control of robotic systems. Controlling robots,\nand multibody systems comprising elastic components in particular, not only\nrequires smooth trajectories but also the time derivatives of the control\nforces/torques, hence of the EOM. This paper presents the time derivatives of\nthe EOM in closed form up to second-order as an alternative formulation to the\nexisting recursive algorithms for this purpose, which provides a direct insight\ninto the structure of the derivatives. The Lie group formulation for rigid body\nsystems is used giving rise to very compact and easily parameterized equations.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e8c\u9636\u95ed\u5f0f\u5f62\u5f0f\u7684\u8fd0\u52a8\u65b9\u7a0b\u65f6\u95f4\u5bfc\u6570\uff0c\u66ff\u4ee3\u73b0\u6709\u9012\u5f52\u7b97\u6cd5\uff0c\u4e3a\u673a\u5668\u4eba\u63a7\u5236\u63d0\u4f9b\u66f4\u76f4\u89c2\u7684\u7ed3\u6784\u7406\u89e3\u3002", "motivation": "\u673a\u5668\u4eba\u63a7\u5236\u9700\u8981\u5e73\u6ed1\u8f68\u8ff9\u53ca\u63a7\u5236\u529b/\u529b\u77e9\u7684\u65f6\u95f4\u5bfc\u6570\uff0c\u5c24\u5176\u662f\u542b\u5f39\u6027\u90e8\u4ef6\u7684\u591a\u4f53\u7cfb\u7edf\u3002\u73b0\u6709\u9012\u5f52\u7b97\u6cd5\u590d\u6742\uff0c\u9700\u66f4\u76f4\u63a5\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u674e\u7fa4\u7406\u8bba\u63cf\u8ff0\u521a\u4f53\u7cfb\u7edf\uff0c\u63a8\u5bfc\u51fa\u7d27\u51d1\u4e14\u6613\u53c2\u6570\u5316\u7684\u4e8c\u9636\u65f6\u95f4\u5bfc\u6570\u95ed\u5f0f\u89e3\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u63d0\u4f9b\u4e86\u8fd0\u52a8\u65b9\u7a0b\u65f6\u95f4\u5bfc\u6570\u7684\u76f4\u63a5\u89e3\u6790\u5f62\u5f0f\uff0c\u7ed3\u6784\u6e05\u6670\u4e14\u8ba1\u7b97\u9ad8\u6548\u3002", "conclusion": "\u95ed\u5f0f\u4e8c\u9636\u5bfc\u6570\u4e3a\u673a\u5668\u4eba\u63a7\u5236\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u66f4\u76f4\u89c2\u548c\u9ad8\u6548\u7684\u5de5\u5177\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u542b\u5f39\u6027\u90e8\u4ef6\u7684\u7cfb\u7edf\u3002"}}
{"id": "2507.11083", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.11083", "abs": "https://arxiv.org/abs/2507.11083", "authors": ["Longhui Zhang", "Bin Wang", "Jiahao Wang", "Xiaofeng Zhao", "Min Zhang", "Hao Yang", "Meishan Zhang", "Yu Li", "Jing Li", "Jun Yu", "Min Zhang"], "title": "Function-to-Style Guidance of LLMs for Code Translation", "comment": "This paper has been accepted by ICML 2025. Models and benchmarks can\n  be found at https://www.modelscope.cn/collections/F2STrans-42526ff95dd843", "summary": "Large language models (LLMs) have made significant strides in code\ntranslation tasks. However, ensuring both the correctness and readability of\ntranslated code remains a challenge, limiting their effective adoption in\nreal-world software development. In this work, we propose F2STrans, a\nfunction-to-style guiding paradigm designed to progressively improve the\nperformance of LLMs in code translation. Our approach comprises two key stages:\n(1) Functional learning, which optimizes translation correctness using\nhigh-quality source-target code pairs mined from online programming platforms,\nand (2) Style learning, which improves translation readability by incorporating\nboth positive and negative style examples. Additionally, we introduce a novel\ncode translation benchmark that includes up-to-date source code, extensive test\ncases, and manually annotated ground-truth translations, enabling comprehensive\nfunctional and stylistic evaluations. Experiments on both our new benchmark and\nexisting datasets demonstrate that our approach significantly improves code\ntranslation performance. Notably, our approach enables Qwen-1.5B to outperform\nprompt-enhanced Qwen-32B and GPT-4 on average across 20 diverse code\ntranslation scenarios.", "AI": {"tldr": "F2STrans\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u9636\u6bb5\u7684\u4ee3\u7801\u7ffb\u8bd1\u65b9\u6cd5\uff0c\u901a\u8fc7\u529f\u80fd\u5b66\u4e60\u548c\u98ce\u683c\u5b66\u4e60\u63d0\u5347LLMs\u7684\u7ffb\u8bd1\u6027\u80fd\uff0c\u5e76\u5728\u65b0\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u73b0\u6709LLMs\u5728\u4ee3\u7801\u7ffb\u8bd1\u4e2d\u96be\u4ee5\u540c\u65f6\u4fdd\u8bc1\u6b63\u786e\u6027\u548c\u53ef\u8bfb\u6027\uff0c\u9650\u5236\u4e86\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u529f\u80fd\u5b66\u4e60\u4f18\u5316\u7ffb\u8bd1\u6b63\u786e\u6027\uff0c\u98ce\u683c\u5b66\u4e60\u63d0\u5347\u53ef\u8bfb\u6027\uff1b\u5e76\u5f15\u5165\u65b0\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cF2STrans\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0cQwen-1.5B\u751a\u81f3\u4f18\u4e8eQwen-32B\u548cGPT-4\u3002", "conclusion": "F2STrans\u4e3a\u4ee3\u7801\u7ffb\u8bd1\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u5728\u529f\u80fd\u4e0e\u98ce\u683c\u5e73\u8861\u4e0a\u8868\u73b0\u7a81\u51fa\u3002"}}
{"id": "2507.11133", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11133", "abs": "https://arxiv.org/abs/2507.11133", "authors": ["Luca Beber", "Edoardo Lamon", "Giacomo Moretti", "Matteo Saveriano", "Luca Fambri", "Luigi Palopoli", "Daniele Fontanelli"], "title": "Force-Based Viscosity and Elasticity Measurements for Material Biomechanical Characterisation with a Collaborative Robotic Arm", "comment": null, "summary": "Diagnostic activities, such as ultrasound scans and palpation, are relatively\nlow-cost. They play a crucial role in the early detection of health problems\nand in assessing their progression. However, they are also error-prone\nactivities, which require highly skilled medical staff. The use of robotic\nsolutions can be key to decreasing the inherent subjectivity of the results and\nreducing the waiting list. For a robot to perform palpation or ultrasound\nscans, it must effectively manage physical interactions with the human body,\nwhich greatly benefits from precise estimation of the patient's tissue\nbiomechanical properties. This paper assesses the accuracy and precision of a\nrobotic system in estimating the viscoelastic parameters of various materials,\nincluding some tests on ex vivo tissues as a preliminary proof-of-concept\ndemonstration of the method's applicability to biological samples. The\nmeasurements are compared against a ground truth derived from silicone\nspecimens with different viscoelastic properties, characterised using a\nhigh-precision instrument. Experimental results show that the robotic system's\naccuracy closely matches the ground truth, increasing confidence in the\npotential use of robots for such clinical applications.", "AI": {"tldr": "\u8bba\u6587\u8bc4\u4f30\u4e86\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u4f30\u8ba1\u6750\u6599\u7c98\u5f39\u6027\u53c2\u6570\u65b9\u9762\u7684\u51c6\u786e\u6027\uff0c\u5e76\u521d\u6b65\u9a8c\u8bc1\u4e86\u5176\u5728\u751f\u7269\u6837\u672c\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "motivation": "\u8bca\u65ad\u6d3b\u52a8\uff08\u5982\u8d85\u58f0\u626b\u63cf\u548c\u89e6\u8bca\uff09\u6210\u672c\u4f4e\u4f46\u6613\u51fa\u9519\uff0c\u673a\u5668\u4eba\u89e3\u51b3\u65b9\u6848\u53ef\u51cf\u5c11\u7ed3\u679c\u7684\u4e3b\u89c2\u6027\u5e76\u7f29\u77ed\u7b49\u5f85\u65f6\u95f4\u3002", "method": "\u901a\u8fc7\u673a\u5668\u4eba\u7cfb\u7edf\u6d4b\u91cf\u4e0d\u540c\u6750\u6599\uff08\u5305\u62ec\u79bb\u4f53\u7ec4\u7ec7\uff09\u7684\u7c98\u5f39\u6027\u53c2\u6570\uff0c\u5e76\u4e0e\u9ad8\u7cbe\u5ea6\u4eea\u5668\u6d4b\u5f97\u7684\u7845\u80f6\u6837\u672c\u6570\u636e\u5bf9\u6bd4\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u51c6\u786e\u6027\u4e0e\u5730\u9762\u771f\u5b9e\u503c\u9ad8\u5ea6\u5339\u914d\u3002", "conclusion": "\u673a\u5668\u4eba\u7cfb\u7edf\u5728\u4e34\u5e8a\u5e94\u7528\u4e2d\u5177\u6709\u6f5c\u5728\u4ef7\u503c\u3002"}}
{"id": "2507.11117", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11117", "abs": "https://arxiv.org/abs/2507.11117", "authors": ["Ailiya Borjigin", "Cong He", "Charles CC Lee", "Wei Zhou"], "title": "AI Agent Architecture for Decentralized Trading of Alternative Assets", "comment": "8 Pages, 1 figure", "summary": "Decentralized trading of real-world alternative assets (e.g., gold) requires\nbridging physical asset custody with blockchain systems while meeting strict\nrequirements for compliance, liquidity, and risk management. We present\nGoldMine OS, a research oriented architecture that employs multiple specialized\nAI agents to automate and secure the tokenization and exchange of physical gold\ninto a blockchain based stablecoin (\"OZ\"). Our approach combines on chain smart\ncontracts for critical risk controls with off chain AI agents for decision\nmaking, blending the transparency and reliability of blockchains with the\nflexibility of AI driven automation. We describe four cooperative agents\n(Compliance, Token Issuance, Market Making, and Risk Control) and a\ncoordinating core, and evaluate the system through simulation and a controlled\npilot deployment. In experiments the prototype delivers on demand token\nissuance in under 1.2 s, more than 100 times faster than manual workflows. The\nMarket Making agent maintains tight liquidity with spreads often below 0.5\npercent even under volatile conditions. Fault injection tests show resilience:\nan oracle price spoofing attack is detected and mitigated within 10 s, and a\nsimulated vault mis reporting halts issuance immediately with minimal user\nimpact. The architecture scales to 5000 transactions per second with 10000\nconcurrent users in benchmarks. These results indicate that an AI agent based\ndecentralized exchange for alternative assets can satisfy rigorous performance\nand safety requirements. We discuss broader implications for democratizing\naccess to traditionally illiquid assets and explain how our governance model --\nmulti signature agent updates and on chain community voting on risk parameters\n-- provides ongoing transparency, adaptability, and formal assurance of system\nintegrity.", "AI": {"tldr": "GoldMine OS\u662f\u4e00\u4e2a\u7814\u7a76\u5bfc\u5411\u7684\u67b6\u6784\uff0c\u5229\u7528\u591a\u4e2a\u4e13\u7528AI\u4ee3\u7406\u81ea\u52a8\u5316\u5e76\u5b89\u5168\u5730\u5c06\u5b9e\u7269\u9ec4\u91d1\u4ee3\u5e01\u5316\u4e3a\u533a\u5757\u94fe\u7a33\u5b9a\u5e01\uff08OZ\uff09\uff0c\u7ed3\u5408\u94fe\u4e0a\u667a\u80fd\u5408\u7ea6\u548c\u94fe\u4e0bAI\u4ee3\u7406\uff0c\u5b9e\u73b0\u9ad8\u6027\u80fd\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u89e3\u51b3\u5b9e\u7269\u8d44\u4ea7\uff08\u5982\u9ec4\u91d1\uff09\u5728\u533a\u5757\u94fe\u4e0a\u4ea4\u6613\u7684\u5408\u89c4\u6027\u3001\u6d41\u52a8\u6027\u548c\u98ce\u9669\u7ba1\u7406\u95ee\u9898\uff0c\u63a8\u52a8\u4f20\u7edf\u975e\u6d41\u52a8\u6027\u8d44\u4ea7\u7684\u6c11\u4e3b\u5316\u8bbf\u95ee\u3002", "method": "\u91c7\u7528\u94fe\u4e0a\u667a\u80fd\u5408\u7ea6\u63a7\u5236\u5173\u952e\u98ce\u9669\uff0c\u94fe\u4e0bAI\u4ee3\u7406\uff08\u5408\u89c4\u3001\u4ee3\u5e01\u53d1\u884c\u3001\u505a\u5e02\u3001\u98ce\u63a7\uff09\u534f\u4f5c\u51b3\u7b56\uff0c\u901a\u8fc7\u6a21\u62df\u548c\u8bd5\u70b9\u90e8\u7f72\u8bc4\u4f30\u7cfb\u7edf\u6027\u80fd\u3002", "result": "\u539f\u578b\u7cfb\u7edf\u5b9e\u73b01.2\u79d2\u5185\u6309\u9700\u4ee3\u5e01\u53d1\u884c\uff0c\u505a\u5e02\u4ee3\u7406\u4fdd\u63010.5%\u4ee5\u5185\u7684\u4ef7\u5dee\uff0c\u653b\u51fb\u68c0\u6d4b\u548c\u7f13\u89e3\u65f6\u95f4\u572810\u79d2\u5185\uff0c\u652f\u63015000 TPS\u548c10000\u5e76\u53d1\u7528\u6237\u3002", "conclusion": "AI\u4ee3\u7406\u9a71\u52a8\u7684\u53bb\u4e2d\u5fc3\u5316\u4ea4\u6613\u6240\u53ef\u6ee1\u8db3\u9ad8\u6027\u80fd\u548c\u5b89\u5168\u6027\u9700\u6c42\uff0c\u5176\u6cbb\u7406\u6a21\u578b\uff08\u591a\u7b7e\u540d\u66f4\u65b0\u548c\u94fe\u4e0a\u6295\u7968\uff09\u786e\u4fdd\u900f\u660e\u6027\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2507.11170", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.11170", "abs": "https://arxiv.org/abs/2507.11170", "authors": ["Giulio Giacomuzzo", "Mohamed Abdelwahab", "Marco Cal\u00ec", "Alberto Dalla Libera", "Ruggero Carli"], "title": "A Robust Controller based on Gaussian Processes for Robotic Manipulators with Unknown Uncertainty", "comment": null, "summary": "In this paper, we propose a novel learning-based robust feedback\nlinearization strategy to ensure precise trajectory tracking for an important\nfamily of Lagrangian systems. We assume a nominal knowledge of the dynamics is\ngiven but no a-priori bounds on the model mismatch are available. In our\napproach, the key ingredient is the adoption of a regression framework based on\nGaussian Processes (GPR) to estimate the model mismatch. This estimate is added\nto the outer loop of a classical feedback linearization scheme based on the\nnominal knowledge available. Then, to compensate for the residual uncertainty,\nwe robustify the controller including an additional term whose size is designed\nbased on the variance provided by the GPR framework. We proved that, with high\nprobability, the proposed scheme is able to guarantee asymptotic tracking of a\ndesired trajectory. We tested numerically our strategy on a 2 degrees of\nfreedom planar robot.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u9c81\u68d2\u53cd\u9988\u7ebf\u6027\u5316\u7b56\u7565\uff0c\u7528\u4e8e\u62c9\u683c\u6717\u65e5\u7cfb\u7edf\u7684\u7cbe\u786e\u8f68\u8ff9\u8ddf\u8e2a\u3002", "motivation": "\u89e3\u51b3\u6a21\u578b\u5931\u914d\u95ee\u9898\uff0c\u786e\u4fdd\u8f68\u8ff9\u8ddf\u8e2a\u7684\u7cbe\u786e\u6027\uff0c\u5c24\u5176\u662f\u5728\u7f3a\u4e4f\u5148\u9a8c\u6a21\u578b\u5931\u914d\u8fb9\u754c\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u91c7\u7528\u9ad8\u65af\u8fc7\u7a0b\u56de\u5f52\uff08GPR\uff09\u4f30\u8ba1\u6a21\u578b\u5931\u914d\uff0c\u5e76\u5c06\u5176\u4e0e\u7ecf\u5178\u53cd\u9988\u7ebf\u6027\u5316\u65b9\u6848\u7ed3\u5408\uff0c\u901a\u8fc7\u9c81\u68d2\u5316\u63a7\u5236\u5668\u8865\u507f\u5269\u4f59\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u7406\u8bba\u8bc1\u660e\u9ad8\u6982\u7387\u4e0b\u80fd\u4fdd\u8bc1\u6e10\u8fd1\u8ddf\u8e2a\u76ee\u6807\u8f68\u8ff9\uff0c\u5e76\u57282\u81ea\u7531\u5ea6\u5e73\u9762\u673a\u5668\u4eba\u4e0a\u8fdb\u884c\u4e86\u6570\u503c\u9a8c\u8bc1\u3002", "conclusion": "\u6240\u63d0\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u6a21\u578b\u5931\u914d\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u8f68\u8ff9\u8ddf\u8e2a\u3002"}}
{"id": "2507.11127", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11127", "abs": "https://arxiv.org/abs/2507.11127", "authors": ["Lennert De Smet", "Luc De Raedt"], "title": "Defining neurosymbolic AI", "comment": null, "summary": "Neurosymbolic AI focuses on integrating learning and reasoning, in\nparticular, on unifying logical and neural representations. Despite the\nexistence of an alphabet soup of neurosymbolic AI systems, the field is lacking\na generally accepted formal definition of what neurosymbolic models and\ninference really are. We introduce a formal definition for neurosymbolic AI\nthat makes abstraction of its key ingredients. More specifically, we define\nneurosymbolic inference as the computation of an integral over a product of a\nlogical and a belief function. We show that our neurosymbolic AI definition\nmakes abstraction of key representative neurosymbolic AI systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5f62\u5f0f\u5316\u5b9a\u4e49\uff0c\u5c06\u795e\u7ecf\u7b26\u53f7AI\u62bd\u8c61\u4e3a\u903b\u8f91\u51fd\u6570\u548c\u4fe1\u5ff5\u51fd\u6570\u7684\u79ef\u5206\u8ba1\u7b97\u3002", "motivation": "\u5c3d\u7ba1\u795e\u7ecf\u7b26\u53f7AI\u9886\u57df\u5df2\u6709\u591a\u79cd\u7cfb\u7edf\uff0c\u4f46\u7f3a\u4e4f\u516c\u8ba4\u7684\u5f62\u5f0f\u5316\u5b9a\u4e49\uff0c\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5b9a\u4e49\u795e\u7ecf\u7b26\u53f7\u63a8\u7406\u4e3a\u903b\u8f91\u51fd\u6570\u548c\u4fe1\u5ff5\u51fd\u6570\u7684\u79ef\u5206\u8ba1\u7b97\uff0c\u62bd\u8c61\u51fa\u5173\u952e\u8981\u7d20\u3002", "result": "\u8be5\u5b9a\u4e49\u80fd\u591f\u6db5\u76d6\u4ee3\u8868\u6027\u7684\u795e\u7ecf\u7b26\u53f7AI\u7cfb\u7edf\u3002", "conclusion": "\u63d0\u51fa\u7684\u5f62\u5f0f\u5316\u5b9a\u4e49\u4e3a\u795e\u7ecf\u7b26\u53f7AI\u9886\u57df\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\u3002"}}
{"id": "2507.11211", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.11211", "abs": "https://arxiv.org/abs/2507.11211", "authors": ["Chen Cai", "Ernesto Dickel Saraiva", "Ya-jun Pan", "Steven Liu"], "title": "MPC-based Coarse-to-Fine Motion Planning for Robotic Object Transportation in Cluttered Environments", "comment": "10 pages, 5 figures, submitted to IEEE Robotics and Automation\n  Letters (RA-L)", "summary": "This letter presents a novel coarse-to-fine motion planning framework for\nrobotic manipulation in cluttered, unmodeled environments. The system\nintegrates a dual-camera perception setup with a B-spline-based model\npredictive control (MPC) scheme. Initially, the planner generates feasible\nglobal trajectories from partial and uncertain observations. As new visual data\nare incrementally fused, both the environment model and motion planning are\nprogressively refined. A vision-based cost function promotes target-driven\nexploration, while a refined kernel-perceptron collision detector enables\nefficient constraint updates for real-time planning. The framework accommodates\nclosed-chain kinematics and supports dynamic replanning. Experiments on a\nmulti-arm platform validate its robustness and adaptability under uncertainties\nand clutter.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4ece\u7c97\u5230\u7ec6\u7684\u8fd0\u52a8\u89c4\u5212\u6846\u67b6\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u5728\u6742\u4e71\u3001\u672a\u5efa\u6a21\u73af\u5883\u4e2d\u7684\u64cd\u4f5c\u3002", "motivation": "\u89e3\u51b3\u673a\u5668\u4eba\u5728\u590d\u6742\u3001\u672a\u77e5\u73af\u5883\u4e2d\u7684\u8fd0\u52a8\u89c4\u5212\u95ee\u9898\uff0c\u63d0\u9ad8\u5176\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u3002", "method": "\u7ed3\u5408\u53cc\u6444\u50cf\u5934\u611f\u77e5\u7cfb\u7edf\u548c\u57fa\u4e8eB\u6837\u6761\u7684\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u65b9\u6848\uff0c\u9010\u6b65\u4f18\u5316\u73af\u5883\u6a21\u578b\u548c\u8fd0\u52a8\u89c4\u5212\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u5728\u4e0d\u786e\u5b9a\u6027\u548c\u6742\u4e71\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6548\u652f\u6301\u52a8\u6001\u91cd\u89c4\u5212\u548c\u95ed\u73af\u8fd0\u52a8\u5b66\uff0c\u9002\u7528\u4e8e\u590d\u6742\u73af\u5883\u4e2d\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u3002"}}
{"id": "2507.11135", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11135", "abs": "https://arxiv.org/abs/2507.11135", "authors": ["Selma Saidi", "Omar Laimona", "Christoph Schmickler", "Dirk Ziegenbein"], "title": "Collaborative Trustworthiness for Good Decision Making in Autonomous Systems", "comment": null, "summary": "Autonomous systems are becoming an integral part of many application domains,\nlike in the mobility sector. However, ensuring their safe and correct behaviour\nin dynamic and complex environments remains a significant challenge, where\nsystems should autonomously make decisions e.g., about manoeuvring. We propose\nin this paper a general collaborative approach for increasing the level of\ntrustworthiness in the environment of operation and improve reliability and\ngood decision making in autonomous system. In the presence of conflicting\ninformation, aggregation becomes a major issue for trustworthy decision making\nbased on collaborative data sharing. Unlike classical approaches in the\nliterature that rely on consensus or majority as aggregation rule, we exploit\nthe fact that autonomous systems have different quality attributes like\nperception quality. We use this criteria to determine which autonomous systems\nare trustworthy and borrow concepts from social epistemology to define\naggregation and propagation rules, used for automated decision making. We use\nBinary Decision Diagrams (BDDs) as formal models for beliefs aggregation and\npropagation, and formulate reduction rules to reduce the size of the BDDs and\nallow efficient computation structures for collaborative automated reasoning.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u534f\u4f5c\u6570\u636e\u5171\u4eab\u7684\u81ea\u4e3b\u7cfb\u7edf\u53ef\u4fe1\u51b3\u7b56\u65b9\u6cd5\uff0c\u5229\u7528\u611f\u77e5\u8d28\u91cf\u7b49\u5c5e\u6027\u8bc4\u4f30\u7cfb\u7edf\u53ef\u4fe1\u5ea6\uff0c\u91c7\u7528BDD\u6a21\u578b\u8fdb\u884c\u4fe1\u5ff5\u805a\u5408\u4e0e\u4f20\u64ad\u3002", "motivation": "\u52a8\u6001\u590d\u6742\u73af\u5883\u4e2d\u786e\u4fdd\u81ea\u4e3b\u7cfb\u7edf\u7684\u5b89\u5168\u4e0e\u6b63\u786e\u884c\u4e3a\u662f\u6311\u6218\uff0c\u9700\u63d0\u9ad8\u5176\u51b3\u7b56\u7684\u53ef\u9760\u6027\u4e0e\u53ef\u4fe1\u5ea6\u3002", "method": "\u5229\u7528\u611f\u77e5\u8d28\u91cf\u7b49\u5c5e\u6027\u8bc4\u4f30\u7cfb\u7edf\u53ef\u4fe1\u5ea6\uff0c\u7ed3\u5408\u793e\u4f1a\u8ba4\u8bc6\u8bba\u5b9a\u4e49\u805a\u5408\u4e0e\u4f20\u64ad\u89c4\u5219\uff0c\u4f7f\u7528BDD\u6a21\u578b\u8fdb\u884c\u4fe1\u5ff5\u805a\u5408\u4e0e\u4f20\u64ad\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u53ef\u4fe1\u51b3\u7b56\u65b9\u6cd5\uff0c\u901a\u8fc7BDD\u6a21\u578b\u548c\u7b80\u5316\u89c4\u5219\u5b9e\u73b0\u534f\u4f5c\u81ea\u52a8\u63a8\u7406\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u534f\u4f5c\u6570\u636e\u5171\u4eab\u548cBDD\u6a21\u578b\u63d0\u5347\u4e86\u81ea\u4e3b\u7cfb\u7edf\u7684\u51b3\u7b56\u53ef\u4fe1\u5ea6\u4e0e\u6548\u7387\u3002"}}
{"id": "2507.11241", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11241", "abs": "https://arxiv.org/abs/2507.11241", "authors": ["Tobias Kern", "Leon Tolksdorf", "Christian Birkner"], "title": "Comparison of Localization Algorithms between Reduced-Scale and Real-Sized Vehicles Using Visual and Inertial Sensors", "comment": null, "summary": "Physically reduced-scale vehicles are emerging to accelerate the development\nof advanced automated driving functions. In this paper, we investigate the\neffects of scaling on self-localization accuracy with visual and\nvisual-inertial algorithms using cameras and an inertial measurement unit\n(IMU). For this purpose, ROS2-compatible visual and visual-inertial algorithms\nare selected, and datasets are chosen as a baseline for real-sized vehicles. A\ntest drive is conducted to record data of reduced-scale vehicles. We compare\nthe selected localization algorithms, OpenVINS, VINS-Fusion, and RTAB-Map, in\nterms of their pose accuracy against the ground-truth and against data from\nreal-sized vehicles. When comparing the implementation of the selected\nlocalization algorithms to real-sized vehicles, OpenVINS has the lowest average\nlocalization error. Although all selected localization algorithms have\noverlapping error ranges, OpenVINS also performs best when applied to a\nreduced-scale vehicle. When reduced-scale vehicles were compared to real-sized\nvehicles, minor differences were found in translational vehicle motion\nestimation accuracy. However, no significant differences were found when\ncomparing the estimation accuracy of rotational vehicle motion, allowing RSVRs\nto be used as testing platforms for self-localization algorithms.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u7269\u7406\u7f29\u6bd4\u8f66\u8f86\u5bf9\u89c6\u89c9\u548c\u89c6\u89c9-\u60ef\u6027\u81ea\u5b9a\u4f4d\u7b97\u6cd5\u7cbe\u5ea6\u7684\u5f71\u54cd\uff0c\u53d1\u73b0OpenVINS\u5728\u7f29\u6bd4\u548c\u771f\u5b9e\u5c3a\u5bf8\u8f66\u8f86\u4e2d\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u63a2\u7d22\u7f29\u6bd4\u8f66\u8f86\u4f5c\u4e3a\u6d4b\u8bd5\u5e73\u53f0\u5bf9\u81ea\u52a8\u9a7e\u9a76\u529f\u80fd\u5f00\u53d1\u7684\u53ef\u884c\u6027\uff0c\u7279\u522b\u662f\u81ea\u5b9a\u4f4d\u7b97\u6cd5\u7684\u7cbe\u5ea6\u5f71\u54cd\u3002", "method": "\u9009\u62e9ROS2\u517c\u5bb9\u7684\u89c6\u89c9\u548c\u89c6\u89c9-\u60ef\u6027\u7b97\u6cd5\uff08OpenVINS\u3001VINS-Fusion\u3001RTAB-Map\uff09\uff0c\u4f7f\u7528\u771f\u5b9e\u5c3a\u5bf8\u8f66\u8f86\u6570\u636e\u4f5c\u4e3a\u57fa\u51c6\uff0c\u8bb0\u5f55\u7f29\u6bd4\u8f66\u8f86\u6570\u636e\u5e76\u6bd4\u8f83\u7b97\u6cd5\u7cbe\u5ea6\u3002", "result": "OpenVINS\u5728\u7f29\u6bd4\u548c\u771f\u5b9e\u5c3a\u5bf8\u8f66\u8f86\u4e2d\u5b9a\u4f4d\u8bef\u5dee\u6700\u4f4e\uff0c\u7f29\u6bd4\u8f66\u8f86\u5728\u5e73\u79fb\u8fd0\u52a8\u4f30\u8ba1\u4e0a\u6709\u5fae\u5c0f\u5dee\u5f02\uff0c\u4f46\u65cb\u8f6c\u8fd0\u52a8\u4f30\u8ba1\u65e0\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "\u7f29\u6bd4\u8f66\u8f86\u53ef\u4f5c\u4e3a\u81ea\u5b9a\u4f4d\u7b97\u6cd5\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0cOpenVINS\u662f\u8868\u73b0\u6700\u4f73\u7684\u7b97\u6cd5\u3002"}}
{"id": "2507.11150", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.11150", "abs": "https://arxiv.org/abs/2507.11150", "authors": ["Alessandro Bertagnon", "Marcello Dalpasso", "Michele Favalli", "Marco Gavanelli"], "title": "Fine-grained Timing Analysis of Digital Integrated Circuits in Answer Set Programming", "comment": "Accepted for publication in the issues of Theory and Practice of\n  Logic Programming (TPLP) dedicated to ICLP 2025, 16 pages, 9 figures", "summary": "In the design of integrated circuits, one critical metric is the maximum\ndelay introduced by combinational modules within the circuit. This delay is\ncrucial because it represents the time required to perform a computation: in an\nArithmetic-Logic Unit it represents the maximum time taken by the circuit to\nperform an arithmetic operation. When such a circuit is part of a larger,\nsynchronous system, like a CPU, the maximum delay directly impacts the maximum\nclock frequency of the entire system. Typically, hardware designers use Static\nTiming Analysis to compute an upper bound of the maximum delay because it can\nbe determined in polynomial time. However, relying on this upper bound can lead\nto suboptimal processor speeds, thereby missing performance opportunities. In\nthis work, we tackle the challenging task of computing the actual maximum\ndelay, rather than an approximate value. Since the problem is computationally\nhard, we model it in Answer Set Programming (ASP), a logic language featuring\nextremely efficient solvers. We propose non-trivial encodings of the problem\ninto ASP. Experimental results show that ASP is a viable solution to address\ncomplex problems in hardware design.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u7b54\u6848\u96c6\u7f16\u7a0b\uff08ASP\uff09\u8ba1\u7b97\u7ec4\u5408\u7535\u8def\u5b9e\u9645\u6700\u5927\u5ef6\u8fdf\u7684\u65b9\u6cd5\uff0c\u4ee5\u66ff\u4ee3\u4f20\u7edf\u7684\u9759\u6001\u65f6\u5e8f\u5206\u6790\uff0c\u4ece\u800c\u4f18\u5316\u5904\u7406\u5668\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u9759\u6001\u65f6\u5e8f\u5206\u6790\u867d\u80fd\u5728\u591a\u9879\u5f0f\u65f6\u95f4\u5185\u8ba1\u7b97\u6700\u5927\u5ef6\u8fdf\u7684\u4e0a\u754c\uff0c\u4f46\u53ef\u80fd\u5bfc\u81f4\u5904\u7406\u5668\u6027\u80fd\u672a\u8fbe\u6700\u4f18\u3002\u672c\u6587\u65e8\u5728\u76f4\u63a5\u8ba1\u7b97\u5b9e\u9645\u6700\u5927\u5ef6\u8fdf\uff0c\u4ee5\u63d0\u5347\u7cfb\u7edf\u6027\u80fd\u3002", "method": "\u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u7b54\u6848\u96c6\u7f16\u7a0b\uff08ASP\uff09\uff0c\u5e76\u63d0\u51fa\u975e\u5e73\u51e1\u7684\u7f16\u7801\u65b9\u6cd5\uff0c\u5229\u7528ASP\u7684\u9ad8\u6548\u6c42\u89e3\u5668\u89e3\u51b3\u8fd9\u4e00\u8ba1\u7b97\u96be\u9898\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cASP\u80fd\u6709\u6548\u89e3\u51b3\u786c\u4ef6\u8bbe\u8ba1\u4e2d\u7684\u590d\u6742\u95ee\u9898\uff0c\u4e3a\u8ba1\u7b97\u5b9e\u9645\u6700\u5927\u5ef6\u8fdf\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002", "conclusion": "ASP\u662f\u89e3\u51b3\u786c\u4ef6\u8bbe\u8ba1\u4e2d\u590d\u6742\u8ba1\u7b97\u95ee\u9898\u7684\u6709\u6548\u5de5\u5177\uff0c\u80fd\u591f\u66ff\u4ee3\u4f20\u7edf\u65b9\u6cd5\u4ee5\u4f18\u5316\u6027\u80fd\u3002"}}
{"id": "2507.11270", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11270", "abs": "https://arxiv.org/abs/2507.11270", "authors": ["Ting-Wei Ou", "Jia-Hao Jiang", "Guan-Lin Huang", "Kuu-Young Young"], "title": "Development of an Autonomous Mobile Robotic System for Efficient and Precise Disinfection", "comment": "Accepted to the IEEE International Conference on Systems, Man, and\n  Cybernetics (SMC) 2025", "summary": "The COVID-19 pandemic has severely affected public health, healthcare\nsystems, and daily life, especially amid resource shortages and limited\nworkers. This crisis has underscored the urgent need for automation in hospital\nenvironments, particularly disinfection, which is crucial to controlling virus\ntransmission and improving the safety of healthcare personnel and patients.\nUltraviolet (UV) light disinfection, known for its high efficiency, has been\nwidely adopted in hospital settings. However, most existing research focuses on\nmaximizing UV coverage while paying little attention to the impact of human\nactivity on virus distribution. To address this issue, we propose a mobile\nrobotic system for UV disinfection focusing on the virus hotspot. The system\nprioritizes disinfection in high-risk areas and employs an approach for\noptimized UV dosage to ensure that all surfaces receive an adequate level of UV\nexposure while significantly reducing disinfection time. It not only improves\ndisinfection efficiency but also minimizes unnecessary exposure in low-risk\nareas. In two representative hospital scenarios, our method achieves the same\ndisinfection effectiveness while reducing disinfection time by 30.7% and 31.9%,\nrespectively. The video of the experiment is available at:\nhttps://youtu.be/wHcWzOcoMPM.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u75c5\u6bd2\u70ed\u70b9\u533a\u57df\u7684\u79fb\u52a8\u673a\u5668\u4eba\u7d2b\u5916\u7ebf\u6d88\u6bd2\u7cfb\u7edf\uff0c\u4f18\u5316\u6d88\u6bd2\u5242\u91cf\uff0c\u663e\u8457\u51cf\u5c11\u6d88\u6bd2\u65f6\u95f4\u5e76\u63d0\u9ad8\u6548\u7387\u3002", "motivation": "COVID-19\u75ab\u60c5\u51f8\u663e\u4e86\u533b\u9662\u73af\u5883\u4e2d\u81ea\u52a8\u6d88\u6bd2\u7684\u7d27\u8feb\u6027\uff0c\u73b0\u6709\u7814\u7a76\u591a\u5173\u6ce8\u7d2b\u5916\u7ebf\u8986\u76d6\uff0c\u800c\u5ffd\u7565\u4e86\u4eba\u7c7b\u6d3b\u52a8\u5bf9\u75c5\u6bd2\u5206\u5e03\u7684\u5f71\u54cd\u3002", "method": "\u8bbe\u8ba1\u79fb\u52a8\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u4f18\u5148\u6d88\u6bd2\u9ad8\u98ce\u9669\u533a\u57df\uff0c\u4f18\u5316\u7d2b\u5916\u7ebf\u5242\u91cf\uff0c\u786e\u4fdd\u8868\u9762\u5145\u5206\u66b4\u9732\u540c\u65f6\u51cf\u5c11\u6d88\u6bd2\u65f6\u95f4\u3002", "result": "\u5728\u4e24\u4e2a\u5178\u578b\u533b\u9662\u573a\u666f\u4e2d\uff0c\u6d88\u6bd2\u65f6\u95f4\u5206\u522b\u51cf\u5c1130.7%\u548c31.9%\uff0c\u540c\u65f6\u4fdd\u6301\u76f8\u540c\u6d88\u6bd2\u6548\u679c\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u9ad8\u6548\u4e14\u9488\u5bf9\u6027\u5f3a\uff0c\u9002\u7528\u4e8e\u533b\u9662\u73af\u5883\uff0c\u663e\u8457\u63d0\u5347\u6d88\u6bd2\u6548\u7387\u5e76\u51cf\u5c11\u4e0d\u5fc5\u8981\u7684\u4f4e\u98ce\u9669\u533a\u57df\u66b4\u9732\u3002"}}
{"id": "2507.11229", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.11229", "abs": "https://arxiv.org/abs/2507.11229", "authors": ["Jin Li", "Zezhong Ding", "Xike Xie"], "title": "DuetGraph: Coarse-to-Fine Knowledge Graph Reasoning with Dual-Pathway Global-Local Fusion", "comment": null, "summary": "Knowledge graphs (KGs) are vital for enabling knowledge reasoning across\nvarious domains. Recent KG reasoning methods that integrate both global and\nlocal information have achieved promising results. However, existing methods\noften suffer from score over-smoothing, which blurs the distinction between\ncorrect and incorrect answers and hinders reasoning effectiveness. To address\nthis, we propose DuetGraph, a coarse-to-fine KG reasoning mechanism with\ndual-pathway global-local fusion. DuetGraph tackles over-smoothing by\nsegregating -- rather than stacking -- the processing of local (via message\npassing) and global (via attention) information into two distinct pathways,\npreventing mutual interference and preserving representational discrimination.\nIn addition, DuetGraph introduces a coarse-to-fine optimization, which\npartitions entities into high- and low-score subsets. This strategy narrows the\ncandidate space and sharpens the score gap between the two subsets, which\nalleviates over-smoothing and enhances inference quality. Extensive experiments\non various datasets demonstrate that DuetGraph achieves state-of-the-art (SOTA)\nperformance, with up to an 8.7% improvement in reasoning quality and a\n1.8$\\times$ acceleration in training efficiency.", "AI": {"tldr": "DuetGraph\u63d0\u51fa\u4e86\u4e00\u79cd\u53cc\u8def\u5f84\u5168\u5c40-\u5c40\u90e8\u878d\u5408\u7684KG\u63a8\u7406\u673a\u5236\uff0c\u901a\u8fc7\u5206\u79bb\u5168\u5c40\u548c\u5c40\u90e8\u4fe1\u606f\u5904\u7406\u8def\u5f84\uff0c\u89e3\u51b3\u4e86\u5206\u6570\u8fc7\u5e73\u6ed1\u95ee\u9898\uff0c\u5e76\u91c7\u7528\u7c97\u5230\u7ec6\u4f18\u5316\u7b56\u7565\u63d0\u5347\u63a8\u7406\u8d28\u91cf\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709KG\u63a8\u7406\u65b9\u6cd5\u5728\u5904\u7406\u5168\u5c40\u548c\u5c40\u90e8\u4fe1\u606f\u65f6\u5bb9\u6613\u5bfc\u81f4\u5206\u6570\u8fc7\u5e73\u6ed1\uff0c\u6a21\u7cca\u6b63\u786e\u4e0e\u9519\u8bef\u7b54\u6848\u7684\u533a\u5206\uff0c\u5f71\u54cd\u63a8\u7406\u6548\u679c\u3002", "method": "DuetGraph\u91c7\u7528\u53cc\u8def\u5f84\u673a\u5236\uff0c\u5206\u522b\u5904\u7406\u5c40\u90e8\uff08\u6d88\u606f\u4f20\u9012\uff09\u548c\u5168\u5c40\uff08\u6ce8\u610f\u529b\uff09\u4fe1\u606f\uff0c\u907f\u514d\u76f8\u4e92\u5e72\u6270\uff1b\u5e76\u901a\u8fc7\u7c97\u5230\u7ec6\u4f18\u5316\u7b56\u7565\u5212\u5206\u5b9e\u4f53\u5b50\u96c6\uff0c\u7f29\u5c0f\u5019\u9009\u7a7a\u95f4\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cDuetGraph\u5728\u63a8\u7406\u8d28\u91cf\u4e0a\u63d0\u53478.7%\uff0c\u8bad\u7ec3\u6548\u7387\u52a0\u901f1.8\u500d\uff0c\u8fbe\u5230SOTA\u6027\u80fd\u3002", "conclusion": "DuetGraph\u901a\u8fc7\u53cc\u8def\u5f84\u878d\u5408\u548c\u7c97\u5230\u7ec6\u4f18\u5316\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5206\u6570\u8fc7\u5e73\u6ed1\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86KG\u63a8\u7406\u7684\u6027\u80fd\u548c\u6548\u7387\u3002"}}
{"id": "2507.11283", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.11283", "abs": "https://arxiv.org/abs/2507.11283", "authors": ["Weiyi Liu", "Jingzehua Xu", "Guanwen Xie", "Yi Li"], "title": "Ocean Diviner: A Diffusion-Augmented Reinforcement Learning for AUV Robust Control in the Underwater Tasks", "comment": null, "summary": "This paper presents a diffusion-augmented reinforcement learning (RL)\napproach for robust autonomous underwater vehicle (AUV) control, addressing key\nchallenges in underwater trajectory planning and dynamic environment\nadaptation. The proposed method integrates three core innovations: (1) A\ndiffusion-based trajectory generation framework that produces physically\nfeasible multi-step trajectories, enhanced by a high-dimensional state encoding\nmechanism combining current observations with historical states and actions\nthrough a novel diffusion U-Net architecture, significantly improving\nlong-horizon planning. (2) A sample-efficient hybrid learning architecture that\nsynergizes diffusion-guided exploration with RL policy optimization, where the\ndiffusion model generates diverse candidate actions and the RL critic selects\noptimal actions, achieving higher exploration efficiency and policy stability\nin dynamic underwater environments. Extensive simulation experiments validating\nthe method's superior robustness and flexibility, outperforms conventional\ncontrol methods in challenging marine conditions, offering enhanced\nadaptability and reliability for AUV operations in the underwater tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6269\u6563\u589e\u5f3a\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u81ea\u4e3b\u6c34\u4e0b\u8f66\u8f86\uff08AUV\uff09\u7684\u9c81\u68d2\u63a7\u5236\uff0c\u89e3\u51b3\u4e86\u6c34\u4e0b\u8f68\u8ff9\u89c4\u5212\u548c\u52a8\u6001\u73af\u5883\u9002\u5e94\u4e2d\u7684\u5173\u952e\u6311\u6218\u3002", "motivation": "\u6c34\u4e0b\u73af\u5883\u590d\u6742\u591a\u53d8\uff0c\u4f20\u7edf\u63a7\u5236\u65b9\u6cd5\u5728\u52a8\u6001\u6761\u4ef6\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9c81\u68d2\u548c\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u7ed3\u5408\u6269\u6563\u6a21\u578b\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u901a\u8fc7\u6269\u6563U-Net\u67b6\u6784\u751f\u6210\u591a\u6b65\u8f68\u8ff9\uff0c\u5e76\u5229\u7528RL\u4f18\u5316\u7b56\u7565\uff0c\u5b9e\u73b0\u9ad8\u6548\u63a2\u7d22\u548c\u7a33\u5b9a\u63a7\u5236\u3002", "result": "\u4eff\u771f\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u6d77\u6d0b\u6761\u4ef6\u4e0b\u4f18\u4e8e\u4f20\u7edf\u63a7\u5236\u65b9\u6cd5\uff0c\u5177\u6709\u66f4\u9ad8\u7684\u9002\u5e94\u6027\u548c\u53ef\u9760\u6027\u3002", "conclusion": "\u6269\u6563\u589e\u5f3a\u7684RL\u65b9\u6cd5\u4e3aAUV\u63a7\u5236\u63d0\u4f9b\u4e86\u9c81\u68d2\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u52a8\u6001\u6c34\u4e0b\u4efb\u52a1\u3002"}}
{"id": "2507.11277", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.11277", "abs": "https://arxiv.org/abs/2507.11277", "authors": ["Dany Moshkovich", "Sergey Zeltyn"], "title": "Taming Uncertainty via Automation: Observing, Analyzing, and Optimizing Agentic AI Systems", "comment": null, "summary": "Large Language Models (LLMs) are increasingly deployed within agentic\nsystems-collections of interacting, LLM-powered agents that execute complex,\nadaptive workflows using memory, tools, and dynamic planning. While enabling\npowerful new capabilities, these systems also introduce unique forms of\nuncertainty stemming from probabilistic reasoning, evolving memory states, and\nfluid execution paths. Traditional software observability and operations\npractices fall short in addressing these challenges.\n  This paper introduces AgentOps: a comprehensive framework for observing,\nanalyzing, optimizing, and automating operation of agentic AI systems. We\nidentify distinct needs across four key roles-developers, testers, site\nreliability engineers (SREs), and business users-each of whom engages with the\nsystem at different points in its lifecycle. We present the AgentOps Automation\nPipeline, a six-stage process encompassing behavior observation, metric\ncollection, issue detection, root cause analysis, optimized recommendations,\nand runtime automation. Throughout, we emphasize the critical role of\nautomation in managing uncertainty and enabling self-improving AI systems-not\nby eliminating uncertainty, but by taming it to ensure safe, adaptive, and\neffective operation.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86AgentOps\u6846\u67b6\uff0c\u7528\u4e8e\u89c2\u5bdf\u3001\u5206\u6790\u3001\u4f18\u5316\u548c\u81ea\u52a8\u5316\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u667a\u80fd\u4ee3\u7406\u7cfb\u7edf\u7684\u64cd\u4f5c\u3002", "motivation": "\u968f\u7740LLMs\u5728\u667a\u80fd\u4ee3\u7406\u7cfb\u7edf\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f20\u7edf\u8f6f\u4ef6\u8fd0\u7ef4\u65b9\u6cd5\u65e0\u6cd5\u5e94\u5bf9\u5176\u7279\u6709\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5982\u6982\u7387\u63a8\u7406\u3001\u52a8\u6001\u8bb0\u5fc6\u72b6\u6001\u548c\u7075\u6d3b\u6267\u884c\u8def\u5f84\u3002", "method": "\u63d0\u51fa\u4e86AgentOps\u6846\u67b6\uff0c\u5305\u62ec\u884c\u4e3a\u89c2\u5bdf\u3001\u6307\u6807\u6536\u96c6\u3001\u95ee\u9898\u68c0\u6d4b\u3001\u6839\u56e0\u5206\u6790\u3001\u4f18\u5316\u5efa\u8bae\u548c\u8fd0\u884c\u65f6\u81ea\u52a8\u5316\u516d\u4e2a\u9636\u6bb5\u3002", "result": "\u901a\u8fc7\u81ea\u52a8\u5316\u7ba1\u7406\u4e0d\u786e\u5b9a\u6027\uff0c\u786e\u4fdd\u667a\u80fd\u4ee3\u7406\u7cfb\u7edf\u7684\u5b89\u5168\u3001\u9002\u5e94\u6027\u548c\u9ad8\u6548\u8fd0\u884c\u3002", "conclusion": "AgentOps\u6846\u67b6\u901a\u8fc7\u81ea\u52a8\u5316\u624b\u6bb5\u6709\u6548\u5e94\u5bf9\u667a\u80fd\u4ee3\u7406\u7cfb\u7edf\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u63d0\u5347\u5176\u64cd\u4f5c\u6548\u7387\u548c\u5b89\u5168\u6027\u3002"}}
{"id": "2507.11296", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11296", "abs": "https://arxiv.org/abs/2507.11296", "authors": ["Huilin Xu", "Jian Ding", "Jiakun Xu", "Ruixiang Wang", "Jun Chen", "Jinjie Mai", "Yanwei Fu", "Bernard Ghanem", "Feng Xu", "Mohamed Elhoseiny"], "title": "Diffusion-Based Imaginative Coordination for Bimanual Manipulation", "comment": "15 pages, including 10 figures and 16 tables. Accepted at ICCV 2025", "summary": "Bimanual manipulation is crucial in robotics, enabling complex tasks in\nindustrial automation and household services. However, it poses significant\nchallenges due to the high-dimensional action space and intricate coordination\nrequirements. While video prediction has been recently studied for\nrepresentation learning and control, leveraging its ability to capture rich\ndynamic and behavioral information, its potential for enhancing bimanual\ncoordination remains underexplored. To bridge this gap, we propose a unified\ndiffusion-based framework for the joint optimization of video and action\nprediction. Specifically, we propose a multi-frame latent prediction strategy\nthat encodes future states in a compressed latent space, preserving\ntask-relevant features. Furthermore, we introduce a unidirectional attention\nmechanism where video prediction is conditioned on the action, while action\nprediction remains independent of video prediction. This design allows us to\nomit video prediction during inference, significantly enhancing efficiency.\nExperiments on two simulated benchmarks and a real-world setting demonstrate a\nsignificant improvement in the success rate over the strong baseline ACT using\nour method, achieving a \\textbf{24.9\\%} increase on ALOHA, an \\textbf{11.1\\%}\nincrease on RoboTwin, and a \\textbf{32.5\\%} increase in real-world experiments.\nOur models and code are publicly available at\nhttps://github.com/return-sleep/Diffusion_based_imaginative_Coordination.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u6846\u67b6\uff0c\u8054\u5408\u4f18\u5316\u89c6\u9891\u548c\u52a8\u4f5c\u9884\u6d4b\uff0c\u663e\u8457\u63d0\u5347\u53cc\u624b\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u534f\u8c03\u6027\u548c\u6210\u529f\u7387\u3002", "motivation": "\u53cc\u624b\u673a\u5668\u4eba\u64cd\u4f5c\u5728\u5de5\u4e1a\u81ea\u52a8\u5316\u548c\u5bb6\u5ead\u670d\u52a1\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u9ad8\u7ef4\u52a8\u4f5c\u7a7a\u95f4\u548c\u590d\u6742\u534f\u8c03\u9700\u6c42\u5e26\u6765\u6311\u6218\u3002\u89c6\u9891\u9884\u6d4b\u5728\u8868\u793a\u5b66\u4e60\u548c\u63a7\u5236\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002", "method": "\u91c7\u7528\u6269\u6563\u6846\u67b6\uff0c\u63d0\u51fa\u591a\u5e27\u6f5c\u5728\u9884\u6d4b\u7b56\u7565\u548c\u5355\u5411\u6ce8\u610f\u529b\u673a\u5236\uff0c\u89c6\u9891\u9884\u6d4b\u4f9d\u8d56\u52a8\u4f5c\uff0c\u52a8\u4f5c\u9884\u6d4b\u72ec\u7acb\u4e8e\u89c6\u9891\u9884\u6d4b\uff0c\u63d0\u9ad8\u63a8\u7406\u6548\u7387\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u5b9e\u9a8c\u4e2d\uff0c\u6210\u529f\u7387\u663e\u8457\u63d0\u5347\uff1aALOHA\u63d0\u9ad824.9%\uff0cRoboTwin\u63d0\u9ad811.1%\uff0c\u771f\u5b9e\u5b9e\u9a8c\u63d0\u9ad832.5%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u8054\u5408\u4f18\u5316\u89c6\u9891\u548c\u52a8\u4f5c\u9884\u6d4b\uff0c\u6709\u6548\u63d0\u5347\u53cc\u624b\u673a\u5668\u4eba\u64cd\u4f5c\u7684\u534f\u8c03\u6027\u548c\u6027\u80fd\uff0c\u4ee3\u7801\u548c\u6a21\u578b\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.11288", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11288", "abs": "https://arxiv.org/abs/2507.11288", "authors": ["Th\u00e9o Fagnoni", "Mahsun Altin", "Chia En Chung", "Phillip Kingston", "Alan Tuning", "Dana O. Mohamed", "In\u00e8s Adnani"], "title": "Opus: A Prompt Intention Framework for Complex Workflow Generation", "comment": "39 pages, 24 figures", "summary": "This paper introduces the Opus Prompt Intention Framework, designed to\nimprove complex Workflow Generation with instruction-tuned Large Language\nModels (LLMs). We propose an intermediate Intention Capture layer between user\nqueries and Workflow Generation, implementing the Opus Workflow Intention\nFramework, which consists of extracting Workflow Signals from user queries,\ninterpreting them into structured Workflow Intention objects, and generating\nWorkflows based on these Intentions. Our results show that this layer enables\nLLMs to produce logical and meaningful outputs that scale reliably as query\ncomplexity increases. On a synthetic benchmark of 1,000 multi-intent\nquery-Workflow(s) pairs, applying the Opus Prompt Intention Framework to\nWorkflow Generation yields consistent improvements in semantic Workflow\nsimilarity metrics. In this paper, we introduce the Opus Prompt Intention\nFramework by applying the concepts of Workflow Signal and Workflow Intention to\nLLM-driven Workflow Generation. We present a reproducible, customizable\nLLM-based Intention Capture system to extract Workflow Signals and Workflow\nIntentions from user queries. Finally, we provide empirical evidence that the\nproposed system significantly improves Workflow Generation quality compared to\ndirect generation from user queries, particularly in cases of Mixed Intention\nElicitation.", "AI": {"tldr": "Opus Prompt Intention Framework\u901a\u8fc7\u5f15\u5165\u4e2d\u95f4\u610f\u56fe\u6355\u6349\u5c42\uff0c\u63d0\u5347\u57fa\u4e8eLLM\u7684\u590d\u6742\u5de5\u4f5c\u6d41\u751f\u6210\u8d28\u91cf\uff0c\u663e\u8457\u6539\u5584\u8bed\u4e49\u76f8\u4f3c\u6027\u6307\u6807\u3002", "motivation": "\u89e3\u51b3\u76f4\u63a5\u6839\u636e\u7528\u6237\u67e5\u8be2\u751f\u6210\u5de5\u4f5c\u6d41\u65f6\u903b\u8f91\u6027\u548c\u53ef\u6269\u5c55\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faOpus Workflow Intention Framework\uff0c\u5305\u62ec\u4ece\u67e5\u8be2\u4e2d\u63d0\u53d6\u5de5\u4f5c\u6d41\u4fe1\u53f7\u3001\u89e3\u6790\u4e3a\u7ed3\u6784\u5316\u610f\u56fe\u5bf9\u8c61\uff0c\u5e76\u57fa\u4e8e\u610f\u56fe\u751f\u6210\u5de5\u4f5c\u6d41\u3002", "result": "\u57281000\u4e2a\u591a\u610f\u56fe\u67e5\u8be2-\u5de5\u4f5c\u6d41\u5bf9\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86\u8bed\u4e49\u76f8\u4f3c\u6027\u6307\u6807\u3002", "conclusion": "Opus Prompt Intention Framework\u80fd\u6709\u6548\u63d0\u5347\u590d\u6742\u5de5\u4f5c\u6d41\u751f\u6210\u7684\u903b\u8f91\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2507.11302", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.11302", "abs": "https://arxiv.org/abs/2507.11302", "authors": ["Jesse J. Hagenaars", "Stein Stroobants", "Sander M. Bohte", "Guido C. H. E. De Croon"], "title": "All Eyes, no IMU: Learning Flight Attitude from Vision Alone", "comment": null, "summary": "Vision is an essential part of attitude control for many flying animals, some\nof which have no dedicated sense of gravity. Flying robots, on the other hand,\ntypically depend heavily on accelerometers and gyroscopes for attitude\nstabilization. In this work, we present the first vision-only approach to\nflight control for use in generic environments. We show that a quadrotor drone\nequipped with a downward-facing event camera can estimate its attitude and\nrotation rate from just the event stream, enabling flight control without\ninertial sensors. Our approach uses a small recurrent convolutional neural\nnetwork trained through supervised learning. Real-world flight tests\ndemonstrate that our combination of event camera and low-latency neural network\nis capable of replacing the inertial measurement unit in a traditional flight\ncontrol loop. Furthermore, we investigate the network's generalization across\ndifferent environments, and the impact of memory and different fields of view.\nWhile networks with memory and access to horizon-like visual cues achieve best\nperformance, variants with a narrower field of view achieve better relative\ngeneralization. Our work showcases vision-only flight control as a promising\ncandidate for enabling autonomous, insect-scale flying robots.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4ec5\u4f9d\u8d56\u89c6\u89c9\u7684\u98de\u884c\u63a7\u5236\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e8b\u4ef6\u76f8\u673a\u548c\u795e\u7ecf\u7f51\u7edc\u66ff\u4ee3\u4f20\u7edf\u60ef\u6027\u4f20\u611f\u5668\u3002", "motivation": "\u8bb8\u591a\u98de\u884c\u751f\u7269\u4f9d\u8d56\u89c6\u89c9\u800c\u975e\u60ef\u6027\u4f20\u611f\u5668\u8fdb\u884c\u59ff\u6001\u63a7\u5236\uff0c\u800c\u98de\u884c\u673a\u5668\u4eba\u901a\u5e38\u4f9d\u8d56\u52a0\u901f\u5ea6\u8ba1\u548c\u9640\u87ba\u4eea\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u4ec5\u4f9d\u8d56\u89c6\u89c9\u7684\u98de\u884c\u63a7\u5236\u65b9\u6cd5\u3002", "method": "\u4f7f\u7528\u5411\u4e0b\u4e8b\u4ef6\u76f8\u673a\u548c\u4f4e\u5ef6\u8fdf\u5faa\u73af\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u901a\u8fc7\u76d1\u7763\u5b66\u4e60\u8bad\u7ec3\uff0c\u4f30\u8ba1\u65e0\u4eba\u673a\u59ff\u6001\u548c\u65cb\u8f6c\u901f\u7387\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u53ef\u66ff\u4ee3\u4f20\u7edf\u60ef\u6027\u6d4b\u91cf\u5355\u5143\uff0c\u4e14\u7f51\u7edc\u5728\u6cdb\u5316\u6027\u548c\u89c6\u91ce\u8303\u56f4\u65b9\u9762\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u89c6\u89c9\u98de\u884c\u63a7\u5236\u662f\u5b9e\u73b0\u5c0f\u578b\u81ea\u4e3b\u98de\u884c\u673a\u5668\u4eba\u7684\u6709\u524d\u666f\u65b9\u6848\u3002"}}
{"id": "2507.11323", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11323", "abs": "https://arxiv.org/abs/2507.11323", "authors": ["Xiang Yin", "Nico Potyka", "Antonio Rago", "Timotheus Kampik", "Francesca Toni"], "title": "Contestability in Quantitative Argumentation", "comment": null, "summary": "Contestable AI requires that AI-driven decisions align with human\npreferences. While various forms of argumentation have been shown to support\ncontestability, Edge-Weighted Quantitative Bipolar Argumentation Frameworks\n(EW-QBAFs) have received little attention. In this work, we show how EW-QBAFs\ncan be deployed for this purpose. Specifically, we introduce the contestability\nproblem for EW-QBAFs, which asks how to modify edge weights (e.g., preferences)\nto achieve a desired strength for a specific argument of interest (i.e., a\ntopic argument). To address this problem, we propose gradient-based relation\nattribution explanations (G-RAEs), which quantify the sensitivity of the topic\nargument's strength to changes in individual edge weights, thus providing\ninterpretable guidance for weight adjustments towards contestability. Building\non G-RAEs, we develop an iterative algorithm that progressively adjusts the\nedge weights to attain the desired strength. We evaluate our approach\nexperimentally on synthetic EW-QBAFs that simulate the structural\ncharacteristics of personalised recommender systems and multi-layer\nperceptrons, and demonstrate that it can solve the problem effectively.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u5229\u7528\u8fb9\u52a0\u6743\u5b9a\u91cf\u53cc\u6781\u8bba\u8bc1\u6846\u67b6\uff08EW-QBAFs\uff09\u5b9e\u73b0\u53ef\u4e89\u8baeAI\uff0c\u63d0\u51fa\u57fa\u4e8e\u68af\u5ea6\u7684\u5173\u7cfb\u5f52\u56e0\u89e3\u91ca\uff08G-RAEs\uff09\u548c\u8fed\u4ee3\u7b97\u6cd5\u6765\u8c03\u6574\u8fb9\u6743\u91cd\uff0c\u4ee5\u5b9e\u73b0\u76ee\u6807\u8bba\u8bc1\u5f3a\u5ea6\u7684\u8c03\u6574\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u786e\u4fddAI\u51b3\u7b56\u4e0e\u4eba\u7c7b\u504f\u597d\u4e00\u81f4\uff0c\u4f46\u76ee\u524dEW-QBAFs\u5728\u652f\u6301\u53ef\u4e89\u8bae\u6027\u65b9\u9762\u7684\u7814\u7a76\u8f83\u5c11\u3002", "method": "\u65b9\u6cd5\u5305\u62ec\u63d0\u51faG-RAEs\u91cf\u5316\u8fb9\u6743\u91cd\u53d8\u5316\u5bf9\u76ee\u6807\u8bba\u8bc1\u5f3a\u5ea6\u7684\u5f71\u54cd\uff0c\u5e76\u8bbe\u8ba1\u8fed\u4ee3\u7b97\u6cd5\u9010\u6b65\u8c03\u6574\u6743\u91cd\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6a21\u62df\u4e2a\u6027\u5316\u63a8\u8350\u7cfb\u7edf\u548c\u591a\u5c42\u611f\u77e5\u5668\u7684\u5408\u6210EW-QBAFs\u4e0a\u6709\u6548\u89e3\u51b3\u4e86\u95ee\u9898\u3002", "conclusion": "\u7ed3\u8bba\u662fEW-QBAFs\u7ed3\u5408G-RAEs\u548c\u8fed\u4ee3\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u652f\u6301AI\u51b3\u7b56\u7684\u53ef\u4e89\u8bae\u6027\u3002"}}
{"id": "2507.11345", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11345", "abs": "https://arxiv.org/abs/2507.11345", "authors": ["Oscar Lima", "Marc Vinci", "Sunandita Patra", "Sebastian Stock", "Joachim Hertzberg", "Martin Atzmueller", "Malik Ghallab", "Dana Nau", "Paolo Traverso"], "title": "Acting and Planning with Hierarchical Operational Models on a Mobile Robot: A Study with RAE+UPOM", "comment": "Accepted in ECMR 2025 conference", "summary": "Robotic task execution faces challenges due to the inconsistency between\nsymbolic planner models and the rich control structures actually running on the\nrobot. In this paper, we present the first physical deployment of an integrated\nactor-planner system that shares hierarchical operational models for both\nacting and planning, interleaving the Reactive Acting Engine (RAE) with an\nanytime UCT-like Monte Carlo planner (UPOM). We implement RAE+UPOM on a mobile\nmanipulator in a real-world deployment for an object collection task. Our\nexperiments demonstrate robust task execution under action failures and sensor\nnoise, and provide empirical insights into the interleaved acting-and-planning\ndecision making process.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u96c6\u6210\u6267\u884c\u5668-\u89c4\u5212\u5668\u7cfb\u7edf\uff08RAE+UPOM\uff09\uff0c\u901a\u8fc7\u5171\u4eab\u5c42\u6b21\u5316\u64cd\u4f5c\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u5728\u673a\u5668\u4eba\u4efb\u52a1\u6267\u884c\u4e2d\u89c4\u5212\u4e0e\u6267\u884c\u7684\u7d27\u5bc6\u7ed3\u5408\uff0c\u5e76\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u9a8c\u8bc1\u4e86\u5176\u9c81\u68d2\u6027\u3002", "motivation": "\u89e3\u51b3\u7b26\u53f7\u89c4\u5212\u6a21\u578b\u4e0e\u5b9e\u9645\u673a\u5668\u4eba\u63a7\u5236\u7ed3\u6784\u4e4b\u95f4\u7684\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u63d0\u5347\u4efb\u52a1\u6267\u884c\u7684\u9c81\u68d2\u6027\u548c\u6548\u7387\u3002", "method": "\u7ed3\u5408Reactive Acting Engine (RAE)\u548cUCT-like Monte Carlo\u89c4\u5212\u5668(UPOM)\uff0c\u5171\u4eab\u5c42\u6b21\u5316\u64cd\u4f5c\u6a21\u578b\uff0c\u5b9e\u73b0\u89c4\u5212\u4e0e\u6267\u884c\u7684\u4ea4\u66ff\u8fdb\u884c\u3002", "result": "\u5728\u79fb\u52a8\u673a\u68b0\u81c2\u4e0a\u6210\u529f\u90e8\u7f72\uff0c\u5b9e\u9a8c\u8868\u660e\u7cfb\u7edf\u80fd\u6709\u6548\u5e94\u5bf9\u52a8\u4f5c\u5931\u8d25\u548c\u4f20\u611f\u5668\u566a\u58f0\uff0c\u4efb\u52a1\u6267\u884c\u9c81\u68d2\u3002", "conclusion": "RAE+UPOM\u7cfb\u7edf\u901a\u8fc7\u5171\u4eab\u6a21\u578b\u548c\u4ea4\u66ff\u89c4\u5212\u4e0e\u6267\u884c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u4efb\u52a1\u6267\u884c\u7684\u9c81\u68d2\u6027\u548c\u9002\u5e94\u6027\u3002"}}
{"id": "2507.11334", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11334", "abs": "https://arxiv.org/abs/2507.11334", "authors": ["Yuehao Huang", "Liang Liu", "Shuangming Lei", "Yukai Ma", "Hao Su", "Jianbiao Mei", "Pengxiang Zhao", "Yaqing Gu", "Yong Liu", "Jiajun Lv"], "title": "CogDDN: A Cognitive Demand-Driven Navigation with Decision Optimization and Dual-Process Thinking", "comment": "Accepted by ACM MM 2025", "summary": "Mobile robots are increasingly required to navigate and interact within\nunknown and unstructured environments to meet human demands. Demand-driven\nnavigation (DDN) enables robots to identify and locate objects based on\nimplicit human intent, even when object locations are unknown. However,\ntraditional data-driven DDN methods rely on pre-collected data for model\ntraining and decision-making, limiting their generalization capability in\nunseen scenarios. In this paper, we propose CogDDN, a VLM-based framework that\nemulates the human cognitive and learning mechanisms by integrating fast and\nslow thinking systems and selectively identifying key objects essential to\nfulfilling user demands. CogDDN identifies appropriate target objects by\nsemantically aligning detected objects with the given instructions.\nFurthermore, it incorporates a dual-process decision-making module, comprising\na Heuristic Process for rapid, efficient decisions and an Analytic Process that\nanalyzes past errors, accumulates them in a knowledge base, and continuously\nimproves performance. Chain of Thought (CoT) reasoning strengthens the\ndecision-making process. Extensive closed-loop evaluations on the AI2Thor\nsimulator with the ProcThor dataset show that CogDDN outperforms single-view\ncamera-only methods by 15%, demonstrating significant improvements in\nnavigation accuracy and adaptability. The project page is available at\nhttps://yuehaohuang.github.io/CogDDN/.", "AI": {"tldr": "CogDDN\u662f\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u8ba4\u77e5\u548c\u5b66\u4e60\u673a\u5236\uff0c\u7ed3\u5408\u5feb\u901f\u548c\u6162\u901f\u601d\u7ef4\u7cfb\u7edf\uff0c\u63d0\u5347\u673a\u5668\u4eba\u5728\u672a\u77e5\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u7684\u6570\u636e\u9a71\u52a8\u9700\u6c42\u5bfc\u822a\uff08DDN\uff09\u65b9\u6cd5\u4f9d\u8d56\u9884\u6536\u96c6\u6570\u636e\uff0c\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u65e0\u6cd5\u9002\u5e94\u672a\u77e5\u573a\u666f\u3002", "method": "CogDDN\u901a\u8fc7\u8bed\u4e49\u5bf9\u9f50\u68c0\u6d4b\u5bf9\u8c61\u4e0e\u6307\u4ee4\uff0c\u7ed3\u5408\u542f\u53d1\u5f0f\u548c\u5206\u6790\u5f0f\u51b3\u7b56\u6a21\u5757\uff0c\u5e76\u5229\u7528\u601d\u7ef4\u94fe\uff08CoT\uff09\u63a8\u7406\u4f18\u5316\u51b3\u7b56\u8fc7\u7a0b\u3002", "result": "\u5728AI2Thor\u6a21\u62df\u5668\u548cProcThor\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cCogDDN\u6bd4\u5355\u89c6\u89d2\u76f8\u673a\u65b9\u6cd5\u6027\u80fd\u63d0\u534715%\u3002", "conclusion": "CogDDN\u663e\u8457\u63d0\u9ad8\u4e86\u5bfc\u822a\u51c6\u786e\u6027\u548c\u9002\u5e94\u6027\uff0c\u4e3a\u672a\u77e5\u73af\u5883\u4e2d\u7684\u673a\u5668\u4eba\u5bfc\u822a\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2507.11402", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11402", "abs": "https://arxiv.org/abs/2507.11402", "authors": ["Supun Dissanayaka", "Alexander Ferrein", "Till Hofmann", "Kosuke Nakajima", "Mario Sanz-Lopez", "Jesus Savage", "Daniel Swoboda", "Matteo Tschesche", "Wataru Uemura", "Tarik Viehmann", "Shohei Yasuda"], "title": "From Production Logistics to Smart Manufacturing: The Vision for a New RoboCup Industrial League", "comment": "RoboCup Symposium 2025", "summary": "The RoboCup Logistics League is a RoboCup competition in a smart factory\nscenario that has focused on task planning, job scheduling, and multi-agent\ncoordination. The focus on production logistics allowed teams to develop highly\ncompetitive strategies, but also meant that some recent developments in the\ncontext of smart manufacturing are not reflected in the competition, weakening\nits relevance over the years. In this paper, we describe the vision for the\nRoboCup Smart Manufacturing League, a new competition designed as a larger\nsmart manufacturing scenario, reflecting all the major aspects of a modern\nfactory. It will consist of several tracks that are initially independent but\ngradually combined into one smart manufacturing scenario. The new tracks will\ncover industrial robotics challenges such as assembly, human-robot\ncollaboration, and humanoid robotics, but also retain a focus on production\nlogistics. We expect the reenvisioned competition to be more attractive to\nnewcomers and well-tried teams, while also shifting the focus to current and\nfuture challenges of industrial robotics.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5c06RoboCup Logistics League\u5347\u7ea7\u4e3aRoboCup Smart Manufacturing League\uff0c\u4ee5\u6db5\u76d6\u73b0\u4ee3\u5de5\u5382\u7684\u66f4\u591a\u65b9\u9762\uff0c\u589e\u5f3a\u7ade\u8d5b\u7684\u5438\u5f15\u529b\u3002", "motivation": "\u73b0\u6709\u7684RoboCup Logistics League\u4e13\u6ce8\u4e8e\u751f\u4ea7\u7269\u6d41\uff0c\u4f46\u672a\u80fd\u53cd\u6620\u667a\u80fd\u5236\u9020\u7684\u8fd1\u671f\u53d1\u5c55\uff0c\u5bfc\u81f4\u5176\u76f8\u5173\u6027\u4e0b\u964d\u3002", "method": "\u8bbe\u8ba1\u65b0\u7684\u7ade\u8d5b\u573a\u666f\uff0c\u5305\u542b\u591a\u4e2a\u72ec\u7acb\u4f46\u9010\u6b65\u6574\u5408\u7684\u8d5b\u9053\uff0c\u6db5\u76d6\u5de5\u4e1a\u673a\u5668\u4eba\u6311\u6218\u5982\u88c5\u914d\u3001\u4eba\u673a\u534f\u4f5c\u548c\u4eba\u5f62\u673a\u5668\u4eba\u3002", "result": "\u9884\u671f\u65b0\u7ade\u8d5b\u5c06\u66f4\u5438\u5f15\u65b0\u8001\u56e2\u961f\uff0c\u5e76\u805a\u7126\u5de5\u4e1a\u673a\u5668\u4eba\u7684\u5f53\u524d\u548c\u672a\u6765\u6311\u6218\u3002", "conclusion": "\u901a\u8fc7\u6269\u5c55\u7ade\u8d5b\u8303\u56f4\uff0c\u63d0\u5347\u5176\u4e0e\u73b0\u4ee3\u667a\u80fd\u5236\u9020\u7684\u5173\u8054\u6027\u548c\u5438\u5f15\u529b\u3002"}}
{"id": "2507.11352", "categories": ["cs.AI", "cs.FL"], "pdf": "https://arxiv.org/pdf/2507.11352", "abs": "https://arxiv.org/abs/2507.11352", "authors": ["Yunhao Yang", "Neel P. Bhatt", "Christian Ellis", "Alvaro Velasquez", "Zhangyang Wang", "Ufuk Topcu"], "title": "Foundation Models for Logistics: Toward Certifiable, Conversational Planning Interfaces", "comment": null, "summary": "Logistics operators, from battlefield coordinators rerouting airlifts ahead\nof a storm to warehouse managers juggling late trucks, often face life-critical\ndecisions that demand both domain expertise and rapid and continuous\nreplanning. While popular methods like integer programming yield logistics\nplans that satisfy user-defined logical constraints, they are slow and assume\nan idealized mathematical model of the environment that does not account for\nuncertainty. On the other hand, large language models (LLMs) can handle\nuncertainty and promise to accelerate replanning while lowering the barrier to\nentry by translating free-form utterances into executable plans, yet they\nremain prone to misinterpretations and hallucinations that jeopardize safety\nand cost. We introduce a neurosymbolic framework that pairs the accessibility\nof natural-language dialogue with verifiable guarantees on goal interpretation.\nIt converts user requests into structured planning specifications, quantifies\nits own uncertainty at the field and token level, and invokes an interactive\nclarification loop whenever confidence falls below an adaptive threshold. A\nlightweight model, fine-tuned on just 100 uncertainty-filtered examples,\nsurpasses the zero-shot performance of GPT-4.1 while cutting inference latency\nby nearly 50%. These preliminary results highlight a practical path toward\ncertifiable, real-time, and user-aligned decision-making for complex logistics.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u81ea\u7136\u8bed\u8a00\u5bf9\u8bdd\u4e0e\u53ef\u9a8c\u8bc1\u4fdd\u8bc1\u7684\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u7528\u4e8e\u590d\u6742\u7269\u6d41\u51b3\u7b56\uff0c\u63d0\u5347\u5b9e\u65f6\u6027\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u7269\u6d41\u51b3\u7b56\u9700\u8981\u5feb\u901f\u8c03\u6574\u4e14\u9762\u4e34\u4e0d\u786e\u5b9a\u6027\uff0c\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u6574\u6570\u89c4\u5212\uff09\u901f\u5ea6\u6162\u4e14\u5ffd\u7565\u4e0d\u786e\u5b9a\u6027\uff0c\u800c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u6613\u4ea7\u751f\u8bef\u89e3\u548c\u5e7b\u89c9\u3002", "method": "\u5f00\u53d1\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u5c06\u7528\u6237\u8bf7\u6c42\u8f6c\u4e3a\u7ed3\u6784\u5316\u89c4\u5212\uff0c\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u5728\u7f6e\u4fe1\u5ea6\u4f4e\u65f6\u89e6\u53d1\u4ea4\u4e92\u6f84\u6e05\u5faa\u73af\u3002", "result": "\u8f7b\u91cf\u7ea7\u6a21\u578b\u5728100\u4e2a\u6837\u672c\u4e0a\u5fae\u8c03\u540e\uff0c\u6027\u80fd\u8d85\u8d8aGPT-4.1\uff0c\u63a8\u7406\u5ef6\u8fdf\u964d\u4f4e\u8fd150%\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u590d\u6742\u7269\u6d41\u63d0\u4f9b\u4e86\u53ef\u9a8c\u8bc1\u3001\u5b9e\u65f6\u4e14\u7528\u6237\u5bf9\u9f50\u7684\u51b3\u7b56\u8def\u5f84\u3002"}}
{"id": "2507.11447", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.11447", "abs": "https://arxiv.org/abs/2507.11447", "authors": ["Shuo Yang", "John Z. Zhang", "Ibrahima Sory Sow", "Zachary Manchester"], "title": "Multi-IMU Sensor Fusion for Legged Robots", "comment": "16 pages", "summary": "This paper presents a state-estimation solution for legged robots that uses a\nset of low-cost, compact, and lightweight sensors to achieve low-drift pose and\nvelocity estimation under challenging locomotion conditions. The key idea is to\nleverage multiple inertial measurement units on different links of the robot to\ncorrect a major error source in standard proprioceptive odometry. We fuse the\ninertial sensor information and joint encoder measurements in an extended\nKalman filter, then combine the velocity estimate from this filter with camera\ndata in a factor-graph-based sliding-window estimator to form a\nvisual-inertial-leg odometry method. We validate our state estimator through\ncomprehensive theoretical analysis and hardware experiments performed using\nreal-world robot data collected during a variety of challenging locomotion\ntasks. Our algorithm consistently achieves minimal position deviation, even in\nscenarios involving substantial ground impact, foot slippage, and sudden body\nrotations. A C++ implementation, along with a large-scale dataset, is available\nat https://github.com/ShuoYangRobotics/Cerberus2.0.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u817f\u5f0f\u673a\u5668\u4eba\u7684\u72b6\u6001\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f4e\u6210\u672c\u4f20\u611f\u5668\u5b9e\u73b0\u4f4e\u6f02\u79fb\u7684\u4f4d\u59ff\u548c\u901f\u5ea6\u4f30\u8ba1\u3002", "motivation": "\u89e3\u51b3\u817f\u5f0f\u673a\u5668\u4eba\u5728\u590d\u6742\u8fd0\u52a8\u6761\u4ef6\u4e0b\u6807\u51c6\u672c\u4f53\u611f\u77e5\u91cc\u7a0b\u8ba1\u7684\u4e3b\u8981\u8bef\u5dee\u95ee\u9898\u3002", "method": "\u5229\u7528\u591a\u4e2a\u60ef\u6027\u6d4b\u91cf\u5355\u5143\u548c\u5173\u8282\u7f16\u7801\u5668\u6570\u636e\uff0c\u7ed3\u5408\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u548c\u89c6\u89c9-\u60ef\u6027-\u817f\u91cc\u7a0b\u8ba1\u65b9\u6cd5\u3002", "result": "\u5728\u591a\u79cd\u6311\u6218\u6027\u8fd0\u52a8\u4efb\u52a1\u4e2d\uff0c\u7b97\u6cd5\u8868\u73b0\u51fa\u6700\u5c0f\u4f4d\u7f6e\u504f\u5dee\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u6761\u4ef6\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5df2\u5f00\u6e90\u3002"}}
{"id": "2507.11467", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.11467", "abs": "https://arxiv.org/abs/2507.11467", "authors": ["Daniel Nichols", "Konstantinos Parasyris", "Harshitha Menon", "Brian R. Bartoldson", "Giorgis Georgakoudis", "Tal Ben-Nun", "Abhinav Bhatele"], "title": "Modeling Code: Is Text All You Need?", "comment": null, "summary": "Code LLMs have become extremely popular recently for modeling source code\nacross a variety of tasks, such as generation, translation, and summarization.\nHowever, transformer-based models are limited in their capabilities to reason\nthrough structured, analytical properties of code, such as control and data\nflow. Previous work has explored the modeling of these properties with\nstructured data and graph neural networks. However, these approaches lack the\ngenerative capabilities and scale of modern LLMs. In this work, we introduce a\nnovel approach to combine the strengths of modeling both code as text and more\nstructured forms.", "AI": {"tldr": "\u7ed3\u5408\u4ee3\u7801\u6587\u672c\u5efa\u6a21\u4e0e\u7ed3\u6784\u5316\u5efa\u6a21\u4f18\u52bf\u7684\u65b0\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709Transformer\u6a21\u578b\u5728\u4ee3\u7801\u7ed3\u6784\u5316\u5206\u6790\uff08\u5982\u63a7\u5236\u6d41\u548c\u6570\u636e\u6d41\uff09\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u540c\u65f6\u4fdd\u7559\u73b0\u4ee3LLM\u7684\u751f\u6210\u80fd\u529b\u548c\u89c4\u6a21\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u4ee3\u7801\u6587\u672c\u5efa\u6a21\u4e0e\u7ed3\u6784\u5316\u5efa\u6a21\u7684\u65b0\u65b9\u6cd5\u3002", "result": "\u672a\u660e\u786e\u63d0\u53ca\u5177\u4f53\u7ed3\u679c\uff0c\u4f46\u65e8\u5728\u63d0\u5347\u4ee3\u7801\u5efa\u6a21\u7684\u7efc\u5408\u80fd\u529b\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u4e24\u79cd\u5efa\u6a21\u65b9\u5f0f\uff0c\u6709\u671b\u5728\u4ee3\u7801\u4efb\u52a1\u4e2d\u5b9e\u73b0\u66f4\u5168\u9762\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2507.11460", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.11460", "abs": "https://arxiv.org/abs/2507.11460", "authors": ["Jacinto Colan", "Ana Davila", "Yutaro Yamada", "Yasuhisa Hasegawa"], "title": "Human-Robot collaboration in surgery: Advances and challenges towards autonomous surgical assistants", "comment": "Accepted at 2025 IEEE International Conference on Robot and Human\n  Interactive Communication (ROMAN)", "summary": "Human-robot collaboration in surgery represents a significant area of\nresearch, driven by the increasing capability of autonomous robotic systems to\nassist surgeons in complex procedures. This systematic review examines the\nadvancements and persistent challenges in the development of autonomous\nsurgical robotic assistants (ASARs), focusing specifically on scenarios where\nrobots provide meaningful and active support to human surgeons. Adhering to the\nPRISMA guidelines, a comprehensive literature search was conducted across the\nIEEE Xplore, Scopus, and Web of Science databases, resulting in the selection\nof 32 studies for detailed analysis. Two primary collaborative setups were\nidentified: teleoperation-based assistance and direct hands-on interaction. The\nfindings reveal a growing research emphasis on ASARs, with predominant\napplications currently in endoscope guidance, alongside emerging progress in\nautonomous tool manipulation. Several key challenges hinder wider adoption,\nincluding the alignment of robotic actions with human surgeon preferences, the\nnecessity for procedural awareness within autonomous systems, the establishment\nof seamless human-robot information exchange, and the complexities of skill\nacquisition in shared workspaces. This review synthesizes current trends,\nidentifies critical limitations, and outlines future research directions\nessential to improve the reliability, safety, and effectiveness of human-robot\ncollaboration in surgical environments.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7efc\u8ff0\u4e86\u624b\u672f\u4e2d\u81ea\u4e3b\u673a\u5668\u4eba\u52a9\u624b\uff08ASARs\uff09\u7684\u7814\u7a76\u8fdb\u5c55\u4e0e\u6311\u6218\uff0c\u91cd\u70b9\u5173\u6ce8\u673a\u5668\u4eba\u5982\u4f55\u4e3a\u5916\u79d1\u533b\u751f\u63d0\u4f9b\u6709\u6548\u652f\u6301\u3002", "motivation": "\u968f\u7740\u81ea\u4e3b\u673a\u5668\u4eba\u7cfb\u7edf\u80fd\u529b\u7684\u63d0\u5347\uff0c\u5176\u5728\u590d\u6742\u624b\u672f\u4e2d\u8f85\u52a9\u5916\u79d1\u533b\u751f\u7684\u6f5c\u529b\u65e5\u76ca\u51f8\u663e\uff0c\u63a8\u52a8\u4e86\u4eba\u673a\u534f\u4f5c\u624b\u672f\u7684\u7814\u7a76\u3002", "method": "\u9075\u5faaPRISMA\u6307\u5357\uff0c\u5bf9IEEE Xplore\u3001Scopus\u548cWeb of Science\u6570\u636e\u5e93\u8fdb\u884c\u6587\u732e\u68c0\u7d22\uff0c\u7b5b\u9009\u51fa32\u9879\u7814\u7a76\u8fdb\u884c\u5206\u6790\u3002", "result": "\u7814\u7a76\u53d1\u73b0ASARs\u7684\u7814\u7a76\u91cd\u70b9\u96c6\u4e2d\u5728\u8fdc\u7a0b\u64cd\u4f5c\u8f85\u52a9\u548c\u76f4\u63a5\u4ea4\u4e92\u4e24\u79cd\u6a21\u5f0f\uff0c\u4e3b\u8981\u5e94\u7528\u4e8e\u5185\u7aa5\u955c\u5f15\u5bfc\uff0c\u5e76\u5728\u81ea\u4e3b\u5de5\u5177\u64cd\u4f5c\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\u3002\u5173\u952e\u6311\u6218\u5305\u62ec\u673a\u5668\u4eba\u884c\u4e3a\u4e0e\u5916\u79d1\u533b\u751f\u504f\u597d\u7684\u5339\u914d\u3001\u81ea\u4e3b\u7cfb\u7edf\u7684\u7a0b\u5e8f\u610f\u8bc6\u3001\u4eba\u673a\u4fe1\u606f\u4ea4\u6362\u7684\u6d41\u7545\u6027\u4ee5\u53ca\u5171\u4eab\u5de5\u4f5c\u7a7a\u95f4\u4e2d\u7684\u6280\u80fd\u83b7\u53d6\u3002", "conclusion": "\u7efc\u8ff0\u603b\u7ed3\u4e86\u5f53\u524d\u8d8b\u52bf\uff0c\u6307\u51fa\u4e86\u5173\u952e\u9650\u5236\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\uff0c\u4ee5\u63d0\u9ad8\u624b\u672f\u4e2d\u4eba\u673a\u534f\u4f5c\u7684\u53ef\u9760\u6027\u3001\u5b89\u5168\u6027\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2507.11473", "categories": ["cs.AI", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.11473", "abs": "https://arxiv.org/abs/2507.11473", "authors": ["Tomek Korbak", "Mikita Balesni", "Elizabeth Barnes", "Yoshua Bengio", "Joe Benton", "Joseph Bloom", "Mark Chen", "Alan Cooney", "Allan Dafoe", "Anca Dragan", "Scott Emmons", "Owain Evans", "David Farhi", "Ryan Greenblatt", "Dan Hendrycks", "Marius Hobbhahn", "Evan Hubinger", "Geoffrey Irving", "Erik Jenner", "Daniel Kokotajlo", "Victoria Krakovna", "Shane Legg", "David Lindner", "David Luan", "Aleksander M\u0105dry", "Julian Michael", "Neel Nanda", "Dave Orr", "Jakub Pachocki", "Ethan Perez", "Mary Phuong", "Fabien Roger", "Joshua Saxe", "Buck Shlegeris", "Mart\u00edn Soto", "Eric Steinberger", "Jasmine Wang", "Wojciech Zaremba", "Bowen Baker", "Rohin Shah", "Vlad Mikulik"], "title": "Chain of Thought Monitorability: A New and Fragile Opportunity for AI Safety", "comment": null, "summary": "AI systems that \"think\" in human language offer a unique opportunity for AI\nsafety: we can monitor their chains of thought (CoT) for the intent to\nmisbehave. Like all other known AI oversight methods, CoT monitoring is\nimperfect and allows some misbehavior to go unnoticed. Nevertheless, it shows\npromise and we recommend further research into CoT monitorability and\ninvestment in CoT monitoring alongside existing safety methods. Because CoT\nmonitorability may be fragile, we recommend that frontier model developers\nconsider the impact of development decisions on CoT monitorability.", "AI": {"tldr": "AI\u7cfb\u7edf\u901a\u8fc7\u4eba\u7c7b\u8bed\u8a00\u201c\u601d\u8003\u201d\u4e3aAI\u5b89\u5168\u63d0\u4f9b\u4e86\u65b0\u673a\u4f1a\uff0c\u5373\u901a\u8fc7\u76d1\u63a7\u601d\u7ef4\u94fe\uff08CoT\uff09\u6765\u68c0\u6d4b\u6f5c\u5728\u6076\u610f\u610f\u56fe\u3002\u5c3d\u7ba1CoT\u76d1\u63a7\u4e0d\u5b8c\u7f8e\uff0c\u4f46\u4ecd\u5177\u6f5c\u529b\uff0c\u5efa\u8bae\u8fdb\u4e00\u6b65\u7814\u7a76\u5e76\u4e0e\u5176\u4ed6\u5b89\u5168\u65b9\u6cd5\u7ed3\u5408\u4f7f\u7528\u3002", "motivation": "\u63a2\u7d22AI\u7cfb\u7edf\u901a\u8fc7\u8bed\u8a00\u8868\u8fbe\u601d\u7ef4\u7684\u76d1\u63a7\u6f5c\u529b\uff0c\u4ee5\u589e\u5f3aAI\u5b89\u5168\u6027\u3002", "method": "\u63d0\u51fa\u76d1\u63a7\u601d\u7ef4\u94fe\uff08CoT\uff09\u7684\u65b9\u6cd5\uff0c\u8bc4\u4f30\u5176\u53ef\u884c\u6027\u4e0e\u5c40\u9650\u6027\u3002", "result": "CoT\u76d1\u63a7\u867d\u4e0d\u5b8c\u7f8e\uff0c\u4f46\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u9700\u7ed3\u5408\u5176\u4ed6\u5b89\u5168\u63aa\u65bd\u3002", "conclusion": "\u5efa\u8bae\u8fdb\u4e00\u6b65\u7814\u7a76CoT\u76d1\u63a7\uff0c\u5e76\u5728\u5f00\u53d1\u524d\u6cbf\u6a21\u578b\u65f6\u8003\u8651\u5176\u5bf9\u76d1\u63a7\u80fd\u529b\u7684\u5f71\u54cd\u3002"}}
{"id": "2507.11464", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.11464", "abs": "https://arxiv.org/abs/2507.11464", "authors": ["Ajay Shankar", "Keisuke Okumura", "Amanda Prorok"], "title": "LF: Online Multi-Robot Path Planning Meets Optimal Trajectory Control", "comment": "9 pages; under review for IEEE Robotics & Automation - Letters (RA-L)", "summary": "We propose a multi-robot control paradigm to solve point-to-point navigation\ntasks for a team of holonomic robots with access to the full environment\ninformation. The framework invokes two processes asynchronously at high\nfrequency: (i) a centralized, discrete, and full-horizon planner for computing\ncollision- and deadlock-free paths rapidly, leveraging recent advances in\nmulti-agent pathfinding (MAPF), and (ii) dynamics-aware, robot-wise optimal\ntrajectory controllers that ensure all robots independently follow their\nassigned paths reliably. This hierarchical shift in planning representation\nfrom (i) discrete and coupled to (ii) continuous and decoupled domains enables\nthe framework to maintain long-term scalable motion synthesis. As an\ninstantiation of this idea, we present LF, which combines a fast\nstate-of-the-art MAPF solver (LaCAM), and a robust feedback control stack\n(Freyja) for executing agile robot maneuvers. LF provides a robust and\nversatile mechanism for lifelong multi-robot navigation even under asynchronous\nand partial goal updates, and adapts to dynamic workspaces simply by quick\nreplanning. We present various multirotor and ground robot demonstrations,\nincluding the deployment of 15 real multirotors with random, consecutive target\nupdates while a person walks through the operational workspace.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u673a\u5668\u4eba\u63a7\u5236\u6846\u67b6\uff0c\u7ed3\u5408\u96c6\u4e2d\u5f0f\u8def\u5f84\u89c4\u5212\u548c\u5206\u5e03\u5f0f\u8f68\u8ff9\u63a7\u5236\uff0c\u5b9e\u73b0\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u70b9\u5bf9\u70b9\u5bfc\u822a\u3002", "motivation": "\u89e3\u51b3\u591a\u673a\u5668\u4eba\u5728\u5168\u73af\u5883\u4fe1\u606f\u4e0b\u7684\u70b9\u5bf9\u70b9\u5bfc\u822a\u95ee\u9898\uff0c\u517c\u987e\u8def\u5f84\u89c4\u5212\u7684\u5feb\u901f\u6027\u548c\u8f68\u8ff9\u63a7\u5236\u7684\u9c81\u68d2\u6027\u3002", "method": "\u91c7\u7528\u5206\u5c42\u6846\u67b6\uff1a1) \u96c6\u4e2d\u5f0f\u79bb\u6563\u8def\u5f84\u89c4\u5212\uff08\u57fa\u4e8eMAPF\uff09\uff1b2) \u5206\u5e03\u5f0f\u8fde\u7eed\u8f68\u8ff9\u63a7\u5236\u3002\u7ed3\u5408LaCAM\u548cFreyja\u5b9e\u73b0\u3002", "result": "\u5c55\u793a\u4e8615\u67b6\u591a\u65cb\u7ffc\u673a\u5668\u4eba\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u9ad8\u6548\u5bfc\u822a\uff0c\u652f\u6301\u5f02\u6b65\u76ee\u6807\u66f4\u65b0\u548c\u5feb\u901f\u91cd\u89c4\u5212\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u591a\u673a\u5668\u4eba\u5bfc\u822a\u4e2d\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u9002\u7528\u4e8e\u52a8\u6001\u73af\u5883\u3002"}}
{"id": "2507.11479", "categories": ["cs.AI", "cs.GR", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.11479", "abs": "https://arxiv.org/abs/2507.11479", "authors": ["Daniel Platnick", "Matti Gruener", "Marjan Alirezaie", "Kent Larson", "Dava J. Newman", "Hossein Rahnama"], "title": "Perspective-Aware AI in Extended Reality", "comment": "Accepted to the International Conference on eXtended Reality (2025),\n  12 pages, 3 figures", "summary": "AI-enhanced Extended Reality (XR) aims to deliver adaptive, immersive\nexperiences-yet current systems fall short due to shallow user modeling and\nlimited cognitive context. We introduce Perspective-Aware AI in Extended\nReality (PAiR), a foundational framework for integrating Perspective-Aware AI\n(PAi) with XR to enable interpretable, context-aware experiences grounded in\nuser identity. PAi is built on Chronicles: reasoning-ready identity models\nlearned from multimodal digital footprints that capture users' cognitive and\nexperiential evolution. PAiR employs these models in a closed-loop system\nlinking dynamic user states with immersive environments. We present PAiR's\narchitecture, detailing its modules and system flow, and demonstrate its\nutility through two proof-of-concept scenarios implemented in the Unity-based\nOpenDome engine. PAiR opens a new direction for human-AI interaction by\nembedding perspective-based identity models into immersive systems.", "AI": {"tldr": "PAiR\u6846\u67b6\u901a\u8fc7\u6574\u5408Perspective-Aware AI\u4e0eXR\uff0c\u5229\u7528\u591a\u6a21\u6001\u6570\u5b57\u8db3\u8ff9\u6784\u5efa\u7528\u6237\u8eab\u4efd\u6a21\u578b\uff0c\u5b9e\u73b0\u53ef\u89e3\u91ca\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u6c89\u6d78\u5f0f\u4f53\u9a8c\u3002", "motivation": "\u5f53\u524dAI\u589e\u5f3a\u7684XR\u7cfb\u7edf\u56e0\u7528\u6237\u5efa\u6a21\u6d45\u663e\u548c\u8ba4\u77e5\u4e0a\u4e0b\u6587\u6709\u9650\u800c\u8868\u73b0\u4e0d\u4f73\uff0cPAiR\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u57fa\u4e8eChronicles\uff08\u591a\u6a21\u6001\u6570\u5b57\u8db3\u8ff9\u5b66\u4e60\uff09\u6784\u5efa\u8eab\u4efd\u6a21\u578b\uff0c\u5e76\u5728\u95ed\u73af\u7cfb\u7edf\u4e2d\u52a8\u6001\u94fe\u63a5\u7528\u6237\u72b6\u6001\u4e0e\u6c89\u6d78\u73af\u5883\u3002", "result": "\u901a\u8fc7Unity\u5f15\u64ce\u5b9e\u73b0\u7684\u4e24\u4e2a\u6982\u5ff5\u9a8c\u8bc1\u573a\u666f\u5c55\u793a\u4e86PAiR\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "PAiR\u4e3a\u4eba\u7c7b-AI\u4ea4\u4e92\u5f00\u8f9f\u4e86\u65b0\u65b9\u5411\uff0c\u901a\u8fc7\u5d4c\u5165\u57fa\u4e8e\u89c6\u89d2\u7684\u8eab\u4efd\u6a21\u578b\u63d0\u5347\u6c89\u6d78\u5f0f\u7cfb\u7edf\u3002"}}
{"id": "2507.11498", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.11498", "abs": "https://arxiv.org/abs/2507.11498", "authors": ["Asad Ali Shahid", "Francesco Braghin", "Loris Roveda"], "title": "Robot Drummer: Learning Rhythmic Skills for Humanoid Drumming", "comment": null, "summary": "Humanoid robots have seen remarkable advances in dexterity, balance, and\nlocomotion, yet their role in expressive domains, such as music performance,\nremains largely unexplored. Musical tasks, like drumming, present unique\nchallenges, including split-second timing, rapid contacts, and multi-limb\ncoordination over pieces lasting minutes. In this paper, we introduce Robot\nDrummer, a humanoid system capable of expressive, high-precision drumming\nacross a diverse repertoire of songs. We formulate humanoid drumming as\nsequential fulfillment of timed-contacts and transform drum scores in to a\nRhythmic Contact Chain. To handle the long-horizon nature of musical\nperformance, we decompose each piece into fixed-length segments and train a\nsingle policy across all segments in parallel using reinforcement learning.\nThrough extensive experiments on over thirty popular rock, metal, and jazz\ntracks, our results demonstrate that Robot Drummer consistently achieves high\nF1 scores. The learned behaviors exhibit emergent human-like drumming\nstrategies, such as cross-arm strikes, and adaptive sticks assignments,\ndemonstrating the potential of reinforcement learning to bring humanoid robots\ninto the domain of creative musical performance. Project page:\n\\href{https://robot-drummer.github.io}{robot-drummer.github.io}", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Robot Drummer\uff0c\u4e00\u79cd\u80fd\u591f\u9ad8\u7cbe\u5ea6\u6f14\u594f\u591a\u79cd\u66f2\u76ee\u7684\u4eba\u5f62\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u5b9e\u73b0\u957f\u65f6\u7a0b\u97f3\u4e50\u8868\u6f14\u3002", "motivation": "\u63a2\u7d22\u4eba\u5f62\u673a\u5668\u4eba\u5728\u97f3\u4e50\u8868\u6f14\u7b49\u8868\u8fbe\u6027\u9886\u57df\u7684\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u9f13\u4e50\u6f14\u594f\u4e2d\u7684\u5feb\u901f\u53cd\u5e94\u548c\u591a\u80a2\u534f\u8c03\u6311\u6218\u3002", "method": "\u5c06\u9f13\u4e50\u8c31\u8f6c\u5316\u4e3a\u8282\u594f\u63a5\u89e6\u94fe\uff0c\u5206\u6bb5\u8bad\u7ec3\u5e76\u884c\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u3002", "result": "\u5728\u4e09\u5341\u591a\u9996\u6d41\u884c\u66f2\u76ee\u4e2d\u8868\u73b0\u51fa\u9ad8F1\u5206\u6570\uff0c\u5e76\u6d8c\u73b0\u51fa\u7c7b\u4f3c\u4eba\u7c7b\u7684\u9f13\u4e50\u7b56\u7565\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u6709\u671b\u5c06\u4eba\u5f62\u673a\u5668\u4eba\u5f15\u5165\u521b\u9020\u6027\u97f3\u4e50\u8868\u6f14\u9886\u57df\u3002"}}
{"id": "2507.11482", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11482", "abs": "https://arxiv.org/abs/2507.11482", "authors": ["Mani Hamidi", "Terrence W. Deacon"], "title": "Illuminating the Three Dogmas of Reinforcement Learning under Evolutionary Light", "comment": null, "summary": "Three core tenets of reinforcement learning (RL)--concerning the definition\nof agency, the objective of learning, and the scope of the reward\nhypothesis--have been highlighted as key targets for conceptual revision, with\nmajor implications for theory and application. We propose a framework, inspired\nby open-ended evolutionary theory, to reconsider these three \"dogmas.\" We\nrevisit each assumption and address related concerns raised alongside them. To\nmake our arguments relevant to RL as a model of biological learning, we first\nestablish that evolutionary dynamics can plausibly operate within living brains\nover an individual's lifetime, and are not confined to cross-generational\nprocesses. We begin by revisiting the second dogma, drawing on evolutionary\ninsights to enrich the \"adaptation-rather-than-search\" view of learning. We\nthen address the third dogma regarding the limits of the reward hypothesis,\nusing analogies from evolutionary fitness to illuminate the scalar reward vs.\nmulti-objective debate. After discussing practical implications for exploration\nin RL, we turn to the first--and arguably most fundamental--issue: the absence\nof a formal account of agency. We argue that unlike the other two problems, the\nevolutionary paradigm alone cannot resolve the agency question, though it\ngestures in a productive direction. We advocate integrating ideas from\norigins-of-life theory, where the thermodynamics of sustenance and replication\noffer promising foundations for understanding agency and resource-constrained\nreinforcement learning in biological systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u5f00\u653e\u5f0f\u8fdb\u5316\u7406\u8bba\u7684\u6846\u67b6\uff0c\u91cd\u65b0\u5ba1\u89c6\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u4e09\u4e2a\u6838\u5fc3\u4fe1\u6761\uff1a\u4ee3\u7406\u7684\u5b9a\u4e49\u3001\u5b66\u4e60\u76ee\u6807\u548c\u5956\u52b1\u5047\u8bbe\u7684\u8303\u56f4\uff0c\u5e76\u63a2\u8ba8\u5176\u7406\u8bba\u548c\u5e94\u7528\u610f\u4e49\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\u7684\u4e09\u4e2a\u6838\u5fc3\u4fe1\u6761\u5728\u7406\u8bba\u548c\u5e94\u7528\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u8fdb\u5316\u7406\u8bba\u7684\u89c6\u89d2\u91cd\u65b0\u601d\u8003\u8fd9\u4e9b\u95ee\u9898\uff0c\u4ee5\u63a8\u52a8RL\u7684\u53d1\u5c55\u3002", "method": "\u901a\u8fc7\u7c7b\u6bd4\u8fdb\u5316\u7406\u8bba\uff0c\u91cd\u65b0\u5206\u6790RL\u7684\u4e09\u4e2a\u4fe1\u6761\uff0c\u5e76\u7ed3\u5408\u751f\u7269\u5b66\u5b66\u4e60\u7684\u80cc\u666f\uff0c\u63a2\u8ba8\u8fdb\u5316\u52a8\u6001\u5728\u4e2a\u4f53\u751f\u547d\u5468\u671f\u5185\u7684\u4f5c\u7528\u3002", "result": "\u8fdb\u5316\u7406\u8bba\u4e3aRL\u7684\u7b2c\u4e8c\u548c\u7b2c\u4e09\u4fe1\u6761\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\uff0c\u4f46\u4ee3\u7406\u95ee\u9898\u4ecd\u9700\u7ed3\u5408\u751f\u547d\u8d77\u6e90\u7406\u8bba\u6765\u89e3\u51b3\u3002", "conclusion": "\u8fdb\u5316\u7406\u8bba\u4e3aRL\u7684\u90e8\u5206\u95ee\u9898\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u4ee3\u7406\u95ee\u9898\u9700\u8981\u66f4\u5e7f\u6cdb\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5982\u751f\u547d\u8d77\u6e90\u7406\u8bba\u3002"}}
{"id": "2507.11525", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.11525", "abs": "https://arxiv.org/abs/2507.11525", "authors": ["Ana Davila", "Jacinto Colan", "Yasuhisa Hasegawa"], "title": "LLM-based ambiguity detection in natural language instructions for collaborative surgical robots", "comment": "Accepted at 2025 IEEE International Conference on Robot and Human\n  Interactive Communication (ROMAN)", "summary": "Ambiguity in natural language instructions poses significant risks in\nsafety-critical human-robot interaction, particularly in domains such as\nsurgery. To address this, we propose a framework that uses Large Language\nModels (LLMs) for ambiguity detection specifically designed for collaborative\nsurgical scenarios. Our method employs an ensemble of LLM evaluators, each\nconfigured with distinct prompting techniques to identify linguistic,\ncontextual, procedural, and critical ambiguities. A chain-of-thought evaluator\nis included to systematically analyze instruction structure for potential\nissues. Individual evaluator assessments are synthesized through conformal\nprediction, which yields non-conformity scores based on comparison to a labeled\ncalibration dataset. Evaluating Llama 3.2 11B and Gemma 3 12B, we observed\nclassification accuracy exceeding 60% in differentiating ambiguous from\nunambiguous surgical instructions. Our approach improves the safety and\nreliability of human-robot collaboration in surgery by offering a mechanism to\nidentify potentially ambiguous instructions before robot action.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u624b\u672f\u573a\u666f\u4e2d\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u7684\u6b67\u4e49\u6027\uff0c\u4ee5\u63d0\u9ad8\u4eba\u673a\u534f\u4f5c\u7684\u5b89\u5168\u6027\u3002", "motivation": "\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u7684\u6b67\u4e49\u6027\u5728\u5b89\u5168\u5173\u952e\u7684\u4eba\u673a\u4ea4\u4e92\uff08\u5982\u624b\u672f\uff09\u4e2d\u5177\u6709\u9ad8\u98ce\u9669\uff0c\u9700\u89e3\u51b3\u6b64\u95ee\u9898\u4ee5\u63d0\u9ad8\u534f\u4f5c\u7684\u53ef\u9760\u6027\u3002", "method": "\u4f7f\u7528\u591a\u79cd\u63d0\u793a\u6280\u672f\u914d\u7f6e\u7684LLM\u8bc4\u4f30\u5668\u96c6\u5408\uff0c\u7ed3\u5408\u94fe\u5f0f\u601d\u7ef4\u8bc4\u4f30\u5668\u548c\u4fdd\u5f62\u9884\u6d4b\uff0c\u68c0\u6d4b\u8bed\u8a00\u3001\u4e0a\u4e0b\u6587\u3001\u6d41\u7a0b\u548c\u5173\u952e\u6b67\u4e49\u3002", "result": "\u5728Llama 3.2 11B\u548cGemma 3 12B\u4e0a\u6d4b\u8bd5\uff0c\u5206\u7c7b\u51c6\u786e\u7387\u8d85\u8fc760%\uff0c\u80fd\u6709\u6548\u533a\u5206\u6b67\u4e49\u4e0e\u975e\u6b67\u4e49\u6307\u4ee4\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u624b\u672f\u4e2d\u7684\u4eba\u673a\u534f\u4f5c\u63d0\u4f9b\u4e86\u4e00\u79cd\u8bc6\u522b\u6f5c\u5728\u6b67\u4e49\u6307\u4ee4\u7684\u673a\u5236\uff0c\u63d0\u5347\u4e86\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2507.11527", "categories": ["cs.AI", "cs.CE"], "pdf": "https://arxiv.org/pdf/2507.11527", "abs": "https://arxiv.org/abs/2507.11527", "authors": ["Yinsheng Li", "Zhen Dong", "Yi Shao"], "title": "DrafterBench: Benchmarking Large Language Models for Tasks Automation in Civil Engineering", "comment": "Project page: https://github.com/Eason-Li-AIS/DrafterBench", "summary": "Large Language Model (LLM) agents have shown great potential for solving\nreal-world problems and promise to be a solution for tasks automation in\nindustry. However, more benchmarks are needed to systematically evaluate\nautomation agents from an industrial perspective, for example, in Civil\nEngineering. Therefore, we propose DrafterBench for the comprehensive\nevaluation of LLM agents in the context of technical drawing revision, a\nrepresentation task in civil engineering. DrafterBench contains twelve types of\ntasks summarized from real-world drawing files, with 46 customized\nfunctions/tools and 1920 tasks in total. DrafterBench is an open-source\nbenchmark to rigorously test AI agents' proficiency in interpreting intricate\nand long-context instructions, leveraging prior knowledge, and adapting to\ndynamic instruction quality via implicit policy awareness. The toolkit\ncomprehensively assesses distinct capabilities in structured data\ncomprehension, function execution, instruction following, and critical\nreasoning. DrafterBench offers detailed analysis of task accuracy and error\nstatistics, aiming to provide deeper insight into agent capabilities and\nidentify improvement targets for integrating LLMs in engineering applications.\nOur benchmark is available at https://github.com/Eason-Li-AIS/DrafterBench,\nwith the test set hosted at\nhttps://huggingface.co/datasets/Eason666/DrafterBench.", "AI": {"tldr": "DrafterBench\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30LLM\u4ee3\u7406\u5728\u571f\u6728\u5de5\u7a0b\u56fe\u7eb8\u4fee\u8ba2\u4efb\u52a1\u4e2d\u7684\u5f00\u6e90\u57fa\u51c6\uff0c\u5305\u542b12\u7c7b\u4efb\u52a1\u300146\u4e2a\u5b9a\u5236\u529f\u80fd\u548c1920\u4e2a\u4efb\u52a1\uff0c\u65e8\u5728\u6d4b\u8bd5\u4ee3\u7406\u7684\u591a\u65b9\u9762\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u4e0d\u8db3\u4ee5\u4ece\u5de5\u4e1a\u89d2\u5ea6\u7cfb\u7edf\u8bc4\u4f30LLM\u4ee3\u7406\uff0c\u7279\u522b\u662f\u5728\u571f\u6728\u5de5\u7a0b\u9886\u57df\uff0c\u56e0\u6b64\u9700\u8981DrafterBench\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "DrafterBench\u603b\u7ed3\u4e86\u771f\u5b9e\u56fe\u7eb8\u6587\u4ef6\u4e2d\u7684\u4efb\u52a1\u7c7b\u578b\uff0c\u63d0\u4f9b\u5b9a\u5236\u529f\u80fd\u548c\u4efb\u52a1\uff0c\u8bc4\u4f30\u4ee3\u7406\u7684\u6307\u4ee4\u7406\u89e3\u3001\u77e5\u8bc6\u5229\u7528\u548c\u52a8\u6001\u9002\u5e94\u80fd\u529b\u3002", "result": "\u57fa\u51c6\u5168\u9762\u8bc4\u4f30\u4e86\u4ee3\u7406\u7684\u7ed3\u6784\u5316\u6570\u636e\u7406\u89e3\u3001\u529f\u80fd\u6267\u884c\u3001\u6307\u4ee4\u9075\u5faa\u548c\u6279\u5224\u6027\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u63d0\u4f9b\u4efb\u52a1\u51c6\u786e\u6027\u548c\u9519\u8bef\u7edf\u8ba1\u7684\u8be6\u7ec6\u5206\u6790\u3002", "conclusion": "DrafterBench\u4e3aLLM\u5728\u5de5\u7a0b\u5e94\u7528\u4e2d\u7684\u96c6\u6210\u63d0\u4f9b\u4e86\u6539\u8fdb\u65b9\u5411\uff0c\u5e76\u5f00\u6e90\u4e86\u6d4b\u8bd5\u96c6\u548c\u5de5\u5177\u5305\u3002"}}
{"id": "2507.11538", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.11538", "abs": "https://arxiv.org/abs/2507.11538", "authors": ["Daniel Jaroslawicz", "Brendan Whiting", "Parth Shah", "Karime Maamari"], "title": "How Many Instructions Can LLMs Follow at Once?", "comment": null, "summary": "Production-grade LLM systems require robust adherence to dozens or even\nhundreds of instructions simultaneously. However, the instruction-following\ncapabilities of LLMs at high instruction densities have not yet been\ncharacterized, as existing benchmarks only evaluate models on tasks with a\nsingle or few instructions. We introduce IFScale, a simple benchmark of 500\nkeyword-inclusion instructions for a business report writing task to measure\nhow instruction-following performance degrades as instruction density\nincreases. We evaluate 20 state-of-the-art models across seven major providers\nand find that even the best frontier models only achieve 68% accuracy at the\nmax density of 500 instructions. Our analysis reveals model size and reasoning\ncapability to correlate with 3 distinct performance degradation patterns, bias\ntowards earlier instructions, and distinct categories of instruction-following\nerrors. Our insights can help inform design of instruction-dense prompts in\nreal-world applications and highlight important performance-latency tradeoffs.\nWe open-source the benchmark and all results for further analysis at\nhttps://distylai.github.io/IFScale.", "AI": {"tldr": "IFScale\u662f\u4e00\u4e2a\u8bc4\u4f30LLM\u5728\u9ad8\u6307\u4ee4\u5bc6\u5ea6\u4e0b\u6027\u80fd\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u53d1\u73b0\u5373\u4f7f\u524d\u6cbf\u6a21\u578b\u5728500\u6761\u6307\u4ee4\u65f6\u51c6\u786e\u7387\u4ec568%\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u4ec5\u8bc4\u4f30\u5355\u6307\u4ee4\u6216\u5c11\u91cf\u6307\u4ee4\u4efb\u52a1\uff0c\u65e0\u6cd5\u53cd\u6620\u751f\u4ea7\u7ea7LLM\u7cfb\u7edf\u9700\u540c\u65f6\u9075\u5faa\u5927\u91cf\u6307\u4ee4\u7684\u5b9e\u9645\u60c5\u51b5\u3002", "method": "\u5f15\u5165IFScale\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b500\u6761\u5173\u952e\u8bcd\u5305\u542b\u6307\u4ee4\uff0c\u8bc4\u4f3020\u4e2a\u524d\u6cbf\u6a21\u578b\u5728\u9ad8\u6307\u4ee4\u5bc6\u5ea6\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u6700\u4f73\u6a21\u578b\u5728500\u6761\u6307\u4ee4\u65f6\u51c6\u786e\u7387\u4e3a68%\uff0c\u6a21\u578b\u5927\u5c0f\u548c\u63a8\u7406\u80fd\u529b\u4e0e\u6027\u80fd\u4e0b\u964d\u6a21\u5f0f\u76f8\u5173\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u6709\u52a9\u4e8e\u8bbe\u8ba1\u9ad8\u6307\u4ee4\u5bc6\u5ea6\u63d0\u793a\uff0c\u5e76\u63ed\u793a\u4e86\u6027\u80fd\u4e0e\u5ef6\u8fdf\u7684\u6743\u8861\u3002"}}
