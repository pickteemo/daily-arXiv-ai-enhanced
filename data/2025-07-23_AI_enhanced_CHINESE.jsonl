{"id": "2507.15865", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15865", "abs": "https://arxiv.org/abs/2507.15865", "authors": ["Shai Shalev-Shwartz", "Amnon Shashua"], "title": "From Reasoning to Super-Intelligence: A Search-Theoretic Perspective", "comment": null, "summary": "Chain-of-Thought (CoT) reasoning has emerged as a powerful tool for enhancing\nthe problem-solving capabilities of large language models (LLMs). However, the\ntheoretical foundations of learning from CoT data remain underdeveloped, and\nexisting approaches -- such as Supervised Fine-Tuning (SFT), Reinforcement\nLearning (RL), Tree-of-Thoughts (ToT), and Monte Carlo Tree Search (MCTS) --\noften fail on complex reasoning tasks. In this work, we identify core obstacles\nthat hinder effective CoT learning, including distribution drift, lack of\nembedded search, and exponential inference costs. We introduce the Diligent\nLearner, a new learning paradigm that explicitly models reasoning as a\ndepth-first search guided by a validator and supports backtracking upon\nfailure. Under two mild and realistic assumptions, we prove that the Diligent\nLearner can efficiently learn from CoT data while existing methods fail to do\nso. This framework offers a path toward building scalable and reliable\nreasoning systems trained on naturally occurring, incomplete data -- paving the\nway for the development of Large Reasoning Models (LRMs) with robust,\ninterpretable problem-solving abilities.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\"\u52e4\u594b\u5b66\u4e60\u8005\"(Diligent Learner)\u8303\u5f0f\uff0c\u901a\u8fc7\u6df1\u5ea6\u4f18\u5148\u641c\u7d22\u548c\u9a8c\u8bc1\u5668\u6307\u5bfc\u6765\u6539\u8fdb\u5927\u8bed\u8a00\u6a21\u578b\u7684\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u5b66\u4e60\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u5931\u6548\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u94fe\u5f0f\u601d\u7ef4(CoT)\u63a8\u7406\u65b9\u6cd5\u5982\u76d1\u7763\u5fae\u8c03\u3001\u5f3a\u5316\u5b66\u4e60\u3001\u601d\u7ef4\u6811\u7b49\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u7ecf\u5e38\u5931\u6548\uff0c\u4e14\u7f3a\u4e4f\u575a\u5b9e\u7684\u7406\u8bba\u57fa\u7840\u3002\u6838\u5fc3\u969c\u788d\u5305\u62ec\u5206\u5e03\u6f02\u79fb\u3001\u7f3a\u4e4f\u5d4c\u5165\u5f0f\u641c\u7d22\u548c\u6307\u6570\u7ea7\u63a8\u7406\u6210\u672c\u3002", "method": "\u63d0\u51fa\"\u52e4\u594b\u5b66\u4e60\u8005\"\u5b66\u4e60\u8303\u5f0f\uff0c\u5c06\u63a8\u7406\u660e\u786e\u5efa\u6a21\u4e3a\u7531\u9a8c\u8bc1\u5668\u6307\u5bfc\u7684\u6df1\u5ea6\u4f18\u5148\u641c\u7d22\u8fc7\u7a0b\uff0c\u652f\u6301\u5931\u8d25\u65f6\u7684\u56de\u6eaf\u673a\u5236\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u4ece\u81ea\u7136\u51fa\u73b0\u7684\u4e0d\u5b8c\u6574CoT\u6570\u636e\u4e2d\u5b66\u4e60\u3002", "result": "\u5728\u4e24\u4e2a\u6e29\u548c\u4e14\u73b0\u5b9e\u7684\u5047\u8bbe\u6761\u4ef6\u4e0b\uff0c\u7406\u8bba\u8bc1\u660e\u4e86\u52e4\u594b\u5b66\u4e60\u8005\u80fd\u591f\u9ad8\u6548\u5730\u4eceCoT\u6570\u636e\u4e2d\u5b66\u4e60\uff0c\u800c\u73b0\u6709\u65b9\u6cd5\u5728\u6b64\u6761\u4ef6\u4e0b\u4f1a\u5931\u6548\u3002\u8be5\u6846\u67b6\u4e3a\u6784\u5efa\u53ef\u6269\u5c55\u4e14\u53ef\u9760\u7684\u63a8\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u8def\u5f84\u3002", "conclusion": "\u52e4\u594b\u5b66\u4e60\u8005\u8303\u5f0f\u4e3a\u5f00\u53d1\u5177\u6709\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u7684\u5927\u578b\u63a8\u7406\u6a21\u578b(LRMs)\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u80fd\u591f\u5728\u81ea\u7136\u51fa\u73b0\u7684\u4e0d\u5b8c\u6574\u6570\u636e\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u89e3\u51b3\u4e86\u73b0\u6709CoT\u5b66\u4e60\u65b9\u6cd5\u7684\u6838\u5fc3\u9650\u5236\u3002"}}
{"id": "2507.15866", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15866", "abs": "https://arxiv.org/abs/2507.15866", "authors": ["Marek Vlk", "Premysl Sucha", "Jaroslaw Rudy", "Radoslaw Idzikowski"], "title": "Purchase and Production Optimization in a Meat Processing Plant", "comment": "25 pages, 5 figures", "summary": "The food production industry, especially the meat production sector, faces\nmany challenges that have even escalated due to the recent outbreak of the\nenergy crisis in the European Union. Therefore, efficient use of input\nmaterials is an essential aspect affecting the profit of such companies. This\npaper addresses an optimization problem concerning the purchase and subsequent\nmaterial processing we solved for a meat processing company. Unlike the\nmajority of existing papers, we do not concentrate on how this problem concerns\nsupply chain management, but we focus purely on the production stage. The\nproblem involves the concept of alternative ways of material processing, stock\nof material with different expiration dates, and extra constraints widely\nneglected in the current literature, namely, the minimum order quantity and the\nminimum percentage in alternatives. We prove that each of these two constraints\nmakes the problem \\mbox{$\\mathcal{NP}$-hard}, and hence we design a simple\niterative approach based on integer linear programming that allows us to solve\nreal-life instances even using an open-source integer linear programming\nsolver. Another advantage of this approach is that it mitigates numerical\nissues, caused by the extensive range of data values, we experienced with a\ncommercial solver. The results obtained using real data from the meat\nprocessing company showed that our algorithm can find the optimum solution in a\nfew seconds for all considered use cases.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u8089\u7c7b\u52a0\u5de5\u4f01\u4e1a\u7684\u6750\u6599\u91c7\u8d2d\u548c\u52a0\u5de5\u4f18\u5316\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u57fa\u4e8e\u6574\u6570\u7ebf\u6027\u89c4\u5212\u7684\u8fed\u4ee3\u65b9\u6cd5\uff0c\u5728\u8003\u8651\u6700\u5c0f\u8ba2\u8d2d\u91cf\u548c\u66ff\u4ee3\u65b9\u6848\u6700\u5c0f\u767e\u5206\u6bd4\u7b49\u5b9e\u9645\u7ea6\u675f\u4e0b\uff0c\u80fd\u591f\u5feb\u901f\u627e\u5230\u6700\u4f18\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u98df\u54c1\u751f\u4ea7\u884c\u4e1a\uff0c\u7279\u522b\u662f\u8089\u7c7b\u751f\u4ea7\u90e8\u95e8\u9762\u4e34\u8bf8\u591a\u6311\u6218\uff0c\u6b27\u76df\u80fd\u6e90\u5371\u673a\u7684\u7206\u53d1\u66f4\u662f\u96ea\u4e0a\u52a0\u971c\u3002\u56e0\u6b64\uff0c\u9ad8\u6548\u5229\u7528\u6295\u5165\u6750\u6599\u662f\u5f71\u54cd\u6b64\u7c7b\u4f01\u4e1a\u76c8\u5229\u7684\u5173\u952e\u56e0\u7d20\u3002\u73b0\u6709\u6587\u732e\u4e3b\u8981\u5173\u6ce8\u4f9b\u5e94\u94fe\u7ba1\u7406\uff0c\u800c\u5ffd\u89c6\u4e86\u751f\u4ea7\u9636\u6bb5\u7684\u4f18\u5316\u95ee\u9898\u3002", "method": "\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u6574\u6570\u7ebf\u6027\u89c4\u5212\u7684\u7b80\u5355\u8fed\u4ee3\u65b9\u6cd5\u6765\u89e3\u51b3\u5b9e\u9645\u95ee\u9898\u5b9e\u4f8b\u3002\u8be5\u65b9\u6cd5\u8003\u8651\u4e86\u6750\u6599\u5904\u7406\u7684\u66ff\u4ee3\u65b9\u5f0f\u3001\u4e0d\u540c\u5230\u671f\u65e5\u671f\u7684\u6750\u6599\u5e93\u5b58\uff0c\u4ee5\u53ca\u6700\u5c0f\u8ba2\u8d2d\u91cf\u548c\u66ff\u4ee3\u65b9\u6848\u6700\u5c0f\u767e\u5206\u6bd4\u7b49\u5728\u5f53\u524d\u6587\u732e\u4e2d\u88ab\u5e7f\u6cdb\u5ffd\u89c6\u7684\u989d\u5916\u7ea6\u675f\u6761\u4ef6\u3002", "result": "\u4f7f\u7528\u8089\u7c7b\u52a0\u5de5\u516c\u53f8\u7684\u771f\u5b9e\u6570\u636e\u8fdb\u884c\u6d4b\u8bd5\uff0c\u7b97\u6cd5\u80fd\u591f\u5728\u51e0\u79d2\u949f\u5185\u4e3a\u6240\u6709\u8003\u8651\u7684\u7528\u4f8b\u627e\u5230\u6700\u4f18\u89e3\u3002\u8be5\u65b9\u6cd5\u8fd8\u80fd\u591f\u7f13\u89e3\u7531\u4e8e\u6570\u636e\u503c\u8303\u56f4\u5e7f\u6cdb\u800c\u5bfc\u81f4\u7684\u6570\u503c\u95ee\u9898\uff0c\u8fd9\u4e9b\u95ee\u9898\u5728\u5546\u4e1a\u6c42\u89e3\u5668\u4e2d\u66fe\u7ecf\u9047\u5230\u8fc7\u3002", "conclusion": "\u8bc1\u660e\u4e86\u6700\u5c0f\u8ba2\u8d2d\u91cf\u548c\u66ff\u4ee3\u65b9\u6848\u6700\u5c0f\u767e\u5206\u6bd4\u8fd9\u4e24\u4e2a\u7ea6\u675f\u6761\u4ef6\u90fd\u4f7f\u95ee\u9898\u53d8\u4e3aNP\u96be\u95ee\u9898\u3002\u63d0\u51fa\u7684\u8fed\u4ee3\u65b9\u6cd5\u4e0d\u4ec5\u80fd\u591f\u4f7f\u7528\u5f00\u6e90\u6574\u6570\u7ebf\u6027\u89c4\u5212\u6c42\u89e3\u5668\u89e3\u51b3\u5b9e\u9645\u95ee\u9898\uff0c\u8fd8\u5177\u6709\u5feb\u901f\u6c42\u89e3\u548c\u907f\u514d\u6570\u503c\u95ee\u9898\u7684\u4f18\u52bf\u3002"}}
{"id": "2507.15874", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.15874", "abs": "https://arxiv.org/abs/2507.15874", "authors": ["Yin Wu", "Daniel Slieter", "Vivek Subramanian", "Ahmed Abouelazm", "Robin Bohn", "J. Marius Z\u00f6llner"], "title": "Why Braking? Scenario Extraction and Reasoning Utilizing LLM", "comment": null, "summary": "The growing number of ADAS-equipped vehicles has led to a dramatic increase\nin driving data, yet most of them capture routine driving behavior. Identifying\nand understanding safety-critical corner cases within this vast dataset remains\na significant challenge. Braking events are particularly indicative of\npotentially hazardous situations, motivating the central question of our\nresearch: Why does a vehicle brake? Existing approaches primarily rely on\nrule-based heuristics to retrieve target scenarios using predefined condition\nfilters. While effective in simple environments such as highways, these methods\nlack generalization in complex urban settings. In this paper, we propose a\nnovel framework that leverages Large Language Model (LLM) for scenario\nunderstanding and reasoning. Our method bridges the gap between low-level\nnumerical signals and natural language descriptions, enabling LLM to interpret\nand classify driving scenarios. We propose a dual-path scenario retrieval that\nsupports both category-based search for known scenarios and embedding-based\nretrieval for unknown Out-of-Distribution (OOD) scenarios. To facilitate\nevaluation, we curate scenario annotations on the Argoverse 2 Sensor Dataset.\nExperimental results show that our method outperforms rule-based baselines and\ngeneralizes well to OOD scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b(LLM)\u7684\u6846\u67b6\u6765\u7406\u89e3\u548c\u5206\u6790\u9a7e\u9a76\u573a\u666f\u4e2d\u7684\u5236\u52a8\u4e8b\u4ef6\uff0c\u901a\u8fc7\u53cc\u8def\u5f84\u573a\u666f\u68c0\u7d22\u65b9\u6cd5\u80fd\u591f\u8bc6\u522b\u5df2\u77e5\u573a\u666f\u548c\u672a\u77e5\u7684\u5206\u5e03\u5916\u573a\u666f\uff0c\u5728Argoverse 2\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\u8be5\u65b9\u6cd5\u4f18\u4e8e\u57fa\u4e8e\u89c4\u5219\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740ADAS\u8bbe\u5907\u8f66\u8f86\u6570\u91cf\u589e\u957f\uff0c\u9a7e\u9a76\u6570\u636e\u6025\u5267\u589e\u52a0\uff0c\u4f46\u5927\u591a\u6570\u6570\u636e\u53ea\u662f\u5e38\u89c4\u9a7e\u9a76\u884c\u4e3a\u3002\u5728\u5e9e\u5927\u6570\u636e\u96c6\u4e2d\u8bc6\u522b\u548c\u7406\u89e3\u5b89\u5168\u5173\u952e\u7684\u8fb9\u7f18\u6848\u4f8b\u4ecd\u7136\u662f\u91cd\u5927\u6311\u6218\u3002\u5236\u52a8\u4e8b\u4ef6\u7279\u522b\u80fd\u6307\u793a\u6f5c\u5728\u5371\u9669\u60c5\u51b5\uff0c\u56e0\u6b64\u7814\u7a76\u7684\u6838\u5fc3\u95ee\u9898\u662f\uff1a\u8f66\u8f86\u4e3a\u4ec0\u4e48\u5236\u52a8\uff1f\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u57fa\u4e8e\u89c4\u5219\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u5728\u9ad8\u901f\u516c\u8def\u7b49\u7b80\u5355\u73af\u5883\u4e2d\u6709\u6548\uff0c\u4f46\u5728\u590d\u6742\u57ce\u5e02\u73af\u5883\u4e2d\u7f3a\u4e4f\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b(LLM)\u8fdb\u884c\u573a\u666f\u7406\u89e3\u548c\u63a8\u7406\u7684\u65b0\u6846\u67b6\u3002\u8be5\u65b9\u6cd5\u5728\u4f4e\u7ea7\u6570\u503c\u4fe1\u53f7\u548c\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u4e4b\u95f4\u5efa\u7acb\u6865\u6881\uff0c\u4f7fLLM\u80fd\u591f\u89e3\u91ca\u548c\u5206\u7c7b\u9a7e\u9a76\u573a\u666f\u3002\u63d0\u51fa\u4e86\u53cc\u8def\u5f84\u573a\u666f\u68c0\u7d22\u65b9\u6cd5\uff0c\u652f\u6301\u57fa\u4e8e\u7c7b\u522b\u7684\u5df2\u77e5\u573a\u666f\u641c\u7d22\u548c\u57fa\u4e8e\u5d4c\u5165\u7684\u672a\u77e5\u5206\u5e03\u5916(OOD)\u573a\u666f\u68c0\u7d22\u3002\u5728Argoverse 2\u4f20\u611f\u5668\u6570\u636e\u96c6\u4e0a\u7b56\u5212\u573a\u666f\u6ce8\u91ca\u4ee5\u4fbf\u4e8e\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u57fa\u4e8e\u89c4\u5219\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u4e14\u5728\u5206\u5e03\u5916(OOD)\u573a\u666f\u4e2d\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002\u8be5\u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u8bc6\u522b\u548c\u5206\u7c7b\u9a7e\u9a76\u573a\u666f\u4e2d\u7684\u5236\u52a8\u4e8b\u4ef6\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u7684\u57ce\u5e02\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u7814\u7a76\u6210\u529f\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u9a7e\u9a76\u573a\u666f\u7406\u89e3\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u57fa\u4e8e\u89c4\u5219\u65b9\u6cd5\u5728\u590d\u6742\u73af\u5883\u4e2d\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\u3002\u901a\u8fc7\u53cc\u8def\u5f84\u68c0\u7d22\u673a\u5236\uff0c\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u80fd\u5904\u7406\u5df2\u77e5\u573a\u666f\uff0c\u8fd8\u80fd\u8bc6\u522b\u672a\u77e5\u7684\u8fb9\u7f18\u6848\u4f8b\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u5b89\u5168\u5173\u952e\u573a\u666f\u7684\u8bc6\u522b\u548c\u7406\u89e3\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.15875", "categories": ["cs.AI", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.15875", "abs": "https://arxiv.org/abs/2507.15875", "authors": ["Jerry Li", "Timothy Oh", "Joseph Hoang", "Vardhit Veeramachaneni"], "title": "Differential Multimodal Transformers", "comment": null, "summary": "Small language models have gained significant popularity due to their\nefficiency and growing capabilities. However, incorporating additional\nmodalities, such as vision, can exacerbate the challenge of limited context\nwindows by introducing noise. Recent studies have highlighted that Transformer\nattention mechanisms often disproportionately focus on irrelevant contexts. In\nthis work, we extend the Differential Attention mechanism, originally designed\nfor text-only models, to the text-vision model PaliGemma. Our aim is to\nevaluate its ability to mitigate noisy information retrieval and reduce\nhallucinations. To this end, we fine-tuned the PaliGemma 3B model using LoRA,\nincorporating Differential Attention, and experimented with various parameter\nsettings and configurations. We demonstrate that Differential Attention can be\nadapted and integrated into the fine-tuning of existing models to enhance noisy\ninformation retrieval and question-answering capabilities.", "AI": {"tldr": "\u672c\u6587\u5c06\u5dee\u5206\u6ce8\u610f\u529b\u673a\u5236\u6269\u5c55\u5230\u6587\u672c-\u89c6\u89c9\u6a21\u578bPaliGemma\u4e2d\uff0c\u901a\u8fc7LoRA\u5fae\u8c03\u65b9\u6cd5\u51cf\u5c11\u566a\u58f0\u4fe1\u606f\u68c0\u7d22\u548c\u5e7b\u89c9\u95ee\u9898\uff0c\u63d0\u5347\u5c0f\u8bed\u8a00\u6a21\u578b\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u5c0f\u8bed\u8a00\u6a21\u578b\u867d\u7136\u9ad8\u6548\u4e14\u80fd\u529b\u4e0d\u65ad\u589e\u957f\uff0c\u4f46\u5728\u5f15\u5165\u89c6\u89c9\u7b49\u989d\u5916\u6a21\u6001\u65f6\uff0c\u4f1a\u56e0\u4e3a\u566a\u58f0\u4fe1\u606f\u52a0\u5267\u6709\u9650\u4e0a\u4e0b\u6587\u7a97\u53e3\u7684\u6311\u6218\u3002\u73b0\u6709\u7814\u7a76\u8868\u660eTransformer\u6ce8\u610f\u529b\u673a\u5236\u7ecf\u5e38\u8fc7\u5ea6\u5173\u6ce8\u65e0\u5173\u4e0a\u4e0b\u6587\uff0c\u56e0\u6b64\u9700\u8981\u6539\u8fdb\u6ce8\u610f\u529b\u673a\u5236\u6765\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5c06\u539f\u672c\u4e3a\u7eaf\u6587\u672c\u6a21\u578b\u8bbe\u8ba1\u7684\u5dee\u5206\u6ce8\u610f\u529b\u673a\u5236\u6269\u5c55\u5230\u6587\u672c-\u89c6\u89c9\u6a21\u578bPaliGemma\u4e2d\uff0c\u4f7f\u7528LoRA\u65b9\u6cd5\u5bf9PaliGemma 3B\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u5e76\u5b9e\u9a8c\u4e86\u5404\u79cd\u53c2\u6570\u8bbe\u7f6e\u548c\u914d\u7f6e\u6765\u96c6\u6210\u5dee\u5206\u6ce8\u610f\u529b\u673a\u5236\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u5dee\u5206\u6ce8\u610f\u529b\u673a\u5236\u53ef\u4ee5\u6210\u529f\u9002\u914d\u5e76\u96c6\u6210\u5230\u73b0\u6709\u6a21\u578b\u7684\u5fae\u8c03\u8fc7\u7a0b\u4e2d\uff0c\u80fd\u591f\u589e\u5f3a\u566a\u58f0\u4fe1\u606f\u68c0\u7d22\u80fd\u529b\u548c\u95ee\u7b54\u6027\u80fd\uff0c\u6709\u6548\u51cf\u5c11\u5e7b\u89c9\u95ee\u9898\u3002", "conclusion": "\u5dee\u5206\u6ce8\u610f\u529b\u673a\u5236\u53ef\u4ee5\u6709\u6548\u5730\u5e94\u7528\u4e8e\u591a\u6a21\u6001\u6a21\u578b\u7684\u5fae\u8c03\u4e2d\uff0c\u901a\u8fc7\u6539\u5584\u6ce8\u610f\u529b\u5206\u914d\u6765\u51cf\u5c11\u5bf9\u65e0\u5173\u4fe1\u606f\u7684\u5173\u6ce8\uff0c\u4ece\u800c\u63d0\u5347\u6a21\u578b\u5728\u5904\u7406\u5305\u542b\u89c6\u89c9\u4fe1\u606f\u7684\u4efb\u52a1\u65f6\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2507.15975", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.15975", "abs": "https://arxiv.org/abs/2507.15975", "authors": ["Qiwei Du", "Bowen Li", "Yi Du", "Shaoshu Su", "Taimeng Fu", "Zitong Zhan", "Zhipeng Zhao", "Chen Wang"], "title": "Fast Task Planning with Neuro-Symbolic Relaxation", "comment": "8 pages, 6 figures", "summary": "Real-world task planning requires long-horizon reasoning over large sets of\nentities with complex relationships and attributes, leading to a combinatorial\nexplosion for classical symbolic planners. To prune the search space, recent\nmethods prioritize searching on a simplified task only containing a few\n\"important\" entities predicted by a neural network. However, such a simple\nneuro-symbolic (NeSy) integration risks omitting critical entities and wasting\nresources on unsolvable simplified tasks. To enable Fast and reliable planning,\nwe introduce a NeSy relaxation strategy (Flax), combining neural importance\nprediction with symbolic expansion. Specifically, we first learn a graph neural\nnetwork to predict entity importance to create a simplified task and solve it\nwith a symbolic planner. Then, we solve a rule-relaxed task to obtain a quick\nrough plan, and reintegrate all referenced entities into the simplified task to\nrecover any overlooked but essential elements. Finally, we apply complementary\nrules to refine the updated task, keeping it both reliable and compact.\nExtensive experiments are conducted on both synthetic and real-world maze\nnavigation benchmarks where a robot must traverse through a maze and interact\nwith movable objects. The results show that Flax boosts the average success\nrate by 20.82% and cuts mean wall-clock planning time by 17.65% compared with\nthe state-of-the-art NeSy baseline. We expect that Flax offers a practical path\ntoward fast, scalable, long-horizon task planning in complex environments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Flax\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u795e\u7ecf\u7f51\u7edc\u91cd\u8981\u6027\u9884\u6d4b\u548c\u7b26\u53f7\u6269\u5c55\u6765\u89e3\u51b3\u957f\u65f6\u57df\u4efb\u52a1\u89c4\u5212\u4e2d\u7684\u7ec4\u5408\u7206\u70b8\u95ee\u9898\uff0c\u5728\u8ff7\u5bab\u5bfc\u822a\u4efb\u52a1\u4e2d\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u5347\u4e8620.82%\u7684\u6210\u529f\u7387\u5e76\u51cf\u5c11\u4e8617.65%\u7684\u89c4\u5212\u65f6\u95f4\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u7684\u957f\u65f6\u57df\u4efb\u52a1\u89c4\u5212\u9700\u8981\u5bf9\u5927\u91cf\u5177\u6709\u590d\u6742\u5173\u7cfb\u548c\u5c5e\u6027\u7684\u5b9e\u4f53\u8fdb\u884c\u63a8\u7406\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u7ecf\u5178\u7b26\u53f7\u89c4\u5212\u5668\u7684\u7ec4\u5408\u7206\u70b8\u95ee\u9898\u3002\u73b0\u6709\u7684\u795e\u7ecf\u7b26\u53f7\u96c6\u6210\u65b9\u6cd5\u867d\u7136\u901a\u8fc7\u9884\u6d4b\"\u91cd\u8981\"\u5b9e\u4f53\u6765\u7b80\u5316\u4efb\u52a1\uff0c\u4f46\u5b58\u5728\u9057\u6f0f\u5173\u952e\u5b9e\u4f53\u548c\u5728\u4e0d\u53ef\u89e3\u7684\u7b80\u5316\u4efb\u52a1\u4e0a\u6d6a\u8d39\u8d44\u6e90\u7684\u98ce\u9669\u3002", "method": "\u63d0\u51faFlax\u795e\u7ecf\u7b26\u53f7\u677e\u5f1b\u7b56\u7565\uff0c\u5305\u542b\u4e09\u4e2a\u6b65\u9aa4\uff1a1) \u4f7f\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u5b9e\u4f53\u91cd\u8981\u6027\u5e76\u521b\u5efa\u7b80\u5316\u4efb\u52a1\uff1b2) \u89e3\u51b3\u89c4\u5219\u677e\u5f1b\u7684\u4efb\u52a1\u83b7\u5f97\u7c97\u7cd9\u89c4\u5212\uff0c\u5e76\u5c06\u6240\u6709\u5f15\u7528\u7684\u5b9e\u4f53\u91cd\u65b0\u6574\u5408\u5230\u7b80\u5316\u4efb\u52a1\u4e2d\uff1b3) \u5e94\u7528\u4e92\u8865\u89c4\u5219\u6765\u7ec6\u5316\u66f4\u65b0\u7684\u4efb\u52a1\uff0c\u4fdd\u6301\u5176\u53ef\u9760\u6027\u548c\u7d27\u51d1\u6027\u3002", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u8ff7\u5bab\u5bfc\u822a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFlax\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u795e\u7ecf\u7b26\u53f7\u57fa\u7ebf\u65b9\u6cd5\u5e73\u5747\u6210\u529f\u7387\u63d0\u5347\u4e8620.82%\uff0c\u5e73\u5747\u89c4\u5212\u65f6\u95f4\u51cf\u5c11\u4e8617.65%\u3002", "conclusion": "Flax\u4e3a\u590d\u6742\u73af\u5883\u4e2d\u7684\u5feb\u901f\u3001\u53ef\u6269\u5c55\u7684\u957f\u65f6\u57df\u4efb\u52a1\u89c4\u5212\u63d0\u4f9b\u4e86\u4e00\u6761\u5b9e\u7528\u7684\u8def\u5f84\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u795e\u7ecf\u7b26\u53f7\u96c6\u6210\u4e2d\u7684\u5173\u952e\u95ee\u9898\u3002"}}
{"id": "2507.15876", "categories": ["cs.AI", "q-fin.PR", "q-fin.ST", "q-fin.TR"], "pdf": "https://arxiv.org/pdf/2507.15876", "abs": "https://arxiv.org/abs/2507.15876", "authors": ["Eric Benhamou", "Jean-Jacques Ohana", "Alban Etienne", "B\u00e9atrice Guez", "Ethan Setrouk", "Thomas Jacquot"], "title": "Re-evaluating Short- and Long-Term Trend Factors in CTA Replication: A Bayesian Graphical Approach", "comment": "13 pages", "summary": "Commodity Trading Advisors (CTAs) have historically relied on trend-following\nrules that operate on vastly different horizons from long-term breakouts that\ncapture major directional moves to short-term momentum signals that thrive in\nfast-moving markets. Despite a large body of work on trend following, the\nrelative merits and interactions of short-versus long-term trend systems remain\ncontroversial. This paper adds to the debate by (i) dynamically decomposing CTA\nreturns into short-term trend, long-term trend and market beta factors using a\nBayesian graphical model, and (ii) showing how the blend of horizons shapes the\nstrategy's risk-adjusted performance.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u8d1d\u53f6\u65af\u56fe\u6a21\u578b\u52a8\u6001\u5206\u89e3CTA\u6536\u76ca\uff0c\u7814\u7a76\u77ed\u671f\u8d8b\u52bf\u3001\u957f\u671f\u8d8b\u52bf\u548c\u5e02\u573a\u8d1d\u5854\u56e0\u5b50\u5bf9\u7b56\u7565\u98ce\u9669\u8c03\u6574\u6536\u76ca\u7684\u5f71\u54cd", "motivation": "\u5546\u54c1\u4ea4\u6613\u987e\u95ee(CTA)\u5386\u53f2\u4e0a\u4f9d\u8d56\u4e0d\u540c\u65f6\u95f4\u8de8\u5ea6\u7684\u8d8b\u52bf\u8ddf\u8e2a\u89c4\u5219\uff0c\u4f46\u77ed\u671f\u4e0e\u957f\u671f\u8d8b\u52bf\u7cfb\u7edf\u7684\u76f8\u5bf9\u4f18\u52bf\u548c\u76f8\u4e92\u4f5c\u7528\u4ecd\u5b58\u5728\u4e89\u8bae\uff0c\u9700\u8981\u6df1\u5165\u7814\u7a76\u4e0d\u540c\u65f6\u95f4\u8de8\u5ea6\u8d8b\u52bf\u7b56\u7565\u7684\u5f71\u54cd\u673a\u5236", "method": "\u91c7\u7528\u8d1d\u53f6\u65af\u56fe\u6a21\u578b\u52a8\u6001\u5206\u89e3CTA\u6536\u76ca\uff0c\u5c06\u6536\u76ca\u5206\u89e3\u4e3a\u77ed\u671f\u8d8b\u52bf\u3001\u957f\u671f\u8d8b\u52bf\u548c\u5e02\u573a\u8d1d\u5854\u4e09\u4e2a\u56e0\u5b50\uff0c\u5e76\u5206\u6790\u4e0d\u540c\u65f6\u95f4\u8de8\u5ea6\u7ec4\u5408\u5bf9\u7b56\u7565\u8868\u73b0\u7684\u5f71\u54cd", "result": "\u901a\u8fc7\u52a8\u6001\u5206\u89e3\u65b9\u6cd5\u6210\u529f\u8bc6\u522b\u51faCTA\u6536\u76ca\u4e2d\u7684\u77ed\u671f\u8d8b\u52bf\u3001\u957f\u671f\u8d8b\u52bf\u548c\u5e02\u573a\u8d1d\u5854\u56e0\u5b50\uff0c\u5e76\u63ed\u793a\u4e86\u4e0d\u540c\u65f6\u95f4\u8de8\u5ea6\u7ec4\u5408\u5982\u4f55\u5f71\u54cd\u7b56\u7565\u7684\u98ce\u9669\u8c03\u6574\u6536\u76ca\u8868\u73b0", "conclusion": "\u4e0d\u540c\u65f6\u95f4\u8de8\u5ea6\u8d8b\u52bf\u7b56\u7565\u7684\u7ec4\u5408\u65b9\u5f0f\u663e\u8457\u5f71\u54cdCTA\u7b56\u7565\u7684\u98ce\u9669\u8c03\u6574\u6536\u76ca\uff0c\u4e3a\u7406\u89e3\u548c\u4f18\u5316CTA\u7b56\u7565\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u548c\u5b9e\u8bc1\u8bc1\u636e"}}
{"id": "2507.16000", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.16000", "abs": "https://arxiv.org/abs/2507.16000", "authors": ["Easton Potokar", "Michael Kaess"], "title": "A Comprehensive Evaluation of LiDAR Odometry Techniques", "comment": "Accepted to IROS 2025", "summary": "Light Detection and Ranging (LiDAR) sensors have become the sensor of choice\nfor many robotic state estimation tasks. Because of this, in recent years there\nhas been significant work done to fine the most accurate method to perform\nstate estimation using these sensors. In each of these prior works, an\nexplosion of possible technique combinations has occurred, with each work\ncomparing LiDAR Odometry (LO) \"pipelines\" to prior \"pipelines\". Unfortunately,\nlittle work up to this point has performed the significant amount of ablation\nstudies comparing the various building-blocks of a LO pipeline. In this work,\nwe summarize the various techniques that go into defining a LO pipeline and\nempirically evaluate these LO components on an expansive number of datasets\nacross environments, LiDAR types, and vehicle motions. Finally, we make\nempirically-backed recommendations for the design of future LO pipelines to\nprovide the most accurate and reliable performance.", "AI": {"tldr": "\u672c\u6587\u5bf9\u6fc0\u5149\u96f7\u8fbe\u91cc\u7a0b\u8ba1(LiDAR Odometry)\u7ba1\u9053\u7684\u5404\u4e2a\u7ec4\u4ef6\u8fdb\u884c\u4e86\u5168\u9762\u7684\u6d88\u878d\u7814\u7a76\u548c\u5b9e\u8bc1\u8bc4\u4f30\uff0c\u5e76\u57fa\u4e8e\u5b9e\u9a8c\u7ed3\u679c\u4e3a\u672a\u6765LO\u7ba1\u9053\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5efa\u8bae\u3002", "motivation": "\u5c3d\u7ba1\u6fc0\u5149\u96f7\u8fbe\u4f20\u611f\u5668\u5728\u673a\u5668\u4eba\u72b6\u6001\u4f30\u8ba1\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u4e4b\u524d\u7684\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u6574\u4f53\u7ba1\u9053\u6bd4\u8f83\uff0c\u7f3a\u4e4f\u5bf9LO\u7ba1\u9053\u5404\u4e2a\u6784\u5efa\u6a21\u5757\u7684\u8be6\u7ec6\u6d88\u878d\u7814\u7a76\uff0c\u5bfc\u81f4\u96be\u4ee5\u7406\u89e3\u5404\u7ec4\u4ef6\u7684\u5177\u4f53\u8d21\u732e\u3002", "method": "\u603b\u7ed3\u4e86\u5b9a\u4e49LO\u7ba1\u9053\u7684\u5404\u79cd\u6280\u672f\uff0c\u5e76\u5728\u5927\u91cf\u6570\u636e\u96c6\u4e0a\u5bf9\u8fd9\u4e9bLO\u7ec4\u4ef6\u8fdb\u884c\u5b9e\u8bc1\u8bc4\u4f30\uff0c\u6db5\u76d6\u4e0d\u540c\u73af\u5883\u3001\u6fc0\u5149\u96f7\u8fbe\u7c7b\u578b\u548c\u8f66\u8f86\u8fd0\u52a8\u6a21\u5f0f\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u6d88\u878d\u7814\u7a76\uff0c\u8bc6\u522b\u51fa\u4e86LO\u7ba1\u9053\u4e2d\u5404\u4e2a\u7ec4\u4ef6\u7684\u6027\u80fd\u8868\u73b0\uff0c\u4e3a\u4e0d\u540c\u5e94\u7528\u573a\u666f\u4e0b\u7684\u7ec4\u4ef6\u9009\u62e9\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u4f9d\u636e\u3002", "conclusion": "\u57fa\u4e8e\u5b9e\u8bc1\u7814\u7a76\u7ed3\u679c\uff0c\u4e3a\u672a\u6765LO\u7ba1\u9053\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5177\u4f53\u5efa\u8bae\uff0c\u4ee5\u5b9e\u73b0\u6700\u51c6\u786e\u548c\u53ef\u9760\u7684\u6027\u80fd\u8868\u73b0\u3002"}}
{"id": "2507.15877", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15877", "abs": "https://arxiv.org/abs/2507.15877", "authors": ["Simon Ouellette"], "title": "Out-of-Distribution Generalization in the ARC-AGI Domain: Comparing Execution-Guided Neural Program Synthesis and Test-Time Fine-Tuning", "comment": null, "summary": "We run a controlled compositional generalization experiment in the ARC-AGI\ndomain: an open-world problem domain in which the ability to generalize\nout-of-distribution is, by design, an essential characteristic for success. We\ncompare neural program synthesis and test-time fine-tuning approaches on this\nexperiment. We find that execution-guided neural program synthesis outperforms\nall reference algorithms in its ability to compose novel solutions. Our\nempirical findings also suggest that the success of TTFT on ARC-AGI lies mainly\nin eliciting in-distribution knowledge that the LLM otherwise fails to rely on\ndirectly.", "AI": {"tldr": "\u7814\u7a76\u5728ARC-AGI\u9886\u57df\u6bd4\u8f83\u4e86\u795e\u7ecf\u7a0b\u5e8f\u5408\u6210\u548c\u6d4b\u8bd5\u65f6\u5fae\u8c03\u65b9\u6cd5\u7684\u7ec4\u5408\u6cdb\u5316\u80fd\u529b\uff0c\u53d1\u73b0\u6267\u884c\u5f15\u5bfc\u7684\u795e\u7ecf\u7a0b\u5e8f\u5408\u6210\u5728\u7ec4\u5408\u65b0\u89e3\u51b3\u65b9\u6848\u65b9\u9762\u8868\u73b0\u6700\u4f73", "motivation": "\u63a2\u7d22\u5728\u5f00\u653e\u4e16\u754c\u95ee\u9898\u57df\u4e2d\u4e0d\u540c\u65b9\u6cd5\u7684\u5206\u5e03\u5916\u6cdb\u5316\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728ARC-AGI\u8fd9\u79cd\u9700\u8981\u7ec4\u5408\u6cdb\u5316\u80fd\u529b\u624d\u80fd\u6210\u529f\u7684\u4efb\u52a1\u4e2d\u6bd4\u8f83\u795e\u7ecf\u7a0b\u5e8f\u5408\u6210\u548c\u6d4b\u8bd5\u65f6\u5fae\u8c03\u7684\u6548\u679c", "method": "\u5728ARC-AGI\u9886\u57df\u8fdb\u884c\u53d7\u63a7\u7684\u7ec4\u5408\u6cdb\u5316\u5b9e\u9a8c\uff0c\u6bd4\u8f83\u795e\u7ecf\u7a0b\u5e8f\u5408\u6210\u548c\u6d4b\u8bd5\u65f6\u5fae\u8c03(TTFT)\u65b9\u6cd5\uff0c\u4f7f\u7528\u6267\u884c\u5f15\u5bfc\u7684\u795e\u7ecf\u7a0b\u5e8f\u5408\u6210\u4f5c\u4e3a\u4e3b\u8981\u7814\u7a76\u65b9\u6cd5", "result": "\u6267\u884c\u5f15\u5bfc\u7684\u795e\u7ecf\u7a0b\u5e8f\u5408\u6210\u5728\u7ec4\u5408\u65b0\u89e3\u51b3\u65b9\u6848\u7684\u80fd\u529b\u4e0a\u4f18\u4e8e\u6240\u6709\u53c2\u8003\u7b97\u6cd5\uff1b\u6d4b\u8bd5\u65f6\u5fae\u8c03\u5728ARC-AGI\u4e0a\u7684\u6210\u529f\u4e3b\u8981\u6e90\u4e8e\u6fc0\u53d1\u5927\u8bed\u8a00\u6a21\u578b\u539f\u672c\u65e0\u6cd5\u76f4\u63a5\u4f9d\u8d56\u7684\u5206\u5e03\u5185\u77e5\u8bc6", "conclusion": "\u795e\u7ecf\u7a0b\u5e8f\u5408\u6210\u65b9\u6cd5\u5728\u9700\u8981\u7ec4\u5408\u6cdb\u5316\u7684\u5f00\u653e\u4e16\u754c\u95ee\u9898\u4e2d\u8868\u73b0\u66f4\u4f18\uff0c\u800c\u6d4b\u8bd5\u65f6\u5fae\u8c03\u7684\u6548\u679c\u4e3b\u8981\u6765\u81ea\u4e8e\u66f4\u597d\u5730\u5229\u7528\u5df2\u6709\u77e5\u8bc6\u800c\u975e\u771f\u6b63\u7684\u5206\u5e03\u5916\u6cdb\u5316"}}
{"id": "2507.16034", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.16034", "abs": "https://arxiv.org/abs/2507.16034", "authors": ["Xuying Huang", "Sicong Pan", "Olga Zatsarynna", "Juergen Gall", "Maren Bennewitz"], "title": "Improved Semantic Segmentation from Ultra-Low-Resolution RGB Images Applied to Privacy-Preserving Object-Goal Navigation", "comment": "Submitted to RA-L", "summary": "User privacy in mobile robotics has become a critical concern. Existing\nmethods typically prioritize either the performance of downstream robotic tasks\nor privacy protection, with the latter often constraining the effectiveness of\ntask execution. To jointly address both objectives, we study semantic-based\nrobot navigation in an ultra-low-resolution setting to preserve visual privacy.\nA key challenge in such scenarios is recovering semantic segmentation from\nultra-low-resolution RGB images. In this work, we introduce a novel fully\njoint-learning method that integrates an agglomerative feature extractor and a\nsegmentation-aware discriminator to solve ultra-low-resolution semantic\nsegmentation, thereby enabling privacy-preserving, semantic object-goal\nnavigation. Our method outperforms different baselines on ultra-low-resolution\nsemantic segmentation and our improved segmentation results increase the\nsuccess rate of the semantic object-goal navigation in a real-world\nprivacy-constrained scenario.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u8d85\u4f4e\u5206\u8fa8\u7387\u8bbe\u7f6e\u4e0b\u7684\u9690\u79c1\u4fdd\u62a4\u8bed\u4e49\u673a\u5668\u4eba\u5bfc\u822a\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u5b66\u4e60\u7684\u7279\u5f81\u63d0\u53d6\u5668\u548c\u5206\u5272\u611f\u77e5\u5224\u522b\u5668\u6765\u89e3\u51b3\u8d85\u4f4e\u5206\u8fa8\u7387\u8bed\u4e49\u5206\u5272\u95ee\u9898\uff0c\u5728\u4fdd\u62a4\u89c6\u89c9\u9690\u79c1\u7684\u540c\u65f6\u63d0\u9ad8\u5bfc\u822a\u6210\u529f\u7387\u3002", "motivation": "\u79fb\u52a8\u673a\u5668\u4eba\u4e2d\u7684\u7528\u6237\u9690\u79c1\u4fdd\u62a4\u5df2\u6210\u4e3a\u5173\u952e\u95ee\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u8981\u4e48\u4f18\u5148\u8003\u8651\u4e0b\u6e38\u673a\u5668\u4eba\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u8981\u4e48\u4f18\u5148\u8003\u8651\u9690\u79c1\u4fdd\u62a4\uff0c\u540e\u8005\u5f80\u5f80\u4f1a\u9650\u5236\u4efb\u52a1\u6267\u884c\u7684\u6709\u6548\u6027\u3002\u9700\u8981\u5728\u8d85\u4f4e\u5206\u8fa8\u7387\u73af\u5883\u4e0b\u5b9e\u73b0\u8bed\u4e49\u673a\u5668\u4eba\u5bfc\u822a\u4ee5\u4fdd\u62a4\u89c6\u89c9\u9690\u79c1\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5b8c\u5168\u8054\u5408\u5b66\u4e60\u65b9\u6cd5\uff0c\u96c6\u6210\u4e86\u805a\u5408\u7279\u5f81\u63d0\u53d6\u5668(agglomerative feature extractor)\u548c\u5206\u5272\u611f\u77e5\u5224\u522b\u5668(segmentation-aware discriminator)\u6765\u89e3\u51b3\u8d85\u4f4e\u5206\u8fa8\u7387\u8bed\u4e49\u5206\u5272\u95ee\u9898\uff0c\u4ece\u800c\u5b9e\u73b0\u9690\u79c1\u4fdd\u62a4\u7684\u8bed\u4e49\u5bf9\u8c61\u76ee\u6807\u5bfc\u822a\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u8d85\u4f4e\u5206\u8fa8\u7387\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u4e0a\u4f18\u4e8e\u4e0d\u540c\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u6539\u8fdb\u7684\u5206\u5272\u7ed3\u679c\u63d0\u9ad8\u4e86\u5728\u771f\u5b9e\u4e16\u754c\u9690\u79c1\u7ea6\u675f\u573a\u666f\u4e0b\u8bed\u4e49\u5bf9\u8c61\u76ee\u6807\u5bfc\u822a\u7684\u6210\u529f\u7387\u3002", "conclusion": "\u901a\u8fc7\u8054\u5408\u5b66\u4e60\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u9690\u79c1\u4fdd\u62a4\u4e0e\u673a\u5668\u4eba\u5bfc\u822a\u6027\u80fd\u4e4b\u95f4\u7684\u5e73\u8861\u95ee\u9898\uff0c\u5728\u8d85\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\u4e0a\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u8bed\u4e49\u5206\u5272\u548c\u5bfc\u822a\uff0c\u4e3a\u9690\u79c1\u4fdd\u62a4\u7684\u673a\u5668\u4eba\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.15880", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15880", "abs": "https://arxiv.org/abs/2507.15880", "authors": ["Andy E. Williams"], "title": "The Recursive Coherence Principle: A Formal Constraint on Scalable Intelligence, Alignment, and Reasoning Architecture", "comment": null, "summary": "Intelligence-biological, artificial, or collective-requires structural\ncoherence across recursive reasoning processes to scale effectively. As complex\nsystems grow, coherence becomes fragile unless a higher-order structure ensures\nsemantic consistency. This paper introduces the Recursive Coherence Principle\n(RCP): a foundational constraint stating that for any reasoning system of order\nN, composed of systems operating over conceptual spaces of order N-1, semantic\ncoherence is preserved only by a recursively evaluable generalization operator\nthat spans and aligns those lower-order conceptual spaces. Crucially, this\ncoherence enables structural alignment. Without recursive coherence, no system\ncan reliably preserve goals, meanings, or reasoning consistency at scale. We\nformally define the Functional Model of Intelligence (FMI) as the only known\noperator capable of satisfying the RCP at any scale. The FMI is a minimal,\ncomposable architecture with internal functions (evaluation, modeling,\nadaptation, stability, decomposition, bridging) and external functions\n(storage, recall, System 1 and System 2 reasoning) vital for preserving\nsemantic structure across inference and coordination layers. We prove that any\nsystem lacking the FMI will experience recursive coherence breakdown as it\nscales, arguing that common AI issues like misalignment, hallucination, and\ninstability are symptoms of this structural coherence loss. Unlike other\nfoundational principles, RCP uniquely captures the internal, recursive dynamics\nneeded for coherent, alignable intelligence, modeling semantic coherence under\nrecursion. This work significantly impacts AI alignment, advocating a shift\nfrom behavioral constraints to structural coherence, and offers a pathway for\nsafely generalizable, robustly coherent AI at scale.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u9012\u5f52\u4e00\u81f4\u6027\u539f\u7406(RCP)\uff0c\u8ba4\u4e3a\u667a\u80fd\u7cfb\u7edf\u8981\u60f3\u6709\u6548\u6269\u5c55\u5fc5\u987b\u5728\u9012\u5f52\u63a8\u7406\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u7ed3\u6784\u4e00\u81f4\u6027\uff0c\u5e76\u4ecb\u7ecd\u4e86\u529f\u80fd\u667a\u80fd\u6a21\u578b(FMI)\u4f5c\u4e3a\u6ee1\u8db3\u8be5\u539f\u7406\u7684\u552f\u4e00\u5df2\u77e5\u7b97\u5b50\uff0c\u65e8\u5728\u89e3\u51b3AI\u5bf9\u9f50\u3001\u5e7b\u89c9\u548c\u4e0d\u7a33\u5b9a\u6027\u7b49\u95ee\u9898\u3002", "motivation": "\u590d\u6742\u7cfb\u7edf\u5728\u6269\u5c55\u8fc7\u7a0b\u4e2d\u9762\u4e34\u4e00\u81f4\u6027\u8106\u5f31\u7684\u95ee\u9898\uff0c\u7f3a\u4e4f\u9ad8\u9636\u7ed3\u6784\u6765\u786e\u4fdd\u8bed\u4e49\u4e00\u81f4\u6027\u3002\u73b0\u6709AI\u7cfb\u7edf\u666e\u904d\u5b58\u5728\u9519\u4f4d\u3001\u5e7b\u89c9\u548c\u4e0d\u7a33\u5b9a\u6027\u7b49\u95ee\u9898\uff0c\u8fd9\u4e9b\u90fd\u662f\u7ed3\u6784\u4e00\u81f4\u6027\u7f3a\u5931\u7684\u75c7\u72b6\u3002\u9700\u8981\u627e\u5230\u4e00\u4e2a\u80fd\u591f\u5728\u4efb\u4f55\u89c4\u6a21\u4e0b\u4fdd\u6301\u8bed\u4e49\u7ed3\u6784\u548c\u63a8\u7406\u4e00\u81f4\u6027\u7684\u57fa\u7840\u539f\u7406\u3002", "method": "1. \u6b63\u5f0f\u5b9a\u4e49\u9012\u5f52\u4e00\u81f4\u6027\u539f\u7406(RCP)\uff1a\u5bf9\u4e8e\u4efb\u4f55N\u9636\u63a8\u7406\u7cfb\u7edf\uff0c\u8bed\u4e49\u4e00\u81f4\u6027\u53ea\u80fd\u901a\u8fc7\u9012\u5f52\u53ef\u8bc4\u4f30\u7684\u6cdb\u5316\u7b97\u5b50\u6765\u4fdd\u6301\uff1b2. \u63d0\u51fa\u529f\u80fd\u667a\u80fd\u6a21\u578b(FMI)\u4f5c\u4e3a\u6ee1\u8db3RCP\u7684\u6700\u5c0f\u53ef\u7ec4\u5408\u67b6\u6784\uff1b3. FMI\u5305\u542b\u5185\u90e8\u51fd\u6570(\u8bc4\u4f30\u3001\u5efa\u6a21\u3001\u9002\u5e94\u3001\u7a33\u5b9a\u3001\u5206\u89e3\u3001\u6865\u63a5)\u548c\u5916\u90e8\u51fd\u6570(\u5b58\u50a8\u3001\u56de\u5fc6\u3001\u7cfb\u7edf1\u548c\u7cfb\u7edf2\u63a8\u7406)\uff1b4. \u901a\u8fc7\u6570\u5b66\u8bc1\u660e\u5c55\u793a\u7f3a\u4e4fFMI\u7684\u7cfb\u7edf\u5728\u6269\u5c55\u65f6\u4f1a\u51fa\u73b0\u9012\u5f52\u4e00\u81f4\u6027\u5d29\u6e83\u3002", "result": "\u8bc1\u660e\u4e86FMI\u662f\u552f\u4e00\u80fd\u591f\u5728\u4efb\u4f55\u89c4\u6a21\u4e0b\u6ee1\u8db3\u9012\u5f52\u4e00\u81f4\u6027\u539f\u7406\u7684\u5df2\u77e5\u7b97\u5b50\u3002\u7814\u7a76\u8868\u660e\uff0c\u7f3a\u4e4fFMI\u67b6\u6784\u7684\u7cfb\u7edf\u5728\u6269\u5c55\u65f6\u5fc5\u7136\u4f1a\u7ecf\u5386\u9012\u5f52\u4e00\u81f4\u6027\u5d29\u6e83\uff0c\u800c\u5e38\u89c1\u7684AI\u95ee\u9898\u5982\u9519\u4f4d\u3001\u5e7b\u89c9\u548c\u4e0d\u7a33\u5b9a\u6027\u90fd\u662f\u8fd9\u79cd\u7ed3\u6784\u4e00\u81f4\u6027\u4e27\u5931\u7684\u75c7\u72b6\u3002\u8be5\u6a21\u578b\u80fd\u591f\u5728\u63a8\u7406\u548c\u534f\u8c03\u5c42\u9762\u4fdd\u6301\u8bed\u4e49\u7ed3\u6784\u3002", "conclusion": "\u9012\u5f52\u4e00\u81f4\u6027\u539f\u7406\u4e3a\u6784\u5efa\u53ef\u9760\u3001\u53ef\u5bf9\u9f50\u7684\u667a\u80fd\u7cfb\u7edf\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u3002\u8be5\u5de5\u4f5c\u5bf9AI\u5bf9\u9f50\u9886\u57df\u4ea7\u751f\u91cd\u5927\u5f71\u54cd\uff0c\u5021\u5bfc\u4ece\u884c\u4e3a\u7ea6\u675f\u8f6c\u5411\u7ed3\u6784\u4e00\u81f4\u6027\u7684\u8303\u5f0f\u8f6c\u53d8\u3002\u901a\u8fc7\u91c7\u7528FMI\u67b6\u6784\uff0c\u53ef\u4ee5\u4e3a\u5b89\u5168\u53ef\u6cdb\u5316\u3001\u9c81\u68d2\u4e00\u81f4\u7684\u5927\u89c4\u6a21AI\u7cfb\u7edf\u63d0\u4f9b\u5b9e\u73b0\u8def\u5f84\uff0c\u4ece\u6839\u672c\u4e0a\u89e3\u51b3\u5f53\u524dAI\u7cfb\u7edf\u7684\u7ed3\u6784\u6027\u95ee\u9898\u3002"}}
{"id": "2507.16059", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.16059", "abs": "https://arxiv.org/abs/2507.16059", "authors": ["Emek Bar\u0131\u015f K\u00fc\u00e7\u00fcktabak", "Matthew R. Short", "Lorenzo Vianello", "Daniel Ludvig", "Levi Hargrove", "Kevin Lynch", "Jose Pons"], "title": "Therapist-Exoskeleton-Patient Interaction: An Immersive Gait Therapy", "comment": null, "summary": "Following a stroke, individuals often experience mobility and balance\nimpairments due to lower-limb weakness and loss of independent joint control.\nGait recovery is a key goal of rehabilitation, traditionally achieved through\nhigh-intensity therapist-led training. However, manual assistance can be\nphysically demanding and limits the therapist's ability to interact with\nmultiple joints simultaneously. Robotic exoskeletons offer multi-joint support,\nreduce therapist strain, and provide objective feedback, but current control\nstrategies often limit therapist involvement and adaptability.\n  We present a novel gait rehabilitation paradigm based on physical\nHuman-Robot-Human Interaction (pHRHI), where both the therapist and the\npost-stroke individual wear lower-limb exoskeletons virtually connected at the\nhips and knees via spring-damper elements. This enables bidirectional\ninteraction, allowing the therapist to guide movement and receive haptic\nfeedback. In a study with eight chronic stroke patients, pHRHI training\noutperformed conventional therapist-guided treadmill walking, leading to\nincreased joint range of motion, step metrics, muscle activation, and\nmotivation. These results highlight pHRHI's potential to combine robotic\nprecision with therapist intuition for improved rehabilitation outcomes.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6b65\u6001\u5eb7\u590d\u65b9\u6cd5\uff0c\u901a\u8fc7\u7269\u7406\u4eba-\u673a\u5668\u4eba-\u4eba\u4ea4\u4e92(pHRHI)\u7cfb\u7edf\uff0c\u8ba9\u6cbb\u7597\u5e08\u548c\u4e2d\u98ce\u60a3\u8005\u540c\u65f6\u7a7f\u6234\u4e0b\u80a2\u5916\u9aa8\u9abc\uff0c\u901a\u8fc7\u865a\u62df\u5f39\u7c27-\u963b\u5c3c\u5143\u4ef6\u8fde\u63a5\uff0c\u5b9e\u73b0\u53cc\u5411\u4ea4\u4e92\u6307\u5bfc\u548c\u53cd\u9988\uff0c\u57288\u540d\u6162\u6027\u4e2d\u98ce\u60a3\u8005\u7684\u7814\u7a76\u4e2d\u663e\u793a\u51fa\u4f18\u4e8e\u4f20\u7edf\u6cbb\u7597\u5e08\u6307\u5bfc\u8dd1\u6b65\u673a\u8bad\u7ec3\u7684\u6548\u679c\u3002", "motivation": "\u4e2d\u98ce\u540e\u60a3\u8005\u5e38\u51fa\u73b0\u4e0b\u80a2\u65e0\u529b\u548c\u72ec\u7acb\u5173\u8282\u63a7\u5236\u4e27\u5931\u5bfc\u81f4\u7684\u884c\u52a8\u548c\u5e73\u8861\u969c\u788d\u3002\u4f20\u7edf\u7684\u9ad8\u5f3a\u5ea6\u6cbb\u7597\u5e08\u6307\u5bfc\u8bad\u7ec3\u867d\u7136\u6709\u6548\uff0c\u4f46\u5bf9\u6cbb\u7597\u5e08\u4f53\u529b\u8981\u6c42\u9ad8\u4e14\u96be\u4ee5\u540c\u65f6\u64cd\u63a7\u591a\u4e2a\u5173\u8282\u3002\u73b0\u6709\u673a\u5668\u4eba\u5916\u9aa8\u9abc\u867d\u80fd\u63d0\u4f9b\u591a\u5173\u8282\u652f\u6301\uff0c\u4f46\u63a7\u5236\u7b56\u7565\u5f80\u5f80\u9650\u5236\u4e86\u6cbb\u7597\u5e08\u7684\u53c2\u4e0e\u5ea6\u548c\u9002\u5e94\u6027\u3002", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8e\u7269\u7406\u4eba-\u673a\u5668\u4eba-\u4eba\u4ea4\u4e92(pHRHI)\u7684\u6b65\u6001\u5eb7\u590d\u65b0\u8303\u5f0f\u3002\u6cbb\u7597\u5e08\u548c\u4e2d\u98ce\u60a3\u8005\u90fd\u7a7f\u6234\u4e0b\u80a2\u5916\u9aa8\u9abc\uff0c\u901a\u8fc7\u9acb\u5173\u8282\u548c\u819d\u5173\u8282\u7684\u865a\u62df\u5f39\u7c27-\u963b\u5c3c\u5143\u4ef6\u8fde\u63a5\uff0c\u5b9e\u73b0\u53cc\u5411\u4ea4\u4e92\uff0c\u4f7f\u6cbb\u7597\u5e08\u80fd\u591f\u6307\u5bfc\u8fd0\u52a8\u5e76\u63a5\u6536\u89e6\u89c9\u53cd\u9988\u3002", "result": "\u57288\u540d\u6162\u6027\u4e2d\u98ce\u60a3\u8005\u7684\u7814\u7a76\u4e2d\uff0cpHRHI\u8bad\u7ec3\u76f8\u6bd4\u4f20\u7edf\u6cbb\u7597\u5e08\u6307\u5bfc\u7684\u8dd1\u6b65\u673a\u884c\u8d70\u8bad\u7ec3\u8868\u73b0\u66f4\u4f73\uff0c\u5728\u5173\u8282\u6d3b\u52a8\u8303\u56f4\u3001\u6b65\u6001\u6307\u6807\u3001\u808c\u8089\u6fc0\u6d3b\u548c\u60a3\u8005\u79ef\u6781\u6027\u65b9\u9762\u90fd\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "pHRHI\u7cfb\u7edf\u6210\u529f\u7ed3\u5408\u4e86\u673a\u5668\u4eba\u7684\u7cbe\u786e\u6027\u548c\u6cbb\u7597\u5e08\u7684\u76f4\u89c9\u5224\u65ad\uff0c\u4e3a\u6539\u5584\u5eb7\u590d\u6548\u679c\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\uff0c\u5c55\u73b0\u4e86\u5728\u4e2d\u98ce\u540e\u6b65\u6001\u5eb7\u590d\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.15885", "categories": ["cs.AI", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15885", "abs": "https://arxiv.org/abs/2507.15885", "authors": ["Pierluca D'Oro", "Caley Drooff", "Joy Chen", "Joseph Tighe"], "title": "ADEPTS: A Capability Framework for Human-Centered Agent Design", "comment": null, "summary": "Large language models have paved the way to powerful and flexible AI agents,\nassisting humans by increasingly integrating into their daily life. This\nflexibility, potential, and growing adoption demands a holistic and\ncross-disciplinary approach to developing, monitoring and discussing the\ncapabilities required for agent-driven user experiences. However, current\nguidance on human-centered AI agent development is scattered: UX heuristics\nfocus on interface behaviors, engineering taxonomies describe internal\npipelines, and ethics checklists address high-level governance. There is no\nconcise, user-facing vocabulary that tells teams what an agent should\nfundamentally be able to do. We introduce ADEPTS, a capability framework\ndefining a set of core user-facing capabilities to provide unified guidance\naround the development of AI agents. ADEPTS is based on six principles for\nhuman-centered agent design, that express the minimal, user-facing capabilities\nan AI agent should demonstrate to be understandable, controllable and\ntrustworthy in everyday use. ADEPTS complements existing frameworks and\ntaxonomies; differently from them, it sits at the interface between technical\nand experience development. By presenting ADEPTS, we aim to condense complex\nAI-UX requirements into a compact framework that is actionable guidance for AI\nresearchers, designers, engineers, and policy reviewers alike. We believe\nADEPTS has the potential of accelerating the improvement of user-relevant agent\ncapabilities, of easing the design of experiences that take advantage of those\ncapabilities, and of providing a shared language to track and discuss progress\naround the development of AI agents.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86ADEPTS\u6846\u67b6\uff0c\u8fd9\u662f\u4e00\u4e2a\u9762\u5411\u7528\u6237\u7684AI\u667a\u80fd\u4f53\u80fd\u529b\u6846\u67b6\uff0c\u65e8\u5728\u4e3aAI\u667a\u80fd\u4f53\u5f00\u53d1\u63d0\u4f9b\u7edf\u4e00\u6307\u5bfc\uff0c\u4f7f\u5176\u5728\u65e5\u5e38\u4f7f\u7528\u4e2d\u66f4\u52a0\u53ef\u7406\u89e3\u3001\u53ef\u63a7\u5236\u548c\u53ef\u4fe1\u4efb\u3002", "motivation": "\u5f53\u524dAI\u667a\u80fd\u4f53\u5f00\u53d1\u6307\u5bfc\u5206\u6563\u4e14\u7f3a\u4e4f\u7edf\u4e00\u6027\uff1aUX\u542f\u53d1\u5f0f\u5173\u6ce8\u754c\u9762\u884c\u4e3a\uff0c\u5de5\u7a0b\u5206\u7c7b\u6cd5\u63cf\u8ff0\u5185\u90e8\u7ba1\u9053\uff0c\u4f26\u7406\u68c0\u67e5\u8868\u6d89\u53ca\u9ad8\u5c42\u6cbb\u7406\uff0c\u4f46\u7f3a\u4e4f\u7b80\u6d01\u7684\u9762\u5411\u7528\u6237\u7684\u8bcd\u6c47\u6765\u5b9a\u4e49\u667a\u80fd\u4f53\u5e94\u5177\u5907\u7684\u57fa\u672c\u80fd\u529b\u3002\u9700\u8981\u4e00\u4e2a\u8de8\u5b66\u79d1\u7684\u6574\u4f53\u65b9\u6cd5\u6765\u5f00\u53d1\u3001\u76d1\u63a7\u548c\u8ba8\u8bba\u667a\u80fd\u4f53\u9a71\u52a8\u7684\u7528\u6237\u4f53\u9a8c\u6240\u9700\u7684\u80fd\u529b\u3002", "method": "\u57fa\u4e8e\u516d\u4e2a\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u667a\u80fd\u4f53\u8bbe\u8ba1\u539f\u5219\uff0c\u5f00\u53d1\u4e86ADEPTS\u80fd\u529b\u6846\u67b6\u3002\u8be5\u6846\u67b6\u5b9a\u4e49\u4e86\u4e00\u5957\u6838\u5fc3\u7684\u9762\u5411\u7528\u6237\u7684\u80fd\u529b\uff0c\u4f4d\u4e8e\u6280\u672f\u5f00\u53d1\u548c\u4f53\u9a8c\u5f00\u53d1\u7684\u63a5\u53e3\u5904\uff0c\u8865\u5145\u73b0\u6709\u6846\u67b6\u548c\u5206\u7c7b\u6cd5\uff0c\u4e3aAI\u7814\u7a76\u8005\u3001\u8bbe\u8ba1\u5e08\u3001\u5de5\u7a0b\u5e08\u548c\u653f\u7b56\u5ba1\u67e5\u8005\u63d0\u4f9b\u53ef\u64cd\u4f5c\u7684\u6307\u5bfc\u3002", "result": "ADEPTS\u6846\u67b6\u6210\u529f\u5c06\u590d\u6742\u7684AI-UX\u9700\u6c42\u6d53\u7f29\u4e3a\u4e00\u4e2a\u7d27\u51d1\u7684\u6846\u67b6\uff0c\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u6307\u5bfc\u8bed\u8a00\uff0c\u5e2e\u52a9\u56e2\u961f\u660e\u786eAI\u667a\u80fd\u4f53\u5e94\u5177\u5907\u7684\u57fa\u672c\u80fd\u529b\uff0c\u4f7f\u667a\u80fd\u4f53\u5728\u65e5\u5e38\u4f7f\u7528\u4e2d\u8868\u73b0\u51fa\u53ef\u7406\u89e3\u6027\u3001\u53ef\u63a7\u5236\u6027\u548c\u53ef\u4fe1\u4efb\u6027\u3002", "conclusion": "ADEPTS\u6846\u67b6\u6709\u6f5c\u529b\u52a0\u901f\u7528\u6237\u76f8\u5173\u667a\u80fd\u4f53\u80fd\u529b\u7684\u6539\u8fdb\uff0c\u7b80\u5316\u5229\u7528\u8fd9\u4e9b\u80fd\u529b\u7684\u4f53\u9a8c\u8bbe\u8ba1\uff0c\u5e76\u4e3a\u8ddf\u8e2a\u548c\u8ba8\u8bbaAI\u667a\u80fd\u4f53\u53d1\u5c55\u8fdb\u5c55\u63d0\u4f9b\u5171\u540c\u8bed\u8a00\u3002\u8be5\u6846\u67b6\u4e3aAI\u667a\u80fd\u4f53\u7684\u4eba\u673a\u4ea4\u4e92\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u8df5\u6307\u5bfc\u3002"}}
{"id": "2507.16068", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.16068", "abs": "https://arxiv.org/abs/2507.16068", "authors": ["Zhehui Huang", "Guangyao Shi", "Yuwei Wu", "Vijay Kumar", "Gaurav S. Sukhatme"], "title": "Compositional Coordination for Multi-Robot Teams with Large Language Models", "comment": "9 pages, 4 figures", "summary": "Multi-robot coordination has traditionally relied on a task-specific and\nexpert-driven pipeline, where natural language mission descriptions are\nmanually translated by domain experts into mathematical formulation, algorithm\ndesign, and executable code. This conventional process is labor-intensive,\ninaccessible to non-experts, and inflexible to changes in mission requirements.\nHere, we propose LAN2CB (Language to Collective Behavior), a novel framework\nthat leverages large language models (LLMs) to streamline and generalize the\nmulti-robot coordination pipeline. LAN2CB directly converts natural language\nmission descriptions into executable Python code for multi-robot systems\nthrough two key components: (1) Mission Decomposition for Task Representation,\nwhich parses the mission into a task graph with dependencies, and (2) Code\nGeneration, which uses the task graph and a structured knowledge base to\ngenerate deployable robot control code. We further introduce a dataset of\nnatural language mission specifications to support development and\nbenchmarking. Experimental results in both simulation and real-world settings\nshow that LAN2CB enables effective and flexible multi-robot coordination from\nnatural language, significantly reducing the need for manual engineering while\nsupporting generalization across mission types. Website:\nhttps://sites.google.com/view/lan2cb.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86LAN2CB\u6846\u67b6\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u76f4\u63a5\u5c06\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u63cf\u8ff0\u8f6c\u6362\u4e3a\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u53ef\u6267\u884cPython\u4ee3\u7801\uff0c\u7b80\u5316\u4e86\u4f20\u7edf\u7684\u4e13\u5bb6\u9a71\u52a8\u534f\u8c03\u6d41\u6c34\u7ebf", "motivation": "\u4f20\u7edf\u591a\u673a\u5668\u4eba\u534f\u8c03\u4f9d\u8d56\u4efb\u52a1\u7279\u5b9a\u7684\u4e13\u5bb6\u9a71\u52a8\u6d41\u6c34\u7ebf\uff0c\u9700\u8981\u9886\u57df\u4e13\u5bb6\u624b\u52a8\u5c06\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u63cf\u8ff0\u8f6c\u6362\u4e3a\u6570\u5b66\u516c\u5f0f\u3001\u7b97\u6cd5\u8bbe\u8ba1\u548c\u53ef\u6267\u884c\u4ee3\u7801\uff0c\u8fd9\u4e2a\u8fc7\u7a0b\u52b3\u52a8\u5bc6\u96c6\u3001\u975e\u4e13\u5bb6\u96be\u4ee5\u63a5\u89e6\u4e14\u5bf9\u4efb\u52a1\u9700\u6c42\u53d8\u5316\u7f3a\u4e4f\u7075\u6d3b\u6027", "method": "LAN2CB\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a(1)\u4efb\u52a1\u5206\u89e3\u6a21\u5757\uff0c\u5c06\u4efb\u52a1\u89e3\u6790\u4e3a\u5e26\u4f9d\u8d56\u5173\u7cfb\u7684\u4efb\u52a1\u56fe\uff1b(2)\u4ee3\u7801\u751f\u6210\u6a21\u5757\uff0c\u4f7f\u7528\u4efb\u52a1\u56fe\u548c\u7ed3\u6784\u5316\u77e5\u8bc6\u5e93\u751f\u6210\u53ef\u90e8\u7f72\u7684\u673a\u5668\u4eba\u63a7\u5236\u4ee3\u7801\u3002\u6b64\u5916\u8fd8\u5f15\u5165\u4e86\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u89c4\u8303\u6570\u636e\u96c6\u7528\u4e8e\u5f00\u53d1\u548c\u57fa\u51c6\u6d4b\u8bd5", "result": "\u5728\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cLAN2CB\u80fd\u591f\u4ece\u81ea\u7136\u8bed\u8a00\u5b9e\u73b0\u6709\u6548\u4e14\u7075\u6d3b\u7684\u591a\u673a\u5668\u4eba\u534f\u8c03\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u624b\u52a8\u5de5\u7a0b\u7684\u9700\u6c42\uff0c\u540c\u65f6\u652f\u6301\u8de8\u4efb\u52a1\u7c7b\u578b\u7684\u6cdb\u5316", "conclusion": "LAN2CB\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u4ece\u81ea\u7136\u8bed\u8a00\u5230\u591a\u673a\u5668\u4eba\u534f\u8c03\u7684\u76f4\u63a5\u8f6c\u6362\uff0c\u4e3a\u591a\u673a\u5668\u4eba\u7cfb\u7edf\u63d0\u4f9b\u4e86\u66f4\u52a0\u901a\u7528\u3001\u7075\u6d3b\u548c\u6613\u4e8e\u4f7f\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u964d\u4f4e\u4e86\u6280\u672f\u95e8\u69db\u5e76\u63d0\u9ad8\u4e86\u7cfb\u7edf\u7684\u9002\u5e94\u6027"}}
{"id": "2507.15895", "categories": ["cs.AI", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.15895", "abs": "https://arxiv.org/abs/2507.15895", "authors": ["Lisa Dargasz"], "title": "Integrating Reason-Based Moral Decision-Making in the Reinforcement Learning Architecture", "comment": "Master's thesis, April 2025, 122 pages", "summary": "Reinforcement Learning is a machine learning methodology that has\ndemonstrated strong performance across a variety of tasks. In particular, it\nplays a central role in the development of artificial autonomous agents. As\nthese agents become increasingly capable, market readiness is rapidly\napproaching, which means those agents, for example taking the form of humanoid\nrobots or autonomous cars, are poised to transition from laboratory prototypes\nto autonomous operation in real-world environments. This transition raises\nconcerns leading to specific requirements for these systems - among them, the\nrequirement that they are designed to behave ethically. Crucially, research\ndirected toward building agents that fulfill the requirement to behave\nethically - referred to as artificial moral agents(AMAs) - has to address a\nrange of challenges at the intersection of computer science and philosophy.\nThis study explores the development of reason-based artificial moral agents\n(RBAMAs). RBAMAs are build on an extension of the reinforcement learning\narchitecture to enable moral decision-making based on sound normative\nreasoning, which is achieved by equipping the agent with the capacity to learn\na reason-theory - a theory which enables it to process morally relevant\npropositions to derive moral obligations - through case-based feedback. They\nare designed such that they adapt their behavior to ensure conformance to these\nobligations while they pursue their designated tasks. These features contribute\nto the moral justifiability of the their actions, their moral robustness, and\ntheir moral trustworthiness, which proposes the extended architecture as a\nconcrete and deployable framework for the development of AMAs that fulfills key\nethical desiderata. This study presents a first implementation of an RBAMA and\ndemonstrates the potential of RBAMAs in initial experiments.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u57fa\u4e8e\u63a8\u7406\u7684\u4eba\u5de5\u9053\u5fb7\u667a\u80fd\u4f53(RBAMAs)\uff0c\u901a\u8fc7\u6269\u5c55\u5f3a\u5316\u5b66\u4e60\u67b6\u6784\u6765\u5b9e\u73b0\u57fa\u4e8e\u89c4\u8303\u63a8\u7406\u7684\u9053\u5fb7\u51b3\u7b56\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u5b66\u4e60\u63a8\u7406\u7406\u8bba\u5e76\u5728\u6267\u884c\u4efb\u52a1\u65f6\u9075\u5b88\u9053\u5fb7\u4e49\u52a1\u3002", "motivation": "\u968f\u7740\u4eba\u5de5\u667a\u80fd\u81ea\u4e3b\u667a\u80fd\u4f53(\u5982\u4eba\u5f62\u673a\u5668\u4eba\u3001\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66)\u5373\u5c06\u4ece\u5b9e\u9a8c\u5ba4\u539f\u578b\u8f6c\u5411\u771f\u5b9e\u4e16\u754c\u5e94\u7528\uff0c\u8feb\u5207\u9700\u8981\u786e\u4fdd\u8fd9\u4e9b\u7cfb\u7edf\u80fd\u591f\u9053\u5fb7\u5730\u884c\u4e3a\u3002\u5f53\u524d\u7f3a\u4e4f\u4e00\u4e2a\u5177\u4f53\u53ef\u90e8\u7f72\u7684\u6846\u67b6\u6765\u5f00\u53d1\u6ee1\u8db3\u5173\u952e\u9053\u5fb7\u8981\u6c42\u7684\u4eba\u5de5\u9053\u5fb7\u667a\u80fd\u4f53\u3002", "method": "\u901a\u8fc7\u6269\u5c55\u5f3a\u5316\u5b66\u4e60\u67b6\u6784\u6784\u5efa\u57fa\u4e8e\u63a8\u7406\u7684\u4eba\u5de5\u9053\u5fb7\u667a\u80fd\u4f53(RBAMAs)\u3002\u8be5\u65b9\u6cd5\u4e3a\u667a\u80fd\u4f53\u914d\u5907\u5b66\u4e60\u63a8\u7406\u7406\u8bba\u7684\u80fd\u529b\uff0c\u4f7f\u5176\u80fd\u591f\u5904\u7406\u9053\u5fb7\u76f8\u5173\u547d\u9898\u5e76\u63a8\u5bfc\u9053\u5fb7\u4e49\u52a1\uff0c\u901a\u8fc7\u57fa\u4e8e\u6848\u4f8b\u7684\u53cd\u9988\u8fdb\u884c\u5b66\u4e60\uff0c\u5e76\u5728\u6267\u884c\u6307\u5b9a\u4efb\u52a1\u65f6\u8c03\u6574\u884c\u4e3a\u4ee5\u786e\u4fdd\u7b26\u5408\u9053\u5fb7\u4e49\u52a1\u3002", "result": "\u7814\u7a76\u9996\u6b21\u5b9e\u73b0\u4e86RBAMA\u5e76\u901a\u8fc7\u521d\u6b65\u5b9e\u9a8c\u5c55\u793a\u4e86\u5176\u6f5c\u529b\u3002\u8be5\u67b6\u6784\u80fd\u591f\u589e\u5f3a\u667a\u80fd\u4f53\u884c\u4e3a\u7684\u9053\u5fb7\u6b63\u5f53\u6027\u3001\u9053\u5fb7\u7a33\u5065\u6027\u548c\u9053\u5fb7\u53ef\u4fe1\u6027\u3002", "conclusion": "RBAMAs\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5177\u4f53\u53ef\u90e8\u7f72\u7684\u6846\u67b6\u7528\u4e8e\u5f00\u53d1\u6ee1\u8db3\u5173\u952e\u9053\u5fb7\u8981\u6c42\u7684\u4eba\u5de5\u9053\u5fb7\u667a\u80fd\u4f53\u3002\u8be5\u6269\u5c55\u67b6\u6784\u901a\u8fc7\u57fa\u4e8e\u58f0\u97f3\u89c4\u8303\u63a8\u7406\u7684\u9053\u5fb7\u51b3\u7b56\u673a\u5236\uff0c\u4e3a\u89e3\u51b3\u8ba1\u7b97\u673a\u79d1\u5b66\u4e0e\u54f2\u5b66\u4ea4\u53c9\u9886\u57df\u7684\u6311\u6218\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002"}}
{"id": "2507.16120", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.16120", "abs": "https://arxiv.org/abs/2507.16120", "authors": ["Shanshan Zhang", "Qi Zhang", "Siyue Wang", "Tianshui Wen", "Ziheng Zhou", "Lingxiang Zheng", "Yu Yang"], "title": "FTIN: Frequency-Time Integration Network for Inertial Odometry", "comment": null, "summary": "In recent years, machine learning has achieved significant advancements in\ninertial odometry. However, most existing inertial odometry methods primarily\nrely on CNNs in the time domain. These methods often struggle to capture\nlong-term dependency in inertial measurement unit data, thereby constraining\nthe potential for further improvements in localization accuracy. To address\nthese issues, we propose a novel network architecture that integrates both\nfrequency-domain and time-domain information. Specifically, we leverage the\nglobal view and energy compaction properties of frequency-domain learning to\neffectively model long-term dependency and reduce redundancy in IMU data.\nAdditionally, we introduce a Scalar LSTM to capture sequential dependencies in\nthe time domain, enabling cross-domain information fusion and providing a\nstable and reliable reference for localization. Experimental evaluations on\nmultiple public datasets (e.g., RIDI, RoNIN, OxIOD, RNIN, TLIO, and IMUNet)\ndemonstrate the effectiveness of the proposed frequency-time domain fusion\nstrategy. Notably, on the RoNIN dataset, our method achieves a 43.0% reduction\nin absolute trajectory error and a 13.1% reduction in relative trajectory error\ncompared to RoNIN ResNet.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u878d\u5408\u9891\u57df\u548c\u65f6\u57df\u4fe1\u606f\u7684\u65b0\u578b\u7f51\u7edc\u67b6\u6784\u7528\u4e8e\u60ef\u6027\u91cc\u7a0b\u8ba1\uff0c\u901a\u8fc7\u9891\u57df\u5b66\u4e60\u5efa\u6a21\u957f\u671f\u4f9d\u8d56\u5173\u7cfb\u5e76\u7ed3\u5408Scalar LSTM\u6355\u83b7\u65f6\u5e8f\u4f9d\u8d56\uff0c\u5728\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u663e\u8457\u63d0\u5347\u4e86\u5b9a\u4f4d\u7cbe\u5ea6", "motivation": "\u73b0\u6709\u7684\u60ef\u6027\u91cc\u7a0b\u8ba1\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u65f6\u57dfCNN\uff0c\u96be\u4ee5\u6355\u83b7IMU\u6570\u636e\u4e2d\u7684\u957f\u671f\u4f9d\u8d56\u5173\u7cfb\uff0c\u9650\u5236\u4e86\u5b9a\u4f4d\u7cbe\u5ea6\u7684\u8fdb\u4e00\u6b65\u63d0\u5347", "method": "\u63d0\u51fa\u878d\u5408\u9891\u57df\u548c\u65f6\u57df\u4fe1\u606f\u7684\u7f51\u7edc\u67b6\u6784\uff1a\u5229\u7528\u9891\u57df\u5b66\u4e60\u7684\u5168\u5c40\u89c6\u89d2\u548c\u80fd\u91cf\u538b\u7f29\u7279\u6027\u5efa\u6a21\u957f\u671f\u4f9d\u8d56\u5e76\u51cf\u5c11IMU\u6570\u636e\u5197\u4f59\uff1b\u5f15\u5165Scalar LSTM\u6355\u83b7\u65f6\u57df\u5e8f\u5217\u4f9d\u8d56\uff1b\u5b9e\u73b0\u8de8\u57df\u4fe1\u606f\u878d\u5408\u4e3a\u5b9a\u4f4d\u63d0\u4f9b\u7a33\u5b9a\u53ef\u9760\u7684\u53c2\u8003", "result": "\u5728\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\uff08RIDI\u3001RoNIN\u3001OxIOD\u3001RNIN\u3001TLIO\u3001IMUNet\uff09\u4e0a\u9a8c\u8bc1\u4e86\u9891\u57df-\u65f6\u57df\u878d\u5408\u7b56\u7565\u7684\u6709\u6548\u6027\uff1b\u5728RoNIN\u6570\u636e\u96c6\u4e0a\u76f8\u6bd4RoNIN ResNet\u5b9e\u73b0\u4e8643.0%\u7684\u7edd\u5bf9\u8f68\u8ff9\u8bef\u5dee\u964d\u4f4e\u548c13.1%\u7684\u76f8\u5bf9\u8f68\u8ff9\u8bef\u5dee\u964d\u4f4e", "conclusion": "\u9891\u57df-\u65f6\u57df\u878d\u5408\u7684\u7f51\u7edc\u67b6\u6784\u80fd\u591f\u6709\u6548\u89e3\u51b3\u73b0\u6709\u60ef\u6027\u91cc\u7a0b\u8ba1\u65b9\u6cd5\u5728\u957f\u671f\u4f9d\u8d56\u5efa\u6a21\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u663e\u8457\u63d0\u5347\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u4e3a\u60ef\u6027\u5bfc\u822a\u7cfb\u7edf\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u7684\u6280\u672f\u8def\u5f84"}}
{"id": "2507.15901", "categories": ["cs.AI", "cs.CY", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.15901", "abs": "https://arxiv.org/abs/2507.15901", "authors": ["Joydeep Chandra", "Satyam Kumar Navneet"], "title": "Advancing Responsible Innovation in Agentic AI: A study of Ethical Frameworks for Household Automation", "comment": null, "summary": "The implementation of Artificial Intelligence (AI) in household environments,\nespecially in the form of proactive autonomous agents, brings about\npossibilities of comfort and attention as well as it comes with intra or\nextramural ethical challenges. This article analyzes agentic AI and its\napplications, focusing on its move from reactive to proactive autonomy,\nprivacy, fairness and user control. We review responsible innovation\nframeworks, human-centered design principles, and governance practices to\ndistill practical guidance for ethical smart home systems. Vulnerable user\ngroups such as elderly individuals, children, and neurodivergent who face\nhigher risks of surveillance, bias, and privacy risks were studied in detail in\ncontext of Agentic AI. Design imperatives are highlighted such as tailored\nexplainability, granular consent mechanisms, and robust override controls,\nsupported by participatory and inclusive methodologies. It was also explored\nhow data-driven insights, including social media analysis via Natural Language\nProcessing(NLP), can inform specific user needs and ethical concerns. This\nsurvey aims to provide both a conceptual foundation and suggestions for\ndeveloping transparent, inclusive, and trustworthy agentic AI in household\nautomation.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u5bb6\u5ead\u73af\u5883\u4e2d\u7684\u4e3b\u52a8\u667a\u80fd\u4ee3\u7406\u7cfb\u7edf\uff0c\u63a2\u8ba8\u4e86\u4ece\u53cd\u5e94\u5f0f\u5411\u4e3b\u52a8\u5f0f\u81ea\u4e3b\u8f6c\u53d8\u6240\u5e26\u6765\u7684\u9690\u79c1\u3001\u516c\u5e73\u6027\u548c\u7528\u6237\u63a7\u5236\u7b49\u4f26\u7406\u6311\u6218\uff0c\u5e76\u4e3a\u5f00\u53d1\u900f\u660e\u3001\u5305\u5bb9\u3001\u53ef\u4fe1\u8d56\u7684\u5bb6\u5ead\u81ea\u52a8\u5316\u667a\u80fd\u4ee3\u7406\u63d0\u4f9b\u4e86\u6982\u5ff5\u57fa\u7840\u548c\u5b9e\u7528\u5efa\u8bae\u3002", "motivation": "\u968f\u7740\u4eba\u5de5\u667a\u80fd\u5728\u5bb6\u5ead\u73af\u5883\u4e2d\u7684\u5e94\u7528\u65e5\u76ca\u666e\u53ca\uff0c\u7279\u522b\u662f\u4e3b\u52a8\u81ea\u4e3b\u4ee3\u7406\u7684\u51fa\u73b0\uff0c\u867d\u7136\u5e26\u6765\u4e86\u8212\u9002\u548c\u4fbf\u5229\uff0c\u4f46\u540c\u65f6\u4e5f\u4ea7\u751f\u4e86\u91cd\u8981\u7684\u4f26\u7406\u6311\u6218\u3002\u7279\u522b\u662f\u5bf9\u4e8e\u8001\u5e74\u4eba\u3001\u513f\u7ae5\u548c\u795e\u7ecf\u591a\u6837\u6027\u7fa4\u4f53\u7b49\u8106\u5f31\u7528\u6237\u7fa4\u4f53\uff0c\u9762\u4e34\u7740\u66f4\u9ad8\u7684\u76d1\u63a7\u3001\u504f\u89c1\u548c\u9690\u79c1\u98ce\u9669\uff0c\u8feb\u5207\u9700\u8981\u5efa\u7acb\u8d1f\u8d23\u4efb\u7684\u521b\u65b0\u6846\u67b6\u548c\u6cbb\u7406\u5b9e\u8df5\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e86\u7efc\u5408\u6027\u7684\u5206\u6790\u65b9\u6cd5\uff0c\u5305\u62ec\uff1a(1)\u5ba1\u67e5\u8d1f\u8d23\u4efb\u521b\u65b0\u6846\u67b6\uff1b(2)\u5e94\u7528\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u8bbe\u8ba1\u539f\u5219\uff1b(3)\u5206\u6790\u6cbb\u7406\u5b9e\u8df5\uff1b(4)\u5229\u7528\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\u8fdb\u884c\u793e\u4ea4\u5a92\u4f53\u5206\u6790\uff1b(5)\u91c7\u7528\u53c2\u4e0e\u5f0f\u548c\u5305\u5bb9\u6027\u65b9\u6cd5\u8bba\uff1b(6)\u91cd\u70b9\u7814\u7a76\u8106\u5f31\u7528\u6237\u7fa4\u4f53\u7684\u7279\u6b8a\u9700\u6c42\u548c\u98ce\u9669\u3002", "result": "\u7814\u7a76\u63d0\u51fa\u4e86\u667a\u80fd\u5bb6\u5c45\u7cfb\u7edf\u7684\u5173\u952e\u8bbe\u8ba1\u8981\u6c42\uff0c\u5305\u62ec\uff1a\u5b9a\u5236\u5316\u7684\u53ef\u89e3\u91ca\u6027\u673a\u5236\u3001\u7ec6\u7c92\u5ea6\u7684\u540c\u610f\u673a\u5236\u3001\u5f3a\u5927\u7684\u8986\u76d6\u63a7\u5236\u529f\u80fd\u3002\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u7684\u6d1e\u5bdf\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5206\u6790\uff0c\u8bc6\u522b\u4e86\u7279\u5b9a\u7528\u6237\u9700\u6c42\u548c\u4f26\u7406\u5173\u5207\u3002\u4e3a\u8106\u5f31\u7528\u6237\u7fa4\u4f53\u63d0\u4f9b\u4e86\u9488\u5bf9\u6027\u7684\u4fdd\u62a4\u63aa\u65bd\u548c\u8bbe\u8ba1\u5efa\u8bae\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u5f00\u53d1\u900f\u660e\u3001\u5305\u5bb9\u548c\u53ef\u4fe1\u8d56\u7684\u5bb6\u5ead\u81ea\u52a8\u5316\u667a\u80fd\u4ee3\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u6982\u5ff5\u57fa\u7840\u548c\u5b9e\u8df5\u6307\u5bfc\u3002\u5f3a\u8c03\u4e86\u5728\u667a\u80fd\u5bb6\u5c45\u53d1\u5c55\u4e2d\u5fc5\u987b\u4f18\u5148\u8003\u8651\u7528\u6237\u9690\u79c1\u3001\u516c\u5e73\u6027\u548c\u81ea\u4e3b\u63a7\u5236\u6743\uff0c\u7279\u522b\u662f\u8981\u5173\u6ce8\u548c\u4fdd\u62a4\u8106\u5f31\u7528\u6237\u7fa4\u4f53\u7684\u6743\u76ca\uff0c\u4ee5\u5b9e\u73b0\u6280\u672f\u521b\u65b0\u4e0e\u4f26\u7406\u8d23\u4efb\u7684\u5e73\u8861\u3002"}}
{"id": "2507.16121", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.16121", "abs": "https://arxiv.org/abs/2507.16121", "authors": ["Shanshan Zhang", "Qi Zhang", "Siyue Wang", "Tianshui Wen", "Ziheng Zhou", "Lingxiang Zheng", "Yu Yang"], "title": "DWSFormer: A Lightweight Inertial Odometry Network for Complex Motion Modeling", "comment": null, "summary": "Inertial odometry (IO) directly estimates the position of a carrier from\ninertial sensor measurements and serves as a core technology for the widespread\ndeployment of consumer grade localization systems. While existing IO methods\ncan accurately reconstruct simple and near linear motion trajectories, they\noften fail to account for drift errors caused by complex motion patterns such\nas turning. This limitation significantly degrades localization accuracy and\nrestricts the applicability of IO systems in real world scenarios. To address\nthese challenges, we propose a lightweight IO framework. Specifically, inertial\ndata is projected into a high dimensional implicit nonlinear feature space\nusing the Star Operation method, enabling the extraction of complex motion\nfeatures that are typically overlooked. We further introduce a collaborative\nattention mechanism that jointly models global motion dynamics across both\nchannel and temporal dimensions. In addition, we design Multi Scale Gated\nConvolution Units to capture fine grained dynamic variations throughout the\nmotion process, thereby enhancing the model's ability to learn rich and\nexpressive motion representations. Extensive experiments demonstrate that our\nproposed method consistently outperforms SOTA baselines across six widely used\ninertial datasets. Compared to baseline models on the RoNIN dataset, it\nachieves reductions in ATE ranging from 2.26% to 65.78%, thereby establishing a\nnew benchmark in the field.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u60ef\u6027\u91cc\u7a0b\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7Star Operation\u65b9\u6cd5\u5c06\u60ef\u6027\u6570\u636e\u6295\u5f71\u5230\u9ad8\u7ef4\u9690\u5f0f\u975e\u7ebf\u6027\u7279\u5f81\u7a7a\u95f4\uff0c\u7ed3\u5408\u534f\u540c\u6ce8\u610f\u529b\u673a\u5236\u548c\u591a\u5c3a\u5ea6\u95e8\u63a7\u5377\u79ef\u5355\u5143\uff0c\u663e\u8457\u63d0\u5347\u4e86\u590d\u6742\u8fd0\u52a8\u6a21\u5f0f\u4e0b\u7684\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u5728RoNIN\u6570\u636e\u96c6\u4e0aATE\u8bef\u5dee\u964d\u4f4e2.26%-65.78%\u3002", "motivation": "\u73b0\u6709\u60ef\u6027\u91cc\u7a0b\u8ba1\u65b9\u6cd5\u5728\u7b80\u5355\u7ebf\u6027\u8fd0\u52a8\u8f68\u8ff9\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u8f6c\u5f2f\u7b49\u590d\u6742\u8fd0\u52a8\u6a21\u5f0f\u4e0b\u4f1a\u4ea7\u751f\u6f02\u79fb\u8bef\u5dee\uff0c\u4e25\u91cd\u5f71\u54cd\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u9650\u5236\u4e86\u6d88\u8d39\u7ea7\u5b9a\u4f4d\u7cfb\u7edf\u7684\u5b9e\u9645\u5e94\u7528\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5904\u7406\u590d\u6742\u8fd0\u52a8\u6a21\u5f0f\u7684\u60ef\u6027\u91cc\u7a0b\u8ba1\u6280\u672f\u3002", "method": "\u4f7f\u7528Star Operation\u65b9\u6cd5\u5c06\u60ef\u6027\u6570\u636e\u6295\u5f71\u5230\u9ad8\u7ef4\u9690\u5f0f\u975e\u7ebf\u6027\u7279\u5f81\u7a7a\u95f4\u6765\u63d0\u53d6\u590d\u6742\u8fd0\u52a8\u7279\u5f81\uff1b\u5f15\u5165\u534f\u540c\u6ce8\u610f\u529b\u673a\u5236\u5bf9\u901a\u9053\u548c\u65f6\u95f4\u7ef4\u5ea6\u7684\u5168\u5c40\u8fd0\u52a8\u52a8\u6001\u8fdb\u884c\u8054\u5408\u5efa\u6a21\uff1b\u8bbe\u8ba1\u591a\u5c3a\u5ea6\u95e8\u63a7\u5377\u79ef\u5355\u5143\u6355\u83b7\u8fd0\u52a8\u8fc7\u7a0b\u4e2d\u7684\u7ec6\u7c92\u5ea6\u52a8\u6001\u53d8\u5316\uff0c\u589e\u5f3a\u6a21\u578b\u5b66\u4e60\u4e30\u5bcc\u8fd0\u52a8\u8868\u793a\u7684\u80fd\u529b\u3002", "result": "\u5728\u516d\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u60ef\u6027\u6570\u636e\u96c6\u4e0a\u6301\u7eed\u8d85\u8d8aSOTA\u57fa\u7ebf\u65b9\u6cd5\u3002\u5728RoNIN\u6570\u636e\u96c6\u4e0a\u76f8\u6bd4\u57fa\u7ebf\u6a21\u578b\uff0cATE\u8bef\u5dee\u964d\u4f4e\u8303\u56f4\u4e3a2.26%\u523065.78%\uff0c\u5728\u8be5\u9886\u57df\u5efa\u7acb\u4e86\u65b0\u7684\u57fa\u51c6\u3002", "conclusion": "\u8be5\u8f7b\u91cf\u7ea7\u60ef\u6027\u91cc\u7a0b\u8ba1\u6846\u67b6\u901a\u8fc7\u521b\u65b0\u7684\u7279\u5f81\u63d0\u53d6\u548c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u590d\u6742\u8fd0\u52a8\u6a21\u5f0f\u4e0b\u7684\u6f02\u79fb\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u4e3a\u6d88\u8d39\u7ea7\u5b9a\u4f4d\u7cfb\u7edf\u7684\u5e7f\u6cdb\u90e8\u7f72\u63d0\u4f9b\u4e86\u6838\u5fc3\u6280\u672f\u652f\u6491\u3002"}}
{"id": "2507.15974", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.15974", "abs": "https://arxiv.org/abs/2507.15974", "authors": ["Tong Wu", "Chong Xiang", "Jiachen T. Wang", "Weichen Yu", "Chawin Sitawarin", "Vikash Sehwag", "Prateek Mittal"], "title": "Does More Inference-Time Compute Really Help Robustness?", "comment": "Preprint", "summary": "Recently, Zaremba et al. demonstrated that increasing inference-time\ncomputation improves robustness in large proprietary reasoning LLMs. In this\npaper, we first show that smaller-scale, open-source models (e.g., DeepSeek R1,\nQwen3, Phi-reasoning) can also benefit from inference-time scaling using a\nsimple budget forcing strategy. More importantly, we reveal and critically\nexamine an implicit assumption in prior work: intermediate reasoning steps are\nhidden from adversaries. By relaxing this assumption, we identify an important\nsecurity risk, intuitively motivated and empirically verified as an inverse\nscaling law: if intermediate reasoning steps become explicitly accessible,\nincreased inference-time computation consistently reduces model robustness.\nFinally, we discuss practical scenarios where models with hidden reasoning\nchains are still vulnerable to attacks, such as models with tool-integrated\nreasoning and advanced reasoning extraction attacks. Our findings collectively\ndemonstrate that the robustness benefits of inference-time scaling depend\nheavily on the adversarial setting and deployment context. We urge\npractitioners to carefully weigh these subtle trade-offs before applying\ninference-time scaling in security-sensitive, real-world applications.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u63a8\u7406\u65f6\u8ba1\u7b97\u6269\u5c55\u5bf9\u8bed\u8a00\u6a21\u578b\u9c81\u68d2\u6027\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5f53\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\u5bf9\u653b\u51fb\u8005\u53ef\u89c1\u65f6\uff0c\u589e\u52a0\u63a8\u7406\u8ba1\u7b97\u53cd\u800c\u4f1a\u964d\u4f4e\u6a21\u578b\u7684\u9c81\u68d2\u6027\uff0c\u63ed\u793a\u4e86\u4e00\u4e2a\u9006\u5411\u6269\u5c55\u5b9a\u5f8b", "motivation": "\u5148\u524d\u7814\u7a76\u8868\u660e\u589e\u52a0\u63a8\u7406\u65f6\u8ba1\u7b97\u53ef\u4ee5\u63d0\u5347\u5927\u578b\u4e13\u6709\u63a8\u7406\u6a21\u578b\u7684\u9c81\u68d2\u6027\uff0c\u4f46\u8fd9\u4e9b\u7814\u7a76\u9690\u542b\u5047\u8bbe\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\u5bf9\u653b\u51fb\u8005\u662f\u9690\u85cf\u7684\u3002\u672c\u6587\u65e8\u5728\u68c0\u9a8c\u8fd9\u4e00\u5047\u8bbe\uff0c\u5e76\u63a2\u8ba8\u5f53\u63a8\u7406\u6b65\u9aa4\u66b4\u9732\u65f6\u5bf9\u6a21\u578b\u5b89\u5168\u6027\u7684\u5f71\u54cd", "method": "\u4f7f\u7528\u9884\u7b97\u5f3a\u5236\u7b56\u7565\u5728\u5c0f\u89c4\u6a21\u5f00\u6e90\u6a21\u578b\uff08\u5982DeepSeek R1\u3001Qwen3\u3001Phi-reasoning\uff09\u4e0a\u9a8c\u8bc1\u63a8\u7406\u65f6\u6269\u5c55\u7684\u6548\u679c\uff1b\u901a\u8fc7\u653e\u5bbd\"\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\u5bf9\u653b\u51fb\u8005\u9690\u85cf\"\u7684\u5047\u8bbe\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u5f53\u63a8\u7406\u6b65\u9aa4\u53ef\u89c1\u65f6\u63a8\u7406\u8ba1\u7b97\u5bf9\u9c81\u68d2\u6027\u7684\u5f71\u54cd\uff1b\u5206\u6790\u5b9e\u9645\u90e8\u7f72\u573a\u666f\u4e2d\u7684\u5b89\u5168\u98ce\u9669", "result": "\u9a8c\u8bc1\u4e86\u5c0f\u89c4\u6a21\u5f00\u6e90\u6a21\u578b\u4e5f\u80fd\u4ece\u63a8\u7406\u65f6\u6269\u5c55\u4e2d\u53d7\u76ca\uff1b\u53d1\u73b0\u5e76\u9a8c\u8bc1\u4e86\u9006\u5411\u6269\u5c55\u5b9a\u5f8b\uff1a\u5f53\u4e2d\u95f4\u63a8\u7406\u6b65\u9aa4\u5bf9\u653b\u51fb\u8005\u53ef\u89c1\u65f6\uff0c\u589e\u52a0\u63a8\u7406\u65f6\u8ba1\u7b97\u4f1a\u6301\u7eed\u964d\u4f4e\u6a21\u578b\u9c81\u68d2\u6027\uff1b\u8bc6\u522b\u4e86\u5373\u4f7f\u9690\u85cf\u63a8\u7406\u94fe\u7684\u6a21\u578b\u4ecd\u7136\u5bb9\u6613\u53d7\u5230\u653b\u51fb\u7684\u5b9e\u9645\u573a\u666f", "conclusion": "\u63a8\u7406\u65f6\u6269\u5c55\u7684\u9c81\u68d2\u6027\u6536\u76ca\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4f9d\u8d56\u4e8e\u5bf9\u6297\u8bbe\u7f6e\u548c\u90e8\u7f72\u73af\u5883\u3002\u5efa\u8bae\u4ece\u4e1a\u8005\u5728\u5b89\u5168\u654f\u611f\u7684\u5b9e\u9645\u5e94\u7528\u4e2d\u4f7f\u7528\u63a8\u7406\u65f6\u6269\u5c55\u4e4b\u524d\uff0c\u5e94\u4ed4\u7ec6\u6743\u8861\u8fd9\u4e9b\u5fae\u5999\u7684\u6743\u8861\u5173\u7cfb"}}
{"id": "2507.16124", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16124", "abs": "https://arxiv.org/abs/2507.16124", "authors": ["Dakota Sullivan", "Shirley Zhang", "Jennica Li", "Heather Kirkorian", "Bilge Mutlu", "Kassem Fawaz"], "title": "Benchmarking LLM Privacy Recognition for Social Robot Decision Making", "comment": "18 pages, 7 figures. Dakota Sullivan and Shirley Zhang contributed\n  equally to this work", "summary": "Social robots are embodied agents that interact with people while following\nhuman communication norms. These robots interact using verbal and non-verbal\ncues, and share the physical environments of people. While social robots have\npreviously utilized rule-based systems or probabilistic models for user\ninteraction, the rapid evolution of large language models (LLMs) presents new\nopportunities to develop LLM-empowered social robots for enhanced human-robot\ninteraction. To fully realize these capabilities, however, robots need to\ncollect data such as audio, fine-grained images, video, and locations. As a\nresult, LLMs often process sensitive personal information, particularly within\nhome environments. Given the tension between utility and privacy risks,\nevaluating how current LLMs manage sensitive data is critical. Specifically, we\naim to explore the extent to which out-of-the-box LLMs are privacy-aware in the\ncontext of household social robots. In this study, we present a set of\nprivacy-relevant scenarios crafted through the lens of Contextual Integrity\n(CI). We first survey users' privacy preferences regarding in-home social robot\nbehaviors and then examine how their privacy orientation affects their choices\nof these behaviors (N = 450). We then provide the same set of scenarios and\nquestions to state-of-the-art LLMs (N = 10) and find that the agreement between\nhumans and LLMs is low. To further investigate the capabilities of LLMs as a\npotential privacy controller, we implement four additional prompting strategies\nand compare their results. Finally, we discuss the implications and potential\nof AI privacy awareness in human-robot interaction.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u793e\u4ea4\u673a\u5668\u4eba\u5728\u5bb6\u5ead\u73af\u5883\u4e2d\u7684\u9690\u79c1\u610f\u8bc6\u95ee\u9898\uff0c\u53d1\u73b0\u5f53\u524dLLM\u5728\u9690\u79c1\u4fdd\u62a4\u65b9\u9762\u4e0e\u4eba\u7c7b\u7528\u6237\u7684\u504f\u597d\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u7b56\u7565\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u6280\u672f\u7684\u53d1\u5c55\uff0c\u793e\u4ea4\u673a\u5668\u4eba\u80fd\u591f\u66f4\u597d\u5730\u4e0e\u4eba\u7c7b\u4ea4\u4e92\uff0c\u4f46\u5728\u5bb6\u5ead\u73af\u5883\u4e2d\u8fd9\u4e9b\u673a\u5668\u4eba\u9700\u8981\u6536\u96c6\u654f\u611f\u4e2a\u4eba\u4fe1\u606f\uff08\u97f3\u9891\u3001\u56fe\u50cf\u3001\u89c6\u9891\u3001\u4f4d\u7f6e\u7b49\uff09\uff0c\u56e0\u6b64\u8feb\u5207\u9700\u8981\u8bc4\u4f30LLM\u5728\u5904\u7406\u654f\u611f\u6570\u636e\u65f6\u7684\u9690\u79c1\u610f\u8bc6\u80fd\u529b\u3002", "method": "\u91c7\u7528\u60c5\u5883\u5b8c\u6574\u6027(Contextual Integrity, CI)\u7406\u8bba\u6784\u5efa\u9690\u79c1\u76f8\u5173\u573a\u666f\uff0c\u9996\u5148\u8c03\u67e5\u7528\u6237\u5bf9\u5bb6\u5ead\u793e\u4ea4\u673a\u5668\u4eba\u884c\u4e3a\u7684\u9690\u79c1\u504f\u597d(N=450)\uff0c\u7136\u540e\u5c06\u76f8\u540c\u573a\u666f\u63d0\u4f9b\u7ed910\u4e2a\u6700\u5148\u8fdb\u7684\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u6d4b\u8bd5\uff0c\u5e76\u5b9e\u65bd\u56db\u79cd\u989d\u5916\u7684\u63d0\u793a\u7b56\u7565\u6765\u6bd4\u8f83\u7ed3\u679c\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4eba\u7c7b\u7528\u6237\u548c\u5927\u8bed\u8a00\u6a21\u578b\u4e4b\u95f4\u5728\u9690\u79c1\u5224\u65ad\u4e0a\u7684\u4e00\u81f4\u6027\u5f88\u4f4e\uff0c\u8bf4\u660e\u5f53\u524d\u7684LLM\u7f3a\u4e4f\u8db3\u591f\u7684\u9690\u79c1\u610f\u8bc6\u3002\u901a\u8fc7\u4e0d\u540c\u7684\u63d0\u793a\u7b56\u7565\u53ef\u4ee5\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u6539\u5584LLM\u7684\u9690\u79c1\u5224\u65ad\u80fd\u529b\u3002", "conclusion": "\u5f53\u524d\u7684\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4f5c\u4e3a\u9690\u79c1\u63a7\u5236\u5668\u65b9\u9762\u80fd\u529b\u6709\u9650\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u548c\u6539\u8fdbAI\u7cfb\u7edf\u7684\u9690\u79c1\u610f\u8bc6\uff0c\u4ee5\u786e\u4fddLLM\u9a71\u52a8\u7684\u793e\u4ea4\u673a\u5668\u4eba\u80fd\u591f\u66f4\u597d\u5730\u4fdd\u62a4\u7528\u6237\u9690\u79c1\u5e76\u7b26\u5408\u4eba\u7c7b\u7684\u9690\u79c1\u671f\u671b\u3002"}}
{"id": "2507.16020", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16020", "abs": "https://arxiv.org/abs/2507.16020", "authors": ["Xi Yang", "Jiachen Wang", "Song Han", "Suining He"], "title": "Micromobility Flow Prediction: A Bike Sharing Station-level Study via Multi-level Spatial-Temporal Attention Neural Network", "comment": "6 pages, UrbComp 2024", "summary": "Efficient use of urban micromobility resources such as bike sharing is\nchallenging due to the unbalanced station-level demand and supply, which causes\nthe maintenance of the bike sharing systems painstaking. Prior efforts have\nbeen made on accurate prediction of bike traffics, i.e., demand/pick-up and\nreturn/drop-off, to achieve system efficiency. However, bike station-level\ntraffic prediction is difficult because of the spatial-temporal complexity of\nbike sharing systems. Moreover, such level of prediction over entire bike\nsharing systems is also challenging due to the large number of bike stations.\nTo fill this gap, we propose BikeMAN, a multi-level spatio-temporal attention\nneural network to predict station-level bike traffic for entire bike sharing\nsystems. The proposed network consists of an encoder and a decoder with an\nattention mechanism representing the spatial correlation between features of\nbike stations in the system and another attention mechanism describing the\ntemporal characteristic of bike station traffic. Through experimental study on\nover 10 millions trips of bike sharing systems (> 700 stations) of New York\nCity, our network showed high accuracy in predicting the bike station traffic\nof all stations in the city.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faBikeMAN\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u5c42\u6b21\u65f6\u7a7a\u6ce8\u610f\u529b\u673a\u5236\u9884\u6d4b\u6574\u4e2a\u57ce\u5e02\u81ea\u884c\u8f66\u5171\u4eab\u7cfb\u7edf\u4e2d\u6240\u6709\u7ad9\u70b9\u7684\u4ea4\u901a\u6d41\u91cf\uff0c\u5728\u7ebd\u7ea6\u5e02700\u591a\u4e2a\u7ad9\u70b9\u76841000\u4e07\u6b21\u51fa\u884c\u6570\u636e\u4e0a\u9a8c\u8bc1\u4e86\u6a21\u578b\u7684\u9ad8\u7cbe\u5ea6\u9884\u6d4b\u80fd\u529b\u3002", "motivation": "\u57ce\u5e02\u5fae\u51fa\u884c\u8d44\u6e90\uff08\u5982\u5171\u4eab\u5355\u8f66\uff09\u56e0\u7ad9\u70b9\u7ea7\u9700\u6c42\u4e0e\u4f9b\u7ed9\u4e0d\u5e73\u8861\u5bfc\u81f4\u7cfb\u7edf\u7ef4\u62a4\u56f0\u96be\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u9884\u6d4b\u6574\u4e2a\u5171\u4eab\u5355\u8f66\u7cfb\u7edf\u7684\u7ad9\u70b9\u7ea7\u4ea4\u901a\u6d41\u91cf\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u4e3b\u8981\u7531\u4e8e\u65f6\u7a7a\u590d\u6742\u6027\u548c\u7ad9\u70b9\u6570\u91cf\u5e9e\u5927\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51faBikeMAN\u591a\u5c42\u6b21\u65f6\u7a7a\u6ce8\u610f\u529b\u795e\u7ecf\u7f51\u7edc\uff0c\u5305\u542b\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u7ed3\u6784\uff0c\u91c7\u7528\u4e24\u79cd\u6ce8\u610f\u529b\u673a\u5236\uff1a\u4e00\u79cd\u8868\u793a\u7cfb\u7edf\u4e2d\u5404\u7ad9\u70b9\u7279\u5f81\u95f4\u7684\u7a7a\u95f4\u76f8\u5173\u6027\uff0c\u53e6\u4e00\u79cd\u63cf\u8ff0\u7ad9\u70b9\u4ea4\u901a\u7684\u65f6\u95f4\u7279\u5f81\u3002", "result": "\u5728\u7ebd\u7ea6\u5e02\u5171\u4eab\u5355\u8f66\u7cfb\u7edf\u76841000\u591a\u4e07\u6b21\u51fa\u884c\u6570\u636e\uff08\u8d85\u8fc7700\u4e2a\u7ad9\u70b9\uff09\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8be5\u7f51\u7edc\u5728\u9884\u6d4b\u57ce\u5e02\u6240\u6709\u7ad9\u70b9\u7684\u81ea\u884c\u8f66\u4ea4\u901a\u6d41\u91cf\u65b9\u9762\u8868\u73b0\u51fa\u9ad8\u7cbe\u5ea6\u3002", "conclusion": "BikeMAN\u6a21\u578b\u6210\u529f\u89e3\u51b3\u4e86\u6574\u4e2a\u5171\u4eab\u5355\u8f66\u7cfb\u7edf\u7ad9\u70b9\u7ea7\u4ea4\u901a\u9884\u6d4b\u7684\u6311\u6218\uff0c\u901a\u8fc7\u591a\u5c42\u6b21\u65f6\u7a7a\u6ce8\u610f\u529b\u673a\u5236\u6709\u6548\u6355\u83b7\u4e86\u7cfb\u7edf\u7684\u590d\u6742\u65f6\u7a7a\u7279\u5f81\uff0c\u4e3a\u5171\u4eab\u5355\u8f66\u7cfb\u7edf\u7684\u9ad8\u6548\u8fd0\u8425\u63d0\u4f9b\u4e86\u6709\u529b\u652f\u6491\u3002"}}
{"id": "2507.16139", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.16139", "abs": "https://arxiv.org/abs/2507.16139", "authors": ["Arsh Tangri", "Nichols Crawford Taylor", "Haojie Huang", "Robert Platt"], "title": "Equivariant Goal Conditioned Contrastive Reinforcement Learning", "comment": null, "summary": "Contrastive Reinforcement Learning (CRL) provides a promising framework for\nextracting useful structured representations from unlabeled interactions. By\npulling together state-action pairs and their corresponding future states,\nwhile pushing apart negative pairs, CRL enables learning nontrivial policies\nwithout manually designed rewards. In this work, we propose Equivariant CRL\n(ECRL), which further structures the latent space using equivariant\nconstraints. By leveraging inherent symmetries in goal-conditioned manipulation\ntasks, our method improves both sample efficiency and spatial generalization.\nSpecifically, we formally define Goal-Conditioned Group-Invariant MDPs to\ncharacterize rotation-symmetric robotic manipulation tasks, and build on this\nby introducing a novel rotation-invariant critic representation paired with a\nrotation-equivariant actor for Contrastive RL. Our approach consistently\noutperforms strong baselines across a range of simulated tasks in both\nstate-based and image-based settings. Finally, we extend our method to the\noffline RL setting, demonstrating its effectiveness across multiple tasks.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u7b49\u53d8\u5bf9\u6bd4\u5f3a\u5316\u5b66\u4e60(ECRL)\uff0c\u901a\u8fc7\u5229\u7528\u76ee\u6807\u6761\u4ef6\u64cd\u4f5c\u4efb\u52a1\u4e2d\u7684\u65cb\u8f6c\u5bf9\u79f0\u6027\u6765\u6539\u8fdb\u5bf9\u6bd4\u5f3a\u5316\u5b66\u4e60\uff0c\u5728\u72b6\u6001\u8868\u793a\u548c\u56fe\u50cf\u8868\u793a\u8bbe\u7f6e\u4e0b\u90fd\u663e\u8457\u63d0\u5347\u4e86\u6837\u672c\u6548\u7387\u548c\u7a7a\u95f4\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u5bf9\u6bd4\u5f3a\u5316\u5b66\u4e60(CRL)\u867d\u7136\u80fd\u4ece\u65e0\u6807\u7b7e\u4ea4\u4e92\u4e2d\u5b66\u4e60\u6709\u7528\u7684\u7ed3\u6784\u5316\u8868\u793a\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u6f5c\u5728\u7a7a\u95f4\u7684\u8fdb\u4e00\u6b65\u7ed3\u6784\u5316\u7ea6\u675f\u3002\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u5177\u6709\u5185\u5728\u7684\u5bf9\u79f0\u6027(\u5982\u65cb\u8f6c\u5bf9\u79f0\u6027)\uff0c\u53ef\u4ee5\u88ab\u5229\u7528\u6765\u6539\u8fdb\u5b66\u4e60\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u7b49\u53d8\u5bf9\u6bd4\u5f3a\u5316\u5b66\u4e60(ECRL)\u65b9\u6cd5\uff1a1)\u6b63\u5f0f\u5b9a\u4e49\u4e86\u76ee\u6807\u6761\u4ef6\u7fa4\u4e0d\u53d8\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u6765\u523b\u753b\u65cb\u8f6c\u5bf9\u79f0\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\uff1b2)\u5f15\u5165\u65b0\u9896\u7684\u65cb\u8f6c\u4e0d\u53d8\u6279\u8bc4\u5668\u8868\u793a\u4e0e\u65cb\u8f6c\u7b49\u53d8\u6f14\u5458\u76f8\u914d\u5408\uff1b3)\u901a\u8fc7\u7b49\u53d8\u7ea6\u675f\u8fdb\u4e00\u6b65\u7ed3\u6784\u5316\u6f5c\u5728\u7a7a\u95f4\uff0c\u5229\u7528\u4efb\u52a1\u7684\u5185\u5728\u5bf9\u79f0\u6027\u3002", "result": "\u5728\u591a\u4e2a\u4eff\u771f\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u72b6\u6001\u8868\u793a\u548c\u56fe\u50cf\u8868\u793a\u8bbe\u7f6e\u4e0b\u90fd\u6301\u7eed\u8d85\u8d8a\u4e86\u5f3a\u57fa\u7ebf\u65b9\u6cd5\uff0c\u663e\u793a\u51fa\u66f4\u597d\u7684\u6837\u672c\u6548\u7387\u548c\u7a7a\u95f4\u6cdb\u5316\u80fd\u529b\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u8fd8\u6210\u529f\u6269\u5c55\u5230\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u8bbe\u7f6e\uff0c\u5728\u591a\u4e2a\u4efb\u52a1\u4e2d\u5c55\u73b0\u4e86\u6709\u6548\u6027\u3002", "conclusion": "\u7b49\u53d8\u5bf9\u6bd4\u5f3a\u5316\u5b66\u4e60\u901a\u8fc7\u5229\u7528\u76ee\u6807\u6761\u4ef6\u64cd\u4f5c\u4efb\u52a1\u7684\u65cb\u8f6c\u5bf9\u79f0\u6027\uff0c\u6709\u6548\u6539\u8fdb\u4e86\u5bf9\u6bd4\u5f3a\u5316\u5b66\u4e60\u7684\u6027\u80fd\u3002\u65cb\u8f6c\u4e0d\u53d8\u6279\u8bc4\u5668\u4e0e\u65cb\u8f6c\u7b49\u53d8\u6f14\u5458\u7684\u7ec4\u5408\u8bbe\u8ba1\u80fd\u591f\u66f4\u597d\u5730\u7ed3\u6784\u5316\u6f5c\u5728\u7a7a\u95f4\uff0c\u4ece\u800c\u63d0\u5347\u6837\u672c\u6548\u7387\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u4e14\u8be5\u65b9\u6cd5\u53ef\u4ee5\u6210\u529f\u5e94\u7528\u4e8e\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u573a\u666f\u3002"}}
{"id": "2507.16028", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16028", "abs": "https://arxiv.org/abs/2507.16028", "authors": ["Tehseen Rug", "Felix B\u00f6hmer", "Tessa Pfattheicher"], "title": "From Logic to Language: A Trust Index for Problem Solving with LLMs", "comment": "17 pages, 2 figures", "summary": "Classical computation, grounded in formal, logical systems, has been the\nengine of technological progress for decades, excelling at problems that can be\ndescribed with unambiguous rules. This paradigm, however, leaves a vast ocean\nof human problems -- those characterized by ambiguity, dynamic environments,\nand subjective context -- largely untouched. The advent of Large Language\nModels (LLMs) represents a fundamental shift, enabling computational systems to\nengage with this previously inaccessible domain using natural language. This\npaper introduces a unified framework to understand and contrast these\nproblem-solving paradigms. We define and delineate the problem spaces\naddressable by formal languages versus natural language. While solutions to the\nformer problem class can be evaluated using binary quality measures, the latter\nrequires a much more nuanced definition of approximate solution space taking\ninto account the vagueness, subjectivity and ambiguity inherent to natural\nlanguage. We therefore introduce a vector-valued trust index Q, which reflects\nsolution quality and distinguishes the binary correctness of formal solutions\nfrom the continuous adequacy spectrum characteristic of natural language\nsolutions. Within this framework, we propose two statistical quality\ndimensions. Normalized bi-semantic entropy measures robustness and conceptual\ndiversity of LLM answers given semantic variation in problem formulations.\nEmotional valence maps subjective valuation of a solution to a quantifiable\nmetric that can be maximized by invoking statistical measures. The concepts\nintroduced in this work will provide a more rigorous understanding of the\ncapabilities, limitations, and inherent nature of problem-solving in the age of\nLLMs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\u6765\u7406\u89e3\u548c\u5bf9\u6bd4\u7ecf\u5178\u8ba1\u7b97\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\u5728\u95ee\u9898\u89e3\u51b3\u65b9\u9762\u7684\u4e0d\u540c\u8303\u5f0f\uff0c\u5f15\u5165\u4e86\u5411\u91cf\u503c\u4fe1\u4efb\u6307\u6570Q\u6765\u8bc4\u4f30\u81ea\u7136\u8bed\u8a00\u89e3\u51b3\u65b9\u6848\u7684\u8d28\u91cf\uff0c\u5e76\u63d0\u51fa\u4e86\u4e24\u4e2a\u7edf\u8ba1\u8d28\u91cf\u7ef4\u5ea6\u6765\u8861\u91cfLLM\u89e3\u51b3\u65b9\u6848\u7684\u9c81\u68d2\u6027\u548c\u4e3b\u89c2\u4ef7\u503c\u3002", "motivation": "\u7ecf\u5178\u8ba1\u7b97\u57fa\u4e8e\u5f62\u5f0f\u903b\u8f91\u7cfb\u7edf\uff0c\u64c5\u957f\u5904\u7406\u53ef\u7528\u660e\u786e\u89c4\u5219\u63cf\u8ff0\u7684\u95ee\u9898\uff0c\u4f46\u5bf9\u4e8e\u5145\u6ee1\u6b67\u4e49\u6027\u3001\u52a8\u6001\u73af\u5883\u548c\u4e3b\u89c2\u8bed\u5883\u7684\u4eba\u7c7b\u95ee\u9898\u5374\u65e0\u80fd\u4e3a\u529b\u3002\u5927\u8bed\u8a00\u6a21\u578b\u7684\u51fa\u73b0\u4ee3\u8868\u4e86\u6839\u672c\u6027\u8f6c\u53d8\uff0c\u4f7f\u8ba1\u7b97\u7cfb\u7edf\u80fd\u591f\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u5904\u7406\u8fd9\u4e9b\u4ee5\u524d\u65e0\u6cd5\u89e6\u53ca\u7684\u9886\u57df\u3002", "method": "\u8bba\u6587\u5b9a\u4e49\u5e76\u5212\u5206\u4e86\u5f62\u5f0f\u8bed\u8a00\u4e0e\u81ea\u7136\u8bed\u8a00\u53ef\u89e3\u51b3\u7684\u95ee\u9898\u7a7a\u95f4\uff0c\u5f15\u5165\u4e86\u5411\u91cf\u503c\u4fe1\u4efb\u6307\u6570Q\u6765\u53cd\u6620\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\uff0c\u533a\u5206\u5f62\u5f0f\u89e3\u51b3\u65b9\u6848\u7684\u4e8c\u5143\u6b63\u786e\u6027\u548c\u81ea\u7136\u8bed\u8a00\u89e3\u51b3\u65b9\u6848\u7684\u8fde\u7eed\u9002\u5f53\u6027\u8c31\u3002\u63d0\u51fa\u4e86\u4e24\u4e2a\u7edf\u8ba1\u8d28\u91cf\u7ef4\u5ea6\uff1a\u6807\u51c6\u5316\u53cc\u8bed\u4e49\u71b5\uff08\u8861\u91cf\u9c81\u68d2\u6027\u548c\u6982\u5ff5\u591a\u6837\u6027\uff09\u548c\u60c5\u611f\u4ef7\u503c\uff08\u5c06\u4e3b\u89c2\u8bc4\u4ef7\u6620\u5c04\u4e3a\u53ef\u91cf\u5316\u6307\u6807\uff09\u3002", "result": "\u5efa\u7acb\u4e86\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\u6765\u7406\u89e3\u7ecf\u5178\u8ba1\u7b97\u548cLLM\u95ee\u9898\u89e3\u51b3\u8303\u5f0f\u7684\u5dee\u5f02\uff0c\u6210\u529f\u5b9a\u4e49\u4e86\u8bc4\u4f30\u81ea\u7136\u8bed\u8a00\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u7684\u65b0\u65b9\u6cd5\uff0c\u5305\u62ec\u4fe1\u4efb\u6307\u6570Q\u548c\u4e24\u4e2a\u7edf\u8ba1\u8d28\u91cf\u7ef4\u5ea6\uff0c\u4e3aLLM\u65f6\u4ee3\u7684\u95ee\u9898\u89e3\u51b3\u63d0\u4f9b\u4e86\u66f4\u4e25\u683c\u7684\u7406\u8bba\u57fa\u7840\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u7406\u89e3LLM\u65f6\u4ee3\u95ee\u9898\u89e3\u51b3\u7684\u80fd\u529b\u3001\u5c40\u9650\u6027\u548c\u5185\u5728\u672c\u8d28\u63d0\u4f9b\u4e86\u66f4\u4e25\u683c\u7684\u7406\u8bba\u6846\u67b6\uff0c\u4e3a\u8bc4\u4f30\u548c\u4f18\u5316\u81ea\u7136\u8bed\u8a00\u8ba1\u7b97\u7cfb\u7edf\u7684\u6027\u80fd\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.16175", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.16175", "abs": "https://arxiv.org/abs/2507.16175", "authors": ["Euijeong Lee", "Kyung Min Han", "Young J. Kim"], "title": "Scanning Bot: Efficient Scan Planning using Panoramic Cameras", "comment": null, "summary": "Panoramic RGB-D cameras are known for their ability to produce high quality\n3D scene reconstructions. However, operating these cameras involves manually\nselecting viewpoints and physically transporting the camera, making the\ngeneration of a 3D model time consuming and tedious. Additionally, the process\ncan be challenging for novice users due to spatial constraints, such as\nensuring sufficient feature overlap between viewpoint frames. To address these\nchallenges, we propose a fully autonomous scan planning that generates an\nefficient tour plan for environment scanning, ensuring collision-free\nnavigation and adequate overlap between viewpoints within the plan. Extensive\nexperiments conducted in both synthetic and real-world environments validate\nthe performance of our planner against state-of-the-art view planners. In\nparticular, our method achieved an average scan coverage of 99 percent in the\nreal-world experiment, with our approach being up to 3 times faster than\nstate-of-the-art planners in total scan time.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5168\u81ea\u4e3b\u7684\u626b\u63cf\u89c4\u5212\u7cfb\u7edf\uff0c\u7528\u4e8e\u5168\u666fRGB-D\u76f8\u673a\u76843D\u573a\u666f\u91cd\u5efa\uff0c\u901a\u8fc7\u81ea\u52a8\u751f\u6210\u9ad8\u6548\u7684\u626b\u63cf\u8def\u5f84\u6765\u89e3\u51b3\u624b\u52a8\u64cd\u4f5c\u7684\u8017\u65f6\u548c\u590d\u6742\u6027\u95ee\u9898", "motivation": "\u5168\u666fRGB-D\u76f8\u673a\u867d\u7136\u80fd\u4ea7\u751f\u9ad8\u8d28\u91cf\u76843D\u573a\u666f\u91cd\u5efa\uff0c\u4f46\u9700\u8981\u624b\u52a8\u9009\u62e9\u89c6\u70b9\u548c\u7269\u7406\u642c\u8fd0\u76f8\u673a\uff0c\u4f7f\u5f973D\u6a21\u578b\u751f\u6210\u8fc7\u7a0b\u8017\u65f6\u4e14\u7e41\u7410\u3002\u5bf9\u4e8e\u65b0\u624b\u7528\u6237\u6765\u8bf4\uff0c\u7531\u4e8e\u7a7a\u95f4\u7ea6\u675f\uff08\u5982\u786e\u4fdd\u89c6\u70b9\u5e27\u4e4b\u95f4\u6709\u8db3\u591f\u7684\u7279\u5f81\u91cd\u53e0\uff09\u4f7f\u5f97\u64cd\u4f5c\u8fc7\u7a0b\u66f4\u52a0\u56f0\u96be", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5168\u81ea\u4e3b\u7684\u626b\u63cf\u89c4\u5212\u65b9\u6cd5\uff0c\u80fd\u591f\u4e3a\u73af\u5883\u626b\u63cf\u751f\u6210\u9ad8\u6548\u7684\u8def\u5f84\u89c4\u5212\uff0c\u786e\u4fdd\u65e0\u78b0\u649e\u5bfc\u822a\u548c\u89c4\u5212\u5185\u89c6\u70b9\u4e4b\u95f4\u7684\u5145\u5206\u91cd\u53e0", "result": "\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u89c4\u5212\u5668\u76f8\u5bf9\u4e8e\u6700\u5148\u8fdb\u89c6\u56fe\u89c4\u5212\u5668\u7684\u6027\u80fd\u3002\u5728\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5e73\u574799%\u7684\u626b\u63cf\u8986\u76d6\u7387\uff0c\u603b\u626b\u63cf\u65f6\u95f4\u6bd4\u6700\u5148\u8fdb\u7684\u89c4\u5212\u5668\u5feb3\u500d", "conclusion": "\u8be5\u5168\u81ea\u4e3b\u626b\u63cf\u89c4\u5212\u7cfb\u7edf\u6210\u529f\u89e3\u51b3\u4e86\u5168\u666fRGB-D\u76f8\u673a\u624b\u52a8\u64cd\u4f5c\u7684\u5c40\u9650\u6027\uff0c\u5728\u4fdd\u8bc1\u9ad8\u626b\u63cf\u8986\u76d6\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u626b\u63cf\u6548\u7387\uff0c\u4e3a3D\u573a\u666f\u91cd\u5efa\u63d0\u4f9b\u4e86\u66f4\u52a0\u5b9e\u7528\u548c\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2507.16067", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.16067", "abs": "https://arxiv.org/abs/2507.16067", "authors": ["Jeroen Spaans", "Jesse Heyninck"], "title": "A Unifying Framework for Semiring-Based Constraint Logic Programming With Negation (full version)", "comment": "Full version, including proofs and appendices, of paper accepted at\n  IJCAI 2025", "summary": "Constraint Logic Programming (CLP) is a logic programming formalism used to\nsolve problems requiring the consideration of constraints, like resource\nallocation and automated planning and scheduling. It has previously been\nextended in various directions, for example to support fuzzy constraint\nsatisfaction, uncertainty, or negation, with different notions of semiring\nbeing used as a unifying abstraction for these generalizations. None of these\nextensions have studied clauses with negation allowed in the body. We\ninvestigate an extension of CLP which unifies many of these extensions and\nallows negation in the body. We provide semantics for such programs, using the\nframework of approximation fixpoint theory, and give a detailed overview of the\nimpacts of properties of the semirings on the resulting semantics. As such, we\nprovide a unifying framework that captures existing approaches and allows\nextending them with a more expressive language.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u7ea6\u675f\u903b\u8f91\u7f16\u7a0b\u6269\u5c55\u6846\u67b6\uff0c\u652f\u6301\u5728\u5b50\u53e5\u4f53\u4e2d\u4f7f\u7528\u5426\u5b9a\uff0c\u5e76\u57fa\u4e8e\u8fd1\u4f3c\u4e0d\u52a8\u70b9\u7406\u8bba\u63d0\u4f9b\u8bed\u4e49\u5b66\uff0c\u7edf\u4e00\u4e86\u73b0\u6709\u7684\u5404\u79cdCLP\u6269\u5c55\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u7ea6\u675f\u903b\u8f91\u7f16\u7a0b(CLP)\u6269\u5c55\u867d\u7136\u652f\u6301\u6a21\u7cca\u7ea6\u675f\u6ee1\u8db3\u3001\u4e0d\u786e\u5b9a\u6027\u6216\u5426\u5b9a\u7b49\u7279\u6027\uff0c\u4f46\u90fd\u6ca1\u6709\u7814\u7a76\u5728\u5b50\u53e5\u4f53\u4e2d\u5141\u8bb8\u5426\u5b9a\u7684\u60c5\u51b5\u3002\u9700\u8981\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\u6765\u6574\u5408\u8fd9\u4e9b\u6269\u5c55\u5e76\u652f\u6301\u66f4\u5177\u8868\u8fbe\u529b\u7684\u8bed\u8a00\u7279\u6027\u3002", "method": "\u4f7f\u7528\u534a\u73af\u4f5c\u4e3a\u7edf\u4e00\u62bd\u8c61\u6765\u6269\u5c55CLP\uff0c\u57fa\u4e8e\u8fd1\u4f3c\u4e0d\u52a8\u70b9\u7406\u8bba\u6846\u67b6\u4e3a\u5305\u542b\u4f53\u5426\u5b9a\u7684\u7a0b\u5e8f\u63d0\u4f9b\u8bed\u4e49\u5b66\uff0c\u5e76\u8be6\u7ec6\u5206\u6790\u534a\u73af\u6027\u8d28\u5bf9\u7ed3\u679c\u8bed\u4e49\u7684\u5f71\u54cd\u3002", "result": "\u6210\u529f\u6784\u5efa\u4e86\u4e00\u4e2a\u7edf\u4e00\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u6355\u83b7\u73b0\u6709\u7684\u5404\u79cdCLP\u6269\u5c55\u65b9\u6cd5\uff0c\u540c\u65f6\u652f\u6301\u5728\u5b50\u53e5\u4f53\u4e2d\u4f7f\u7528\u5426\u5b9a\uff0c\u63d0\u4f9b\u4e86\u5b8c\u6574\u7684\u8bed\u4e49\u5b66\u5b9a\u4e49\u548c\u534a\u73af\u6027\u8d28\u7684\u5f71\u54cd\u5206\u6790\u3002", "conclusion": "\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684CLP\u6269\u5c55\u6846\u67b6\uff0c\u4e0d\u4ec5\u6574\u5408\u4e86\u73b0\u6709\u65b9\u6cd5\uff0c\u8fd8\u901a\u8fc7\u652f\u6301\u4f53\u5426\u5b9a\u6269\u5c55\u4e86\u8bed\u8a00\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u4e3a\u7ea6\u675f\u903b\u8f91\u7f16\u7a0b\u9886\u57df\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u548c\u7075\u6d3b\u7684\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2507.16214", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16214", "abs": "https://arxiv.org/abs/2507.16214", "authors": ["Batu Candan", "Simone Servadio"], "title": "Adaptive Relative Pose Estimation Framework with Dual Noise Tuning for Safe Approaching Maneuvers", "comment": null, "summary": "Accurate and robust relative pose estimation is crucial for enabling\nchallenging Active Debris Removal (ADR) missions targeting tumbling derelict\nsatellites such as ESA's ENVISAT. This work presents a complete pipeline\nintegrating advanced computer vision techniques with adaptive nonlinear\nfiltering to address this challenge. A Convolutional Neural Network (CNN),\nenhanced with image preprocessing, detects structural markers (corners) from\nchaser imagery, whose 2D coordinates are converted to 3D measurements using\ncamera modeling. These measurements are fused within an Unscented Kalman Filter\n(UKF) framework, selected for its ability to handle nonlinear relative\ndynamics, to estimate the full relative pose. Key contributions include the\nintegrated system architecture and a dual adaptive strategy within the UKF:\ndynamic tuning of the measurement noise covariance compensates for varying CNN\nmeasurement uncertainty, while adaptive tuning of the process noise covariance,\nutilizing measurement residual analysis, accounts for unmodeled dynamics or\nmaneuvers online. This dual adaptation enhances robustness against both\nmeasurement imperfections and dynamic model uncertainties. The performance of\nthe proposed adaptive integrated system is evaluated through high-fidelity\nsimulations using a realistic ENVISAT model, comparing estimates against ground\ntruth under various conditions, including measurement outages. This\ncomprehensive approach offers an enhanced solution for robust onboard relative\nnavigation, significantly advancing the capabilities required for safe\nproximity operations during ADR missions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684\u76f8\u5bf9\u4f4d\u59ff\u4f30\u8ba1\u7ba1\u9053\uff0c\u7ed3\u5408CNN\u548c\u81ea\u9002\u5e94UKF\u6ee4\u6ce2\u5668\uff0c\u7528\u4e8e\u4e3b\u52a8\u788e\u7247\u6e05\u9664\u4efb\u52a1\u4e2d\u7ffb\u6eda\u536b\u661f\u7684\u9c81\u68d2\u5bfc\u822a", "motivation": "\u51c6\u786e\u4e14\u9c81\u68d2\u7684\u76f8\u5bf9\u4f4d\u59ff\u4f30\u8ba1\u5bf9\u4e8e\u5b9e\u73b0\u5177\u6709\u6311\u6218\u6027\u7684\u4e3b\u52a8\u788e\u7247\u6e05\u9664\uff08ADR\uff09\u4efb\u52a1\u81f3\u5173\u91cd\u8981\uff0c\u7279\u522b\u662f\u9488\u5bf9\u50cfESA\u7684ENVISAT\u8fd9\u6837\u7684\u7ffb\u6eda\u5e9f\u5f03\u536b\u661f\u76ee\u6807", "method": "\u96c6\u6210\u5148\u8fdb\u8ba1\u7b97\u673a\u89c6\u89c9\u6280\u672f\u4e0e\u81ea\u9002\u5e94\u975e\u7ebf\u6027\u6ee4\u6ce2\uff1a\u4f7f\u7528\u7ecf\u8fc7\u56fe\u50cf\u9884\u5904\u7406\u589e\u5f3a\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u4ece\u8ffd\u8e2a\u5668\u56fe\u50cf\u4e2d\u68c0\u6d4b\u7ed3\u6784\u6807\u8bb0\uff08\u89d2\u70b9\uff09\uff0c\u5c062D\u5750\u6807\u901a\u8fc7\u76f8\u673a\u5efa\u6a21\u8f6c\u6362\u4e3a3D\u6d4b\u91cf\u503c\uff0c\u7136\u540e\u5728\u65e0\u8ff9\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\uff08UKF\uff09\u6846\u67b6\u5185\u878d\u5408\u8fd9\u4e9b\u6d4b\u91cf\u503c\u3002\u91c7\u7528\u53cc\u81ea\u9002\u5e94\u7b56\u7565\uff1a\u52a8\u6001\u8c03\u6574\u6d4b\u91cf\u566a\u58f0\u534f\u65b9\u5dee\u4ee5\u8865\u507fCNN\u6d4b\u91cf\u4e0d\u786e\u5b9a\u6027\u7684\u53d8\u5316\uff0c\u4ee5\u53ca\u5229\u7528\u6d4b\u91cf\u6b8b\u5dee\u5206\u6790\u81ea\u9002\u5e94\u8c03\u6574\u8fc7\u7a0b\u566a\u58f0\u534f\u65b9\u5dee\u6765\u5728\u7ebf\u5904\u7406\u672a\u5efa\u6a21\u52a8\u529b\u5b66\u6216\u673a\u52a8", "result": "\u901a\u8fc7\u4f7f\u7528\u771f\u5b9eENVISAT\u6a21\u578b\u7684\u9ad8\u4fdd\u771f\u4eff\u771f\u8bc4\u4f30\u4e86\u6240\u63d0\u51fa\u7684\u81ea\u9002\u5e94\u96c6\u6210\u7cfb\u7edf\u6027\u80fd\uff0c\u5728\u5404\u79cd\u6761\u4ef6\u4e0b\uff08\u5305\u62ec\u6d4b\u91cf\u4e2d\u65ad\uff09\u5c06\u4f30\u8ba1\u7ed3\u679c\u4e0e\u771f\u5b9e\u503c\u8fdb\u884c\u6bd4\u8f83\u3002\u53cc\u81ea\u9002\u5e94\u7b56\u7565\u589e\u5f3a\u4e86\u5bf9\u6d4b\u91cf\u7f3a\u9677\u548c\u52a8\u6001\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u7684\u9c81\u68d2\u6027", "conclusion": "\u8fd9\u79cd\u7efc\u5408\u65b9\u6cd5\u4e3a\u9c81\u68d2\u7684\u661f\u8f7d\u76f8\u5bf9\u5bfc\u822a\u63d0\u4f9b\u4e86\u589e\u5f3a\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86ADR\u4efb\u52a1\u4e2d\u5b89\u5168\u90bb\u8fd1\u64cd\u4f5c\u6240\u9700\u7684\u80fd\u529b"}}
{"id": "2507.16110", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.16110", "abs": "https://arxiv.org/abs/2507.16110", "authors": ["Shengchao Liu", "Hannan Xu", "Yan Ai", "Huanxin Li", "Yoshua Bengio", "Harry Guo"], "title": "Expert-Guided LLM Reasoning for Battery Discovery: From AI-Driven Hypothesis to Synthesis and Characterization", "comment": null, "summary": "Large language models (LLMs) leverage chain-of-thought (CoT) techniques to\ntackle complex problems, representing a transformative breakthrough in\nartificial intelligence (AI). However, their reasoning capabilities have\nprimarily been demonstrated in solving math and coding problems, leaving their\npotential for domain-specific applications-such as battery discovery-largely\nunexplored. Inspired by the idea that reasoning mirrors a form of guided\nsearch, we introduce ChatBattery, a novel agentic framework that integrates\ndomain knowledge to steer LLMs toward more effective reasoning in materials\ndesign. Using ChatBattery, we successfully identify, synthesize, and\ncharacterize three novel lithium-ion battery cathode materials, which achieve\npractical capacity improvements of 28.8%, 25.2%, and 18.5%, respectively, over\nthe widely used cathode material, LiNi0.8Mn0.1Co0.1O2 (NMC811). Beyond this\ndiscovery, ChatBattery paves a new path by showing a successful LLM-driven and\nreasoning-based platform for battery materials invention. This complete\nAI-driven cycle-from design to synthesis to characterization-demonstrates the\ntransformative potential of AI-driven reasoning in revolutionizing materials\ndiscovery.", "AI": {"tldr": "\u7814\u7a76\u8005\u5f00\u53d1\u4e86ChatBattery\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u8fdb\u884c\u7535\u6c60\u6750\u6599\u53d1\u73b0\uff0c\u6210\u529f\u8bbe\u8ba1\u5e76\u5408\u6210\u4e86\u4e09\u79cd\u65b0\u578b\u9502\u79bb\u5b50\u7535\u6c60\u6b63\u6781\u6750\u6599\uff0c\u5bb9\u91cf\u5206\u522b\u63d0\u5347\u4e8628.8%\u300125.2%\u548c18.5%", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\u4e3b\u8981\u5728\u6570\u5b66\u548c\u7f16\u7a0b\u95ee\u9898\u4e0a\u5f97\u5230\u9a8c\u8bc1\uff0c\u4f46\u5728\u7279\u5b9a\u9886\u57df\u5e94\u7528\uff08\u5982\u7535\u6c60\u53d1\u73b0\uff09\u65b9\u9762\u7684\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u63a2\u7d22\u3002\u7814\u7a76\u8005\u5e0c\u671b\u5c06\u63a8\u7406\u80fd\u529b\u6269\u5c55\u5230\u6750\u6599\u8bbe\u8ba1\u9886\u57df", "method": "\u63d0\u51faChatBattery\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5c06\u9886\u57df\u77e5\u8bc6\u6574\u5408\u5230\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\uff0c\u5f15\u5bfc\u5176\u5728\u6750\u6599\u8bbe\u8ba1\u4e2d\u8fdb\u884c\u66f4\u6709\u6548\u7684\u63a8\u7406\u3002\u8be5\u6846\u67b6\u91c7\u7528\u63a8\u7406\u9a71\u52a8\u7684\u65b9\u6cd5\u8fdb\u884c\u6750\u6599\u53d1\u73b0", "result": "\u6210\u529f\u8bc6\u522b\u3001\u5408\u6210\u5e76\u8868\u5f81\u4e86\u4e09\u79cd\u65b0\u578b\u9502\u79bb\u5b50\u7535\u6c60\u6b63\u6781\u6750\u6599\uff0c\u76f8\u6bd4\u5e7f\u6cdb\u4f7f\u7528\u7684NMC811\u6b63\u6781\u6750\u6599\uff0c\u5b9e\u9645\u5bb9\u91cf\u5206\u522b\u63d0\u5347\u4e8628.8%\u300125.2%\u548c18.5%", "conclusion": "ChatBattery\u5c55\u793a\u4e86\u4ece\u8bbe\u8ba1\u5230\u5408\u6210\u518d\u5230\u8868\u5f81\u7684\u5b8c\u6574AI\u9a71\u52a8\u5faa\u73af\uff0c\u8bc1\u660e\u4e86AI\u9a71\u52a8\u63a8\u7406\u5728\u9769\u547d\u6027\u6750\u6599\u53d1\u73b0\u65b9\u9762\u7684\u53d8\u9769\u6f5c\u529b\uff0c\u4e3aLLM\u9a71\u52a8\u7684\u6750\u6599\u53d1\u660e\u5e73\u53f0\u5f00\u8f9f\u4e86\u65b0\u8def\u5f84"}}
{"id": "2507.16233", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.16233", "abs": "https://arxiv.org/abs/2507.16233", "authors": ["Yue Lin", "Xiaoxuan Zhang", "Yang Liu", "Dong Wang", "Huchuan Lu"], "title": "GFM-Planner: Perception-Aware Trajectory Planning with Geometric Feature Metric", "comment": "Accepted by IROS 2025", "summary": "Like humans who rely on landmarks for orientation, autonomous robots depend\non feature-rich environments for accurate localization. In this paper, we\npropose the GFM-Planner, a perception-aware trajectory planning framework based\non the geometric feature metric, which enhances LiDAR localization accuracy by\nguiding the robot to avoid degraded areas. First, we derive the Geometric\nFeature Metric (GFM) from the fundamental LiDAR localization problem. Next, we\ndesign a 2D grid-based Metric Encoding Map (MEM) to efficiently store GFM\nvalues across the environment. A constant-time decoding algorithm is further\nproposed to retrieve GFM values for arbitrary poses from the MEM. Finally, we\ndevelop a perception-aware trajectory planning algorithm that improves LiDAR\nlocalization capabilities by guiding the robot in selecting trajectories\nthrough feature-rich areas. Both simulation and real-world experiments\ndemonstrate that our approach enables the robot to actively select trajectories\nthat significantly enhance LiDAR localization accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86GFM-Planner\uff0c\u4e00\u4e2a\u57fa\u4e8e\u51e0\u4f55\u7279\u5f81\u5ea6\u91cf\u7684\u611f\u77e5\u611f\u77e5\u8f68\u8ff9\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5bfc\u673a\u5668\u4eba\u907f\u5f00\u9000\u5316\u533a\u57df\u6765\u63d0\u9ad8LiDAR\u5b9a\u4f4d\u7cbe\u5ea6", "motivation": "\u81ea\u4e3b\u673a\u5668\u4eba\u4f9d\u8d56\u7279\u5f81\u4e30\u5bcc\u7684\u73af\u5883\u8fdb\u884c\u51c6\u786e\u5b9a\u4f4d\uff0c\u7c7b\u4f3c\u4e8e\u4eba\u7c7b\u4f9d\u9760\u5730\u6807\u8fdb\u884c\u65b9\u5411\u5b9a\u4f4d\uff0c\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u4e3b\u52a8\u9009\u62e9\u8f68\u8ff9\u4ee5\u63d0\u9ad8LiDAR\u5b9a\u4f4d\u80fd\u529b\u7684\u89c4\u5212\u6846\u67b6", "method": "1) \u4ece\u57fa\u7840LiDAR\u5b9a\u4f4d\u95ee\u9898\u4e2d\u63a8\u5bfc\u51fa\u51e0\u4f55\u7279\u5f81\u5ea6\u91cf(GFM)\uff1b2) \u8bbe\u8ba1\u57fa\u4e8e2D\u7f51\u683c\u7684\u5ea6\u91cf\u7f16\u7801\u56fe(MEM)\u6765\u9ad8\u6548\u5b58\u50a8\u73af\u5883\u4e2d\u7684GFM\u503c\uff1b3) \u63d0\u51fa\u5e38\u6570\u65f6\u95f4\u89e3\u7801\u7b97\u6cd5\u4eceMEM\u4e2d\u68c0\u7d22\u4efb\u610f\u59ff\u6001\u7684GFM\u503c\uff1b4) \u5f00\u53d1\u611f\u77e5\u611f\u77e5\u8f68\u8ff9\u89c4\u5212\u7b97\u6cd5\uff0c\u5f15\u5bfc\u673a\u5668\u4eba\u9009\u62e9\u901a\u8fc7\u7279\u5f81\u4e30\u5bcc\u533a\u57df\u7684\u8f68\u8ff9", "result": "\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u90fd\u8bc1\u660e\u8be5\u65b9\u6cd5\u80fd\u591f\u4f7f\u673a\u5668\u4eba\u4e3b\u52a8\u9009\u62e9\u663e\u8457\u63d0\u9ad8LiDAR\u5b9a\u4f4d\u7cbe\u5ea6\u7684\u8f68\u8ff9", "conclusion": "GFM-Planner\u6846\u67b6\u6210\u529f\u5730\u901a\u8fc7\u51e0\u4f55\u7279\u5f81\u5ea6\u91cf\u6307\u5bfc\u8f68\u8ff9\u89c4\u5212\uff0c\u6709\u6548\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u7684LiDAR\u5b9a\u4f4d\u80fd\u529b\uff0c\u4e3a\u81ea\u4e3b\u673a\u5668\u4eba\u5bfc\u822a\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2507.16126", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16126", "abs": "https://arxiv.org/abs/2507.16126", "authors": ["Michael R. Bock", "Kara Molisee", "Zachary Ozer", "Sumit Shah"], "title": "TaxCalcBench: Evaluating Frontier Models on the Tax Calculation Task", "comment": null, "summary": "Can AI file your taxes? Not yet. Calculating US personal income taxes is a\ntask that requires building an understanding of vast amounts of English text\nand using that knowledge to carefully compute results. We propose TaxCalcBench,\na benchmark for determining models' abilities to calculate personal income tax\nreturns given all of the necessary information. Our experiment shows that\nstate-of-the-art models succeed in calculating less than a third of federal\nincome tax returns even on this simplified sample set. Our analysis concludes\nthat models consistently misuse tax tables, make errors in tax calculation, and\nincorrectly determine eligibility. Our findings point to the need for\nadditional infrastructure to apply LLMs to the personal income tax calculation\ntask.", "AI": {"tldr": "\u7814\u7a76\u8005\u63d0\u51fa\u4e86TaxCalcBench\u57fa\u51c6\u6765\u6d4b\u8bd5AI\u6a21\u578b\u8ba1\u7b97\u4e2a\u4eba\u6240\u5f97\u7a0e\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u6700\u5148\u8fdb\u7684\u6a21\u578b\u5728\u7b80\u5316\u6837\u672c\u96c6\u4e0a\u7684\u6210\u529f\u7387\u4e0d\u5230\u4e09\u5206\u4e4b\u4e00\uff0c\u4e3b\u8981\u95ee\u9898\u5305\u62ec\u8bef\u7528\u7a0e\u8868\u3001\u8ba1\u7b97\u9519\u8bef\u548c\u8d44\u683c\u5224\u65ad\u9519\u8bef\u3002", "motivation": "\u76ee\u524d\u5c1a\u65e0AI\u80fd\u591f\u51c6\u786e\u5904\u7406\u7f8e\u56fd\u4e2a\u4eba\u6240\u5f97\u7a0e\u8ba1\u7b97\u8fd9\u4e00\u590d\u6742\u4efb\u52a1\u3002\u8be5\u4efb\u52a1\u9700\u8981\u7406\u89e3\u5927\u91cf\u82f1\u6587\u6587\u672c\u5e76\u8fd0\u7528\u76f8\u5173\u77e5\u8bc6\u8fdb\u884c\u7cbe\u786e\u8ba1\u7b97\uff0c\u56e0\u6b64\u9700\u8981\u5efa\u7acb\u4e13\u95e8\u7684\u57fa\u51c6\u6765\u8bc4\u4f30\u6a21\u578b\u5728\u6b64\u65b9\u9762\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51faTaxCalcBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u6a21\u578b\u5728\u7ed9\u5b9a\u6240\u6709\u5fc5\u8981\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\u8ba1\u7b97\u4e2a\u4eba\u6240\u5f97\u7a0e\u7533\u62a5\u8868\u7684\u80fd\u529b\u3002\u901a\u8fc7\u5728\u7b80\u5316\u6837\u672c\u96c6\u4e0a\u6d4b\u8bd5\u6700\u5148\u8fdb\u7684\u6a21\u578b\u6765\u5206\u6790\u5176\u8868\u73b0\u3002", "result": "\u6700\u5148\u8fdb\u7684\u6a21\u578b\u5728\u8ba1\u7b97\u8054\u90a6\u6240\u5f97\u7a0e\u7533\u62a5\u8868\u65b9\u9762\u7684\u6210\u529f\u7387\u4e0d\u5230\u4e09\u5206\u4e4b\u4e00\u3002\u6a21\u578b\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u6301\u7eed\u8bef\u7528\u7a0e\u8868\u3001\u5728\u7a0e\u52a1\u8ba1\u7b97\u4e2d\u51fa\u73b0\u9519\u8bef\u3001\u4ee5\u53ca\u9519\u8bef\u5224\u65ad\u8d44\u683c\u6761\u4ef6\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u4e2a\u4eba\u6240\u5f97\u7a0e\u8ba1\u7b97\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u989d\u5916\u7684\u57fa\u7840\u8bbe\u65bd\u652f\u6301\u624d\u80fd\u5c06LLMs\u6709\u6548\u5e94\u7528\u4e8e\u4e2a\u4eba\u6240\u5f97\u7a0e\u8ba1\u7b97\u4efb\u52a1\u3002"}}
{"id": "2507.16305", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.16305", "abs": "https://arxiv.org/abs/2507.16305", "authors": ["Xiao Liu", "Weijun Wang", "Tianlun Huang", "Zhiyong Wang", "Wei Feng"], "title": "Trajectory Planning of a Curtain Wall Installation Robot Based on Biomimetic Mechanisms", "comment": null, "summary": "As the robotics market rapidly evolves, energy consumption has become a\ncritical issue, particularly restricting the application of construction\nrobots. To tackle this challenge, our study innovatively draws inspiration from\nthe mechanics of human upper limb movements during weight lifting, proposing a\nbio-inspired trajectory planning framework that incorporates human energy\nconversion principles. By collecting motion trajectories and electromyography\n(EMG) signals during dumbbell curls, we construct an anthropomorphic trajectory\nplanning that integrates human force exertion patterns and energy consumption\npatterns. Utilizing the Particle Swarm Optimization (PSO) algorithm, we achieve\ndynamic load distribution for robotic arm trajectory planning based on\nhuman-like movement features. In practical application, these bio-inspired\nmovement characteristics are applied to curtain wall installation tasks,\nvalidating the correctness and superiority of our trajectory planning method.\nSimulation results demonstrate a 48.4% reduction in energy consumption through\nintelligent conversion between kinetic and potential energy. This approach\nprovides new insights and theoretical support for optimizing energy use in\ncurtain wall installation robots during actual handling tasks.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u53d7\u4eba\u4f53\u4e0a\u80a2\u4e3e\u91cd\u8fd0\u52a8\u542f\u53d1\u7684\u673a\u5668\u4eba\u8f68\u8ff9\u89c4\u5212\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u529b\u91cf\u65bd\u5c55\u548c\u80fd\u91cf\u6d88\u8017\u6a21\u5f0f\uff0c\u7ed3\u5408\u7c92\u5b50\u7fa4\u4f18\u5316\u7b97\u6cd5\uff0c\u5728\u5e55\u5899\u5b89\u88c5\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e8648.4%\u7684\u80fd\u8017\u964d\u4f4e\u3002", "motivation": "\u968f\u7740\u673a\u5668\u4eba\u5e02\u573a\u5feb\u901f\u53d1\u5c55\uff0c\u80fd\u8017\u95ee\u9898\u6210\u4e3a\u5173\u952e\u6311\u6218\uff0c\u7279\u522b\u662f\u9650\u5236\u4e86\u5efa\u7b51\u673a\u5668\u4eba\u7684\u5e94\u7528\u3002\u4e3a\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\uff0c\u7814\u7a76\u56e2\u961f\u4ece\u4eba\u4f53\u4e0a\u80a2\u4e3e\u91cd\u8fd0\u52a8\u7684\u529b\u5b66\u673a\u5236\u4e2d\u6c72\u53d6\u7075\u611f\uff0c\u5e0c\u671b\u901a\u8fc7\u878d\u5408\u4eba\u7c7b\u80fd\u91cf\u8f6c\u6362\u539f\u7406\u6765\u4f18\u5316\u673a\u5668\u4eba\u7684\u80fd\u8017\u8868\u73b0\u3002", "method": "\u901a\u8fc7\u6536\u96c6\u54d1\u94c3\u5f2f\u4e3e\u8fc7\u7a0b\u4e2d\u7684\u8fd0\u52a8\u8f68\u8ff9\u548c\u808c\u7535\u56fe(EMG)\u4fe1\u53f7\uff0c\u6784\u5efa\u878d\u5408\u4eba\u7c7b\u529b\u91cf\u65bd\u5c55\u6a21\u5f0f\u548c\u80fd\u91cf\u6d88\u8017\u6a21\u5f0f\u7684\u62df\u4eba\u5316\u8f68\u8ff9\u89c4\u5212\u3002\u5229\u7528\u7c92\u5b50\u7fa4\u4f18\u5316(PSO)\u7b97\u6cd5\uff0c\u57fa\u4e8e\u7c7b\u4eba\u8fd0\u52a8\u7279\u5f81\u5b9e\u73b0\u673a\u68b0\u81c2\u8f68\u8ff9\u89c4\u5212\u7684\u52a8\u6001\u8d1f\u8f7d\u5206\u914d\uff0c\u5e76\u5c06\u8fd9\u4e9b\u4eff\u751f\u8fd0\u52a8\u7279\u6027\u5e94\u7528\u4e8e\u5e55\u5899\u5b89\u88c5\u4efb\u52a1\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u901a\u8fc7\u52a8\u80fd\u548c\u52bf\u80fd\u4e4b\u95f4\u7684\u667a\u80fd\u8f6c\u6362\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e8648.4%\u7684\u80fd\u8017\u964d\u4f4e\u3002\u5728\u5e55\u5899\u5b89\u88c5\u4efb\u52a1\u7684\u5b9e\u9645\u5e94\u7528\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u8f68\u8ff9\u89c4\u5212\u65b9\u6cd5\u7684\u6b63\u786e\u6027\u548c\u4f18\u8d8a\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5e55\u5899\u5b89\u88c5\u673a\u5668\u4eba\u5728\u5b9e\u9645\u642c\u8fd0\u4efb\u52a1\u4e2d\u7684\u80fd\u8017\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\u548c\u7406\u8bba\u652f\u6301\uff0c\u8bc1\u660e\u4e86\u4eff\u751f\u5b66\u65b9\u6cd5\u5728\u89e3\u51b3\u673a\u5668\u4eba\u80fd\u8017\u95ee\u9898\u65b9\u9762\u7684\u6709\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2507.16145", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.16145", "abs": "https://arxiv.org/abs/2507.16145", "authors": ["Shuhao Mei", "Yongchao Long", "Shan Cao", "Xiaobo Han", "Shijia Geng", "Jinbo Sun", "Yuxi Zhou", "Shenda Hong"], "title": "SpiroLLM: Finetuning Pretrained LLMs to Understand Spirogram Time Series with Clinical Validation in COPD Reporting", "comment": null, "summary": "Chronic Obstructive Pulmonary Disease (COPD), a major chronic respiratory\ndisease with persistent airflow limitation, is a leading global cause of\ndisability and mortality. Respiratory spirogram time series, routinely\ncollected during pulmonary function tests (PFTs), play a critical role in the\nearly detection of repsiratory diseases and in monitoring lung function over\ntime. However, most current AI models for COPD diagnosis are limited to\noutputting classification results without providing a rationale for their\ndiagnostic process, while current Large Language Models (LLMs) cannot\nunderstand spirograms yet, which severely limits their clinical trust and\nadoption. To tackle this challenge, we leverage a cohort of 234,028 individuals\nfrom the UK Biobank (UKB) to propose SpiroLLM, the first multimodal large\nlanguage model that can understand spirogram. The model extracts morphological\nfeatures from respiratory curves via a SpiroEncoder and aligns them with PFT\nnumerical values in a unified latent space using a SpiroProjector, ultimately\nempowering a large language model to generate a comprehensive diagnostic\nreport. Experimental results confirm that SpiroLLM achieved a diagnostic AUROC\nof 0.8980 (95% CI: 0.8820-0.9132). In a robustness test with missing core data,\nit maintained a 100% valid response rate, far surpassing the 13.4% of a\ntext-only model and showcasing the superiority of its multimodal design. This\nwork demonstrates the substantial potential of deeply fusing physiological\nsignals with large language models, establishing a new paradigm for the next\ngeneration of interpretable and reliable clinical decision support tools.", "AI": {"tldr": "\u7814\u7a76\u56e2\u961f\u5f00\u53d1\u4e86SpiroLLM\uff0c\u8fd9\u662f\u9996\u4e2a\u80fd\u591f\u7406\u89e3\u80ba\u529f\u80fd\u56fe\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u7528\u4e8e\u6162\u6027\u963b\u585e\u6027\u80ba\u75c5(COPD)\u8bca\u65ad\uff0c\u572823.4\u4e07\u4eba\u7684\u82f1\u56fd\u751f\u7269\u94f6\u884c\u6570\u636e\u96c6\u4e0a\u8fbe\u52300.8980\u7684AUROC\uff0c\u5e76\u80fd\u751f\u6210\u53ef\u89e3\u91ca\u7684\u8bca\u65ad\u62a5\u544a\u3002", "motivation": "\u5f53\u524dCOPD\u8bca\u65ad\u7684AI\u6a21\u578b\u53ea\u80fd\u8f93\u51fa\u5206\u7c7b\u7ed3\u679c\u800c\u65e0\u6cd5\u63d0\u4f9b\u8bca\u65ad\u4f9d\u636e\uff0c\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u53c8\u65e0\u6cd5\u7406\u89e3\u80ba\u529f\u80fd\u56fe\uff0c\u8fd9\u4e25\u91cd\u9650\u5236\u4e86\u4e34\u5e8a\u4fe1\u4efb\u5ea6\u548c\u5e94\u7528\u3002\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u80fd\u591f\u7406\u89e3\u80ba\u529f\u80fd\u56fe\u5e76\u63d0\u4f9b\u53ef\u89e3\u91ca\u8bca\u65ad\u7684\u6a21\u578b\u3002", "method": "\u63d0\u51faSpiroLLM\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4f7f\u7528SpiroEncoder\u4ece\u547c\u5438\u66f2\u7ebf\u4e2d\u63d0\u53d6\u5f62\u6001\u5b66\u7279\u5f81\uff0c\u901a\u8fc7SpiroProjector\u5c06\u8fd9\u4e9b\u7279\u5f81\u4e0e\u80ba\u529f\u80fd\u6d4b\u8bd5\u6570\u503c\u5728\u7edf\u4e00\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5bf9\u9f50\uff0c\u6700\u7ec8\u4f7f\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u751f\u6210\u7efc\u5408\u8bca\u65ad\u62a5\u544a\u3002", "result": "\u5728\u82f1\u56fd\u751f\u7269\u94f6\u884c23.4\u4e07\u4eba\u6570\u636e\u96c6\u4e0a\uff0cSpiroLLM\u8fbe\u52300.8980\u7684\u8bca\u65adAUROC\uff0895% CI: 0.8820-0.9132\uff09\u3002\u5728\u7f3a\u5931\u6838\u5fc3\u6570\u636e\u7684\u9c81\u68d2\u6027\u6d4b\u8bd5\u4e2d\uff0c\u4fdd\u6301100%\u6709\u6548\u54cd\u5e94\u7387\uff0c\u8fdc\u8d85\u4ec5\u6587\u672c\u6a21\u578b\u768413.4%\uff0c\u5c55\u73b0\u4e86\u591a\u6a21\u6001\u8bbe\u8ba1\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u8bc1\u660e\u4e86\u751f\u7406\u4fe1\u53f7\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\u6df1\u5ea6\u878d\u5408\u7684\u5de8\u5927\u6f5c\u529b\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u53ef\u89e3\u91ca\u4e14\u53ef\u9760\u7684\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u5de5\u5177\u5efa\u7acb\u4e86\u65b0\u7684\u8303\u5f0f\uff0c\u4e3aCOPD\u7b49\u547c\u5438\u7cfb\u7edf\u75be\u75c5\u7684\u667a\u80fd\u8bca\u65ad\u63d0\u4f9b\u4e86\u91cd\u8981\u7a81\u7834\u3002"}}
{"id": "2507.16328", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.16328", "abs": "https://arxiv.org/abs/2507.16328", "authors": ["Xiao Liu", "Xianlong Yang", "Weijun Wang", "Wei Feng"], "title": "Design and Dimensional Optimization of Legged Structures for Construction Robots", "comment": null, "summary": "Faced with complex and unstructured construction environments, wheeled and\ntracked robots exhibit significant limitations in terrain adaptability and\nflexibility, making it difficult to meet the requirements of autonomous\noperation. Inspired by ants in nature, this paper proposes a leg configuration\ndesign and optimization method tailored for construction scenarios, aiming to\nenhance the autonomous mobility of construction robots. This paper analyzes the\nfull operational motion performance of the leg during both swing and stance\nphases. First, based on kinematic modeling and multi-dimensional workspace\nanalysis, the concept of an \"improved workspace\" is introduced, and graphical\nmethods are used to optimize the leg dimensions during the swing phase.\nFurthermore, a new concept of \"average manipulability\" is introduced based on\nthe velocity Jacobian matrix, and numerical solutions are applied to obtain the\nleg segment ratio that maximizes manipulability. To overcome the difficulties\nassociated with traditional analytical methods, virtual prototype simulations\nare conducted in ADAMS to explore the relationship between the robot body's\noptimal flexibility and leg segment proportions. In summary, the leg segment\nproportions with the best comprehensive motion performance are obtained. This\nstudy presents the first multi-dimensional quantitative evaluation framework\nfor leg motion performance tailored for construction environments, providing a\nstructural design foundation for legged construction robots to achieve\nautonomous mobility in complex terrains.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u5efa\u7b51\u73af\u5883\u63d0\u51fa\u4e86\u4e00\u79cd\u4eff\u751f\u8682\u8681\u7684\u817f\u90e8\u914d\u7f6e\u8bbe\u8ba1\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u7ef4\u5de5\u4f5c\u7a7a\u95f4\u5206\u6790\u548c\u5e73\u5747\u53ef\u64cd\u4f5c\u6027\u6982\u5ff5\uff0c\u4e3a\u5efa\u7b51\u673a\u5668\u4eba\u5728\u590d\u6742\u5730\u5f62\u4e2d\u7684\u81ea\u4e3b\u79fb\u52a8\u63d0\u4f9b\u7ed3\u6784\u8bbe\u8ba1\u57fa\u7840\u3002", "motivation": "\u8f6e\u5f0f\u548c\u5c65\u5e26\u5f0f\u673a\u5668\u4eba\u5728\u590d\u6742\u975e\u7ed3\u6784\u5316\u5efa\u7b51\u73af\u5883\u4e2d\u5730\u5f62\u9002\u5e94\u6027\u548c\u7075\u6d3b\u6027\u5b58\u5728\u663e\u8457\u5c40\u9650\uff0c\u96be\u4ee5\u6ee1\u8db3\u81ea\u4e3b\u4f5c\u4e1a\u8981\u6c42\uff0c\u56e0\u6b64\u9700\u8981\u8bbe\u8ba1\u66f4\u9002\u5408\u5efa\u7b51\u573a\u666f\u7684\u817f\u5f0f\u673a\u5668\u4eba\u914d\u7f6e\u3002", "method": "\u57fa\u4e8e\u8fd0\u52a8\u5b66\u5efa\u6a21\u548c\u591a\u7ef4\u5de5\u4f5c\u7a7a\u95f4\u5206\u6790\u5f15\u5165\"\u6539\u8fdb\u5de5\u4f5c\u7a7a\u95f4\"\u6982\u5ff5\uff0c\u4f7f\u7528\u56fe\u5f62\u5316\u65b9\u6cd5\u4f18\u5316\u6446\u52a8\u76f8\u817f\u90e8\u5c3a\u5bf8\uff1b\u57fa\u4e8e\u901f\u5ea6\u96c5\u53ef\u6bd4\u77e9\u9635\u5f15\u5165\"\u5e73\u5747\u53ef\u64cd\u4f5c\u6027\"\u6982\u5ff5\uff0c\u91c7\u7528\u6570\u503c\u89e3\u83b7\u5f97\u6700\u5927\u53ef\u64cd\u4f5c\u6027\u7684\u817f\u6bb5\u6bd4\u4f8b\uff1b\u5728ADAMS\u4e2d\u8fdb\u884c\u865a\u62df\u6837\u673a\u4eff\u771f\u63a2\u7d22\u673a\u5668\u4eba\u672c\u4f53\u6700\u4f18\u7075\u6d3b\u6027\u4e0e\u817f\u6bb5\u6bd4\u4f8b\u7684\u5173\u7cfb\u3002", "result": "\u83b7\u5f97\u4e86\u5177\u6709\u6700\u4f73\u7efc\u5408\u8fd0\u52a8\u6027\u80fd\u7684\u817f\u6bb5\u6bd4\u4f8b\uff0c\u5efa\u7acb\u4e86\u9996\u4e2a\u9488\u5bf9\u5efa\u7b51\u73af\u5883\u7684\u817f\u90e8\u8fd0\u52a8\u6027\u80fd\u591a\u7ef4\u5b9a\u91cf\u8bc4\u4f30\u6846\u67b6\u3002", "conclusion": "\u7814\u7a76\u4e3a\u817f\u5f0f\u5efa\u7b51\u673a\u5668\u4eba\u5728\u590d\u6742\u5730\u5f62\u4e2d\u5b9e\u73b0\u81ea\u4e3b\u79fb\u52a8\u63d0\u4f9b\u4e86\u7ed3\u6784\u8bbe\u8ba1\u57fa\u7840\uff0c\u901a\u8fc7\u4eff\u751f\u8bbe\u8ba1\u548c\u591a\u7ef4\u4f18\u5316\u65b9\u6cd5\u6709\u6548\u63d0\u5347\u4e86\u5efa\u7b51\u673a\u5668\u4eba\u7684\u5730\u5f62\u9002\u5e94\u6027\u548c\u8fd0\u52a8\u7075\u6d3b\u6027\u3002"}}
{"id": "2507.16184", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.16184", "abs": "https://arxiv.org/abs/2507.16184", "authors": ["Myung Ho Kim"], "title": "Emergent Cognitive Convergence via Implementation: A Structured Loop Reflecting Four Theories of Mind (A Position Paper)", "comment": "21 pages", "summary": "We report the discovery of a structural convergence across four influential\ntheories of mind: Kahneman's dual-system theory, Friston's predictive\nprocessing, Minsky's society of mind, and Clark's extended mind-emerging\nunintentionally within a practical AI agent architecture called Agentic Flow.\nDesigned to address limitations in large language models (LLMs), Agentic Flow\ncomprises five interdependent modules such as Retrieval, Cognition, Control,\nMemory, and Action arranged in a recurrent cognitive loop. Although originally\ninspired only by Minsky and Clark, the system's structure retrospectively\naligns with computational motifs found in all four theories, including\npredictive modeling, associative recall, and error-sensitive control.\n  To assess this convergence, we conducted comparative experiments with\nbaseline LLM agents on multi-step reasoning tasks. The structured agent\nachieved 95.8% task success and exhibited strong constraint adherence, while\nthe baseline system succeeded 62.3% of the time. These results were not aimed\nat proving superiority, but at illustrating how theoretical structures may\nemerge through practical design choices rather than top-down theory.\n  We introduce PEACE as a descriptive meta-architecture that captures\ndesign-level regularities observed in Agentic Flow. Not intended as a new\ntheory, PEACE provides a shared vocabulary for understanding architectures\nshaped by real-world implementation demands. This paper should be read as a\nposition paper - an exploratory reflection on how implementation can surface\nlatent structural echoes of cognitive theory, without asserting theoretical\nunification.", "AI": {"tldr": "\u672c\u6587\u53d1\u73b0\u4e86\u4e00\u4e2a\u540d\u4e3aAgentic Flow\u7684AI\u667a\u80fd\u4f53\u67b6\u6784\u610f\u5916\u5730\u878d\u5408\u4e86\u56db\u4e2a\u91cd\u8981\u5fc3\u667a\u7406\u8bba\u7684\u7ed3\u6784\u7279\u5f81\uff0c\u5305\u62ec\u5361\u5c3c\u66fc\u7684\u53cc\u7cfb\u7edf\u7406\u8bba\u3001\u5f17\u91cc\u65af\u987f\u7684\u9884\u6d4b\u5904\u7406\u7406\u8bba\u3001\u660e\u65af\u57fa\u7684\u5fc3\u667a\u793e\u4f1a\u7406\u8bba\u548c\u514b\u62c9\u514b\u7684\u5ef6\u5c55\u5fc3\u667a\u7406\u8bba\uff0c\u5e76\u63d0\u51faPEACE\u5143\u67b6\u6784\u6765\u63cf\u8ff0\u8fd9\u79cd\u8bbe\u8ba1\u5c42\u9762\u7684\u89c4\u5f8b\u6027\u3002", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u8981\u66f4\u597d\u7684\u667a\u80fd\u4f53\u67b6\u6784\u6765\u89e3\u51b3\u591a\u6b65\u63a8\u7406\u4efb\u52a1\u3002\u540c\u65f6\uff0c\u4f5c\u8005\u53d1\u73b0\u5b9e\u9645\u7684AI\u7cfb\u7edf\u8bbe\u8ba1\u53ef\u80fd\u81ea\u7136\u5730\u4f53\u73b0\u8ba4\u77e5\u7406\u8bba\u7684\u7ed3\u6784\u7279\u5f81\uff0c\u8fd9\u4e3a\u7406\u89e3\u667a\u80fd\u4f53\u67b6\u6784\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002", "method": "\u8bbe\u8ba1\u4e86Agentic Flow\u67b6\u6784\uff0c\u5305\u542b\u68c0\u7d22\u3001\u8ba4\u77e5\u3001\u63a7\u5236\u3001\u8bb0\u5fc6\u548c\u884c\u52a8\u4e94\u4e2a\u76f8\u4e92\u4f9d\u8d56\u7684\u6a21\u5757\uff0c\u5f62\u6210\u5faa\u73af\u8ba4\u77e5\u56de\u8def\u3002\u901a\u8fc7\u4e0e\u57fa\u7ebfLLM\u667a\u80fd\u4f53\u5728\u591a\u6b65\u63a8\u7406\u4efb\u52a1\u4e0a\u7684\u5bf9\u6bd4\u5b9e\u9a8c\u6765\u8bc4\u4f30\u7cfb\u7edf\u6027\u80fd\u3002\u63d0\u51faPEACE\u5143\u67b6\u6784\u4f5c\u4e3a\u63cf\u8ff0\u6027\u6846\u67b6\u6765\u6355\u83b7\u8bbe\u8ba1\u5c42\u9762\u7684\u89c4\u5f8b\u6027\u3002", "result": "\u7ed3\u6784\u5316\u667a\u80fd\u4f53\u5728\u4efb\u52a1\u6210\u529f\u7387\u4e0a\u8fbe\u523095.8%\uff0c\u8868\u73b0\u51fa\u5f3a\u7ea6\u675f\u9075\u5faa\u80fd\u529b\uff0c\u800c\u57fa\u7ebf\u7cfb\u7edf\u6210\u529f\u7387\u4ec5\u4e3a62.3%\u3002\u5b9e\u9a8c\u7ed3\u679c\u5c55\u793a\u4e86\u7406\u8bba\u7ed3\u6784\u5982\u4f55\u901a\u8fc7\u5b9e\u9645\u8bbe\u8ba1\u9009\u62e9\u800c\u975e\u81ea\u4e0a\u800c\u4e0b\u7684\u7406\u8bba\u6307\u5bfc\u81ea\u7136\u6d8c\u73b0\u3002", "conclusion": "\u672c\u6587\u4f5c\u4e3a\u7acb\u573a\u8bba\u6587\uff0c\u63a2\u7d22\u6027\u5730\u53cd\u601d\u4e86\u5b9e\u73b0\u8fc7\u7a0b\u5982\u4f55\u80fd\u591f\u6d6e\u73b0\u8ba4\u77e5\u7406\u8bba\u7684\u6f5c\u5728\u7ed3\u6784\u56de\u58f0\uff0c\u4f46\u5e76\u4e0d\u4e3b\u5f20\u7406\u8bba\u7edf\u4e00\u3002PEACE\u63d0\u4f9b\u4e86\u7406\u89e3\u53d7\u73b0\u5b9e\u4e16\u754c\u5b9e\u73b0\u9700\u6c42\u5851\u9020\u7684\u67b6\u6784\u7684\u5171\u4eab\u8bcd\u6c47\u8868\uff0c\u4e3aAI\u667a\u80fd\u4f53\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u89c6\u89d2\u3002"}}
{"id": "2507.16335", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.16335", "abs": "https://arxiv.org/abs/2507.16335", "authors": ["Xiao Liu", "Xianlong Yang", "Weijun Wang", "Wei Feng"], "title": "Topology Optimization of Leg Structures for Construction Robots Based on Variable Density Method", "comment": null, "summary": "In complex terrain construction environments, there are high demands for\nrobots to achieve both high payload capacity and mobility flexibility. As the\nkey load-bearing component, the optimization of robotic leg structures is of\nparticular importance. Therefore, this study focuses on the optimization of leg\nstructures for construction robots, proposing a topology optimization strategy\nbased on the SIMP (Solid Isotropic Microstructures with Penalization) variable\ndensity method along with a structural re-design approach. The design\nperformance is comprehensively validated through finite element analysis using\nANSYS. First, static and modal analyses are conducted to evaluate the\nrationality of the initial design. Then, topology optimization using the\nSIMP-based variable density method is applied to the femur section, which\naccounts for the largest proportion of the leg's weight. Based on iterative\ncalculations, the femur undergoes secondary structural reconstruction. After\noptimization, the mass of the femur is reduced by 19.45\\%, and the overall leg\nmass decreases by 7.92\\%, achieving the goal of lightweight design. Finally,\nstatic and modal analyses are conducted on the reconstructed leg. The results\ndemonstrate that the optimized leg still meets structural performance\nrequirements, validating the feasibility of lightweight design. This research\nprovides robust theoretical and technical support for lightweight construction\nrobot design and lays a foundation for their efficient operation in complex\nconstruction environments.", "AI": {"tldr": "\u672c\u7814\u7a76\u9488\u5bf9\u5efa\u7b51\u673a\u5668\u4eba\u817f\u90e8\u7ed3\u6784\u63d0\u51fa\u57fa\u4e8eSIMP\u53d8\u5bc6\u5ea6\u65b9\u6cd5\u7684\u62d3\u6251\u4f18\u5316\u7b56\u7565\uff0c\u901a\u8fc7\u5bf9\u80a1\u9aa8\u6bb5\u8fdb\u884c\u4f18\u5316\u91cd\u6784\uff0c\u5b9e\u73b0\u80a1\u9aa8\u8d28\u91cf\u51cf\u8f7b19.45%\uff0c\u6574\u4f53\u817f\u90e8\u8d28\u91cf\u51cf\u8f7b7.92%\u7684\u8f7b\u91cf\u5316\u8bbe\u8ba1\u76ee\u6807\uff0c\u4e3a\u590d\u6742\u5efa\u7b51\u73af\u5883\u4e2d\u7684\u673a\u5668\u4eba\u8bbe\u8ba1\u63d0\u4f9b\u6280\u672f\u652f\u6491\u3002", "motivation": "\u5728\u590d\u6742\u5730\u5f62\u5efa\u7b51\u73af\u5883\u4e2d\uff0c\u673a\u5668\u4eba\u9700\u8981\u540c\u65f6\u5177\u5907\u9ad8\u8f7d\u8377\u80fd\u529b\u548c\u79fb\u52a8\u7075\u6d3b\u6027\uff0c\u800c\u817f\u90e8\u7ed3\u6784\u4f5c\u4e3a\u5173\u952e\u627f\u91cd\u90e8\u4ef6\uff0c\u5176\u4f18\u5316\u8bbe\u8ba1\u5bf9\u673a\u5668\u4eba\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002\u56e0\u6b64\u9700\u8981\u7814\u7a76\u5efa\u7b51\u673a\u5668\u4eba\u817f\u90e8\u7ed3\u6784\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u4ee5\u5b9e\u73b0\u8f7b\u91cf\u5316\u8bbe\u8ba1\u3002", "method": "\u91c7\u7528\u57fa\u4e8eSIMP\uff08\u56fa\u4f53\u5404\u5411\u540c\u6027\u5fae\u7ed3\u6784\u60e9\u7f5a\uff09\u53d8\u5bc6\u5ea6\u65b9\u6cd5\u7684\u62d3\u6251\u4f18\u5316\u7b56\u7565\uff0c\u7ed3\u5408\u7ed3\u6784\u91cd\u65b0\u8bbe\u8ba1\u65b9\u6cd5\u3002\u9996\u5148\u5bf9\u521d\u59cb\u8bbe\u8ba1\u8fdb\u884c\u9759\u529b\u5b66\u548c\u6a21\u6001\u5206\u6790\u8bc4\u4f30\u5408\u7406\u6027\uff0c\u7136\u540e\u5bf9\u5360\u817f\u90e8\u91cd\u91cf\u6700\u5927\u6bd4\u4f8b\u7684\u80a1\u9aa8\u6bb5\u5e94\u7528SIMP\u53d8\u5bc6\u5ea6\u62d3\u6251\u4f18\u5316\u65b9\u6cd5\uff0c\u57fa\u4e8e\u8fed\u4ee3\u8ba1\u7b97\u8fdb\u884c\u80a1\u9aa8\u4e8c\u6b21\u7ed3\u6784\u91cd\u6784\uff0c\u5e76\u4f7f\u7528ANSYS\u6709\u9650\u5143\u5206\u6790\u9a8c\u8bc1\u8bbe\u8ba1\u6027\u80fd\u3002", "result": "\u4f18\u5316\u540e\u80a1\u9aa8\u8d28\u91cf\u51cf\u8f7b19.45%\uff0c\u6574\u4f53\u817f\u90e8\u8d28\u91cf\u51cf\u5c117.92%\uff0c\u5b9e\u73b0\u4e86\u8f7b\u91cf\u5316\u8bbe\u8ba1\u76ee\u6807\u3002\u5bf9\u91cd\u6784\u540e\u7684\u817f\u90e8\u8fdb\u884c\u9759\u529b\u5b66\u548c\u6a21\u6001\u5206\u6790\uff0c\u7ed3\u679c\u8868\u660e\u4f18\u5316\u540e\u7684\u817f\u90e8\u4ecd\u6ee1\u8db3\u7ed3\u6784\u6027\u80fd\u8981\u6c42\uff0c\u9a8c\u8bc1\u4e86\u8f7b\u91cf\u5316\u8bbe\u8ba1\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u6210\u529f\u5b9e\u73b0\u4e86\u5efa\u7b51\u673a\u5668\u4eba\u817f\u90e8\u7ed3\u6784\u7684\u8f7b\u91cf\u5316\u4f18\u5316\u8bbe\u8ba1\uff0c\u5728\u4fdd\u8bc1\u7ed3\u6784\u6027\u80fd\u7684\u524d\u63d0\u4e0b\u663e\u8457\u51cf\u8f7b\u4e86\u91cd\u91cf\u3002\u8be5\u7814\u7a76\u4e3a\u8f7b\u91cf\u5316\u5efa\u7b51\u673a\u5668\u4eba\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u575a\u5b9e\u7684\u7406\u8bba\u548c\u6280\u672f\u652f\u6491\uff0c\u4e3a\u5176\u5728\u590d\u6742\u5efa\u7b51\u73af\u5883\u4e2d\u7684\u9ad8\u6548\u8fd0\u884c\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.16204", "categories": ["cs.AI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.16204", "abs": "https://arxiv.org/abs/2507.16204", "authors": ["Li-Hsiang Shen", "Jyun-Jhe Huang"], "title": "CHIMERA: Compressed Hybrid Intelligence for Twin-Model Enhanced Multi-Agent Deep Reinforcement Learning for Multi-Functional RIS-Assisted Space-Air-Ground Integrated Networks", "comment": null, "summary": "A space-air-ground integrated network (SAGIN) architecture is proposed,\nempowered by multi-functional reconfigurable intelligent surfaces (MF-RIS)\ncapable of simultaneously reflecting, amplifying, and harvesting wireless\nenergy. The MF-RIS plays a pivotal role in addressing the energy shortages of\nlow-Earth orbit (LEO) satellites operating in shadowed regions, while\nexplicitly accounting for both communication and computing energy consumption\nacross the SAGIN nodes. To maximize the long-term energy efficiency (EE), we\nformulate a joint optimization problem over the MF-RIS parameters, including\nsignal amplification, phase-shifts, energy harvesting ratio, and active element\nselection as well as the SAGIN parameters of beamforming vectors, high-altitude\nplatform station (HAPS) deployment, user association, and computing capability.\nThe formulated problem is highly non-convex and non-linear and contains mixed\ndiscrete-continuous parameters. To tackle this, we conceive a compressed hybrid\nintelligence for twin-model enhanced multi-agent deep reinforcement learning\n(CHIMERA) framework, which integrates semantic state-action compression and\nparametrized sharing under hybrid reinforcement learning to efficiently explore\nsuitable complex actions. The simulation results have demonstrated that the\nproposed CHIMERA scheme substantially outperforms the conventional benchmarks,\nincluding fixed-configuration or non-harvesting MF-RIS, traditional RIS, and\nno-RIS cases, as well as centralized and multi-agent deep reinforcement\nlearning baselines in terms of the highest EE. Moreover, the proposed\nSAGIN-MF-RIS architecture achieves superior EE performance due to its\ncomplementary coverage, offering notable advantages over either standalone\nsatellite, aerial, or ground-only deployments.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7531\u591a\u529f\u80fd\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762(MF-RIS)\u8d4b\u80fd\u7684\u5929-\u7a7a-\u5730\u4e00\u4f53\u5316\u7f51\u7edc\u67b6\u6784\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u7f51\u7edc\u53c2\u6570\u548cMF-RIS\u914d\u7f6e\u6765\u6700\u5927\u5316\u957f\u671f\u80fd\u6548\uff0c\u5e76\u8bbe\u8ba1\u4e86CHIMERA\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u6765\u89e3\u51b3\u8fd9\u4e00\u590d\u6742\u7684\u975e\u51f8\u4f18\u5316\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u4f4e\u8f68\u536b\u661f\u5728\u9634\u5f71\u533a\u57df\u7684\u80fd\u91cf\u77ed\u7f3a\u95ee\u9898\uff0c\u540c\u65f6\u8003\u8651\u5929-\u7a7a-\u5730\u4e00\u4f53\u5316\u7f51\u7edc\u4e2d\u901a\u4fe1\u548c\u8ba1\u7b97\u7684\u80fd\u8017\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u540c\u65f6\u53cd\u5c04\u3001\u653e\u5927\u548c\u6536\u96c6\u65e0\u7ebf\u80fd\u91cf\u7684\u667a\u80fd\u8868\u9762\u6280\u672f\u6765\u63d0\u5347\u7f51\u7edc\u7684\u957f\u671f\u80fd\u6548\u3002", "method": "\u63d0\u51fa\u4e86\u591a\u529f\u80fd\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762(MF-RIS)\u67b6\u6784\uff0c\u80fd\u591f\u540c\u65f6\u8fdb\u884c\u4fe1\u53f7\u53cd\u5c04\u3001\u653e\u5927\u548c\u80fd\u91cf\u6536\u96c6\uff1b\u8bbe\u8ba1\u4e86\u538b\u7f29\u6df7\u5408\u667a\u80fd\u53cc\u6a21\u578b\u589e\u5f3a\u591a\u667a\u80fd\u4f53\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60(CHIMERA)\u6846\u67b6\uff0c\u96c6\u6210\u4e86\u8bed\u4e49\u72b6\u6001-\u52a8\u4f5c\u538b\u7f29\u548c\u53c2\u6570\u5316\u5171\u4eab\u673a\u5236\u6765\u9ad8\u6548\u63a2\u7d22\u590d\u6742\u52a8\u4f5c\u7a7a\u95f4\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684CHIMERA\u65b9\u6848\u5728\u80fd\u6548\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u57fa\u51c6\u65b9\u6cd5\uff08\u5305\u62ec\u56fa\u5b9a\u914d\u7f6e\u6216\u975e\u80fd\u91cf\u6536\u96c6MF-RIS\u3001\u4f20\u7edfRIS\u3001\u65e0RIS\u60c5\u51b5\uff09\uff0c\u4ee5\u53ca\u96c6\u4e2d\u5f0f\u548c\u591a\u667a\u80fd\u4f53\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u57fa\u7ebf\u65b9\u6cd5\uff1bSAGIN-MF-RIS\u67b6\u6784\u7531\u4e8e\u5176\u4e92\u8865\u8986\u76d6\u7279\u6027\uff0c\u76f8\u6bd4\u5355\u72ec\u7684\u536b\u661f\u3001\u7a7a\u4e2d\u6216\u5730\u9762\u90e8\u7f72\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002", "conclusion": "\u63d0\u51fa\u7684\u5929-\u7a7a-\u5730\u4e00\u4f53\u5316\u7f51\u7edc\u4e0e\u591a\u529f\u80fd\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762\u76f8\u7ed3\u5408\u7684\u67b6\u6784\u80fd\u591f\u6709\u6548\u89e3\u51b3\u4f4e\u8f68\u536b\u661f\u80fd\u91cf\u77ed\u7f3a\u95ee\u9898\uff0c\u901a\u8fc7CHIMERA\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u5b9e\u73b0\u4e86\u7cfb\u7edf\u957f\u671f\u80fd\u6548\u7684\u6700\u5927\u5316\uff0c\u4e3a\u672a\u67656G\u7f51\u7edc\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u6280\u672f\u65b9\u6848\u3002"}}
{"id": "2507.16369", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.16369", "abs": "https://arxiv.org/abs/2507.16369", "authors": ["Thanh D V Nguyen", "Vincent Bonnet", "Pierre Fernbach", "David Daney", "Florent Lamiraux"], "title": "Humanoid Robot Whole-body Geometric Calibration with Embedded Sensors and a Single Plane", "comment": null, "summary": "Whole-body geometric calibration of humanoid robots using classical robot\ncalibration methods is a timeconsuming and experimentally burdensome task.\nHowever, despite its significance for accurate control and simulation, it is\noften overlooked in the humanoid robotics community. To address this issue, we\npropose a novel practical method that utilizes a single plane, embedded force\nsensors, and an admittance controller to calibrate the whole-body kinematics of\nhumanoids without requiring manual intervention. Given the complexity of\nhumanoid robots, it is crucial to generate and determine a minimal set of\noptimal calibration postures. To do so, we propose a new algorithm called IROC\n(Information Ranking algorithm for selecting Optimal Calibration postures).\nIROC requires a pool of feasible candidate postures to build a normalized\nweighted information matrix for each posture. Then, contrary to other\nalgorithms from the literature, IROC will determine the minimal number of\noptimal postures that are to be played onto a robot for its calibration. Both\nIROC and the single-plane calibration method were experimentally validated on a\nTALOS humanoid robot. The total whole-body kinematics chain was calibrated\nusing solely 31 optimal postures with 3-point contacts on a table by the robot\ngripper. In a cross-validation experiment, the average root-mean-square (RMS)\nerror was reduced by a factor of 2.3 compared to the manufacturer's model.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u5355\u5e73\u9762\u3001\u5d4c\u5165\u5f0f\u529b\u4f20\u611f\u5668\u548c\u5bfc\u7eb3\u63a7\u5236\u5668\u7684\u4eba\u5f62\u673a\u5668\u4eba\u5168\u8eab\u51e0\u4f55\u6807\u5b9a\u65b0\u65b9\u6cd5\uff0c\u65e0\u9700\u4eba\u5de5\u5e72\u9884\uff0c\u5e76\u5f00\u53d1\u4e86IROC\u7b97\u6cd5\u6765\u9009\u62e9\u6700\u5c11\u7684\u6700\u4f18\u6807\u5b9a\u59ff\u6001", "motivation": "\u4f20\u7edf\u7684\u4eba\u5f62\u673a\u5668\u4eba\u5168\u8eab\u51e0\u4f55\u6807\u5b9a\u65b9\u6cd5\u8017\u65f6\u957f\u3001\u5b9e\u9a8c\u8d1f\u62c5\u91cd\uff0c\u5c3d\u7ba1\u5bf9\u7cbe\u786e\u63a7\u5236\u548c\u4eff\u771f\u5f88\u91cd\u8981\uff0c\u4f46\u5728\u4eba\u5f62\u673a\u5668\u4eba\u793e\u533a\u4e2d\u7ecf\u5e38\u88ab\u5ffd\u89c6", "method": "\u63d0\u51fa\u4e86\u4f7f\u7528\u5355\u5e73\u9762\u3001\u5d4c\u5165\u5f0f\u529b\u4f20\u611f\u5668\u548c\u5bfc\u7eb3\u63a7\u5236\u5668\u7684\u5b9e\u7528\u6807\u5b9a\u65b9\u6cd5\uff0c\u4ee5\u53caIROC\uff08\u4fe1\u606f\u6392\u5e8f\u7b97\u6cd5\uff09\u6765\u9009\u62e9\u6700\u4f18\u6807\u5b9a\u59ff\u6001\u3002IROC\u901a\u8fc7\u4e3a\u6bcf\u4e2a\u59ff\u6001\u6784\u5efa\u5f52\u4e00\u5316\u52a0\u6743\u4fe1\u606f\u77e9\u9635\uff0c\u786e\u5b9a\u6700\u5c11\u6570\u91cf\u7684\u6700\u4f18\u59ff\u6001\u7528\u4e8e\u673a\u5668\u4eba\u6807\u5b9a", "result": "\u5728TALOS\u4eba\u5f62\u673a\u5668\u4eba\u4e0a\u9a8c\u8bc1\uff0c\u4ec5\u4f7f\u752831\u4e2a\u6700\u4f18\u59ff\u6001\u901a\u8fc7\u673a\u5668\u4eba\u6293\u624b\u5728\u684c\u9762\u4e0a\u76843\u70b9\u63a5\u89e6\u5b8c\u6210\u5168\u8eab\u8fd0\u52a8\u94fe\u6807\u5b9a\u3002\u4ea4\u53c9\u9a8c\u8bc1\u5b9e\u9a8c\u4e2d\uff0c\u5e73\u5747\u5747\u65b9\u6839\u8bef\u5dee\u6bd4\u5236\u9020\u5546\u6a21\u578b\u51cf\u5c11\u4e862.3\u500d", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5b9e\u73b0\u4e86\u4eba\u5f62\u673a\u5668\u4eba\u5168\u8eab\u8fd0\u52a8\u5b66\u7684\u9ad8\u6548\u81ea\u52a8\u5316\u6807\u5b9a\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6807\u5b9a\u7cbe\u5ea6\uff0c\u4e3a\u4eba\u5f62\u673a\u5668\u4eba\u7684\u7cbe\u786e\u63a7\u5236\u548c\u4eff\u771f\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848"}}
{"id": "2507.16226", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.16226", "abs": "https://arxiv.org/abs/2507.16226", "authors": ["Dong Ben", "Hui Feng", "Qian Wang"], "title": "Distilled Large Language Model in Confidential Computing Environment for System-on-Chip Design", "comment": "7 pages, 4 figures;", "summary": "Large Language Models (LLMs) are increasingly used in circuit design tasks\nand have typically undergone multiple rounds of training. Both the trained\nmodels and their associated training data are considered confidential\nintellectual property (IP) and must be protected from exposure. Confidential\nComputing offers a promising solution to protect data and models through\nTrusted Execution Environments (TEEs). However, existing TEE implementations\nare not designed to support the resource-intensive nature of LLMs efficiently.\nIn this work, we first present a comprehensive evaluation of the LLMs within a\nTEE-enabled confidential computing environment, specifically utilizing Intel\nTrust Domain Extensions (TDX). We constructed experiments on three\nenvironments: TEE-based, CPU-only, and CPU-GPU hybrid implementations, and\nevaluated their performance in terms of tokens per second.\n  Our first observation is that distilled models, i.e., DeepSeek, surpass other\nmodels in performance due to their smaller parameters, making them suitable for\nresource-constrained devices. Also, in the quantized models such as 4-bit\nquantization (Q4) and 8-bit quantization (Q8), we observed a performance gain\nof up to 3x compared to FP16 models. Our findings indicate that for fewer\nparameter sets, such as DeepSeek-r1-1.5B, the TDX implementation outperforms\nthe CPU version in executing computations within a secure environment. We\nfurther validate the results using a testbench designed for SoC design tasks.\nThese validations demonstrate the potential of efficiently deploying\nlightweight LLMs on resource-constrained systems for semiconductor CAD\napplications.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u53ef\u4fe1\u6267\u884c\u73af\u5883(TEE)\u4e2d\u90e8\u7f72\u5927\u8bed\u8a00\u6a21\u578b(LLM)\u7528\u4e8e\u7535\u8def\u8bbe\u8ba1\u4efb\u52a1\u7684\u6027\u80fd\u8bc4\u4f30\uff0c\u91cd\u70b9\u5173\u6ce8Intel TDX\u6280\u672f\u7684\u5e94\u7528\u6548\u679c\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7535\u8def\u8bbe\u8ba1\u4e2d\u5e94\u7528\u5e7f\u6cdb\uff0c\u4f46\u5176\u8bad\u7ec3\u6570\u636e\u548c\u6a21\u578b\u5c5e\u4e8e\u673a\u5bc6\u77e5\u8bc6\u4ea7\u6743\u9700\u8981\u4fdd\u62a4\u3002\u73b0\u6709TEE\u5b9e\u73b0\u4e0d\u80fd\u9ad8\u6548\u652f\u6301\u8d44\u6e90\u5bc6\u96c6\u578b\u7684LLM\uff0c\u56e0\u6b64\u9700\u8981\u8bc4\u4f30TEE\u73af\u5883\u4e0bLLM\u7684\u6027\u80fd\u8868\u73b0\u3002", "method": "\u6784\u5efa\u4e86\u4e09\u79cd\u5b9e\u9a8c\u73af\u5883\uff1a\u57fa\u4e8eTEE\u7684\u3001\u4ec5CPU\u7684\u548cCPU-GPU\u6df7\u5408\u5b9e\u73b0\uff0c\u4f7f\u7528Intel Trust Domain Extensions (TDX)\u6280\u672f\uff0c\u901a\u8fc7\u6bcf\u79d2token\u6570\u8bc4\u4f30\u6027\u80fd\u3002\u6d4b\u8bd5\u4e86\u4e0d\u540c\u6a21\u578b\u5305\u62ec\u84b8\u998f\u6a21\u578b\u548c\u91cf\u5316\u6a21\u578b\uff0c\u5e76\u4f7f\u7528SoC\u8bbe\u8ba1\u4efb\u52a1\u6d4b\u8bd5\u53f0\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u84b8\u998f\u6a21\u578b(\u5982DeepSeek)\u7531\u4e8e\u53c2\u6570\u8f83\u5c11\u5728\u6027\u80fd\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u9002\u5408\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u3002\u91cf\u5316\u6a21\u578b(4\u4f4d\u548c8\u4f4d\u91cf\u5316)\u76f8\u6bd4FP16\u6a21\u578b\u6027\u80fd\u63d0\u5347\u53ef\u8fbe3\u500d\u3002\u5bf9\u4e8e\u53c2\u6570\u8f83\u5c11\u7684\u6a21\u578b\u5982DeepSeek-r1-1.5B\uff0cTDX\u5b9e\u73b0\u5728\u5b89\u5168\u73af\u5883\u4e2d\u7684\u8ba1\u7b97\u6027\u80fd\u8d85\u8fc7CPU\u7248\u672c\u3002", "conclusion": "\u7814\u7a76\u9a8c\u8bc1\u4e86\u5728\u8d44\u6e90\u53d7\u9650\u7cfb\u7edf\u4e0a\u9ad8\u6548\u90e8\u7f72\u8f7b\u91cf\u7ea7LLM\u7528\u4e8e\u534a\u5bfc\u4f53CAD\u5e94\u7528\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u5728\u4fdd\u62a4\u77e5\u8bc6\u4ea7\u6743\u7684\u540c\u65f6\u5b9e\u73b0\u9ad8\u6027\u80fdLLM\u63a8\u7406\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.16382", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16382", "abs": "https://arxiv.org/abs/2507.16382", "authors": ["Chenhao Yao", "Zike Yuan", "Xiaoxu Liu", "Chi Zhu"], "title": "Application of LLM Guided Reinforcement Learning in Formation Control with Collision Avoidance", "comment": "Accepted by IROS 2025", "summary": "Multi-Agent Systems (MAS) excel at accomplishing complex objectives through\nthe collaborative efforts of individual agents. Among the methodologies\nemployed in MAS, Multi-Agent Reinforcement Learning (MARL) stands out as one of\nthe most efficacious algorithms. However, when confronted with the complex\nobjective of Formation Control with Collision Avoidance (FCCA): designing an\neffective reward function that facilitates swift convergence of the policy\nnetwork to an optimal solution. In this paper, we introduce a novel framework\nthat aims to overcome this challenge. By giving large language models (LLMs) on\nthe prioritization of tasks and the observable information available to each\nagent, our framework generates reward functions that can be dynamically\nadjusted online based on evaluation outcomes by employing more advanced\nevaluation metrics rather than the rewards themselves. This mechanism enables\nthe MAS to simultaneously achieve formation control and obstacle avoidance in\ndynamic environments with enhanced efficiency, requiring fewer iterations to\nreach superior performance levels. Our empirical studies, conducted in both\nsimulation and real-world settings, validate the practicality and effectiveness\nof our proposed approach.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u7f16\u961f\u63a7\u5236\u4e0e\u907f\u78b0\u95ee\u9898\u7684\u5956\u52b1\u51fd\u6570\u8bbe\u8ba1\u96be\u9898\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u5956\u52b1\u51fd\u6570\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u7b56\u7565\u5b66\u4e60\u3002", "motivation": "\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u5728\u5904\u7406\u7f16\u961f\u63a7\u5236\u4e0e\u907f\u78b0(FCCA)\u8fd9\u7c7b\u590d\u6742\u76ee\u6807\u65f6\uff0c\u9762\u4e34\u8bbe\u8ba1\u6709\u6548\u5956\u52b1\u51fd\u6570\u7684\u6311\u6218\uff0c\u96be\u4ee5\u4f7f\u7b56\u7565\u7f51\u7edc\u5feb\u901f\u6536\u655b\u5230\u6700\u4f18\u89e3\u3002\u4f20\u7edf\u65b9\u6cd5\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u6548\u7387\u4e0d\u9ad8\uff0c\u9700\u8981\u66f4\u591a\u8fed\u4ee3\u624d\u80fd\u8fbe\u5230\u7406\u60f3\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u8ba9LLM\u7406\u89e3\u4efb\u52a1\u4f18\u5148\u7ea7\u548c\u5404\u667a\u80fd\u4f53\u53ef\u89c2\u5bdf\u4fe1\u606f\uff0c\u751f\u6210\u53ef\u52a8\u6001\u8c03\u6574\u7684\u5956\u52b1\u51fd\u6570\u3002\u8be5\u6846\u67b6\u91c7\u7528\u66f4\u5148\u8fdb\u7684\u8bc4\u4f30\u6307\u6807\u800c\u975e\u5956\u52b1\u672c\u8eab\u6765\u5728\u7ebf\u8c03\u6574\u5956\u52b1\u51fd\u6570\uff0c\u4f7f\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u80fd\u591f\u540c\u65f6\u5b9e\u73b0\u7f16\u961f\u63a7\u5236\u548c\u969c\u788d\u7269\u907f\u514d\u3002", "result": "\u5b9e\u9a8c\u7814\u7a76\u5728\u4eff\u771f\u548c\u771f\u5b9e\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u5b9e\u7528\u6027\u548c\u6709\u6548\u6027\u3002\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u4ee5\u66f4\u5c11\u7684\u8fed\u4ee3\u6b21\u6570\u8fbe\u5230\u66f4\u4f18\u7684\u6027\u80fd\u6c34\u5e73\uff0c\u5728\u52a8\u6001\u73af\u5883\u4e2d\u540c\u65f6\u5b9e\u73b0\u7f16\u961f\u63a7\u5236\u548c\u907f\u78b0\u7684\u6548\u7387\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u52a8\u6001\u5956\u52b1\u51fd\u6570\u8c03\u6574\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u7f16\u961f\u63a7\u5236\u4e0e\u907f\u78b0\u7684\u5956\u52b1\u8bbe\u8ba1\u96be\u9898\uff0c\u63d0\u9ad8\u4e86\u5b66\u4e60\u6548\u7387\u548c\u6027\u80fd\uff0c\u5728\u4eff\u771f\u548c\u5b9e\u9645\u5e94\u7528\u4e2d\u90fd\u5c55\u73b0\u4e86\u826f\u597d\u7684\u5b9e\u7528\u6027\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2507.16229", "categories": ["cs.AI", "cs.CY", "cs.HC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.16229", "abs": "https://arxiv.org/abs/2507.16229", "authors": ["Bo Wen", "Chen Wang", "Qiwei Han", "Raquel Norel", "Julia Liu", "Thaddeus Stappenbeck", "Jeffrey L. Rogers"], "title": "Voice-based AI Agents: Filling the Economic Gaps in Digital Health Delivery", "comment": "IEEE International Conference on Digital Health (ICDH) 2025", "summary": "The integration of voice-based AI agents in healthcare presents a\ntransformative opportunity to bridge economic and accessibility gaps in digital\nhealth delivery. This paper explores the role of large language model\n(LLM)-powered voice assistants in enhancing preventive care and continuous\npatient monitoring, particularly in underserved populations. Drawing insights\nfrom the development and pilot study of Agent PULSE (Patient Understanding and\nLiaison Support Engine) -- a collaborative initiative between IBM Research,\nCleveland Clinic Foundation, and Morehouse School of Medicine -- we present an\neconomic model demonstrating how AI agents can provide cost-effective\nhealthcare services where human intervention is economically unfeasible. Our\npilot study with 33 inflammatory bowel disease patients revealed that 70\\%\nexpressed acceptance of AI-driven monitoring, with 37\\% preferring it over\ntraditional modalities. Technical challenges, including real-time\nconversational AI processing, integration with healthcare systems, and privacy\ncompliance, are analyzed alongside policy considerations surrounding\nregulation, bias mitigation, and patient autonomy. Our findings suggest that\nAI-driven voice agents not only enhance healthcare scalability and efficiency\nbut also improve patient engagement and accessibility. For healthcare\nexecutives, our cost-utility analysis demonstrates huge potential savings for\nroutine monitoring tasks, while technologists can leverage our framework to\nprioritize improvements yielding the highest patient impact. By addressing\ncurrent limitations and aligning AI development with ethical and regulatory\nframeworks, voice-based AI agents can serve as a critical entry point for\nequitable, sustainable digital healthcare solutions.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7Agent PULSE\u8bd5\u70b9\u9879\u76ee\uff0c\u5c55\u793a\u4e86\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u97f3AI\u52a9\u624b\u5728\u533b\u7597\u4fdd\u5065\u4e2d\u7684\u5e94\u7528\uff0c\u7279\u522b\u662f\u5728\u670d\u52a1\u4e0d\u8db3\u4eba\u7fa4\u7684\u9884\u9632\u6027\u62a4\u7406\u548c\u6301\u7eed\u60a3\u8005\u76d1\u6d4b\u65b9\u9762\u7684\u5de8\u5927\u6f5c\u529b\u548c\u7ecf\u6d4e\u6548\u76ca\u3002", "motivation": "\u89e3\u51b3\u6570\u5b57\u533b\u7597\u670d\u52a1\u4e2d\u7684\u7ecf\u6d4e\u548c\u53ef\u53ca\u6027\u5dee\u8ddd\uff0c\u7279\u522b\u662f\u5728\u4f20\u7edf\u4eba\u5de5\u5e72\u9884\u7ecf\u6d4e\u4e0a\u4e0d\u53ef\u884c\u7684\u60c5\u51b5\u4e0b\uff0c\u4e3a\u670d\u52a1\u4e0d\u8db3\u7684\u4eba\u7fa4\u63d0\u4f9b\u6210\u672c\u6548\u76ca\u9ad8\u7684\u533b\u7597\u4fdd\u5065\u670d\u52a1\u3002", "method": "\u5f00\u53d1\u5e76\u8bd5\u70b9\u6d4b\u8bd5Agent PULSE\uff08\u60a3\u8005\u7406\u89e3\u548c\u8054\u7edc\u652f\u6301\u5f15\u64ce\uff09\u2014\u2014\u4e00\u4e2a\u7531IBM\u7814\u7a76\u9662\u3001\u514b\u5229\u592b\u5170\u8bca\u6240\u57fa\u91d1\u4f1a\u548c\u6469\u5c14\u8c6a\u65af\u533b\u5b66\u9662\u5408\u4f5c\u5f00\u53d1\u7684\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bed\u97f3AI\u52a9\u624b\u7cfb\u7edf\uff0c\u5e76\u5efa\u7acb\u7ecf\u6d4e\u6a21\u578b\u5206\u6790\u5176\u6210\u672c\u6548\u76ca\u3002", "result": "\u572833\u540d\u708e\u75c7\u6027\u80a0\u75c5\u60a3\u8005\u7684\u8bd5\u70b9\u7814\u7a76\u4e2d\uff0c70%\u7684\u60a3\u8005\u8868\u793a\u63a5\u53d7AI\u9a71\u52a8\u7684\u76d1\u6d4b\uff0c37%\u7684\u60a3\u8005\u66f4\u504f\u597dAI\u76d1\u6d4b\u800c\u975e\u4f20\u7edf\u65b9\u5f0f\u3002\u6210\u672c\u6548\u7528\u5206\u6790\u663e\u793a\u5728\u5e38\u89c4\u76d1\u6d4b\u4efb\u52a1\u4e2d\u5177\u6709\u5de8\u5927\u7684\u6f5c\u5728\u8282\u7ea6\u6548\u679c\u3002", "conclusion": "\u57fa\u4e8e\u8bed\u97f3\u7684AI\u4ee3\u7406\u4e0d\u4ec5\u80fd\u591f\u63d0\u9ad8\u533b\u7597\u4fdd\u5065\u7684\u53ef\u6269\u5c55\u6027\u548c\u6548\u7387\uff0c\u8fd8\u80fd\u6539\u5584\u60a3\u8005\u53c2\u4e0e\u5ea6\u548c\u53ef\u53ca\u6027\u3002\u901a\u8fc7\u89e3\u51b3\u5f53\u524d\u9650\u5236\u5e76\u4f7fAI\u53d1\u5c55\u4e0e\u4f26\u7406\u548c\u76d1\u7ba1\u6846\u67b6\u4fdd\u6301\u4e00\u81f4\uff0c\u8bed\u97f3AI\u4ee3\u7406\u53ef\u4ee5\u6210\u4e3a\u516c\u5e73\u3001\u53ef\u6301\u7eed\u6570\u5b57\u533b\u7597\u89e3\u51b3\u65b9\u6848\u7684\u5173\u952e\u5165\u53e3\u70b9\u3002"}}
{"id": "2507.16398", "categories": ["cs.RO", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.16398", "abs": "https://arxiv.org/abs/2507.16398", "authors": ["Lavinia Hriscu", "Alberto Sanfeliu", "Anais Garrell"], "title": "AI or Human? Understanding Perceptions of Embodied Robots with LLMs", "comment": null, "summary": "The pursuit of artificial intelligence has long been associated to the the\nchallenge of effectively measuring intelligence. Even if the Turing Test was\nintroduced as a means of assessing a system intelligence, its relevance and\napplication within the field of human-robot interaction remain largely\nunderexplored. This study investigates the perception of intelligence in\nembodied robots by performing a Turing Test within a robotic platform. A total\nof 34 participants were tasked with distinguishing between AI- and\nhuman-operated robots while engaging in two interactive tasks: an information\nretrieval and a package handover. These tasks assessed the robot perception and\nnavigation abilities under both static and dynamic conditions. Results indicate\nthat participants were unable to reliably differentiate between AI- and\nhuman-controlled robots beyond chance levels. Furthermore, analysis of\nparticipant responses reveals key factors influencing the perception of\nartificial versus human intelligence in embodied robotic systems. These\nfindings provide insights into the design of future interactive robots and\ncontribute to the ongoing discourse on intelligence assessment in AI-driven\nsystems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u5728\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u5b9e\u65bd\u56fe\u7075\u6d4b\u8bd5\uff0c\u63a2\u8ba8\u4e86\u5177\u8eab\u673a\u5668\u4eba\u7684\u667a\u80fd\u611f\u77e5\u95ee\u9898\u300234\u540d\u53c2\u4e0e\u8005\u5728\u4fe1\u606f\u68c0\u7d22\u548c\u5305\u88f9\u4ea4\u63a5\u4e24\u4e2a\u4ea4\u4e92\u4efb\u52a1\u4e2d\u65e0\u6cd5\u53ef\u9760\u533a\u5206AI\u63a7\u5236\u548c\u4eba\u7c7b\u63a7\u5236\u7684\u673a\u5668\u4eba\uff0c\u4e3a\u672a\u6765\u4ea4\u4e92\u673a\u5668\u4eba\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002", "motivation": "\u56fe\u7075\u6d4b\u8bd5\u4f5c\u4e3a\u8bc4\u4f30\u7cfb\u7edf\u667a\u80fd\u7684\u624b\u6bb5\uff0c\u5176\u5728\u4eba\u673a\u4ea4\u4e92\u9886\u57df\u7684\u76f8\u5173\u6027\u548c\u5e94\u7528\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5177\u8eab\u673a\u5668\u4eba\u5e73\u53f0\u4e0a\u7684\u56fe\u7075\u6d4b\u8bd5\u6765\u8c03\u67e5\u4eba\u4eec\u5bf9\u673a\u5668\u4eba\u667a\u80fd\u7684\u611f\u77e5\u3002", "method": "\u62db\u52df34\u540d\u53c2\u4e0e\u8005\uff0c\u8ba9\u4ed6\u4eec\u5728\u4e24\u4e2a\u4ea4\u4e92\u4efb\u52a1\uff08\u4fe1\u606f\u68c0\u7d22\u548c\u5305\u88f9\u4ea4\u63a5\uff09\u4e2d\u533a\u5206AI\u63a7\u5236\u548c\u4eba\u7c7b\u63a7\u5236\u7684\u673a\u5668\u4eba\u3002\u8fd9\u4e9b\u4efb\u52a1\u5728\u9759\u6001\u548c\u52a8\u6001\u6761\u4ef6\u4e0b\u8bc4\u4f30\u673a\u5668\u4eba\u7684\u611f\u77e5\u548c\u5bfc\u822a\u80fd\u529b\u3002", "result": "\u53c2\u4e0e\u8005\u65e0\u6cd5\u53ef\u9760\u5730\u533a\u5206AI\u63a7\u5236\u548c\u4eba\u7c7b\u63a7\u5236\u7684\u673a\u5668\u4eba\uff0c\u533a\u5206\u51c6\u786e\u7387\u4ec5\u4e3a\u968f\u673a\u6c34\u5e73\u3002\u901a\u8fc7\u5206\u6790\u53c2\u4e0e\u8005\u53cd\u5e94\uff0c\u8bc6\u522b\u51fa\u5f71\u54cd\u5177\u8eab\u673a\u5668\u4eba\u7cfb\u7edf\u4e2d\u4eba\u5de5\u667a\u80fd\u4e0e\u4eba\u7c7b\u667a\u80fd\u611f\u77e5\u7684\u5173\u952e\u56e0\u7d20\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u672a\u6765\u4ea4\u4e92\u673a\u5668\u4eba\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u89c1\u89e3\uff0c\u5e76\u4e3aAI\u9a71\u52a8\u7cfb\u7edf\u4e2d\u667a\u80fd\u8bc4\u4f30\u7684\u6301\u7eed\u8ba8\u8bba\u505a\u51fa\u4e86\u8d21\u732e\u3002\u8868\u660e\u5f53\u524dAI\u6280\u672f\u5728\u5177\u8eab\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u5df2\u8fbe\u5230\u8f83\u9ad8\u7684\u62df\u4eba\u5316\u6c34\u5e73\u3002"}}
{"id": "2507.16280", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16280", "abs": "https://arxiv.org/abs/2507.16280", "authors": ["Tianze Xu", "Pengrui Lu", "Lyumanshan Ye", "Xiangkun Hu", "Pengfei Liu"], "title": "ResearcherBench: Evaluating Deep AI Research Systems on the Frontiers of Scientific Inquiry", "comment": "22 pages, 3 figures", "summary": "The emergence of deep research systems presents significant capabilities in\nproblem-solving, extending from basic queries to sophisticated research tasks.\nHowever, existing benchmarks primarily evaluate these systems as agents for web\nretrieval and report generation, overlooking their potential to discover novel\ninsights on the frontiers of scientific research. To address this gap, we\nintroduce ResearcherBench, the first benchmark focused on evaluating the\ncapabilities of these advanced, agentic systems - which we refer to as Deep AI\nResearch Systems (DARS) - on frontier AI scientific questions. We compiled a\ndataset of 65 research questions expertly selected from real-world scientific\nscenarios such as laboratory discussions and interviews, spanning 35 different\nAI subjects and categorized into three types: technical details, literature\nreview, and open consulting. Our dual evaluation framework combines rubric\nassessment, which uses expert-designed criteria to evaluate insight quality,\nwith factual assessment, which measures citation accuracy (faithfulness) and\ncoverage (groundedness). We evaluated several leading commercial DARS and\nbaseline systems. Results show that OpenAI Deep Research and Gemini Deep\nResearch significantly outperform other systems, with particular strength in\nopen-ended consulting questions. Such capabilities represent a meaningful step\ntoward AI self-improvement, aligning with the vision of ASI for AI. We\nopen-source ResearcherBench to provide a standardized platform for promoting\nthe development of next-generation AI research assistants, hoping to foster a\nnew perspective in AI research evaluation for a novel pattern of scientific\ncollaboration: https://github.com/GAIR-NLP/ResearcherBench.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86ResearcherBench\uff0c\u8fd9\u662f\u9996\u4e2a\u4e13\u95e8\u8bc4\u4f30\u6df1\u5ea6AI\u7814\u7a76\u7cfb\u7edf(DARS)\u5728\u524d\u6cbfAI\u79d1\u5b66\u95ee\u9898\u4e0a\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b65\u4e2a\u7814\u7a76\u95ee\u9898\uff0c\u91c7\u7528\u53cc\u91cd\u8bc4\u4f30\u6846\u67b6\uff0c\u7ed3\u679c\u663e\u793aOpenAI Deep Research\u548cGemini Deep Research\u8868\u73b0\u6700\u4f73\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u8bc4\u4f30AI\u7cfb\u7edf\u4f5c\u4e3a\u7f51\u7edc\u68c0\u7d22\u548c\u62a5\u544a\u751f\u6210\u4ee3\u7406\u7684\u80fd\u529b\uff0c\u5ffd\u7565\u4e86\u5b83\u4eec\u5728\u79d1\u5b66\u7814\u7a76\u524d\u6cbf\u53d1\u73b0\u65b0\u89c1\u89e3\u7684\u6f5c\u529b\u3002\u4e3a\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u9700\u8981\u4e13\u95e8\u8bc4\u4f30\u6df1\u5ea6AI\u7814\u7a76\u7cfb\u7edf\u5728\u524d\u6cbfAI\u79d1\u5b66\u95ee\u9898\u4e0a\u7684\u80fd\u529b\u3002", "method": "\u6784\u5efa\u5305\u542b65\u4e2a\u7814\u7a76\u95ee\u9898\u7684\u6570\u636e\u96c6\uff0c\u8fd9\u4e9b\u95ee\u9898\u6765\u81ea\u771f\u5b9e\u79d1\u5b66\u573a\u666f\uff08\u5982\u5b9e\u9a8c\u5ba4\u8ba8\u8bba\u548c\u8bbf\u8c08\uff09\uff0c\u6db5\u76d635\u4e2a\u4e0d\u540cAI\u4e3b\u9898\uff0c\u5206\u4e3a\u6280\u672f\u7ec6\u8282\u3001\u6587\u732e\u7efc\u8ff0\u548c\u5f00\u653e\u54a8\u8be2\u4e09\u7c7b\u3002\u91c7\u7528\u53cc\u91cd\u8bc4\u4f30\u6846\u67b6\uff1a\u4f7f\u7528\u4e13\u5bb6\u8bbe\u8ba1\u6807\u51c6\u8bc4\u4f30\u6d1e\u5bdf\u8d28\u91cf\u7684\u91cf\u89c4\u8bc4\u4f30\uff0c\u4ee5\u53ca\u6d4b\u91cf\u5f15\u7528\u51c6\u786e\u6027\u548c\u8986\u76d6\u5ea6\u7684\u4e8b\u5b9e\u8bc4\u4f30\u3002", "result": "\u8bc4\u4f30\u4e86\u591a\u4e2a\u9886\u5148\u7684\u5546\u4e1aDARS\u548c\u57fa\u7ebf\u7cfb\u7edf\u3002\u7ed3\u679c\u663e\u793aOpenAI Deep Research\u548cGemini Deep Research\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u7cfb\u7edf\uff0c\u5728\u5f00\u653e\u5f0f\u54a8\u8be2\u95ee\u9898\u4e0a\u8868\u73b0\u5c24\u4e3a\u7a81\u51fa\u3002\u8fd9\u4e9b\u80fd\u529b\u4ee3\u8868\u4e86AI\u81ea\u6211\u6539\u8fdb\u7684\u91cd\u8981\u8fdb\u6b65\u3002", "conclusion": "ResearcherBench\u4e3a\u8bc4\u4f30\u4e0b\u4e00\u4ee3AI\u7814\u7a76\u52a9\u624b\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u5e73\u53f0\uff0c\u6709\u671b\u4fc3\u8fdbAI\u7814\u7a76\u8bc4\u4f30\u7684\u65b0\u89c6\u89d2\uff0c\u63a8\u52a8\u4eba\u5de5\u667a\u80fd\u4e0e\u79d1\u5b66\u7814\u7a76\u534f\u4f5c\u7684\u65b0\u6a21\u5f0f\u53d1\u5c55\uff0c\u5e76\u5c06\u57fa\u51c6\u5f00\u6e90\u4ee5\u4fc3\u8fdb\u76f8\u5173\u9886\u57df\u53d1\u5c55\u3002"}}
{"id": "2507.16458", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.16458", "abs": "https://arxiv.org/abs/2507.16458", "authors": ["Yang Xu", "Jes\u00fas Bautista", "Jos\u00e9 Hinojosa", "H\u00e9ctor Garc\u00eda de Marina"], "title": "Distributed Oscillatory Guidance for Formation Flight of Fixed-Wing Drones", "comment": "Yang Xu and Jes\\'us Bautista contributed equally to this work. In the\n  proceedings of the IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS) 2025", "summary": "The autonomous formation flight of fixed-wing drones is hard when the\ncoordination requires the actuation over their speeds since they are critically\nbounded and aircraft are mostly designed to fly at a nominal airspeed. This\npaper proposes an algorithm to achieve formation flights of fixed-wing drones\nwithout requiring any actuation over their speed. In particular, we guide all\nthe drones to travel over specific paths, e.g., parallel straight lines, and we\nsuperpose an oscillatory behavior onto the guiding vector field that drives the\ndrones to the paths. This oscillation enables control over the average velocity\nalong the path, thereby facilitating inter-drone coordination. Each drone\nadjusts its oscillation amplitude distributively in a closed-loop manner by\ncommunicating with neighboring agents in an undirected and connected graph. A\nnovel consensus algorithm is introduced, leveraging a non-negative, asymmetric\nsaturation function. This unconventional saturation is justified since negative\namplitudes do not make drones travel backward or have a negative velocity along\nthe path. Rigorous theoretical analysis of the algorithm is complemented by\nvalidation through numerical simulations and a real-world formation flight.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u56fa\u5b9a\u7ffc\u65e0\u4eba\u673a\u7f16\u961f\u98de\u884c\u7b97\u6cd5\uff0c\u901a\u8fc7\u5728\u5f15\u5bfc\u5411\u91cf\u573a\u4e0a\u53e0\u52a0\u632f\u8361\u884c\u4e3a\u6765\u63a7\u5236\u5e73\u5747\u901f\u5ea6\uff0c\u5b9e\u73b0\u65e0\u9700\u8c03\u8282\u98de\u884c\u901f\u5ea6\u7684\u7f16\u961f\u534f\u8c03\u63a7\u5236", "motivation": "\u56fa\u5b9a\u7ffc\u65e0\u4eba\u673a\u7684\u901f\u5ea6\u63a7\u5236\u53d7\u5230\u4e25\u683c\u9650\u5236\uff0c\u4e14\u5927\u591a\u8bbe\u8ba1\u4e3a\u5728\u6807\u79f0\u7a7a\u901f\u4e0b\u98de\u884c\uff0c\u8fd9\u4f7f\u5f97\u9700\u8981\u901f\u5ea6\u8c03\u8282\u7684\u7f16\u961f\u98de\u884c\u53d8\u5f97\u56f0\u96be", "method": "\u63d0\u51fa\u5f15\u5bfc\u6240\u6709\u65e0\u4eba\u673a\u6cbf\u7279\u5b9a\u8def\u5f84\uff08\u5982\u5e73\u884c\u76f4\u7ebf\uff09\u98de\u884c\uff0c\u5728\u5f15\u5bfc\u5411\u91cf\u573a\u4e0a\u53e0\u52a0\u632f\u8361\u884c\u4e3a\u6765\u63a7\u5236\u6cbf\u8def\u5f84\u7684\u5e73\u5747\u901f\u5ea6\uff1b\u6bcf\u67b6\u65e0\u4eba\u673a\u901a\u8fc7\u4e0e\u90bb\u8fd1\u4ee3\u7406\u901a\u4fe1\uff0c\u4ee5\u5206\u5e03\u5f0f\u95ed\u73af\u65b9\u5f0f\u8c03\u8282\u5176\u632f\u8361\u5e45\u5ea6\uff1b\u5f15\u5165\u91c7\u7528\u975e\u8d1f\u975e\u5bf9\u79f0\u9971\u548c\u51fd\u6570\u7684\u65b0\u578b\u4e00\u81f4\u6027\u7b97\u6cd5", "result": "\u901a\u8fc7\u6570\u503c\u4eff\u771f\u548c\u771f\u5b9e\u4e16\u754c\u7f16\u961f\u98de\u884c\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u7684\u6709\u6548\u6027\uff0c\u5b9e\u73b0\u4e86\u65e0\u9700\u901f\u5ea6\u8c03\u8282\u7684\u56fa\u5b9a\u7ffc\u65e0\u4eba\u673a\u7f16\u961f\u98de\u884c", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8e\u632f\u8361\u884c\u4e3a\u7684\u7f16\u961f\u63a7\u5236\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u89e3\u51b3\u56fa\u5b9a\u7ffc\u65e0\u4eba\u673a\u7f16\u961f\u98de\u884c\u4e2d\u7684\u901f\u5ea6\u7ea6\u675f\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2507.16296", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16296", "abs": "https://arxiv.org/abs/2507.16296", "authors": ["Cairong Zhao", "Yufeng Jin", "Zifan Song", "Haonan Chen", "Duoqian Miao", "Guosheng Hu"], "title": "Cross-Modal Distillation For Widely Differing Modalities", "comment": "14 pages, 9 figures", "summary": "Deep learning achieved great progress recently, however, it is not easy or\nefficient to further improve its performance by increasing the size of the\nmodel. Multi-modal learning can mitigate this challenge by introducing richer\nand more discriminative information as input. To solve the problem of limited\naccess to multi-modal data at the time of use, we conduct multi-modal learning\nby introducing a teacher model to transfer discriminative knowledge to a\nstudent model during training. However, this knowledge transfer via\ndistillation is not trivial because the big domain gap between the widely\ndiffering modalities can easily lead to overfitting. In this work, we introduce\na cross-modal distillation framework. Specifically, we find hard constrained\nloss, e.g. l2 loss forcing the student being exact the same as the teacher, can\neasily lead to overfitting in cross-modality distillation. To address this, we\npropose two soft constrained knowledge distillation strategies at the feature\nlevel and classifier level respectively. In addition, we propose a\nquality-based adaptive weights module to weigh input samples via quantified\ndata quality, leading to robust model training. We conducted experiments on\nspeaker recognition and image classification tasks, and the results show that\nour approach is able to effectively achieve knowledge transfer between the\ncommonly used and widely differing modalities of image, text, and speech.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u8de8\u6a21\u6001\u77e5\u8bc6\u84b8\u998f\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u6559\u5e08\u6a21\u578b\u5c06\u5224\u522b\u6027\u77e5\u8bc6\u8f6c\u79fb\u7ed9\u5b66\u751f\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u6570\u636e\u5728\u4f7f\u7528\u65f6\u83b7\u53d6\u56f0\u96be\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u8f6f\u7ea6\u675f\u77e5\u8bc6\u84b8\u998f\u7b56\u7565\u548c\u57fa\u4e8e\u8d28\u91cf\u7684\u81ea\u9002\u5e94\u6743\u91cd\u6a21\u5757\u6765\u907f\u514d\u8fc7\u62df\u5408\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u901a\u8fc7\u589e\u52a0\u6a21\u578b\u5927\u5c0f\u6765\u63d0\u5347\u6027\u80fd\u53d8\u5f97\u56f0\u96be\u4e14\u4f4e\u6548\uff0c\u591a\u6a21\u6001\u5b66\u4e60\u53ef\u4ee5\u901a\u8fc7\u5f15\u5165\u66f4\u4e30\u5bcc\u548c\u66f4\u5177\u5224\u522b\u6027\u7684\u4fe1\u606f\u6765\u7f13\u89e3\u8fd9\u4e00\u6311\u6218\u3002\u7136\u800c\uff0c\u5728\u4f7f\u7528\u65f6\u83b7\u53d6\u591a\u6a21\u6001\u6570\u636e\u7684\u9014\u5f84\u6709\u9650\uff0c\u4e14\u4e0d\u540c\u6a21\u6001\u95f4\u7684\u5de8\u5927\u9886\u57df\u5dee\u8ddd\u5bb9\u6613\u5bfc\u81f4\u8fc7\u62df\u5408\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u8de8\u6a21\u6001\u84b8\u998f\u6846\u67b6\uff0c\u5305\u62ec\uff1a1\uff09\u5728\u7279\u5f81\u5c42\u548c\u5206\u7c7b\u5668\u5c42\u5206\u522b\u63d0\u51fa\u4e24\u79cd\u8f6f\u7ea6\u675f\u77e5\u8bc6\u84b8\u998f\u7b56\u7565\uff0c\u907f\u514d\u786c\u7ea6\u675f\u635f\u5931\uff08\u5982L2\u635f\u5931\uff09\u5bfc\u81f4\u7684\u8fc7\u62df\u5408\uff1b2\uff09\u63d0\u51fa\u57fa\u4e8e\u8d28\u91cf\u7684\u81ea\u9002\u5e94\u6743\u91cd\u6a21\u5757\uff0c\u901a\u8fc7\u91cf\u5316\u6570\u636e\u8d28\u91cf\u6765\u5bf9\u8f93\u5165\u6837\u672c\u8fdb\u884c\u52a0\u6743\uff0c\u5b9e\u73b0\u9c81\u68d2\u7684\u6a21\u578b\u8bad\u7ec3\u3002", "result": "\u5728\u8bf4\u8bdd\u4eba\u8bc6\u522b\u548c\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u5728\u56fe\u50cf\u3001\u6587\u672c\u548c\u8bed\u97f3\u8fd9\u4e9b\u5e38\u7528\u4e14\u5dee\u5f02\u8f83\u5927\u7684\u6a21\u6001\u4e4b\u95f4\u5b9e\u73b0\u77e5\u8bc6\u8f6c\u79fb\u3002", "conclusion": "\u901a\u8fc7\u8f6f\u7ea6\u675f\u77e5\u8bc6\u84b8\u998f\u7b56\u7565\u548c\u81ea\u9002\u5e94\u6743\u91cd\u6a21\u5757\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u8de8\u6a21\u6001\u77e5\u8bc6\u84b8\u998f\u4e2d\u7684\u8fc7\u62df\u5408\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u4e0d\u540c\u6a21\u6001\u95f4\u7684\u6709\u6548\u77e5\u8bc6\u8f6c\u79fb\uff0c\u4e3a\u591a\u6a21\u6001\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.16480", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.ET", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.16480", "abs": "https://arxiv.org/abs/2507.16480", "authors": ["Sabrina Livanec", "Laura Londo\u00f1o", "Michael Gorki", "Adrian R\u00f6fer", "Abhinav Valada", "Andrea Kiesel"], "title": "Designing for Difference: How Human Characteristics Shape Perceptions of Collaborative Robots", "comment": null, "summary": "The development of assistive robots for social collaboration raises critical\nquestions about responsible and inclusive design, especially when interacting\nwith individuals from protected groups such as those with disabilities or\nadvanced age. Currently, research is scarce on how participants assess varying\nrobot behaviors in combination with diverse human needs, likely since\nparticipants have limited real-world experience with advanced domestic robots.\nIn the current study, we aim to address this gap while using methods that\nenable participants to assess robot behavior, as well as methods that support\nmeaningful reflection despite limited experience. In an online study, 112\nparticipants (from both experimental and control groups) evaluated 7 videos\nfrom a total of 28 variations of human-robot collaboration types. The\nexperimental group first completed a cognitive-affective mapping (CAM) exercise\non human-robot collaboration before providing their ratings. Although CAM\nreflection did not significantly affect overall ratings, it led to more\npronounced assessments for certain combinations of robot behavior and human\ncondition. Most importantly, the type of human-robot collaboration influences\nthe assessment. Antisocial robot behavior was consistently rated as the lowest,\nwhile collaboration with aged individuals elicited more sensitive evaluations.\nScenarios involving object handovers were viewed more positively than those\nwithout them. These findings suggest that both human characteristics and\ninteraction paradigms influence the perceived acceptability of collaborative\nrobots, underscoring the importance of prosocial design. They also highlight\nthe potential of reflective methods, such as CAM, to elicit nuanced feedback,\nsupporting the development of user-centered and socially responsible robotic\nsystems tailored to diverse populations.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u5728\u7ebf\u5b9e\u9a8c\u8bc4\u4f30\u4e86112\u540d\u53c2\u4e0e\u8005\u5bf9\u4e0d\u540c\u4eba\u673a\u534f\u4f5c\u573a\u666f\u7684\u770b\u6cd5\uff0c\u53d1\u73b0\u53cd\u793e\u4f1a\u673a\u5668\u4eba\u884c\u4e3a\u8bc4\u5206\u6700\u4f4e\uff0c\u4e0e\u8001\u5e74\u4eba\u7684\u534f\u4f5c\u5f15\u53d1\u66f4\u654f\u611f\u7684\u8bc4\u4f30\uff0c\u7269\u4f53\u4f20\u9012\u573a\u666f\u6bd4\u65e0\u4f20\u9012\u573a\u666f\u8bc4\u4ef7\u66f4\u79ef\u6781\uff0c\u5f3a\u8c03\u4e86\u4eb2\u793e\u4f1a\u8bbe\u8ba1\u7684\u91cd\u8981\u6027", "motivation": "\u76ee\u524d\u5173\u4e8e\u53c2\u4e0e\u8005\u5982\u4f55\u8bc4\u4f30\u4e0d\u540c\u673a\u5668\u4eba\u884c\u4e3a\u4e0e\u591a\u6837\u5316\u4eba\u7c7b\u9700\u6c42\u7ed3\u5408\u7684\u7814\u7a76\u7a00\u7f3a\uff0c\u7279\u522b\u662f\u9488\u5bf9\u6b8b\u75be\u4eba\u6216\u9ad8\u9f84\u7b49\u53d7\u4fdd\u62a4\u7fa4\u4f53\u7684\u8f85\u52a9\u673a\u5668\u4eba\u793e\u4f1a\u534f\u4f5c\u8bbe\u8ba1\u7f3a\u4e4f\u8d1f\u8d23\u4efb\u548c\u5305\u5bb9\u6027\u7684\u7814\u7a76", "method": "\u91c7\u7528\u5728\u7ebf\u7814\u7a76\u65b9\u6cd5\uff0c112\u540d\u53c2\u4e0e\u8005\uff08\u5b9e\u9a8c\u7ec4\u548c\u5bf9\u7167\u7ec4\uff09\u8bc4\u4f30\u4e8628\u79cd\u4eba\u673a\u534f\u4f5c\u7c7b\u578b\u53d8\u5316\u4e2d\u76847\u4e2a\u89c6\u9891\u3002\u5b9e\u9a8c\u7ec4\u5728\u8bc4\u5206\u524d\u5148\u5b8c\u6210\u8ba4\u77e5-\u60c5\u611f\u6620\u5c04\uff08CAM\uff09\u7ec3\u4e60\uff0c\u5bf9\u7167\u7ec4\u76f4\u63a5\u8bc4\u5206", "result": "CAM\u53cd\u601d\u867d\u672a\u663e\u8457\u5f71\u54cd\u6574\u4f53\u8bc4\u5206\uff0c\u4f46\u5bf9\u7279\u5b9a\u673a\u5668\u4eba\u884c\u4e3a\u548c\u4eba\u7c7b\u6761\u4ef6\u7ec4\u5408\u4ea7\u751f\u4e86\u66f4\u660e\u663e\u7684\u8bc4\u4f30\u3002\u53cd\u793e\u4f1a\u673a\u5668\u4eba\u884c\u4e3a\u8bc4\u5206\u4e00\u81f4\u6700\u4f4e\uff0c\u4e0e\u8001\u5e74\u4eba\u534f\u4f5c\u5f15\u53d1\u66f4\u654f\u611f\u8bc4\u4ef7\uff0c\u6d89\u53ca\u7269\u4f53\u4f20\u9012\u7684\u573a\u666f\u6bd4\u65e0\u4f20\u9012\u573a\u666f\u8bc4\u4ef7\u66f4\u79ef\u6781", "conclusion": "\u4eba\u7c7b\u7279\u5f81\u548c\u4ea4\u4e92\u8303\u5f0f\u90fd\u4f1a\u5f71\u54cd\u534f\u4f5c\u673a\u5668\u4eba\u7684\u53ef\u63a5\u53d7\u6027\u611f\u77e5\uff0c\u5f3a\u8c03\u4e86\u4eb2\u793e\u4f1a\u8bbe\u8ba1\u7684\u91cd\u8981\u6027\u3002CAM\u7b49\u53cd\u601d\u65b9\u6cd5\u6709\u6f5c\u529b\u5f15\u53d1\u7ec6\u81f4\u5165\u5fae\u7684\u53cd\u9988\uff0c\u652f\u6301\u5f00\u53d1\u9762\u5411\u7528\u6237\u3001\u793e\u4f1a\u8d1f\u8d23\u4e14\u9002\u5e94\u4e0d\u540c\u4eba\u7fa4\u7684\u673a\u5668\u4eba\u7cfb\u7edf"}}
{"id": "2507.16322", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16322", "abs": "https://arxiv.org/abs/2507.16322", "authors": ["Fred Mutisya", "Shikoh Gitau", "Christine Syovata", "Diana Oigara", "Ibrahim Matende", "Muna Aden", "Munira Ali", "Ryan Nyotu", "Diana Marion", "Job Nyangena", "Nasubo Ongoma", "Keith Mbae", "Elizabeth Wamicha", "Eric Mibuari", "Jean Philbert Nsengemana", "Talkmore Chidede"], "title": "Mind the Gap: Evaluating the Representativeness of Quantitative Medical Language Reasoning LLM Benchmarks for African Disease Burdens", "comment": "Preprint. 26 pages, includes appendix and tables", "summary": "Introduction: Existing medical LLM benchmarks largely reflect examination\nsyllabi and disease profiles from high income settings, raising questions about\ntheir validity for African deployment where malaria, HIV, TB, sickle cell\ndisease and other neglected tropical diseases (NTDs) dominate burden and\nnational guidelines drive care. Methodology: We systematically reviewed 31\nquantitative LLM evaluation papers (Jan 2019 May 2025) identifying 19 English\nmedical QA benchmarks. Alama Health QA was developed using a retrieval\naugmented generation framework anchored on the Kenyan Clinical Practice\nGuidelines. Six widely used sets (AfriMedQA, MMLUMedical, PubMedQA, MedMCQA,\nMedQAUSMLE, and guideline grounded Alama Health QA) underwent harmonized\nsemantic profiling (NTD proportion, recency, readability, lexical diversity\nmetrics) and blinded expert rating across five dimensions: clinical relevance,\nguideline alignment, clarity, distractor plausibility, and language/cultural\nfit. Results: Alama Health QA captured >40% of all NTD mentions across corpora\nand the highest within set frequencies for malaria (7.7%), HIV (4.1%), and TB\n(5.2%); AfriMedQA ranked second but lacked formal guideline linkage. Global\nbenchmarks showed minimal representation (e.g., sickle cell disease absent in\nthree sets) despite large scale. Qualitatively, Alama scored highest for\nrelevance and guideline alignment; PubMedQA lowest for clinical utility.\nDiscussion: Quantitative medical LLM benchmarks widely used in the literature\nunderrepresent African disease burdens and regulatory contexts, risking\nmisleading performance claims. Guideline anchored, regionally curated resources\nsuch as Alama Health QA and expanded disease specific derivatives are essential\nfor safe, equitable model evaluation and deployment across African health\nsystems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u53d1\u73b0\u73b0\u6709\u533b\u5b66\u5927\u8bed\u8a00\u6a21\u578b\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u53cd\u6620\u9ad8\u6536\u5165\u56fd\u5bb6\u7684\u75be\u75c5\u7279\u5f81\uff0c\u4e0d\u9002\u7528\u4e8e\u4ee5\u759f\u75be\u3001HIV\u3001\u7ed3\u6838\u75c5\u7b49\u75be\u75c5\u4e3a\u4e3b\u7684\u975e\u6d32\u5730\u533a\u3002\u7814\u7a76\u5f00\u53d1\u4e86\u57fa\u4e8e\u80af\u5c3c\u4e9a\u4e34\u5e8a\u5b9e\u8df5\u6307\u5357\u7684Alama Health QA\u57fa\u51c6\u6d4b\u8bd5\uff0c\u66f4\u597d\u5730\u4ee3\u8868\u4e86\u975e\u6d32\u75be\u75c5\u8d1f\u62c5\u548c\u76d1\u7ba1\u73af\u5883\u3002", "motivation": "\u73b0\u6709\u533b\u5b66\u5927\u8bed\u8a00\u6a21\u578b\u57fa\u51c6\u6d4b\u8bd5\u4e3b\u8981\u57fa\u4e8e\u9ad8\u6536\u5165\u56fd\u5bb6\u7684\u8003\u8bd5\u5927\u7eb2\u548c\u75be\u75c5\u7279\u5f81\uff0c\u5728\u975e\u6d32\u90e8\u7f72\u65f6\u5b58\u5728\u6709\u6548\u6027\u95ee\u9898\uff0c\u56e0\u4e3a\u975e\u6d32\u5730\u533a\u4e3b\u8981\u75be\u75c5\u8d1f\u62c5\u662f\u759f\u75be\u3001HIV\u3001\u7ed3\u6838\u75c5\u3001\u9570\u72b6\u7ec6\u80de\u75c5\u7b49\u88ab\u5ffd\u89c6\u7684\u70ed\u5e26\u75be\u75c5\uff0c\u4e14\u533b\u7597\u5b9e\u8df5\u7531\u56fd\u5bb6\u6307\u5357\u9a71\u52a8\u3002", "method": "\u7cfb\u7edf\u56de\u987e\u4e8631\u7bc7\u5b9a\u91cf\u5927\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u8bba\u6587\uff082019\u5e741\u6708\u81f32025\u5e745\u6708\uff09\uff0c\u8bc6\u522b\u51fa19\u4e2a\u82f1\u8bed\u533b\u5b66\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u3002\u4f7f\u7528\u57fa\u4e8e\u80af\u5c3c\u4e9a\u4e34\u5e8a\u5b9e\u8df5\u6307\u5357\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6846\u67b6\u5f00\u53d1\u4e86Alama Health QA\u3002\u5bf96\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u51c6\u6d4b\u8bd5\u8fdb\u884c\u4e86\u7edf\u4e00\u7684\u8bed\u4e49\u5206\u6790\uff08\u88ab\u5ffd\u89c6\u70ed\u5e26\u75be\u75c5\u6bd4\u4f8b\u3001\u65f6\u6548\u6027\u3001\u53ef\u8bfb\u6027\u3001\u8bcd\u6c47\u591a\u6837\u6027\u6307\u6807\uff09\u548c\u4e13\u5bb6\u76f2\u8bc4\uff08\u4e34\u5e8a\u76f8\u5173\u6027\u3001\u6307\u5357\u4e00\u81f4\u6027\u3001\u6e05\u6670\u5ea6\u3001\u5e72\u6270\u9879\u5408\u7406\u6027\u3001\u8bed\u8a00\u6587\u5316\u9002\u914d\u6027\uff09\u3002", "result": "Alama Health QA\u6db5\u76d6\u4e86\u6240\u6709\u8bed\u6599\u5e93\u4e2d\u8d85\u8fc740%\u7684\u88ab\u5ffd\u89c6\u70ed\u5e26\u75be\u75c5\u63d0\u53ca\uff0c\u5728\u759f\u75be\uff087.7%\uff09\u3001HIV\uff084.1%\uff09\u548c\u7ed3\u6838\u75c5\uff085.2%\uff09\u65b9\u9762\u5177\u6709\u6700\u9ad8\u7684\u96c6\u5408\u5185\u9891\u7387\uff1bAfriMedQA\u6392\u540d\u7b2c\u4e8c\u4f46\u7f3a\u4e4f\u6b63\u5f0f\u7684\u6307\u5357\u5173\u8054\u3002\u5168\u7403\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\u6700\u5c11\u7684\u4ee3\u8868\u6027\uff08\u5982\u9570\u72b6\u7ec6\u80de\u75c5\u5728\u4e09\u4e2a\u6d4b\u8bd5\u96c6\u4e2d\u7f3a\u5931\uff09\u3002\u5b9a\u6027\u8bc4\u4f30\u4e2d\uff0cAlama\u5728\u76f8\u5173\u6027\u548c\u6307\u5357\u4e00\u81f4\u6027\u65b9\u9762\u5f97\u5206\u6700\u9ad8\uff1bPubMedQA\u5728\u4e34\u5e8a\u5b9e\u7528\u6027\u65b9\u9762\u5f97\u5206\u6700\u4f4e\u3002", "conclusion": "\u6587\u732e\u4e2d\u5e7f\u6cdb\u4f7f\u7528\u7684\u5b9a\u91cf\u533b\u5b66\u5927\u8bed\u8a00\u6a21\u578b\u57fa\u51c6\u6d4b\u8bd5\u672a\u80fd\u5145\u5206\u4ee3\u8868\u975e\u6d32\u75be\u75c5\u8d1f\u62c5\u548c\u76d1\u7ba1\u73af\u5883\uff0c\u5b58\u5728\u8bef\u5bfc\u6027\u80fd\u58f0\u660e\u7684\u98ce\u9669\u3002\u57fa\u4e8e\u6307\u5357\u3001\u533a\u57df\u7b56\u5212\u7684\u8d44\u6e90\u5982Alama Health QA\u53ca\u5176\u6269\u5c55\u7684\u75be\u75c5\u7279\u5f02\u6027\u884d\u751f\u7248\u672c\uff0c\u5bf9\u4e8e\u975e\u6d32\u536b\u751f\u7cfb\u7edf\u4e2d\u6a21\u578b\u7684\u5b89\u5168\u3001\u516c\u5e73\u8bc4\u4f30\u548c\u90e8\u7f72\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2507.16481", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.16481", "abs": "https://arxiv.org/abs/2507.16481", "authors": ["Riccardo Bussola", "Michele Focchi", "Giulio Turrisi", "Claudio Semini", "Luigi Palopoli"], "title": "Guided Reinforcement Learning for Omnidirectional 3D Jumping in Quadruped Robots", "comment": null, "summary": "Jumping poses a significant challenge for quadruped robots, despite being\ncrucial for many operational scenarios. While optimisation methods exist for\ncontrolling such motions, they are often time-consuming and demand extensive\nknowledge of robot and terrain parameters, making them less robust in\nreal-world scenarios. Reinforcement learning (RL) is emerging as a viable\nalternative, yet conventional end-to-end approaches lack efficiency in terms of\nsample complexity, requiring extensive training in simulations, and\npredictability of the final motion, which makes it difficult to certify the\nsafety of the final motion. To overcome these limitations, this paper\nintroduces a novel guided reinforcement learning approach that leverages\nphysical intuition for efficient and explainable jumping, by combining B\\'ezier\ncurves with a Uniformly Accelerated Rectilinear Motion (UARM) model. Extensive\nsimulation and experimental results clearly demonstrate the advantages of our\napproach over existing alternatives.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5f15\u5bfc\u5f0f\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u7ed3\u5408\u8d1d\u585e\u5c14\u66f2\u7ebf\u548c\u5300\u52a0\u901f\u76f4\u7ebf\u8fd0\u52a8\u6a21\u578b\uff0c\u7528\u4e8e\u56db\u8db3\u673a\u5668\u4eba\u7684\u9ad8\u6548\u53ef\u89e3\u91ca\u8df3\u8dc3\u63a7\u5236\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u4f18\u5316\u65b9\u6cd5\u8017\u65f6\u957f\u548c\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u6837\u672c\u590d\u6742\u5ea6\u9ad8\u7684\u95ee\u9898\u3002", "motivation": "\u56db\u8db3\u673a\u5668\u4eba\u8df3\u8dc3\u63a7\u5236\u9762\u4e34\u91cd\u5927\u6311\u6218\uff1a\u4f20\u7edf\u4f18\u5316\u65b9\u6cd5\u8017\u65f6\u4e14\u9700\u8981\u5927\u91cf\u673a\u5668\u4eba\u548c\u5730\u5f62\u53c2\u6570\u77e5\u8bc6\uff0c\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u7f3a\u4e4f\u9c81\u68d2\u6027\uff1b\u4f20\u7edf\u7aef\u5230\u7aef\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u6837\u672c\u590d\u6742\u5ea6\u9ad8\uff0c\u9700\u8981\u5927\u91cf\u4eff\u771f\u8bad\u7ec3\uff0c\u4e14\u6700\u7ec8\u8fd0\u52a8\u7684\u53ef\u9884\u6d4b\u6027\u5dee\uff0c\u96be\u4ee5\u4fdd\u8bc1\u5b89\u5168\u6027\u8ba4\u8bc1\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u5f15\u5bfc\u5f0f\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u8d1d\u585e\u5c14\u66f2\u7ebf\u4e0e\u5300\u52a0\u901f\u76f4\u7ebf\u8fd0\u52a8(UARM)\u6a21\u578b\u6765\u5229\u7528\u7269\u7406\u76f4\u89c9\uff0c\u5b9e\u73b0\u9ad8\u6548\u4e14\u53ef\u89e3\u91ca\u7684\u8df3\u8dc3\u63a7\u5236\u3002", "result": "\u5927\u91cf\u4eff\u771f\u548c\u5b9e\u9a8c\u7ed3\u679c\u6e05\u695a\u5730\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u76f8\u6bd4\u73b0\u6709\u66ff\u4ee3\u65b9\u6848\u7684\u4f18\u52bf\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u5f15\u5bfc\u5f0f\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u56db\u8db3\u673a\u5668\u4eba\u8df3\u8dc3\u63a7\u5236\u4e2d\u7684\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u95ee\u9898\uff0c\u5728\u4eff\u771f\u548c\u5b9e\u9a8c\u4e2d\u5747\u8868\u73b0\u51fa\u660e\u663e\u4f18\u52bf\u3002"}}
{"id": "2507.16334", "categories": ["cs.AI", "cs.LG", "math.DG"], "pdf": "https://arxiv.org/pdf/2507.16334", "abs": "https://arxiv.org/abs/2507.16334", "authors": ["Alexander Strunk", "Roland Assam"], "title": "Higher Gauge Flow Models", "comment": null, "summary": "This paper introduces Higher Gauge Flow Models, a novel class of Generative\nFlow Models. Building upon ordinary Gauge Flow Models (arXiv:2507.13414), these\nHigher Gauge Flow Models leverage an L$_{\\infty}$-algebra, effectively\nextending the Lie Algebra. This expansion allows for the integration of the\nhigher geometry and higher symmetries associated with higher groups into the\nframework of Generative Flow Models. Experimental evaluation on a Gaussian\nMixture Model dataset revealed substantial performance improvements compared to\ntraditional Flow Models.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9ad8\u89c4\u8303\u6d41\u6a21\u578b(Higher Gauge Flow Models)\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u751f\u6210\u6d41\u6a21\u578b\u7c7b\u522b\uff0c\u901a\u8fc7\u5229\u7528L\u221e\u4ee3\u6570\u6269\u5c55\u674e\u4ee3\u6570\uff0c\u5c06\u9ad8\u51e0\u4f55\u548c\u9ad8\u5bf9\u79f0\u6027\u96c6\u6210\u5230\u751f\u6210\u6d41\u6a21\u578b\u6846\u67b6\u4e2d\uff0c\u5728\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u4f20\u7edf\u7684\u751f\u6210\u6d41\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u51e0\u4f55\u7ed3\u6784\u548c\u9ad8\u9636\u5bf9\u79f0\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u5f15\u5165\u9ad8\u89c4\u8303\u7ed3\u6784\u6765\u6269\u5c55\u666e\u901a\u89c4\u8303\u6d41\u6a21\u578b\u7684\u80fd\u529b\uff0c\u5229\u7528L\u221e\u4ee3\u6570\u7684\u6570\u5b66\u6846\u67b6\u6765\u6355\u83b7\u66f4\u4e30\u5bcc\u7684\u51e0\u4f55\u548c\u5bf9\u79f0\u6027\u4fe1\u606f\uff0c\u4ece\u800c\u63d0\u5347\u751f\u6210\u6a21\u578b\u7684\u8868\u73b0\u529b\u3002", "method": "\u57fa\u4e8e\u666e\u901a\u89c4\u8303\u6d41\u6a21\u578b(Gauge Flow Models)\u7684\u57fa\u7840\u4e0a\uff0c\u5229\u7528L\u221e\u4ee3\u6570\u6709\u6548\u6269\u5c55\u674e\u4ee3\u6570\u7ed3\u6784\u3002\u8fd9\u79cd\u6269\u5c55\u5141\u8bb8\u5c06\u4e0e\u9ad8\u7fa4\u76f8\u5173\u7684\u9ad8\u51e0\u4f55(higher geometry)\u548c\u9ad8\u5bf9\u79f0\u6027(higher symmetries)\u96c6\u6210\u5230\u751f\u6210\u6d41\u6a21\u578b\u7684\u6846\u67b6\u4e2d\uff0c\u5f62\u6210\u9ad8\u89c4\u8303\u6d41\u6a21\u578b\u3002", "result": "\u5728\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5b9e\u9a8c\u8bc4\u4f30\uff0c\u9ad8\u89c4\u8303\u6d41\u6a21\u578b\u76f8\u6bd4\u4f20\u7edf\u6d41\u6a21\u578b\u8868\u73b0\u51fa\u663e\u8457\u7684\u6027\u80fd\u6539\u8fdb\u3002", "conclusion": "\u9ad8\u89c4\u8303\u6d41\u6a21\u578b\u901a\u8fc7\u5f15\u5165L\u221e\u4ee3\u6570\u548c\u9ad8\u89c4\u8303\u7ed3\u6784\uff0c\u6210\u529f\u6269\u5c55\u4e86\u751f\u6210\u6d41\u6a21\u578b\u7684\u80fd\u529b\uff0c\u80fd\u591f\u66f4\u597d\u5730\u5904\u7406\u590d\u6742\u7684\u51e0\u4f55\u7ed3\u6784\u548c\u5bf9\u79f0\u6027\uff0c\u4e3a\u751f\u6210\u5efa\u6a21\u9886\u57df\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u65b0\u65b9\u5411\u3002"}}
{"id": "2507.16621", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.16621", "abs": "https://arxiv.org/abs/2507.16621", "authors": ["Lorenzo Gentilini", "Pierpaolo Serio", "Valentina Donzella", "Lorenzo Pollini"], "title": "A Target-based Multi-LiDAR Multi-Camera Extrinsic Calibration System", "comment": null, "summary": "Extrinsic Calibration represents the cornerstone of autonomous driving. Its\naccuracy plays a crucial role in the perception pipeline, as any errors can\nhave implications for the safety of the vehicle. Modern sensor systems collect\ndifferent types of data from the environment, making it harder to align the\ndata. To this end, we propose a target-based extrinsic calibration system\ntailored for a multi-LiDAR and multi-camera sensor suite. This system enables\ncross-calibration between LiDARs and cameras with limited prior knowledge using\na custom ChArUco board and a tailored nonlinear optimization method. We test\nthe system with real-world data gathered in a warehouse. Results demonstrated\nthe effectiveness of the proposed method, highlighting the feasibility of a\nunique pipeline tailored for various types of sensors.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6807\u5b9a\u677f\u7684\u591a\u6fc0\u5149\u96f7\u8fbe\u548c\u591a\u6444\u50cf\u5934\u5916\u53c2\u6807\u5b9a\u7cfb\u7edf\uff0c\u4f7f\u7528\u5b9a\u5236\u7684ChArUco\u6807\u5b9a\u677f\u548c\u975e\u7ebf\u6027\u4f18\u5316\u65b9\u6cd5\uff0c\u5728\u4ed3\u5e93\u73af\u5883\u7684\u771f\u5b9e\u6570\u636e\u4e0a\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u4e2d\u5916\u53c2\u6807\u5b9a\u7684\u51c6\u786e\u6027\u5bf9\u611f\u77e5\u6d41\u6c34\u7ebf\u81f3\u5173\u91cd\u8981\uff0c\u4efb\u4f55\u8bef\u5dee\u90fd\u53ef\u80fd\u5f71\u54cd\u8f66\u8f86\u5b89\u5168\u3002\u73b0\u4ee3\u4f20\u611f\u5668\u7cfb\u7edf\u6536\u96c6\u4e0d\u540c\u7c7b\u578b\u7684\u73af\u5883\u6570\u636e\uff0c\u4f7f\u5f97\u6570\u636e\u5bf9\u9f50\u53d8\u5f97\u66f4\u52a0\u56f0\u96be\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u6709\u6548\u7684\u591a\u4f20\u611f\u5668\u5916\u53c2\u6807\u5b9a\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6807\u5b9a\u677f\u7684\u5916\u53c2\u6807\u5b9a\u7cfb\u7edf\uff0c\u4e13\u95e8\u9488\u5bf9\u591a\u6fc0\u5149\u96f7\u8fbe\u548c\u591a\u6444\u50cf\u5934\u4f20\u611f\u5668\u5957\u4ef6\u8bbe\u8ba1\u3002\u8be5\u7cfb\u7edf\u4f7f\u7528\u5b9a\u5236\u7684ChArUco\u6807\u5b9a\u677f\uff0c\u7ed3\u5408\u91cf\u8eab\u5b9a\u5236\u7684\u975e\u7ebf\u6027\u4f18\u5316\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u6709\u9650\u5148\u9a8c\u77e5\u8bc6\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u6fc0\u5149\u96f7\u8fbe\u548c\u6444\u50cf\u5934\u4e4b\u95f4\u7684\u4ea4\u53c9\u6807\u5b9a\u3002", "result": "\u5728\u4ed3\u5e93\u73af\u5883\u4e2d\u6536\u96c6\u7684\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e0a\u6d4b\u8bd5\u4e86\u8be5\u7cfb\u7edf\uff0c\u7ed3\u679c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u7a81\u51fa\u4e86\u9488\u5bf9\u5404\u79cd\u7c7b\u578b\u4f20\u611f\u5668\u7684\u72ec\u7279\u6807\u5b9a\u6d41\u6c34\u7ebf\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7684\u591a\u4f20\u611f\u5668\u5916\u53c2\u6807\u5b9a\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u591f\u6210\u529f\u5b9e\u73b0\u591a\u6fc0\u5149\u96f7\u8fbe\u548c\u591a\u6444\u50cf\u5934\u7cfb\u7edf\u7684\u7cbe\u786e\u6807\u5b9a\uff0c\u4e3a\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u6807\u5b9a\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.16356", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16356", "abs": "https://arxiv.org/abs/2507.16356", "authors": ["Arpan Dasgupta", "Mizhaan Maniyar", "Awadhesh Srivastava", "Sanat Kumar", "Amrita Mahale", "Aparna Hedge", "Arun Suggala", "Karthikeyan Shanmugam", "Aparna Taneja", "Milind Tambe"], "title": "Learning to Call: A Field Trial of a Collaborative Bandit Algorithm for Improved Message Delivery in Mobile Maternal Health", "comment": null, "summary": "Mobile health (mHealth) programs utilize automated voice messages to deliver\nhealth information, particularly targeting underserved communities,\ndemonstrating the effectiveness of using mobile technology to disseminate\ncrucial health information to these populations, improving health outcomes\nthrough increased awareness and behavioral change. India's Kilkari program\ndelivers vital maternal health information via weekly voice calls to millions\nof mothers. However, the current random call scheduling often results in missed\ncalls and reduced message delivery. This study presents a field trial of a\ncollaborative bandit algorithm designed to optimize call timing by learning\nindividual mothers' preferred call times. We deployed the algorithm with around\n$6500$ Kilkari participants as a pilot study, comparing its performance to the\nbaseline random calling approach. Our results demonstrate a statistically\nsignificant improvement in call pick-up rates with the bandit algorithm,\nindicating its potential to enhance message delivery and impact millions of\nmothers across India. This research highlights the efficacy of personalized\nscheduling in mobile health interventions and underscores the potential of\nmachine learning to improve maternal health outreach at scale.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9488\u5bf9\u5370\u5ea6Kilkari\u6bcd\u5a74\u5065\u5eb7\u9879\u76ee\u5f00\u53d1\u4e86\u4e00\u79cd\u534f\u4f5c\u8001\u864e\u673a\u7b97\u6cd5\uff0c\u901a\u8fc7\u5b66\u4e60\u4e2a\u4f53\u6bcd\u4eb2\u7684\u504f\u597d\u901a\u8bdd\u65f6\u95f4\u6765\u4f18\u5316\u901a\u8bdd\u8c03\u5ea6\uff0c\u57286500\u540d\u53c2\u4e0e\u8005\u7684\u5b9e\u5730\u8bd5\u9a8c\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u901a\u8bdd\u63a5\u542c\u7387\uff0c\u5c55\u73b0\u4e86\u4e2a\u6027\u5316\u8c03\u5ea6\u5728\u79fb\u52a8\u5065\u5eb7\u5e72\u9884\u4e2d\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5370\u5ea6Kilkari\u9879\u76ee\u901a\u8fc7\u8bed\u97f3\u901a\u8bdd\u5411\u6570\u767e\u4e07\u6bcd\u4eb2\u4f20\u9012\u91cd\u8981\u7684\u6bcd\u5a74\u5065\u5eb7\u4fe1\u606f\uff0c\u4f46\u5f53\u524d\u968f\u673a\u901a\u8bdd\u8c03\u5ea6\u5bfc\u81f4\u5927\u91cf\u672a\u63a5\u901a\u8bdd\u548c\u4fe1\u606f\u4f20\u9012\u6548\u679c\u964d\u4f4e\uff0c\u9700\u8981\u4f18\u5316\u901a\u8bdd\u65f6\u95f4\u5b89\u6392\u4ee5\u63d0\u9ad8\u4fe1\u606f\u4f20\u9012\u6548\u7387\u3002", "method": "\u8bbe\u8ba1\u5e76\u90e8\u7f72\u4e86\u4e00\u79cd\u534f\u4f5c\u8001\u864e\u673a\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u80fd\u591f\u5b66\u4e60\u6bcf\u4f4d\u6bcd\u4eb2\u7684\u4e2a\u4eba\u504f\u597d\u901a\u8bdd\u65f6\u95f4\uff0c\u4ece\u800c\u4f18\u5316\u901a\u8bdd\u8c03\u5ea6\u3002\u5728\u7ea66500\u540dKilkari\u9879\u76ee\u53c2\u4e0e\u8005\u4e2d\u8fdb\u884c\u5b9e\u5730\u8bd5\u9a8c\uff0c\u5c06\u7b97\u6cd5\u6548\u679c\u4e0e\u57fa\u7ebf\u968f\u673a\u901a\u8bdd\u65b9\u6cd5\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "\u4f7f\u7528\u534f\u4f5c\u8001\u864e\u673a\u7b97\u6cd5\u7684\u901a\u8bdd\u63a5\u542c\u7387\u76f8\u6bd4\u968f\u673a\u8c03\u5ea6\u65b9\u6cd5\u6709\u7edf\u8ba1\u5b66\u610f\u4e49\u4e0a\u7684\u663e\u8457\u63d0\u5347\uff0c\u8bc1\u660e\u4e86\u4e2a\u6027\u5316\u8c03\u5ea6\u80fd\u591f\u6709\u6548\u6539\u5584\u4fe1\u606f\u4f20\u9012\u6548\u679c\u3002", "conclusion": "\u4e2a\u6027\u5316\u8c03\u5ea6\u5728\u79fb\u52a8\u5065\u5eb7\u5e72\u9884\u4e2d\u5177\u6709\u663e\u8457\u6548\u679c\uff0c\u673a\u5668\u5b66\u4e60\u6280\u672f\u6709\u6f5c\u529b\u5927\u89c4\u6a21\u6539\u5584\u6bcd\u5a74\u5065\u5eb7\u63a8\u5e7f\u5de5\u4f5c\uff0c\u8be5\u7b97\u6cd5\u6709\u671b\u5f71\u54cd\u5370\u5ea6\u6570\u767e\u4e07\u6bcd\u4eb2\u7684\u5065\u5eb7\u4fe1\u606f\u83b7\u53d6\u3002"}}
{"id": "2507.16645", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.16645", "abs": "https://arxiv.org/abs/2507.16645", "authors": ["Zongzheng Zhang", "Jiawen Yang", "Ziqiao Peng", "Meng Yang", "Jianzhu Ma", "Lin Cheng", "Huazhe Xu", "Hang Zhao", "Hao Zhao"], "title": "Morpheus: A Neural-driven Animatronic Face with Hybrid Actuation and Diverse Emotion Control", "comment": "Accepted to RSS 2025, Project Page:\n  https://jiawenyang-ch.github.io/Morpheus-Hardware-Design/", "summary": "Previous animatronic faces struggle to express emotions effectively due to\nhardware and software limitations. On the hardware side, earlier approaches\neither use rigid-driven mechanisms, which provide precise control but are\ndifficult to design within constrained spaces, or tendon-driven mechanisms,\nwhich are more space-efficient but challenging to control. In contrast, we\npropose a hybrid actuation approach that combines the best of both worlds. The\neyes and mouth-key areas for emotional expression-are controlled using rigid\nmechanisms for precise movement, while the nose and cheek, which convey subtle\nfacial microexpressions, are driven by strings. This design allows us to build\na compact yet versatile hardware platform capable of expressing a wide range of\nemotions. On the algorithmic side, our method introduces a self-modeling\nnetwork that maps motor actions to facial landmarks, allowing us to\nautomatically establish the relationship between blendshape coefficients for\ndifferent facial expressions and the corresponding motor control signals\nthrough gradient backpropagation. We then train a neural network to map speech\ninput to corresponding blendshape controls. With our method, we can generate\ndistinct emotional expressions such as happiness, fear, disgust, and anger,\nfrom any given sentence, each with nuanced, emotion-specific control signals-a\nfeature that has not been demonstrated in earlier systems. We release the\nhardware design and code at https://github.com/ZZongzheng0918/Morpheus-Hardware\nand https://github.com/ZZongzheng0918/Morpheus-Software.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u9a71\u52a8\u7684\u52a8\u753b\u9762\u90e8\u673a\u5668\u4eba\u7cfb\u7edf\uff0c\u7ed3\u5408\u521a\u6027\u548c\u67d4\u6027\u9a71\u52a8\u673a\u5236\uff0c\u901a\u8fc7\u81ea\u5efa\u6a21\u7f51\u7edc\u5b9e\u73b0\u4ece\u8bed\u97f3\u8f93\u5165\u5230\u60c5\u611f\u8868\u8fbe\u7684\u81ea\u52a8\u63a7\u5236\uff0c\u80fd\u591f\u751f\u6210\u591a\u79cd\u7ec6\u81f4\u7684\u9762\u90e8\u8868\u60c5\u3002", "motivation": "\u4ee5\u5f80\u7684\u52a8\u753b\u9762\u90e8\u673a\u5668\u4eba\u5728\u60c5\u611f\u8868\u8fbe\u65b9\u9762\u5b58\u5728\u786c\u4ef6\u548c\u8f6f\u4ef6\u9650\u5236\uff1a\u521a\u6027\u9a71\u52a8\u673a\u5236\u63a7\u5236\u7cbe\u786e\u4f46\u8bbe\u8ba1\u56f0\u96be\uff0c\u8171\u9a71\u52a8\u673a\u5236\u8282\u7701\u7a7a\u95f4\u4f46\u63a7\u5236\u590d\u6742\uff0c\u4e14\u7f3a\u4e4f\u4ece\u8bed\u97f3\u8f93\u5165\u76f4\u63a5\u751f\u6210\u60c5\u611f\u7279\u5b9a\u63a7\u5236\u4fe1\u53f7\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u6df7\u5408\u9a71\u52a8\u65b9\u6cd5\uff1a\u773c\u90e8\u548c\u5634\u90e8\u91c7\u7528\u521a\u6027\u673a\u5236\u5b9e\u73b0\u7cbe\u786e\u63a7\u5236\uff0c\u9f3b\u90e8\u548c\u8138\u988a\u91c7\u7528\u7ef3\u7d22\u9a71\u52a8\u5b9e\u73b0\u5fae\u8868\u60c5\u63a7\u5236\uff1b\u5f00\u53d1\u81ea\u5efa\u6a21\u7f51\u7edc\u5c06\u7535\u673a\u52a8\u4f5c\u6620\u5c04\u5230\u9762\u90e8\u5173\u952e\u70b9\uff0c\u901a\u8fc7\u68af\u5ea6\u53cd\u5411\u4f20\u64ad\u5efa\u7acb\u6df7\u5408\u5f62\u72b6\u7cfb\u6570\u4e0e\u7535\u673a\u63a7\u5236\u4fe1\u53f7\u7684\u5173\u7cfb\uff1b\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u5c06\u8bed\u97f3\u8f93\u5165\u6620\u5c04\u5230\u76f8\u5e94\u7684\u6df7\u5408\u5f62\u72b6\u63a7\u5236\u3002", "result": "\u7cfb\u7edf\u80fd\u591f\u4ece\u4efb\u610f\u7ed9\u5b9a\u53e5\u5b50\u751f\u6210\u5feb\u4e50\u3001\u6050\u60e7\u3001\u538c\u6076\u3001\u6124\u6012\u7b49\u4e0d\u540c\u7684\u60c5\u611f\u8868\u8fbe\uff0c\u6bcf\u79cd\u8868\u60c5\u90fd\u5177\u6709\u7ec6\u81f4\u7684\u3001\u60c5\u611f\u7279\u5b9a\u7684\u63a7\u5236\u4fe1\u53f7\uff0c\u8fd9\u662f\u65e9\u671f\u7cfb\u7edf\u672a\u80fd\u5b9e\u73b0\u7684\u529f\u80fd\u3002", "conclusion": "\u6210\u529f\u6784\u5efa\u4e86\u7d27\u51d1\u4e14\u591a\u529f\u80fd\u7684\u786c\u4ef6\u5e73\u53f0\uff0c\u5b9e\u73b0\u4e86\u5e7f\u6cdb\u7684\u60c5\u611f\u8868\u8fbe\u80fd\u529b\uff0c\u5e76\u5f00\u6e90\u4e86\u786c\u4ef6\u8bbe\u8ba1\u548c\u8f6f\u4ef6\u4ee3\u7801\uff0c\u4e3a\u52a8\u753b\u9762\u90e8\u673a\u5668\u4eba\u7684\u60c5\u611f\u8868\u8fbe\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.16370", "categories": ["cs.AI", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2507.16370", "abs": "https://arxiv.org/abs/2507.16370", "authors": ["Lucas de Lara"], "title": "Canonical Representations of Markovian Structural Causal Models: A Framework for Counterfactual Reasoning", "comment": null, "summary": "Counterfactual reasoning aims at answering contrary-to-fact questions like\n''Would have Alice recovered had she taken aspirin?'' and corresponds to the\nmost fine-grained layer of causation. Critically, while many counterfactual\nstatements cannot be falsified -- even by randomized experiments -- they\nunderpin fundamental concepts like individual-wise fairness. Therefore,\nproviding models to formalize and implement counterfactual beliefs remains a\nfundamental scientific problem. In the Markovian setting of Pearl's causal\nframework, we propose an alternative approach to structural causal models to\nrepresent counterfactuals compatible with a given causal graphical model. More\nprecisely, we introduce counterfactual models, also called canonical\nrepresentations of structural causal models. They enable analysts to choose a\ncounterfactual conception via random-process probability distributions with\npreassigned marginals and characterize the counterfactual equivalence class of\nstructural causal models. Then, we present a normalization procedure to\ndescribe and implement various counterfactual conceptions. Compared to\nstructural causal models, it allows to specify many counterfactual conceptions\nwithout altering the observational and interventional constraints. Moreover,\nthe content of the model corresponding to the counterfactual layer does not\nneed to be estimated; only to make a choice. Finally, we illustrate the\nspecific role of counterfactuals in causality and the benefits of our approach\non theoretical and numerical examples.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cd\u4e8b\u5b9e\u6a21\u578b\u7684\u65b0\u65b9\u6cd5\u6765\u8868\u793a\u548c\u5b9e\u73b0\u53cd\u4e8b\u5b9e\u63a8\u7406\uff0c\u8be5\u65b9\u6cd5\u5141\u8bb8\u5206\u6790\u5e08\u901a\u8fc7\u9884\u8bbe\u8fb9\u9645\u5206\u5e03\u7684\u968f\u673a\u8fc7\u7a0b\u6765\u9009\u62e9\u53cd\u4e8b\u5b9e\u6982\u5ff5\uff0c\u5e76\u63d0\u4f9b\u4e86\u89c4\u8303\u5316\u7a0b\u5e8f\u6765\u63cf\u8ff0\u5404\u79cd\u53cd\u4e8b\u5b9e\u6982\u5ff5\u3002", "motivation": "\u53cd\u4e8b\u5b9e\u63a8\u7406\u662f\u56e0\u679c\u5173\u7cfb\u6700\u7ec6\u7c92\u5ea6\u7684\u5c42\u9762\uff0c\u867d\u7136\u8bb8\u591a\u53cd\u4e8b\u5b9e\u9648\u8ff0\u65e0\u6cd5\u88ab\u8bc1\u4f2a\uff08\u5373\u4f7f\u901a\u8fc7\u968f\u673a\u5b9e\u9a8c\uff09\uff0c\u4f46\u5b83\u4eec\u652f\u6491\u7740\u4e2a\u4f53\u516c\u5e73\u6027\u7b49\u57fa\u672c\u6982\u5ff5\u3002\u56e0\u6b64\uff0c\u63d0\u4f9b\u5f62\u5f0f\u5316\u548c\u5b9e\u73b0\u53cd\u4e8b\u5b9e\u4fe1\u5ff5\u7684\u6a21\u578b\u4ecd\u7136\u662f\u4e00\u4e2a\u57fa\u672c\u7684\u79d1\u5b66\u95ee\u9898\u3002", "method": "\u5728Pearl\u56e0\u679c\u6846\u67b6\u7684\u9a6c\u5c14\u53ef\u592b\u8bbe\u5b9a\u4e0b\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u7ed3\u6784\u56e0\u679c\u6a21\u578b\u7684\u66ff\u4ee3\u65b9\u6cd5\u6765\u8868\u793a\u4e0e\u7ed9\u5b9a\u56e0\u679c\u56fe\u6a21\u578b\u517c\u5bb9\u7684\u53cd\u4e8b\u5b9e\u3002\u5177\u4f53\u5f15\u5165\u4e86\u53cd\u4e8b\u5b9e\u6a21\u578b\uff08\u4e5f\u79f0\u4e3a\u7ed3\u6784\u56e0\u679c\u6a21\u578b\u7684\u6807\u51c6\u8868\u793a\uff09\uff0c\u4f7f\u5206\u6790\u5e08\u80fd\u591f\u901a\u8fc7\u5177\u6709\u9884\u8bbe\u8fb9\u9645\u5206\u5e03\u7684\u968f\u673a\u8fc7\u7a0b\u6982\u7387\u5206\u5e03\u6765\u9009\u62e9\u53cd\u4e8b\u5b9e\u6982\u5ff5\uff0c\u5e76\u523b\u753b\u7ed3\u6784\u56e0\u679c\u6a21\u578b\u7684\u53cd\u4e8b\u5b9e\u7b49\u4ef7\u7c7b\u3002\u7136\u540e\u63d0\u51fa\u89c4\u8303\u5316\u7a0b\u5e8f\u6765\u63cf\u8ff0\u548c\u5b9e\u73b0\u5404\u79cd\u53cd\u4e8b\u5b9e\u6982\u5ff5\u3002", "result": "\u4e0e\u7ed3\u6784\u56e0\u679c\u6a21\u578b\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5141\u8bb8\u5728\u4e0d\u6539\u53d8\u89c2\u5bdf\u548c\u5e72\u9884\u7ea6\u675f\u7684\u60c5\u51b5\u4e0b\u6307\u5b9a\u8bb8\u591a\u53cd\u4e8b\u5b9e\u6982\u5ff5\u3002\u6b64\u5916\uff0c\u5bf9\u5e94\u53cd\u4e8b\u5b9e\u5c42\u7684\u6a21\u578b\u5185\u5bb9\u4e0d\u9700\u8981\u88ab\u4f30\u8ba1\uff0c\u53ea\u9700\u8981\u505a\u51fa\u9009\u62e9\u3002\u7406\u8bba\u548c\u6570\u503c\u4f8b\u5b50\u8bf4\u660e\u4e86\u53cd\u4e8b\u5b9e\u5728\u56e0\u679c\u5173\u7cfb\u4e2d\u7684\u7279\u5b9a\u4f5c\u7528\u4ee5\u53ca\u8be5\u65b9\u6cd5\u7684\u4f18\u52bf\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u53cd\u4e8b\u5b9e\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u7406\u8bba\u6846\u67b6\uff0c\u901a\u8fc7\u53cd\u4e8b\u5b9e\u6a21\u578b\u548c\u89c4\u8303\u5316\u7a0b\u5e8f\uff0c\u4f7f\u5f97\u5728\u4fdd\u6301\u89c2\u5bdf\u548c\u5e72\u9884\u7ea6\u675f\u4e0d\u53d8\u7684\u524d\u63d0\u4e0b\uff0c\u80fd\u591f\u7075\u6d3b\u5730\u9009\u62e9\u548c\u5b9e\u73b0\u4e0d\u540c\u7684\u53cd\u4e8b\u5b9e\u6982\u5ff5\uff0c\u4e3a\u56e0\u679c\u63a8\u7406\u63d0\u4f9b\u4e86\u66f4\u52a0\u7cbe\u7ec6\u548c\u7075\u6d3b\u7684\u5de5\u5177\u3002"}}
{"id": "2507.16713", "categories": ["cs.RO", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.16713", "abs": "https://arxiv.org/abs/2507.16713", "authors": ["Guowei Lan", "Kaixian Qu", "Ren\u00e9 Zurbr\u00fcgg", "Changan Chen", "Christopher E. Mower", "Haitham Bou-Ammar", "Marco Hutter"], "title": "Experience is the Best Teacher: Grounding VLMs for Robotics through Self-Generated Memory", "comment": null, "summary": "Vision-language models (VLMs) have been widely adopted in robotics to enable\nautonomous planning. However, grounding VLMs, originally trained on internet\ndata, to diverse real-world robots remains a challenge. This paper presents\nExpTeach, a framework that grounds VLMs to physical robots by building a\nself-generated memory of real-world experiences. In ExpTeach, the VLM\nautonomously plans actions, verifies outcomes, reflects on failures, and adapts\nrobot behaviors in a closed loop. The self-generated experiences during this\nprocess are then summarized into a long-term memory, enabling retrieval of\nlearned knowledge to guide future tasks via retrieval-augmented generation\n(RAG). Additionally, ExpTeach enhances the spatial understanding of VLMs with\nan on-demand image annotation module. In experiments, we show that reflection\nimproves success rates from 36% to 84% on four challenging robotic tasks and\nobserve the emergence of intelligent object interactions, including creative\ntool use. Across extensive tests on 12 real-world scenarios (including eight\nunseen ones), we find that grounding with long-term memory boosts single-trial\nsuccess rates from 22% to 80%, demonstrating the effectiveness and\ngeneralizability of ExpTeach.", "AI": {"tldr": "ExpTeach\u662f\u4e00\u4e2a\u901a\u8fc7\u6784\u5efa\u81ea\u751f\u6210\u7684\u771f\u5b9e\u4e16\u754c\u7ecf\u9a8c\u8bb0\u5fc6\u6765\u5c06\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5e94\u7528\u4e8e\u7269\u7406\u673a\u5668\u4eba\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u53cd\u601d\u673a\u5236\u548c\u957f\u671f\u8bb0\u5fc6\u68c0\u7d22\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u4efb\u52a1\u7684\u6210\u529f\u7387\u3002", "motivation": "\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u867d\u7136\u5728\u673a\u5668\u4eba\u81ea\u4e3b\u89c4\u5212\u4e2d\u88ab\u5e7f\u6cdb\u91c7\u7528\uff0c\u4f46\u5c06\u539f\u672c\u5728\u4e92\u8054\u7f51\u6570\u636e\u4e0a\u8bad\u7ec3\u7684VLMs\u9002\u914d\u5230\u591a\u6837\u5316\u7684\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u6765\u8fde\u63a5\u6a21\u578b\u4e0e\u7269\u7406\u673a\u5668\u4eba\u64cd\u4f5c\u3002", "method": "ExpTeach\u6846\u67b6\u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u5de5\u4f5c\uff1a1\uff09VLM\u81ea\u4e3b\u89c4\u5212\u52a8\u4f5c\u3001\u9a8c\u8bc1\u7ed3\u679c\u3001\u53cd\u601d\u5931\u8d25\u5e76\u5728\u95ed\u73af\u4e2d\u8c03\u6574\u673a\u5668\u4eba\u884c\u4e3a\uff1b2\uff09\u5c06\u81ea\u751f\u6210\u7684\u7ecf\u9a8c\u603b\u7ed3\u4e3a\u957f\u671f\u8bb0\u5fc6\uff1b3\uff09\u901a\u8fc7\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u68c0\u7d22\u5b66\u4e60\u5230\u7684\u77e5\u8bc6\u6307\u5bfc\u672a\u6765\u4efb\u52a1\uff1b4\uff09\u4f7f\u7528\u6309\u9700\u56fe\u50cf\u6807\u6ce8\u6a21\u5757\u589e\u5f3aVLMs\u7684\u7a7a\u95f4\u7406\u89e3\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff1a1\uff09\u53cd\u601d\u673a\u5236\u5c06\u56db\u4e2a\u6311\u6218\u6027\u673a\u5668\u4eba\u4efb\u52a1\u7684\u6210\u529f\u7387\u4ece36%\u63d0\u5347\u523084%\uff1b2\uff09\u89c2\u5bdf\u5230\u667a\u80fd\u7269\u4f53\u4ea4\u4e92\u7684\u51fa\u73b0\uff0c\u5305\u62ec\u521b\u9020\u6027\u5de5\u5177\u4f7f\u7528\uff1b3\uff09\u572812\u4e2a\u771f\u5b9e\u4e16\u754c\u573a\u666f\uff08\u5305\u62ec8\u4e2a\u672a\u89c1\u8fc7\u7684\u573a\u666f\uff09\u7684\u5e7f\u6cdb\u6d4b\u8bd5\u4e2d\uff0c\u957f\u671f\u8bb0\u5fc6\u7684\u5e94\u7528\u5c06\u5355\u6b21\u8bd5\u9a8c\u6210\u529f\u7387\u4ece22%\u63d0\u5347\u523080%\u3002", "conclusion": "ExpTeach\u901a\u8fc7\u81ea\u751f\u6210\u7ecf\u9a8c\u8bb0\u5fc6\u548c\u53cd\u601d\u673a\u5236\u6709\u6548\u5730\u5c06\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u9002\u914d\u5230\u7269\u7406\u673a\u5668\u4eba\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u4efb\u52a1\u7684\u6210\u529f\u7387\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u4e3aVLMs\u5728\u771f\u5b9e\u4e16\u754c\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u7684\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.16395", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.16395", "abs": "https://arxiv.org/abs/2507.16395", "authors": ["Bo Hou", "Xin Tan", "Kai Zheng", "Fang Liu", "Yinghao Zhu", "Li Zhang"], "title": "LLM-Driven Collaborative Model for Untangling Commits via Explicit and Implicit Dependency Reasoning", "comment": null, "summary": "Atomic commits, each of which addresses a single development concern, are a\nbest practice in software development. However, developers frequently produce\ntangled commits that mix unrelated changes due to practical constraints or\nunclear boundaries, negatively impacting code review and maintenance. Although\nprior commit untangling approaches: rule-based, feature-based, or graph-based,\nhave made progress, they often rely on shallow signals and fail to distinguish\nbetween explicit dependencies (e.g., control/data flow) and implicit ones\n(e.g., semantic or conceptual relationships). In this paper, we propose\nColaUntangle, a new collaborative consultation framework for commit untangling\nthat models both explicit and implicit dependencies among code changes.\nColaUntangle integrates Large Language Model (LLM)-driven agents in a\nmulti-agent architecture: one agent specializes in explicit dependencies,\nanother in implicit ones, and a reviewer agent synthesizes their perspectives\nthrough iterative consultation. To capture explicit and implicit contextual\ninformation, we construct multi-version Program Dependency Graphs (delta-PDG),\nenabling agents to reason over code relationships with both symbolic and\nsemantic depth. We evaluate ColaUntangle on two widely-used datasets (1,612 C#\nand 14k Java tangled commits). Experimental results show that ColaUntangle\noutperforms the best-performing baseline, achieving an improvement of 44% on\nthe C# dataset and 100% on the Java dataset. These findings highlight the\npotential of LLM-based collaborative frameworks for advancing automated commit\nuntangling tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86ColaUntangle\u6846\u67b6\uff0c\u4f7f\u7528\u591a\u667a\u80fd\u4f53\u67b6\u6784\u548c\u5927\u8bed\u8a00\u6a21\u578b\u6765\u89e3\u51b3\u4ee3\u7801\u63d0\u4ea4\u89e3\u8026\u95ee\u9898\uff0c\u901a\u8fc7\u5efa\u6a21\u663e\u5f0f\u548c\u9690\u5f0f\u4f9d\u8d56\u5173\u7cfb\uff0c\u5728C#\u548cJava\u6570\u636e\u96c6\u4e0a\u5206\u522b\u5b9e\u73b044%\u548c100%\u7684\u6027\u80fd\u63d0\u5347", "motivation": "\u5f00\u53d1\u8005\u7ecf\u5e38\u4ea7\u751f\u6df7\u5408\u4e0d\u76f8\u5173\u53d8\u66f4\u7684\u7ea0\u7f20\u63d0\u4ea4\uff0c\u5f71\u54cd\u4ee3\u7801\u5ba1\u67e5\u548c\u7ef4\u62a4\u3002\u73b0\u6709\u7684\u57fa\u4e8e\u89c4\u5219\u3001\u7279\u5f81\u6216\u56fe\u7684\u89e3\u8026\u65b9\u6cd5\u4f9d\u8d56\u6d45\u5c42\u4fe1\u53f7\uff0c\u65e0\u6cd5\u6709\u6548\u533a\u5206\u663e\u5f0f\u4f9d\u8d56\uff08\u63a7\u5236/\u6570\u636e\u6d41\uff09\u548c\u9690\u5f0f\u4f9d\u8d56\uff08\u8bed\u4e49\u6216\u6982\u5ff5\u5173\u7cfb\uff09", "method": "\u63d0\u51faColaUntangle\u534f\u4f5c\u54a8\u8be2\u6846\u67b6\uff0c\u91c7\u7528\u591a\u667a\u80fd\u4f53\u67b6\u6784\uff1a\u4e00\u4e2a\u667a\u80fd\u4f53\u4e13\u95e8\u5904\u7406\u663e\u5f0f\u4f9d\u8d56\uff0c\u53e6\u4e00\u4e2a\u5904\u7406\u9690\u5f0f\u4f9d\u8d56\uff0c\u5ba1\u67e5\u667a\u80fd\u4f53\u901a\u8fc7\u8fed\u4ee3\u54a8\u8be2\u7efc\u5408\u4e24\u8005\u89c2\u70b9\u3002\u6784\u5efa\u591a\u7248\u672c\u7a0b\u5e8f\u4f9d\u8d56\u56fe\uff08delta-PDG\uff09\u6765\u6355\u83b7\u663e\u5f0f\u548c\u9690\u5f0f\u4e0a\u4e0b\u6587\u4fe1\u606f", "result": "\u5728\u4e24\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u6570\u636e\u96c6\uff081,612\u4e2aC#\u548c14k\u4e2aJava\u7ea0\u7f20\u63d0\u4ea4\uff09\u4e0a\u8bc4\u4f30\uff0cColaUntangle\u8d85\u8d8a\u6700\u4f73\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728C#\u6570\u636e\u96c6\u4e0a\u5b9e\u73b044%\u6539\u8fdb\uff0c\u5728Java\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0100%\u6539\u8fdb", "conclusion": "\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u534f\u4f5c\u6846\u67b6\u5728\u81ea\u52a8\u5316\u63d0\u4ea4\u89e3\u8026\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u901a\u8fc7\u5efa\u6a21\u663e\u5f0f\u548c\u9690\u5f0f\u4f9d\u8d56\u5173\u7cfb\u80fd\u591f\u663e\u8457\u63d0\u5347\u63d0\u4ea4\u89e3\u8026\u7684\u6027\u80fd"}}
{"id": "2507.16405", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.16405", "abs": "https://arxiv.org/abs/2507.16405", "authors": ["Stassa Patsantzis"], "title": "Self-Supervised Inductive Logic Programming", "comment": null, "summary": "Inductive Logic Programming (ILP) approaches like Meta \\-/ Interpretive\nLearning (MIL) can learn, from few examples, recursive logic programs with\ninvented predicates that generalise well to unseen instances. This ability\nrelies on a background theory and negative examples, both carefully selected\nwith expert knowledge of a learning problem and its solutions. But what if such\na problem-specific background theory or negative examples are not available? We\nformalise this question as a new setting for Self-Supervised ILP and present a\nnew MIL algorithm that learns in the new setting from some positive labelled,\nand zero or more unlabelled examples, and automatically generates, and labels,\nnew positive and negative examples during learning. We implement this algorithm\nin Prolog in a new MIL system, called Poker. We compare Poker to\nstate-of-the-art MIL system Louise on experiments learning grammars for\nContext-Free and L-System languages from labelled, positive example strings, no\nnegative examples, and just the terminal vocabulary of a language, seen in\nexamples, as a first-order background theory. We introduce a new approach for\nthe principled selection of a second-order background theory as a Second Order\nDefinite Normal Form (SONF), sufficiently general to learn all programs in a\nclass, thus removing the need for a backgound theory tailored to a learning\ntask. We find that Poker's performance improves with increasing numbers of\nautomatically generated examples while Louise, bereft of negative examples,\nover-generalises.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u81ea\u76d1\u7763\u5f52\u7eb3\u903b\u8f91\u7f16\u7a0b(ILP)\u65b9\u6cd5Poker\uff0c\u80fd\u591f\u5728\u7f3a\u4e4f\u4e13\u5bb6\u77e5\u8bc6\u8bbe\u8ba1\u7684\u80cc\u666f\u7406\u8bba\u548c\u8d1f\u4f8b\u7684\u60c5\u51b5\u4e0b\uff0c\u4ec5\u4ece\u5c11\u91cf\u6b63\u4f8b\u548c\u65e0\u6807\u7b7e\u6837\u672c\u4e2d\u5b66\u4e60\u9012\u5f52\u903b\u8f91\u7a0b\u5e8f\uff0c\u5e76\u5728\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u81ea\u52a8\u751f\u6210\u548c\u6807\u6ce8\u65b0\u7684\u6b63\u8d1f\u4f8b\u3002", "motivation": "\u4f20\u7edf\u7684\u5f52\u7eb3\u903b\u8f91\u7f16\u7a0b\u65b9\u6cd5\u5982\u5143\u89e3\u91ca\u5b66\u4e60(MIL)\u9700\u8981\u4f9d\u8d56\u4e13\u5bb6\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u95ee\u9898\u7279\u5b9a\u80cc\u666f\u7406\u8bba\u548c\u8d1f\u4f8b\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8fd9\u4e9b\u5148\u9a8c\u77e5\u8bc6\u5f80\u5f80\u4e0d\u53ef\u83b7\u5f97\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5728\u7f3a\u4e4f\u8fd9\u4e9b\u4e13\u5bb6\u77e5\u8bc6\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u81ea\u76d1\u7763\u5b66\u4e60\u7684ILP\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684MIL\u7b97\u6cd5\uff0c\u5b9e\u73b0\u5728Poker\u7cfb\u7edf\u4e2d\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u4ece\u6b63\u4f8b\u6807\u6ce8\u6837\u672c\u548c\u96f6\u4e2a\u6216\u591a\u4e2a\u65e0\u6807\u7b7e\u6837\u672c\u4e2d\u5b66\u4e60\uff0c\u5728\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u81ea\u52a8\u751f\u6210\u5e76\u6807\u6ce8\u65b0\u7684\u6b63\u8d1f\u4f8b\u3002\u540c\u65f6\u5f15\u5165\u4e86\u4e8c\u9636\u786e\u5b9a\u6b63\u89c4\u5f62\u5f0f(SONF)\u6765\u8fdb\u884c\u80cc\u666f\u7406\u8bba\u7684\u539f\u5219\u6027\u9009\u62e9\uff0c\u4f7f\u5176\u8db3\u591f\u901a\u7528\u4ee5\u5b66\u4e60\u67d0\u7c7b\u4e2d\u7684\u6240\u6709\u7a0b\u5e8f\u3002", "result": "\u5728\u4e0a\u4e0b\u6587\u65e0\u5173\u8bed\u6cd5\u548cL-\u7cfb\u7edf\u8bed\u8a00\u7684\u8bed\u6cd5\u5b66\u4e60\u5b9e\u9a8c\u4e2d\uff0cPoker\u7cfb\u7edf\u7684\u6027\u80fd\u968f\u7740\u81ea\u52a8\u751f\u6210\u6837\u672c\u6570\u91cf\u7684\u589e\u52a0\u800c\u63d0\u5347\uff0c\u800c\u7f3a\u4e4f\u8d1f\u4f8b\u7684Louise\u7cfb\u7edf\u5219\u51fa\u73b0\u8fc7\u6cdb\u5316\u95ee\u9898\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u4ec5\u4f7f\u7528\u7ec8\u7aef\u8bcd\u6c47\u8868\u4f5c\u4e3a\u4e00\u9636\u80cc\u666f\u7406\u8bba\u7684\u60c5\u51b5\u4e0b\u7684\u6709\u6548\u6027\u3002", "conclusion": "Poker\u7cfb\u7edf\u6210\u529f\u89e3\u51b3\u4e86\u81ea\u76d1\u7763ILP\u8bbe\u7f6e\u4e0b\u7684\u5b66\u4e60\u95ee\u9898\uff0c\u901a\u8fc7\u81ea\u52a8\u751f\u6210\u8bad\u7ec3\u6837\u672c\u907f\u514d\u4e86\u5bf9\u4e13\u5bb6\u8bbe\u8ba1\u80cc\u666f\u7406\u8bba\u548c\u8d1f\u4f8b\u7684\u4f9d\u8d56\uff0c\u4e3aILP\u5728\u7f3a\u4e4f\u5148\u9a8c\u77e5\u8bc6\u573a\u666f\u4e0b\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.16414", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16414", "abs": "https://arxiv.org/abs/2507.16414", "authors": ["Hongyi Tang", "Zhihao Zhu", "Yi Yang"], "title": "Identifying Pre-training Data in LLMs: A Neuron Activation-Based Detection Framework", "comment": null, "summary": "The performance of large language models (LLMs) is closely tied to their\ntraining data, which can include copyrighted material or private information,\nraising legal and ethical concerns. Additionally, LLMs face criticism for\ndataset contamination and internalizing biases. To address these issues, the\nPre-Training Data Detection (PDD) task was proposed to identify if specific\ndata was included in an LLM's pre-training corpus. However, existing PDD\nmethods often rely on superficial features like prediction confidence and loss,\nresulting in mediocre performance. To improve this, we introduce NA-PDD, a\nnovel algorithm analyzing differential neuron activation patterns between\ntraining and non-training data in LLMs. This is based on the observation that\nthese data types activate different neurons during LLM inference. We also\nintroduce CCNewsPDD, a temporally unbiased benchmark employing rigorous data\ntransformations to ensure consistent time distributions between training and\nnon-training data. Our experiments demonstrate that NA-PDD significantly\noutperforms existing methods across three benchmarks and multiple LLMs.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86NA-PDD\u7b97\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u5927\u8bed\u8a00\u6a21\u578b\u4e2d\u8bad\u7ec3\u6570\u636e\u548c\u975e\u8bad\u7ec3\u6570\u636e\u7684\u795e\u7ecf\u5143\u6fc0\u6d3b\u6a21\u5f0f\u5dee\u5f02\u6765\u68c0\u6d4b\u9884\u8bad\u7ec3\u6570\u636e\uff0c\u5e76\u6784\u5efa\u4e86CCNewsPDD\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bad\u7ec3\u6570\u636e\u53ef\u80fd\u5305\u542b\u7248\u6743\u6750\u6599\u6216\u79c1\u4eba\u4fe1\u606f\uff0c\u5f15\u53d1\u6cd5\u5f8b\u548c\u4f26\u7406\u95ee\u9898\uff0c\u540c\u65f6\u9762\u4e34\u6570\u636e\u96c6\u6c61\u67d3\u548c\u504f\u89c1\u5185\u5316\u7684\u6279\u8bc4\u3002\u73b0\u6709\u7684\u9884\u8bad\u7ec3\u6570\u636e\u68c0\u6d4b(PDD)\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u548c\u635f\u5931\u7b49\u8868\u9762\u7279\u5f81\uff0c\u6027\u80fd\u4e00\u822c\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u63d0\u51faNA-PDD\u7b97\u6cd5\uff0c\u57fa\u4e8e\u8bad\u7ec3\u6570\u636e\u548c\u975e\u8bad\u7ec3\u6570\u636e\u5728\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u8fc7\u7a0b\u4e2d\u6fc0\u6d3b\u4e0d\u540c\u795e\u7ecf\u5143\u7684\u89c2\u5bdf\uff0c\u5206\u6790\u5dee\u5f02\u5316\u795e\u7ecf\u5143\u6fc0\u6d3b\u6a21\u5f0f\u3002\u540c\u65f6\u6784\u5efaCCNewsPDD\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u91c7\u7528\u4e25\u683c\u7684\u6570\u636e\u53d8\u6362\u786e\u4fdd\u8bad\u7ec3\u548c\u975e\u8bad\u7ec3\u6570\u636e\u4e4b\u95f4\u65f6\u95f4\u5206\u5e03\u7684\u4e00\u81f4\u6027\uff0c\u6d88\u9664\u65f6\u95f4\u504f\u5dee\u3002", "result": "NA-PDD\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u548c\u591a\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u9884\u8bad\u7ec3\u6570\u636e\u68c0\u6d4b\u65b9\u6cd5\uff0c\u6027\u80fd\u6709\u660e\u663e\u63d0\u5347\u3002", "conclusion": "\u901a\u8fc7\u5206\u6790\u795e\u7ecf\u5143\u6fc0\u6d3b\u6a21\u5f0f\u5dee\u5f02\u7684NA-PDD\u7b97\u6cd5\u4e3a\u9884\u8bad\u7ec3\u6570\u636e\u68c0\u6d4b\u63d0\u4f9b\u4e86\u66f4\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u66f4\u51c6\u786e\u5730\u8bc6\u522b\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u7279\u5b9a\u5185\u5bb9\uff0c\u6709\u52a9\u4e8e\u89e3\u51b3\u7248\u6743\u548c\u9690\u79c1\u76f8\u5173\u7684\u6cd5\u5f8b\u4f26\u7406\u95ee\u9898\u3002"}}
{"id": "2507.16434", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.16434", "abs": "https://arxiv.org/abs/2507.16434", "authors": ["Stassa Patsantzis"], "title": "From model-based learning to model-free behaviour with Meta-Interpretive Learning", "comment": null, "summary": "A \"model\" is a theory that describes the state of an environment and the\neffects of an agent's decisions on the environment. A model-based agent can use\nits model to predict the effects of its future actions and so plan ahead, but\nmust know the state of the environment. A model-free agent cannot plan, but can\nact without a model and without completely observing the environment. An\nautonomous agent capable of acting independently in novel environments must\ncombine both sets of capabilities. We show how to create such an agent with\nMeta-Interpretive Learning used to learn a model-based Solver used to train a\nmodel-free Controller that can solve the same planning problems as the Solver.\nWe demonstrate the equivalence in problem-solving ability of the two agents on\ngrid navigation problems in two kinds of environment: randomly generated mazes,\nand lake maps with wide open areas. We find that all navigation problems solved\nby the Solver are also solved by the Controller, indicating the two are\nequivalent.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6a21\u578b\u9a71\u52a8\u548c\u65e0\u6a21\u578b\u65b9\u6cd5\u7684\u81ea\u4e3b\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u901a\u8fc7\u5143\u89e3\u91ca\u5b66\u4e60\u8bad\u7ec3\u6a21\u578b\u9a71\u52a8\u6c42\u89e3\u5668\uff0c\u518d\u7528\u5176\u8bad\u7ec3\u65e0\u6a21\u578b\u63a7\u5236\u5668\uff0c\u5728\u7f51\u683c\u5bfc\u822a\u4efb\u52a1\u4e2d\u9a8c\u8bc1\u4e86\u4e24\u8005\u7684\u7b49\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u667a\u80fd\u4f53\u8981\u4e48\u662f\u6a21\u578b\u9a71\u52a8\u7684\uff08\u80fd\u89c4\u5212\u4f46\u9700\u8981\u5b8c\u5168\u89c2\u5bdf\u73af\u5883\u72b6\u6001\uff09\uff0c\u8981\u4e48\u662f\u65e0\u6a21\u578b\u7684\uff08\u4e0d\u9700\u8981\u6a21\u578b\u4f46\u65e0\u6cd5\u89c4\u5212\uff09\u3002\u4e3a\u4e86\u5728\u65b0\u73af\u5883\u4e2d\u72ec\u7acb\u884c\u52a8\uff0c\u9700\u8981\u7ed3\u5408\u4e24\u79cd\u80fd\u529b\u7684\u81ea\u4e3b\u667a\u80fd\u4f53\u3002", "method": "\u4f7f\u7528\u5143\u89e3\u91ca\u5b66\u4e60(Meta-Interpretive Learning)\u6765\u5b66\u4e60\u6a21\u578b\u9a71\u52a8\u7684\u6c42\u89e3\u5668(Solver)\uff0c\u7136\u540e\u7528\u8be5\u6c42\u89e3\u5668\u8bad\u7ec3\u65e0\u6a21\u578b\u7684\u63a7\u5236\u5668(Controller)\uff0c\u4f7f\u63a7\u5236\u5668\u80fd\u591f\u89e3\u51b3\u4e0e\u6c42\u89e3\u5668\u76f8\u540c\u7684\u89c4\u5212\u95ee\u9898\u3002", "result": "\u5728\u4e24\u79cd\u7f51\u683c\u5bfc\u822a\u73af\u5883\uff08\u968f\u673a\u751f\u6210\u7684\u8ff7\u5bab\u548c\u5177\u6709\u5f00\u9614\u533a\u57df\u7684\u6e56\u6cca\u5730\u56fe\uff09\u4e2d\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u6c42\u89e3\u5668\u80fd\u89e3\u51b3\u7684\u6240\u6709\u5bfc\u822a\u95ee\u9898\uff0c\u63a7\u5236\u5668\u4e5f\u90fd\u80fd\u89e3\u51b3\uff0c\u8bc1\u660e\u4e86\u4e24\u8005\u7684\u7b49\u6548\u6027\u3002", "conclusion": "\u6210\u529f\u521b\u5efa\u4e86\u4e00\u4e2a\u7ed3\u5408\u6a21\u578b\u9a71\u52a8\u548c\u65e0\u6a21\u578b\u80fd\u529b\u7684\u81ea\u4e3b\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u5143\u89e3\u91ca\u5b66\u4e60\u5b9e\u73b0\u4e86\u77e5\u8bc6\u4ece\u6a21\u578b\u9a71\u52a8\u6c42\u89e3\u5668\u5230\u65e0\u6a21\u578b\u63a7\u5236\u5668\u7684\u6709\u6548\u8f6c\u79fb\uff0c\u4e24\u8005\u5728\u95ee\u9898\u89e3\u51b3\u80fd\u529b\u4e0a\u5177\u6709\u7b49\u6548\u6027\u3002"}}
{"id": "2507.16454", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.16454", "abs": "https://arxiv.org/abs/2507.16454", "authors": ["Pierangela Bruno", "Carmine Dodaro", "Giuseppe Galat\u00e0", "Marco Maratea", "Marco Mochi"], "title": "Improving ASP-based ORS Schedules through Machine Learning Predictions", "comment": "17 pages, International Conference on Logic Programming, Under\n  consideration in Theory and Practice of Logic Programming (TPLP)", "summary": "The Operating Room Scheduling (ORS) problem deals with the optimization of\ndaily operating room surgery schedules. It is a challenging problem subject to\nmany constraints, like to determine the starting time of different surgeries\nand allocating the required resources, including the availability of beds in\ndifferent department units. Recently, solutions to this problem based on Answer\nSet Programming (ASP) have been delivered. Such solutions are overall\nsatisfying but, when applied to real data, they can currently only verify\nwhether the encoding aligns with the actual data and, at most, suggest\nalternative schedules that could have been computed. As a consequence, it is\nnot currently possible to generate provisional schedules. Furthermore, the\nresulting schedules are not always robust.\n  In this paper, we integrate inductive and deductive techniques for solving\nthese issues. We first employ machine learning algorithms to predict the\nsurgery duration, from historical data, to compute provisional schedules. Then,\nwe consider the confidence of such predictions as an additional input to our\nproblem and update the encoding correspondingly in order to compute more robust\nschedules. Results on historical data from the ASL1 Liguria in Italy confirm\nthe viability of our integration.\n  Under consideration in Theory and Practice of Logic Programming (TPLP).", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u548c\u7b54\u96c6\u7f16\u7a0b(ASP)\u6280\u672f\uff0c\u89e3\u51b3\u624b\u672f\u5ba4\u8c03\u5ea6\u95ee\u9898\u4e2d\u65e0\u6cd5\u751f\u6210\u4e34\u65f6\u8c03\u5ea6\u548c\u8c03\u5ea6\u4e0d\u591f\u9c81\u68d2\u7684\u95ee\u9898\uff0c\u4f7f\u7528\u5386\u53f2\u6570\u636e\u9884\u6d4b\u624b\u672f\u65f6\u957f\u5e76\u8003\u8651\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u6765\u8ba1\u7b97\u66f4\u9c81\u68d2\u7684\u8c03\u5ea6\u65b9\u6848\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u7b54\u96c6\u7f16\u7a0b\u7684\u624b\u672f\u5ba4\u8c03\u5ea6\u89e3\u51b3\u65b9\u6848\u53ea\u80fd\u9a8c\u8bc1\u7f16\u7801\u662f\u5426\u4e0e\u5b9e\u9645\u6570\u636e\u5bf9\u9f50\uff0c\u6700\u591a\u5efa\u8bae\u53ef\u8ba1\u7b97\u7684\u66ff\u4ee3\u8c03\u5ea6\uff0c\u65e0\u6cd5\u751f\u6210\u4e34\u65f6\u8c03\u5ea6\uff0c\u4e14\u751f\u6210\u7684\u8c03\u5ea6\u65b9\u6848\u4e0d\u591f\u9c81\u68d2\u3002", "method": "\u96c6\u6210\u5f52\u7eb3\u548c\u6f14\u7ece\u6280\u672f\uff1a\u9996\u5148\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u4ece\u5386\u53f2\u6570\u636e\u4e2d\u9884\u6d4b\u624b\u672f\u6301\u7eed\u65f6\u95f4\u6765\u8ba1\u7b97\u4e34\u65f6\u8c03\u5ea6\uff1b\u7136\u540e\u5c06\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u4f5c\u4e3a\u989d\u5916\u8f93\u5165\uff0c\u76f8\u5e94\u66f4\u65b0\u7f16\u7801\u4ee5\u8ba1\u7b97\u66f4\u9c81\u68d2\u7684\u8c03\u5ea6\u65b9\u6848\u3002", "result": "\u5728\u610f\u5927\u5229\u5229\u53e4\u91cc\u4e9aASL1\u7684\u5386\u53f2\u6570\u636e\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u5b9e\u4e86\u8be5\u96c6\u6210\u65b9\u6cd5\u7684\u53ef\u884c\u6027\uff0c\u80fd\u591f\u751f\u6210\u4e34\u65f6\u8c03\u5ea6\u5e76\u63d0\u9ad8\u8c03\u5ea6\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u901a\u8fc7\u5c06\u673a\u5668\u5b66\u4e60\u9884\u6d4b\u4e0e\u7b54\u96c6\u7f16\u7a0b\u76f8\u7ed3\u5408\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u624b\u672f\u5ba4\u8c03\u5ea6\u95ee\u9898\u4e2d\u7684\u5173\u952e\u9650\u5236\uff0c\u5b9e\u73b0\u4e86\u4e34\u65f6\u8c03\u5ea6\u751f\u6210\u548c\u66f4\u9c81\u68d2\u7684\u8c03\u5ea6\u8ba1\u7b97\uff0c\u4e3a\u5b9e\u9645\u533b\u9662\u8fd0\u8425\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.16473", "categories": ["cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2507.16473", "abs": "https://arxiv.org/abs/2507.16473", "authors": ["Chang Li", "Yaren Zhang", "Haoran Lv", "Qiong Cao", "Chao Xue", "Xiaodong He"], "title": "Learning Temporal Abstractions via Variational Homomorphisms in Option-Induced Abstract MDPs", "comment": null, "summary": "Large Language Models (LLMs) have shown remarkable reasoning ability through\nexplicit Chain-of-Thought (CoT) prompting, but generating these step-by-step\ntextual explanations is computationally expensive and slow. To overcome this,\nwe aim to develop a framework for efficient, implicit reasoning, where the\nmodel \"thinks\" in a latent space without generating explicit text for every\nstep. We propose that these latent thoughts can be modeled as\ntemporally-extended abstract actions, or options, within a hierarchical\nreinforcement learning framework. To effectively learn a diverse library of\noptions as latent embeddings, we first introduce the Variational Markovian\nOption Critic (VMOC), an off-policy algorithm that uses variational inference\nwithin the HiT-MDP framework. To provide a rigorous foundation for using these\noptions as an abstract reasoning space, we extend the theory of continuous MDP\nhomomorphisms. This proves that learning a policy in the simplified, abstract\nlatent space, for which VMOC is suited, preserves the optimality of the\nsolution to the original, complex problem. Finally, we propose a cold-start\nprocedure that leverages supervised fine-tuning (SFT) data to distill human\nreasoning demonstrations into this latent option space, providing a rich\ninitialization for the model's reasoning capabilities. Extensive experiments\ndemonstrate that our approach achieves strong performance on complex logical\nreasoning benchmarks and challenging locomotion tasks, validating our framework\nas a principled method for learning abstract skills for both language and\ncontrol.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u9690\u5f0f\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u9009\u9879(options)\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u8fdb\u884c\"\u601d\u8003\"\uff0c\u907f\u514d\u4e86\u663e\u5f0f\u94fe\u5f0f\u601d\u7ef4\u63a8\u7406\u7684\u8ba1\u7b97\u5f00\u9500\uff0c\u5e76\u5728\u590d\u6742\u903b\u8f91\u63a8\u7406\u548c\u8fd0\u52a8\u63a7\u5236\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u826f\u597d\u6548\u679c\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u901a\u8fc7\u663e\u5f0f\u7684\u94fe\u5f0f\u601d\u7ef4(CoT)\u63d0\u793a\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u751f\u6210\u9010\u6b65\u7684\u6587\u672c\u89e3\u91ca\u5728\u8ba1\u7b97\u4e0a\u6602\u8d35\u4e14\u7f13\u6162\u3002\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u9ad8\u6548\u7684\u9690\u5f0f\u63a8\u7406\u6846\u67b6\uff0c\u8ba9\u6a21\u578b\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\"\u601d\u8003\"\u800c\u65e0\u9700\u4e3a\u6bcf\u4e00\u6b65\u751f\u6210\u663e\u5f0f\u6587\u672c\u3002", "method": "\u63d0\u51fa\u5c06\u6f5c\u5728\u601d\u7ef4\u5efa\u6a21\u4e3a\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u4e2d\u7684\u65f6\u95f4\u6269\u5c55\u62bd\u8c61\u52a8\u4f5c(\u9009\u9879)\u3002\u5f15\u5165\u53d8\u5206\u9a6c\u5c14\u53ef\u592b\u9009\u9879\u8bc4\u4f30\u5668(VMOC)\u4f5c\u4e3a\u79bb\u7b56\u7565\u7b97\u6cd5\uff0c\u4f7f\u7528\u53d8\u5206\u63a8\u7406\u5b66\u4e60\u591a\u6837\u5316\u7684\u9009\u9879\u5e93\u3002\u6269\u5c55\u8fde\u7eedMDP\u540c\u6001\u7406\u8bba\u4e3a\u4f7f\u7528\u9009\u9879\u4f5c\u4e3a\u62bd\u8c61\u63a8\u7406\u7a7a\u95f4\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u3002\u8bbe\u8ba1\u51b7\u542f\u52a8\u7a0b\u5e8f\uff0c\u5229\u7528\u76d1\u7763\u5fae\u8c03\u6570\u636e\u5c06\u4eba\u7c7b\u63a8\u7406\u6f14\u793a\u84b8\u998f\u5230\u6f5c\u5728\u9009\u9879\u7a7a\u95f4\u4e2d\u3002", "result": "\u5728\u590d\u6742\u903b\u8f91\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u548c\u5177\u6709\u6311\u6218\u6027\u7684\u8fd0\u52a8\u4efb\u52a1\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u53d6\u5f97\u4e86\u5f3a\u52b2\u7684\u6027\u80fd\u8868\u73b0\uff0c\u9a8c\u8bc1\u4e86\u6846\u67b6\u4f5c\u4e3a\u5b66\u4e60\u8bed\u8a00\u548c\u63a7\u5236\u62bd\u8c61\u6280\u80fd\u7684\u539f\u5219\u6027\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u9ad8\u6548\u9690\u5f0f\u63a8\u7406\uff0c\u901a\u8fc7\u5206\u5c42\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u9009\u9879\u673a\u5236\u907f\u514d\u4e86\u663e\u5f0f\u6587\u672c\u751f\u6210\u7684\u8ba1\u7b97\u5f00\u9500\uff0c\u4e3a\u8bed\u8a00\u6a21\u578b\u7684\u62bd\u8c61\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u4e00\u79cd\u539f\u5219\u6027\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.16478", "categories": ["cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.16478", "abs": "https://arxiv.org/abs/2507.16478", "authors": ["Shreya Saxena", "Siva Prasad", "Zishan Ahmad", "Vishal Vaddina"], "title": "ACT: Bridging the Gap in Code Translation through Synthetic Data Generation & Adaptive Training", "comment": null, "summary": "Code translation is a crucial process in software development and migration\nprojects, enabling interoperability between different programming languages and\nenhancing software adaptability and thus longevity. Traditional automated\ntranslation methods rely heavily on handcrafted transformation rules, which\noften lack flexibility and scalability. Meanwhile, advanced language models\npresent promising alternatives but are often limited by proprietary, API-based\nimplementations that raise concerns over data security and reliance. In this\npaper, we present Auto-Train for Code Translation (ACT), an innovative\nframework that aims to improve code translation capabilities by enabling\nin-house finetuning of open-source Large Language Models (LLMs). ACT's\nautomated pipeline significantly boosts the performance of these models,\nnarrowing the gap between open-source accessibility and the high performance of\nclosed-source solutions. Central to ACT is its synthetic data generation\nmodule, which builds extensive, high-quality datasets from initial code\nsamples, incorporating unit tests to ensure functional accuracy and diversity.\nACT's evaluation framework incorporates execution-level checks, offering a\ncomprehensive assessment of translation quality. A key feature in ACT is its\ncontroller module, which manages the entire pipeline by dynamically adjusting\nhyperparameters, orchestrating iterative data generation, and finetuning based\non real-time evaluations. This enables ACT to intelligently optimize when to\ncontinue training, generate additional targeted training data, or stop the\nprocess. Our results demonstrate that ACT consistently enhances the\neffectiveness of open-source models, offering businesses and developers a\nsecure and reliable alternative. Additionally, applying our data generation\npipeline to industry-scale migration projects has led to a notable increase in\ndeveloper acceleration.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86ACT\uff08Auto-Train for Code Translation\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u5fae\u8c03\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\u6765\u6539\u8fdb\u4ee3\u7801\u7ffb\u8bd1\u80fd\u529b\uff0c\u4f7f\u7528\u5408\u6210\u6570\u636e\u751f\u6210\u548c\u52a8\u6001\u63a7\u5236\u5668\u7ba1\u7406\u6574\u4e2a\u8bad\u7ec3\u6d41\u7a0b\uff0c\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u5b89\u5168\u53ef\u9760\u7684\u4ee3\u7801\u7ffb\u8bd1\u66ff\u4ee3\u65b9\u6848\u3002", "motivation": "\u4f20\u7edf\u7684\u81ea\u52a8\u5316\u4ee3\u7801\u7ffb\u8bd1\u65b9\u6cd5\u4f9d\u8d56\u624b\u5de5\u5236\u4f5c\u7684\u8f6c\u6362\u89c4\u5219\uff0c\u7f3a\u4e4f\u7075\u6d3b\u6027\u548c\u53ef\u6269\u5c55\u6027\uff1b\u800c\u5148\u8fdb\u7684\u8bed\u8a00\u6a21\u578b\u867d\u7136\u6709\u524d\u666f\uff0c\u4f46\u5f80\u5f80\u53d7\u9650\u4e8e\u4e13\u6709API\u5b9e\u73b0\uff0c\u5b58\u5728\u6570\u636e\u5b89\u5168\u548c\u4f9d\u8d56\u6027\u95ee\u9898\u3002\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u80fd\u591f\u6539\u8fdb\u5f00\u6e90\u6a21\u578b\u4ee3\u7801\u7ffb\u8bd1\u80fd\u529b\u7684\u6846\u67b6\u3002", "method": "ACT\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a1\uff09\u5408\u6210\u6570\u636e\u751f\u6210\u6a21\u5757\uff1a\u4ece\u521d\u59cb\u4ee3\u7801\u6837\u672c\u6784\u5efa\u5927\u89c4\u6a21\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\uff0c\u7ed3\u5408\u5355\u5143\u6d4b\u8bd5\u786e\u4fdd\u529f\u80fd\u51c6\u786e\u6027\u548c\u591a\u6837\u6027\uff1b2\uff09\u8bc4\u4f30\u6846\u67b6\uff1a\u91c7\u7528\u6267\u884c\u7ea7\u68c0\u67e5\u63d0\u4f9b\u5168\u9762\u7684\u7ffb\u8bd1\u8d28\u91cf\u8bc4\u4f30\uff1b3\uff09\u63a7\u5236\u5668\u6a21\u5757\uff1a\u7ba1\u7406\u6574\u4e2a\u6d41\u7a0b\uff0c\u52a8\u6001\u8c03\u6574\u8d85\u53c2\u6570\uff0c\u57fa\u4e8e\u5b9e\u65f6\u8bc4\u4f30\u534f\u8c03\u8fed\u4ee3\u6570\u636e\u751f\u6210\u548c\u5fae\u8c03\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eACT\u6846\u67b6\u80fd\u591f\u6301\u7eed\u63d0\u5347\u5f00\u6e90\u6a21\u578b\u7684\u6548\u679c\uff0c\u7f29\u5c0f\u4e86\u5f00\u6e90\u6a21\u578b\u53ef\u8bbf\u95ee\u6027\u4e0e\u95ed\u6e90\u89e3\u51b3\u65b9\u6848\u9ad8\u6027\u80fd\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002\u5c06\u6570\u636e\u751f\u6210\u6d41\u7a0b\u5e94\u7528\u5230\u5de5\u4e1a\u7ea7\u8fc1\u79fb\u9879\u76ee\u4e2d\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5f00\u53d1\u8005\u7684\u5de5\u4f5c\u6548\u7387\u3002", "conclusion": "ACT\u4e3a\u4f01\u4e1a\u548c\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b89\u5168\u53ef\u9760\u7684\u4ee3\u7801\u7ffb\u8bd1\u66ff\u4ee3\u65b9\u6848\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u5fae\u8c03\u5f00\u6e90\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u6709\u6548\u6539\u8fdb\u4e86\u4ee3\u7801\u7ffb\u8bd1\u80fd\u529b\uff0c\u5e76\u5728\u5b9e\u9645\u5de5\u4e1a\u5e94\u7528\u4e2d\u8bc1\u660e\u4e86\u5176\u4ef7\u503c\u3002"}}
{"id": "2507.16507", "categories": ["cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.16507", "abs": "https://arxiv.org/abs/2507.16507", "authors": ["Jean Lelong", "Adnane Errazine", "Annabelle Blangero"], "title": "Agentic RAG with Knowledge Graphs for Complex Multi-Hop Reasoning in Real-World Applications", "comment": "ECAI 2025 demo track, 4 pages", "summary": "Conventional Retrieval-Augmented Generation (RAG) systems enhance Large\nLanguage Models (LLMs) but often fall short on complex queries, delivering\nlimited, extractive answers and struggling with multiple targeted retrievals or\nnavigating intricate entity relationships. This is a critical gap in\nknowledge-intensive domains. We introduce INRAExplorer, an agentic RAG system\nfor exploring the scientific data of INRAE (France's National Research\nInstitute for Agriculture, Food and Environment). INRAExplorer employs an\nLLM-based agent with a multi-tool architecture to dynamically engage a rich\nknowledge base, through a comprehensive knowledge graph derived from open\naccess INRAE publications. This design empowers INRAExplorer to conduct\niterative, targeted queries, retrieve exhaustive datasets (e.g., all\npublications by an author), perform multi-hop reasoning, and deliver\nstructured, comprehensive answers. INRAExplorer serves as a concrete\nillustration of enhancing knowledge interaction in specialized fields.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86INRAExplorer\uff0c\u4e00\u4e2a\u57fa\u4e8e\u667a\u80fd\u4f53\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u7cfb\u7edf\uff0c\u4e13\u95e8\u7528\u4e8e\u63a2\u7d22\u6cd5\u56fd\u56fd\u5bb6\u519c\u4e1a\u98df\u54c1\u4e0e\u73af\u5883\u7814\u7a76\u9662(INRAE)\u7684\u79d1\u5b66\u6570\u636e\uff0c\u901a\u8fc7\u591a\u5de5\u5177\u67b6\u6784\u548c\u77e5\u8bc6\u56fe\u8c31\u5b9e\u73b0\u590d\u6742\u67e5\u8be2\u5904\u7406\u548c\u591a\u8df3\u63a8\u7406\u3002", "motivation": "\u4f20\u7edf\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210(RAG)\u7cfb\u7edf\u5728\u5904\u7406\u590d\u6742\u67e5\u8be2\u65f6\u5b58\u5728\u660e\u663e\u4e0d\u8db3\uff0c\u53ea\u80fd\u63d0\u4f9b\u6709\u9650\u7684\u63d0\u53d6\u5f0f\u7b54\u6848\uff0c\u5728\u591a\u76ee\u6807\u68c0\u7d22\u548c\u590d\u6742\u5b9e\u4f53\u5173\u7cfb\u5bfc\u822a\u65b9\u9762\u8868\u73b0\u56f0\u96be\uff0c\u8fd9\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u9886\u57df\u662f\u4e00\u4e2a\u5173\u952e\u7f3a\u9677\u3002", "method": "\u5f15\u5165INRAExplorer\uff0c\u4e00\u4e2a\u57fa\u4e8eLLM\u667a\u80fd\u4f53\u7684RAG\u7cfb\u7edf\uff0c\u91c7\u7528\u591a\u5de5\u5177\u67b6\u6784\u6765\u52a8\u6001\u63a5\u5165\u4e30\u5bcc\u7684\u77e5\u8bc6\u5e93\uff0c\u901a\u8fc7\u4ece\u5f00\u653e\u83b7\u53d6\u7684INRAE\u51fa\u7248\u7269\u6784\u5efa\u7684\u7efc\u5408\u77e5\u8bc6\u56fe\u8c31\u6765\u5b9e\u73b0\u529f\u80fd\u3002", "result": "INRAExplorer\u80fd\u591f\u8fdb\u884c\u8fed\u4ee3\u5f0f\u76ee\u6807\u67e5\u8be2\uff0c\u68c0\u7d22\u8be6\u5c3d\u7684\u6570\u636e\u96c6(\u5982\u67d0\u4f5c\u8005\u7684\u5168\u90e8\u51fa\u7248\u7269)\uff0c\u6267\u884c\u591a\u8df3\u63a8\u7406\uff0c\u5e76\u63d0\u4f9b\u7ed3\u6784\u5316\u7684\u7efc\u5408\u6027\u7b54\u6848\u3002", "conclusion": "INRAExplorer\u4e3a\u5728\u4e13\u4e1a\u9886\u57df\u4e2d\u589e\u5f3a\u77e5\u8bc6\u4ea4\u4e92\u63d0\u4f9b\u4e86\u5177\u4f53\u7684\u89e3\u51b3\u65b9\u6848\u793a\u4f8b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edfRAG\u7cfb\u7edf\u5728\u590d\u6742\u67e5\u8be2\u5904\u7406\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2507.16534", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.16534", "abs": "https://arxiv.org/abs/2507.16534", "authors": ["Shanghai AI Lab", ":", "Xiaoyang Chen", "Yunhao Chen", "Zeren Chen", "Zhiyun Chen", "Hanyun Cui", "Yawen Duan", "Jiaxuan Guo", "Qi Guo", "Xuhao Hu", "Hong Huang", "Lige Huang", "Chunxiao Li", "Juncheng Li", "Qihao Lin", "Dongrui Liu", "Xinmin Liu", "Zicheng Liu", "Chaochao Lu", "Xiaoya Lu", "Jingjing Qu", "Qibing Ren", "Jing Shao", "Jingwei Shi", "Jingwei Sun", "Peng Wang", "Weibing Wang", "Jia Xu", "Lewen Yan", "Xiao Yu", "Yi Yu", "Boxuan Zhang", "Jie Zhang", "Weichen Zhang", "Zhijie Zheng", "Tianyi Zhou", "Bowen Zhou"], "title": "Frontier AI Risk Management Framework in Practice: A Risk Analysis Technical Report", "comment": "97 pages, 37 figures", "summary": "To understand and identify the unprecedented risks posed by rapidly advancing\nartificial intelligence (AI) models, this report presents a comprehensive\nassessment of their frontier risks. Drawing on the E-T-C analysis (deployment\nenvironment, threat source, enabling capability) from the Frontier AI Risk\nManagement Framework (v1.0) (SafeWork-F1-Framework), we identify critical risks\nin seven areas: cyber offense, biological and chemical risks, persuasion and\nmanipulation, uncontrolled autonomous AI R\\&D, strategic deception and\nscheming, self-replication, and collusion. Guided by the \"AI-$45^\\circ$ Law,\"\nwe evaluate these risks using \"red lines\" (intolerable thresholds) and \"yellow\nlines\" (early warning indicators) to define risk zones: green (manageable risk\nfor routine deployment and continuous monitoring), yellow (requiring\nstrengthened mitigations and controlled deployment), and red (necessitating\nsuspension of development and/or deployment). Experimental results show that\nall recent frontier AI models reside in green and yellow zones, without\ncrossing red lines. Specifically, no evaluated models cross the yellow line for\ncyber offense or uncontrolled AI R\\&D risks. For self-replication, and\nstrategic deception and scheming, most models remain in the green zone, except\nfor certain reasoning models in the yellow zone. In persuasion and\nmanipulation, most models are in the yellow zone due to their effective\ninfluence on humans. For biological and chemical risks, we are unable to rule\nout the possibility of most models residing in the yellow zone, although\ndetailed threat modeling and in-depth assessment are required to make further\nclaims. This work reflects our current understanding of AI frontier risks and\nurges collective action to mitigate these challenges.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5168\u9762\u7684AI\u524d\u6cbf\u98ce\u9669\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7E-T-C\u5206\u6790\u65b9\u6cd5\u8bc6\u522b\u4e86\u4e03\u4e2a\u5173\u952e\u98ce\u9669\u9886\u57df\uff0c\u5e76\u4f7f\u7528\u7ea2\u7ebf\u3001\u9ec4\u7ebf\u548c\u7eff\u7ebf\u7cfb\u7edf\u5bf9\u5f53\u524d\u524d\u6cbfAI\u6a21\u578b\u8fdb\u884c\u98ce\u9669\u5206\u533a\u8bc4\u4f30\uff0c\u53d1\u73b0\u6240\u6709\u6a21\u578b\u90fd\u5904\u4e8e\u7eff\u533a\u548c\u9ec4\u533a\uff0c\u672a\u8de8\u8d8a\u7ea2\u7ebf\u3002", "motivation": "\u968f\u7740\u4eba\u5de5\u667a\u80fd\u6a21\u578b\u5feb\u901f\u53d1\u5c55\uff0c\u9700\u8981\u7406\u89e3\u548c\u8bc6\u522b\u5176\u5e26\u6765\u7684\u524d\u6240\u672a\u6709\u7684\u98ce\u9669\uff0c\u5efa\u7acb\u7cfb\u7edf\u6027\u7684\u98ce\u9669\u8bc4\u4f30\u6846\u67b6\u6765\u6307\u5bfcAI\u7684\u5b89\u5168\u90e8\u7f72\u548c\u53d1\u5c55\u3002", "method": "\u91c7\u7528\u524d\u6cbfAI\u98ce\u9669\u7ba1\u7406\u6846\u67b6\u4e2d\u7684E-T-C\u5206\u6790\u65b9\u6cd5\uff08\u90e8\u7f72\u73af\u5883\u3001\u5a01\u80c1\u6e90\u3001\u4f7f\u80fd\u80fd\u529b\uff09\uff0c\u8bc6\u522b\u4e03\u4e2a\u5173\u952e\u98ce\u9669\u9886\u57df\uff0c\u5e76\u57fa\u4e8e\"AI-45\u00b0\u5b9a\u5f8b\"\u4f7f\u7528\u7ea2\u7ebf\uff08\u4e0d\u53ef\u5bb9\u5fcd\u9608\u503c\uff09\u3001\u9ec4\u7ebf\uff08\u65e9\u671f\u8b66\u544a\u6307\u6807\uff09\u548c\u7eff\u7ebf\u6765\u5212\u5206\u98ce\u9669\u533a\u57df\u3002", "result": "\u6240\u6709\u8fd1\u671f\u524d\u6cbfAI\u6a21\u578b\u90fd\u5904\u4e8e\u7eff\u533a\u548c\u9ec4\u533a\uff0c\u672a\u8de8\u8d8a\u7ea2\u7ebf\u3002\u7f51\u7edc\u653b\u51fb\u548c\u5931\u63a7AI\u7814\u53d1\u98ce\u9669\u672a\u8fbe\u5230\u9ec4\u7ebf\uff1b\u81ea\u6211\u590d\u5236\u548c\u6218\u7565\u6b3a\u9a97\u98ce\u9669\u591a\u6570\u5728\u7eff\u533a\uff0c\u90e8\u5206\u63a8\u7406\u6a21\u578b\u5728\u9ec4\u533a\uff1b\u8bf4\u670d\u64cd\u63a7\u98ce\u9669\u591a\u6570\u6a21\u578b\u5728\u9ec4\u533a\uff1b\u751f\u7269\u5316\u5b66\u98ce\u9669\u65e0\u6cd5\u6392\u9664\u591a\u6570\u6a21\u578b\u5904\u4e8e\u9ec4\u533a\u7684\u53ef\u80fd\u6027\u3002", "conclusion": "\u5f53\u524d\u5bf9AI\u524d\u6cbf\u98ce\u9669\u7684\u7406\u89e3\u8868\u660e\u73b0\u6709\u6a21\u578b\u5c1a\u672a\u8fbe\u5230\u6700\u5371\u9669\u7684\u7ea2\u7ebf\u9608\u503c\uff0c\u4f46\u5728\u591a\u4e2a\u9886\u57df\u5b58\u5728\u9700\u8981\u8b66\u60d5\u7684\u9ec4\u533a\u98ce\u9669\uff0c\u9700\u8981\u96c6\u4f53\u884c\u52a8\u6765\u7f13\u89e3\u8fd9\u4e9b\u6311\u6218\uff0c\u52a0\u5f3a\u98ce\u9669\u7ba1\u63a7\u548c\u5b89\u5168\u90e8\u7f72\u63aa\u65bd\u3002"}}
{"id": "2507.16635", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16635", "abs": "https://arxiv.org/abs/2507.16635", "authors": ["Ali Mohamed Ali", "Luca Tirel", "Hashim A. Hashim"], "title": "Novel Multi-Agent Action Masked Deep Reinforcement Learning for General Industrial Assembly Lines Balancing Problems", "comment": null, "summary": "Efficient planning of activities is essential for modern industrial assembly\nlines to uphold manufacturing standards, prevent project constraint violations,\nand achieve cost-effective operations. While exact solutions to such challenges\ncan be obtained through Integer Programming (IP), the dependence of the search\nspace on input parameters often makes IP computationally infeasible for\nlarge-scale scenarios. Heuristic methods, such as Genetic Algorithms, can also\nbe applied, but they frequently produce suboptimal solutions in extensive\ncases. This paper introduces a novel mathematical model of a generic industrial\nassembly line formulated as a Markov Decision Process (MDP), without imposing\nassumptions on the type of assembly line a notable distinction from most\nexisting models. The proposed model is employed to create a virtual environment\nfor training Deep Reinforcement Learning (DRL) agents to optimize task and\nresource scheduling. To enhance the efficiency of agent training, the paper\nproposes two innovative tools. The first is an action-masking technique, which\nensures the agent selects only feasible actions, thereby reducing training\ntime. The second is a multi-agent approach, where each workstation is managed\nby an individual agent, as a result, the state and action spaces were reduced.\nA centralized training framework with decentralized execution is adopted,\noffering a scalable learning architecture for optimizing industrial assembly\nlines. This framework allows the agents to learn offline and subsequently\nprovide real-time solutions during operations by leveraging a neural network\nthat maps the current factory state to the optimal action. The effectiveness of\nthe proposed scheme is validated through numerical simulations, demonstrating\nsignificantly faster convergence to the optimal solution compared to a\ncomparable model-based approach.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u7684\u5de5\u4e1a\u88c5\u914d\u7ebf\u6570\u5b66\u6a21\u578b\uff0c\u5e76\u4f7f\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u8fdb\u884c\u4efb\u52a1\u548c\u8d44\u6e90\u8c03\u5ea6\u4f18\u5316\uff0c\u901a\u8fc7\u52a8\u4f5c\u63a9\u7801\u6280\u672f\u548c\u591a\u667a\u80fd\u4f53\u65b9\u6cd5\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\uff0c\u5b9e\u73b0\u4e86\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u3002", "motivation": "\u73b0\u4ee3\u5de5\u4e1a\u88c5\u914d\u7ebf\u9700\u8981\u9ad8\u6548\u7684\u6d3b\u52a8\u89c4\u5212\u6765\u7ef4\u6301\u5236\u9020\u6807\u51c6\u3001\u9632\u6b62\u9879\u76ee\u7ea6\u675f\u8fdd\u89c4\u5e76\u5b9e\u73b0\u6210\u672c\u6548\u76ca\u8fd0\u8425\u3002\u867d\u7136\u6574\u6570\u89c4\u5212\u53ef\u4ee5\u63d0\u4f9b\u7cbe\u786e\u89e3\uff0c\u4f46\u5bf9\u4e8e\u5927\u89c4\u6a21\u573a\u666f\u5f80\u5f80\u8ba1\u7b97\u4e0d\u53ef\u884c\uff1b\u9057\u4f20\u7b97\u6cd5\u7b49\u542f\u53d1\u5f0f\u65b9\u6cd5\u867d\u7136\u9002\u7528\uff0c\u4f46\u5728\u5927\u89c4\u6a21\u6848\u4f8b\u4e2d\u7ecf\u5e38\u4ea7\u751f\u6b21\u4f18\u89e3\u3002\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u88c5\u914d\u7ebf\u4f18\u5316\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u901a\u7528\u5de5\u4e1a\u88c5\u914d\u7ebf\u7684\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u6570\u5b66\u6a21\u578b\uff0c\u4e0d\u5bf9\u88c5\u914d\u7ebf\u7c7b\u578b\u505a\u4efb\u4f55\u5047\u8bbe\u3002\u4f7f\u7528\u8be5\u6a21\u578b\u521b\u5efa\u865a\u62df\u73af\u5883\u6765\u8bad\u7ec3\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u667a\u80fd\u4f53\u8fdb\u884c\u4efb\u52a1\u548c\u8d44\u6e90\u8c03\u5ea6\u4f18\u5316\u3002\u4e3a\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\uff0c\u63d0\u51fa\u4e24\u4e2a\u521b\u65b0\u5de5\u5177\uff1a1\uff09\u52a8\u4f5c\u63a9\u7801\u6280\u672f\uff0c\u786e\u4fdd\u667a\u80fd\u4f53\u53ea\u9009\u62e9\u53ef\u884c\u52a8\u4f5c\uff1b2\uff09\u591a\u667a\u80fd\u4f53\u65b9\u6cd5\uff0c\u6bcf\u4e2a\u5de5\u4f5c\u7ad9\u7531\u72ec\u7acb\u667a\u80fd\u4f53\u7ba1\u7406\u3002\u91c7\u7528\u96c6\u4e2d\u8bad\u7ec3\u5206\u6563\u6267\u884c\u7684\u6846\u67b6\u3002", "result": "\u901a\u8fc7\u6570\u503c\u4eff\u771f\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6848\u7684\u6709\u6548\u6027\uff0c\u4e0e\u53ef\u6bd4\u8f83\u7684\u57fa\u4e8e\u6a21\u578b\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5c55\u793a\u4e86\u663e\u8457\u66f4\u5feb\u7684\u6700\u4f18\u89e3\u6536\u655b\u901f\u5ea6\u3002\u8be5\u6846\u67b6\u5141\u8bb8\u667a\u80fd\u4f53\u79bb\u7ebf\u5b66\u4e60\uff0c\u5e76\u5728\u8fd0\u8425\u671f\u95f4\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u5c06\u5f53\u524d\u5de5\u5382\u72b6\u6001\u6620\u5c04\u5230\u6700\u4f18\u52a8\u4f5c\u6765\u63d0\u4f9b\u5b9e\u65f6\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u5de5\u4e1a\u88c5\u914d\u7ebf\u4f18\u5316\u6846\u67b6\u5177\u6709\u53ef\u6269\u5c55\u6027\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u5927\u89c4\u6a21\u88c5\u914d\u7ebf\u8c03\u5ea6\u95ee\u9898\u3002\u901a\u8fc7\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u5efa\u6a21\u3001\u52a8\u4f5c\u63a9\u7801\u6280\u672f\u548c\u591a\u667a\u80fd\u4f53\u65b9\u6cd5\u7684\u7ed3\u5408\uff0c\u5b9e\u73b0\u4e86\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e3a\u5de5\u4e1a\u88c5\u914d\u7ebf\u7684\u667a\u80fd\u5316\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002"}}
{"id": "2507.16670", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16670", "abs": "https://arxiv.org/abs/2507.16670", "authors": ["Amandeep Kaur", "Gyan Prakash"], "title": "Adaptive Inventory Strategies using Deep Reinforcement Learning for Dynamic Agri-Food Supply Chains", "comment": null, "summary": "Agricultural products are often subject to seasonal fluctuations in\nproduction and demand. Predicting and managing inventory levels in response to\nthese variations can be challenging, leading to either excess inventory or\nstockouts. Additionally, the coordination among stakeholders at various level\nof food supply chain is not considered in the existing body of literature. To\nbridge these research gaps, this study focuses on inventory management of\nagri-food products under demand and lead time uncertainties. By implementing\neffective inventory replenishment policy results in maximize the overall profit\nthroughout the supply chain. However, the complexity of the problem increases\ndue to these uncertainties and shelf-life of the product, that makes\nchallenging to implement traditional approaches to generate optimal set of\nsolutions. Thus, the current study propose a novel Deep Reinforcement Learning\n(DRL) algorithm that combines the benefits of both value- and policy-based DRL\napproaches for inventory optimization under uncertainties. The proposed\nalgorithm can incentivize collaboration among stakeholders by aligning their\ninterests and objectives through shared optimization goal of maximizing\nprofitability along the agri-food supply chain while considering perishability,\nand uncertainty simultaneously. By selecting optimal order quantities with\ncontinuous action space, the proposed algorithm effectively addresses the\ninventory optimization challenges. To rigorously evaluate this algorithm, the\nempirical data from fresh agricultural products supply chain inventory is\nconsidered. Experimental results corroborate the improved performance of the\nproposed inventory replenishment policy under stochastic demand patterns and\nlead time scenarios. The research findings hold managerial implications for\npolicymakers to manage the inventory of agricultural products more effectively\nunder uncertainty.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u7ed3\u5408\u4ef7\u503c\u57fa\u548c\u7b56\u7565\u57fa\u65b9\u6cd5\u6765\u4f18\u5316\u519c\u4ea7\u54c1\u4f9b\u5e94\u94fe\u4e2d\u5b58\u5728\u9700\u6c42\u548c\u4ea4\u8d27\u65f6\u95f4\u4e0d\u786e\u5b9a\u6027\u7684\u5e93\u5b58\u7ba1\u7406\u95ee\u9898\uff0c\u4ee5\u6700\u5927\u5316\u6574\u4f53\u5229\u6da6\u5e76\u4fc3\u8fdb\u5229\u76ca\u76f8\u5173\u8005\u534f\u4f5c\u3002", "motivation": "\u519c\u4ea7\u54c1\u5b58\u5728\u5b63\u8282\u6027\u751f\u4ea7\u548c\u9700\u6c42\u6ce2\u52a8\uff0c\u4f20\u7edf\u5e93\u5b58\u7ba1\u7406\u65b9\u6cd5\u96be\u4ee5\u5e94\u5bf9\u9700\u6c42\u548c\u4ea4\u8d27\u65f6\u95f4\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u540c\u65f6\u73b0\u6709\u6587\u732e\u7f3a\u4e4f\u5bf9\u98df\u54c1\u4f9b\u5e94\u94fe\u5404\u5c42\u7ea7\u5229\u76ca\u76f8\u5173\u8005\u534f\u8c03\u7684\u8003\u8651\uff0c\u5bfc\u81f4\u5e93\u5b58\u8fc7\u5269\u6216\u7f3a\u8d27\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4ef7\u503c\u57fa\u548c\u7b56\u7565\u57fa\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u7684\u65b0\u7b97\u6cd5\uff0c\u901a\u8fc7\u8fde\u7eed\u52a8\u4f5c\u7a7a\u95f4\u9009\u62e9\u6700\u4f18\u8ba2\u8d27\u91cf\uff0c\u540c\u65f6\u8003\u8651\u4ea7\u54c1\u6613\u8150\u6027\u548c\u4e0d\u786e\u5b9a\u6027\uff0c\u901a\u8fc7\u5171\u4eab\u7684\u5229\u6da6\u6700\u5927\u5316\u76ee\u6807\u6765\u6fc0\u52b1\u4f9b\u5e94\u94fe\u4e2d\u5229\u76ca\u76f8\u5173\u8005\u7684\u534f\u4f5c\u3002", "result": "\u57fa\u4e8e\u65b0\u9c9c\u519c\u4ea7\u54c1\u4f9b\u5e94\u94fe\u5e93\u5b58\u7684\u5b9e\u8bc1\u6570\u636e\u9a8c\u8bc1\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u6240\u63d0\u51fa\u7684\u5e93\u5b58\u8865\u8d27\u7b56\u7565\u5728\u968f\u673a\u9700\u6c42\u6a21\u5f0f\u548c\u4ea4\u8d27\u65f6\u95f4\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u6539\u8fdb\u7684\u6027\u80fd\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5e93\u5b58\u4f18\u5316\u6311\u6218\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u653f\u7b56\u5236\u5b9a\u8005\u5728\u4e0d\u786e\u5b9a\u6027\u6761\u4ef6\u4e0b\u66f4\u6709\u6548\u5730\u7ba1\u7406\u519c\u4ea7\u54c1\u5e93\u5b58\u63d0\u4f9b\u4e86\u7ba1\u7406\u542f\u793a\uff0c\u8bc1\u660e\u4e86\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u519c\u4ea7\u54c1\u4f9b\u5e94\u94fe\u5e93\u5b58\u7ba1\u7406\u4e2d\u7684\u6709\u6548\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2507.16727", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16727", "abs": "https://arxiv.org/abs/2507.16727", "authors": ["Zhenyun Yin", "Shujie Wang", "Xuhong Wang", "Xingjun Ma", "Yinchun Wang"], "title": "Deliberative Searcher: Improving LLM Reliability via Reinforcement Learning with constraints", "comment": null, "summary": "Improving the reliability of large language models (LLMs) is critical for\ndeploying them in real-world scenarios. In this paper, we propose\n\\textbf{Deliberative Searcher}, the first framework to integrate certainty\ncalibration with retrieval-based search for open-domain question answering. The\nagent performs multi-step reflection and verification over Wikipedia data and\nis trained with a reinforcement learning algorithm that optimizes for accuracy\nunder a soft reliability constraint. Empirical results show that proposed\nmethod improves alignment between model confidence and correctness, leading to\nmore trustworthy outputs. This paper will be continuously updated.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Deliberative Searcher\u6846\u67b6\uff0c\u8fd9\u662f\u9996\u4e2a\u5c06\u786e\u5b9a\u6027\u6821\u51c6\u4e0e\u57fa\u4e8e\u68c0\u7d22\u7684\u641c\u7d22\u76f8\u7ed3\u5408\u7684\u5f00\u653e\u57df\u95ee\u7b54\u7cfb\u7edf\uff0c\u901a\u8fc7\u591a\u6b65\u53cd\u601d\u548c\u9a8c\u8bc1\u63d0\u9ad8\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u9760\u6027\u3002", "motivation": "\u63d0\u9ad8\u5927\u8bed\u8a00\u6a21\u578b(LLMs)\u5728\u73b0\u5b9e\u573a\u666f\u90e8\u7f72\u4e2d\u7684\u53ef\u9760\u6027\u662f\u5173\u952e\u95ee\u9898\uff0c\u9700\u8981\u89e3\u51b3\u6a21\u578b\u7f6e\u4fe1\u5ea6\u4e0e\u6b63\u786e\u6027\u4e4b\u95f4\u7684\u5bf9\u9f50\u95ee\u9898\uff0c\u4ee5\u4ea7\u751f\u66f4\u53ef\u4fe1\u7684\u8f93\u51fa\u3002", "method": "\u63d0\u51faDeliberative Searcher\u6846\u67b6\uff0c\u5c06\u786e\u5b9a\u6027\u6821\u51c6\u4e0e\u57fa\u4e8e\u68c0\u7d22\u7684\u641c\u7d22\u76f8\u7ed3\u5408\u7528\u4e8e\u5f00\u653e\u57df\u95ee\u7b54\uff1b\u667a\u80fd\u4f53\u5728\u7ef4\u57fa\u767e\u79d1\u6570\u636e\u4e0a\u6267\u884c\u591a\u6b65\u53cd\u601d\u548c\u9a8c\u8bc1\uff1b\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u8bad\u7ec3\uff0c\u5728\u8f6f\u53ef\u9760\u6027\u7ea6\u675f\u4e0b\u4f18\u5316\u51c6\u786e\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u6539\u5584\u4e86\u6a21\u578b\u7f6e\u4fe1\u5ea6\u4e0e\u6b63\u786e\u6027\u4e4b\u95f4\u7684\u5bf9\u9f50\uff0c\u4ea7\u751f\u4e86\u66f4\u53ef\u4fe1\u7684\u8f93\u51fa\u3002", "conclusion": "Deliberative Searcher\u6846\u67b6\u6210\u529f\u63d0\u9ad8\u4e86\u5927\u8bed\u8a00\u6a21\u578b\u7684\u53ef\u9760\u6027\uff0c\u901a\u8fc7\u6574\u5408\u786e\u5b9a\u6027\u6821\u51c6\u548c\u68c0\u7d22\u641c\u7d22\uff0c\u5b9e\u73b0\u4e86\u7f6e\u4fe1\u5ea6\u4e0e\u51c6\u786e\u6027\u7684\u66f4\u597d\u5bf9\u9f50\uff0c\u4e3aLLMs\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u4fe1\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.16768", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16768", "abs": "https://arxiv.org/abs/2507.16768", "authors": ["Ran Wang", "Xiaoxuan Liu", "Hao Ren", "Gang Chen", "Fanchao Qi", "Maosong Sun"], "title": "WGRAMMAR: Leverage Prior Knowledge to Accelerate Structured Decoding", "comment": null, "summary": "Structured decoding enables large language models (LLMs) to generate outputs\nin formats required by downstream systems, such as HTML or JSON. However,\nexisting methods suffer from efficiency bottlenecks due to grammar compilation,\nstate tracking, and mask creation. We observe that many real-world tasks embed\nstrong prior knowledge about output structure. Leveraging this, we propose a\ndecomposition of constraints into static and dynamic components -- precompiling\nstatic structures offline and instantiating dynamic arguments at runtime using\ngrammar snippets. Instead of relying on pushdown automata, we employ a\ncompositional set of operators to model regular formats, achieving lower\ntransition latency. We introduce wgrammar, a lightweight decoding engine that\nintegrates domain-aware simplification, constraint decomposition, and mask\ncaching, achieving up to 250x speedup over existing systems. wgrammar's source\ncode is publicly available at https://github.com/wrran/wgrammar.", "AI": {"tldr": "\u63d0\u51fa\u4e86wgrammar\uff0c\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u89e3\u7801\u5f15\u64ce\uff0c\u901a\u8fc7\u7ea6\u675f\u5206\u89e3\u548c\u63a9\u7801\u7f13\u5b58\u7b49\u6280\u672f\uff0c\u5c06\u7ed3\u6784\u5316\u89e3\u7801\u901f\u5ea6\u63d0\u5347\u4e86250\u500d", "motivation": "\u73b0\u6709\u7684\u7ed3\u6784\u5316\u89e3\u7801\u65b9\u6cd5\u5b58\u5728\u6548\u7387\u74f6\u9888\uff0c\u5305\u62ec\u8bed\u6cd5\u7f16\u8bd1\u3001\u72b6\u6001\u8ddf\u8e2a\u548c\u63a9\u7801\u521b\u5efa\u7b49\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u89e3\u7801\u65b9\u6848\u6765\u751f\u6210\u7b26\u5408\u4e0b\u6e38\u7cfb\u7edf\u8981\u6c42\u7684\u683c\u5f0f\u5316\u8f93\u51fa", "method": "\u5c06\u7ea6\u675f\u5206\u89e3\u4e3a\u9759\u6001\u548c\u52a8\u6001\u7ec4\u4ef6\uff0c\u9759\u6001\u7ed3\u6784\u79bb\u7ebf\u9884\u7f16\u8bd1\uff0c\u8fd0\u884c\u65f6\u4f7f\u7528\u8bed\u6cd5\u7247\u6bb5\u5b9e\u4f8b\u5316\u52a8\u6001\u53c2\u6570\uff1b\u4f7f\u7528\u7ec4\u5408\u64cd\u4f5c\u7b26\u96c6\u5408\u800c\u975e\u4e0b\u63a8\u81ea\u52a8\u673a\u6765\u5efa\u6a21\u5e38\u89c4\u683c\u5f0f\uff1b\u96c6\u6210\u9886\u57df\u611f\u77e5\u7b80\u5316\u3001\u7ea6\u675f\u5206\u89e3\u548c\u63a9\u7801\u7f13\u5b58\u6280\u672f", "result": "wgrammar\u89e3\u7801\u5f15\u64ce\u76f8\u6bd4\u73b0\u6709\u7cfb\u7edf\u5b9e\u73b0\u4e86\u9ad8\u8fbe250\u500d\u7684\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u751f\u6210HTML\u3001JSON\u7b49\u7ed3\u6784\u5316\u8f93\u51fa\u7684\u80fd\u529b", "conclusion": "\u901a\u8fc7\u7ea6\u675f\u5206\u89e3\u548c\u4f18\u5316\u7684\u64cd\u4f5c\u7b26\u8bbe\u8ba1\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u7ed3\u6784\u5316\u89e3\u7801\u7684\u6548\u7387\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u683c\u5f0f\u5316\u8f93\u51fa\u751f\u6210\u63d0\u4f9b\u4e86\u66f4\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2507.16792", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16792", "abs": "https://arxiv.org/abs/2507.16792", "authors": ["Roman Mayr", "Michel Schimpf", "Thomas Bohn\u00e9"], "title": "ChatChecker: A Framework for Dialogue System Testing and Evaluation Through Non-cooperative User Simulation", "comment": null, "summary": "While modern dialogue systems heavily rely on large language models (LLMs),\ntheir implementation often goes beyond pure LLM interaction. Developers\nintegrate multiple LLMs, external tools, and databases. Therefore, assessment\nof the underlying LLM alone does not suffice, and the dialogue systems must be\ntested and evaluated as a whole. However, this remains a major challenge. With\nmost previous work focusing on turn-level analysis, less attention has been\npaid to integrated dialogue-level quality assurance. To address this, we\npresent ChatChecker, a framework for automated evaluation and testing of\ncomplex dialogue systems. ChatChecker uses LLMs to simulate diverse user\ninteractions, identify dialogue breakdowns, and evaluate quality. Compared to\nprevious approaches, our design reduces setup effort and is generalizable, as\nit does not require reference dialogues and is decoupled from the\nimplementation of the target dialogue system. We improve breakdown detection\nperformance over a prior LLM-based approach by including an error taxonomy in\nthe prompt. Additionally, we propose a novel non-cooperative user simulator\nbased on challenging personas that uncovers weaknesses in target dialogue\nsystems more effectively. Through this, ChatChecker contributes to thorough and\nscalable testing. This enables both researchers and practitioners to accelerate\nthe development of robust dialogue systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faChatChecker\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u8bc4\u4f30\u548c\u6d4b\u8bd5\u590d\u6742\u5bf9\u8bdd\u7cfb\u7edf\uff0c\u901a\u8fc7LLM\u6a21\u62df\u7528\u6237\u4ea4\u4e92\u6765\u8bc6\u522b\u5bf9\u8bdd\u6545\u969c\u5e76\u8bc4\u4f30\u8d28\u91cf\uff0c\u76f8\u6bd4\u4ee5\u5f80\u65b9\u6cd5\u51cf\u5c11\u4e86\u8bbe\u7f6e\u5de5\u4f5c\u91cf\u4e14\u5177\u6709\u66f4\u597d\u7684\u901a\u7528\u6027\u3002", "motivation": "\u73b0\u4ee3\u5bf9\u8bdd\u7cfb\u7edf\u901a\u5e38\u96c6\u6210\u591a\u4e2aLLM\u3001\u5916\u90e8\u5de5\u5177\u548c\u6570\u636e\u5e93\uff0c\u4ec5\u8bc4\u4f30\u5e95\u5c42LLM\u4e0d\u8db3\u4ee5\u5168\u9762\u6d4b\u8bd5\u7cfb\u7edf\u8d28\u91cf\u3002\u73b0\u6709\u5de5\u4f5c\u4e3b\u8981\u5173\u6ce8\u56de\u5408\u7ea7\u5206\u6790\uff0c\u7f3a\u4e4f\u5bf9\u6574\u4f53\u5bf9\u8bdd\u7ea7\u8d28\u91cf\u4fdd\u8bc1\u7684\u5173\u6ce8\uff0c\u8fd9\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002", "method": "\u63d0\u51faChatChecker\u6846\u67b6\uff0c\u4f7f\u7528LLM\u6a21\u62df\u591a\u6837\u5316\u7528\u6237\u4ea4\u4e92\uff0c\u8bc6\u522b\u5bf9\u8bdd\u6545\u969c\u5e76\u8bc4\u4f30\u8d28\u91cf\u3002\u8bbe\u8ba1\u4e0d\u9700\u8981\u53c2\u8003\u5bf9\u8bdd\u4e14\u4e0e\u76ee\u6807\u5bf9\u8bdd\u7cfb\u7edf\u5b9e\u73b0\u89e3\u8026\u3002\u5728\u63d0\u793a\u4e2d\u5305\u542b\u9519\u8bef\u5206\u7c7b\u6cd5\u6765\u6539\u8fdb\u6545\u969c\u68c0\u6d4b\u6027\u80fd\uff0c\u5e76\u57fa\u4e8e\u6311\u6218\u6027\u4eba\u683c\u63d0\u51fa\u65b0\u9896\u7684\u975e\u5408\u4f5c\u7528\u6237\u6a21\u62df\u5668\u3002", "result": "\u76f8\u6bd4\u4e4b\u524d\u57fa\u4e8eLLM\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u63d0\u793a\u4e2d\u5305\u542b\u9519\u8bef\u5206\u7c7b\u6cd5\u63d0\u9ad8\u4e86\u6545\u969c\u68c0\u6d4b\u6027\u80fd\u3002\u975e\u5408\u4f5c\u7528\u6237\u6a21\u62df\u5668\u80fd\u66f4\u6709\u6548\u5730\u53d1\u73b0\u76ee\u6807\u5bf9\u8bdd\u7cfb\u7edf\u7684\u5f31\u70b9\uff0c\u5b9e\u73b0\u4e86\u5168\u9762\u4e14\u53ef\u6269\u5c55\u7684\u6d4b\u8bd5\u3002", "conclusion": "ChatChecker\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u52a0\u901f\u5f00\u53d1\u7a33\u5065\u5bf9\u8bdd\u7cfb\u7edf\u7684\u5de5\u5177\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u8bc4\u4f30\u6846\u67b6\u5b9e\u73b0\u4e86\u5bf9\u590d\u6742\u5bf9\u8bdd\u7cfb\u7edf\u7684\u5168\u9762\u6d4b\u8bd5\uff0c\u5177\u6709\u51cf\u5c11\u8bbe\u7f6e\u5de5\u4f5c\u91cf\u3001\u901a\u7528\u6027\u5f3a\u7b49\u4f18\u52bf\u3002"}}
{"id": "2507.16796", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.16796", "abs": "https://arxiv.org/abs/2507.16796", "authors": ["Mian Ibad Ali Shah", "Enda Barrett", "Karl Mason"], "title": "Uncertainty-Aware Knowledge Transformers for Peer-to-Peer Energy Trading with Multi-Agent Reinforcement Learning", "comment": "7 pages, 4 figures, 1 table, Proceedings of the Main Track of the\n  European Conference on Artificial Intelligence (ECAI 2025), October 25-30,\n  2025", "summary": "This paper presents a novel framework for Peer-to-Peer (P2P) energy trading\nthat integrates uncertainty-aware prediction with multi-agent reinforcement\nlearning (MARL), addressing a critical gap in current literature. In contrast\nto previous works relying on deterministic forecasts, the proposed approach\nemploys a heteroscedastic probabilistic transformer-based prediction model\ncalled Knowledge Transformer with Uncertainty (KTU) to explicitly quantify\nprediction uncertainty, which is essential for robust decision-making in the\nstochastic environment of P2P energy trading. The KTU model leverages\ndomain-specific features and is trained with a custom loss function that\nensures reliable probabilistic forecasts and confidence intervals for each\nprediction. Integrating these uncertainty-aware forecasts into the MARL\nframework enables agents to optimize trading strategies with a clear\nunderstanding of risk and variability. Experimental results show that the\nuncertainty-aware Deep Q-Network (DQN) reduces energy purchase costs by up to\n5.7% without P2P trading and 3.2% with P2P trading, while increasing\nelectricity sales revenue by 6.4% and 44.7%, respectively. Additionally, peak\nhour grid demand is reduced by 38.8% without P2P and 45.6% with P2P. These\nimprovements are even more pronounced when P2P trading is enabled, highlighting\nthe synergy between advanced forecasting and market mechanisms for resilient,\neconomically efficient energy communities.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u9884\u6d4b\u548c\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684P2P\u80fd\u6e90\u4ea4\u6613\u6846\u67b6\uff0c\u901a\u8fc7KTU\u6a21\u578b\u91cf\u5316\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u9c81\u68d2\u7684\u80fd\u6e90\u4ea4\u6613\u51b3\u7b56\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u80fd\u6e90\u8d2d\u4e70\u6210\u672c\u5e76\u589e\u52a0\u4e86\u9500\u552e\u6536\u5165\u3002", "motivation": "\u5f53\u524dP2P\u80fd\u6e90\u4ea4\u6613\u7814\u7a76\u4e3b\u8981\u4f9d\u8d56\u786e\u5b9a\u6027\u9884\u6d4b\uff0c\u7f3a\u4e4f\u5bf9\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u7684\u8003\u8651\uff0c\u8fd9\u5728\u968f\u673a\u6027\u5f88\u5f3a\u7684\u80fd\u6e90\u4ea4\u6613\u73af\u5883\u4e2d\u4f1a\u5bfc\u81f4\u51b3\u7b56\u4e0d\u591f\u9c81\u68d2\u3002\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u80fd\u591f\u663e\u5f0f\u91cf\u5316\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u5e76\u5c06\u5176\u6574\u5408\u5230\u4ea4\u6613\u7b56\u7565\u4e2d\u7684\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u96c6\u6210\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u9884\u6d4b\u548c\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684P2P\u80fd\u6e90\u4ea4\u6613\u6846\u67b6\u3002\u6838\u5fc3\u7ec4\u4ef6\u5305\u62ec\uff1a1\uff09Knowledge Transformer with Uncertainty (KTU)\u6a21\u578b\uff0c\u91c7\u7528\u5f02\u65b9\u5dee\u6982\u7387\u53d8\u6362\u5668\u8fdb\u884c\u9884\u6d4b\u5e76\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\uff1b2\uff09\u4f7f\u7528\u5b9a\u5236\u635f\u5931\u51fd\u6570\u8bad\u7ec3KTU\u4ee5\u786e\u4fdd\u53ef\u9760\u7684\u6982\u7387\u9884\u6d4b\u548c\u7f6e\u4fe1\u533a\u95f4\uff1b3\uff09\u5c06\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u9884\u6d4b\u6574\u5408\u5230MARL\u6846\u67b6\u4e2d\uff0c\u4f7f\u667a\u80fd\u4f53\u80fd\u591f\u5728\u660e\u786e\u7406\u89e3\u98ce\u9669\u548c\u53d8\u5f02\u6027\u7684\u60c5\u51b5\u4e0b\u4f18\u5316\u4ea4\u6613\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u6df1\u5ea6Q\u7f51\u7edc(DQN)\u5728\u65e0P2P\u4ea4\u6613\u65f6\u964d\u4f4e\u80fd\u6e90\u8d2d\u4e70\u6210\u672c\u9ad8\u8fbe5.7%\uff0c\u5728\u6709P2P\u4ea4\u6613\u65f6\u964d\u4f4e3.2%\uff1b\u540c\u65f6\u5206\u522b\u589e\u52a0\u7535\u529b\u9500\u552e\u6536\u51656.4%\u548c44.7%\u3002\u6b64\u5916\uff0c\u9ad8\u5cf0\u65f6\u6bb5\u7535\u7f51\u9700\u6c42\u5728\u65e0P2P\u548c\u6709P2P\u60c5\u51b5\u4e0b\u5206\u522b\u51cf\u5c11\u4e8638.8%\u548c45.6%\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5f53\u542f\u7528P2P\u4ea4\u6613\u65f6\uff0c\u6539\u8fdb\u6548\u679c\u66f4\u52a0\u663e\u8457\uff0c\u7a81\u51fa\u4e86\u5148\u8fdb\u9884\u6d4b\u6280\u672f\u4e0e\u5e02\u573a\u673a\u5236\u4e4b\u95f4\u7684\u534f\u540c\u4f5c\u7528\uff0c\u4e3a\u6784\u5efa\u5177\u6709\u97e7\u6027\u548c\u7ecf\u6d4e\u6548\u7387\u7684\u80fd\u6e90\u793e\u533a\u63d0\u4f9b\u4e86\u6709\u6548\u65b9\u6848\u3002\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u9884\u6d4b\u4e0e\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684\u7ed3\u5408\u4e3aP2P\u80fd\u6e90\u4ea4\u6613\u63d0\u4f9b\u4e86\u66f4\u9c81\u68d2\u548c\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
