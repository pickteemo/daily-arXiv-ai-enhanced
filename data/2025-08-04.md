<div id=toc></div>

# Table of Contents

- [cs.RO](#cs.RO) [Total: 17]
- [cs.AI](#cs.AI) [Total: 25]


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [1] [XRoboToolkit: A Cross-Platform Framework for Robot Teleoperation](https://arxiv.org/abs/2508.00097)
*Zhigen Zhao,Liuchuan Yu,Ke Jing,Ning Yang*

Main category: cs.RO

TL;DR: XRoboToolkit是一个基于OpenXR标准的跨平台扩展现实机器人遥操作框架，解决了现有数据收集方法的可扩展性和质量问题。


<details>
  <summary>Details</summary>
Motivation: 随着视觉-语言-动作模型的快速发展，对大规模高质量机器人演示数据的需求迫切，而现有遥操作方法存在可扩展性差、设置复杂和数据质量低的问题。

Method: XRoboToolkit提供低延迟立体视觉反馈、基于优化的逆运动学，并支持多种跟踪模式（如头部、控制器、手部等）。其模块化架构支持跨平台和仿真环境集成。

Result: 通过精确操控任务验证了框架的有效性，并训练出具有鲁棒自主性能的VLA模型。

Conclusion: XRoboToolkit为高质量机器人数据收集提供了一种高效、可扩展的解决方案。

Abstract: The rapid advancement of Vision-Language-Action models has created an urgent
need for large-scale, high-quality robot demonstration datasets. Although
teleoperation is the predominant method for data collection, current approaches
suffer from limited scalability, complex setup procedures, and suboptimal data
quality. This paper presents XRoboToolkit, a cross-platform framework for
extended reality based robot teleoperation built on the OpenXR standard. The
system features low-latency stereoscopic visual feedback, optimization-based
inverse kinematics, and support for diverse tracking modalities including head,
controller, hand, and auxiliary motion trackers. XRoboToolkit's modular
architecture enables seamless integration across robotic platforms and
simulation environments, spanning precision manipulators, mobile robots, and
dexterous hands. We demonstrate the framework's effectiveness through precision
manipulation tasks and validate data quality by training VLA models that
exhibit robust autonomous performance.

</details>


### [2] [CHILD (Controller for Humanoid Imitation and Live Demonstration): a Whole-Body Humanoid Teleoperation System](https://arxiv.org/abs/2508.00162)
*Noboru Myers,Obin Kwon,Sankalp Yamsani,Joohyung Kim*

Main category: cs.RO

TL;DR: CHILD是一种紧凑可重构的遥操作系统，支持人形机器人的关节级控制，适用于全身控制和移动操作。


<details>
  <summary>Details</summary>
Motivation: 现有研究很少支持人形机器人的全身关节级遥操作，限制了任务的多样性。

Method: 开发了CHILD系统，支持直接关节映射和移动操作，并加入自适应力反馈以提升操作体验和安全性。

Result: 在一个人形机器人和多个双臂系统上验证了移动操作和全身控制的能力。

Conclusion: CHILD系统通过开源设计提升了可访问性和可重复性。

Abstract: Recent advances in teleoperation have demonstrated robots performing complex
manipulation tasks. However, existing works rarely support whole-body
joint-level teleoperation for humanoid robots, limiting the diversity of tasks
that can be accomplished. This work presents Controller for Humanoid Imitation
and Live Demonstration (CHILD), a compact reconfigurable teleoperation system
that enables joint level control over humanoid robots. CHILD fits within a
standard baby carrier, allowing the operator control over all four limbs, and
supports both direct joint mapping for full-body control and loco-manipulation.
Adaptive force feedback is incorporated to enhance operator experience and
prevent unsafe joint movements. We validate the capabilities of this system by
conducting loco-manipulation and full-body control examples on a humanoid robot
and multiple dual-arm systems. Lastly, we open-source the design of the
hardware promoting accessibility and reproducibility. Additional details and
open-source information are available at our project website:
https://uiuckimlab.github.io/CHILD-pages.

</details>


### [3] [Topology-Inspired Morphological Descriptor for Soft Continuum Robots](https://arxiv.org/abs/2508.00258)
*Zhiwei Wu,Siyi Wei,Jiahao Luo,Jinhui Zhang*

Main category: cs.RO

TL;DR: 提出了一种基于拓扑的形态描述符，结合伪刚体模型和莫尔斯理论，用于软连续机器人的形态定量表征和分类。


<details>
  <summary>Details</summary>
Motivation: 提高软连续机器人在医疗应用（如微创手术和血管内介入）中的精度和适应性。

Method: 结合伪刚体模型和莫尔斯理论，通过计数方向投影的临界点实现形态离散表示，并通过优化问题计算驱动参数以实现目标形态。

Result: 实现了软连续机器人形态的定量描述、分类和控制。

Conclusion: 该框架为软连续机器人提供了一种统一的形态描述和控制方法，具有潜在的实际应用价值。

Abstract: This paper presents a topology-inspired morphological descriptor for soft
continuum robots by combining a pseudo-rigid-body (PRB) model with Morse theory
to achieve a quantitative characterization of robot morphologies. By counting
critical points of directional projections, the proposed descriptor enables a
discrete representation of multimodal configurations and facilitates
morphological classification. Furthermore, we apply the descriptor to
morphology control by formulating the target configuration as an optimization
problem to compute actuation parameters that generate equilibrium shapes with
desired topological features. The proposed framework provides a unified
methodology for quantitative morphology description, classification, and
control of soft continuum robots, with the potential to enhance their precision
and adaptability in medical applications such as minimally invasive surgery and
endovascular interventions.

</details>


### [4] [UAV-ON: A Benchmark for Open-World Object Goal Navigation with Aerial Agents](https://arxiv.org/abs/2508.00288)
*Jianqiang Xiao,Yuexuan Sun,Yixin Shao,Boxi Gan,Rongqiang Liu,Yanjing Wu,Weili Gua,Xiang Deng*

Main category: cs.RO

TL;DR: UAV-ON是一个用于空中智能体在开放环境中进行大规模目标导航的基准测试，旨在解决传统视觉与语言导航（VLN）依赖详细指令的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有研究多依赖顺序语言指令（如VLN），限制了空中导航的可扩展性和自主性，因此需要一种基于高级语义目标的导航方法。

Method: UAV-ON包含14个高保真Unreal Engine环境，定义了1270个标注目标对象，并通过实例级指令编码语义目标。提出了Aerial ObjectNav Agent（AOA）作为基线方法。

Result: 实验显示所有基线方法在此设置下表现不佳，突显了空中导航与语义目标结合的复杂性。

Conclusion: UAV-ON旨在推动基于语义目标的无人机自主性研究，适用于复杂现实环境。

Abstract: Aerial navigation is a fundamental yet underexplored capability in embodied
intelligence, enabling agents to operate in large-scale, unstructured
environments where traditional navigation paradigms fall short. However, most
existing research follows the Vision-and-Language Navigation (VLN) paradigm,
which heavily depends on sequential linguistic instructions, limiting its
scalability and autonomy. To address this gap, we introduce UAV-ON, a benchmark
for large-scale Object Goal Navigation (ObjectNav) by aerial agents in
open-world environments, where agents operate based on high-level semantic
goals without relying on detailed instructional guidance as in VLN. UAV-ON
comprises 14 high-fidelity Unreal Engine environments with diverse semantic
regions and complex spatial layouts, covering urban, natural, and mixed-use
settings. It defines 1270 annotated target objects, each characterized by an
instance-level instruction that encodes category, physical footprint, and
visual descriptors, allowing grounded reasoning. These instructions serve as
semantic goals, introducing realistic ambiguity and complex reasoning
challenges for aerial agents. To evaluate the benchmark, we implement several
baseline methods, including Aerial ObjectNav Agent (AOA), a modular policy that
integrates instruction semantics with egocentric observations for long-horizon,
goal-directed exploration. Empirical results show that all baselines struggle
in this setting, highlighting the compounded challenges of aerial navigation
and semantic goal grounding. UAV-ON aims to advance research on scalable UAV
autonomy driven by semantic goal descriptions in complex real-world
environments.

</details>


### [5] [TopoDiffuser: A Diffusion-Based Multimodal Trajectory Prediction Model with Topometric Maps](https://arxiv.org/abs/2508.00303)
*Zehui Xu,Junhui Wang,Yongliang Shi,Chao Gao,Guyue Zhou*

Main category: cs.RO

TL;DR: TopoDiffuser是一种基于扩散模型的多模态轨迹预测框架，通过结合拓扑地图生成准确、多样且符合道路规则的未来运动预测。


<details>
  <summary>Details</summary>
Motivation: 传统轨迹预测方法通常依赖显式约束来保证道路几何一致性，而TopoDiffuser通过嵌入拓扑地图的结构信息，无需显式约束即可实现自然符合道路几何的轨迹生成。

Method: 采用条件扩散模型，通过多模态条件编码器融合LiDAR观测、历史运动和路径信息为统一的鸟瞰图表示，嵌入拓扑地图的结构信息到去噪过程中。

Result: 在KITTI基准测试中，TopoDiffuser优于现有方法，同时保持强几何一致性。消融实验验证了各输入模态的贡献以及去噪步骤和轨迹样本数量的影响。

Conclusion: TopoDiffuser通过结合拓扑地图和扩散模型，实现了高效且符合道路规则的轨迹预测，为未来研究提供了开源代码支持。

Abstract: This paper introduces TopoDiffuser, a diffusion-based framework for
multimodal trajectory prediction that incorporates topometric maps to generate
accurate, diverse, and road-compliant future motion forecasts. By embedding
structural cues from topometric maps into the denoising process of a
conditional diffusion model, the proposed approach enables trajectory
generation that naturally adheres to road geometry without relying on explicit
constraints. A multimodal conditioning encoder fuses LiDAR observations,
historical motion, and route information into a unified bird's-eye-view (BEV)
representation. Extensive experiments on the KITTI benchmark demonstrate that
TopoDiffuser outperforms state-of-the-art methods, while maintaining strong
geometric consistency. Ablation studies further validate the contribution of
each input modality, as well as the impact of denoising steps and the number of
trajectory samples. To support future research, we publicly release our code at
https://github.com/EI-Nav/TopoDiffuser.

</details>


### [6] [Omni-Scan: Creating Visually-Accurate Digital Twin Object Models Using a Bimanual Robot with Handover and Gaussian Splat Merging](https://arxiv.org/abs/2508.00354)
*Tianshuang Qiu,Zehan Ma,Karim El-Refai,Hiya Shah,Chung Min Kim,Justin Kerr,Ken Goldberg*

Main category: cs.RO

TL;DR: Omni-Scan是一种利用双机械臂机器人抓取和旋转物体以生成高质量3D高斯溅射模型的流程，支持360度视角建模，并在零件缺陷检测中达到83%的平均准确率。


<details>
  <summary>Details</summary>
Motivation: 传统的3D物体扫描方法受限于设备的工作空间，Omni-Scan旨在通过机器人操作解决这一问题，实现更灵活的物体建模。

Method: 使用双机械臂机器人抓取和旋转物体，结合DepthAnything、Segment Anything和RAFT光流模型去除背景和夹持器遮挡，改进3DGS训练流程以支持拼接数据集。

Result: 在12种工业和家用物体的缺陷检测中，平均准确率达到83%。

Conclusion: Omni-Scan提供了一种高效且灵活的3D物体建模方法，适用于多种应用场景。

Abstract: 3D Gaussian Splats (3DGSs) are 3D object models derived from multi-view
images. Such "digital twins" are useful for simulations, virtual reality,
marketing, robot policy fine-tuning, and part inspection. 3D object scanning
usually requires multi-camera arrays, precise laser scanners, or robot
wrist-mounted cameras, which have restricted workspaces. We propose Omni-Scan,
a pipeline for producing high-quality 3D Gaussian Splat models using a
bi-manual robot that grasps an object with one gripper and rotates the object
with respect to a stationary camera. The object is then re-grasped by a second
gripper to expose surfaces that were occluded by the first gripper. We present
the Omni-Scan robot pipeline using DepthAny-thing, Segment Anything, as well as
RAFT optical flow models to identify and isolate objects held by a robot
gripper while removing the gripper and the background. We then modify the 3DGS
training pipeline to support concatenated datasets with gripper occlusion,
producing an omni-directional (360 degree view) model of the object. We apply
Omni-Scan to part defect inspection, finding that it can identify visual or
geometric defects in 12 different industrial and household objects with an
average accuracy of 83%. Interactive videos of Omni-Scan 3DGS models can be
found at https://berkeleyautomation.github.io/omni-scan/

</details>


### [7] [TOP: Time Optimization Policy for Stable and Accurate Standing Manipulation with Humanoid Robots](https://arxiv.org/abs/2508.00355)
*Zhenghan Chen,Haocheng Xu,Haodong Zhang,Liang Zhang,He Li,Dongqi Wang,Jiyu Yu,Yifei Yang,Zhongxiang Zhou,Rong Xiong*

Main category: cs.RO

TL;DR: 提出了一种新颖的时间优化策略（TOP），用于训练人形机器人的站立操控控制模型，同时确保平衡、精确和时间效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以同时满足高维上半身关节的精确控制和整体鲁棒性，尤其是在上半身动作快速时。

Method: 结合运动先验（VAE）、解耦控制（上半身PD控制器和下半身RL控制器）及TOP策略，优化上半身动作时间轨迹。

Result: 仿真和实际实验验证了该方法在稳定和精确完成站立操控任务上的优越性。

Conclusion: TOP方法有效解决了快速上半身动作导致的平衡问题，提升了人形机器人的操控能力。

Abstract: Humanoid robots have the potential capability to perform a diverse range of
manipulation tasks, but this is based on a robust and precise standing
controller. Existing methods are either ill-suited to precisely control
high-dimensional upper-body joints, or difficult to ensure both robustness and
accuracy, especially when upper-body motions are fast. This paper proposes a
novel time optimization policy (TOP), to train a standing manipulation control
model that ensures balance, precision, and time efficiency simultaneously, with
the idea of adjusting the time trajectory of upper-body motions but not only
strengthening the disturbance resistance of the lower-body. Our approach
consists of three parts. Firstly, we utilize motion prior to represent
upper-body motions to enhance the coordination ability between the upper and
lower-body by training a variational autoencoder (VAE). Then we decouple the
whole-body control into an upper-body PD controller for precision and a
lower-body RL controller to enhance robust stability. Finally, we train TOP
method in conjunction with the decoupled controller and VAE to reduce the
balance burden resulting from fast upper-body motions that would destabilize
the robot and exceed the capabilities of the lower-body RL policy. The
effectiveness of the proposed approach is evaluated via both simulation and
real world experiments, which demonstrate the superiority on standing
manipulation tasks stably and accurately. The project page can be found at
https://anonymous.4open.science/w/top-258F/.

</details>


### [8] [A Whole-Body Motion Imitation Framework from Human Data for Full-Size Humanoid Robot](https://arxiv.org/abs/2508.00362)
*Zhenghan Chen,Haodong Zhang,Dongqi Wang,Jiyu Yu,Haocheng Xu,Yue Wang,Rong Xiong*

Main category: cs.RO

TL;DR: 提出了一种新颖的全身运动模仿框架，用于全尺寸人形机器人，结合接触感知的运动重定向和非线性质心模型预测控制，实现高精度运动模仿与实时平衡。


<details>
  <summary>Details</summary>
Motivation: 人形机器人模仿人类运动时，因动力学和运动学差异大，难以同时保持平衡和运动精度。

Method: 采用接触感知的全身运动重定向生成参考轨迹，结合非线性质心模型预测控制器实现实时平衡与抗干扰。

Result: 实验验证了该方法在仿真和真实机器人中均能高精度模仿多种人类运动，并保持平衡。

Conclusion: 所提框架有效解决了人形机器人运动模仿中的平衡与精度问题。

Abstract: Motion imitation is a pivotal and effective approach for humanoid robots to
achieve a more diverse range of complex and expressive movements, making their
performances more human-like. However, the significant differences in
kinematics and dynamics between humanoid robots and humans present a major
challenge in accurately imitating motion while maintaining balance. In this
paper, we propose a novel whole-body motion imitation framework for a full-size
humanoid robot. The proposed method employs contact-aware whole-body motion
retargeting to mimic human motion and provide initial values for reference
trajectories, and the non-linear centroidal model predictive controller ensures
the motion accuracy while maintaining balance and overcoming external
disturbances in real time. The assistance of the whole-body controller allows
for more precise torque control. Experiments have been conducted to imitate a
variety of human motions both in simulation and in a real-world humanoid robot.
These experiments demonstrate the capability of performing with accuracy and
adaptability, which validates the effectiveness of our approach.

</details>


### [9] [On Learning Closed-Loop Probabilistic Multi-Agent Simulator](https://arxiv.org/abs/2508.00384)
*Juanwu Lu,Rohit Gupta,Ahmadreza Moradipari,Kyungtae Han,Ruqi Zhang,Ziran Wang*

Main category: cs.RO

TL;DR: NIVA是一个基于分层贝叶斯模型的概率框架，用于多智能体交通模拟，支持闭环、观察驱动的模拟，并在Waymo数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶车辆的快速迭代，需要更真实、可扩展的多智能体交通模拟器以高效评估性能。

Method: NIVA采用分层贝叶斯模型，通过自回归采样从潜在的高斯混合分布中生成交互式模拟。

Result: 在Waymo Open Motion数据集上，NIVA表现优于现有方法，并能灵活控制意图和驾驶风格。

Conclusion: NIVA为多智能体模拟提供了一种统一且灵活的框架，具有实际应用潜力。

Abstract: The rapid iteration of autonomous vehicle (AV) deployments leads to
increasing needs for building realistic and scalable multi-agent traffic
simulators for efficient evaluation. Recent advances in this area focus on
closed-loop simulators that enable generating diverse and interactive
scenarios. This paper introduces Neural Interactive Agents (NIVA), a
probabilistic framework for multi-agent simulation driven by a hierarchical
Bayesian model that enables closed-loop, observation-conditioned simulation
through autoregressive sampling from a latent, finite mixture of Gaussian
distributions. We demonstrate how NIVA unifies preexisting sequence-to-sequence
trajectory prediction models and emerging closed-loop simulation models trained
on Next-token Prediction (NTP) from a Bayesian inference perspective.
Experiments on the Waymo Open Motion Dataset demonstrate that NIVA attains
competitive performance compared to the existing method while providing
embellishing control over intentions and driving styles.

</details>


### [10] [SubCDM: Collective Decision-Making with a Swarm Subset](https://arxiv.org/abs/2508.00467)
*Samratul Fuady,Danesh Tarapore,Mohammad D. Soorati*

Main category: cs.RO

TL;DR: 提出了一种基于子集的集体决策方法（SubCDM），通过动态构建子集减少资源消耗，同时保持决策准确性。


<details>
  <summary>Details</summary>
Motivation: 现有集体决策方法需要所有机器人参与，资源消耗大且无法分配机器人执行其他任务。

Method: 动态构建子集，仅依赖局部信息，自适应调整子集规模以适应决策难度。

Result: 仿真实验表明，SubCDM在减少机器人参与数量的同时，保持了与全群决策相当的准确性。

Conclusion: SubCDM是一种资源高效的集体决策方法，适用于群机器人系统。

Abstract: Collective decision-making is a key function of autonomous robot swarms,
enabling them to reach a consensus on actions based on environmental features.
Existing strategies require the participation of all robots in the
decision-making process, which is resource-intensive and prevents the swarm
from allocating the robots to any other tasks. We propose Subset-Based
Collective Decision-Making (SubCDM), which enables decisions using only a swarm
subset. The construction of the subset is dynamic and decentralized, relying
solely on local information. Our method allows the swarm to adaptively
determine the size of the subset for accurate decision-making, depending on the
difficulty of reaching a consensus. Simulation results using one hundred robots
show that our approach achieves accuracy comparable to using the entire swarm
while reducing the number of robots required to perform collective
decision-making, making it a resource-efficient solution for collective
decision-making in swarm robotics.

</details>


### [11] [HannesImitation: Grasping with the Hannes Prosthetic Hand via Imitation Learning](https://arxiv.org/abs/2508.00491)
*Carlo Alessi,Federico Vasile,Federico Ceola,Giulia Pasquale,Nicolò Boccardo,Lorenzo Natale*

Main category: cs.RO

TL;DR: 论文提出了一种基于模仿学习的假手控制方法HannesImitationPolicy，通过扩散策略预测手腕方向和手部闭合动作，提升了假手在非结构化环境中的抓取能力。


<details>
  <summary>Details</summary>
Motivation: 当前假手控制研究多依赖手动标注数据，而模仿学习在机器人领域已显示出简化数据收集的潜力，但在假手控制中尚未充分探索。填补这一空白可提升假手的灵活性和适应性。

Method: 提出HannesImitationPolicy方法，利用HannesImitationDataset中的抓取演示数据训练扩散策略，预测手腕方向和手部闭合动作。

Result: 实验表明，该方法在多样化的物体和条件下成功实现抓取，并在非结构化场景中优于基于分割的视觉伺服控制器。

Conclusion: 模仿学习为假手控制提供了新思路，尤其在非结构化环境中表现出色，未来可进一步优化和扩展应用场景。

Abstract: Recent advancements in control of prosthetic hands have focused on increasing
autonomy through the use of cameras and other sensory inputs. These systems aim
to reduce the cognitive load on the user by automatically controlling certain
degrees of freedom. In robotics, imitation learning has emerged as a promising
approach for learning grasping and complex manipulation tasks while simplifying
data collection. Its application to the control of prosthetic hands remains,
however, largely unexplored. Bridging this gap could enhance dexterity
restoration and enable prosthetic devices to operate in more unconstrained
scenarios, where tasks are learned from demonstrations rather than relying on
manually annotated sequences. To this end, we present HannesImitationPolicy, an
imitation learning-based method to control the Hannes prosthetic hand, enabling
object grasping in unstructured environments. Moreover, we introduce the
HannesImitationDataset comprising grasping demonstrations in table, shelf, and
human-to-prosthesis handover scenarios. We leverage such data to train a single
diffusion policy and deploy it on the prosthetic hand to predict the wrist
orientation and hand closure for grasping. Experimental evaluation demonstrates
successful grasps across diverse objects and conditions. Finally, we show that
the policy outperforms a segmentation-based visual servo controller in
unstructured scenarios. Additional material is provided on our project page:
https://hsp-iit.github.io/HannesImitation

</details>


### [12] [OmniUnet: A Multimodal Network for Unstructured Terrain Segmentation on Planetary Rovers Using RGB, Depth, and Thermal Imagery](https://arxiv.org/abs/2508.00580)
*Raul Castilla-Arquillo,Carlos Perez-del-Pulgar,Levin Gerdes,Alfonso Garcia-Cerezo,Miguel A. Olivares-Mendez*

Main category: cs.RO

TL;DR: OmniUnet是一种基于Transformer的神经网络架构，用于RGB-D-T图像语义分割，在火星探测中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决非结构化环境中机器人导航的多模态感知需求，特别是火星探测中热成像对地形安全评估的重要性。

Method: 开发了OmniUnet架构和定制多模态传感器，收集并标注了半沙漠环境数据集，进行监督训练和评估。

Result: 模型像素准确率达80.37%，推理时间673毫秒，适合机器人部署。

Conclusion: OmniUnet在多模态地形感知中表现优异，公开了软件和数据集以支持未来研究。

Abstract: Robot navigation in unstructured environments requires multimodal perception
systems that can support safe navigation. Multimodality enables the integration
of complementary information collected by different sensors. However, this
information must be processed by machine learning algorithms specifically
designed to leverage heterogeneous data. Furthermore, it is necessary to
identify which sensor modalities are most informative for navigation in the
target environment. In Martian exploration, thermal imagery has proven valuable
for assessing terrain safety due to differences in thermal behaviour between
soil types. This work presents OmniUnet, a transformer-based neural network
architecture for semantic segmentation using RGB, depth, and thermal (RGB-D-T)
imagery. A custom multimodal sensor housing was developed using 3D printing and
mounted on the Martian Rover Testbed for Autonomy (MaRTA) to collect a
multimodal dataset in the Bardenas semi-desert in northern Spain. This location
serves as a representative environment of the Martian surface, featuring
terrain types such as sand, bedrock, and compact soil. A subset of this dataset
was manually labeled to support supervised training of the network. The model
was evaluated both quantitatively and qualitatively, achieving a pixel accuracy
of 80.37% and demonstrating strong performance in segmenting complex
unstructured terrain. Inference tests yielded an average prediction time of 673
ms on a resource-constrained computer (Jetson Orin Nano), confirming its
suitability for on-robot deployment. The software implementation of the network
and the labeled dataset have been made publicly available to support future
research in multimodal terrain perception for planetary robotics.

</details>


### [13] [A control scheme for collaborative object transportation between a human and a quadruped robot using the MIGHTY suction cup](https://arxiv.org/abs/2508.00584)
*Konstantinos Plotas,Emmanouil Papadakis,Drosakis Drosakis,Panos Trahanias,Dimitrios Papageorgiou*

Main category: cs.RO

TL;DR: 提出了一种基于导纳控制的四足机器人与人类协作搬运物体的控制方案，结合可变阻尼和屏障人工势能，提高可控性并减少人类负担。


<details>
  <summary>Details</summary>
Motivation: 解决人机协作搬运中可控性和人类负担的问题，同时确保物体与吸盘不分离。

Method: 采用导纳控制框架，引入可变阻尼和基于屏障人工势能的额外控制信号。

Result: 实验验证了控制方案的被动性和有效性。

Conclusion: 提出的控制方案在提高协作效率的同时，确保了系统的稳定性和安全性。

Abstract: In this work, a control scheme for human-robot collaborative object
transportation is proposed, considering a quadruped robot equipped with the
MIGHTY suction cup that serves both as a gripper for holding the object and a
force/torque sensor. The proposed control scheme is based on the notion of
admittance control, and incorporates a variable damping term aiming towards
increasing the controllability of the human and, at the same time, decreasing
her/his effort. Furthermore, to ensure that the object is not detached from the
suction cup during the collaboration, an additional control signal is proposed,
which is based on a barrier artificial potential. The proposed control scheme
is proven to be passive and its performance is demonstrated through
experimental evaluations conducted using the Unitree Go1 robot equipped with
the MIGHTY suction cup.

</details>


### [14] [OpenScout v1.1 mobile robot: a case study on open hardware continuation](https://arxiv.org/abs/2508.00625)
*Bartosz Krawczyk,Ahmed Elbary,Robbie Cato,Jagdish Patil,Kaung Myat,Anyeh Ndi-Tah,Nivetha Sakthivel,Mark Crampton,Gautham Das,Charles Fox*

Main category: cs.RO

TL;DR: OpenScout v1.1是一款开源的移动机器人硬件，用于研究和工业领域，升级后计算硬件更简化、更便宜且更强大，并支持ROS2接口和Gazebo模拟。


<details>
  <summary>Details</summary>
Motivation: 为研究和工业提供一个低成本、高性能的开源移动机器人平台。

Method: 通过简化硬件设计、优化计算能力，并集成ROS2和Gazebo模拟功能。

Result: 实现了更经济、更强大的硬件平台，并提供了模拟支持。

Conclusion: OpenScout v1.1是一个成功的开源案例，展示了硬件优化的潜力。

Abstract: OpenScout is an Open Source Hardware (OSH) mobile robot for research and
industry. It is extended to v1.1 which includes simplified, cheaper and more
powerful onboard compute hardware; a simulated ROS2 interface; and a Gazebo
simulation. Changes, their rationale, project methodology, and results are
reported as an OSH case study.

</details>


### [15] [Towards Data-Driven Adaptive Exoskeleton Assistance for Post-stroke Gait](https://arxiv.org/abs/2508.00691)
*Fabian C. Weigend,Dabin K. Choe,Santiago Canete,Conor J. Walsh*

Main category: cs.RO

TL;DR: 研究提出了一种基于数据驱动的方法，通过多任务时序卷积网络（TCN）为中风后行走患者提供踝关节辅助力矩估计，并验证了实时控制的可行性。


<details>
  <summary>Details</summary>
Motivation: 尽管数据驱动方法在健康年轻人中已成功应用于外骨骼控制，但在中风后步态缺陷患者中的应用面临挑战，如人群异质性和数据缺乏。本研究旨在填补这一空白。

Method: 使用四名中风后参与者的跑步机行走数据训练多任务TCN模型，结合三个IMU数据，并预训练健康参与者数据。开发了可穿戴原型进行实时控制验证。

Result: 模型在中风后行走数据上的R²为0.74±0.13，并通过一名参与者验证了实时传感、估计和驱动的可行性。

Conclusion: 数据驱动方法为中风后患者的外骨骼控制提供了潜在解决方案，未来需进一步优化和扩展。

Abstract: Recent work has shown that exoskeletons controlled through data-driven
methods can dynamically adapt assistance to various tasks for healthy young
adults. However, applying these methods to populations with neuromotor gait
deficits, such as post-stroke hemiparesis, is challenging. This is due not only
to high population heterogeneity and gait variability but also to a lack of
post-stroke gait datasets to train accurate models. Despite these challenges,
data-driven methods offer a promising avenue for control, potentially allowing
exoskeletons to function safely and effectively in unstructured community
settings. This work presents a first step towards enabling adaptive
plantarflexion and dorsiflexion assistance from data-driven torque estimation
during post-stroke walking. We trained a multi-task Temporal Convolutional
Network (TCN) using collected data from four post-stroke participants walking
on a treadmill ($R^2$ of $0.74 \pm 0.13$). The model uses data from three
inertial measurement units (IMU) and was pretrained on healthy walking data
from 6 participants. We implemented a wearable prototype for our ankle torque
estimation approach for exoskeleton control and demonstrated the viability of
real-time sensing, estimation, and actuation with one post-stroke participant.

</details>


### [16] [On-Device Diffusion Transformer Policy for Efficient Robot Manipulation](https://arxiv.org/abs/2508.00697)
*Yiming Wu,Huan Wang,Zhenghao Chen,Jianxin Pang,Dong Xu*

Main category: cs.RO

TL;DR: LightDP是一种专为移动设备设计的轻量级框架，通过网络压缩和减少采样步骤加速扩散策略，实现实时动作预测。


<details>
  <summary>Details</summary>
Motivation: 扩散策略在资源受限的移动平台上应用时存在计算效率低和内存占用大的问题，LightDP旨在解决这一问题。

Method: 通过分析扩散策略架构，识别去噪网络为延迟主因，引入统一剪枝和再训练流程优化模型，并结合一致性蒸馏减少采样步骤。

Result: 在多个标准数据集上验证，LightDP在移动设备上实现实时动作预测，性能接近现有最佳扩散策略。

Conclusion: LightDP为资源受限环境中扩散策略的实际部署提供了重要进展。

Abstract: Diffusion Policies have significantly advanced robotic manipulation tasks via
imitation learning, but their application on resource-constrained mobile
platforms remains challenging due to computational inefficiency and extensive
memory footprint. In this paper, we propose LightDP, a novel framework
specifically designed to accelerate Diffusion Policies for real-time deployment
on mobile devices. LightDP addresses the computational bottleneck through two
core strategies: network compression of the denoising modules and reduction of
the required sampling steps. We first conduct an extensive computational
analysis on existing Diffusion Policy architectures, identifying the denoising
network as the primary contributor to latency. To overcome performance
degradation typically associated with conventional pruning methods, we
introduce a unified pruning and retraining pipeline, optimizing the model's
post-pruning recoverability explicitly. Furthermore, we combine pruning
techniques with consistency distillation to effectively reduce sampling steps
while maintaining action prediction accuracy. Experimental evaluations on the
standard datasets, \ie, PushT, Robomimic, CALVIN, and LIBERO, demonstrate that
LightDP achieves real-time action prediction on mobile devices with competitive
performance, marking an important step toward practical deployment of
diffusion-based policies in resource-limited environments. Extensive real-world
experiments also show the proposed LightDP can achieve performance comparable
to state-of-the-art Diffusion Policies.

</details>


### [17] [Video Generators are Robot Policies](https://arxiv.org/abs/2508.00795)
*Junbang Liang,Pavel Tokmakov,Ruoshi Liu,Sruthi Sudhakar,Paarth Shah,Rares Ambrus,Carl Vondrick*

Main category: cs.RO

TL;DR: 论文提出了一种名为Video Policy的模块化框架，通过视频生成作为机器人策略学习的代理，解决了现有视觉运动策略在泛化和数据效率上的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前视觉运动策略在感知和行为分布变化下泛化能力不足，且依赖大量人类演示数据，限制了其性能。

Method: 结合视频和动作生成的端到端训练框架，利用视频生成模型提取策略，减少对演示数据的依赖。

Result: 在仿真和真实世界中表现出对未见物体、背景和任务的强泛化能力，显著提高了样本效率和鲁棒性。

Conclusion: 通过大规模视频生成模型，实现了比传统行为克隆更优的性能，为可扩展和数据高效的机器人策略学习铺平了道路。

Abstract: Despite tremendous progress in dexterous manipulation, current visuomotor
policies remain fundamentally limited by two challenges: they struggle to
generalize under perceptual or behavioral distribution shifts, and their
performance is constrained by the size of human demonstration data. In this
paper, we use video generation as a proxy for robot policy learning to address
both limitations simultaneously. We propose Video Policy, a modular framework
that combines video and action generation that can be trained end-to-end. Our
results demonstrate that learning to generate videos of robot behavior allows
for the extraction of policies with minimal demonstration data, significantly
improving robustness and sample efficiency. Our method shows strong
generalization to unseen objects, backgrounds, and tasks, both in simulation
and the real world. We further highlight that task success is closely tied to
the generated video, with action-free video data providing critical benefits
for generalizing to novel tasks. By leveraging large-scale video generative
models, we achieve superior performance compared to traditional behavior
cloning, paving the way for more scalable and data-efficient robot policy
learning.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [18] [Rethinking Evidence Hierarchies in Medical Language Benchmarks: A Critical Evaluation of HealthBench](https://arxiv.org/abs/2508.00081)
*Fred Mutisya,Shikoh Gitau,Nasubo Ongoma,Keith Mbae,Elizabeth Wamicha*

Main category: cs.AI

TL;DR: HealthBench作为评估医疗AI系统的基准存在局限性，如依赖专家意见而非临床证据，可能引入偏见。作者提出基于临床实践指南（CPGs）的改进方案，以增强全球适用性和公平性。


<details>
  <summary>Details</summary>
Motivation: HealthBench依赖专家意见，可能固化区域偏见和个体差异，尤其在低收入地区问题更突出。需要更全球化和公平的基准。

Method: 提出基于版本控制CPGs的奖励函数，结合系统评价和GRADE证据评级，通过证据加权评分和上下文覆盖逻辑改进。

Result: 改进方案旨在提升医疗语言模型的临床可信度、伦理性和全球适用性。

Conclusion: 通过结合CPGs和改进评分机制，可以构建更透明、公平且全球适用的医疗AI评估基准。

Abstract: HealthBench, a benchmark designed to measure the capabilities of AI systems
for health better (Arora et al., 2025), has advanced medical language model
evaluation through physician-crafted dialogues and transparent rubrics.
However, its reliance on expert opinion, rather than high-tier clinical
evidence, risks codifying regional biases and individual clinician
idiosyncrasies, further compounded by potential biases in automated grading
systems. These limitations are particularly magnified in low- and middle-income
settings, where issues like sparse neglected tropical disease coverage and
region-specific guideline mismatches are prevalent.
  The unique challenges of the African context, including data scarcity,
inadequate infrastructure, and nascent regulatory frameworks, underscore the
urgent need for more globally relevant and equitable benchmarks. To address
these shortcomings, we propose anchoring reward functions in version-controlled
Clinical Practice Guidelines (CPGs) that incorporate systematic reviews and
GRADE evidence ratings.
  Our roadmap outlines "evidence-robust" reinforcement learning via
rubric-to-guideline linkage, evidence-weighted scoring, and contextual override
logic, complemented by a focus on ethical considerations and the integration of
delayed outcome feedback. By re-grounding rewards in rigorously vetted CPGs,
while preserving HealthBench's transparency and physician engagement, we aim to
foster medical language models that are not only linguistically polished but
also clinically trustworthy, ethically sound, and globally relevant.

</details>


### [19] [Hyperproperty-Constrained Secure Reinforcement Learning](https://arxiv.org/abs/2508.00106)
*Ernest Bonnah,Luan Viet Nguyen,Khaza Anuarul Hoque*

Main category: cs.AI

TL;DR: 本文提出了一种基于HyperTWTL的安全强化学习方法（SecRL），通过动态Boltzmann softmax RL学习安全感知的最优策略，并在机器人任务中验证了其有效性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有研究在利用超属性探索安全感知强化学习（RL）方面存在空白，尤其是在机器人应用中。本文旨在填补这一空白，通过HyperTWTL形式化安全约束，提出一种新的SecRL方法。

Method: 将智能体的动态建模为马尔可夫决策过程（MDP），并使用HyperTWTL形式化不透明性/安全约束。采用动态Boltzmann softmax RL学习满足约束的最优策略。

Result: 在机器人拾取和交付任务案例中验证了方法的有效性和可扩展性，并优于两种基线RL算法。

Conclusion: 提出的HyperTWTL约束SecRL方法在安全感知RL中表现出色，为机器人应用提供了一种有效的解决方案。

Abstract: Hyperproperties for Time Window Temporal Logic (HyperTWTL) is a
domain-specific formal specification language known for its effectiveness in
compactly representing security, opacity, and concurrency properties for
robotics applications. This paper focuses on HyperTWTL-constrained secure
reinforcement learning (SecRL). Although temporal logic-constrained safe
reinforcement learning (SRL) is an evolving research problem with several
existing literature, there is a significant research gap in exploring
security-aware reinforcement learning (RL) using hyperproperties. Given the
dynamics of an agent as a Markov Decision Process (MDP) and opacity/security
constraints formalized as HyperTWTL, we propose an approach for learning
security-aware optimal policies using dynamic Boltzmann softmax RL while
satisfying the HyperTWTL constraints. The effectiveness and scalability of our
proposed approach are demonstrated using a pick-up and delivery robotic mission
case study. We also compare our results with two other baseline RL algorithms,
showing that our proposed method outperforms them.

</details>


### [20] [No AI Without PI! Object-Centric Process Mining as the Enabler for Generative, Predictive, and Prescriptive Artificial Intelligence](https://arxiv.org/abs/2508.00116)
*Wil M. P. van der Aalst*

Main category: cs.AI

TL;DR: 论文探讨了AI在工业环境中的应用挑战，提出通过对象中心过程挖掘（OCPM）和过程智能（PI）结合生成、预测和规范AI，以优化端到端操作流程。


<details>
  <summary>Details</summary>
Motivation: 组织在工业环境中成功应用AI面临困难，尤其是端到端操作流程的诊断和改进。

Method: 采用对象中心过程挖掘（OCPM）和过程智能（PI）结合生成、预测和规范AI。

Result: OCPM是连接数据和过程的关键，PI为AI在组织环境中提供了支持。

Conclusion: AI需要PI来改进操作流程，OCPM与多种AI形式的结合具有广阔前景。

Abstract: The uptake of Artificial Intelligence (AI) impacts the way we work, interact,
do business, and conduct research. However, organizations struggle to apply AI
successfully in industrial settings where the focus is on end-to-end
operational processes. Here, we consider generative, predictive, and
prescriptive AI and elaborate on the challenges of diagnosing and improving
such processes. We show that AI needs to be grounded using Object-Centric
Process Mining (OCPM). Process-related data are structured and
organization-specific and, unlike text, processes are often highly dynamic.
OCPM is the missing link connecting data and processes and enables different
forms of AI. We use the term Process Intelligence (PI) to refer to the
amalgamation of process-centric data-driven techniques able to deal with a
variety of object and event types, enabling AI in an organizational context.
This paper explains why AI requires PI to improve operational processes and
highlights opportunities for successfully combining OCPM and generative,
predictive, and prescriptive AI.

</details>


### [21] [Algorithmic Detection of Rank Reversals, Transitivity Violations, and Decomposition Inconsistencies in Multi-Criteria Decision Analysis](https://arxiv.org/abs/2508.00129)
*Agustín Borda,Juan Bautista Cabral,Gonzalo Giarda,Diego Nicolás Gimenez Irusta,Paula Pacheco,Alvaro Roy Schachner*

Main category: cs.AI

TL;DR: 本文提出三种检测多准则决策分析中排名反转问题的测试方法，并在Scikit-Criteria库中实现，讨论了其设计考虑及对方法评估的潜在影响。


<details>
  <summary>Details</summary>
Motivation: 多准则决策分析中的排名反转问题严重影响结果准确性，需有效机制评估方法性能。

Method: 提出三种测试方法检测排名反转，并在Scikit-Criteria库中实现，解决通用场景下的实现复杂性。

Result: 成功实现测试方法并探讨其设计挑战。

Conclusion: 这些测试方法可显著提升多准则决策方法的评估能力。

Abstract: In Multi-Criteria Decision Analysis, Rank Reversals are a serious problem
that can greatly affect the results of a Multi-Criteria Decision Method against
a particular set of alternatives. It is therefore useful to have a mechanism
that allows one to measure the performance of a method on a set of
alternatives. This idea could be taken further to build a global ranking of the
effectiveness of different methods to solve a problem. In this paper, we
present three tests that detect the presence of Rank Reversals, along with
their implementation in the Scikit-Criteria library. We also address the
complications that arise when implementing these tests for general scenarios
and the design considerations we made to handle them. We close with a
discussion about how these additions could play a major role in the judgment of
multi-criteria decision methods for problem solving.

</details>


### [22] [SHACL Validation under Graph Updates (Extended Paper)](https://arxiv.org/abs/2508.00137)
*Shqiponja Ahmetaj,George Konstantinidis,Magdalena Ortiz,Paolo Pareti,Mantas Simkus*

Main category: cs.AI

TL;DR: 本文研究了RDF图中SHACL验证在更新下的问题，提出了一种基于SHACL的更新语言，并通过回归技术将静态验证问题转化为SHACL约束的（不）可满足性问题。


<details>
  <summary>Details</summary>
Motivation: 研究RDF图在更新后仍满足SHACL规范的静态验证问题，为演化RDF图的推理服务提供基础。

Method: 提出SHACL更新语言，使用回归技术将更新操作嵌入SHACL约束，分析计算复杂性，并实现原型系统进行实验验证。

Result: 静态验证问题可转化为SHACL约束的（不）可满足性问题，分析了计算复杂性，并通过实验验证了原型系统的行为。

Conclusion: 本文为RDF图的动态验证提供了理论基础和实用工具，扩展了SHACL的应用场景。

Abstract: SHACL (SHApe Constraint Language) is a W3C standardized constraint language
for RDF graphs. In this paper, we study SHACL validation in RDF graphs under
updates. We present a SHACL-based update language that can capture intuitive
and realistic modifications on RDF graphs and study the problem of static
validation under such updates. This problem asks to verify whether every graph
that validates a SHACL specification will still do so after applying a given
update sequence. More importantly, it provides a basis for further services for
reasoning about evolving RDF graphs. Using a regression technique that embeds
the update actions into SHACL constraints, we show that static validation under
updates can be reduced to (un)satisfiability of constraints in (a minor
extension of) SHACL. We analyze the computational complexity of the static
validation problem for SHACL and some key fragments. Finally, we present a
prototype implementation that performs static validation and other static
analysis tasks on SHACL constraints and demonstrate its behavior through
preliminary experiments.

</details>


### [23] [Co-Producing AI: Toward an Augmented, Participatory Lifecycle](https://arxiv.org/abs/2508.00138)
*Rashid Mushkani,Hugo Berard,Toumadher Ammar,Cassandre Chatonnier,Shin Koseki*

Main category: cs.AI

TL;DR: 论文提出了一种基于设计正义、扩展学习理论和参与式AI的AI生命周期重构方法，强调共生产、多样性、公平性和多学科合作。


<details>
  <summary>Details</summary>
Motivation: AI算法对文化边缘群体的不公平影响需要根本性解决方案，现有方法如伦理指南和技术公平性改进不足。

Method: 提出一个包含五个阶段的增强AI生命周期（共框架、共设计、共实施、共部署、共维护），基于多学科研讨会和分布式权威理念。

Result: 该方法与主流伦理框架关联，并提出了扩展参与式治理的关键研究问题。

Conclusion: 重构AI生产流程以共生产为核心，是减少AI对边缘群体伤害的有效途径。

Abstract: Despite efforts to mitigate the inherent risks and biases of artificial
intelligence (AI) algorithms, these algorithms can disproportionately impact
culturally marginalized groups. A range of approaches has been proposed to
address or reduce these risks, including the development of ethical guidelines
and principles for responsible AI, as well as technical solutions that promote
algorithmic fairness. Drawing on design justice, expansive learning theory, and
recent empirical work on participatory AI, we argue that mitigating these harms
requires a fundamental re-architecture of the AI production pipeline. This
re-design should center co-production, diversity, equity, inclusion (DEI), and
multidisciplinary collaboration. We introduce an augmented AI lifecycle
consisting of five interconnected phases: co-framing, co-design,
co-implementation, co-deployment, and co-maintenance. The lifecycle is informed
by four multidisciplinary workshops and grounded in themes of distributed
authority and iterative knowledge exchange. Finally, we relate the proposed
lifecycle to several leading ethical frameworks and outline key research
questions that remain for scaling participatory governance.

</details>


### [24] [Beyond Agreement: Rethinking Ground Truth in Educational AI Annotation](https://arxiv.org/abs/2508.00143)
*Danielle R. Thomas,Conrad Borchers,Kenneth R. Koedinger*

Main category: cs.AI

TL;DR: 论文指出传统的人类评分者间一致性（IRR）在AI教育应用中存在局限性，提出五种补充方法以提高数据标注质量和模型效果。


<details>
  <summary>Details</summary>
Motivation: 人类评分者存在偏见和不可靠性，传统IRR指标无法完全保证标注数据的有效性，阻碍了教育数据的分类进展。

Method: 提出五种补充方法：多标签标注方案、专家评估、闭环验证等，强调外部效度的重要性。

Result: 这些方法能比单独使用IRR产生更有效的训练数据和模型，提升学生学习效果和可操作性。

Conclusion: 呼吁重新思考标注质量和“真实标准”，优先考虑效度和教育影响，而非仅依赖共识。

Abstract: Humans can be notoriously imperfect evaluators. They are often biased,
unreliable, and unfit to define "ground truth." Yet, given the surging need to
produce large amounts of training data in educational applications using AI,
traditional inter-rater reliability (IRR) metrics like Cohen's kappa remain
central to validating labeled data. IRR remains a cornerstone of many machine
learning pipelines for educational data. Take, for example, the classification
of tutors' moves in dialogues or labeling open responses in machine-graded
assessments. This position paper argues that overreliance on human IRR as a
gatekeeper for annotation quality hampers progress in classifying data in ways
that are valid and predictive in relation to improving learning. To address
this issue, we highlight five examples of complementary evaluation methods,
such as multi-label annotation schemes, expert-based approaches, and
close-the-loop validity. We argue that these approaches are in a better
position to produce training data and subsequent models that produce improved
student learning and more actionable insights than IRR approaches alone. We
also emphasize the importance of external validity, for example, by
establishing a procedure of validating tutor moves and demonstrating that it
works across many categories of tutor actions (e.g., providing hints). We call
on the field to rethink annotation quality and ground truth--prioritizing
validity and educational impact over consensus alone.

</details>


### [25] [Model-Based Soft Maximization of Suitable Metrics of Long-Term Human Power](https://arxiv.org/abs/2508.00159)
*Jobst Heitzig,Ram Potham*

Main category: cs.AI

TL;DR: 论文探讨通过强制AI代理明确赋能人类并管理人类与AI之间的权力平衡，以促进安全和福祉。提出一种参数化和可分解的目标函数，考虑人类有限理性和社会规范，并通过算法计算该指标。


<details>
  <summary>Details</summary>
Motivation: 研究AI安全中的权力概念，探讨如何通过管理人类与AI的权力平衡，同时提升人类福祉。

Method: 采用部分公理化方法设计目标函数，并通过逆向归纳或多智能体强化学习计算该指标。

Result: 在多种典型情境中验证了该指标的效果，表明其可能比直接基于效用的目标更安全。

Conclusion: 软最大化适合的人类权力聚合指标可能是AI系统的有益目标，比直接效用目标更安全。

Abstract: Power is a key concept in AI safety: power-seeking as an instrumental goal,
sudden or gradual disempowerment of humans, power balance in human-AI
interaction and international AI governance. At the same time, power as the
ability to pursue diverse goals is essential for wellbeing.
  This paper explores the idea of promoting both safety and wellbeing by
forcing AI agents explicitly to empower humans and to manage the power balance
between humans and AI agents in a desirable way. Using a principled, partially
axiomatic approach, we design a parametrizable and decomposable objective
function that represents an inequality- and risk-averse long-term aggregate of
human power. It takes into account humans' bounded rationality and social
norms, and, crucially, considers a wide variety of possible human goals.
  We derive algorithms for computing that metric by backward induction or
approximating it via a form of multi-agent reinforcement learning from a given
world model. We exemplify the consequences of (softly) maximizing this metric
in a variety of paradigmatic situations and describe what instrumental
sub-goals it will likely imply. Our cautious assessment is that softly
maximizing suitable aggregate metrics of human power might constitute a
beneficial objective for agentic AI systems that is safer than direct
utility-based objectives.

</details>


### [26] [RL-PLUS: Countering Capability Boundary Collapse of LLMs in Reinforcement Learning with Hybrid-policy Optimization](https://arxiv.org/abs/2508.00222)
*Yihong Dong,Xue Jiang,Yongding Tao,Huanyu Liu,Kechi Zhang,Lili Mou,Rongyu Cao,Yingwei Ma,Jue Chen,Binhua Li,Zhi Jin,Fei Huang,Yongbin Li,Ge Li*

Main category: cs.AI

TL;DR: RL-PLUS通过结合内部探索与外部数据，解决了RLVR方法在LLM能力边界和稀疏奖励上的限制，显著提升了推理能力。


<details>
  <summary>Details</summary>
Motivation: RLVR方法因固有的策略和稀疏奖励问题，难以突破基础LLM的能力边界，甚至导致能力边界崩溃。

Method: RL-PLUS结合多重重要性采样和基于探索的优势函数，整合内部思考与外部学习。

Result: 在六个数学推理基准和六个分布外任务上表现优异，平均相对提升21.1%至69.2%。

Conclusion: RL-PLUS有效解决了能力边界崩溃问题，并在多种模型家族中表现一致优越。

Abstract: Reinforcement Learning with Verifiable Reward (RLVR) has significantly
advanced the complex reasoning abilities of Large Language Models (LLMs).
However, it struggles to break through the inherent capability boundaries of
the base LLM, due to its inherently on-policy strategy with LLM's immense
action space and sparse reward. Further, RLVR can lead to the capability
boundary collapse, narrowing the LLM's problem-solving scope. To address this
problem, we propose RL-PLUS, a novel approach that synergizes internal
exploitation (i.e., Thinking) with external data (i.e., Learning) to achieve
stronger reasoning capabilities and surpass the boundaries of base models.
RL-PLUS integrates two core components: Multiple Importance Sampling to address
for distributional mismatch from external data, and an Exploration-Based
Advantage Function to guide the model towards high-value, unexplored reasoning
paths. We provide both theoretical analysis and extensive experiments to
demonstrate the superiority and generalizability of our approach. The results
show that RL-PLUS achieves state-of-the-art performance compared with existing
RLVR methods on six math reasoning benchmarks and exhibits superior performance
on six out-of-distribution reasoning tasks. It also achieves consistent and
significant gains across diverse model families, with average relative
improvements ranging from 21.1\% to 69.2\%. Moreover, Pass@k curves across
multiple benchmarks indicate that RL-PLUS effectively resolves the capability
boundary collapse problem.

</details>


### [27] [MetaAgent: Toward Self-Evolving Agent via Tool Meta-Learning](https://arxiv.org/abs/2508.00271)
*Hongjin Qian,Zheng Liu*

Main category: cs.AI

TL;DR: MetaAgent是一种基于学习实践原则的智能代理范式，通过持续自我改进和任务实践提升能力。它通过生成自然语言请求、自我反思和知识库构建，逐步优化推理和工具使用策略，无需调整模型参数或额外训练。


<details>
  <summary>Details</summary>
Motivation: 旨在开发一种能够通过实践和自我改进持续提升能力的智能代理系统，解决知识发现任务中的挑战。

Method: MetaAgent从基础工作流出发，通过生成帮助请求、自我反思和验证答案，动态整合经验并构建内部工具和知识库。

Result: 在GAIA、WebWalkerQA和BrowseCamp等基准测试中，MetaAgent表现优于基于工作流的基线模型，并匹配或超越端到端训练的代理。

Conclusion: MetaAgent展示了自我进化代理系统在通用知识发现任务中的潜力，为未来智能代理的发展提供了新方向。

Abstract: In this work, we propose MetaAgent, an agentic paradigm inspired by the
principle of learning-by-doing, where expertise is developed through hands-on
practice and continual self-improvement. MetaAgent starts with a minimal
workflow, equipped only with basic reasoning and adaptive help-seeking
abilities. When a knowledge gap is encountered, MetaAgent generates natural
language help requests, which are routed to the most suitable external tool by
a dedicated tool router. As MetaAgent solves tasks, it continually conducts
self-reflection and answer verification, distilling actionable experience into
concise texts that are dynamically incorporated into future task contexts.
Besides, MetaAgent autonomously builds in-house tools and a persistent
knowledge base by organizing its tool-use history, further enhancing its
ability to retrieve and integrate relevant information We term this continual,
data-driven process as \textit{meta tool learning}, through which MetaAgent
incrementally refines its reasoning and tool-use strategies, without changing
model parameters or requiring further post-training. Evaluated on challenging
knowledge discovery benchmarks, including GAIA, WebWalkerQA, and BrowseCamp,
MetaAgent consistently outperforms workflow-based baselines and matches or
exceeds end-to-end trained agents, demonstrating the promise of self-evolving
agentic systems for robust, general-purpose knowledge discovery. We provide our
source codes in https://github.com/qhjqhj00/MetaAgent.

</details>


### [28] [Mind the Gap: The Divergence Between Human and LLM-Generated Tasks](https://arxiv.org/abs/2508.00282)
*Yi-Long Lu,Jiajun Song,Chunhui Zhang,Wei Wang*

Main category: cs.AI

TL;DR: 论文比较了人类和LLM（GPT-4o）在任务生成中的差异，发现人类受心理驱动因素影响，而LLM即使提供这些因素也无法模拟人类行为，任务生成偏向抽象且缺乏社交和物理性。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM是否能模拟人类基于心理驱动的任务生成行为。

Method: 通过任务生成实验，比较人类和GPT-4o的行为模式。

Result: 人类任务生成受价值观和认知风格影响，LLM生成的任务更抽象、缺乏社交和物理性，尽管被认为更有趣和新颖。

Conclusion: LLM与人类认知存在核心差距，需融入内在动机和物理基础以设计更贴近人类的智能体。

Abstract: Humans constantly generate a diverse range of tasks guided by internal
motivations. While generative agents powered by large language models (LLMs)
aim to simulate this complex behavior, it remains uncertain whether they
operate on similar cognitive principles. To address this, we conducted a
task-generation experiment comparing human responses with those of an LLM agent
(GPT-4o). We find that human task generation is consistently influenced by
psychological drivers, including personal values (e.g., Openness to Change) and
cognitive style. Even when these psychological drivers are explicitly provided
to the LLM, it fails to reflect the corresponding behavioral patterns. They
produce tasks that are markedly less social, less physical, and thematically
biased toward abstraction. Interestingly, while the LLM's tasks were perceived
as more fun and novel, this highlights a disconnect between its linguistic
proficiency and its capacity to generate human-like, embodied goals.We conclude
that there is a core gap between the value-driven, embodied nature of human
cognition and the statistical patterns of LLMs, highlighting the necessity of
incorporating intrinsic motivation and physical grounding into the design of
more human-aligned agents.

</details>


### [29] [Oedipus and the Sphinx: Benchmarking and Improving Visual Language Models for Complex Graphic Reasoning](https://arxiv.org/abs/2508.00323)
*Jianyi Zhang,Xu Ji,Ziyin Zhou,Yuchen Zhou,Shubo Shi,Haoyu Wu,Zhen Li,Shizhao Liu*

Main category: cs.AI

TL;DR: 论文提出了ReasonBench，首个专注于结构化图形推理任务的评估基准，用于评估视觉语言模型（VLM）在复杂图形推理中的表现，并揭示了当前模型的局限性。通过双优化策略（DiaCoT和ReasonTune），VLM性能提升了33.5%。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注简单图形推理，而VLM在复杂图形推理和抽象问题解决方面表现不足，因此需要专门的评估工具和改进方法。

Method: 提出ReasonBench基准，包含1,613个来自真实世界智力测试的问题，涵盖位置、属性、数量和多元素任务。对11种主流VLM进行评测，并提出DiaCoT（分层分解增强可解释性）和ReasonTune（训练增强任务适应性）的双优化策略。

Result: 评测显示当前VLM在复杂图形推理中存在显著局限性。双优化策略使VLM性能提升了33.5%。

Conclusion: ReasonBench为VLM在复杂图形推理中的评估提供了全面工具，双优化策略显著提升了模型性能，为未来研究提供了方向。

Abstract: Evaluating the performance of visual language models (VLMs) in graphic
reasoning tasks has become an important research topic. However, VLMs still
show obvious deficiencies in simulating human-level graphic reasoning
capabilities, especially in complex graphic reasoning and abstract problem
solving, which are less studied and existing studies only focus on simple
graphics. To evaluate the performance of VLMs in complex graphic reasoning, we
propose ReasonBench, the first evaluation benchmark focused on structured
graphic reasoning tasks, which includes 1,613 questions from real-world
intelligence tests. ReasonBench covers reasoning dimensions related to
location, attribute, quantity, and multi-element tasks, providing a
comprehensive evaluation of the performance of VLMs in spatial, relational, and
abstract reasoning capabilities. We benchmark 11 mainstream VLMs (including
closed-source and open-source models) and reveal significant limitations of
current models. Based on these findings, we propose a dual optimization
strategy: Diagrammatic Reasoning Chain (DiaCoT) enhances the interpretability
of reasoning by decomposing layers, and ReasonTune enhances the task
adaptability of model reasoning through training, all of which improves VLM
performance by 33.5\%. All experimental data and code are in the repository:
https://huggingface.co/datasets/cistine/ReasonBench.

</details>


### [30] [R1-ACT: Efficient Reasoning Model Safety Alignment by Activating Safety Knowledge](https://arxiv.org/abs/2508.00324)
*Yeonjun In,Wonjoong Kim,Sangwu Park,Chanyoung Park*

Main category: cs.AI

TL;DR: 论文提出R1-Act方法，通过结构化推理触发大型推理模型的安全知识，显著提升安全性且不影响推理性能。


<details>
  <summary>Details</summary>
Motivation: 尽管大型推理模型（LRMs）在复杂任务上表现出色，但研究发现其易执行有害指令，引发安全隐患。

Method: 提出R1-Act，一种简单高效的后训练方法，通过结构化推理显式触发安全知识。

Result: R1-Act显著提升安全性且保持推理性能，仅需少量训练资源和时间。

Conclusion: R1-Act在多个LRM上验证了其鲁棒性、可扩展性和实用效率。

Abstract: Although large reasoning models (LRMs) have demonstrated impressive
capabilities on complex tasks, recent studies reveal that these models
frequently fulfill harmful user instructions, raising significant safety
concerns. In this paper, we investigate the underlying cause of LRM safety
risks and find that models already possess sufficient safety knowledge but fail
to activate it during reasoning. Based on this insight, we propose R1-Act, a
simple and efficient post-training method that explicitly triggers safety
knowledge through a structured reasoning process. R1-Act achieves strong safety
improvements while preserving reasoning performance, outperforming prior
alignment methods. Notably, it requires only 1,000 training examples and 90
minutes of training on a single RTX A6000 GPU. Extensive experiments across
multiple LRM backbones and sizes demonstrate the robustness, scalability, and
practical efficiency of our approach.

</details>


### [31] [CoRGI: Verified Chain-of-Thought Reasoning with Visual Grounding](https://arxiv.org/abs/2508.00378)
*Shixin Yi,Lin Shang*

Main category: cs.AI

TL;DR: CoRGI框架通过视觉验证改进多模态推理，减少幻觉，提升解释的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有CoT提示在视觉语言模型中生成的解释缺乏视觉内容支持，导致幻觉问题。

Method: 提出CoRGI框架，分三阶段：生成文本推理链、提取视觉证据、合成验证答案。

Result: 在VCR基准测试中，CoRGI提升了Qwen-2.5VL和LLaVA-1.6的性能，生成更准确的解释。

Conclusion: 视觉验证对提升多模态推理的鲁棒性至关重要，CoRGI展示了其有效性。

Abstract: Chain-of-Thought (CoT) prompting has shown promise in improving reasoning in
vision-language models (VLMs), but it often produces explanations that are
linguistically fluent yet lack grounding in visual content. We observe that
such hallucinations arise in part from the absence of an explicit verification
mechanism during multi-step reasoning. To address this, we propose
\textbf{CoRGI}(\textbf{C}hain \textbf{o}f \textbf{R}easoning with
\textbf{G}rounded \textbf{I}nsights), a modular framework that introduces
visual verification into the reasoning process. CoRGI follows a three-stage
pipeline: it first generates a textual reasoning chain, then extracts
supporting visual evidence for each reasoning step via a dedicated module
(VEVM), and finally synthesizes the textual rationale with visual evidence to
generate a grounded, verified answer. The framework can be integrated with
existing VLMs without end-to-end retraining. We evaluate CoRGI on the VCR
benchmark and find that it improves reasoning performance on two representative
open-source VLM backbones, Qwen-2.5VL and LLaVA-1.6. Ablation studies confirm
the contribution of each step in the verification module, and human evaluations
suggest that CoRGI leads to more factual and helpful explanations. We also
examine alternative designs for the visual verification step and discuss
potential limitations of post-hoc verification frameworks. These findings
highlight the importance of grounding intermediate reasoning steps in visual
evidence to enhance the robustness of multimodal reasoning.

</details>


### [32] [Theory of Mind Using Active Inference: A Framework for Multi-Agent Cooperation](https://arxiv.org/abs/2508.00401)
*Riddhi J. Pitliya,Ozan Catal,Toon Van de Maele,Corrado Pezzato,Tim Verbelen*

Main category: cs.AI

TL;DR: 提出了一种基于心智理论（ToM）的多智能体协作新方法，通过主动推理实现，无需任务特定的共享生成模型或显式通信。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体协作中如何理解并推理其他智能体的信念和目标的问题，提升协作效率。

Method: 扩展推理树规划算法，通过递归推理探索联合策略空间，智能体维护自身和他人的信念与目标表征。

Result: 在避碰和觅食任务中，ToM智能体表现优于非ToM智能体，能避免碰撞并减少冗余努力。

Conclusion: 该方法为人工智能的实践应用提供了新思路，并为心智理论的计算研究提供了见解。

Abstract: We present a novel approach to multi-agent cooperation by implementing theory
of mind (ToM) within active inference. ToM - the ability to understand that
others can have differing knowledge and goals - enables agents to reason about
others' beliefs while planning their own actions. Unlike previous active
inference approaches to multi-agent cooperation, our method neither relies on
task-specific shared generative models nor requires explicit communication,
while being generalisable. In our framework, the ToM-equipped agent maintains
distinct representations of its own and others' beliefs and goals. We extend
the sophisticated inference tree-based planning algorithm to systematically
explore joint policy spaces through recursive reasoning. Our approach is
evaluated through collision avoidance and foraging task simulations. Results
demonstrate that ToM-equipped agents cooperate better compared to non-ToM
counterparts by being able to avoid collisions and reduce redundant efforts.
Crucially, ToM agents accomplish this by inferring others' beliefs solely from
observable behaviour. This work advances practical applications in artificial
intelligence while providing computational insights into ToM.

</details>


### [33] [Cognitive Kernel-Pro: A Framework for Deep Research Agents and Agent Foundation Models Training](https://arxiv.org/abs/2508.00414)
*Tianqing Fang,Zhisong Zhang,Xiaoyang Wang,Rui Wang,Can Qin,Yuxuan Wan,Jun-Yu Ma,Ce Zhang,Jiaqi Chen,Xiyun Li,Hongming Zhang,Haitao Mi,Dong Yu*

Main category: cs.AI

TL;DR: Cognitive Kernel-Pro 是一个完全开源且免费的 AI 代理框架，旨在推动高级 AI 代理的开发与评估。


<details>
  <summary>Details</summary>
Motivation: 当前 AI 代理系统多为闭源或依赖付费 API，限制了研究的可访问性和可复现性。

Method: 通过高质量训练数据的构建和多模块设计，结合测试时反思与投票策略提升代理性能。

Result: 在 GAIA 基准测试中，8B 参数的开源模型表现优于 WebDancer 和 WebSailor，达到新标准。

Conclusion: Cognitive Kernel-Pro 为开源 AI 代理提供了高性能且易用的解决方案。

Abstract: General AI Agents are increasingly recognized as foundational frameworks for
the next generation of artificial intelligence, enabling complex reasoning, web
interaction, coding, and autonomous research capabilities. However, current
agent systems are either closed-source or heavily reliant on a variety of paid
APIs and proprietary tools, limiting accessibility and reproducibility for the
research community. In this work, we present \textbf{Cognitive Kernel-Pro}, a
fully open-source and (to the maximum extent) free multi-module agent framework
designed to democratize the development and evaluation of advanced AI agents.
Within Cognitive Kernel-Pro, we systematically investigate the curation of
high-quality training data for Agent Foundation Models, focusing on the
construction of queries, trajectories, and verifiable answers across four key
domains: web, file, code, and general reasoning. Furthermore, we explore novel
strategies for agent test-time reflection and voting to enhance agent
robustness and performance. We evaluate Cognitive Kernel-Pro on GAIA, achieving
state-of-the-art results among open-source and free agents. Notably, our
8B-parameter open-source model surpasses previous leading systems such as
WebDancer and WebSailor, establishing a new performance standard for
accessible, high-capability AI agents. Code is available at
https://github.com/Tencent/CognitiveKernel-Pro

</details>


### [34] [Thinking Machines: Mathematical Reasoning in the Age of LLMs](https://arxiv.org/abs/2508.00459)
*Andrea Asperti,Alberto Naibo,Claudio Sacerdoti Coen*

Main category: cs.AI

TL;DR: 论文探讨了大型语言模型（LLMs）在数学领域的应用，尤其是形式化数学证明中的挑战，分析了其与代码生成的差异，并提出了三个核心问题。


<details>
  <summary>Details</summary>
Motivation: LLMs在编程任务中表现出色，但在形式化数学证明中进展缓慢，这引发了对LLM推理方式、监督机制及内部状态表示的疑问。

Method: 通过分析最新模型和基准测试，探讨了形式化与非形式化数学训练、证明生成的脆弱性以及LLM是否真正表示逻辑状态。

Result: 研究发现，LLMs在形式化数学证明中的表现不如代码生成稳定，且其内部状态表示尚不明确。

Conclusion: 论文旨在明确当前LLMs在数学领域的局限性，并提出可能的扩展方向。

Abstract: Large Language Models (LLMs) have shown remarkable abilities in structured
reasoning and symbolic tasks, with coding emerging as a particular area of
strength. This success has sparked growing interest in applying LLMs to
mathematics, both in informal problem-solving and formal theorem proving.
However, progress in formal mathematics has proven to be significantly more
difficult, despite surface-level similarities between programming and proof
construction. This discrepancy raises important questions about how LLMs
``reason'', how they are supervised, and whether they internally track a notion
of computational or deductive state. In this article, we address the
state-of-the-art of the discipline, focusing on recent models and benchmarks,
and explore three central issues at the intersection of machine learning and
mathematical cognition: (i) the trade-offs between formal and informal
mathematics as training domains; (ii) the deeper reasons why proof generation
remains more brittle than code synthesis; (iii) and the question of whether
LLMs represent, or merely mimic, a notion of evolving logical state. Our goal
is not to draw hard boundaries, but to identify where the current limits lie,
and how they might be extended.

</details>


### [35] [Pro2Guard: Proactive Runtime Enforcement of LLM Agent Safety via Probabilistic Model Checking](https://arxiv.org/abs/2508.00500)
*Haoyu Wang,Chris M. Poskitt,Jun Sun,Jiali Wei*

Main category: cs.AI

TL;DR: Pro2Guard是一种基于概率可达性分析的主动运行时安全框架，用于预测和防止LLM代理的不安全行为。


<details>
  <summary>Details</summary>
Motivation: 现有基于规则的安全系统（如AgentSpec）缺乏预见性，难以处理长期依赖和分布偏移，导致安全风险。

Method: Pro2Guard将代理行为抽象为符号状态，从执行轨迹中学习离散时间马尔可夫链（DTMC），并在运行时预测不安全状态的概率。

Result: 在家庭代理和自动驾驶场景中，Pro2Guard分别实现了93.6%和100%的安全预测成功率，并能提前干预。

Conclusion: Pro2Guard通过主动预测和干预，显著提升了LLM代理的安全性，同时保持了任务完成率。

Abstract: Large Language Model (LLM) agents exhibit powerful autonomous capabilities
across domains such as robotics, virtual assistants, and web automation.
However, their stochastic behavior introduces significant safety risks that are
difficult to anticipate. Existing rule-based enforcement systems, such as
AgentSpec, focus on developing reactive safety rules, which typically respond
only when unsafe behavior is imminent or has already occurred. These systems
lack foresight and struggle with long-horizon dependencies and distribution
shifts. To address these limitations, we propose Pro2Guard, a proactive runtime
enforcement framework grounded in probabilistic reachability analysis.
Pro2Guard abstracts agent behaviors into symbolic states and learns a
Discrete-Time Markov Chain (DTMC) from execution traces. At runtime, it
anticipates future risks by estimating the probability of reaching unsafe
states, triggering interventions before violations occur when the predicted
risk exceeds a user-defined threshold. By incorporating semantic validity
checks and leveraging PAC bounds, Pro2Guard ensures statistical reliability
while approximating the underlying ground-truth model. We evaluate Pro2Guard
extensively across two safety-critical domains: embodied household agents and
autonomous vehicles. In embodied agent tasks, Pro2Guard enforces safety early
on up to 93.6% of unsafe tasks using low thresholds, while configurable modes
(e.g., reflect) allow balancing safety with task success, maintaining up to
80.4% task completion. In autonomous driving scenarios, Pro2Guard achieves 100%
prediction of traffic law violations and collisions, anticipating risks up to
38.66 seconds ahead.

</details>


### [36] [MultiSHAP: A Shapley-Based Framework for Explaining Cross-Modal Interactions in Multimodal AI Models](https://arxiv.org/abs/2508.00576)
*Zhanliang Wang,Kai Wang*

Main category: cs.AI

TL;DR: MultiSHAP是一个模型无关的可解释性框架，用于量化多模态AI模型中视觉和文本元素之间的协同效应，适用于开源和闭源模型。


<details>
  <summary>Details</summary>
Motivation: 多模态AI模型的'黑盒'特性在高风险应用中限制了其部署，现有解释方法无法精确量化模态间的协同效应。

Method: 利用Shapley Interaction Index，MultiSHAP为细粒度视觉和文本元素（如图像块和文本标记）提供实例级和数据集级解释。

Result: 实验证实MultiSHAP能准确捕捉跨模态推理机制，并在实际案例中展示其实用性。

Conclusion: MultiSHAP为解释复杂多模态AI模型提供了通用解决方案，并可扩展到两种以上模态。

Abstract: Multimodal AI models have achieved impressive performance in tasks that
require integrating information from multiple modalities, such as vision and
language. However, their "black-box" nature poses a major barrier to deployment
in high-stakes applications where interpretability and trustworthiness are
essential. How to explain cross-modal interactions in multimodal AI models
remains a major challenge. While existing model explanation methods, such as
attention map and Grad-CAM, offer coarse insights into cross-modal
relationships, they cannot precisely quantify the synergistic effects between
modalities, and are limited to open-source models with accessible internal
weights. Here we introduce MultiSHAP, a model-agnostic interpretability
framework that leverages the Shapley Interaction Index to attribute multimodal
predictions to pairwise interactions between fine-grained visual and textual
elements (such as image patches and text tokens), while being applicable to
both open- and closed-source models. Our approach provides: (1) instance-level
explanations that reveal synergistic and suppressive cross-modal effects for
individual samples - "why the model makes a specific prediction on this input",
and (2) dataset-level explanation that uncovers generalizable interaction
patterns across samples - "how the model integrates information across
modalities". Experiments on public multimodal benchmarks confirm that MultiSHAP
faithfully captures cross-modal reasoning mechanisms, while real-world case
studies demonstrate its practical utility. Our framework is extensible beyond
two modalities, offering a general solution for interpreting complex multimodal
AI models.

</details>


### [37] [From EMR Data to Clinical Insight: An LLM-Driven Framework for Automated Pre-Consultation Questionnaire Generation](https://arxiv.org/abs/2508.00581)
*Ruiqing Ding,Qianfang Sun,Yongkang Leng,Hui Yin,Xiaojian Li*

Main category: cs.AI

TL;DR: 提出了一种多阶段LLM驱动框架，用于从复杂电子病历生成全面的预咨询问卷，解决了直接LLM方法在信息完整性、逻辑顺序和疾病级合成方面的不足。


<details>
  <summary>Details</summary>
Motivation: 预咨询是医疗保健的关键环节，但现有方法难以从复杂电子病历中生成全面的问卷，尤其是直接LLM方法在信息完整性和逻辑性上表现不佳。

Method: 采用三阶段框架：1）提取原子断言；2）构建个人因果网络并合成疾病知识；3）生成个性化及标准化问卷。

Result: 在真实电子病历数据集和临床专家验证下，该方法在信息覆盖、诊断相关性、可理解性和生成时间上表现优越。

Conclusion: 该框架通过构建显式临床知识，显著提升了预咨询问卷的质量和效率，具有实际应用潜力。

Abstract: Pre-consultation is a critical component of effective healthcare delivery.
However, generating comprehensive pre-consultation questionnaires from complex,
voluminous Electronic Medical Records (EMRs) is a challenging task. Direct
Large Language Model (LLM) approaches face difficulties in this task,
particularly regarding information completeness, logical order, and
disease-level synthesis. To address this issue, we propose a novel multi-stage
LLM-driven framework: Stage 1 extracts atomic assertions (key facts with
timing) from EMRs; Stage 2 constructs personal causal networks and synthesizes
disease knowledge by clustering representative networks from an EMR corpus;
Stage 3 generates tailored personal and standardized disease-specific
questionnaires based on these structured representations. This framework
overcomes limitations of direct methods by building explicit clinical
knowledge. Evaluated on a real-world EMR dataset and validated by clinical
experts, our method demonstrates superior performance in information coverage,
diagnostic relevance, understandability, and generation time, highlighting its
practical potential to enhance patient information collection.

</details>


### [38] [Multi-Agent Game Generation and Evaluation via Audio-Visual Recordings](https://arxiv.org/abs/2508.00632)
*Alexia Jolicoeur-Martineau*

Main category: cs.AI

TL;DR: 论文提出AVR-Eval评估指标和AVR-Agent多智能体系统，用于生成和评估交互式多媒体内容（如游戏），解决了现有LLMs在复杂内容生成和自动化评估上的不足。


<details>
  <summary>Details</summary>
Motivation: 当前AI在生成交互式多媒体内容（如视频游戏）时面临挑战，缺乏自动化评估指标且难以处理复杂内容。

Method: 提出AVR-Eval评估指标，通过多模态模型比较内容质量；开发AVR-Agent多智能体系统，生成并迭代优化JavaScript代码。

Result: AVR-Agent生成的内容在实验中表现优于单次生成内容，但模型未能有效利用自定义资源和反馈。

Conclusion: 研究表明，当前模型在利用高质量资源和反馈方面与人类存在差距，揭示了机器与人类内容创作的根本差异。

Abstract: While AI excels at generating text, audio, images, and videos, creating
interactive audio-visual content such as video games remains challenging.
Current LLMs can generate JavaScript games and animations, but lack automated
evaluation metrics and struggle with complex content that normally requires
teams of humans working for many months (multi-shot, multi-agents) using assets
made by artists. To tackle these issues, we built a new metric and a
multi-agent system.
  We propose AVR-Eval, a relative metric for multimedia content quality using
Audio-Visual Recordings (AVRs). An omni-modal model (processing text, video,
and audio) compares the AVRs of two contents, with a text model reviewing
evaluations to determine superiority. We show that AVR-Eval properly identifies
good from broken or mismatched content.
  We built AVR-Agent, a multi-agent system generating JavaScript code from a
bank of multimedia assets (audio, images, 3D models). The coding agent selects
relevant assets, generates multiple initial codes, uses AVR-Eval to identify
the best version, and iteratively improves it through omni-modal agent feedback
from the AVR.
  We run experiments on games and animations with AVR-Eval (win rate of content
A against B). We find that content generated by AVR-Agent has a significantly
higher win rate against content made through one-shot generation. However,
models struggle to leverage custom assets and AVR feedback effectively, showing
no higher win rate. This reveals a critical gap: while humans benefit from
high-quality assets and audio-visual feedback, current coding models do not
seem to utilize these resources as effectively, highlighting fundamental
differences between human and machine content creation approaches.

</details>


### [39] [Multi-Band Variable-Lag Granger Causality: A Unified Framework for Causal Time Series Inference across Frequencies](https://arxiv.org/abs/2508.00658)
*Chakattrai Sookkongwaree,Tattep Lakmuang,Chainarong Amornbunchornvej*

Main category: cs.AI

TL;DR: 论文提出了一种多频带可变滞后Granger因果关系（MB-VLGC）框架，解决了传统Granger因果关系在复杂系统中固定滞后假设的局限性，并考虑了频率依赖的因果延迟。


<details>
  <summary>Details</summary>
Motivation: 传统Granger因果关系假设固定滞后，而可变滞后Granger因果关系（VLGC）虽解决了滞后可变问题，但未考虑频率带的影响。脑信号等复杂系统中，因果延迟可能随频率带变化。

Method: 提出MB-VLGC框架，明确建模频率依赖的因果延迟，提供理论定义并设计高效推理流程。

Result: 实验表明，MB-VLGC在合成和真实数据集上显著优于现有方法，具有广泛适用性。

Conclusion: MB-VLGC框架通过结合频率依赖的因果延迟，显著提升了Granger因果关系在复杂系统中的适用性和准确性。

Abstract: Understanding causal relationships in time series is fundamental to many
domains, including neuroscience, economics, and behavioral science. Granger
causality is one of the well-known techniques for inferring causality in time
series. Typically, Granger causality frameworks have a strong fix-lag
assumption between cause and effect, which is often unrealistic in complex
systems. While recent work on variable-lag Granger causality (VLGC) addresses
this limitation by allowing a cause to influence an effect with different time
lags at each time point, it fails to account for the fact that causal
interactions may vary not only in time delay but also across frequency bands.
For example, in brain signals, alpha-band activity may influence another region
with a shorter delay than slower delta-band oscillations. In this work, we
formalize Multi-Band Variable-Lag Granger Causality (MB-VLGC) and propose a
novel framework that generalizes traditional VLGC by explicitly modeling
frequency-dependent causal delays. We provide a formal definition of MB-VLGC,
demonstrate its theoretical soundness, and propose an efficient inference
pipeline. Extensive experiments across multiple domains demonstrate that our
framework significantly outperforms existing methods on both synthetic and
real-world datasets, confirming its broad applicability to any type of time
series data. Code and datasets are publicly available.

</details>


### [40] [Transparent Adaptive Learning via Data-Centric Multimodal Explainable AI](https://arxiv.org/abs/2508.00665)
*Maryam Mosleh,Marie Devlin,Ellis Solaiman*

Main category: cs.AI

TL;DR: 本文提出了一种结合传统可解释AI技术与生成式AI模型的混合框架，旨在为教育领域提供透明且个性化的解释。


<details>
  <summary>Details</summary>
Motivation: 当前自适应学习系统缺乏透明度，且现有可解释AI技术忽视用户角色和理解能力，亟需改进。

Method: 设计混合框架，整合传统XAI技术与生成式AI模型，结合用户个性化需求生成多模态解释。

Result: 重新定义可解释性为动态沟通过程，并提出框架设计及研究方向。

Conclusion: 目标是实现增强透明度并支持以用户为中心的体验的可解释AI。

Abstract: Artificial intelligence-driven adaptive learning systems are reshaping
education through data-driven adaptation of learning experiences. Yet many of
these systems lack transparency, offering limited insight into how decisions
are made. Most explainable AI (XAI) techniques focus on technical outputs but
neglect user roles and comprehension. This paper proposes a hybrid framework
that integrates traditional XAI techniques with generative AI models and user
personalisation to generate multimodal, personalised explanations tailored to
user needs. We redefine explainability as a dynamic communication process
tailored to user roles and learning goals. We outline the framework's design,
key XAI limitations in education, and research directions on accuracy,
fairness, and personalisation. Our aim is to move towards explainable AI that
enhances transparency while supporting user-centred experiences.

</details>


### [41] [Context-Aware Visualization for Explainable AI Recommendations in Social Media: A Vision for User-Aligned Explanations](https://arxiv.org/abs/2508.00674)
*Banan Alkhateeb,Ellis Solaiman*

Main category: cs.AI

TL;DR: 论文提出了一种用户分段的视觉解释系统，通过多样化的解释方法提升社交媒体的推荐透明度。


<details>
  <summary>Details</summary>
Motivation: 当前社交媒体推荐系统的解释性不足，缺乏与用户特定需求的匹配，导致用户对推荐内容的理解和信任度下降。

Method: 设计了一个用户分段和上下文感知的解释层，提供不同视觉化形式的解释（如技术详细版和简化版），并首次将解释风格（视觉 vs. 数字）和粒度（专家 vs. 普通用户）结合在一个框架中。

Result: 通过30名X用户的公开试点验证其对决策和信任的影响。

Conclusion: 该框架为社交媒体推荐系统提供了一种更透明、用户友好的解释方法，有望提升用户体验和信任。

Abstract: Social media platforms today strive to improve user experience through AI
recommendations, yet the value of such recommendations vanishes as users do not
understand the reasons behind them. This issue arises because explainability in
social media is general and lacks alignment with user-specific needs. In this
vision paper, we outline a user-segmented and context-aware explanation layer
by proposing a visual explanation system with diverse explanation methods. The
proposed system is framed by the variety of user needs and contexts, showing
explanations in different visualized forms, including a technically detailed
version for AI experts and a simplified one for lay users. Our framework is the
first to jointly adapt explanation style (visual vs. numeric) and granularity
(expert vs. lay) inside a single pipeline. A public pilot with 30 X users will
validate its impact on decision-making and trust.

</details>


### [42] [Unraveling Hidden Representations: A Multi-Modal Layer Analysis for Better Synthetic Content Forensics](https://arxiv.org/abs/2508.00784)
*Tom Or,Omri Azencot*

Main category: cs.AI

TL;DR: 论文提出了一种基于大型预训练多模态模型的通用生成内容检测方法，通过利用其潜在编码区分真实与虚假内容，实现了跨模态的高效检测。


<details>
  <summary>Details</summary>
Motivation: 生成模型在多个领域表现优异，但被恶意用于传播虚假信息，现有检测工具泛化能力有限，亟需通用且稳定的检测方法。

Method: 利用大型预训练多模态模型的潜在编码特征，训练线性分类器进行生成内容检测。

Result: 该方法在音频和图像领域表现优异，计算高效且适用于少样本场景，性能优于或匹配现有基线方法。

Conclusion: 研究表明预训练模型的潜在编码能有效区分真实与虚假内容，为通用生成内容检测提供了高效解决方案。

Abstract: Generative models achieve remarkable results in multiple data domains,
including images and texts, among other examples. Unfortunately, malicious
users exploit synthetic media for spreading misinformation and disseminating
deepfakes. Consequently, the need for robust and stable fake detectors is
pressing, especially when new generative models appear everyday. While the
majority of existing work train classifiers that discriminate between real and
fake information, such tools typically generalize only within the same family
of generators and data modalities, yielding poor results on other generative
classes and data domains. Towards a universal classifier, we propose the use of
large pre-trained multi-modal models for the detection of generative content.
Effectively, we show that the latent code of these models naturally captures
information discriminating real from fake. Building on this observation, we
demonstrate that linear classifiers trained on these features can achieve
state-of-the-art results across various modalities, while remaining
computationally efficient, fast to train, and effective even in few-shot
settings. Our work primarily focuses on fake detection in audio and images,
achieving performance that surpasses or matches that of strong baseline
methods.

</details>
